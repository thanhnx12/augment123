#############params############
cuda:0
Task=FewRel, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 12.7807016CurrentTrain: epoch  0, batch     1 | loss: 12.7539797CurrentTrain: epoch  0, batch     2 | loss: 12.5999565CurrentTrain: epoch  0, batch     3 | loss: 12.5775681CurrentTrain: epoch  0, batch     4 | loss: 12.2534428CurrentTrain: epoch  0, batch     5 | loss: 12.0535641CurrentTrain: epoch  0, batch     6 | loss: 11.9299412CurrentTrain: epoch  0, batch     7 | loss: 11.8334656CurrentTrain: epoch  0, batch     8 | loss: 11.6542406CurrentTrain: epoch  0, batch     9 | loss: 11.3954353CurrentTrain: epoch  0, batch    10 | loss: 11.2784853CurrentTrain: epoch  0, batch    11 | loss: 11.0749092CurrentTrain: epoch  0, batch    12 | loss: 11.1766300CurrentTrain: epoch  0, batch    13 | loss: 10.9853172CurrentTrain: epoch  0, batch    14 | loss: 10.6299648CurrentTrain: epoch  0, batch    15 | loss: 10.6395683CurrentTrain: epoch  0, batch    16 | loss: 10.9335289CurrentTrain: epoch  0, batch    17 | loss: 10.7724361CurrentTrain: epoch  0, batch    18 | loss: 10.2475872CurrentTrain: epoch  0, batch    19 | loss: 10.3488636CurrentTrain: epoch  0, batch    20 | loss: 10.7349796CurrentTrain: epoch  0, batch    21 | loss: 10.8694067CurrentTrain: epoch  0, batch    22 | loss: 10.3300247CurrentTrain: epoch  0, batch    23 | loss: 10.6644430CurrentTrain: epoch  0, batch    24 | loss: 10.2083092CurrentTrain: epoch  0, batch    25 | loss: 10.4096355CurrentTrain: epoch  0, batch    26 | loss: 10.8333778CurrentTrain: epoch  0, batch    27 | loss: 10.4295607CurrentTrain: epoch  0, batch    28 | loss: 10.0802040CurrentTrain: epoch  0, batch    29 | loss: 10.0121708CurrentTrain: epoch  0, batch    30 | loss: 9.5349045CurrentTrain: epoch  0, batch    31 | loss: 10.0359325CurrentTrain: epoch  0, batch    32 | loss: 10.5084333CurrentTrain: epoch  0, batch    33 | loss: 9.7875328CurrentTrain: epoch  0, batch    34 | loss: 10.1426582CurrentTrain: epoch  0, batch    35 | loss: 9.6395369CurrentTrain: epoch  0, batch    36 | loss: 9.6095905CurrentTrain: epoch  0, batch    37 | loss: 10.2514019CurrentTrain: epoch  0, batch    38 | loss: 9.2863846CurrentTrain: epoch  0, batch    39 | loss: 9.7686253CurrentTrain: epoch  0, batch    40 | loss: 9.5201225CurrentTrain: epoch  0, batch    41 | loss: 9.4223843CurrentTrain: epoch  0, batch    42 | loss: 9.5692406CurrentTrain: epoch  0, batch    43 | loss: 9.1514626CurrentTrain: epoch  0, batch    44 | loss: 9.1801224CurrentTrain: epoch  0, batch    45 | loss: 8.8626480CurrentTrain: epoch  0, batch    46 | loss: 8.9549904CurrentTrain: epoch  0, batch    47 | loss: 8.9507484CurrentTrain: epoch  0, batch    48 | loss: 8.8127728CurrentTrain: epoch  0, batch    49 | loss: 9.1142864CurrentTrain: epoch  0, batch    50 | loss: 8.2792244CurrentTrain: epoch  0, batch    51 | loss: 8.7598867CurrentTrain: epoch  0, batch    52 | loss: 8.7031498CurrentTrain: epoch  0, batch    53 | loss: 9.4445562CurrentTrain: epoch  0, batch    54 | loss: 8.5798998CurrentTrain: epoch  0, batch    55 | loss: 8.8594408CurrentTrain: epoch  0, batch    56 | loss: 8.5297556CurrentTrain: epoch  0, batch    57 | loss: 8.0497913CurrentTrain: epoch  0, batch    58 | loss: 8.0619164CurrentTrain: epoch  0, batch    59 | loss: 8.0606709CurrentTrain: epoch  0, batch    60 | loss: 7.9827671CurrentTrain: epoch  0, batch    61 | loss: 8.1937866CurrentTrain: epoch  0, batch    62 | loss: 6.7449417CurrentTrain: epoch  1, batch     0 | loss: 8.4355049CurrentTrain: epoch  1, batch     1 | loss: 7.8977480CurrentTrain: epoch  1, batch     2 | loss: 8.0915241CurrentTrain: epoch  1, batch     3 | loss: 7.5351739CurrentTrain: epoch  1, batch     4 | loss: 7.5956969CurrentTrain: epoch  1, batch     5 | loss: 8.0023746CurrentTrain: epoch  1, batch     6 | loss: 7.5479126CurrentTrain: epoch  1, batch     7 | loss: 7.3652711CurrentTrain: epoch  1, batch     8 | loss: 7.7264891CurrentTrain: epoch  1, batch     9 | loss: 7.4230328CurrentTrain: epoch  1, batch    10 | loss: 8.2561760CurrentTrain: epoch  1, batch    11 | loss: 8.5476646CurrentTrain: epoch  1, batch    12 | loss: 7.9492130CurrentTrain: epoch  1, batch    13 | loss: 7.7443485CurrentTrain: epoch  1, batch    14 | loss: 7.6516824CurrentTrain: epoch  1, batch    15 | loss: 8.6796303CurrentTrain: epoch  1, batch    16 | loss: 7.4179511CurrentTrain: epoch  1, batch    17 | loss: 7.5795631CurrentTrain: epoch  1, batch    18 | loss: 7.9991302CurrentTrain: epoch  1, batch    19 | loss: 7.3478260CurrentTrain: epoch  1, batch    20 | loss: 7.8383918CurrentTrain: epoch  1, batch    21 | loss: 7.3426986CurrentTrain: epoch  1, batch    22 | loss: 7.3429441CurrentTrain: epoch  1, batch    23 | loss: 7.3746061CurrentTrain: epoch  1, batch    24 | loss: 7.3166962CurrentTrain: epoch  1, batch    25 | loss: 7.4099517CurrentTrain: epoch  1, batch    26 | loss: 7.4666419CurrentTrain: epoch  1, batch    27 | loss: 7.0708532CurrentTrain: epoch  1, batch    28 | loss: 6.8601818CurrentTrain: epoch  1, batch    29 | loss: 7.0492015CurrentTrain: epoch  1, batch    30 | loss: 7.3624258CurrentTrain: epoch  1, batch    31 | loss: 7.4542532CurrentTrain: epoch  1, batch    32 | loss: 6.2958145CurrentTrain: epoch  1, batch    33 | loss: 6.8904948CurrentTrain: epoch  1, batch    34 | loss: 7.0656781CurrentTrain: epoch  1, batch    35 | loss: 7.8844690CurrentTrain: epoch  1, batch    36 | loss: 6.4552107CurrentTrain: epoch  1, batch    37 | loss: 7.0552192CurrentTrain: epoch  1, batch    38 | loss: 7.4491553CurrentTrain: epoch  1, batch    39 | loss: 6.7807417CurrentTrain: epoch  1, batch    40 | loss: 6.6982427CurrentTrain: epoch  1, batch    41 | loss: 6.5107403CurrentTrain: epoch  1, batch    42 | loss: 6.8977194CurrentTrain: epoch  1, batch    43 | loss: 6.8068624CurrentTrain: epoch  1, batch    44 | loss: 7.5962529CurrentTrain: epoch  1, batch    45 | loss: 7.0068684CurrentTrain: epoch  1, batch    46 | loss: 6.0658150CurrentTrain: epoch  1, batch    47 | loss: 5.9586954CurrentTrain: epoch  1, batch    48 | loss: 6.5234947CurrentTrain: epoch  1, batch    49 | loss: 6.9289856CurrentTrain: epoch  1, batch    50 | loss: 6.4239540CurrentTrain: epoch  1, batch    51 | loss: 6.4321213CurrentTrain: epoch  1, batch    52 | loss: 6.8064418CurrentTrain: epoch  1, batch    53 | loss: 7.1960468CurrentTrain: epoch  1, batch    54 | loss: 6.2763462CurrentTrain: epoch  1, batch    55 | loss: 6.8155584CurrentTrain: epoch  1, batch    56 | loss: 6.1869373CurrentTrain: epoch  1, batch    57 | loss: 6.8459101CurrentTrain: epoch  1, batch    58 | loss: 6.2807941CurrentTrain: epoch  1, batch    59 | loss: 7.0984192CurrentTrain: epoch  1, batch    60 | loss: 6.6055236CurrentTrain: epoch  1, batch    61 | loss: 5.4397373CurrentTrain: epoch  1, batch    62 | loss: 5.1134691CurrentTrain: epoch  2, batch     0 | loss: 6.6287928CurrentTrain: epoch  2, batch     1 | loss: 5.1035161CurrentTrain: epoch  2, batch     2 | loss: 5.9932604CurrentTrain: epoch  2, batch     3 | loss: 7.3226933CurrentTrain: epoch  2, batch     4 | loss: 6.6956525CurrentTrain: epoch  2, batch     5 | loss: 5.9080563CurrentTrain: epoch  2, batch     6 | loss: 6.0324783CurrentTrain: epoch  2, batch     7 | loss: 6.3720984CurrentTrain: epoch  2, batch     8 | loss: 5.7227039CurrentTrain: epoch  2, batch     9 | loss: 5.7503409CurrentTrain: epoch  2, batch    10 | loss: 5.8918438CurrentTrain: epoch  2, batch    11 | loss: 6.0373464CurrentTrain: epoch  2, batch    12 | loss: 5.9816947CurrentTrain: epoch  2, batch    13 | loss: 5.5005121CurrentTrain: epoch  2, batch    14 | loss: 6.1880379CurrentTrain: epoch  2, batch    15 | loss: 6.0433660CurrentTrain: epoch  2, batch    16 | loss: 5.5230861CurrentTrain: epoch  2, batch    17 | loss: 5.6102753CurrentTrain: epoch  2, batch    18 | loss: 7.0365171CurrentTrain: epoch  2, batch    19 | loss: 5.9359818CurrentTrain: epoch  2, batch    20 | loss: 6.0337553CurrentTrain: epoch  2, batch    21 | loss: 6.2495522CurrentTrain: epoch  2, batch    22 | loss: 6.1759357CurrentTrain: epoch  2, batch    23 | loss: 5.6207476CurrentTrain: epoch  2, batch    24 | loss: 5.8494282CurrentTrain: epoch  2, batch    25 | loss: 6.1729712CurrentTrain: epoch  2, batch    26 | loss: 5.9559126CurrentTrain: epoch  2, batch    27 | loss: 6.5260296CurrentTrain: epoch  2, batch    28 | loss: 6.1659842CurrentTrain: epoch  2, batch    29 | loss: 6.0940218CurrentTrain: epoch  2, batch    30 | loss: 5.9625854CurrentTrain: epoch  2, batch    31 | loss: 5.9537320CurrentTrain: epoch  2, batch    32 | loss: 5.7809978CurrentTrain: epoch  2, batch    33 | loss: 5.4454889CurrentTrain: epoch  2, batch    34 | loss: 5.5378551CurrentTrain: epoch  2, batch    35 | loss: 5.7525315CurrentTrain: epoch  2, batch    36 | loss: 6.3268232CurrentTrain: epoch  2, batch    37 | loss: 6.0724087CurrentTrain: epoch  2, batch    38 | loss: 5.7148414CurrentTrain: epoch  2, batch    39 | loss: 5.7856793CurrentTrain: epoch  2, batch    40 | loss: 5.4209099CurrentTrain: epoch  2, batch    41 | loss: 5.4390478CurrentTrain: epoch  2, batch    42 | loss: 5.5550890CurrentTrain: epoch  2, batch    43 | loss: 5.9459438CurrentTrain: epoch  2, batch    44 | loss: 6.6466322CurrentTrain: epoch  2, batch    45 | loss: 5.6763067CurrentTrain: epoch  2, batch    46 | loss: 5.2037420CurrentTrain: epoch  2, batch    47 | loss: 5.5665159CurrentTrain: epoch  2, batch    48 | loss: 6.1390314CurrentTrain: epoch  2, batch    49 | loss: 6.1607985CurrentTrain: epoch  2, batch    50 | loss: 5.5216017CurrentTrain: epoch  2, batch    51 | loss: 5.3782978CurrentTrain: epoch  2, batch    52 | loss: 5.6020679CurrentTrain: epoch  2, batch    53 | loss: 5.6078300CurrentTrain: epoch  2, batch    54 | loss: 6.2458177CurrentTrain: epoch  2, batch    55 | loss: 5.0822134CurrentTrain: epoch  2, batch    56 | loss: 5.6673641CurrentTrain: epoch  2, batch    57 | loss: 5.5232277CurrentTrain: epoch  2, batch    58 | loss: 5.0177593CurrentTrain: epoch  2, batch    59 | loss: 5.6791325CurrentTrain: epoch  2, batch    60 | loss: 5.2652750CurrentTrain: epoch  2, batch    61 | loss: 4.9217978CurrentTrain: epoch  2, batch    62 | loss: 4.7562971CurrentTrain: epoch  3, batch     0 | loss: 4.9559503CurrentTrain: epoch  3, batch     1 | loss: 5.3599176CurrentTrain: epoch  3, batch     2 | loss: 5.4492846CurrentTrain: epoch  3, batch     3 | loss: 5.2220488CurrentTrain: epoch  3, batch     4 | loss: 5.1972389CurrentTrain: epoch  3, batch     5 | loss: 5.2305508CurrentTrain: epoch  3, batch     6 | loss: 5.1213026CurrentTrain: epoch  3, batch     7 | loss: 4.8594131CurrentTrain: epoch  3, batch     8 | loss: 5.0868168CurrentTrain: epoch  3, batch     9 | loss: 5.4064302CurrentTrain: epoch  3, batch    10 | loss: 5.3327475CurrentTrain: epoch  3, batch    11 | loss: 5.6959295CurrentTrain: epoch  3, batch    12 | loss: 5.2919760CurrentTrain: epoch  3, batch    13 | loss: 4.9032564CurrentTrain: epoch  3, batch    14 | loss: 5.0401831CurrentTrain: epoch  3, batch    15 | loss: 5.4902782CurrentTrain: epoch  3, batch    16 | loss: 4.7586775CurrentTrain: epoch  3, batch    17 | loss: 5.8124328CurrentTrain: epoch  3, batch    18 | loss: 5.2609272CurrentTrain: epoch  3, batch    19 | loss: 6.1624336CurrentTrain: epoch  3, batch    20 | loss: 5.3622174CurrentTrain: epoch  3, batch    21 | loss: 5.5165768CurrentTrain: epoch  3, batch    22 | loss: 5.3602695CurrentTrain: epoch  3, batch    23 | loss: 4.9084949CurrentTrain: epoch  3, batch    24 | loss: 4.6899810CurrentTrain: epoch  3, batch    25 | loss: 5.1456690CurrentTrain: epoch  3, batch    26 | loss: 5.1932864CurrentTrain: epoch  3, batch    27 | loss: 4.7177806CurrentTrain: epoch  3, batch    28 | loss: 6.1713924CurrentTrain: epoch  3, batch    29 | loss: 5.2458081CurrentTrain: epoch  3, batch    30 | loss: 4.9140725CurrentTrain: epoch  3, batch    31 | loss: 5.4297934CurrentTrain: epoch  3, batch    32 | loss: 5.7149172CurrentTrain: epoch  3, batch    33 | loss: 4.9920878CurrentTrain: epoch  3, batch    34 | loss: 5.1700444CurrentTrain: epoch  3, batch    35 | loss: 4.7932262CurrentTrain: epoch  3, batch    36 | loss: 4.6696205CurrentTrain: epoch  3, batch    37 | loss: 5.1058683CurrentTrain: epoch  3, batch    38 | loss: 4.7787733CurrentTrain: epoch  3, batch    39 | loss: 4.8343921CurrentTrain: epoch  3, batch    40 | loss: 4.5151272CurrentTrain: epoch  3, batch    41 | loss: 4.9029260CurrentTrain: epoch  3, batch    42 | loss: 5.1019230CurrentTrain: epoch  3, batch    43 | loss: 4.7009802CurrentTrain: epoch  3, batch    44 | loss: 5.2793999CurrentTrain: epoch  3, batch    45 | loss: 4.7721677CurrentTrain: epoch  3, batch    46 | loss: 5.7082119CurrentTrain: epoch  3, batch    47 | loss: 4.8959608CurrentTrain: epoch  3, batch    48 | loss: 4.8165760CurrentTrain: epoch  3, batch    49 | loss: 4.9383698CurrentTrain: epoch  3, batch    50 | loss: 4.9791088CurrentTrain: epoch  3, batch    51 | loss: 4.9800439CurrentTrain: epoch  3, batch    52 | loss: 4.9952784CurrentTrain: epoch  3, batch    53 | loss: 5.0657778CurrentTrain: epoch  3, batch    54 | loss: 4.7023325CurrentTrain: epoch  3, batch    55 | loss: 5.0856791CurrentTrain: epoch  3, batch    56 | loss: 5.2625990CurrentTrain: epoch  3, batch    57 | loss: 4.9002271CurrentTrain: epoch  3, batch    58 | loss: 4.9464731CurrentTrain: epoch  3, batch    59 | loss: 4.3692284CurrentTrain: epoch  3, batch    60 | loss: 4.6403189CurrentTrain: epoch  3, batch    61 | loss: 4.6860266CurrentTrain: epoch  3, batch    62 | loss: 5.4158154CurrentTrain: epoch  4, batch     0 | loss: 4.7789822CurrentTrain: epoch  4, batch     1 | loss: 4.5644565CurrentTrain: epoch  4, batch     2 | loss: 4.7922945CurrentTrain: epoch  4, batch     3 | loss: 4.5662317CurrentTrain: epoch  4, batch     4 | loss: 4.7927341CurrentTrain: epoch  4, batch     5 | loss: 4.7869301CurrentTrain: epoch  4, batch     6 | loss: 4.6627588CurrentTrain: epoch  4, batch     7 | loss: 4.8947401CurrentTrain: epoch  4, batch     8 | loss: 4.7288041CurrentTrain: epoch  4, batch     9 | loss: 4.9738121CurrentTrain: epoch  4, batch    10 | loss: 4.8039675CurrentTrain: epoch  4, batch    11 | loss: 4.7061791CurrentTrain: epoch  4, batch    12 | loss: 4.5820608CurrentTrain: epoch  4, batch    13 | loss: 4.7715254CurrentTrain: epoch  4, batch    14 | loss: 4.5080452CurrentTrain: epoch  4, batch    15 | loss: 4.7742100CurrentTrain: epoch  4, batch    16 | loss: 4.6374722CurrentTrain: epoch  4, batch    17 | loss: 4.4478269CurrentTrain: epoch  4, batch    18 | loss: 4.5523367CurrentTrain: epoch  4, batch    19 | loss: 4.4983926CurrentTrain: epoch  4, batch    20 | loss: 4.5318160CurrentTrain: epoch  4, batch    21 | loss: 4.5876098CurrentTrain: epoch  4, batch    22 | loss: 4.7768202CurrentTrain: epoch  4, batch    23 | loss: 4.6748343CurrentTrain: epoch  4, batch    24 | loss: 5.4958267CurrentTrain: epoch  4, batch    25 | loss: 4.5620637CurrentTrain: epoch  4, batch    26 | loss: 4.4878020CurrentTrain: epoch  4, batch    27 | loss: 4.7440400CurrentTrain: epoch  4, batch    28 | loss: 4.5478315CurrentTrain: epoch  4, batch    29 | loss: 4.3891678CurrentTrain: epoch  4, batch    30 | loss: 5.1537085CurrentTrain: epoch  4, batch    31 | loss: 4.3668299CurrentTrain: epoch  4, batch    32 | loss: 4.5458450CurrentTrain: epoch  4, batch    33 | loss: 4.6473064CurrentTrain: epoch  4, batch    34 | loss: 4.4753389CurrentTrain: epoch  4, batch    35 | loss: 5.3909826CurrentTrain: epoch  4, batch    36 | loss: 4.4831686CurrentTrain: epoch  4, batch    37 | loss: 4.5535631CurrentTrain: epoch  4, batch    38 | loss: 4.5240545CurrentTrain: epoch  4, batch    39 | loss: 4.6040974CurrentTrain: epoch  4, batch    40 | loss: 5.4083357CurrentTrain: epoch  4, batch    41 | loss: 4.3056211CurrentTrain: epoch  4, batch    42 | loss: 4.4637475CurrentTrain: epoch  4, batch    43 | loss: 5.0431819CurrentTrain: epoch  4, batch    44 | loss: 4.4682393CurrentTrain: epoch  4, batch    45 | loss: 4.3633137CurrentTrain: epoch  4, batch    46 | loss: 4.5992146CurrentTrain: epoch  4, batch    47 | loss: 4.7499666CurrentTrain: epoch  4, batch    48 | loss: 4.3858690CurrentTrain: epoch  4, batch    49 | loss: 4.6934900CurrentTrain: epoch  4, batch    50 | loss: 4.4996810CurrentTrain: epoch  4, batch    51 | loss: 4.6601553CurrentTrain: epoch  4, batch    52 | loss: 4.5638704CurrentTrain: epoch  4, batch    53 | loss: 4.5786867CurrentTrain: epoch  4, batch    54 | loss: 4.4388614CurrentTrain: epoch  4, batch    55 | loss: 4.9535861CurrentTrain: epoch  4, batch    56 | loss: 4.5433388CurrentTrain: epoch  4, batch    57 | loss: 4.5788102CurrentTrain: epoch  4, batch    58 | loss: 4.5018616CurrentTrain: epoch  4, batch    59 | loss: 4.9385352CurrentTrain: epoch  4, batch    60 | loss: 4.3446951CurrentTrain: epoch  4, batch    61 | loss: 4.6549535CurrentTrain: epoch  4, batch    62 | loss: 4.3735919CurrentTrain: epoch  5, batch     0 | loss: 4.5700197CurrentTrain: epoch  5, batch     1 | loss: 4.3170042CurrentTrain: epoch  5, batch     2 | loss: 4.3828192CurrentTrain: epoch  5, batch     3 | loss: 4.4654274CurrentTrain: epoch  5, batch     4 | loss: 4.3699145CurrentTrain: epoch  5, batch     5 | loss: 4.5076532CurrentTrain: epoch  5, batch     6 | loss: 4.5996194CurrentTrain: epoch  5, batch     7 | loss: 4.3835783CurrentTrain: epoch  5, batch     8 | loss: 4.3491344CurrentTrain: epoch  5, batch     9 | loss: 4.4170713CurrentTrain: epoch  5, batch    10 | loss: 4.4065351CurrentTrain: epoch  5, batch    11 | loss: 4.4104013CurrentTrain: epoch  5, batch    12 | loss: 4.4397726CurrentTrain: epoch  5, batch    13 | loss: 4.5682378CurrentTrain: epoch  5, batch    14 | loss: 4.3139696CurrentTrain: epoch  5, batch    15 | loss: 4.5497055CurrentTrain: epoch  5, batch    16 | loss: 4.3523560CurrentTrain: epoch  5, batch    17 | loss: 4.4614534CurrentTrain: epoch  5, batch    18 | loss: 4.3755074CurrentTrain: epoch  5, batch    19 | loss: 4.3704801CurrentTrain: epoch  5, batch    20 | loss: 4.3345833CurrentTrain: epoch  5, batch    21 | loss: 4.4443316CurrentTrain: epoch  5, batch    22 | loss: 4.3310366CurrentTrain: epoch  5, batch    23 | loss: 4.3423061CurrentTrain: epoch  5, batch    24 | loss: 4.2741604CurrentTrain: epoch  5, batch    25 | loss: 4.2450008CurrentTrain: epoch  5, batch    26 | loss: 4.3501625CurrentTrain: epoch  5, batch    27 | loss: 4.3844314CurrentTrain: epoch  5, batch    28 | loss: 4.4630451CurrentTrain: epoch  5, batch    29 | loss: 4.3562431CurrentTrain: epoch  5, batch    30 | loss: 4.3102927CurrentTrain: epoch  5, batch    31 | loss: 4.3086309CurrentTrain: epoch  5, batch    32 | loss: 4.3092146CurrentTrain: epoch  5, batch    33 | loss: 4.3298826CurrentTrain: epoch  5, batch    34 | loss: 4.4573951CurrentTrain: epoch  5, batch    35 | loss: 4.5009260CurrentTrain: epoch  5, batch    36 | loss: 4.2569170CurrentTrain: epoch  5, batch    37 | loss: 4.3217702CurrentTrain: epoch  5, batch    38 | loss: 4.2533889CurrentTrain: epoch  5, batch    39 | loss: 4.2617831CurrentTrain: epoch  5, batch    40 | loss: 4.4131241CurrentTrain: epoch  5, batch    41 | loss: 4.6655769CurrentTrain: epoch  5, batch    42 | loss: 4.3106651CurrentTrain: epoch  5, batch    43 | loss: 4.6107025CurrentTrain: epoch  5, batch    44 | loss: 4.6016760CurrentTrain: epoch  5, batch    45 | loss: 4.3948612CurrentTrain: epoch  5, batch    46 | loss: 4.2523308CurrentTrain: epoch  5, batch    47 | loss: 4.3147106CurrentTrain: epoch  5, batch    48 | loss: 4.2220211CurrentTrain: epoch  5, batch    49 | loss: 5.2232513CurrentTrain: epoch  5, batch    50 | loss: 4.4027901CurrentTrain: epoch  5, batch    51 | loss: 4.5129819CurrentTrain: epoch  5, batch    52 | loss: 4.3551188CurrentTrain: epoch  5, batch    53 | loss: 4.5922856CurrentTrain: epoch  5, batch    54 | loss: 4.3549786CurrentTrain: epoch  5, batch    55 | loss: 4.4475021CurrentTrain: epoch  5, batch    56 | loss: 4.3401222CurrentTrain: epoch  5, batch    57 | loss: 4.3261194CurrentTrain: epoch  5, batch    58 | loss: 4.5429430CurrentTrain: epoch  5, batch    59 | loss: 4.5346861CurrentTrain: epoch  5, batch    60 | loss: 4.4519653CurrentTrain: epoch  5, batch    61 | loss: 4.1750894CurrentTrain: epoch  5, batch    62 | loss: 4.3634920CurrentTrain: epoch  6, batch     0 | loss: 4.2963495CurrentTrain: epoch  6, batch     1 | loss: 4.7609825CurrentTrain: epoch  6, batch     2 | loss: 4.2707157CurrentTrain: epoch  6, batch     3 | loss: 4.2842207CurrentTrain: epoch  6, batch     4 | loss: 4.4981341CurrentTrain: epoch  6, batch     5 | loss: 4.2145281CurrentTrain: epoch  6, batch     6 | loss: 4.2792206CurrentTrain: epoch  6, batch     7 | loss: 4.4667554CurrentTrain: epoch  6, batch     8 | loss: 4.2446489CurrentTrain: epoch  6, batch     9 | loss: 4.3127079CurrentTrain: epoch  6, batch    10 | loss: 4.3153353CurrentTrain: epoch  6, batch    11 | loss: 4.3154244CurrentTrain: epoch  6, batch    12 | loss: 4.4480486CurrentTrain: epoch  6, batch    13 | loss: 4.2017117CurrentTrain: epoch  6, batch    14 | loss: 4.3441668CurrentTrain: epoch  6, batch    15 | loss: 4.1756206CurrentTrain: epoch  6, batch    16 | loss: 4.3158541CurrentTrain: epoch  6, batch    17 | loss: 4.2151847CurrentTrain: epoch  6, batch    18 | loss: 4.1577506CurrentTrain: epoch  6, batch    19 | loss: 4.3055515CurrentTrain: epoch  6, batch    20 | loss: 4.1694317CurrentTrain: epoch  6, batch    21 | loss: 4.2989368CurrentTrain: epoch  6, batch    22 | loss: 4.1875486CurrentTrain: epoch  6, batch    23 | loss: 4.2411370CurrentTrain: epoch  6, batch    24 | loss: 4.2934990CurrentTrain: epoch  6, batch    25 | loss: 4.2988625CurrentTrain: epoch  6, batch    26 | loss: 4.2368240CurrentTrain: epoch  6, batch    27 | loss: 4.2733655CurrentTrain: epoch  6, batch    28 | loss: 4.2174940CurrentTrain: epoch  6, batch    29 | loss: 4.2537398CurrentTrain: epoch  6, batch    30 | loss: 4.2549887CurrentTrain: epoch  6, batch    31 | loss: 4.3372474CurrentTrain: epoch  6, batch    32 | loss: 4.3627553CurrentTrain: epoch  6, batch    33 | loss: 4.1762357CurrentTrain: epoch  6, batch    34 | loss: 4.2253361CurrentTrain: epoch  6, batch    35 | loss: 4.1891856CurrentTrain: epoch  6, batch    36 | loss: 4.2155056CurrentTrain: epoch  6, batch    37 | loss: 4.1944118CurrentTrain: epoch  6, batch    38 | loss: 4.1185222CurrentTrain: epoch  6, batch    39 | loss: 4.2631836CurrentTrain: epoch  6, batch    40 | loss: 4.2158060CurrentTrain: epoch  6, batch    41 | loss: 4.1979213CurrentTrain: epoch  6, batch    42 | loss: 4.2107663CurrentTrain: epoch  6, batch    43 | loss: 4.1817737CurrentTrain: epoch  6, batch    44 | loss: 4.2329426CurrentTrain: epoch  6, batch    45 | loss: 4.2719040CurrentTrain: epoch  6, batch    46 | loss: 4.1773024CurrentTrain: epoch  6, batch    47 | loss: 4.1546474CurrentTrain: epoch  6, batch    48 | loss: 4.1758466CurrentTrain: epoch  6, batch    49 | loss: 4.2371674CurrentTrain: epoch  6, batch    50 | loss: 4.1608939CurrentTrain: epoch  6, batch    51 | loss: 4.1588178CurrentTrain: epoch  6, batch    52 | loss: 4.1904993CurrentTrain: epoch  6, batch    53 | loss: 4.1684041CurrentTrain: epoch  6, batch    54 | loss: 4.1068316CurrentTrain: epoch  6, batch    55 | loss: 4.1553726CurrentTrain: epoch  6, batch    56 | loss: 4.3479109CurrentTrain: epoch  6, batch    57 | loss: 4.1698589CurrentTrain: epoch  6, batch    58 | loss: 4.1409726CurrentTrain: epoch  6, batch    59 | loss: 4.1781926CurrentTrain: epoch  6, batch    60 | loss: 4.1144800CurrentTrain: epoch  6, batch    61 | loss: 4.1344490CurrentTrain: epoch  6, batch    62 | loss: 4.1572695CurrentTrain: epoch  7, batch     0 | loss: 4.1518402CurrentTrain: epoch  7, batch     1 | loss: 4.1346889CurrentTrain: epoch  7, batch     2 | loss: 4.2143755CurrentTrain: epoch  7, batch     3 | loss: 4.2565560CurrentTrain: epoch  7, batch     4 | loss: 4.1361575CurrentTrain: epoch  7, batch     5 | loss: 4.1378574CurrentTrain: epoch  7, batch     6 | loss: 4.1272521CurrentTrain: epoch  7, batch     7 | loss: 4.1506472CurrentTrain: epoch  7, batch     8 | loss: 4.1639481CurrentTrain: epoch  7, batch     9 | loss: 4.1042252CurrentTrain: epoch  7, batch    10 | loss: 4.1683192CurrentTrain: epoch  7, batch    11 | loss: 4.2416806CurrentTrain: epoch  7, batch    12 | loss: 4.1504674CurrentTrain: epoch  7, batch    13 | loss: 4.1433473CurrentTrain: epoch  7, batch    14 | loss: 4.1628175CurrentTrain: epoch  7, batch    15 | loss: 4.1597385CurrentTrain: epoch  7, batch    16 | loss: 4.0950089CurrentTrain: epoch  7, batch    17 | loss: 4.1180215CurrentTrain: epoch  7, batch    18 | loss: 4.1492443CurrentTrain: epoch  7, batch    19 | loss: 4.1653686CurrentTrain: epoch  7, batch    20 | loss: 4.1615391CurrentTrain: epoch  7, batch    21 | loss: 4.1179972CurrentTrain: epoch  7, batch    22 | loss: 4.1629667CurrentTrain: epoch  7, batch    23 | loss: 4.1897769CurrentTrain: epoch  7, batch    24 | loss: 4.0768051CurrentTrain: epoch  7, batch    25 | loss: 4.1906166CurrentTrain: epoch  7, batch    26 | loss: 4.1560073CurrentTrain: epoch  7, batch    27 | loss: 4.0987163CurrentTrain: epoch  7, batch    28 | loss: 4.1243191CurrentTrain: epoch  7, batch    29 | loss: 4.1219835CurrentTrain: epoch  7, batch    30 | loss: 4.1513543CurrentTrain: epoch  7, batch    31 | loss: 4.0912542CurrentTrain: epoch  7, batch    32 | loss: 4.0991230CurrentTrain: epoch  7, batch    33 | loss: 4.1334829CurrentTrain: epoch  7, batch    34 | loss: 4.1638746CurrentTrain: epoch  7, batch    35 | loss: 4.1423850CurrentTrain: epoch  7, batch    36 | loss: 4.1659226CurrentTrain: epoch  7, batch    37 | loss: 4.1689997CurrentTrain: epoch  7, batch    38 | loss: 4.1375542CurrentTrain: epoch  7, batch    39 | loss: 4.1067266CurrentTrain: epoch  7, batch    40 | loss: 4.0489745CurrentTrain: epoch  7, batch    41 | loss: 4.1229477CurrentTrain: epoch  7, batch    42 | loss: 4.1094170CurrentTrain: epoch  7, batch    43 | loss: 4.1292000CurrentTrain: epoch  7, batch    44 | loss: 4.1309009CurrentTrain: epoch  7, batch    45 | loss: 4.1784892CurrentTrain: epoch  7, batch    46 | loss: 4.2161212CurrentTrain: epoch  7, batch    47 | loss: 4.0884805CurrentTrain: epoch  7, batch    48 | loss: 4.1212149CurrentTrain: epoch  7, batch    49 | loss: 4.3764687CurrentTrain: epoch  7, batch    50 | loss: 4.1176906CurrentTrain: epoch  7, batch    51 | loss: 4.1474185CurrentTrain: epoch  7, batch    52 | loss: 4.1021829CurrentTrain: epoch  7, batch    53 | loss: 4.1255121CurrentTrain: epoch  7, batch    54 | loss: 4.1091528CurrentTrain: epoch  7, batch    55 | loss: 4.1353688CurrentTrain: epoch  7, batch    56 | loss: 4.0862060CurrentTrain: epoch  7, batch    57 | loss: 4.1389127CurrentTrain: epoch  7, batch    58 | loss: 4.0826421CurrentTrain: epoch  7, batch    59 | loss: 4.0488992CurrentTrain: epoch  7, batch    60 | loss: 4.1058369CurrentTrain: epoch  7, batch    61 | loss: 4.1328487CurrentTrain: epoch  7, batch    62 | loss: 4.1462550CurrentTrain: epoch  8, batch     0 | loss: 4.1170592CurrentTrain: epoch  8, batch     1 | loss: 4.1219454CurrentTrain: epoch  8, batch     2 | loss: 4.1133604CurrentTrain: epoch  8, batch     3 | loss: 4.1039467CurrentTrain: epoch  8, batch     4 | loss: 4.1165805CurrentTrain: epoch  8, batch     5 | loss: 4.1161656CurrentTrain: epoch  8, batch     6 | loss: 4.0897017CurrentTrain: epoch  8, batch     7 | loss: 4.0924377CurrentTrain: epoch  8, batch     8 | loss: 4.0994425CurrentTrain: epoch  8, batch     9 | loss: 4.0802631CurrentTrain: epoch  8, batch    10 | loss: 4.1304002CurrentTrain: epoch  8, batch    11 | loss: 4.1440930CurrentTrain: epoch  8, batch    12 | loss: 4.0925980CurrentTrain: epoch  8, batch    13 | loss: 4.1026077CurrentTrain: epoch  8, batch    14 | loss: 4.1175032CurrentTrain: epoch  8, batch    15 | loss: 4.0526991CurrentTrain: epoch  8, batch    16 | loss: 4.1504312CurrentTrain: epoch  8, batch    17 | loss: 4.0830088CurrentTrain: epoch  8, batch    18 | loss: 4.0597262CurrentTrain: epoch  8, batch    19 | loss: 4.0707216CurrentTrain: epoch  8, batch    20 | loss: 4.0748901CurrentTrain: epoch  8, batch    21 | loss: 4.0919380CurrentTrain: epoch  8, batch    22 | loss: 4.1050892CurrentTrain: epoch  8, batch    23 | loss: 4.0705080CurrentTrain: epoch  8, batch    24 | loss: 4.1378794CurrentTrain: epoch  8, batch    25 | loss: 4.0783544CurrentTrain: epoch  8, batch    26 | loss: 4.0644517CurrentTrain: epoch  8, batch    27 | loss: 4.1079750CurrentTrain: epoch  8, batch    28 | loss: 4.0924873CurrentTrain: epoch  8, batch    29 | loss: 4.0743232CurrentTrain: epoch  8, batch    30 | loss: 4.0647502CurrentTrain: epoch  8, batch    31 | loss: 4.0496244CurrentTrain: epoch  8, batch    32 | loss: 4.0861073CurrentTrain: epoch  8, batch    33 | loss: 4.0805988CurrentTrain: epoch  8, batch    34 | loss: 4.0974984CurrentTrain: epoch  8, batch    35 | loss: 4.0713406CurrentTrain: epoch  8, batch    36 | loss: 4.0820394CurrentTrain: epoch  8, batch    37 | loss: 4.0703039CurrentTrain: epoch  8, batch    38 | loss: 4.0536284CurrentTrain: epoch  8, batch    39 | loss: 4.0852070CurrentTrain: epoch  8, batch    40 | loss: 4.0639081CurrentTrain: epoch  8, batch    41 | loss: 4.4067688CurrentTrain: epoch  8, batch    42 | loss: 4.0455756CurrentTrain: epoch  8, batch    43 | loss: 4.0706520CurrentTrain: epoch  8, batch    44 | loss: 4.0873914CurrentTrain: epoch  8, batch    45 | loss: 4.1658125CurrentTrain: epoch  8, batch    46 | loss: 4.1034098CurrentTrain: epoch  8, batch    47 | loss: 4.1061697CurrentTrain: epoch  8, batch    48 | loss: 4.0881147CurrentTrain: epoch  8, batch    49 | loss: 4.0711222CurrentTrain: epoch  8, batch    50 | loss: 4.0482736CurrentTrain: epoch  8, batch    51 | loss: 4.0732179CurrentTrain: epoch  8, batch    52 | loss: 4.0536041CurrentTrain: epoch  8, batch    53 | loss: 4.0700755CurrentTrain: epoch  8, batch    54 | loss: 4.0719433CurrentTrain: epoch  8, batch    55 | loss: 4.0612750CurrentTrain: epoch  8, batch    56 | loss: 4.0232401CurrentTrain: epoch  8, batch    57 | loss: 4.0494976CurrentTrain: epoch  8, batch    58 | loss: 4.0676832CurrentTrain: epoch  8, batch    59 | loss: 4.1198330CurrentTrain: epoch  8, batch    60 | loss: 4.0596371CurrentTrain: epoch  8, batch    61 | loss: 4.0560021CurrentTrain: epoch  8, batch    62 | loss: 4.0910730CurrentTrain: epoch  9, batch     0 | loss: 4.1280947CurrentTrain: epoch  9, batch     1 | loss: 4.0780363CurrentTrain: epoch  9, batch     2 | loss: 4.0328398CurrentTrain: epoch  9, batch     3 | loss: 4.0170112CurrentTrain: epoch  9, batch     4 | loss: 4.0578289CurrentTrain: epoch  9, batch     5 | loss: 4.0599127CurrentTrain: epoch  9, batch     6 | loss: 4.0447617CurrentTrain: epoch  9, batch     7 | loss: 4.1088829CurrentTrain: epoch  9, batch     8 | loss: 4.0467539CurrentTrain: epoch  9, batch     9 | loss: 4.0754585CurrentTrain: epoch  9, batch    10 | loss: 4.0628672CurrentTrain: epoch  9, batch    11 | loss: 4.0351973CurrentTrain: epoch  9, batch    12 | loss: 4.0790787CurrentTrain: epoch  9, batch    13 | loss: 4.1029682CurrentTrain: epoch  9, batch    14 | loss: 4.0730219CurrentTrain: epoch  9, batch    15 | loss: 4.0310135CurrentTrain: epoch  9, batch    16 | loss: 4.0650387CurrentTrain: epoch  9, batch    17 | loss: 4.3548994CurrentTrain: epoch  9, batch    18 | loss: 4.0946584CurrentTrain: epoch  9, batch    19 | loss: 4.0468769CurrentTrain: epoch  9, batch    20 | loss: 4.1648083CurrentTrain: epoch  9, batch    21 | loss: 4.0685883CurrentTrain: epoch  9, batch    22 | loss: 4.0317202CurrentTrain: epoch  9, batch    23 | loss: 4.0599823CurrentTrain: epoch  9, batch    24 | loss: 4.0695624CurrentTrain: epoch  9, batch    25 | loss: 4.0857372CurrentTrain: epoch  9, batch    26 | loss: 4.0360875CurrentTrain: epoch  9, batch    27 | loss: 4.0314069CurrentTrain: epoch  9, batch    28 | loss: 4.0782228CurrentTrain: epoch  9, batch    29 | loss: 4.0465031CurrentTrain: epoch  9, batch    30 | loss: 4.0623903CurrentTrain: epoch  9, batch    31 | loss: 4.0959110CurrentTrain: epoch  9, batch    32 | loss: 4.0720968CurrentTrain: epoch  9, batch    33 | loss: 4.0551329CurrentTrain: epoch  9, batch    34 | loss: 4.1079183CurrentTrain: epoch  9, batch    35 | loss: 4.0253458CurrentTrain: epoch  9, batch    36 | loss: 4.0659108CurrentTrain: epoch  9, batch    37 | loss: 4.1231499CurrentTrain: epoch  9, batch    38 | loss: 3.9706440CurrentTrain: epoch  9, batch    39 | loss: 4.0081553CurrentTrain: epoch  9, batch    40 | loss: 4.0945053CurrentTrain: epoch  9, batch    41 | loss: 4.0690365CurrentTrain: epoch  9, batch    42 | loss: 4.1305213CurrentTrain: epoch  9, batch    43 | loss: 4.0544872CurrentTrain: epoch  9, batch    44 | loss: 4.0510955CurrentTrain: epoch  9, batch    45 | loss: 4.0752978CurrentTrain: epoch  9, batch    46 | loss: 4.0643692CurrentTrain: epoch  9, batch    47 | loss: 4.0746832CurrentTrain: epoch  9, batch    48 | loss: 4.0554657CurrentTrain: epoch  9, batch    49 | loss: 4.0631976CurrentTrain: epoch  9, batch    50 | loss: 4.0628271CurrentTrain: epoch  9, batch    51 | loss: 4.0392108CurrentTrain: epoch  9, batch    52 | loss: 4.0771990CurrentTrain: epoch  9, batch    53 | loss: 4.0560203CurrentTrain: epoch  9, batch    54 | loss: 4.0234680CurrentTrain: epoch  9, batch    55 | loss: 4.0500603CurrentTrain: epoch  9, batch    56 | loss: 4.0492225CurrentTrain: epoch  9, batch    57 | loss: 4.0403166CurrentTrain: epoch  9, batch    58 | loss: 4.0370760CurrentTrain: epoch  9, batch    59 | loss: 4.0347786CurrentTrain: epoch  9, batch    60 | loss: 4.0762630CurrentTrain: epoch  9, batch    61 | loss: 4.0505691CurrentTrain: epoch  9, batch    62 | loss: 4.0673161
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: in 1874 , janáček became friends with antonín dvořák , and began composing in a relatively traditional romantic style .
Head Entity: antonín dvořák
Tail Entity: romantic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The Impressionist movement, which began in the late 19th century, was significantly influenced by the works of Claude Monet and his contemporaries.  
Head Entity: Claude Monet  
Tail Entity: Impressionist  

Relation: movement  
Context: In the early 20th century, the Dada movement emerged as a reaction to the horrors of World War I, with artists like Marcel Duchamp leading the charge.  
Head Entity: Marcel Duchamp  
Tail Entity: Dada  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: guo was born in xuanhua county , zhangjiakou , hebei in july 1971 .
Head Entity: zhangjiakou
Tail Entity: hebei
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of san francisco is situated in the state of california, known for its iconic golden gate bridge.  
Head Entity: san francisco  
Tail Entity: california  

Relation: located in the administrative territorial entity  
Context: the university of toronto is located in the city of toronto, which is the capital of ontario, canada.  
Head Entity: toronto  
Tail Entity: ontario  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: he also went on a road trip from san antonio to oakland for the magnum project “ postcards from america ” with jim goldberg , alec soth , mikhael subotzky and susan meiselas .
Head Entity: jim goldberg
Tail Entity: magnum
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of dedication and hard work, she was finally recognized as a key member of the prestigious art collective known as the "Creative Minds."  
Head Entity: she  
Tail Entity: Creative Minds  

Relation: member of  
Context: The renowned guitarist joined the band in 2015, contributing to their unique sound and becoming an integral member of the musical group "The Soundwaves."  
Head Entity: guitarist  
Tail Entity: The Soundwaves  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the tobin bridge , a major regional transportation artery , carries route 1 from chelsea across the mystic river to charlestown .
Head Entity: tobin bridge
Tail Entity: mystic river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge spans the golden gate strait, connecting san francisco to marin county and allowing vehicles to cross over the water.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the subway tunnel runs beneath the city, allowing trains to cross under the busy streets and connect various neighborhoods.  
Head Entity: subway tunnel  
Tail Entity: city streets  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the cucuveanu river is a tributary of the argova river in romania .
Head Entity: argova river
Tail Entity: cucuveanu river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: the missouri river is a major tributary of the mississippi river in the united states.  
Head Entity: mississippi river  
Tail Entity: missouri river  

Relation: tributary  
Context: the thames river serves as a tributary to the river medway in southeastern england.  
Head Entity: river medway  
Tail Entity: thames river  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: sir arthur stanley , the governor of victoria , and sir edmund barton , a justice of the high court and a former prime minister of australia , were also in attendance .
Head Entity: sir edmund barton
Tail Entity: prime minister of australia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: after serving as the mayor of new york city, bill de blasio was elected as the 109th mayor of the city, succeeding michael bloomberg.  
Head Entity: bill de blasio  
Tail Entity: mayor of new york city  

Relation: position held  
Context: during her tenure, angela merkel served as the chancellor of germany, leading the country through numerous crises and becoming one of the world's most powerful women.  
Head Entity: angela merkel  
Tail Entity: chancellor of germany  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: " jedi outcast " was developed by raven software and powered by the i d tech 3 game engine .
Head Entity: jedi outcast
Tail Entity: raven software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and has received numerous awards for its storytelling and gameplay.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Minecraft" was developed by Mojang Studios and has become one of the best-selling video games of all time.  
Head Entity: Minecraft  
Tail Entity: Mojang Studios  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: neptune is the second studio album by the london - based band the duke spirit and the last with the original lead guitarist , dan higgins .
Head Entity: the duke spirit
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in silicon valley by a group of innovative engineers looking to revolutionize the industry.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was established in new york city, where they quickly gained a following and changed the music scene.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: during the same month , the band supported kings of leon 's tour of the united states , and coldplay 's tour of the united kingdom .
Head Entity: kings of leon
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous Italian dish, pizza, has gained popularity worldwide, but its roots can be traced back to Naples, Italy.  
Head Entity: pizza  
Tail Entity: Italy  

Relation: country of origin  
Context: The iconic brand, Rolex, is renowned for its luxury watches, which are crafted in Switzerland, known for its precision and quality in watchmaking.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.74%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 95.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.49%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.46%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.74%   
cur_acc:  ['0.9474']
his_acc:  ['0.9474']
CurrentTrain: epoch  0, batch     0 | loss: 6.9312401CurrentTrain: epoch  0, batch     1 | loss: 6.3583331CurrentTrain: epoch  0, batch     2 | loss: 5.7382846CurrentTrain: epoch  0, batch     3 | loss: 5.4255152CurrentTrain: epoch  1, batch     0 | loss: 6.5463080CurrentTrain: epoch  1, batch     1 | loss: 5.5049410CurrentTrain: epoch  1, batch     2 | loss: 5.0311370CurrentTrain: epoch  1, batch     3 | loss: 2.5477853CurrentTrain: epoch  2, batch     0 | loss: 4.9564080CurrentTrain: epoch  2, batch     1 | loss: 5.3451695CurrentTrain: epoch  2, batch     2 | loss: 3.8275187CurrentTrain: epoch  2, batch     3 | loss: 4.7866859CurrentTrain: epoch  3, batch     0 | loss: 4.5532179CurrentTrain: epoch  3, batch     1 | loss: 4.1583767CurrentTrain: epoch  3, batch     2 | loss: 4.0656939CurrentTrain: epoch  3, batch     3 | loss: 3.3862386CurrentTrain: epoch  4, batch     0 | loss: 3.5757720CurrentTrain: epoch  4, batch     1 | loss: 3.7165279CurrentTrain: epoch  4, batch     2 | loss: 4.1740627CurrentTrain: epoch  4, batch     3 | loss: 3.3496485CurrentTrain: epoch  5, batch     0 | loss: 3.8002968CurrentTrain: epoch  5, batch     1 | loss: 3.1489561CurrentTrain: epoch  5, batch     2 | loss: 3.4332728CurrentTrain: epoch  5, batch     3 | loss: 4.9560938CurrentTrain: epoch  6, batch     0 | loss: 3.9993639CurrentTrain: epoch  6, batch     1 | loss: 3.2874713CurrentTrain: epoch  6, batch     2 | loss: 3.0425377CurrentTrain: epoch  6, batch     3 | loss: 3.4019604CurrentTrain: epoch  7, batch     0 | loss: 2.9125724CurrentTrain: epoch  7, batch     1 | loss: 3.4735479CurrentTrain: epoch  7, batch     2 | loss: 3.2434099CurrentTrain: epoch  7, batch     3 | loss: 3.1444087CurrentTrain: epoch  8, batch     0 | loss: 3.0627279CurrentTrain: epoch  8, batch     1 | loss: 3.2768397CurrentTrain: epoch  8, batch     2 | loss: 2.6400051CurrentTrain: epoch  8, batch     3 | loss: 1.8748622CurrentTrain: epoch  9, batch     0 | loss: 2.7670147CurrentTrain: epoch  9, batch     1 | loss: 2.6882076CurrentTrain: epoch  9, batch     2 | loss: 2.5459964CurrentTrain: epoch  9, batch     3 | loss: 3.1930909
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and producer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama since 1992, and they have two daughters together.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: they briefly reformed in 1992 , when sub pop re - released " primal rock therapy " on cd with five additional unreleased tracks .
Head Entity: primal rock therapy
Tail Entity: sub pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Future Nostalgia" was released under the Warner Records label, showcasing Dua Lipa's unique sound and style.  
Head Entity: Future Nostalgia  
Tail Entity: Warner Records  

Relation: record label  
Context: After signing with Columbia Records, the band released their highly anticipated debut album, which quickly climbed the charts.  
Head Entity: debut album  
Tail Entity: Columbia Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: " the many adventures of winnie the pooh " is a 1977 american animated buddy musical comedy film produced by walt disney productions and distributed by buena vista distribution .
Head Entity: the many adventures of winnie the pooh
Tail Entity: buena vista distribution
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: "Inception" is a 2010 science fiction film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: Inception  
Tail Entity: Warner Bros. Pictures  

Relation: distributor  
Context: "The Dark Knight" is a 2008 superhero film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: The Dark Knight  
Tail Entity: Warner Bros. Pictures  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Venice is famous for its canals and is built on a group of 118 small islands separated by canals and linked by bridges, situated in the Adriatic Sea.  
Head Entity: Venice  
Tail Entity: Adriatic Sea  

Relation: located in or next to body of water  
Context: The town of Key West is known for its beautiful sunsets and is located at the southernmost point of the continental United States, right next to the Gulf of Mexico.  
Head Entity: Key West  
Tail Entity: Gulf of Mexico  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Sample 1:  
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Sample 2:  
Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: anders morelius from sweden placed third in the individual race .
Head Entity: anders morelius
Tail Entity: sweden
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: maria gonzalez was born in mexico but later moved to the united states.  
Head Entity: maria gonzalez  
Tail Entity: mexico  

Relation: country of citizenship  
Context: after living in canada for several years, liu wei decided to apply for citizenship.  
Head Entity: liu wei  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: buck moved to austin in the mid-1970s and joined the blues rock group , the fabulous thunderbirds , along with keith ferguson , jimmie vaughan , and kim wilson .
Head Entity: the fabulous thunderbirds
Tail Entity: blues rock
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the album "thriller" by michael jackson is widely regarded as a landmark in pop music history, blending elements of rock, funk, and soul.  
Head Entity: michael jackson  
Tail Entity: pop  

Relation: genre  
Context: the film "inception," directed by christopher nolan, is a complex narrative that combines science fiction with psychological thriller elements.  
Head Entity: inception  
Tail Entity: science fiction  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the thames river empties into the north sea, serving as a vital waterway for trade and transportation in london.  
Head Entity: thames river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
Mixup data size:  199
MixupTrain:  epoch  0, batch     0 | loss: 6.1636926MixupTrain:  epoch  0, batch     1 | loss: 5.6845208MixupTrain:  epoch  0, batch     2 | loss: 6.4845629MixupTrain:  epoch  0, batch     3 | loss: 5.2649505MixupTrain:  epoch  0, batch     4 | loss: 5.3681219MixupTrain:  epoch  0, batch     5 | loss: 4.9314484MixupTrain:  epoch  0, batch     6 | loss: 5.0743785MixupTrain:  epoch  0, batch     7 | loss: 5.1125376MixupTrain:  epoch  0, batch     8 | loss: 5.1352491MixupTrain:  epoch  0, batch     9 | loss: 4.8084108MixupTrain:  epoch  0, batch    10 | loss: 5.3326581MixupTrain:  epoch  0, batch    11 | loss: 4.4682921MixupTrain:  epoch  0, batch    12 | loss: 4.0315549
MemoryTrain:  epoch  0, batch     0 | loss: 4.1271014MemoryTrain:  epoch  0, batch     1 | loss: 3.5789051MemoryTrain:  epoch  0, batch     2 | loss: 4.2372761MemoryTrain:  epoch  0, batch     3 | loss: 4.1274009MemoryTrain:  epoch  1, batch     0 | loss: 3.5691042MemoryTrain:  epoch  1, batch     1 | loss: 3.8574953MemoryTrain:  epoch  1, batch     2 | loss: 3.3893206MemoryTrain:  epoch  1, batch     3 | loss: 2.8785999MemoryTrain:  epoch  2, batch     0 | loss: 3.3091164MemoryTrain:  epoch  2, batch     1 | loss: 2.9004171MemoryTrain:  epoch  2, batch     2 | loss: 3.0582125MemoryTrain:  epoch  2, batch     3 | loss: 2.8763492MemoryTrain:  epoch  3, batch     0 | loss: 3.2800121MemoryTrain:  epoch  3, batch     1 | loss: 2.5163279MemoryTrain:  epoch  3, batch     2 | loss: 3.1624718MemoryTrain:  epoch  3, batch     3 | loss: 2.5160100MemoryTrain:  epoch  4, batch     0 | loss: 3.4162292MemoryTrain:  epoch  4, batch     1 | loss: 2.3217862MemoryTrain:  epoch  4, batch     2 | loss: 2.0544317MemoryTrain:  epoch  4, batch     3 | loss: 2.3485961MemoryTrain:  epoch  5, batch     0 | loss: 2.0806355MemoryTrain:  epoch  5, batch     1 | loss: 2.2479265MemoryTrain:  epoch  5, batch     2 | loss: 1.6918701MemoryTrain:  epoch  5, batch     3 | loss: 2.7618759MemoryTrain:  epoch  6, batch     0 | loss: 1.8051780MemoryTrain:  epoch  6, batch     1 | loss: 2.0442500MemoryTrain:  epoch  6, batch     2 | loss: 2.1444197MemoryTrain:  epoch  6, batch     3 | loss: 1.6430506MemoryTrain:  epoch  7, batch     0 | loss: 1.7339287MemoryTrain:  epoch  7, batch     1 | loss: 1.5054250MemoryTrain:  epoch  7, batch     2 | loss: 1.9456871MemoryTrain:  epoch  7, batch     3 | loss: 1.8460348MemoryTrain:  epoch  8, batch     0 | loss: 1.6950562MemoryTrain:  epoch  8, batch     1 | loss: 1.8558831MemoryTrain:  epoch  8, batch     2 | loss: 1.5371997MemoryTrain:  epoch  8, batch     3 | loss: 1.9348996MemoryTrain:  epoch  9, batch     0 | loss: 1.7826231MemoryTrain:  epoch  9, batch     1 | loss: 1.3731242MemoryTrain:  epoch  9, batch     2 | loss: 1.4664314MemoryTrain:  epoch  9, batch     3 | loss: 1.5141747
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 86.03%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 86.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 87.22%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 87.23%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 87.24%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 85.55%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 84.74%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 83.93%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 82.77%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 82.57%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 82.37%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 82.19%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 81.86%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 81.55%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 81.69%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   44 | acc: 25.00%,  total acc: 80.00%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 79.08%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 78.06%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 77.86%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 76.66%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 76.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 77.89%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 78.30%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 79.06%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 79.42%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 79.77%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 80.10%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 80.75%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 80.26%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 89.88%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 89.13%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.96%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 90.30%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.93%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.21%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.48%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.61%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 91.32%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.55%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 91.61%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.83%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.41%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 92.59%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 92.64%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.80%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.69%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 92.71%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.04%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.84%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 92.98%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 92.89%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.02%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 93.03%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.94%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 92.87%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 92.98%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 93.09%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 93.00%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 93.01%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 93.04%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 92.96%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 92.80%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 92.64%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 92.31%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 92.17%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 91.96%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 91.83%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 91.61%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 91.48%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.51%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 91.34%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 91.40%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 91.50%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 91.38%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 91.36%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 91.18%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 91.21%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 90.96%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 90.79%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 90.49%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 90.33%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 90.17%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 89.88%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 89.54%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 89.33%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 89.19%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 88.92%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 88.85%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 88.65%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 88.46%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 88.33%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 88.27%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 87.85%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 87.38%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 86.81%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 86.53%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 86.09%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 85.77%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 85.62%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 86.63%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 86.95%   
cur_acc:  ['0.9474', '0.8026']
his_acc:  ['0.9474', '0.8695']
CurrentTrain: epoch  0, batch     0 | loss: 6.7016764CurrentTrain: epoch  0, batch     1 | loss: 6.5355482CurrentTrain: epoch  0, batch     2 | loss: 6.3403797CurrentTrain: epoch  0, batch     3 | loss: 6.2709999CurrentTrain: epoch  1, batch     0 | loss: 5.9834661CurrentTrain: epoch  1, batch     1 | loss: 5.3550625CurrentTrain: epoch  1, batch     2 | loss: 5.1590128CurrentTrain: epoch  1, batch     3 | loss: 3.7320180CurrentTrain: epoch  2, batch     0 | loss: 5.5759621CurrentTrain: epoch  2, batch     1 | loss: 5.1368451CurrentTrain: epoch  2, batch     2 | loss: 4.6166573CurrentTrain: epoch  2, batch     3 | loss: 7.3824406CurrentTrain: epoch  3, batch     0 | loss: 4.9647126CurrentTrain: epoch  3, batch     1 | loss: 4.7188888CurrentTrain: epoch  3, batch     2 | loss: 4.7129784CurrentTrain: epoch  3, batch     3 | loss: 5.5661526CurrentTrain: epoch  4, batch     0 | loss: 4.3187218CurrentTrain: epoch  4, batch     1 | loss: 4.7699938CurrentTrain: epoch  4, batch     2 | loss: 4.3698978CurrentTrain: epoch  4, batch     3 | loss: 3.2234569CurrentTrain: epoch  5, batch     0 | loss: 4.3497286CurrentTrain: epoch  5, batch     1 | loss: 4.2723980CurrentTrain: epoch  5, batch     2 | loss: 3.6762862CurrentTrain: epoch  5, batch     3 | loss: 2.7877455CurrentTrain: epoch  6, batch     0 | loss: 3.3512282CurrentTrain: epoch  6, batch     1 | loss: 3.9772906CurrentTrain: epoch  6, batch     2 | loss: 4.2001429CurrentTrain: epoch  6, batch     3 | loss: 4.4429932CurrentTrain: epoch  7, batch     0 | loss: 2.9961050CurrentTrain: epoch  7, batch     1 | loss: 4.1551180CurrentTrain: epoch  7, batch     2 | loss: 3.7954814CurrentTrain: epoch  7, batch     3 | loss: 6.1168275CurrentTrain: epoch  8, batch     0 | loss: 3.6102877CurrentTrain: epoch  8, batch     1 | loss: 3.8832159CurrentTrain: epoch  8, batch     2 | loss: 3.3619814CurrentTrain: epoch  8, batch     3 | loss: 2.3205848CurrentTrain: epoch  9, batch     0 | loss: 3.7006462CurrentTrain: epoch  9, batch     1 | loss: 3.1536880CurrentTrain: epoch  9, batch     2 | loss: 3.2077980CurrentTrain: epoch  9, batch     3 | loss: 4.4452505
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the novel "pride and prejudice" explores the themes of love and social class in early 19th century England.  
Head Entity: pride and prejudice  
Tail Entity: love  

Relation: main subject  
Context: the documentary "our planet" showcases the beauty of nature and the impact of climate change on wildlife.  
Head Entity: our planet  
Tail Entity: climate change  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: group b of uefa euro 2016 contained england , russia , wales and slovakia .
Head Entity: uefa euro 2016
Tail Entity: slovakia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: the 2020 summer olympics featured athletes from countries like usa, china, and japan.  
Head Entity: 2020 summer olympics  
Tail Entity: japan  

Relation: participating team  
Context: the fifa world cup 2018 included teams such as france, croatia, and belgium.  
Head Entity: fifa world cup 2018  
Tail Entity: croatia  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: in 1971 , chris and pat joined ratchell with bassist howard messer and former steppenwolf guitarist larry byrom .
Head Entity: ratchell
Tail Entity: larry byrom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The car is equipped with a powerful engine, which includes a turbocharger and a high-performance exhaust system.  
Head Entity: the car  
Tail Entity: turbocharger  

Relation: has part  
Context: The human body consists of various organs, including the heart, lungs, and liver, each playing a crucial role in maintaining health.  
Head Entity: human body  
Tail Entity: heart  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: In 2020, the popular series "Succession" was nominated for several Emmy Awards, showcasing its critical acclaim and audience popularity.  
Head Entity: Succession  
Tail Entity: Emmy Awards  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone has revolutionized the way we communicate and interact with technology, becoming a benchmark for mobile devices worldwide.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is an ancient series of walls and fortifications that were built to protect Chinese states from invasions, serving as a symbol of China's historical strength.  
Head Entity: Great Wall of China  
Tail Entity: wall
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: the valley is confined by the zillertal alps in the northeast and the sarntal alps in the southwest , rising up to the tagewaldhorn peak at .
Head Entity: tagewaldhorn
Tail Entity: sarntal alps
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the majestic peaks of the rocky mountains stretch across several states, with the highest point being mount elbert, which towers over the surrounding landscape.  
Head Entity: mount elbert  
Tail Entity: rocky mountains  

Relation: mountain range  
Context: the andes mountains run along the western edge of south america, with the iconic aconcagua standing as the tallest peak in this vast range.  
Head Entity: aconcagua  
Tail Entity: andes mountains  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: it would also be the last episode to feature a grounding , until " the marge - ian chronicles " in season 27 , six years later ( also written by brian kelley ) .
Head Entity: the marge - ian chronicles
Tail Entity: brian kelley
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The film "Inception" was a groundbreaking project that showcased the talents of many, but it was the script penned by Christopher Nolan that truly captivated audiences.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The critically acclaimed movie "The Social Network" was based on the book "The Accidental Billionaires," but it was Aaron Sorkin's brilliant screenplay that brought the story to life on the big screen.  
Head Entity: The Social Network  
Tail Entity: Aaron Sorkin  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative that incorporates various cultural elements, primarily drawing from Chinese and Inuit languages.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Chinese and Inuit languages  

Relation: language of work or name  
Context: The novel "One Hundred Years of Solitude" by Gabriel García Márquez is celebrated for its magical realism and is originally written in Spanish, reflecting the author's Colombian heritage.  
Head Entity: One Hundred Years of Solitude  
Tail Entity: Spanish
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: gogukwon 's successor , sosurim , adopted a foreign policy of appeasement and reconciliation with baekje , and concentrated on domestic policies to spread buddhism throughout goguryeo 's social and political systems .
Head Entity: goguryeo
Tail Entity: buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: The ancient temple of Angkor Wat, originally constructed as a Hindu temple dedicated to the god Vishnu, later transformed into a Buddhist temple, reflecting the region's shift in religious practices over the centuries.  
Head Entity: Angkor Wat  
Tail Entity: Buddhism  

Relation: religion  
Context: The community center in the heart of the city serves as a hub for various religious activities, including weekly gatherings for the local Muslim population to observe their prayers and celebrate important festivals.  
Head Entity: community center  
Tail Entity: Islam  
Mixup data size:  259
MixupTrain:  epoch  0, batch     0 | loss: 3.6851199MixupTrain:  epoch  0, batch     1 | loss: 3.3625848MixupTrain:  epoch  0, batch     2 | loss: 3.7446165MixupTrain:  epoch  0, batch     3 | loss: 4.1485735MixupTrain:  epoch  0, batch     4 | loss: 3.6775650MixupTrain:  epoch  0, batch     5 | loss: 3.2577721MixupTrain:  epoch  0, batch     6 | loss: 3.1058237MixupTrain:  epoch  0, batch     7 | loss: 3.1709522MixupTrain:  epoch  0, batch     8 | loss: 3.1713976MixupTrain:  epoch  0, batch     9 | loss: 3.1801605MixupTrain:  epoch  0, batch    10 | loss: 3.1094929MixupTrain:  epoch  0, batch    11 | loss: 3.1620434MixupTrain:  epoch  0, batch    12 | loss: 3.1096636MixupTrain:  epoch  0, batch    13 | loss: 2.8970825MixupTrain:  epoch  0, batch    14 | loss: 2.7383107MixupTrain:  epoch  0, batch    15 | loss: 2.9813040MixupTrain:  epoch  0, batch    16 | loss: 2.5723057
MemoryTrain:  epoch  0, batch     0 | loss: 2.5480413MemoryTrain:  epoch  0, batch     1 | loss: 2.2742615MemoryTrain:  epoch  0, batch     2 | loss: 2.1116018MemoryTrain:  epoch  0, batch     3 | loss: 3.3977599MemoryTrain:  epoch  0, batch     4 | loss: 2.9576740MemoryTrain:  epoch  0, batch     5 | loss: 3.5134866MemoryTrain:  epoch  1, batch     0 | loss: 2.8309164MemoryTrain:  epoch  1, batch     1 | loss: 2.5319130MemoryTrain:  epoch  1, batch     2 | loss: 2.4927711MemoryTrain:  epoch  1, batch     3 | loss: 2.2935848MemoryTrain:  epoch  1, batch     4 | loss: 1.8892689MemoryTrain:  epoch  1, batch     5 | loss: 1.8169073MemoryTrain:  epoch  2, batch     0 | loss: 1.7946947MemoryTrain:  epoch  2, batch     1 | loss: 2.6040983MemoryTrain:  epoch  2, batch     2 | loss: 1.9968746MemoryTrain:  epoch  2, batch     3 | loss: 2.0008693MemoryTrain:  epoch  2, batch     4 | loss: 2.2088919MemoryTrain:  epoch  2, batch     5 | loss: 2.4817150MemoryTrain:  epoch  3, batch     0 | loss: 1.7670190MemoryTrain:  epoch  3, batch     1 | loss: 1.6673238MemoryTrain:  epoch  3, batch     2 | loss: 2.0737100MemoryTrain:  epoch  3, batch     3 | loss: 2.1862037MemoryTrain:  epoch  3, batch     4 | loss: 1.6743745MemoryTrain:  epoch  3, batch     5 | loss: 2.1316311MemoryTrain:  epoch  4, batch     0 | loss: 2.1404223MemoryTrain:  epoch  4, batch     1 | loss: 2.0028327MemoryTrain:  epoch  4, batch     2 | loss: 1.8401845MemoryTrain:  epoch  4, batch     3 | loss: 1.5870104MemoryTrain:  epoch  4, batch     4 | loss: 1.3964597MemoryTrain:  epoch  4, batch     5 | loss: 1.5702784MemoryTrain:  epoch  5, batch     0 | loss: 1.5665393MemoryTrain:  epoch  5, batch     1 | loss: 1.7062900MemoryTrain:  epoch  5, batch     2 | loss: 1.6977295MemoryTrain:  epoch  5, batch     3 | loss: 1.7634348MemoryTrain:  epoch  5, batch     4 | loss: 1.5898094MemoryTrain:  epoch  5, batch     5 | loss: 1.6353543MemoryTrain:  epoch  6, batch     0 | loss: 1.2698958MemoryTrain:  epoch  6, batch     1 | loss: 1.9043787MemoryTrain:  epoch  6, batch     2 | loss: 1.8047870MemoryTrain:  epoch  6, batch     3 | loss: 1.5792878MemoryTrain:  epoch  6, batch     4 | loss: 1.5755043MemoryTrain:  epoch  6, batch     5 | loss: 1.3893766MemoryTrain:  epoch  7, batch     0 | loss: 1.5115254MemoryTrain:  epoch  7, batch     1 | loss: 1.5634089MemoryTrain:  epoch  7, batch     2 | loss: 1.6234668MemoryTrain:  epoch  7, batch     3 | loss: 1.3992013MemoryTrain:  epoch  7, batch     4 | loss: 1.3910811MemoryTrain:  epoch  7, batch     5 | loss: 1.5640501MemoryTrain:  epoch  8, batch     0 | loss: 1.6205280MemoryTrain:  epoch  8, batch     1 | loss: 1.6023716MemoryTrain:  epoch  8, batch     2 | loss: 1.3238366MemoryTrain:  epoch  8, batch     3 | loss: 1.4099839MemoryTrain:  epoch  8, batch     4 | loss: 1.4433358MemoryTrain:  epoch  8, batch     5 | loss: 1.7544994MemoryTrain:  epoch  9, batch     0 | loss: 1.4043261MemoryTrain:  epoch  9, batch     1 | loss: 1.4995031MemoryTrain:  epoch  9, batch     2 | loss: 1.4146404MemoryTrain:  epoch  9, batch     3 | loss: 1.4659476MemoryTrain:  epoch  9, batch     4 | loss: 1.4403787MemoryTrain:  epoch  9, batch     5 | loss: 1.3398999
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 71.69%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 67.76%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 50.00%,  total acc: 74.07%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 73.88%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 74.35%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 74.17%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 73.99%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 74.41%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 75.55%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 76.07%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 76.39%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 76.86%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.04%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 79.61%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 80.09%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 80.40%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 80.00%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 79.76%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 79.26%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 78.78%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 78.44%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 78.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 78.68%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 78.73%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 78.89%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 79.05%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 79.32%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 79.39%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 79.31%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 79.56%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 79.27%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 79.41%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 79.13%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 78.47%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 86.40%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 86.96%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.15%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 90.07%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 89.82%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 89.76%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 89.86%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 89.97%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.47%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 90.55%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 90.70%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.97%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 90.90%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 90.96%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.02%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.23%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.27%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.14%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 91.34%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 91.27%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 91.31%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 91.39%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 91.43%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 91.57%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 91.41%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 91.57%   [EVAL] batch:   66 | acc: 93.75%,  total acc: 91.60%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 91.64%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 91.76%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.99%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 91.93%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 91.87%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 91.72%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 91.58%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 91.69%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 91.56%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 91.43%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 91.22%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 91.28%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 91.31%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 90.96%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 91.00%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 90.81%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 90.92%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 90.59%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 90.52%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 90.42%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 90.45%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 90.35%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 89.99%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 89.76%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 89.74%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 89.52%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 89.24%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 88.97%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 88.88%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 88.61%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 88.48%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 88.29%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 88.16%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 88.04%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 88.03%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 87.85%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 87.38%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 86.93%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 86.65%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 86.20%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 85.79%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 85.91%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 86.15%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 86.27%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 86.39%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 86.56%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 86.57%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 86.68%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 86.74%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 86.95%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 86.86%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 86.66%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 86.67%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 86.58%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 86.54%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 86.55%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 86.60%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 86.70%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 86.71%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 86.76%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 86.86%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 86.86%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 86.73%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 86.24%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 85.85%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 85.46%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 85.12%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 84.79%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 84.42%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 84.59%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 84.65%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 84.76%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 84.92%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 84.89%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 84.66%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 84.56%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 84.58%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 84.48%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 84.39%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 84.45%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 84.51%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 84.57%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 84.59%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 84.65%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 84.74%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 84.83%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 84.92%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 85.02%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 85.24%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 85.11%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 85.01%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 84.85%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 84.68%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 84.55%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 84.54%   [EVAL] batch:  175 | acc: 87.50%,  total acc: 84.55%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 84.53%   [EVAL] batch:  177 | acc: 87.50%,  total acc: 84.55%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 84.57%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 84.62%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 84.63%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 84.53%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 84.58%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 84.46%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 84.48%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 84.36%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 84.11%   
cur_acc:  ['0.9474', '0.8026', '0.7847']
his_acc:  ['0.9474', '0.8695', '0.8411']
CurrentTrain: epoch  0, batch     0 | loss: 4.5233283CurrentTrain: epoch  0, batch     1 | loss: 4.4065223CurrentTrain: epoch  0, batch     2 | loss: 5.5882807CurrentTrain: epoch  0, batch     3 | loss: 4.0980663CurrentTrain: epoch  1, batch     0 | loss: 4.1037970CurrentTrain: epoch  1, batch     1 | loss: 3.5222538CurrentTrain: epoch  1, batch     2 | loss: 3.5187356CurrentTrain: epoch  1, batch     3 | loss: 3.9674840CurrentTrain: epoch  2, batch     0 | loss: 3.2572327CurrentTrain: epoch  2, batch     1 | loss: 2.9252315CurrentTrain: epoch  2, batch     2 | loss: 2.8625362CurrentTrain: epoch  2, batch     3 | loss: 6.2649879CurrentTrain: epoch  3, batch     0 | loss: 3.2872229CurrentTrain: epoch  3, batch     1 | loss: 3.1852074CurrentTrain: epoch  3, batch     2 | loss: 2.5963101CurrentTrain: epoch  3, batch     3 | loss: 2.1566377CurrentTrain: epoch  4, batch     0 | loss: 3.1229715CurrentTrain: epoch  4, batch     1 | loss: 2.9202127CurrentTrain: epoch  4, batch     2 | loss: 2.2622852CurrentTrain: epoch  4, batch     3 | loss: 1.8533266CurrentTrain: epoch  5, batch     0 | loss: 2.4377806CurrentTrain: epoch  5, batch     1 | loss: 2.6547902CurrentTrain: epoch  5, batch     2 | loss: 2.4255867CurrentTrain: epoch  5, batch     3 | loss: 1.9673843CurrentTrain: epoch  6, batch     0 | loss: 2.1816001CurrentTrain: epoch  6, batch     1 | loss: 2.5565114CurrentTrain: epoch  6, batch     2 | loss: 2.6231070CurrentTrain: epoch  6, batch     3 | loss: 1.9504188CurrentTrain: epoch  7, batch     0 | loss: 2.2040548CurrentTrain: epoch  7, batch     1 | loss: 2.5278254CurrentTrain: epoch  7, batch     2 | loss: 2.0497751CurrentTrain: epoch  7, batch     3 | loss: 2.7719250CurrentTrain: epoch  8, batch     0 | loss: 2.1883085CurrentTrain: epoch  8, batch     1 | loss: 2.2978816CurrentTrain: epoch  8, batch     2 | loss: 2.0934820CurrentTrain: epoch  8, batch     3 | loss: 1.9273074CurrentTrain: epoch  9, batch     0 | loss: 2.0968013CurrentTrain: epoch  9, batch     1 | loss: 1.9573963CurrentTrain: epoch  9, batch     2 | loss: 1.9912735CurrentTrain: epoch  9, batch     3 | loss: 2.4837079
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor, she became a prominent figure in the local chapter of the Green Party, advocating for environmental policies and community engagement.  
Head Entity: she  
Tail Entity: Green Party  

Relation: member of political party  
Context: During his tenure in the Senate, he was known for his strong support of the Democratic Party's initiatives on healthcare reform and education.  
Head Entity: he  
Tail Entity: Democratic Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the script for " the great santini " was adapted by carlino from the 1976 novel by pat conroy , with assistance from an un - credited herman raucher .
Head Entity: the great santini
Tail Entity: pat conroy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "inception" was heavily influenced by the concepts presented in the 2001 novel "the dreamers" by j. k. rowling, which explores the nature of dreams and reality.  
Head Entity: inception  
Tail Entity: j. k. rowling  

Relation: after a work by  
Context: the musical "hamilton" draws inspiration from the biography "alexander hamilton" written by ron chernow, which details the life of the founding father.  
Head Entity: hamilton  
Tail Entity: ron chernow  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the group named itself after the popular children 's book harold and the purple crayon by crockett johnson , whose protagonist , harold , uses a purple crayon to draw his imagination into reality .
Head Entity: crockett johnson
Tail Entity: harold and the purple crayon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: The renowned physicist Albert Einstein is best known for his groundbreaking theory of relativity, which revolutionized our understanding of space, time, and gravity.  
Head Entity: Albert Einstein  
Tail Entity: theory of relativity  

Relation: notable work  
Context: The famous painter Vincent van Gogh created numerous masterpieces, but his most celebrated painting, "Starry Night," continues to captivate art lovers around the world.  
Head Entity: Vincent van Gogh  
Tail Entity: Starry Night  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the fashion brand saw a significant increase in sales and brand recognition.  
Head Entity: fashion brand  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most well-known, classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of living organisms, the order Primates encompasses various families, including Hominidae, which contains humans and their closest relatives.  
Head Entity: Hominidae  
Tail Entity: order  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in the field of artificial intelligence and currently works at the tech innovation lab in silicon valley.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. rajesh kumar as the new director of global health initiatives, focusing on infectious diseases.  
Head Entity: dr. rajesh kumar  
Tail Entity: infectious diseases  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: since march 2006 , wkxp 's programming has been simulcasted on 97.3 wzad wurtsboro , new york .
Head Entity: wzad
Tail Entity: wurtsboro , new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: in 2010, the station wxyz began broadcasting to the community of springfield, providing local news and entertainment.  
Head Entity: wxyz  
Tail Entity: springfield  

Relation: licensed to broadcast to  
Context: the radio station kqrs has been granted a license to broadcast to the greater minneapolis area since 1995.  
Head Entity: kqrs  
Tail Entity: greater minneapolis area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: hd 32518 b is an extrasolar planet which orbits the k - type giant star hd 32518 , located approximately 383 light years away in the constellation camelopardalis .
Head Entity: hd 32518
Tail Entity: camelopardalis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the star betelgeuse is located in the constellation orion and is one of the brightest stars in the night sky.  
Head Entity: betelgeuse  
Tail Entity: orion  

Relation: constellation  
Context: the andromeda galaxy is visible in the night sky and is located in the constellation andromeda, which is named after a princess in Greek mythology.  
Head Entity: andromeda galaxy  
Tail Entity: andromeda  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Mixup data size:  318
MixupTrain:  epoch  0, batch     0 | loss: 2.1102210MixupTrain:  epoch  0, batch     1 | loss: 2.5577692MixupTrain:  epoch  0, batch     2 | loss: 1.9628280MixupTrain:  epoch  0, batch     3 | loss: 2.5530538MixupTrain:  epoch  0, batch     4 | loss: 2.5911062MixupTrain:  epoch  0, batch     5 | loss: 2.4337557MixupTrain:  epoch  0, batch     6 | loss: 2.5755601MixupTrain:  epoch  0, batch     7 | loss: 2.0847262MixupTrain:  epoch  0, batch     8 | loss: 2.1039750MixupTrain:  epoch  0, batch     9 | loss: 2.3863031MixupTrain:  epoch  0, batch    10 | loss: 2.3269100MixupTrain:  epoch  0, batch    11 | loss: 2.0342461MixupTrain:  epoch  0, batch    12 | loss: 2.4339228MixupTrain:  epoch  0, batch    13 | loss: 2.0764925MixupTrain:  epoch  0, batch    14 | loss: 1.9691491MixupTrain:  epoch  0, batch    15 | loss: 2.0570121MixupTrain:  epoch  0, batch    16 | loss: 2.1526392MixupTrain:  epoch  0, batch    17 | loss: 1.9830415MixupTrain:  epoch  0, batch    18 | loss: 2.4553862MixupTrain:  epoch  0, batch    19 | loss: 2.1234252
MemoryTrain:  epoch  0, batch     0 | loss: 1.9455193MemoryTrain:  epoch  0, batch     1 | loss: 2.3189044MemoryTrain:  epoch  0, batch     2 | loss: 2.3563199MemoryTrain:  epoch  0, batch     3 | loss: 3.4708233MemoryTrain:  epoch  0, batch     4 | loss: 2.3158121MemoryTrain:  epoch  0, batch     5 | loss: 2.7423401MemoryTrain:  epoch  0, batch     6 | loss: 1.7645960MemoryTrain:  epoch  0, batch     7 | loss: 2.0038941MemoryTrain:  epoch  1, batch     0 | loss: 1.9376874MemoryTrain:  epoch  1, batch     1 | loss: 2.3434300MemoryTrain:  epoch  1, batch     2 | loss: 1.8513899MemoryTrain:  epoch  1, batch     3 | loss: 1.9868422MemoryTrain:  epoch  1, batch     4 | loss: 1.7390207MemoryTrain:  epoch  1, batch     5 | loss: 2.3544250MemoryTrain:  epoch  1, batch     6 | loss: 2.9685097MemoryTrain:  epoch  1, batch     7 | loss: 2.1410654MemoryTrain:  epoch  2, batch     0 | loss: 1.7981131MemoryTrain:  epoch  2, batch     1 | loss: 1.7936428MemoryTrain:  epoch  2, batch     2 | loss: 1.4624493MemoryTrain:  epoch  2, batch     3 | loss: 1.9327742MemoryTrain:  epoch  2, batch     4 | loss: 2.6235583MemoryTrain:  epoch  2, batch     5 | loss: 1.5896606MemoryTrain:  epoch  2, batch     6 | loss: 1.7641661MemoryTrain:  epoch  2, batch     7 | loss: 1.8212495MemoryTrain:  epoch  3, batch     0 | loss: 1.8278713MemoryTrain:  epoch  3, batch     1 | loss: 1.7626995MemoryTrain:  epoch  3, batch     2 | loss: 1.5799042MemoryTrain:  epoch  3, batch     3 | loss: 1.6862450MemoryTrain:  epoch  3, batch     4 | loss: 1.7697132MemoryTrain:  epoch  3, batch     5 | loss: 1.5006052MemoryTrain:  epoch  3, batch     6 | loss: 1.4247489MemoryTrain:  epoch  3, batch     7 | loss: 1.3693062MemoryTrain:  epoch  4, batch     0 | loss: 1.5091118MemoryTrain:  epoch  4, batch     1 | loss: 1.4150943MemoryTrain:  epoch  4, batch     2 | loss: 1.3858626MemoryTrain:  epoch  4, batch     3 | loss: 1.4648751MemoryTrain:  epoch  4, batch     4 | loss: 1.5037324MemoryTrain:  epoch  4, batch     5 | loss: 1.6728306MemoryTrain:  epoch  4, batch     6 | loss: 1.4790766MemoryTrain:  epoch  4, batch     7 | loss: 1.3390698MemoryTrain:  epoch  5, batch     0 | loss: 1.3756940MemoryTrain:  epoch  5, batch     1 | loss: 1.3599277MemoryTrain:  epoch  5, batch     2 | loss: 1.3372792MemoryTrain:  epoch  5, batch     3 | loss: 1.2634969MemoryTrain:  epoch  5, batch     4 | loss: 1.3464162MemoryTrain:  epoch  5, batch     5 | loss: 1.4525070MemoryTrain:  epoch  5, batch     6 | loss: 1.4714799MemoryTrain:  epoch  5, batch     7 | loss: 1.5290663MemoryTrain:  epoch  6, batch     0 | loss: 1.3576791MemoryTrain:  epoch  6, batch     1 | loss: 1.4561462MemoryTrain:  epoch  6, batch     2 | loss: 1.4192542MemoryTrain:  epoch  6, batch     3 | loss: 1.3169284MemoryTrain:  epoch  6, batch     4 | loss: 1.2349613MemoryTrain:  epoch  6, batch     5 | loss: 1.3276625MemoryTrain:  epoch  6, batch     6 | loss: 1.2909634MemoryTrain:  epoch  6, batch     7 | loss: 1.3290585MemoryTrain:  epoch  7, batch     0 | loss: 1.2410657MemoryTrain:  epoch  7, batch     1 | loss: 1.3610158MemoryTrain:  epoch  7, batch     2 | loss: 1.2801499MemoryTrain:  epoch  7, batch     3 | loss: 1.4228532MemoryTrain:  epoch  7, batch     4 | loss: 1.2306941MemoryTrain:  epoch  7, batch     5 | loss: 1.2864614MemoryTrain:  epoch  7, batch     6 | loss: 1.3602574MemoryTrain:  epoch  7, batch     7 | loss: 1.2826540MemoryTrain:  epoch  8, batch     0 | loss: 1.2248061MemoryTrain:  epoch  8, batch     1 | loss: 1.3411429MemoryTrain:  epoch  8, batch     2 | loss: 1.3690794MemoryTrain:  epoch  8, batch     3 | loss: 1.3254077MemoryTrain:  epoch  8, batch     4 | loss: 1.2083926MemoryTrain:  epoch  8, batch     5 | loss: 1.2914655MemoryTrain:  epoch  8, batch     6 | loss: 1.2118078MemoryTrain:  epoch  8, batch     7 | loss: 1.2577510MemoryTrain:  epoch  9, batch     0 | loss: 1.2963395MemoryTrain:  epoch  9, batch     1 | loss: 1.2384429MemoryTrain:  epoch  9, batch     2 | loss: 1.2405655MemoryTrain:  epoch  9, batch     3 | loss: 1.2942536MemoryTrain:  epoch  9, batch     4 | loss: 1.2105392MemoryTrain:  epoch  9, batch     5 | loss: 1.2613266MemoryTrain:  epoch  9, batch     6 | loss: 1.3087466MemoryTrain:  epoch  9, batch     7 | loss: 1.2152739
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.40%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 78.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.09%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 82.62%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 82.58%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 81.99%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 82.12%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 81.59%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.23%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 84.01%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.23%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 84.92%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 85.24%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.84%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 86.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.27%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 86.92%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 87.39%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 87.61%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 88.10%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 87.50%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.94%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 78.80%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.11%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.28%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 84.46%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 84.55%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 84.80%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 85.03%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 85.98%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.31%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 86.34%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.51%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 86.53%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 86.55%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 86.30%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 86.75%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 86.64%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 86.54%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 86.34%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 85.80%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 85.83%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 85.96%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 85.88%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 85.91%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 86.04%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 86.07%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 86.19%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 86.41%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 86.33%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 86.65%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 86.95%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 87.14%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 87.32%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 87.41%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 87.41%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 87.33%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 87.25%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 87.09%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 86.93%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 86.86%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 86.71%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 86.64%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 86.73%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 86.52%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 86.32%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 86.26%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 85.99%   [EVAL] batch:   87 | acc: 100.00%,  total acc: 86.15%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 86.17%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 86.18%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 86.26%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 86.14%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 85.89%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 85.64%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 85.59%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 85.55%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 85.37%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 85.14%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 85.10%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 84.84%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 84.68%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 84.59%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 84.50%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 84.40%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 84.43%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 84.17%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 83.74%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 83.26%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 83.01%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 82.55%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 82.25%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 82.02%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.49%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 82.79%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 83.02%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 83.11%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 83.60%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 83.58%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 83.46%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 83.50%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 83.43%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 83.51%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 83.54%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 83.62%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 83.74%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 83.77%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 84.01%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 84.12%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 84.01%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 83.59%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 83.21%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 82.85%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 82.53%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 82.26%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 81.99%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 82.07%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 82.19%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 82.27%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 82.51%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 82.58%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 82.45%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 82.11%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 81.99%   [EVAL] batch:  153 | acc: 75.00%,  total acc: 81.94%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 81.69%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 81.53%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 81.57%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 81.65%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 81.72%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 81.80%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 81.83%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 81.91%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 81.90%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 81.86%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 81.78%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 81.66%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 81.62%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 81.62%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 81.58%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 81.54%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 81.39%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 81.32%   [EVAL] batch:  174 | acc: 81.25%,  total acc: 81.32%   [EVAL] batch:  175 | acc: 81.25%,  total acc: 81.32%   [EVAL] batch:  176 | acc: 87.50%,  total acc: 81.36%   [EVAL] batch:  177 | acc: 93.75%,  total acc: 81.43%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 81.46%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 81.56%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 81.60%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 81.39%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 81.28%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 81.15%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 81.01%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 80.88%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 80.78%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 80.85%   [EVAL] batch:  189 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 81.05%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 81.15%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 81.19%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 81.12%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 81.12%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 81.03%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 80.87%   [EVAL] batch:  198 | acc: 50.00%,  total acc: 80.72%   [EVAL] batch:  199 | acc: 62.50%,  total acc: 80.62%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 80.66%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 80.69%   [EVAL] batch:  202 | acc: 93.75%,  total acc: 80.76%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 80.73%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 80.73%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 80.70%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 80.74%   [EVAL] batch:  207 | acc: 81.25%,  total acc: 80.74%   [EVAL] batch:  208 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:  209 | acc: 62.50%,  total acc: 80.68%   [EVAL] batch:  210 | acc: 75.00%,  total acc: 80.66%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 80.51%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 80.55%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 80.64%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 80.73%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 80.90%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 81.08%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 80.99%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 81.05%   [EVAL] batch:  221 | acc: 68.75%,  total acc: 81.00%   [EVAL] batch:  222 | acc: 81.25%,  total acc: 81.00%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 80.97%   [EVAL] batch:  224 | acc: 68.75%,  total acc: 80.92%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 81.00%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 81.08%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 81.17%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 81.33%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 81.39%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 81.47%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 81.62%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 81.78%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 81.86%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 81.93%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 81.98%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 82.15%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 82.27%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 82.35%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 82.37%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 82.51%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 82.58%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 82.62%   
cur_acc:  ['0.9474', '0.8026', '0.7847', '0.8750']
his_acc:  ['0.9474', '0.8695', '0.8411', '0.8263']
CurrentTrain: epoch  0, batch     0 | loss: 5.1565113CurrentTrain: epoch  0, batch     1 | loss: 6.3044491CurrentTrain: epoch  0, batch     2 | loss: 5.1652832CurrentTrain: epoch  0, batch     3 | loss: 6.5287991CurrentTrain: epoch  1, batch     0 | loss: 4.8393722CurrentTrain: epoch  1, batch     1 | loss: 5.0771847CurrentTrain: epoch  1, batch     2 | loss: 4.0519528CurrentTrain: epoch  1, batch     3 | loss: 6.0972743CurrentTrain: epoch  2, batch     0 | loss: 4.9095526CurrentTrain: epoch  2, batch     1 | loss: 3.3680725CurrentTrain: epoch  2, batch     2 | loss: 4.0110707CurrentTrain: epoch  2, batch     3 | loss: 4.6194592CurrentTrain: epoch  3, batch     0 | loss: 4.5739679CurrentTrain: epoch  3, batch     1 | loss: 3.3783660CurrentTrain: epoch  3, batch     2 | loss: 4.0870218CurrentTrain: epoch  3, batch     3 | loss: 3.4745107CurrentTrain: epoch  4, batch     0 | loss: 3.9343410CurrentTrain: epoch  4, batch     1 | loss: 3.9716346CurrentTrain: epoch  4, batch     2 | loss: 3.1251605CurrentTrain: epoch  4, batch     3 | loss: 2.5493801CurrentTrain: epoch  5, batch     0 | loss: 3.5021231CurrentTrain: epoch  5, batch     1 | loss: 3.1581750CurrentTrain: epoch  5, batch     2 | loss: 3.5594304CurrentTrain: epoch  5, batch     3 | loss: 2.7419529CurrentTrain: epoch  6, batch     0 | loss: 2.4939353CurrentTrain: epoch  6, batch     1 | loss: 3.4113631CurrentTrain: epoch  6, batch     2 | loss: 3.2742167CurrentTrain: epoch  6, batch     3 | loss: 3.1464529CurrentTrain: epoch  7, batch     0 | loss: 2.7449069CurrentTrain: epoch  7, batch     1 | loss: 3.0570769CurrentTrain: epoch  7, batch     2 | loss: 2.9210093CurrentTrain: epoch  7, batch     3 | loss: 2.9569550CurrentTrain: epoch  8, batch     0 | loss: 2.5459440CurrentTrain: epoch  8, batch     1 | loss: 2.7345657CurrentTrain: epoch  8, batch     2 | loss: 2.7090406CurrentTrain: epoch  8, batch     3 | loss: 4.4488339CurrentTrain: epoch  9, batch     0 | loss: 2.3434424CurrentTrain: epoch  9, batch     1 | loss: 2.7014160CurrentTrain: epoch  9, batch     2 | loss: 2.8198509CurrentTrain: epoch  9, batch     3 | loss: 2.9892704
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions, with Ontario being one of the largest provinces in the country.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The state of California is divided into several counties, including Los Angeles County, which is known for its entertainment industry.  
Head Entity: California  
Tail Entity: Los Angeles County  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: heinz , is a german given name , a diminutive of heinrich and cognate of the given name henry .
Head Entity: heinrich
Tail Entity: henry
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although experts argue there are key differences.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: he considers his father to be biggest influence on his career as his brother javier castellano recipient of four eclipse award for outstanding jockey in the row ( 2013 , 2014,2015 and 2016 ) .
Head Entity: eclipse award for outstanding jockey
Tail Entity: javier castellano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In the thrilling finale of the championship, Sarah Thompson emerged victorious, claiming the title of best player in the tournament, while her teammate, Mark Johnson, was awarded the runner-up position.  
Head Entity: best player in the tournament  
Tail Entity: Sarah Thompson  

Relation: winner  
Context: The annual science fair concluded with Emily Chen taking home the grand prize for her innovative project on renewable energy, while her classmate, David Lee, received an honorable mention for his work on robotics.  
Head Entity: grand prize  
Tail Entity: Emily Chen  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the british navy, famously known for his rank of vice admiral during the napoleonic wars.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: he also worked on activision games like " gun " , " " and " the amazing spider - man " .
Head Entity: gun
Tail Entity: activision
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by J.K. Rowling was released by Bloomsbury Publishing, captivating readers worldwide.  
Head Entity: J.K. Rowling  
Tail Entity: Bloomsbury Publishing  

Relation: publisher  
Context: The popular science magazine was launched by National Geographic, providing insights into nature and exploration.  
Head Entity: science magazine  
Tail Entity: National Geographic  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: following this song were the # 21 " she wants to be wanted again " , the # 2 " loved too much " and the # 17 " i have to surrender " .
Head Entity: she wants to be wanted again
Tail Entity: loved too much
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The top three movies of the year were "Inception," followed by "The Social Network" and "Toy Story 3."  
Head Entity: Inception  
Tail Entity: The Social Network  

Relation: followed by  
Context: The conference schedule included a keynote speech, followed by a panel discussion and then a networking session.  
Head Entity: keynote speech  
Tail Entity: panel discussion  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in San Francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her time at the university, she conducted research in various labs located in Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme activity.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting millions of tourists each year.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti (october 12, 1935 – september 6, 2007) was an italian operatic tenor who became one of the most celebrated and influential opera singers of the 20th century.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey (born march 27, 1969) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: soprano  
Mixup data size:  378
MixupTrain:  epoch  0, batch     0 | loss: 2.1138586MixupTrain:  epoch  0, batch     1 | loss: 2.2156077MixupTrain:  epoch  0, batch     2 | loss: 2.3195138MixupTrain:  epoch  0, batch     3 | loss: 2.2840528MixupTrain:  epoch  0, batch     4 | loss: 1.9152368MixupTrain:  epoch  0, batch     5 | loss: 2.1762806MixupTrain:  epoch  0, batch     6 | loss: 2.2582806MixupTrain:  epoch  0, batch     7 | loss: 1.8484470MixupTrain:  epoch  0, batch     8 | loss: 2.2139750MixupTrain:  epoch  0, batch     9 | loss: 1.7682349MixupTrain:  epoch  0, batch    10 | loss: 1.6669323MixupTrain:  epoch  0, batch    11 | loss: 1.7245190MixupTrain:  epoch  0, batch    12 | loss: 1.7245686MixupTrain:  epoch  0, batch    13 | loss: 2.3070851MixupTrain:  epoch  0, batch    14 | loss: 1.9074934MixupTrain:  epoch  0, batch    15 | loss: 2.9723754MixupTrain:  epoch  0, batch    16 | loss: 1.5192985MixupTrain:  epoch  0, batch    17 | loss: 1.9193650MixupTrain:  epoch  0, batch    18 | loss: 1.6621535MixupTrain:  epoch  0, batch    19 | loss: 1.6950623MixupTrain:  epoch  0, batch    20 | loss: 2.5433289MixupTrain:  epoch  0, batch    21 | loss: 2.3836773MixupTrain:  epoch  0, batch    22 | loss: 2.0105345MixupTrain:  epoch  0, batch    23 | loss: 1.8409805
MemoryTrain:  epoch  0, batch     0 | loss: 1.7928259MemoryTrain:  epoch  0, batch     1 | loss: 2.3167205MemoryTrain:  epoch  0, batch     2 | loss: 2.0166516MemoryTrain:  epoch  0, batch     3 | loss: 1.9859141MemoryTrain:  epoch  0, batch     4 | loss: 2.0002253MemoryTrain:  epoch  0, batch     5 | loss: 1.5745413MemoryTrain:  epoch  0, batch     6 | loss: 2.0375624MemoryTrain:  epoch  0, batch     7 | loss: 2.0330560MemoryTrain:  epoch  0, batch     8 | loss: 2.1311140MemoryTrain:  epoch  0, batch     9 | loss: 1.4834546MemoryTrain:  epoch  1, batch     0 | loss: 1.8982674MemoryTrain:  epoch  1, batch     1 | loss: 2.0456109MemoryTrain:  epoch  1, batch     2 | loss: 2.6394491MemoryTrain:  epoch  1, batch     3 | loss: 1.6846242MemoryTrain:  epoch  1, batch     4 | loss: 1.6663362MemoryTrain:  epoch  1, batch     5 | loss: 1.6776795MemoryTrain:  epoch  1, batch     6 | loss: 1.6410520MemoryTrain:  epoch  1, batch     7 | loss: 1.5089164MemoryTrain:  epoch  1, batch     8 | loss: 1.5152982MemoryTrain:  epoch  1, batch     9 | loss: 1.5354322MemoryTrain:  epoch  2, batch     0 | loss: 1.7295567MemoryTrain:  epoch  2, batch     1 | loss: 1.4433928MemoryTrain:  epoch  2, batch     2 | loss: 1.4988487MemoryTrain:  epoch  2, batch     3 | loss: 1.4523759MemoryTrain:  epoch  2, batch     4 | loss: 1.5399995MemoryTrain:  epoch  2, batch     5 | loss: 1.5927806MemoryTrain:  epoch  2, batch     6 | loss: 1.4657891MemoryTrain:  epoch  2, batch     7 | loss: 1.8846771MemoryTrain:  epoch  2, batch     8 | loss: 1.6333020MemoryTrain:  epoch  2, batch     9 | loss: 1.2724104MemoryTrain:  epoch  3, batch     0 | loss: 1.5201387MemoryTrain:  epoch  3, batch     1 | loss: 1.4965961MemoryTrain:  epoch  3, batch     2 | loss: 1.5094433MemoryTrain:  epoch  3, batch     3 | loss: 1.6937428MemoryTrain:  epoch  3, batch     4 | loss: 1.6135217MemoryTrain:  epoch  3, batch     5 | loss: 1.2480352MemoryTrain:  epoch  3, batch     6 | loss: 1.2506583MemoryTrain:  epoch  3, batch     7 | loss: 1.2284794MemoryTrain:  epoch  3, batch     8 | loss: 1.5973182MemoryTrain:  epoch  3, batch     9 | loss: 1.2666037MemoryTrain:  epoch  4, batch     0 | loss: 1.4804823MemoryTrain:  epoch  4, batch     1 | loss: 1.2169909MemoryTrain:  epoch  4, batch     2 | loss: 1.2561433MemoryTrain:  epoch  4, batch     3 | loss: 1.5481668MemoryTrain:  epoch  4, batch     4 | loss: 1.3095102MemoryTrain:  epoch  4, batch     5 | loss: 1.4842887MemoryTrain:  epoch  4, batch     6 | loss: 1.4033885MemoryTrain:  epoch  4, batch     7 | loss: 1.3654704MemoryTrain:  epoch  4, batch     8 | loss: 1.4061949MemoryTrain:  epoch  4, batch     9 | loss: 1.2386546MemoryTrain:  epoch  5, batch     0 | loss: 1.4925601MemoryTrain:  epoch  5, batch     1 | loss: 1.4704527MemoryTrain:  epoch  5, batch     2 | loss: 1.2440195MemoryTrain:  epoch  5, batch     3 | loss: 1.4267317MemoryTrain:  epoch  5, batch     4 | loss: 1.3618289MemoryTrain:  epoch  5, batch     5 | loss: 1.3286431MemoryTrain:  epoch  5, batch     6 | loss: 1.3591590MemoryTrain:  epoch  5, batch     7 | loss: 1.3255771MemoryTrain:  epoch  5, batch     8 | loss: 1.2251155MemoryTrain:  epoch  5, batch     9 | loss: 1.4232992MemoryTrain:  epoch  6, batch     0 | loss: 1.2567472MemoryTrain:  epoch  6, batch     1 | loss: 1.2520278MemoryTrain:  epoch  6, batch     2 | loss: 1.4061533MemoryTrain:  epoch  6, batch     3 | loss: 1.2986300MemoryTrain:  epoch  6, batch     4 | loss: 1.3163643MemoryTrain:  epoch  6, batch     5 | loss: 1.3008754MemoryTrain:  epoch  6, batch     6 | loss: 1.2918594MemoryTrain:  epoch  6, batch     7 | loss: 1.2315068MemoryTrain:  epoch  6, batch     8 | loss: 1.2989061MemoryTrain:  epoch  6, batch     9 | loss: 1.1888194MemoryTrain:  epoch  7, batch     0 | loss: 1.2973779MemoryTrain:  epoch  7, batch     1 | loss: 1.2819552MemoryTrain:  epoch  7, batch     2 | loss: 1.2750034MemoryTrain:  epoch  7, batch     3 | loss: 1.2230861MemoryTrain:  epoch  7, batch     4 | loss: 1.2200898MemoryTrain:  epoch  7, batch     5 | loss: 1.2112526MemoryTrain:  epoch  7, batch     6 | loss: 1.2259562MemoryTrain:  epoch  7, batch     7 | loss: 1.3234391MemoryTrain:  epoch  7, batch     8 | loss: 1.3359845MemoryTrain:  epoch  7, batch     9 | loss: 1.4681495MemoryTrain:  epoch  8, batch     0 | loss: 1.2331538MemoryTrain:  epoch  8, batch     1 | loss: 1.2880950MemoryTrain:  epoch  8, batch     2 | loss: 1.2391613MemoryTrain:  epoch  8, batch     3 | loss: 1.2801793MemoryTrain:  epoch  8, batch     4 | loss: 1.2702545MemoryTrain:  epoch  8, batch     5 | loss: 1.2576312MemoryTrain:  epoch  8, batch     6 | loss: 1.3189449MemoryTrain:  epoch  8, batch     7 | loss: 1.2219827MemoryTrain:  epoch  8, batch     8 | loss: 1.2401651MemoryTrain:  epoch  8, batch     9 | loss: 1.2562088MemoryTrain:  epoch  9, batch     0 | loss: 1.2296991MemoryTrain:  epoch  9, batch     1 | loss: 1.2134172MemoryTrain:  epoch  9, batch     2 | loss: 1.2599081MemoryTrain:  epoch  9, batch     3 | loss: 1.2738206MemoryTrain:  epoch  9, batch     4 | loss: 1.2461669MemoryTrain:  epoch  9, batch     5 | loss: 1.2831265MemoryTrain:  epoch  9, batch     6 | loss: 1.2483901MemoryTrain:  epoch  9, batch     7 | loss: 1.2352474MemoryTrain:  epoch  9, batch     8 | loss: 1.2144263MemoryTrain:  epoch  9, batch     9 | loss: 1.2017902
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 56.25%,  total acc: 83.80%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 82.33%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 82.08%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 81.85%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 81.64%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 80.33%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 78.82%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 77.53%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 77.80%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 77.72%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 77.97%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 78.20%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 78.49%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 78.55%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 78.94%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 79.12%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 79.34%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 79.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 80.02%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 80.41%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 80.78%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 81.13%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 81.48%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 81.81%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 82.13%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 82.33%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 82.52%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 83.09%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 83.37%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 82.84%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 72.19%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 72.73%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.37%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 80.51%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 80.71%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 80.73%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.58%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.05%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 82.34%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.62%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.74%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 82.85%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 82.95%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 82.92%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 82.88%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 82.58%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 82.29%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 82.02%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 81.74%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 81.73%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.96%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 81.71%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 81.14%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 81.36%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 81.47%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 81.57%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 81.97%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 81.96%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 82.24%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 82.23%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 82.40%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 82.58%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 82.56%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 83.30%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 83.51%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 83.56%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 83.53%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.58%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 83.06%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 82.31%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 81.73%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 81.57%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 81.09%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 80.71%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 80.64%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 80.42%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 80.51%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 80.51%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 80.60%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 80.53%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 80.68%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 80.48%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 80.28%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 80.15%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 80.03%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 79.64%   [EVAL] batch:   93 | acc: 25.00%,  total acc: 79.06%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 79.01%   [EVAL] batch:   95 | acc: 62.50%,  total acc: 78.84%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 78.67%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 78.51%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 78.47%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 78.50%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 78.47%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 78.37%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 78.34%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 78.27%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 78.42%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 78.21%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 77.78%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 77.35%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 77.16%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 76.75%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 76.44%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 77.05%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 77.44%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 77.76%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 77.84%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 77.97%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 78.10%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 78.28%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 78.30%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 78.22%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 78.05%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 78.08%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 78.05%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 78.10%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 78.08%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 78.15%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 78.10%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 78.24%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 78.03%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 77.56%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 77.23%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 76.91%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 76.58%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 76.35%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 76.09%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.21%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 76.53%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 76.96%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 76.70%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 76.32%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 76.06%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 75.93%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 75.56%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 75.40%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 75.36%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.47%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.59%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.70%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 75.78%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 75.96%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 75.99%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 75.95%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 75.94%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 76.01%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 76.08%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 76.11%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 76.14%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 76.17%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 76.16%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 76.12%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 76.08%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 76.18%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 76.17%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 76.20%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 76.23%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 76.29%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 76.45%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 76.34%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 76.30%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 76.32%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 76.22%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 76.11%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 76.00%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 75.93%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 76.03%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 76.24%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 76.27%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 76.45%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 76.35%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 76.21%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 76.14%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 75.98%   [EVAL] batch:  198 | acc: 43.75%,  total acc: 75.82%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 75.72%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 75.75%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 75.80%   [EVAL] batch:  202 | acc: 100.00%,  total acc: 75.92%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 75.92%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 75.95%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 75.94%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 76.00%   [EVAL] batch:  207 | acc: 62.50%,  total acc: 75.93%   [EVAL] batch:  208 | acc: 93.75%,  total acc: 76.02%   [EVAL] batch:  209 | acc: 68.75%,  total acc: 75.98%   [EVAL] batch:  210 | acc: 68.75%,  total acc: 75.95%   [EVAL] batch:  211 | acc: 68.75%,  total acc: 75.91%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 75.97%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 76.60%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 76.51%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 76.53%   [EVAL] batch:  221 | acc: 56.25%,  total acc: 76.44%   [EVAL] batch:  222 | acc: 56.25%,  total acc: 76.35%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 76.34%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 76.22%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 76.43%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 76.54%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 76.74%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 76.81%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 76.91%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 77.11%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 77.49%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 77.56%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 77.63%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 77.79%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 77.95%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 78.04%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 78.07%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 78.16%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 78.25%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 78.34%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 78.40%   [EVAL] batch:  250 | acc: 81.25%,  total acc: 78.41%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 78.45%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 78.51%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 78.52%   [EVAL] batch:  254 | acc: 87.50%,  total acc: 78.55%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 78.64%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 78.65%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 78.66%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 78.64%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 78.61%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 78.51%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 78.56%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 78.50%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 78.51%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 78.48%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 78.49%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 78.54%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 78.62%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 78.66%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 78.74%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 78.81%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 78.89%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 78.95%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 79.01%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 78.93%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 78.84%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 78.81%   [EVAL] batch:  279 | acc: 75.00%,  total acc: 78.79%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 78.78%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 78.77%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:  283 | acc: 43.75%,  total acc: 78.63%   [EVAL] batch:  284 | acc: 62.50%,  total acc: 78.57%   [EVAL] batch:  285 | acc: 43.75%,  total acc: 78.45%   [EVAL] batch:  286 | acc: 31.25%,  total acc: 78.29%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 78.32%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 78.34%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 78.37%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 78.42%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 78.41%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 78.42%   [EVAL] batch:  294 | acc: 87.50%,  total acc: 78.45%   [EVAL] batch:  295 | acc: 87.50%,  total acc: 78.48%   [EVAL] batch:  296 | acc: 87.50%,  total acc: 78.51%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 78.52%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 78.55%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 78.60%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 78.82%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 78.89%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 78.95%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 79.14%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 79.19%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 79.25%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 79.32%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 79.29%   
cur_acc:  ['0.9474', '0.8026', '0.7847', '0.8750', '0.8284']
his_acc:  ['0.9474', '0.8695', '0.8411', '0.8263', '0.7929']
CurrentTrain: epoch  0, batch     0 | loss: 6.7684369CurrentTrain: epoch  0, batch     1 | loss: 5.5840750CurrentTrain: epoch  0, batch     2 | loss: 5.3846674CurrentTrain: epoch  0, batch     3 | loss: 7.0575905CurrentTrain: epoch  1, batch     0 | loss: 5.5356078CurrentTrain: epoch  1, batch     1 | loss: 4.3084717CurrentTrain: epoch  1, batch     2 | loss: 3.8897159CurrentTrain: epoch  1, batch     3 | loss: 5.9146042CurrentTrain: epoch  2, batch     0 | loss: 4.2377338CurrentTrain: epoch  2, batch     1 | loss: 4.2214975CurrentTrain: epoch  2, batch     2 | loss: 3.9964731CurrentTrain: epoch  2, batch     3 | loss: 2.6807194CurrentTrain: epoch  3, batch     0 | loss: 4.4077053CurrentTrain: epoch  3, batch     1 | loss: 3.2261438CurrentTrain: epoch  3, batch     2 | loss: 3.3869202CurrentTrain: epoch  3, batch     3 | loss: 5.9442348CurrentTrain: epoch  4, batch     0 | loss: 3.7860906CurrentTrain: epoch  4, batch     1 | loss: 3.3814087CurrentTrain: epoch  4, batch     2 | loss: 3.1609185CurrentTrain: epoch  4, batch     3 | loss: 3.6966860CurrentTrain: epoch  5, batch     0 | loss: 3.1736088CurrentTrain: epoch  5, batch     1 | loss: 2.9680719CurrentTrain: epoch  5, batch     2 | loss: 3.0020919CurrentTrain: epoch  5, batch     3 | loss: 2.2979686CurrentTrain: epoch  6, batch     0 | loss: 3.1122208CurrentTrain: epoch  6, batch     1 | loss: 2.3869660CurrentTrain: epoch  6, batch     2 | loss: 2.8530889CurrentTrain: epoch  6, batch     3 | loss: 3.2058692CurrentTrain: epoch  7, batch     0 | loss: 2.3894870CurrentTrain: epoch  7, batch     1 | loss: 2.3703184CurrentTrain: epoch  7, batch     2 | loss: 2.6437006CurrentTrain: epoch  7, batch     3 | loss: 3.3991966CurrentTrain: epoch  8, batch     0 | loss: 2.3232818CurrentTrain: epoch  8, batch     1 | loss: 2.7213283CurrentTrain: epoch  8, batch     2 | loss: 2.0643773CurrentTrain: epoch  8, batch     3 | loss: 3.4582982CurrentTrain: epoch  9, batch     0 | loss: 2.5807462CurrentTrain: epoch  9, batch     1 | loss: 2.4898000CurrentTrain: epoch  9, batch     2 | loss: 2.0240343CurrentTrain: epoch  9, batch     3 | loss: 2.0307198
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: the nikon d5500 features other improvements over its predecessor nikon d5300 .
Head Entity: nikon d5500
Tail Entity: nikon d5300
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: in the series of popular smartphone models, the iphone 12 follows the iphone 11, which was a major success.  
Head Entity: iphone 12  
Tail Entity: iphone 11  

Relation: follows  
Context: the latest installment in the series, the harry potter and the deathly hallows, follows the previous book, harry potter and the half-blood prince.  
Head Entity: harry potter and the deathly hallows  
Tail Entity: harry potter and the half-blood prince  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: in 1992 , with grand ceremony , the orioles began their season in a brand new ballpark , oriole park at camden yards , and thus retiring memorial stadium in the major league baseball world .
Head Entity: memorial stadium
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: The 2020 Summer Olympics, originally scheduled to be held in Tokyo, were postponed due to the COVID-19 pandemic, but the athletes continued to train for their respective sports.  
Head Entity: athletes  
Tail Entity: Olympics  

Relation: sport  
Context: After years of dedication and hard work, she finally qualified for the national swimming championships, showcasing her talent in the sport.  
Head Entity: national swimming championships  
Tail Entity: swimming  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: he married his first cousin , elisabeth of bavaria - landshut ( 1478–1504 ) in 1499 , daughter of duke george the rich , duke of bavaria - landshut .
Head Entity: elisabeth of bavaria - landshut
Tail Entity: duke george the rich , duke of bavaria - landshut
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in 1980, he was honored with the title of "father of the nation" for his contributions to the independence movement, and he often spoke about his admiration for his father, who was a prominent leader in the struggle.  
Head Entity: he  
Tail Entity: his father  

Relation: father  
Context: during the family reunion, she shared stories about her father, a renowned scientist, and how he inspired her to pursue a career in research, following in the footsteps of her father.  
Head Entity: her father  
Tail Entity: a renowned scientist  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: he also made a special appearance in kbs ' romantic comedy drama " fight for my way " .
Head Entity: fight for my way
Tail Entity: kbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series "breaking bad" was first aired on amc, gaining a massive following.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: the hit show "stranger things" premiered on netflix and quickly became a cultural phenomenon.  
Head Entity: stranger things  
Tail Entity: netflix  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" received critical acclaim and was originally produced in Korean, showcasing the talents of its director Bong Joon-ho.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Doraemon" has been dubbed in multiple languages, but it was originally created in Japanese, captivating audiences worldwide.  
Head Entity: Doraemon  
Tail Entity: Japanese  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball during the 2020 season.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: other teammates included dominique leray and élisabeth riffiod , whose son boris diaw currently plays in the nba for the spurs ( 2014 ) .
Head Entity: boris diaw
Tail Entity: élisabeth riffiod
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in a recent interview, actress jennifer garner spoke fondly of her children, including violet affleck, whom she describes as a wonderful daughter.  
Head Entity: violet affleck  
Tail Entity: jennifer garner  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much emma loved spending time with her mother, sarah, who always supported her dreams.  
Head Entity: emma  
Tail Entity: sarah  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres, showcasing his talent on the cello.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: ploegsteert is a village in belgium located in the municipality of comines - warneton in the hainaut province and is the most westerly settlement of the walloon region .
Head Entity: hainaut
Tail Entity: belgium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the city of prague is the capital of the czech republic and is known for its beautiful architecture and rich history.  
Head Entity: prague  
Tail Entity: czech republic  

Relation: country  
Context: the great barrier reef is located off the coast of australia and is the largest coral reef system in the world.  
Head Entity: great barrier reef  
Tail Entity: australia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Mixup data size:  438
MixupTrain:  epoch  0, batch     0 | loss: 2.3070567MixupTrain:  epoch  0, batch     1 | loss: 2.4417711MixupTrain:  epoch  0, batch     2 | loss: 1.7828870MixupTrain:  epoch  0, batch     3 | loss: 2.0823261MixupTrain:  epoch  0, batch     4 | loss: 2.1095034MixupTrain:  epoch  0, batch     5 | loss: 2.1819973MixupTrain:  epoch  0, batch     6 | loss: 1.9206096MixupTrain:  epoch  0, batch     7 | loss: 1.8313769MixupTrain:  epoch  0, batch     8 | loss: 2.2335003MixupTrain:  epoch  0, batch     9 | loss: 1.8497454MixupTrain:  epoch  0, batch    10 | loss: 2.1014275MixupTrain:  epoch  0, batch    11 | loss: 2.1113442MixupTrain:  epoch  0, batch    12 | loss: 1.8012693MixupTrain:  epoch  0, batch    13 | loss: 2.6066999MixupTrain:  epoch  0, batch    14 | loss: 2.0092997MixupTrain:  epoch  0, batch    15 | loss: 2.1432217MixupTrain:  epoch  0, batch    16 | loss: 2.4654168MixupTrain:  epoch  0, batch    17 | loss: 1.6717917MixupTrain:  epoch  0, batch    18 | loss: 2.0267733MixupTrain:  epoch  0, batch    19 | loss: 1.8767244MixupTrain:  epoch  0, batch    20 | loss: 2.0059891MixupTrain:  epoch  0, batch    21 | loss: 2.3279143MixupTrain:  epoch  0, batch    22 | loss: 1.9744976MixupTrain:  epoch  0, batch    23 | loss: 2.4123148MixupTrain:  epoch  0, batch    24 | loss: 1.8483470MixupTrain:  epoch  0, batch    25 | loss: 1.9256433MixupTrain:  epoch  0, batch    26 | loss: 1.8955791MixupTrain:  epoch  0, batch    27 | loss: 1.8909771
MemoryTrain:  epoch  0, batch     0 | loss: 1.7236924MemoryTrain:  epoch  0, batch     1 | loss: 2.5033832MemoryTrain:  epoch  0, batch     2 | loss: 1.4634581MemoryTrain:  epoch  0, batch     3 | loss: 1.9576043MemoryTrain:  epoch  0, batch     4 | loss: 2.6597381MemoryTrain:  epoch  0, batch     5 | loss: 2.4628754MemoryTrain:  epoch  0, batch     6 | loss: 1.7376077MemoryTrain:  epoch  0, batch     7 | loss: 2.0987644MemoryTrain:  epoch  0, batch     8 | loss: 2.4254050MemoryTrain:  epoch  0, batch     9 | loss: 2.4749830MemoryTrain:  epoch  0, batch    10 | loss: 1.9670794MemoryTrain:  epoch  0, batch    11 | loss: 2.9431965MemoryTrain:  epoch  1, batch     0 | loss: 1.7173948MemoryTrain:  epoch  1, batch     1 | loss: 2.4835730MemoryTrain:  epoch  1, batch     2 | loss: 1.8717284MemoryTrain:  epoch  1, batch     3 | loss: 2.0546181MemoryTrain:  epoch  1, batch     4 | loss: 1.7223127MemoryTrain:  epoch  1, batch     5 | loss: 2.0468013MemoryTrain:  epoch  1, batch     6 | loss: 1.9407272MemoryTrain:  epoch  1, batch     7 | loss: 1.7509637MemoryTrain:  epoch  1, batch     8 | loss: 2.1563559MemoryTrain:  epoch  1, batch     9 | loss: 1.6207764MemoryTrain:  epoch  1, batch    10 | loss: 1.7602068MemoryTrain:  epoch  1, batch    11 | loss: 1.2246820MemoryTrain:  epoch  2, batch     0 | loss: 1.3442614MemoryTrain:  epoch  2, batch     1 | loss: 2.1040144MemoryTrain:  epoch  2, batch     2 | loss: 1.6677499MemoryTrain:  epoch  2, batch     3 | loss: 1.7435946MemoryTrain:  epoch  2, batch     4 | loss: 1.6461544MemoryTrain:  epoch  2, batch     5 | loss: 1.8939075MemoryTrain:  epoch  2, batch     6 | loss: 1.5269368MemoryTrain:  epoch  2, batch     7 | loss: 1.9609393MemoryTrain:  epoch  2, batch     8 | loss: 1.7758727MemoryTrain:  epoch  2, batch     9 | loss: 1.5795207MemoryTrain:  epoch  2, batch    10 | loss: 1.4901273MemoryTrain:  epoch  2, batch    11 | loss: 1.7275219MemoryTrain:  epoch  3, batch     0 | loss: 1.9165106MemoryTrain:  epoch  3, batch     1 | loss: 1.4110975MemoryTrain:  epoch  3, batch     2 | loss: 1.7035191MemoryTrain:  epoch  3, batch     3 | loss: 1.3506968MemoryTrain:  epoch  3, batch     4 | loss: 1.4312924MemoryTrain:  epoch  3, batch     5 | loss: 1.7819479MemoryTrain:  epoch  3, batch     6 | loss: 1.1732143MemoryTrain:  epoch  3, batch     7 | loss: 1.2875493MemoryTrain:  epoch  3, batch     8 | loss: 1.4244900MemoryTrain:  epoch  3, batch     9 | loss: 1.5052710MemoryTrain:  epoch  3, batch    10 | loss: 1.7142644MemoryTrain:  epoch  3, batch    11 | loss: 1.3356217MemoryTrain:  epoch  4, batch     0 | loss: 1.6213428MemoryTrain:  epoch  4, batch     1 | loss: 1.8628932MemoryTrain:  epoch  4, batch     2 | loss: 1.5172470MemoryTrain:  epoch  4, batch     3 | loss: 1.2982585MemoryTrain:  epoch  4, batch     4 | loss: 1.5747256MemoryTrain:  epoch  4, batch     5 | loss: 1.2869914MemoryTrain:  epoch  4, batch     6 | loss: 1.4524763MemoryTrain:  epoch  4, batch     7 | loss: 1.3959683MemoryTrain:  epoch  4, batch     8 | loss: 1.4784496MemoryTrain:  epoch  4, batch     9 | loss: 1.4418725MemoryTrain:  epoch  4, batch    10 | loss: 1.3854711MemoryTrain:  epoch  4, batch    11 | loss: 1.9862940MemoryTrain:  epoch  5, batch     0 | loss: 1.3244104MemoryTrain:  epoch  5, batch     1 | loss: 1.3704348MemoryTrain:  epoch  5, batch     2 | loss: 1.3847959MemoryTrain:  epoch  5, batch     3 | loss: 1.3101201MemoryTrain:  epoch  5, batch     4 | loss: 1.3039949MemoryTrain:  epoch  5, batch     5 | loss: 1.5584632MemoryTrain:  epoch  5, batch     6 | loss: 1.5868152MemoryTrain:  epoch  5, batch     7 | loss: 1.3922495MemoryTrain:  epoch  5, batch     8 | loss: 1.5406327MemoryTrain:  epoch  5, batch     9 | loss: 1.3580534MemoryTrain:  epoch  5, batch    10 | loss: 1.3713981MemoryTrain:  epoch  5, batch    11 | loss: 1.7395236MemoryTrain:  epoch  6, batch     0 | loss: 1.4104749MemoryTrain:  epoch  6, batch     1 | loss: 1.3590734MemoryTrain:  epoch  6, batch     2 | loss: 1.3525664MemoryTrain:  epoch  6, batch     3 | loss: 1.3145907MemoryTrain:  epoch  6, batch     4 | loss: 1.4428924MemoryTrain:  epoch  6, batch     5 | loss: 1.2487150MemoryTrain:  epoch  6, batch     6 | loss: 1.5173568MemoryTrain:  epoch  6, batch     7 | loss: 1.4045368MemoryTrain:  epoch  6, batch     8 | loss: 1.2534132MemoryTrain:  epoch  6, batch     9 | loss: 1.3146442MemoryTrain:  epoch  6, batch    10 | loss: 1.3080552MemoryTrain:  epoch  6, batch    11 | loss: 1.3653060MemoryTrain:  epoch  7, batch     0 | loss: 1.2864380MemoryTrain:  epoch  7, batch     1 | loss: 1.2612641MemoryTrain:  epoch  7, batch     2 | loss: 1.2970006MemoryTrain:  epoch  7, batch     3 | loss: 1.4039216MemoryTrain:  epoch  7, batch     4 | loss: 1.2792037MemoryTrain:  epoch  7, batch     5 | loss: 1.2698693MemoryTrain:  epoch  7, batch     6 | loss: 1.3052994MemoryTrain:  epoch  7, batch     7 | loss: 1.3789346MemoryTrain:  epoch  7, batch     8 | loss: 1.4030764MemoryTrain:  epoch  7, batch     9 | loss: 1.2278944MemoryTrain:  epoch  7, batch    10 | loss: 1.3121978MemoryTrain:  epoch  7, batch    11 | loss: 1.2147132MemoryTrain:  epoch  8, batch     0 | loss: 1.3273869MemoryTrain:  epoch  8, batch     1 | loss: 1.2863863MemoryTrain:  epoch  8, batch     2 | loss: 1.2391250MemoryTrain:  epoch  8, batch     3 | loss: 1.2553790MemoryTrain:  epoch  8, batch     4 | loss: 1.2211102MemoryTrain:  epoch  8, batch     5 | loss: 1.3460336MemoryTrain:  epoch  8, batch     6 | loss: 1.1926441MemoryTrain:  epoch  8, batch     7 | loss: 1.3535275MemoryTrain:  epoch  8, batch     8 | loss: 1.4354410MemoryTrain:  epoch  8, batch     9 | loss: 1.2824348MemoryTrain:  epoch  8, batch    10 | loss: 1.3244228MemoryTrain:  epoch  8, batch    11 | loss: 1.1655688MemoryTrain:  epoch  9, batch     0 | loss: 1.2622991MemoryTrain:  epoch  9, batch     1 | loss: 1.2578142MemoryTrain:  epoch  9, batch     2 | loss: 1.2459533MemoryTrain:  epoch  9, batch     3 | loss: 1.2672468MemoryTrain:  epoch  9, batch     4 | loss: 1.2417657MemoryTrain:  epoch  9, batch     5 | loss: 1.2665570MemoryTrain:  epoch  9, batch     6 | loss: 1.3648323MemoryTrain:  epoch  9, batch     7 | loss: 1.2595415MemoryTrain:  epoch  9, batch     8 | loss: 1.2403131MemoryTrain:  epoch  9, batch     9 | loss: 1.2202725MemoryTrain:  epoch  9, batch    10 | loss: 1.2433462MemoryTrain:  epoch  9, batch    11 | loss: 1.2754304
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 42.19%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 66.02%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 72.44%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.04%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.66%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 81.62%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.96%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 82.40%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 81.09%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 80.62%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 79.57%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 78.57%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 77.18%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 76.85%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 77.36%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 77.58%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.06%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 78.32%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 78.62%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 78.43%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 77.83%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 77.78%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 77.84%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 77.12%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 76.54%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 76.62%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 76.17%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 76.15%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 76.54%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 76.61%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 75.99%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 61.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 72.62%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 72.44%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 72.28%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 73.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.72%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 79.78%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 79.86%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 80.24%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 80.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.93%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.71%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 81.85%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 81.69%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 81.68%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 81.67%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 81.52%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 80.86%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 80.87%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 80.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 80.53%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 80.66%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 80.32%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 79.89%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 79.91%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 79.71%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 78.88%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 78.60%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 78.54%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 78.48%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 77.78%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 77.05%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 76.54%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 75.95%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 75.28%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 74.82%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 74.64%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 75.35%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 75.51%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 75.68%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 75.92%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 75.58%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 74.52%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 74.29%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 74.06%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 73.77%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 73.78%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 73.72%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 73.81%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 73.97%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 74.28%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 74.50%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 74.44%   [EVAL] batch:   89 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 74.31%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 74.18%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 73.86%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 73.40%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 73.29%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 72.98%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 72.87%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 72.58%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 72.54%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 72.31%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 72.34%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 72.37%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 72.51%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 72.54%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 72.56%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 72.76%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 72.66%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 72.34%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 71.96%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 71.82%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 71.57%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 71.48%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 71.40%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 72.79%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 72.86%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 72.73%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 72.85%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 72.82%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 72.88%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 72.85%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 72.77%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 72.54%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 72.61%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 72.53%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 72.57%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 72.63%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 72.79%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 72.85%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 72.87%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 73.13%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 72.96%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 72.57%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 72.32%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 72.03%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 71.83%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 71.63%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 71.40%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 71.55%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 71.94%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 72.46%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 72.27%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 71.92%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 71.69%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 71.59%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 71.25%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 71.03%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 71.02%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.31%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.45%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 71.55%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 71.74%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 71.68%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 71.63%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 71.54%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 71.56%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 71.61%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 71.56%   [EVAL] batch:  169 | acc: 6.25%,  total acc: 71.18%   [EVAL] batch:  170 | acc: 12.50%,  total acc: 70.83%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 70.49%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 70.20%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 69.97%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 69.64%   [EVAL] batch:  175 | acc: 81.25%,  total acc: 69.71%   [EVAL] batch:  176 | acc: 87.50%,  total acc: 69.81%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 69.87%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 69.97%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 70.20%   [EVAL] batch:  181 | acc: 68.75%,  total acc: 70.19%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 70.29%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 70.63%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 70.64%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 70.77%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 70.89%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 71.13%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 71.36%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 71.28%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 71.17%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 71.03%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 70.93%   [EVAL] batch:  198 | acc: 43.75%,  total acc: 70.79%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 70.72%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 70.68%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 70.76%   [EVAL] batch:  202 | acc: 87.50%,  total acc: 70.84%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 70.88%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 70.90%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 70.92%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 70.82%   [EVAL] batch:  208 | acc: 75.00%,  total acc: 70.84%   [EVAL] batch:  209 | acc: 56.25%,  total acc: 70.77%   [EVAL] batch:  210 | acc: 56.25%,  total acc: 70.70%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 70.61%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 70.60%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.01%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.27%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 71.38%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 71.31%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 71.35%   [EVAL] batch:  221 | acc: 50.00%,  total acc: 71.26%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 71.22%   [EVAL] batch:  223 | acc: 68.75%,  total acc: 71.21%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 71.06%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 72.26%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 72.81%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 73.01%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 73.37%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:  250 | acc: 81.25%,  total acc: 73.78%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 73.86%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 73.91%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 73.94%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 73.97%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 74.03%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 74.08%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 74.06%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 74.04%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 73.92%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 73.90%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 73.98%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 73.93%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 73.94%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 73.92%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 73.92%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 73.97%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 74.12%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 74.59%   [EVAL] batch:  276 | acc: 62.50%,  total acc: 74.55%   [EVAL] batch:  277 | acc: 75.00%,  total acc: 74.55%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 74.53%   [EVAL] batch:  279 | acc: 75.00%,  total acc: 74.53%   [EVAL] batch:  280 | acc: 87.50%,  total acc: 74.58%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 74.58%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 74.54%   [EVAL] batch:  283 | acc: 31.25%,  total acc: 74.38%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 74.28%   [EVAL] batch:  285 | acc: 31.25%,  total acc: 74.13%   [EVAL] batch:  286 | acc: 12.50%,  total acc: 73.91%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 73.87%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 73.90%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 73.94%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 73.99%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 74.06%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 74.08%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 74.13%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 74.15%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 74.18%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 74.18%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 74.20%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 74.25%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 74.29%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 74.71%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 74.86%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 74.90%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 74.94%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 75.04%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 75.12%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 75.06%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  314 | acc: 50.00%,  total acc: 74.92%   [EVAL] batch:  315 | acc: 37.50%,  total acc: 74.80%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 74.68%   [EVAL] batch:  317 | acc: 50.00%,  total acc: 74.61%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 74.53%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 74.57%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 74.59%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 74.63%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 74.69%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 74.73%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 74.73%   [EVAL] batch:  325 | acc: 87.50%,  total acc: 74.77%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 74.73%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 74.71%   [EVAL] batch:  328 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 74.75%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 74.75%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 74.81%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 74.96%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 75.02%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 75.09%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 75.20%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 75.28%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 75.71%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 75.81%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 75.89%   [EVAL] batch:  351 | acc: 37.50%,  total acc: 75.78%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 75.74%   [EVAL] batch:  353 | acc: 31.25%,  total acc: 75.62%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 75.46%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 75.37%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 75.40%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 75.45%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 75.50%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 75.56%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 75.57%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 75.60%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 75.60%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 75.58%   [EVAL] batch:  365 | acc: 62.50%,  total acc: 75.55%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 75.54%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 75.51%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 75.41%   [EVAL] batch:  369 | acc: 68.75%,  total acc: 75.39%   [EVAL] batch:  370 | acc: 68.75%,  total acc: 75.37%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 75.30%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 75.40%   
cur_acc:  ['0.9474', '0.8026', '0.7847', '0.8750', '0.8284', '0.7599']
his_acc:  ['0.9474', '0.8695', '0.8411', '0.8263', '0.7929', '0.7540']
CurrentTrain: epoch  0, batch     0 | loss: 6.7397127CurrentTrain: epoch  0, batch     1 | loss: 6.3251882CurrentTrain: epoch  0, batch     2 | loss: 7.1554918CurrentTrain: epoch  0, batch     3 | loss: 5.8171329CurrentTrain: epoch  1, batch     0 | loss: 6.3005919CurrentTrain: epoch  1, batch     1 | loss: 5.8266973CurrentTrain: epoch  1, batch     2 | loss: 5.1503301CurrentTrain: epoch  1, batch     3 | loss: 6.5257869CurrentTrain: epoch  2, batch     0 | loss: 4.9138260CurrentTrain: epoch  2, batch     1 | loss: 5.3788133CurrentTrain: epoch  2, batch     2 | loss: 4.6446943CurrentTrain: epoch  2, batch     3 | loss: 3.6552441CurrentTrain: epoch  3, batch     0 | loss: 4.8214498CurrentTrain: epoch  3, batch     1 | loss: 4.3554816CurrentTrain: epoch  3, batch     2 | loss: 4.9648199CurrentTrain: epoch  3, batch     3 | loss: 3.1557503CurrentTrain: epoch  4, batch     0 | loss: 3.8442302CurrentTrain: epoch  4, batch     1 | loss: 4.0389185CurrentTrain: epoch  4, batch     2 | loss: 4.5454717CurrentTrain: epoch  4, batch     3 | loss: 6.4321270CurrentTrain: epoch  5, batch     0 | loss: 3.7911685CurrentTrain: epoch  5, batch     1 | loss: 4.2120056CurrentTrain: epoch  5, batch     2 | loss: 3.4842758CurrentTrain: epoch  5, batch     3 | loss: 3.7568982CurrentTrain: epoch  6, batch     0 | loss: 3.6301208CurrentTrain: epoch  6, batch     1 | loss: 3.4640951CurrentTrain: epoch  6, batch     2 | loss: 3.7562377CurrentTrain: epoch  6, batch     3 | loss: 2.5483048CurrentTrain: epoch  7, batch     0 | loss: 3.0761588CurrentTrain: epoch  7, batch     1 | loss: 3.4333792CurrentTrain: epoch  7, batch     2 | loss: 3.0555437CurrentTrain: epoch  7, batch     3 | loss: 2.8444676CurrentTrain: epoch  8, batch     0 | loss: 2.8136866CurrentTrain: epoch  8, batch     1 | loss: 3.0216794CurrentTrain: epoch  8, batch     2 | loss: 2.9406881CurrentTrain: epoch  8, batch     3 | loss: 1.9521198CurrentTrain: epoch  9, batch     0 | loss: 3.0457625CurrentTrain: epoch  9, batch     1 | loss: 2.3662171CurrentTrain: epoch  9, batch     2 | loss: 2.8366075CurrentTrain: epoch  9, batch     3 | loss: 2.6130936
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and organizations come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: elon musk, the CEO of spacex, played a crucial role in the development of the commercial space industry and participated in the first crewed mission to the international space station in 2020.  
Head Entity: elon musk  
Tail Entity: first crewed mission to the international space station  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: the festival opened with " whiplash " directed by damien chazelle and closed with musical drama " rudderless " directed by william h. macy .
Head Entity: whiplash
Tail Entity: damien chazelle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the critically acclaimed series "breaking bad" was brought to life by director vince gilligan, while the spin-off "better call saul" was directed by peter gould.  
Head Entity: breaking bad  
Tail Entity: vince gilligan  

Relation: director  
Context: the animated feature "spider-man: into the spider-verse" was directed by bob persichetti, peter ramsey, and rodney rothman, showcasing a unique visual style.  
Head Entity: spider-man: into the spider-verse  
Tail Entity: bob persichetti  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game developer, Bungie, announced that it would be self-publishing its games after separating from Activision, which had owned the company for several years.  
Head Entity: Bungie  
Tail Entity: Activision  

Relation: owned by  
Context: The luxury fashion brand Gucci, known for its high-end products, is a subsidiary of the Kering Group, which owns several other prestigious labels.  
Head Entity: Gucci  
Tail Entity: Kering Group  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts the local art gallery and is frequently visited by the community.  
Head Entity: cultural center  
Tail Entity: local art gallery  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant co-working space, attracting startups and freelancers from the area.  
Head Entity: co-working space  
Tail Entity: startups  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: he also commissioned renowned architect richard neutra to design the iconic desert kaufmann house ( 1946 ) in palm springs , california .
Head Entity: kaufmann house
Tail Entity: richard neutra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the innovative design of the new cultural center was created by the famous architect zaha hadid, known for her futuristic structures.  
Head Entity: cultural center  
Tail Entity: zaha hadid  

Relation: architect  
Context: after years of planning, the city finally approved the blueprints submitted by local architect frank lloyd wright for the new community library.  
Head Entity: community library  
Tail Entity: frank lloyd wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a fresh start.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida in his teenage years.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the small town of Gettysburg, Pennsylvania, which is now a popular tourist destination.  
Head Entity: historic battle  
Tail Entity: Gettysburg, Pennsylvania  
Mixup data size:  498
MixupTrain:  epoch  0, batch     0 | loss: 2.1164670MixupTrain:  epoch  0, batch     1 | loss: 2.2089150MixupTrain:  epoch  0, batch     2 | loss: 2.3193960MixupTrain:  epoch  0, batch     3 | loss: 2.5956794MixupTrain:  epoch  0, batch     4 | loss: 2.0811938MixupTrain:  epoch  0, batch     5 | loss: 2.5403865MixupTrain:  epoch  0, batch     6 | loss: 1.8195628MixupTrain:  epoch  0, batch     7 | loss: 2.2325865MixupTrain:  epoch  0, batch     8 | loss: 2.0090232MixupTrain:  epoch  0, batch     9 | loss: 1.8901681MixupTrain:  epoch  0, batch    10 | loss: 2.1823052MixupTrain:  epoch  0, batch    11 | loss: 1.9371336MixupTrain:  epoch  0, batch    12 | loss: 2.3349639MixupTrain:  epoch  0, batch    13 | loss: 1.8301053MixupTrain:  epoch  0, batch    14 | loss: 1.7285347MixupTrain:  epoch  0, batch    15 | loss: 2.0594401MixupTrain:  epoch  0, batch    16 | loss: 1.5176865MixupTrain:  epoch  0, batch    17 | loss: 1.8277886MixupTrain:  epoch  0, batch    18 | loss: 2.2100654MixupTrain:  epoch  0, batch    19 | loss: 1.8644119MixupTrain:  epoch  0, batch    20 | loss: 1.6054047MixupTrain:  epoch  0, batch    21 | loss: 2.1307827MixupTrain:  epoch  0, batch    22 | loss: 1.9984866MixupTrain:  epoch  0, batch    23 | loss: 2.0209288MixupTrain:  epoch  0, batch    24 | loss: 1.8287199MixupTrain:  epoch  0, batch    25 | loss: 1.9482258MixupTrain:  epoch  0, batch    26 | loss: 1.7280989MixupTrain:  epoch  0, batch    27 | loss: 1.5760451MixupTrain:  epoch  0, batch    28 | loss: 1.8491739MixupTrain:  epoch  0, batch    29 | loss: 1.5419323MixupTrain:  epoch  0, batch    30 | loss: 1.8643023MixupTrain:  epoch  0, batch    31 | loss: 1.8859726
MemoryTrain:  epoch  0, batch     0 | loss: 2.2202916MemoryTrain:  epoch  0, batch     1 | loss: 2.2004514MemoryTrain:  epoch  0, batch     2 | loss: 2.3538640MemoryTrain:  epoch  0, batch     3 | loss: 2.5648868MemoryTrain:  epoch  0, batch     4 | loss: 2.3043890MemoryTrain:  epoch  0, batch     5 | loss: 2.0573194MemoryTrain:  epoch  0, batch     6 | loss: 1.9413627MemoryTrain:  epoch  0, batch     7 | loss: 1.8359693MemoryTrain:  epoch  0, batch     8 | loss: 1.9541657MemoryTrain:  epoch  0, batch     9 | loss: 2.7440743MemoryTrain:  epoch  0, batch    10 | loss: 2.3526375MemoryTrain:  epoch  0, batch    11 | loss: 2.6879518MemoryTrain:  epoch  0, batch    12 | loss: 2.7222404MemoryTrain:  epoch  0, batch    13 | loss: 1.8380791MemoryTrain:  epoch  1, batch     0 | loss: 2.0913858MemoryTrain:  epoch  1, batch     1 | loss: 2.2806082MemoryTrain:  epoch  1, batch     2 | loss: 1.9588840MemoryTrain:  epoch  1, batch     3 | loss: 1.6762528MemoryTrain:  epoch  1, batch     4 | loss: 1.3770884MemoryTrain:  epoch  1, batch     5 | loss: 1.7835751MemoryTrain:  epoch  1, batch     6 | loss: 2.5547478MemoryTrain:  epoch  1, batch     7 | loss: 2.1411488MemoryTrain:  epoch  1, batch     8 | loss: 1.5331711MemoryTrain:  epoch  1, batch     9 | loss: 1.8167014MemoryTrain:  epoch  1, batch    10 | loss: 2.2170792MemoryTrain:  epoch  1, batch    11 | loss: 1.4692752MemoryTrain:  epoch  1, batch    12 | loss: 2.7209506MemoryTrain:  epoch  1, batch    13 | loss: 1.8299818MemoryTrain:  epoch  2, batch     0 | loss: 1.5423852MemoryTrain:  epoch  2, batch     1 | loss: 1.9530888MemoryTrain:  epoch  2, batch     2 | loss: 1.4577737MemoryTrain:  epoch  2, batch     3 | loss: 1.3687154MemoryTrain:  epoch  2, batch     4 | loss: 1.8094060MemoryTrain:  epoch  2, batch     5 | loss: 2.2343302MemoryTrain:  epoch  2, batch     6 | loss: 2.0041442MemoryTrain:  epoch  2, batch     7 | loss: 1.9470847MemoryTrain:  epoch  2, batch     8 | loss: 1.5279447MemoryTrain:  epoch  2, batch     9 | loss: 1.6945937MemoryTrain:  epoch  2, batch    10 | loss: 1.6483321MemoryTrain:  epoch  2, batch    11 | loss: 1.4603235MemoryTrain:  epoch  2, batch    12 | loss: 1.9694301MemoryTrain:  epoch  2, batch    13 | loss: 5.4168539MemoryTrain:  epoch  3, batch     0 | loss: 1.5772862MemoryTrain:  epoch  3, batch     1 | loss: 2.0187302MemoryTrain:  epoch  3, batch     2 | loss: 1.6059387MemoryTrain:  epoch  3, batch     3 | loss: 1.5864629MemoryTrain:  epoch  3, batch     4 | loss: 2.3857744MemoryTrain:  epoch  3, batch     5 | loss: 1.6809552MemoryTrain:  epoch  3, batch     6 | loss: 1.3756349MemoryTrain:  epoch  3, batch     7 | loss: 1.7544100MemoryTrain:  epoch  3, batch     8 | loss: 1.3736928MemoryTrain:  epoch  3, batch     9 | loss: 1.2731677MemoryTrain:  epoch  3, batch    10 | loss: 1.5476036MemoryTrain:  epoch  3, batch    11 | loss: 1.7458750MemoryTrain:  epoch  3, batch    12 | loss: 1.7234595MemoryTrain:  epoch  3, batch    13 | loss: 1.3866818MemoryTrain:  epoch  4, batch     0 | loss: 2.2932141MemoryTrain:  epoch  4, batch     1 | loss: 1.4887903MemoryTrain:  epoch  4, batch     2 | loss: 1.8156893MemoryTrain:  epoch  4, batch     3 | loss: 1.2742230MemoryTrain:  epoch  4, batch     4 | loss: 1.5402279MemoryTrain:  epoch  4, batch     5 | loss: 1.3721918MemoryTrain:  epoch  4, batch     6 | loss: 1.3397607MemoryTrain:  epoch  4, batch     7 | loss: 1.3211104MemoryTrain:  epoch  4, batch     8 | loss: 1.4038191MemoryTrain:  epoch  4, batch     9 | loss: 1.3295035MemoryTrain:  epoch  4, batch    10 | loss: 1.5866618MemoryTrain:  epoch  4, batch    11 | loss: 1.8406934MemoryTrain:  epoch  4, batch    12 | loss: 1.3523090MemoryTrain:  epoch  4, batch    13 | loss: 1.1240296MemoryTrain:  epoch  5, batch     0 | loss: 1.7033043MemoryTrain:  epoch  5, batch     1 | loss: 1.4709005MemoryTrain:  epoch  5, batch     2 | loss: 1.4185613MemoryTrain:  epoch  5, batch     3 | loss: 1.4854370MemoryTrain:  epoch  5, batch     4 | loss: 1.2264423MemoryTrain:  epoch  5, batch     5 | loss: 1.3474079MemoryTrain:  epoch  5, batch     6 | loss: 1.8715881MemoryTrain:  epoch  5, batch     7 | loss: 1.5336356MemoryTrain:  epoch  5, batch     8 | loss: 1.4568913MemoryTrain:  epoch  5, batch     9 | loss: 1.4043906MemoryTrain:  epoch  5, batch    10 | loss: 1.6807717MemoryTrain:  epoch  5, batch    11 | loss: 1.2721539MemoryTrain:  epoch  5, batch    12 | loss: 1.2643498MemoryTrain:  epoch  5, batch    13 | loss: 1.7123897MemoryTrain:  epoch  6, batch     0 | loss: 1.3309407MemoryTrain:  epoch  6, batch     1 | loss: 1.4815974MemoryTrain:  epoch  6, batch     2 | loss: 1.4624518MemoryTrain:  epoch  6, batch     3 | loss: 1.6421671MemoryTrain:  epoch  6, batch     4 | loss: 1.3863187MemoryTrain:  epoch  6, batch     5 | loss: 1.5488726MemoryTrain:  epoch  6, batch     6 | loss: 1.3124471MemoryTrain:  epoch  6, batch     7 | loss: 1.3149493MemoryTrain:  epoch  6, batch     8 | loss: 1.2801404MemoryTrain:  epoch  6, batch     9 | loss: 1.3683400MemoryTrain:  epoch  6, batch    10 | loss: 1.2102767MemoryTrain:  epoch  6, batch    11 | loss: 1.7523506MemoryTrain:  epoch  6, batch    12 | loss: 1.3697497MemoryTrain:  epoch  6, batch    13 | loss: 1.1625296MemoryTrain:  epoch  7, batch     0 | loss: 1.5045438MemoryTrain:  epoch  7, batch     1 | loss: 1.4360356MemoryTrain:  epoch  7, batch     2 | loss: 1.3286680MemoryTrain:  epoch  7, batch     3 | loss: 1.2754918MemoryTrain:  epoch  7, batch     4 | loss: 1.3333356MemoryTrain:  epoch  7, batch     5 | loss: 1.2770749MemoryTrain:  epoch  7, batch     6 | loss: 1.4077199MemoryTrain:  epoch  7, batch     7 | loss: 1.6322777MemoryTrain:  epoch  7, batch     8 | loss: 1.3691312MemoryTrain:  epoch  7, batch     9 | loss: 1.2477436MemoryTrain:  epoch  7, batch    10 | loss: 1.4060960MemoryTrain:  epoch  7, batch    11 | loss: 1.3457414MemoryTrain:  epoch  7, batch    12 | loss: 1.5974686MemoryTrain:  epoch  7, batch    13 | loss: 1.5595250MemoryTrain:  epoch  8, batch     0 | loss: 1.3126376MemoryTrain:  epoch  8, batch     1 | loss: 1.2829815MemoryTrain:  epoch  8, batch     2 | loss: 1.5480008MemoryTrain:  epoch  8, batch     3 | loss: 1.3905280MemoryTrain:  epoch  8, batch     4 | loss: 1.2494410MemoryTrain:  epoch  8, batch     5 | loss: 1.4541985MemoryTrain:  epoch  8, batch     6 | loss: 1.2128791MemoryTrain:  epoch  8, batch     7 | loss: 1.4263341MemoryTrain:  epoch  8, batch     8 | loss: 1.3885502MemoryTrain:  epoch  8, batch     9 | loss: 1.2466913MemoryTrain:  epoch  8, batch    10 | loss: 1.3824441MemoryTrain:  epoch  8, batch    11 | loss: 1.2235138MemoryTrain:  epoch  8, batch    12 | loss: 1.3716773MemoryTrain:  epoch  8, batch    13 | loss: 1.8954875MemoryTrain:  epoch  9, batch     0 | loss: 1.3149562MemoryTrain:  epoch  9, batch     1 | loss: 1.4160283MemoryTrain:  epoch  9, batch     2 | loss: 1.2722123MemoryTrain:  epoch  9, batch     3 | loss: 1.2699170MemoryTrain:  epoch  9, batch     4 | loss: 1.3465800MemoryTrain:  epoch  9, batch     5 | loss: 1.2645859MemoryTrain:  epoch  9, batch     6 | loss: 1.3767953MemoryTrain:  epoch  9, batch     7 | loss: 1.3076921MemoryTrain:  epoch  9, batch     8 | loss: 1.2385888MemoryTrain:  epoch  9, batch     9 | loss: 1.2442951MemoryTrain:  epoch  9, batch    10 | loss: 1.2287345MemoryTrain:  epoch  9, batch    11 | loss: 1.3740518MemoryTrain:  epoch  9, batch    12 | loss: 1.3077998MemoryTrain:  epoch  9, batch    13 | loss: 1.1541007
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 25.89%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 33.59%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 40.28%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 45.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 53.65%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 57.92%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 58.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 59.93%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 59.06%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 58.33%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 58.52%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 58.15%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 57.29%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 57.50%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 56.01%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 54.40%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 53.35%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 51.94%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 50.62%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 49.19%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 49.61%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 50.95%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 52.39%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 53.75%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 54.86%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 55.74%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 56.91%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 57.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 58.75%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 59.60%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 59.97%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 60.90%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 61.65%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 61.11%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 60.05%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 59.31%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 58.59%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 57.91%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 57.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 57.48%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 58.17%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 58.73%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 59.14%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 59.77%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 60.27%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 60.31%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 60.13%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 59.85%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 59.90%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 59.73%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 60.18%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 59.72%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 68.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 72.62%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 72.44%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 72.55%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 73.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.94%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.43%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.10%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 79.96%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 80.41%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 80.76%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.72%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.01%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 82.27%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 82.24%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 82.08%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 81.93%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 81.78%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 81.38%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 80.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 80.76%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 80.77%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 80.90%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 80.67%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 80.11%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 80.25%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 79.93%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 79.31%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 79.03%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 78.96%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 78.89%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 78.53%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 78.17%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 77.83%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 77.31%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 76.70%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 76.31%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 75.92%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 75.72%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 75.98%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 76.39%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 76.46%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 76.52%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 76.58%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 76.15%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 75.57%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 75.16%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 74.76%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 74.45%   [EVAL] batch:   80 | acc: 31.25%,  total acc: 73.92%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 73.70%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 73.49%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 73.51%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 73.53%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 73.69%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 73.56%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 73.51%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 72.96%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 72.57%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 71.98%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 71.60%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 71.10%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 70.55%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 70.46%   [EVAL] batch:   95 | acc: 43.75%,  total acc: 70.18%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 70.10%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 69.90%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 69.82%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 69.50%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 69.49%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 69.42%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 69.60%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 69.65%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 69.70%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 69.87%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 69.80%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 69.50%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 69.15%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 69.03%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 68.69%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 68.47%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 70.26%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 70.30%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 70.58%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 70.72%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 70.75%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 70.73%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 70.52%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 70.61%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 70.64%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 70.77%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 70.85%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 70.96%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 70.94%   [EVAL] batch:  134 | acc: 68.75%,  total acc: 70.93%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 71.05%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 71.08%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 70.97%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 70.59%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 70.36%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 70.08%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 69.89%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 69.71%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 69.53%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 69.70%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 70.36%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 70.02%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 69.81%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 69.64%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 69.35%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 69.15%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 69.11%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 69.19%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 69.30%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 69.45%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 69.57%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 69.59%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 69.25%   [EVAL] batch:  164 | acc: 50.00%,  total acc: 69.13%   [EVAL] batch:  165 | acc: 6.25%,  total acc: 68.75%   [EVAL] batch:  166 | acc: 31.25%,  total acc: 68.53%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 68.27%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 68.05%   [EVAL] batch:  169 | acc: 6.25%,  total acc: 67.68%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 67.43%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 67.11%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 66.91%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 66.70%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 66.43%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 66.44%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 66.42%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 66.50%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 66.74%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 66.75%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 66.72%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 66.92%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 66.93%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 66.94%   [EVAL] batch:  186 | acc: 62.50%,  total acc: 66.91%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 66.95%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 67.10%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 67.24%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 67.41%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 67.48%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 67.75%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 67.69%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 67.63%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 67.61%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 67.55%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 67.49%   [EVAL] batch:  199 | acc: 62.50%,  total acc: 67.47%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 67.51%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 67.61%   [EVAL] batch:  202 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 67.77%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 67.81%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 67.84%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 67.70%   [EVAL] batch:  208 | acc: 62.50%,  total acc: 67.67%   [EVAL] batch:  209 | acc: 37.50%,  total acc: 67.53%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 67.42%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 67.28%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 67.28%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.89%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  219 | acc: 62.50%,  total acc: 68.12%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 68.21%   [EVAL] batch:  221 | acc: 50.00%,  total acc: 68.13%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 68.11%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 68.14%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 68.00%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 68.80%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 69.82%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 70.12%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 70.34%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 70.53%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 70.97%   [EVAL] batch:  250 | acc: 68.75%,  total acc: 70.97%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 71.03%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 71.05%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 71.04%   [EVAL] batch:  254 | acc: 87.50%,  total acc: 71.10%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 71.18%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 71.22%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 71.21%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 71.20%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 71.10%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 71.09%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 71.14%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 71.16%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 71.17%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 71.18%   [EVAL] batch:  267 | acc: 81.25%,  total acc: 71.22%   [EVAL] batch:  268 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 71.39%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 71.58%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 71.66%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 71.84%   [EVAL] batch:  275 | acc: 68.75%,  total acc: 71.83%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 71.77%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 71.76%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 71.73%   [EVAL] batch:  279 | acc: 75.00%,  total acc: 71.74%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 71.77%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 71.79%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 71.75%   [EVAL] batch:  283 | acc: 56.25%,  total acc: 71.70%   [EVAL] batch:  284 | acc: 50.00%,  total acc: 71.62%   [EVAL] batch:  285 | acc: 37.50%,  total acc: 71.50%   [EVAL] batch:  286 | acc: 50.00%,  total acc: 71.43%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 71.42%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 71.41%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 71.40%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 71.39%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 71.44%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 71.43%   [EVAL] batch:  294 | acc: 87.50%,  total acc: 71.48%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 71.52%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 71.55%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 71.56%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 71.61%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 71.65%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 72.11%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 72.35%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 72.42%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 72.47%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 72.50%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 72.47%   [EVAL] batch:  314 | acc: 37.50%,  total acc: 72.36%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 72.19%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 72.08%   [EVAL] batch:  317 | acc: 56.25%,  total acc: 72.03%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 71.96%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 71.97%   [EVAL] batch:  320 | acc: 68.75%,  total acc: 71.96%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 72.01%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 72.11%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 72.12%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 72.14%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 72.09%   [EVAL] batch:  327 | acc: 62.50%,  total acc: 72.07%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 72.07%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 72.05%   [EVAL] batch:  330 | acc: 81.25%,  total acc: 72.07%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 72.12%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 72.33%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 72.86%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 72.94%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 73.02%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 73.22%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 73.43%   [EVAL] batch:  350 | acc: 43.75%,  total acc: 73.34%   [EVAL] batch:  351 | acc: 43.75%,  total acc: 73.26%   [EVAL] batch:  352 | acc: 56.25%,  total acc: 73.21%   [EVAL] batch:  353 | acc: 43.75%,  total acc: 73.13%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 72.98%   [EVAL] batch:  355 | acc: 50.00%,  total acc: 72.91%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 72.97%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 73.03%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:  360 | acc: 75.00%,  total acc: 73.16%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 73.20%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 73.19%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 73.16%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 73.08%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 73.02%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 72.97%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 72.88%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 72.76%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 72.79%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 72.76%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 72.73%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 72.79%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 72.86%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 72.87%   [EVAL] batch:  375 | acc: 6.25%,  total acc: 72.69%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 72.56%   [EVAL] batch:  377 | acc: 18.75%,  total acc: 72.42%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 72.30%   [EVAL] batch:  379 | acc: 12.50%,  total acc: 72.14%   [EVAL] batch:  380 | acc: 25.00%,  total acc: 72.01%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 72.01%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 72.05%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 72.10%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 72.14%   [EVAL] batch:  385 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:  386 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 72.31%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 72.30%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 72.29%   [EVAL] batch:  390 | acc: 75.00%,  total acc: 72.30%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 72.31%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 72.30%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 72.21%   [EVAL] batch:  394 | acc: 56.25%,  total acc: 72.17%   [EVAL] batch:  395 | acc: 43.75%,  total acc: 72.10%   [EVAL] batch:  396 | acc: 62.50%,  total acc: 72.07%   [EVAL] batch:  397 | acc: 50.00%,  total acc: 72.02%   [EVAL] batch:  398 | acc: 37.50%,  total acc: 71.93%   [EVAL] batch:  399 | acc: 62.50%,  total acc: 71.91%   [EVAL] batch:  400 | acc: 18.75%,  total acc: 71.77%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 71.63%   [EVAL] batch:  402 | acc: 25.00%,  total acc: 71.51%   [EVAL] batch:  403 | acc: 12.50%,  total acc: 71.36%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 71.22%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 71.06%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 71.04%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:  408 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  410 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 71.33%   [EVAL] batch:  412 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 71.44%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 71.56%   [EVAL] batch:  416 | acc: 75.00%,  total acc: 71.57%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 71.61%   [EVAL] batch:  420 | acc: 12.50%,  total acc: 71.47%   [EVAL] batch:  421 | acc: 25.00%,  total acc: 71.36%   [EVAL] batch:  422 | acc: 25.00%,  total acc: 71.25%   [EVAL] batch:  423 | acc: 25.00%,  total acc: 71.14%   [EVAL] batch:  424 | acc: 18.75%,  total acc: 71.01%   [EVAL] batch:  425 | acc: 75.00%,  total acc: 71.02%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 71.08%   [EVAL] batch:  427 | acc: 87.50%,  total acc: 71.12%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 71.14%   [EVAL] batch:  429 | acc: 93.75%,  total acc: 71.19%   [EVAL] batch:  430 | acc: 87.50%,  total acc: 71.23%   [EVAL] batch:  431 | acc: 62.50%,  total acc: 71.21%   [EVAL] batch:  432 | acc: 50.00%,  total acc: 71.16%   [EVAL] batch:  433 | acc: 43.75%,  total acc: 71.10%   [EVAL] batch:  434 | acc: 62.50%,  total acc: 71.08%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 71.03%   [EVAL] batch:  436 | acc: 87.50%,  total acc: 71.07%   [EVAL] batch:  437 | acc: 31.25%,  total acc: 70.98%   
cur_acc:  ['0.9474', '0.8026', '0.7847', '0.8750', '0.8284', '0.7599', '0.5972']
his_acc:  ['0.9474', '0.8695', '0.8411', '0.8263', '0.7929', '0.7540', '0.7098']
CurrentTrain: epoch  0, batch     0 | loss: 6.7718186CurrentTrain: epoch  0, batch     1 | loss: 6.1255484CurrentTrain: epoch  0, batch     2 | loss: 5.3669939CurrentTrain: epoch  0, batch     3 | loss: 7.3710327CurrentTrain: epoch  1, batch     0 | loss: 4.2321901CurrentTrain: epoch  1, batch     1 | loss: 6.0519180CurrentTrain: epoch  1, batch     2 | loss: 5.9438472CurrentTrain: epoch  1, batch     3 | loss: 6.8126941CurrentTrain: epoch  2, batch     0 | loss: 4.9841719CurrentTrain: epoch  2, batch     1 | loss: 5.3758211CurrentTrain: epoch  2, batch     2 | loss: 4.5212932CurrentTrain: epoch  2, batch     3 | loss: 3.5566230CurrentTrain: epoch  3, batch     0 | loss: 4.3068085CurrentTrain: epoch  3, batch     1 | loss: 4.1679378CurrentTrain: epoch  3, batch     2 | loss: 4.6597366CurrentTrain: epoch  3, batch     3 | loss: 4.5200429CurrentTrain: epoch  4, batch     0 | loss: 4.1971011CurrentTrain: epoch  4, batch     1 | loss: 3.8656397CurrentTrain: epoch  4, batch     2 | loss: 3.9142523CurrentTrain: epoch  4, batch     3 | loss: 4.4180717CurrentTrain: epoch  5, batch     0 | loss: 4.0027065CurrentTrain: epoch  5, batch     1 | loss: 3.1724668CurrentTrain: epoch  5, batch     2 | loss: 4.5619497CurrentTrain: epoch  5, batch     3 | loss: 2.5395336CurrentTrain: epoch  6, batch     0 | loss: 3.1587677CurrentTrain: epoch  6, batch     1 | loss: 3.3230810CurrentTrain: epoch  6, batch     2 | loss: 4.3542886CurrentTrain: epoch  6, batch     3 | loss: 5.4892206CurrentTrain: epoch  7, batch     0 | loss: 3.8585334CurrentTrain: epoch  7, batch     1 | loss: 2.8443737CurrentTrain: epoch  7, batch     2 | loss: 2.8446708CurrentTrain: epoch  7, batch     3 | loss: 3.5034454CurrentTrain: epoch  8, batch     0 | loss: 2.2533212CurrentTrain: epoch  8, batch     1 | loss: 3.4435663CurrentTrain: epoch  8, batch     2 | loss: 3.2699058CurrentTrain: epoch  8, batch     3 | loss: 3.7726870CurrentTrain: epoch  9, batch     0 | loss: 2.8004770CurrentTrain: epoch  9, batch     1 | loss: 2.7372417CurrentTrain: epoch  9, batch     2 | loss: 3.1657836CurrentTrain: epoch  9, batch     3 | loss: 2.7580132
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Civil Code governs various aspects of civil law within the state of California, including contracts and property rights.  
Head Entity: California Civil Code  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Queen  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the concert last summer.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that has been produced by Tesla, Inc. since 2012, showcasing cutting-edge technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: norway was represented in the eurovision song contest 2005 by the song " in my dreams " performed by wig wam .
Head Entity: eurovision song contest 2005
Tail Entity: wig wam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: the united states was represented in the 2020 summer olympics by a team of athletes from various sports disciplines.  
Head Entity: 2020 summer olympics  
Tail Entity: united states  

Relation: participant  
Context: the annual tech conference featured several startups, including a presentation by innovative solutions inc.  
Head Entity: annual tech conference  
Tail Entity: innovative solutions inc.  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: " robowar " is a similar game that was released later on the macintosh , and is now available for both mac os and windows .
Head Entity: robowar
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: "The latest version of Adobe Photoshop is compatible with both Windows 10 and macOS Monterey."  
Head Entity: Adobe Photoshop  
Tail Entity: Windows 10  

Relation: operating system  
Context: "Minecraft runs smoothly on various platforms, including Linux and the latest version of Windows."  
Head Entity: Minecraft  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the local political landscape.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring change and transparency to the office.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: the deaths of his brothers wenceslaus ii ( 1487 ) , casimir ii ( 1490 ) and władysław ( 1494 ) allowed jan v to reunificated the whole duchy of zator .
Head Entity: casimir ii
Tail Entity: wenceslaus ii
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: both elizabeth and her brother, charles, were known for their contributions to the arts and culture of their time.  
Head Entity: elizabeth  
Tail Entity: charles  

Relation: sibling  
Context: during the family reunion, it was heartwarming to see how much john and his sister, sarah, resembled each other in both looks and personality.  
Head Entity: john  
Tail Entity: sarah  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: dennis chalker is a retired navy seal , inventor and author who has written six books about the united states navy seals .
Head Entity: dennis chalker
Tail Entity: united states navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: the general served in the air force for over twenty years before retiring and taking on a civilian role in defense consulting.  
Head Entity: the general  
Tail Entity: air force  

Relation: military branch  
Context: during the ceremony, the admiral was recognized for his service in the coast guard, where he led several important missions.  
Head Entity: the admiral  
Tail Entity: coast guard  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character simon is the son of the adventurous couple, marie and tom, who often embark on thrilling quests together.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902 and later became a subject of much speculation.  
Head Entity: albert einstein  
Tail Entity: lieserl  
Mixup data size:  558
MixupTrain:  epoch  0, batch     0 | loss: 2.2181795MixupTrain:  epoch  0, batch     1 | loss: 1.7175873MixupTrain:  epoch  0, batch     2 | loss: 1.6354844MixupTrain:  epoch  0, batch     3 | loss: 1.6955206MixupTrain:  epoch  0, batch     4 | loss: 1.7508107MixupTrain:  epoch  0, batch     5 | loss: 1.8135971MixupTrain:  epoch  0, batch     6 | loss: 2.1887697MixupTrain:  epoch  0, batch     7 | loss: 1.8855640MixupTrain:  epoch  0, batch     8 | loss: 1.8355358MixupTrain:  epoch  0, batch     9 | loss: 2.0244962MixupTrain:  epoch  0, batch    10 | loss: 1.8408084MixupTrain:  epoch  0, batch    11 | loss: 2.0230494MixupTrain:  epoch  0, batch    12 | loss: 1.9672589MixupTrain:  epoch  0, batch    13 | loss: 1.8755079MixupTrain:  epoch  0, batch    14 | loss: 1.3860898MixupTrain:  epoch  0, batch    15 | loss: 2.0725545MixupTrain:  epoch  0, batch    16 | loss: 1.7241974MixupTrain:  epoch  0, batch    17 | loss: 1.8227619MixupTrain:  epoch  0, batch    18 | loss: 2.2542734MixupTrain:  epoch  0, batch    19 | loss: 2.0969643MixupTrain:  epoch  0, batch    20 | loss: 1.8029595MixupTrain:  epoch  0, batch    21 | loss: 1.8968347MixupTrain:  epoch  0, batch    22 | loss: 2.0938191MixupTrain:  epoch  0, batch    23 | loss: 1.5546333MixupTrain:  epoch  0, batch    24 | loss: 1.8783756MixupTrain:  epoch  0, batch    25 | loss: 1.8066206MixupTrain:  epoch  0, batch    26 | loss: 1.9016187MixupTrain:  epoch  0, batch    27 | loss: 2.0505263MixupTrain:  epoch  0, batch    28 | loss: 1.8594982MixupTrain:  epoch  0, batch    29 | loss: 1.6534098MixupTrain:  epoch  0, batch    30 | loss: 1.7864139MixupTrain:  epoch  0, batch    31 | loss: 2.1298567MixupTrain:  epoch  0, batch    32 | loss: 1.9592146MixupTrain:  epoch  0, batch    33 | loss: 1.7777522MixupTrain:  epoch  0, batch    34 | loss: 2.2717513
MemoryTrain:  epoch  0, batch     0 | loss: 1.5640296MemoryTrain:  epoch  0, batch     1 | loss: 1.7264524MemoryTrain:  epoch  0, batch     2 | loss: 1.7808220MemoryTrain:  epoch  0, batch     3 | loss: 2.1165237MemoryTrain:  epoch  0, batch     4 | loss: 1.5986810MemoryTrain:  epoch  0, batch     5 | loss: 2.0514197MemoryTrain:  epoch  0, batch     6 | loss: 1.6037292MemoryTrain:  epoch  0, batch     7 | loss: 1.8882232MemoryTrain:  epoch  0, batch     8 | loss: 1.8851951MemoryTrain:  epoch  0, batch     9 | loss: 2.1864564MemoryTrain:  epoch  0, batch    10 | loss: 1.7344326MemoryTrain:  epoch  0, batch    11 | loss: 2.3246469MemoryTrain:  epoch  0, batch    12 | loss: 2.2846789MemoryTrain:  epoch  0, batch    13 | loss: 2.1825747MemoryTrain:  epoch  0, batch    14 | loss: 2.2734747MemoryTrain:  epoch  1, batch     0 | loss: 2.1589487MemoryTrain:  epoch  1, batch     1 | loss: 1.4613416MemoryTrain:  epoch  1, batch     2 | loss: 1.8956811MemoryTrain:  epoch  1, batch     3 | loss: 1.5138975MemoryTrain:  epoch  1, batch     4 | loss: 1.8550415MemoryTrain:  epoch  1, batch     5 | loss: 1.9037201MemoryTrain:  epoch  1, batch     6 | loss: 1.9422307MemoryTrain:  epoch  1, batch     7 | loss: 1.6723032MemoryTrain:  epoch  1, batch     8 | loss: 1.7871006MemoryTrain:  epoch  1, batch     9 | loss: 1.7041802MemoryTrain:  epoch  1, batch    10 | loss: 1.9982004MemoryTrain:  epoch  1, batch    11 | loss: 1.6014426MemoryTrain:  epoch  1, batch    12 | loss: 1.7174139MemoryTrain:  epoch  1, batch    13 | loss: 1.4664595MemoryTrain:  epoch  1, batch    14 | loss: 1.6824452MemoryTrain:  epoch  2, batch     0 | loss: 1.4678057MemoryTrain:  epoch  2, batch     1 | loss: 1.6569394MemoryTrain:  epoch  2, batch     2 | loss: 1.2383996MemoryTrain:  epoch  2, batch     3 | loss: 1.7922454MemoryTrain:  epoch  2, batch     4 | loss: 1.8542054MemoryTrain:  epoch  2, batch     5 | loss: 1.3648925MemoryTrain:  epoch  2, batch     6 | loss: 1.6067994MemoryTrain:  epoch  2, batch     7 | loss: 1.3425155MemoryTrain:  epoch  2, batch     8 | loss: 1.3578479MemoryTrain:  epoch  2, batch     9 | loss: 1.6095138MemoryTrain:  epoch  2, batch    10 | loss: 1.5568477MemoryTrain:  epoch  2, batch    11 | loss: 2.0109227MemoryTrain:  epoch  2, batch    12 | loss: 1.3611591MemoryTrain:  epoch  2, batch    13 | loss: 1.4346559MemoryTrain:  epoch  2, batch    14 | loss: 1.4514329MemoryTrain:  epoch  3, batch     0 | loss: 1.5142027MemoryTrain:  epoch  3, batch     1 | loss: 1.5027936MemoryTrain:  epoch  3, batch     2 | loss: 1.2810807MemoryTrain:  epoch  3, batch     3 | loss: 1.2856426MemoryTrain:  epoch  3, batch     4 | loss: 1.3999393MemoryTrain:  epoch  3, batch     5 | loss: 1.3686141MemoryTrain:  epoch  3, batch     6 | loss: 1.2969236MemoryTrain:  epoch  3, batch     7 | loss: 1.6982181MemoryTrain:  epoch  3, batch     8 | loss: 1.4730887MemoryTrain:  epoch  3, batch     9 | loss: 1.4240978MemoryTrain:  epoch  3, batch    10 | loss: 1.2961752MemoryTrain:  epoch  3, batch    11 | loss: 1.6631662MemoryTrain:  epoch  3, batch    12 | loss: 1.4684224MemoryTrain:  epoch  3, batch    13 | loss: 1.3147843MemoryTrain:  epoch  3, batch    14 | loss: 1.3206606MemoryTrain:  epoch  4, batch     0 | loss: 1.3009430MemoryTrain:  epoch  4, batch     1 | loss: 1.4190747MemoryTrain:  epoch  4, batch     2 | loss: 1.2381032MemoryTrain:  epoch  4, batch     3 | loss: 1.8432045MemoryTrain:  epoch  4, batch     4 | loss: 1.2801243MemoryTrain:  epoch  4, batch     5 | loss: 1.2806926MemoryTrain:  epoch  4, batch     6 | loss: 1.2726272MemoryTrain:  epoch  4, batch     7 | loss: 1.6161642MemoryTrain:  epoch  4, batch     8 | loss: 1.5161250MemoryTrain:  epoch  4, batch     9 | loss: 1.2549587MemoryTrain:  epoch  4, batch    10 | loss: 1.4561186MemoryTrain:  epoch  4, batch    11 | loss: 1.3630100MemoryTrain:  epoch  4, batch    12 | loss: 1.6020629MemoryTrain:  epoch  4, batch    13 | loss: 1.4944547MemoryTrain:  epoch  4, batch    14 | loss: 1.2362822MemoryTrain:  epoch  5, batch     0 | loss: 1.2369461MemoryTrain:  epoch  5, batch     1 | loss: 1.4469337MemoryTrain:  epoch  5, batch     2 | loss: 1.2390437MemoryTrain:  epoch  5, batch     3 | loss: 1.2646416MemoryTrain:  epoch  5, batch     4 | loss: 1.3295068MemoryTrain:  epoch  5, batch     5 | loss: 1.4109895MemoryTrain:  epoch  5, batch     6 | loss: 1.2898006MemoryTrain:  epoch  5, batch     7 | loss: 1.2033620MemoryTrain:  epoch  5, batch     8 | loss: 1.4732493MemoryTrain:  epoch  5, batch     9 | loss: 1.4901574MemoryTrain:  epoch  5, batch    10 | loss: 1.1820740MemoryTrain:  epoch  5, batch    11 | loss: 1.4874568MemoryTrain:  epoch  5, batch    12 | loss: 1.2856050MemoryTrain:  epoch  5, batch    13 | loss: 1.3804420MemoryTrain:  epoch  5, batch    14 | loss: 1.3119781MemoryTrain:  epoch  6, batch     0 | loss: 1.3032796MemoryTrain:  epoch  6, batch     1 | loss: 1.4296191MemoryTrain:  epoch  6, batch     2 | loss: 1.3375871MemoryTrain:  epoch  6, batch     3 | loss: 1.2969656MemoryTrain:  epoch  6, batch     4 | loss: 1.3783101MemoryTrain:  epoch  6, batch     5 | loss: 1.2772367MemoryTrain:  epoch  6, batch     6 | loss: 1.2422583MemoryTrain:  epoch  6, batch     7 | loss: 1.1830301MemoryTrain:  epoch  6, batch     8 | loss: 1.3538359MemoryTrain:  epoch  6, batch     9 | loss: 1.2711177MemoryTrain:  epoch  6, batch    10 | loss: 1.4580243MemoryTrain:  epoch  6, batch    11 | loss: 1.2825445MemoryTrain:  epoch  6, batch    12 | loss: 1.4128189MemoryTrain:  epoch  6, batch    13 | loss: 1.2532251MemoryTrain:  epoch  6, batch    14 | loss: 1.2498944MemoryTrain:  epoch  7, batch     0 | loss: 1.2507027MemoryTrain:  epoch  7, batch     1 | loss: 1.2899680MemoryTrain:  epoch  7, batch     2 | loss: 1.2131702MemoryTrain:  epoch  7, batch     3 | loss: 1.2863899MemoryTrain:  epoch  7, batch     4 | loss: 1.3843350MemoryTrain:  epoch  7, batch     5 | loss: 1.3955486MemoryTrain:  epoch  7, batch     6 | loss: 1.2072712MemoryTrain:  epoch  7, batch     7 | loss: 1.2478054MemoryTrain:  epoch  7, batch     8 | loss: 1.3386185MemoryTrain:  epoch  7, batch     9 | loss: 1.2210226MemoryTrain:  epoch  7, batch    10 | loss: 1.1979535MemoryTrain:  epoch  7, batch    11 | loss: 1.2240088MemoryTrain:  epoch  7, batch    12 | loss: 1.2986218MemoryTrain:  epoch  7, batch    13 | loss: 1.3928291MemoryTrain:  epoch  7, batch    14 | loss: 1.1847807MemoryTrain:  epoch  8, batch     0 | loss: 1.1870252MemoryTrain:  epoch  8, batch     1 | loss: 1.2576597MemoryTrain:  epoch  8, batch     2 | loss: 1.3433363MemoryTrain:  epoch  8, batch     3 | loss: 1.2185457MemoryTrain:  epoch  8, batch     4 | loss: 1.2187967MemoryTrain:  epoch  8, batch     5 | loss: 1.2260441MemoryTrain:  epoch  8, batch     6 | loss: 1.2965546MemoryTrain:  epoch  8, batch     7 | loss: 1.2161288MemoryTrain:  epoch  8, batch     8 | loss: 1.2905579MemoryTrain:  epoch  8, batch     9 | loss: 1.2577935MemoryTrain:  epoch  8, batch    10 | loss: 1.3254139MemoryTrain:  epoch  8, batch    11 | loss: 1.2021642MemoryTrain:  epoch  8, batch    12 | loss: 1.2883396MemoryTrain:  epoch  8, batch    13 | loss: 1.3064812MemoryTrain:  epoch  8, batch    14 | loss: 1.2328327MemoryTrain:  epoch  9, batch     0 | loss: 1.2023240MemoryTrain:  epoch  9, batch     1 | loss: 1.2407963MemoryTrain:  epoch  9, batch     2 | loss: 1.2234911MemoryTrain:  epoch  9, batch     3 | loss: 1.1695416MemoryTrain:  epoch  9, batch     4 | loss: 1.2747691MemoryTrain:  epoch  9, batch     5 | loss: 1.3563747MemoryTrain:  epoch  9, batch     6 | loss: 1.2394879MemoryTrain:  epoch  9, batch     7 | loss: 1.2424517MemoryTrain:  epoch  9, batch     8 | loss: 1.2302959MemoryTrain:  epoch  9, batch     9 | loss: 1.2097980MemoryTrain:  epoch  9, batch    10 | loss: 1.2343736MemoryTrain:  epoch  9, batch    11 | loss: 1.2922018MemoryTrain:  epoch  9, batch    12 | loss: 1.1999592MemoryTrain:  epoch  9, batch    13 | loss: 1.2453991MemoryTrain:  epoch  9, batch    14 | loss: 1.2062540
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 77.86%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 78.61%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.13%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 83.64%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 84.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 84.55%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.97%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.36%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 86.13%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.63%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 86.65%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 85.83%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 85.05%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 84.44%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 83.55%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 83.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.53%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.73%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 83.68%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 83.86%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 83.77%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 82.97%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 82.20%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 81.77%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 81.05%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 80.65%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 80.06%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 54.55%   [EVAL] batch:   11 | acc: 18.75%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 51.44%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 57.08%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 61.03%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 63.82%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 65.06%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 65.49%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.12%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 75.17%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 75.51%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 75.99%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.19%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 77.83%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 78.05%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 77.92%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 77.58%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 77.13%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 76.56%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 75.89%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 75.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 75.36%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 75.59%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 75.46%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 75.22%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 74.89%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 74.46%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 74.36%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 74.48%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 74.59%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 74.40%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 74.21%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 73.44%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 72.88%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 72.06%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 71.36%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 70.96%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 71.16%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 71.70%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 71.83%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 71.96%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 72.08%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 71.63%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 71.10%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 70.75%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 70.57%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 70.39%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 70.29%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 70.12%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 69.95%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 69.72%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 69.63%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 69.62%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 69.47%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 69.39%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 68.89%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 68.47%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 67.86%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 67.46%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 66.94%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 66.42%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 66.25%   [EVAL] batch:   95 | acc: 31.25%,  total acc: 65.89%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 65.85%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 65.69%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 65.66%   [EVAL] batch:   99 | acc: 31.25%,  total acc: 65.31%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 65.35%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 65.44%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 65.66%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 65.75%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 66.12%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 65.91%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 65.60%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 65.51%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 65.26%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 65.18%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 65.15%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 66.63%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 66.86%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 66.98%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 66.89%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 67.06%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 67.12%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 67.24%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 67.15%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 67.11%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 66.88%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 66.89%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 67.12%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 67.13%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 67.09%   [EVAL] batch:  132 | acc: 25.00%,  total acc: 66.78%   [EVAL] batch:  133 | acc: 50.00%,  total acc: 66.65%   [EVAL] batch:  134 | acc: 50.00%,  total acc: 66.53%   [EVAL] batch:  135 | acc: 43.75%,  total acc: 66.36%   [EVAL] batch:  136 | acc: 43.75%,  total acc: 66.20%   [EVAL] batch:  137 | acc: 31.25%,  total acc: 65.94%   [EVAL] batch:  138 | acc: 6.25%,  total acc: 65.51%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 65.22%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 64.89%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 64.61%   [EVAL] batch:  142 | acc: 25.00%,  total acc: 64.34%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 64.15%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 64.35%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 64.60%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 64.80%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 65.03%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 65.27%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 65.46%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 65.23%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 64.88%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 64.67%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 64.53%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 64.19%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 63.98%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 64.01%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 64.12%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 64.27%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 64.60%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 64.66%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 64.65%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 64.33%   [EVAL] batch:  164 | acc: 50.00%,  total acc: 64.24%   [EVAL] batch:  165 | acc: 0.00%,  total acc: 63.86%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 63.55%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 63.32%   [EVAL] batch:  168 | acc: 25.00%,  total acc: 63.09%   [EVAL] batch:  169 | acc: 6.25%,  total acc: 62.76%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 62.54%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 62.28%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 62.07%   [EVAL] batch:  173 | acc: 43.75%,  total acc: 61.96%   [EVAL] batch:  174 | acc: 18.75%,  total acc: 61.71%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 61.68%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 61.69%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 61.69%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 61.80%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 61.84%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 61.84%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 61.85%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 61.99%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 62.13%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 62.16%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 62.30%   [EVAL] batch:  186 | acc: 56.25%,  total acc: 62.27%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 62.37%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 62.53%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 62.70%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 62.99%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 63.18%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 63.31%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 63.27%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 63.27%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 63.29%   [EVAL] batch:  197 | acc: 56.25%,  total acc: 63.26%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 63.25%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 63.22%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 63.28%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 63.40%   [EVAL] batch:  202 | acc: 93.75%,  total acc: 63.55%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 63.60%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 63.65%   [EVAL] batch:  206 | acc: 68.75%,  total acc: 63.68%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 63.58%   [EVAL] batch:  208 | acc: 62.50%,  total acc: 63.58%   [EVAL] batch:  209 | acc: 37.50%,  total acc: 63.45%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 63.36%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 63.24%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 63.26%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.43%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.11%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 64.24%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 64.18%   [EVAL] batch:  220 | acc: 87.50%,  total acc: 64.28%   [EVAL] batch:  221 | acc: 56.25%,  total acc: 64.25%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 64.24%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 64.17%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 65.37%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 65.93%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 65.99%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 65.98%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 66.12%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 66.20%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 66.27%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 66.49%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 66.87%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 66.97%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 67.01%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 67.09%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 67.17%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 67.20%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 67.25%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 67.41%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 67.49%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 67.43%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 67.46%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 67.49%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 67.45%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 67.45%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 67.37%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 67.30%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 67.40%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 67.48%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 67.69%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  275 | acc: 62.50%,  total acc: 67.98%   [EVAL] batch:  276 | acc: 62.50%,  total acc: 67.96%   [EVAL] batch:  277 | acc: 81.25%,  total acc: 68.01%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 68.01%   [EVAL] batch:  279 | acc: 81.25%,  total acc: 68.06%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 68.10%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 68.18%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 68.20%   [EVAL] batch:  284 | acc: 56.25%,  total acc: 68.16%   [EVAL] batch:  285 | acc: 43.75%,  total acc: 68.07%   [EVAL] batch:  286 | acc: 62.50%,  total acc: 68.05%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 68.01%   [EVAL] batch:  288 | acc: 75.00%,  total acc: 68.04%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 68.08%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 68.08%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 68.20%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 68.20%   [EVAL] batch:  294 | acc: 87.50%,  total acc: 68.26%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 68.31%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 68.35%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 68.39%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 69.14%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 69.20%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 69.26%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 69.33%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 69.39%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 69.43%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 69.37%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 69.25%   [EVAL] batch:  315 | acc: 18.75%,  total acc: 69.09%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 68.99%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 68.89%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 68.83%   [EVAL] batch:  319 | acc: 68.75%,  total acc: 68.83%   [EVAL] batch:  320 | acc: 68.75%,  total acc: 68.83%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 68.87%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 68.94%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 68.98%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 69.00%   [EVAL] batch:  325 | acc: 43.75%,  total acc: 68.92%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 68.77%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 68.60%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 68.48%   [EVAL] batch:  329 | acc: 31.25%,  total acc: 68.37%   [EVAL] batch:  330 | acc: 31.25%,  total acc: 68.26%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 68.30%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 68.37%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 68.53%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 68.60%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 69.12%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 69.52%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  350 | acc: 6.25%,  total acc: 69.57%   [EVAL] batch:  351 | acc: 31.25%,  total acc: 69.46%   [EVAL] batch:  352 | acc: 6.25%,  total acc: 69.28%   [EVAL] batch:  353 | acc: 12.50%,  total acc: 69.12%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 68.94%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 68.79%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 68.82%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 68.92%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 68.99%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 69.03%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 69.06%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 69.04%   [EVAL] batch:  364 | acc: 31.25%,  total acc: 68.94%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 68.89%   [EVAL] batch:  366 | acc: 56.25%,  total acc: 68.85%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 68.77%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 68.65%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 68.68%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 68.65%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 68.60%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 68.65%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 68.73%   [EVAL] batch:  375 | acc: 6.25%,  total acc: 68.57%   [EVAL] batch:  376 | acc: 25.00%,  total acc: 68.45%   [EVAL] batch:  377 | acc: 12.50%,  total acc: 68.30%   [EVAL] batch:  378 | acc: 12.50%,  total acc: 68.16%   [EVAL] batch:  379 | acc: 18.75%,  total acc: 68.03%   [EVAL] batch:  380 | acc: 37.50%,  total acc: 67.95%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 67.95%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 67.98%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 68.05%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 68.10%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:  386 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 68.28%   [EVAL] batch:  388 | acc: 75.00%,  total acc: 68.30%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 68.32%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 68.35%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:  392 | acc: 75.00%,  total acc: 68.42%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 68.34%   [EVAL] batch:  394 | acc: 50.00%,  total acc: 68.29%   [EVAL] batch:  395 | acc: 37.50%,  total acc: 68.21%   [EVAL] batch:  396 | acc: 62.50%,  total acc: 68.20%   [EVAL] batch:  397 | acc: 43.75%,  total acc: 68.14%   [EVAL] batch:  398 | acc: 37.50%,  total acc: 68.06%   [EVAL] batch:  399 | acc: 56.25%,  total acc: 68.03%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 67.88%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 67.74%   [EVAL] batch:  402 | acc: 18.75%,  total acc: 67.62%   [EVAL] batch:  403 | acc: 6.25%,  total acc: 67.47%   [EVAL] batch:  404 | acc: 12.50%,  total acc: 67.33%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 67.18%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 67.15%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 67.20%   [EVAL] batch:  408 | acc: 93.75%,  total acc: 67.27%   [EVAL] batch:  409 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:  410 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 67.45%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 67.51%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 67.56%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  416 | acc: 75.00%,  total acc: 67.72%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  418 | acc: 75.00%,  total acc: 67.81%   [EVAL] batch:  419 | acc: 31.25%,  total acc: 67.72%   [EVAL] batch:  420 | acc: 12.50%,  total acc: 67.59%   [EVAL] batch:  421 | acc: 18.75%,  total acc: 67.48%   [EVAL] batch:  422 | acc: 25.00%,  total acc: 67.38%   [EVAL] batch:  423 | acc: 18.75%,  total acc: 67.26%   [EVAL] batch:  424 | acc: 12.50%,  total acc: 67.13%   [EVAL] batch:  425 | acc: 18.75%,  total acc: 67.02%   [EVAL] batch:  426 | acc: 56.25%,  total acc: 66.99%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 67.00%   [EVAL] batch:  428 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:  429 | acc: 56.25%,  total acc: 66.93%   [EVAL] batch:  430 | acc: 62.50%,  total acc: 66.92%   [EVAL] batch:  431 | acc: 50.00%,  total acc: 66.88%   [EVAL] batch:  432 | acc: 50.00%,  total acc: 66.84%   [EVAL] batch:  433 | acc: 50.00%,  total acc: 66.81%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 66.81%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 66.77%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 66.83%   [EVAL] batch:  437 | acc: 75.00%,  total acc: 66.85%   [EVAL] batch:  438 | acc: 75.00%,  total acc: 66.87%   [EVAL] batch:  439 | acc: 81.25%,  total acc: 66.90%   [EVAL] batch:  440 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:  441 | acc: 81.25%,  total acc: 67.00%   [EVAL] batch:  442 | acc: 75.00%,  total acc: 67.01%   [EVAL] batch:  443 | acc: 75.00%,  total acc: 67.03%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 67.08%   [EVAL] batch:  445 | acc: 75.00%,  total acc: 67.10%   [EVAL] batch:  446 | acc: 62.50%,  total acc: 67.09%   [EVAL] batch:  447 | acc: 93.75%,  total acc: 67.15%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 67.22%   [EVAL] batch:  450 | acc: 81.25%,  total acc: 67.25%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 67.28%   [EVAL] batch:  452 | acc: 68.75%,  total acc: 67.29%   [EVAL] batch:  453 | acc: 68.75%,  total acc: 67.29%   [EVAL] batch:  454 | acc: 75.00%,  total acc: 67.31%   [EVAL] batch:  455 | acc: 93.75%,  total acc: 67.37%   [EVAL] batch:  456 | acc: 75.00%,  total acc: 67.38%   [EVAL] batch:  457 | acc: 62.50%,  total acc: 67.37%   [EVAL] batch:  458 | acc: 75.00%,  total acc: 67.39%   [EVAL] batch:  459 | acc: 81.25%,  total acc: 67.42%   [EVAL] batch:  460 | acc: 56.25%,  total acc: 67.39%   [EVAL] batch:  461 | acc: 75.00%,  total acc: 67.41%   [EVAL] batch:  462 | acc: 87.50%,  total acc: 67.45%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 67.59%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  468 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 68.21%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:  477 | acc: 93.75%,  total acc: 68.45%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 68.58%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  481 | acc: 56.25%,  total acc: 68.61%   [EVAL] batch:  482 | acc: 56.25%,  total acc: 68.58%   [EVAL] batch:  483 | acc: 56.25%,  total acc: 68.56%   [EVAL] batch:  484 | acc: 62.50%,  total acc: 68.54%   [EVAL] batch:  485 | acc: 62.50%,  total acc: 68.53%   [EVAL] batch:  486 | acc: 62.50%,  total acc: 68.52%   [EVAL] batch:  487 | acc: 75.00%,  total acc: 68.53%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 68.57%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  490 | acc: 87.50%,  total acc: 68.67%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 68.71%   [EVAL] batch:  492 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  493 | acc: 93.75%,  total acc: 68.80%   [EVAL] batch:  494 | acc: 37.50%,  total acc: 68.74%   [EVAL] batch:  495 | acc: 43.75%,  total acc: 68.69%   [EVAL] batch:  496 | acc: 37.50%,  total acc: 68.62%   [EVAL] batch:  497 | acc: 62.50%,  total acc: 68.61%   [EVAL] batch:  498 | acc: 43.75%,  total acc: 68.56%   [EVAL] batch:  499 | acc: 68.75%,  total acc: 68.56%   
cur_acc:  ['0.9474', '0.8026', '0.7847', '0.8750', '0.8284', '0.7599', '0.5972', '0.8006']
his_acc:  ['0.9474', '0.8695', '0.8411', '0.8263', '0.7929', '0.7540', '0.7098', '0.6856']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 12.9875031CurrentTrain: epoch  0, batch     1 | loss: 12.7274303CurrentTrain: epoch  0, batch     2 | loss: 12.4121399CurrentTrain: epoch  0, batch     3 | loss: 12.1825714CurrentTrain: epoch  0, batch     4 | loss: 11.8316689CurrentTrain: epoch  0, batch     5 | loss: 11.7734871CurrentTrain: epoch  0, batch     6 | loss: 11.7838268CurrentTrain: epoch  0, batch     7 | loss: 11.5824451CurrentTrain: epoch  0, batch     8 | loss: 11.0885296CurrentTrain: epoch  0, batch     9 | loss: 11.0231600CurrentTrain: epoch  0, batch    10 | loss: 11.3799400CurrentTrain: epoch  0, batch    11 | loss: 11.0791397CurrentTrain: epoch  0, batch    12 | loss: 11.0721607CurrentTrain: epoch  0, batch    13 | loss: 10.5810795CurrentTrain: epoch  0, batch    14 | loss: 10.5741596CurrentTrain: epoch  0, batch    15 | loss: 10.2393389CurrentTrain: epoch  0, batch    16 | loss: 10.2778320CurrentTrain: epoch  0, batch    17 | loss: 10.5440617CurrentTrain: epoch  0, batch    18 | loss: 10.7223072CurrentTrain: epoch  0, batch    19 | loss: 10.3923235CurrentTrain: epoch  0, batch    20 | loss: 10.2047634CurrentTrain: epoch  0, batch    21 | loss: 9.9873428CurrentTrain: epoch  0, batch    22 | loss: 10.6379852CurrentTrain: epoch  0, batch    23 | loss: 9.4756317CurrentTrain: epoch  0, batch    24 | loss: 9.9805641CurrentTrain: epoch  0, batch    25 | loss: 9.4275427CurrentTrain: epoch  0, batch    26 | loss: 9.7372112CurrentTrain: epoch  0, batch    27 | loss: 10.3772526CurrentTrain: epoch  0, batch    28 | loss: 9.5790854CurrentTrain: epoch  0, batch    29 | loss: 9.8431911CurrentTrain: epoch  0, batch    30 | loss: 10.0510387CurrentTrain: epoch  0, batch    31 | loss: 9.5727940CurrentTrain: epoch  0, batch    32 | loss: 9.8294926CurrentTrain: epoch  0, batch    33 | loss: 9.4843626CurrentTrain: epoch  0, batch    34 | loss: 9.5391369CurrentTrain: epoch  0, batch    35 | loss: 9.2926407CurrentTrain: epoch  0, batch    36 | loss: 9.5435028CurrentTrain: epoch  0, batch    37 | loss: 9.4291649CurrentTrain: epoch  0, batch    38 | loss: 10.2913113CurrentTrain: epoch  0, batch    39 | loss: 9.5689535CurrentTrain: epoch  0, batch    40 | loss: 9.1474390CurrentTrain: epoch  0, batch    41 | loss: 9.4329958CurrentTrain: epoch  0, batch    42 | loss: 9.1931715CurrentTrain: epoch  0, batch    43 | loss: 8.6673613CurrentTrain: epoch  0, batch    44 | loss: 9.5394602CurrentTrain: epoch  0, batch    45 | loss: 8.7447968CurrentTrain: epoch  0, batch    46 | loss: 9.3052101CurrentTrain: epoch  0, batch    47 | loss: 9.0317307CurrentTrain: epoch  0, batch    48 | loss: 9.7212324CurrentTrain: epoch  0, batch    49 | loss: 8.9782515CurrentTrain: epoch  0, batch    50 | loss: 8.0877399CurrentTrain: epoch  0, batch    51 | loss: 8.8212461CurrentTrain: epoch  0, batch    52 | loss: 8.5528822CurrentTrain: epoch  0, batch    53 | loss: 8.3130417CurrentTrain: epoch  0, batch    54 | loss: 8.2318430CurrentTrain: epoch  0, batch    55 | loss: 7.9100180CurrentTrain: epoch  0, batch    56 | loss: 8.2569904CurrentTrain: epoch  0, batch    57 | loss: 8.3579454CurrentTrain: epoch  0, batch    58 | loss: 9.0496492CurrentTrain: epoch  0, batch    59 | loss: 7.9878650CurrentTrain: epoch  0, batch    60 | loss: 7.1196599CurrentTrain: epoch  0, batch    61 | loss: 8.1254873CurrentTrain: epoch  0, batch    62 | loss: 7.9026537CurrentTrain: epoch  1, batch     0 | loss: 8.1835003CurrentTrain: epoch  1, batch     1 | loss: 7.7201290CurrentTrain: epoch  1, batch     2 | loss: 7.7901659CurrentTrain: epoch  1, batch     3 | loss: 7.3196583CurrentTrain: epoch  1, batch     4 | loss: 8.1628513CurrentTrain: epoch  1, batch     5 | loss: 7.4152627CurrentTrain: epoch  1, batch     6 | loss: 7.7337770CurrentTrain: epoch  1, batch     7 | loss: 7.7452850CurrentTrain: epoch  1, batch     8 | loss: 7.5881028CurrentTrain: epoch  1, batch     9 | loss: 8.0715332CurrentTrain: epoch  1, batch    10 | loss: 6.8071604CurrentTrain: epoch  1, batch    11 | loss: 6.9203544CurrentTrain: epoch  1, batch    12 | loss: 7.1606722CurrentTrain: epoch  1, batch    13 | loss: 7.5567455CurrentTrain: epoch  1, batch    14 | loss: 7.6707544CurrentTrain: epoch  1, batch    15 | loss: 7.1731319CurrentTrain: epoch  1, batch    16 | loss: 6.8425417CurrentTrain: epoch  1, batch    17 | loss: 7.5679121CurrentTrain: epoch  1, batch    18 | loss: 7.1158314CurrentTrain: epoch  1, batch    19 | loss: 7.3444781CurrentTrain: epoch  1, batch    20 | loss: 7.5488935CurrentTrain: epoch  1, batch    21 | loss: 7.3226094CurrentTrain: epoch  1, batch    22 | loss: 7.2600656CurrentTrain: epoch  1, batch    23 | loss: 7.1184521CurrentTrain: epoch  1, batch    24 | loss: 7.3173409CurrentTrain: epoch  1, batch    25 | loss: 6.8141518CurrentTrain: epoch  1, batch    26 | loss: 6.4385004CurrentTrain: epoch  1, batch    27 | loss: 7.1421537CurrentTrain: epoch  1, batch    28 | loss: 7.1298647CurrentTrain: epoch  1, batch    29 | loss: 7.5034866CurrentTrain: epoch  1, batch    30 | loss: 7.3578434CurrentTrain: epoch  1, batch    31 | loss: 6.5321131CurrentTrain: epoch  1, batch    32 | loss: 6.4268155CurrentTrain: epoch  1, batch    33 | loss: 7.7181087CurrentTrain: epoch  1, batch    34 | loss: 6.6056848CurrentTrain: epoch  1, batch    35 | loss: 7.1461048CurrentTrain: epoch  1, batch    36 | loss: 6.6121387CurrentTrain: epoch  1, batch    37 | loss: 6.9099555CurrentTrain: epoch  1, batch    38 | loss: 6.8786945CurrentTrain: epoch  1, batch    39 | loss: 6.7558928CurrentTrain: epoch  1, batch    40 | loss: 6.0288153CurrentTrain: epoch  1, batch    41 | loss: 6.2823744CurrentTrain: epoch  1, batch    42 | loss: 7.0514202CurrentTrain: epoch  1, batch    43 | loss: 6.6570253CurrentTrain: epoch  1, batch    44 | loss: 6.4624968CurrentTrain: epoch  1, batch    45 | loss: 7.0438752CurrentTrain: epoch  1, batch    46 | loss: 7.1019502CurrentTrain: epoch  1, batch    47 | loss: 5.7928743CurrentTrain: epoch  1, batch    48 | loss: 6.3824301CurrentTrain: epoch  1, batch    49 | loss: 6.5544777CurrentTrain: epoch  1, batch    50 | loss: 7.1988897CurrentTrain: epoch  1, batch    51 | loss: 6.9668918CurrentTrain: epoch  1, batch    52 | loss: 7.7704887CurrentTrain: epoch  1, batch    53 | loss: 6.8106732CurrentTrain: epoch  1, batch    54 | loss: 6.6717606CurrentTrain: epoch  1, batch    55 | loss: 6.0307636CurrentTrain: epoch  1, batch    56 | loss: 6.5275126CurrentTrain: epoch  1, batch    57 | loss: 6.7098360CurrentTrain: epoch  1, batch    58 | loss: 6.1266756CurrentTrain: epoch  1, batch    59 | loss: 6.0174689CurrentTrain: epoch  1, batch    60 | loss: 5.9102564CurrentTrain: epoch  1, batch    61 | loss: 6.8455505CurrentTrain: epoch  1, batch    62 | loss: 6.3442898CurrentTrain: epoch  2, batch     0 | loss: 6.5828319CurrentTrain: epoch  2, batch     1 | loss: 5.7693186CurrentTrain: epoch  2, batch     2 | loss: 5.6504936CurrentTrain: epoch  2, batch     3 | loss: 5.9883871CurrentTrain: epoch  2, batch     4 | loss: 5.6491499CurrentTrain: epoch  2, batch     5 | loss: 5.8806973CurrentTrain: epoch  2, batch     6 | loss: 6.0343714CurrentTrain: epoch  2, batch     7 | loss: 5.9798269CurrentTrain: epoch  2, batch     8 | loss: 6.0651188CurrentTrain: epoch  2, batch     9 | loss: 5.8879051CurrentTrain: epoch  2, batch    10 | loss: 5.3980799CurrentTrain: epoch  2, batch    11 | loss: 6.1530361CurrentTrain: epoch  2, batch    12 | loss: 5.8078341CurrentTrain: epoch  2, batch    13 | loss: 6.1475220CurrentTrain: epoch  2, batch    14 | loss: 6.1851845CurrentTrain: epoch  2, batch    15 | loss: 5.5459619CurrentTrain: epoch  2, batch    16 | loss: 5.3792028CurrentTrain: epoch  2, batch    17 | loss: 5.6457500CurrentTrain: epoch  2, batch    18 | loss: 6.0234175CurrentTrain: epoch  2, batch    19 | loss: 5.5204368CurrentTrain: epoch  2, batch    20 | loss: 6.0698929CurrentTrain: epoch  2, batch    21 | loss: 5.8134189CurrentTrain: epoch  2, batch    22 | loss: 5.4823556CurrentTrain: epoch  2, batch    23 | loss: 6.4090891CurrentTrain: epoch  2, batch    24 | loss: 6.5325079CurrentTrain: epoch  2, batch    25 | loss: 5.6526451CurrentTrain: epoch  2, batch    26 | loss: 6.0878782CurrentTrain: epoch  2, batch    27 | loss: 5.7003574CurrentTrain: epoch  2, batch    28 | loss: 5.6140895CurrentTrain: epoch  2, batch    29 | loss: 6.0587540CurrentTrain: epoch  2, batch    30 | loss: 5.3724828CurrentTrain: epoch  2, batch    31 | loss: 5.4341698CurrentTrain: epoch  2, batch    32 | loss: 5.6300602CurrentTrain: epoch  2, batch    33 | loss: 5.5869179CurrentTrain: epoch  2, batch    34 | loss: 5.9700222CurrentTrain: epoch  2, batch    35 | loss: 5.6210680CurrentTrain: epoch  2, batch    36 | loss: 5.8259211CurrentTrain: epoch  2, batch    37 | loss: 5.8523726CurrentTrain: epoch  2, batch    38 | loss: 6.3174920CurrentTrain: epoch  2, batch    39 | loss: 5.5704155CurrentTrain: epoch  2, batch    40 | loss: 5.9620728CurrentTrain: epoch  2, batch    41 | loss: 5.9833479CurrentTrain: epoch  2, batch    42 | loss: 5.3129597CurrentTrain: epoch  2, batch    43 | loss: 5.5475473CurrentTrain: epoch  2, batch    44 | loss: 5.0235853CurrentTrain: epoch  2, batch    45 | loss: 5.8363390CurrentTrain: epoch  2, batch    46 | loss: 6.0331049CurrentTrain: epoch  2, batch    47 | loss: 5.7065468CurrentTrain: epoch  2, batch    48 | loss: 5.2169275CurrentTrain: epoch  2, batch    49 | loss: 5.7185335CurrentTrain: epoch  2, batch    50 | loss: 5.1914215CurrentTrain: epoch  2, batch    51 | loss: 5.1321783CurrentTrain: epoch  2, batch    52 | loss: 5.6149659CurrentTrain: epoch  2, batch    53 | loss: 5.7487464CurrentTrain: epoch  2, batch    54 | loss: 5.6197653CurrentTrain: epoch  2, batch    55 | loss: 5.6553626CurrentTrain: epoch  2, batch    56 | loss: 5.3108649CurrentTrain: epoch  2, batch    57 | loss: 5.6702890CurrentTrain: epoch  2, batch    58 | loss: 5.7336965CurrentTrain: epoch  2, batch    59 | loss: 5.2619128CurrentTrain: epoch  2, batch    60 | loss: 6.3014355CurrentTrain: epoch  2, batch    61 | loss: 5.5182290CurrentTrain: epoch  2, batch    62 | loss: 5.3521285CurrentTrain: epoch  3, batch     0 | loss: 5.1046844CurrentTrain: epoch  3, batch     1 | loss: 4.9507885CurrentTrain: epoch  3, batch     2 | loss: 5.0789118CurrentTrain: epoch  3, batch     3 | loss: 5.3114796CurrentTrain: epoch  3, batch     4 | loss: 5.4531651CurrentTrain: epoch  3, batch     5 | loss: 5.1030254CurrentTrain: epoch  3, batch     6 | loss: 4.8367243CurrentTrain: epoch  3, batch     7 | loss: 5.0679922CurrentTrain: epoch  3, batch     8 | loss: 4.7276235CurrentTrain: epoch  3, batch     9 | loss: 5.0569425CurrentTrain: epoch  3, batch    10 | loss: 5.2195997CurrentTrain: epoch  3, batch    11 | loss: 5.0153275CurrentTrain: epoch  3, batch    12 | loss: 4.9276667CurrentTrain: epoch  3, batch    13 | loss: 5.2511539CurrentTrain: epoch  3, batch    14 | loss: 5.2741451CurrentTrain: epoch  3, batch    15 | loss: 6.1352968CurrentTrain: epoch  3, batch    16 | loss: 5.3866105CurrentTrain: epoch  3, batch    17 | loss: 5.5618358CurrentTrain: epoch  3, batch    18 | loss: 5.3862057CurrentTrain: epoch  3, batch    19 | loss: 4.9775739CurrentTrain: epoch  3, batch    20 | loss: 4.9961791CurrentTrain: epoch  3, batch    21 | loss: 5.1398373CurrentTrain: epoch  3, batch    22 | loss: 4.6790800CurrentTrain: epoch  3, batch    23 | loss: 4.4825225CurrentTrain: epoch  3, batch    24 | loss: 5.0584378CurrentTrain: epoch  3, batch    25 | loss: 4.7972775CurrentTrain: epoch  3, batch    26 | loss: 5.1756496CurrentTrain: epoch  3, batch    27 | loss: 7.3066974CurrentTrain: epoch  3, batch    28 | loss: 5.0355482CurrentTrain: epoch  3, batch    29 | loss: 4.9685745CurrentTrain: epoch  3, batch    30 | loss: 4.8815126CurrentTrain: epoch  3, batch    31 | loss: 5.1622658CurrentTrain: epoch  3, batch    32 | loss: 4.8767781CurrentTrain: epoch  3, batch    33 | loss: 4.7356791CurrentTrain: epoch  3, batch    34 | loss: 5.0261636CurrentTrain: epoch  3, batch    35 | loss: 4.7006407CurrentTrain: epoch  3, batch    36 | loss: 4.6493115CurrentTrain: epoch  3, batch    37 | loss: 5.0392771CurrentTrain: epoch  3, batch    38 | loss: 5.0050731CurrentTrain: epoch  3, batch    39 | loss: 5.2940350CurrentTrain: epoch  3, batch    40 | loss: 5.4728937CurrentTrain: epoch  3, batch    41 | loss: 5.2939625CurrentTrain: epoch  3, batch    42 | loss: 4.6190023CurrentTrain: epoch  3, batch    43 | loss: 5.1891909CurrentTrain: epoch  3, batch    44 | loss: 4.7517271CurrentTrain: epoch  3, batch    45 | loss: 4.9574203CurrentTrain: epoch  3, batch    46 | loss: 4.5295486CurrentTrain: epoch  3, batch    47 | loss: 5.0338774CurrentTrain: epoch  3, batch    48 | loss: 4.7826614CurrentTrain: epoch  3, batch    49 | loss: 5.0545406CurrentTrain: epoch  3, batch    50 | loss: 4.9820337CurrentTrain: epoch  3, batch    51 | loss: 5.0878944CurrentTrain: epoch  3, batch    52 | loss: 4.8249526CurrentTrain: epoch  3, batch    53 | loss: 4.9799662CurrentTrain: epoch  3, batch    54 | loss: 4.8627386CurrentTrain: epoch  3, batch    55 | loss: 5.4020538CurrentTrain: epoch  3, batch    56 | loss: 4.5064378CurrentTrain: epoch  3, batch    57 | loss: 4.7450233CurrentTrain: epoch  3, batch    58 | loss: 5.0294867CurrentTrain: epoch  3, batch    59 | loss: 4.9546909CurrentTrain: epoch  3, batch    60 | loss: 4.5740528CurrentTrain: epoch  3, batch    61 | loss: 4.9511652CurrentTrain: epoch  3, batch    62 | loss: 4.8592844CurrentTrain: epoch  4, batch     0 | loss: 4.8159904CurrentTrain: epoch  4, batch     1 | loss: 5.2966700CurrentTrain: epoch  4, batch     2 | loss: 4.5615120CurrentTrain: epoch  4, batch     3 | loss: 4.9387488CurrentTrain: epoch  4, batch     4 | loss: 4.7161212CurrentTrain: epoch  4, batch     5 | loss: 4.6673231CurrentTrain: epoch  4, batch     6 | loss: 4.5579844CurrentTrain: epoch  4, batch     7 | loss: 4.7770109CurrentTrain: epoch  4, batch     8 | loss: 4.5696464CurrentTrain: epoch  4, batch     9 | loss: 5.0109062CurrentTrain: epoch  4, batch    10 | loss: 4.5242271CurrentTrain: epoch  4, batch    11 | loss: 5.2469573CurrentTrain: epoch  4, batch    12 | loss: 4.4981880CurrentTrain: epoch  4, batch    13 | loss: 4.5612087CurrentTrain: epoch  4, batch    14 | loss: 4.5339108CurrentTrain: epoch  4, batch    15 | loss: 4.6390123CurrentTrain: epoch  4, batch    16 | loss: 4.8679223CurrentTrain: epoch  4, batch    17 | loss: 4.8430901CurrentTrain: epoch  4, batch    18 | loss: 4.7280130CurrentTrain: epoch  4, batch    19 | loss: 4.7002268CurrentTrain: epoch  4, batch    20 | loss: 4.5782137CurrentTrain: epoch  4, batch    21 | loss: 4.5690851CurrentTrain: epoch  4, batch    22 | loss: 4.7650309CurrentTrain: epoch  4, batch    23 | loss: 4.5915585CurrentTrain: epoch  4, batch    24 | loss: 4.4581628CurrentTrain: epoch  4, batch    25 | loss: 4.5115099CurrentTrain: epoch  4, batch    26 | loss: 4.4550858CurrentTrain: epoch  4, batch    27 | loss: 4.6365743CurrentTrain: epoch  4, batch    28 | loss: 4.5009995CurrentTrain: epoch  4, batch    29 | loss: 4.6835842CurrentTrain: epoch  4, batch    30 | loss: 4.5847664CurrentTrain: epoch  4, batch    31 | loss: 4.6090598CurrentTrain: epoch  4, batch    32 | loss: 4.4191837CurrentTrain: epoch  4, batch    33 | loss: 4.7410359CurrentTrain: epoch  4, batch    34 | loss: 4.4982781CurrentTrain: epoch  4, batch    35 | loss: 4.5012674CurrentTrain: epoch  4, batch    36 | loss: 4.7798600CurrentTrain: epoch  4, batch    37 | loss: 4.3542080CurrentTrain: epoch  4, batch    38 | loss: 4.4343071CurrentTrain: epoch  4, batch    39 | loss: 4.4441185CurrentTrain: epoch  4, batch    40 | loss: 4.6870308CurrentTrain: epoch  4, batch    41 | loss: 4.3326092CurrentTrain: epoch  4, batch    42 | loss: 4.3734436CurrentTrain: epoch  4, batch    43 | loss: 4.7098227CurrentTrain: epoch  4, batch    44 | loss: 4.6742449CurrentTrain: epoch  4, batch    45 | loss: 4.7778144CurrentTrain: epoch  4, batch    46 | loss: 4.5006967CurrentTrain: epoch  4, batch    47 | loss: 4.4367380CurrentTrain: epoch  4, batch    48 | loss: 4.3713846CurrentTrain: epoch  4, batch    49 | loss: 4.4964933CurrentTrain: epoch  4, batch    50 | loss: 4.2798615CurrentTrain: epoch  4, batch    51 | loss: 4.3285236CurrentTrain: epoch  4, batch    52 | loss: 4.6562219CurrentTrain: epoch  4, batch    53 | loss: 4.4834213CurrentTrain: epoch  4, batch    54 | loss: 4.3050976CurrentTrain: epoch  4, batch    55 | loss: 4.5698090CurrentTrain: epoch  4, batch    56 | loss: 4.3312759CurrentTrain: epoch  4, batch    57 | loss: 4.3385706CurrentTrain: epoch  4, batch    58 | loss: 4.5945907CurrentTrain: epoch  4, batch    59 | loss: 4.3391771CurrentTrain: epoch  4, batch    60 | loss: 4.2546229CurrentTrain: epoch  4, batch    61 | loss: 4.2959051CurrentTrain: epoch  4, batch    62 | loss: 4.3664207CurrentTrain: epoch  5, batch     0 | loss: 4.3618369CurrentTrain: epoch  5, batch     1 | loss: 4.2984934CurrentTrain: epoch  5, batch     2 | loss: 4.3297491CurrentTrain: epoch  5, batch     3 | loss: 4.6471181CurrentTrain: epoch  5, batch     4 | loss: 4.3193159CurrentTrain: epoch  5, batch     5 | loss: 4.3784466CurrentTrain: epoch  5, batch     6 | loss: 4.3221283CurrentTrain: epoch  5, batch     7 | loss: 4.2726631CurrentTrain: epoch  5, batch     8 | loss: 4.2760072CurrentTrain: epoch  5, batch     9 | loss: 4.3843031CurrentTrain: epoch  5, batch    10 | loss: 4.3367376CurrentTrain: epoch  5, batch    11 | loss: 4.2899175CurrentTrain: epoch  5, batch    12 | loss: 4.4206314CurrentTrain: epoch  5, batch    13 | loss: 4.2148709CurrentTrain: epoch  5, batch    14 | loss: 4.3793979CurrentTrain: epoch  5, batch    15 | loss: 4.1842766CurrentTrain: epoch  5, batch    16 | loss: 4.2654486CurrentTrain: epoch  5, batch    17 | loss: 4.2091632CurrentTrain: epoch  5, batch    18 | loss: 4.3486457CurrentTrain: epoch  5, batch    19 | loss: 4.5344377CurrentTrain: epoch  5, batch    20 | loss: 4.1927605CurrentTrain: epoch  5, batch    21 | loss: 4.3426313CurrentTrain: epoch  5, batch    22 | loss: 4.4053502CurrentTrain: epoch  5, batch    23 | loss: 4.2827253CurrentTrain: epoch  5, batch    24 | loss: 4.4828348CurrentTrain: epoch  5, batch    25 | loss: 4.1835456CurrentTrain: epoch  5, batch    26 | loss: 4.3166971CurrentTrain: epoch  5, batch    27 | loss: 4.4147635CurrentTrain: epoch  5, batch    28 | loss: 4.2330823CurrentTrain: epoch  5, batch    29 | loss: 4.2054787CurrentTrain: epoch  5, batch    30 | loss: 4.2578306CurrentTrain: epoch  5, batch    31 | loss: 4.2375641CurrentTrain: epoch  5, batch    32 | loss: 4.6935186CurrentTrain: epoch  5, batch    33 | loss: 4.5676537CurrentTrain: epoch  5, batch    34 | loss: 4.2022676CurrentTrain: epoch  5, batch    35 | loss: 4.1855707CurrentTrain: epoch  5, batch    36 | loss: 4.4088163CurrentTrain: epoch  5, batch    37 | loss: 4.2066355CurrentTrain: epoch  5, batch    38 | loss: 4.2257090CurrentTrain: epoch  5, batch    39 | loss: 4.3512568CurrentTrain: epoch  5, batch    40 | loss: 4.2107344CurrentTrain: epoch  5, batch    41 | loss: 4.8017602CurrentTrain: epoch  5, batch    42 | loss: 4.3233852CurrentTrain: epoch  5, batch    43 | loss: 4.2786264CurrentTrain: epoch  5, batch    44 | loss: 4.2790298CurrentTrain: epoch  5, batch    45 | loss: 4.4546695CurrentTrain: epoch  5, batch    46 | loss: 4.3278346CurrentTrain: epoch  5, batch    47 | loss: 4.2586117CurrentTrain: epoch  5, batch    48 | loss: 4.1459093CurrentTrain: epoch  5, batch    49 | loss: 4.2585106CurrentTrain: epoch  5, batch    50 | loss: 4.3664236CurrentTrain: epoch  5, batch    51 | loss: 4.2670860CurrentTrain: epoch  5, batch    52 | loss: 4.1827202CurrentTrain: epoch  5, batch    53 | loss: 4.2901931CurrentTrain: epoch  5, batch    54 | loss: 4.1979394CurrentTrain: epoch  5, batch    55 | loss: 4.2684216CurrentTrain: epoch  5, batch    56 | loss: 4.2661333CurrentTrain: epoch  5, batch    57 | loss: 4.2059231CurrentTrain: epoch  5, batch    58 | loss: 4.3029437CurrentTrain: epoch  5, batch    59 | loss: 4.3001623CurrentTrain: epoch  5, batch    60 | loss: 4.2379656CurrentTrain: epoch  5, batch    61 | loss: 4.1760459CurrentTrain: epoch  5, batch    62 | loss: 4.1227283CurrentTrain: epoch  6, batch     0 | loss: 4.2114019CurrentTrain: epoch  6, batch     1 | loss: 4.2754011CurrentTrain: epoch  6, batch     2 | loss: 4.2381039CurrentTrain: epoch  6, batch     3 | loss: 4.2168131CurrentTrain: epoch  6, batch     4 | loss: 4.1740308CurrentTrain: epoch  6, batch     5 | loss: 4.2208848CurrentTrain: epoch  6, batch     6 | loss: 4.6297994CurrentTrain: epoch  6, batch     7 | loss: 4.2353201CurrentTrain: epoch  6, batch     8 | loss: 4.1515355CurrentTrain: epoch  6, batch     9 | loss: 4.2094584CurrentTrain: epoch  6, batch    10 | loss: 4.2430229CurrentTrain: epoch  6, batch    11 | loss: 4.1741724CurrentTrain: epoch  6, batch    12 | loss: 4.2125044CurrentTrain: epoch  6, batch    13 | loss: 4.4676933CurrentTrain: epoch  6, batch    14 | loss: 4.2459369CurrentTrain: epoch  6, batch    15 | loss: 4.2054043CurrentTrain: epoch  6, batch    16 | loss: 4.3316240CurrentTrain: epoch  6, batch    17 | loss: 4.3626885CurrentTrain: epoch  6, batch    18 | loss: 4.2701750CurrentTrain: epoch  6, batch    19 | loss: 4.2457151CurrentTrain: epoch  6, batch    20 | loss: 4.2158461CurrentTrain: epoch  6, batch    21 | loss: 4.1834440CurrentTrain: epoch  6, batch    22 | loss: 4.1935248CurrentTrain: epoch  6, batch    23 | loss: 4.2030973CurrentTrain: epoch  6, batch    24 | loss: 4.2970591CurrentTrain: epoch  6, batch    25 | loss: 4.2366076CurrentTrain: epoch  6, batch    26 | loss: 4.1877584CurrentTrain: epoch  6, batch    27 | loss: 4.2009521CurrentTrain: epoch  6, batch    28 | loss: 4.3899384CurrentTrain: epoch  6, batch    29 | loss: 4.2046452CurrentTrain: epoch  6, batch    30 | loss: 4.1506400CurrentTrain: epoch  6, batch    31 | loss: 4.1932020CurrentTrain: epoch  6, batch    32 | loss: 4.2061377CurrentTrain: epoch  6, batch    33 | loss: 4.1754751CurrentTrain: epoch  6, batch    34 | loss: 4.1723261CurrentTrain: epoch  6, batch    35 | loss: 4.1669912CurrentTrain: epoch  6, batch    36 | loss: 4.3634920CurrentTrain: epoch  6, batch    37 | loss: 4.2079945CurrentTrain: epoch  6, batch    38 | loss: 4.4317679CurrentTrain: epoch  6, batch    39 | loss: 4.1584020CurrentTrain: epoch  6, batch    40 | loss: 4.2822304CurrentTrain: epoch  6, batch    41 | loss: 4.1651411CurrentTrain: epoch  6, batch    42 | loss: 4.2130671CurrentTrain: epoch  6, batch    43 | loss: 4.1433887CurrentTrain: epoch  6, batch    44 | loss: 4.2346601CurrentTrain: epoch  6, batch    45 | loss: 4.2108784CurrentTrain: epoch  6, batch    46 | loss: 4.2205606CurrentTrain: epoch  6, batch    47 | loss: 4.3463907CurrentTrain: epoch  6, batch    48 | loss: 4.1671524CurrentTrain: epoch  6, batch    49 | loss: 4.5237484CurrentTrain: epoch  6, batch    50 | loss: 4.1993327CurrentTrain: epoch  6, batch    51 | loss: 4.1800079CurrentTrain: epoch  6, batch    52 | loss: 4.1331682CurrentTrain: epoch  6, batch    53 | loss: 4.1808996CurrentTrain: epoch  6, batch    54 | loss: 4.2257566CurrentTrain: epoch  6, batch    55 | loss: 4.2279105CurrentTrain: epoch  6, batch    56 | loss: 4.2540121CurrentTrain: epoch  6, batch    57 | loss: 4.1869807CurrentTrain: epoch  6, batch    58 | loss: 4.2682409CurrentTrain: epoch  6, batch    59 | loss: 4.1721153CurrentTrain: epoch  6, batch    60 | loss: 4.2251797CurrentTrain: epoch  6, batch    61 | loss: 4.1492481CurrentTrain: epoch  6, batch    62 | loss: 4.1288214CurrentTrain: epoch  7, batch     0 | loss: 4.1602974CurrentTrain: epoch  7, batch     1 | loss: 4.1059561CurrentTrain: epoch  7, batch     2 | loss: 4.2134452CurrentTrain: epoch  7, batch     3 | loss: 4.1861134CurrentTrain: epoch  7, batch     4 | loss: 4.2045403CurrentTrain: epoch  7, batch     5 | loss: 4.1456747CurrentTrain: epoch  7, batch     6 | loss: 4.1558428CurrentTrain: epoch  7, batch     7 | loss: 4.1326866CurrentTrain: epoch  7, batch     8 | loss: 4.1931267CurrentTrain: epoch  7, batch     9 | loss: 4.1535048CurrentTrain: epoch  7, batch    10 | loss: 4.1570683CurrentTrain: epoch  7, batch    11 | loss: 4.2379160CurrentTrain: epoch  7, batch    12 | loss: 4.2890239CurrentTrain: epoch  7, batch    13 | loss: 4.1771202CurrentTrain: epoch  7, batch    14 | loss: 4.1479111CurrentTrain: epoch  7, batch    15 | loss: 4.2045336CurrentTrain: epoch  7, batch    16 | loss: 4.1532898CurrentTrain: epoch  7, batch    17 | loss: 4.1501007CurrentTrain: epoch  7, batch    18 | loss: 4.1117764CurrentTrain: epoch  7, batch    19 | loss: 4.1494389CurrentTrain: epoch  7, batch    20 | loss: 4.1453958CurrentTrain: epoch  7, batch    21 | loss: 4.1365013CurrentTrain: epoch  7, batch    22 | loss: 4.0950503CurrentTrain: epoch  7, batch    23 | loss: 4.1512332CurrentTrain: epoch  7, batch    24 | loss: 4.4506779CurrentTrain: epoch  7, batch    25 | loss: 4.1607685CurrentTrain: epoch  7, batch    26 | loss: 4.1719885CurrentTrain: epoch  7, batch    27 | loss: 4.1197572CurrentTrain: epoch  7, batch    28 | loss: 4.0805311CurrentTrain: epoch  7, batch    29 | loss: 4.1425266CurrentTrain: epoch  7, batch    30 | loss: 4.1159716CurrentTrain: epoch  7, batch    31 | loss: 4.1295242CurrentTrain: epoch  7, batch    32 | loss: 4.2202244CurrentTrain: epoch  7, batch    33 | loss: 4.1093559CurrentTrain: epoch  7, batch    34 | loss: 4.1169591CurrentTrain: epoch  7, batch    35 | loss: 4.3915777CurrentTrain: epoch  7, batch    36 | loss: 4.1080170CurrentTrain: epoch  7, batch    37 | loss: 4.1327963CurrentTrain: epoch  7, batch    38 | loss: 4.1652088CurrentTrain: epoch  7, batch    39 | loss: 4.2407045CurrentTrain: epoch  7, batch    40 | loss: 4.1982212CurrentTrain: epoch  7, batch    41 | loss: 4.1364355CurrentTrain: epoch  7, batch    42 | loss: 4.1659956CurrentTrain: epoch  7, batch    43 | loss: 4.2021232CurrentTrain: epoch  7, batch    44 | loss: 4.0970287CurrentTrain: epoch  7, batch    45 | loss: 4.1616898CurrentTrain: epoch  7, batch    46 | loss: 4.1820860CurrentTrain: epoch  7, batch    47 | loss: 4.1313992CurrentTrain: epoch  7, batch    48 | loss: 4.1478963CurrentTrain: epoch  7, batch    49 | loss: 4.1454391CurrentTrain: epoch  7, batch    50 | loss: 4.1156077CurrentTrain: epoch  7, batch    51 | loss: 4.1568618CurrentTrain: epoch  7, batch    52 | loss: 4.1176052CurrentTrain: epoch  7, batch    53 | loss: 4.1044102CurrentTrain: epoch  7, batch    54 | loss: 4.1808615CurrentTrain: epoch  7, batch    55 | loss: 4.1307688CurrentTrain: epoch  7, batch    56 | loss: 4.1429043CurrentTrain: epoch  7, batch    57 | loss: 4.0925655CurrentTrain: epoch  7, batch    58 | loss: 4.0903282CurrentTrain: epoch  7, batch    59 | loss: 4.2297039CurrentTrain: epoch  7, batch    60 | loss: 4.0921507CurrentTrain: epoch  7, batch    61 | loss: 4.0230618CurrentTrain: epoch  7, batch    62 | loss: 4.1256199CurrentTrain: epoch  8, batch     0 | loss: 4.0838771CurrentTrain: epoch  8, batch     1 | loss: 4.1112242CurrentTrain: epoch  8, batch     2 | loss: 4.0654507CurrentTrain: epoch  8, batch     3 | loss: 4.0310116CurrentTrain: epoch  8, batch     4 | loss: 4.0596147CurrentTrain: epoch  8, batch     5 | loss: 4.0674863CurrentTrain: epoch  8, batch     6 | loss: 4.0443583CurrentTrain: epoch  8, batch     7 | loss: 4.1406722CurrentTrain: epoch  8, batch     8 | loss: 4.0792651CurrentTrain: epoch  8, batch     9 | loss: 4.1336117CurrentTrain: epoch  8, batch    10 | loss: 4.0681438CurrentTrain: epoch  8, batch    11 | loss: 4.1170096CurrentTrain: epoch  8, batch    12 | loss: 4.0816779CurrentTrain: epoch  8, batch    13 | loss: 4.1009402CurrentTrain: epoch  8, batch    14 | loss: 4.1490583CurrentTrain: epoch  8, batch    15 | loss: 4.0672035CurrentTrain: epoch  8, batch    16 | loss: 4.0731392CurrentTrain: epoch  8, batch    17 | loss: 4.1153755CurrentTrain: epoch  8, batch    18 | loss: 4.1505003CurrentTrain: epoch  8, batch    19 | loss: 4.0803261CurrentTrain: epoch  8, batch    20 | loss: 4.0602551CurrentTrain: epoch  8, batch    21 | loss: 4.0293779CurrentTrain: epoch  8, batch    22 | loss: 4.1752238CurrentTrain: epoch  8, batch    23 | loss: 4.1746778CurrentTrain: epoch  8, batch    24 | loss: 4.1070285CurrentTrain: epoch  8, batch    25 | loss: 4.0534492CurrentTrain: epoch  8, batch    26 | loss: 4.1108208CurrentTrain: epoch  8, batch    27 | loss: 4.0982785CurrentTrain: epoch  8, batch    28 | loss: 4.0590844CurrentTrain: epoch  8, batch    29 | loss: 4.1246290CurrentTrain: epoch  8, batch    30 | loss: 4.1308556CurrentTrain: epoch  8, batch    31 | loss: 4.0537004CurrentTrain: epoch  8, batch    32 | loss: 4.1343737CurrentTrain: epoch  8, batch    33 | loss: 4.2027402CurrentTrain: epoch  8, batch    34 | loss: 4.0441871CurrentTrain: epoch  8, batch    35 | loss: 4.0756350CurrentTrain: epoch  8, batch    36 | loss: 4.0988936CurrentTrain: epoch  8, batch    37 | loss: 4.0637169CurrentTrain: epoch  8, batch    38 | loss: 4.1193361CurrentTrain: epoch  8, batch    39 | loss: 4.0896769CurrentTrain: epoch  8, batch    40 | loss: 4.0932560CurrentTrain: epoch  8, batch    41 | loss: 4.0625386CurrentTrain: epoch  8, batch    42 | loss: 4.1068277CurrentTrain: epoch  8, batch    43 | loss: 4.0641747CurrentTrain: epoch  8, batch    44 | loss: 4.1196642CurrentTrain: epoch  8, batch    45 | loss: 4.0625472CurrentTrain: epoch  8, batch    46 | loss: 4.0509710CurrentTrain: epoch  8, batch    47 | loss: 4.1082525CurrentTrain: epoch  8, batch    48 | loss: 4.0733352CurrentTrain: epoch  8, batch    49 | loss: 4.0537853CurrentTrain: epoch  8, batch    50 | loss: 4.0807090CurrentTrain: epoch  8, batch    51 | loss: 4.0761127CurrentTrain: epoch  8, batch    52 | loss: 4.1260271CurrentTrain: epoch  8, batch    53 | loss: 4.0381670CurrentTrain: epoch  8, batch    54 | loss: 4.0425768CurrentTrain: epoch  8, batch    55 | loss: 4.0646887CurrentTrain: epoch  8, batch    56 | loss: 4.0517540CurrentTrain: epoch  8, batch    57 | loss: 4.0789580CurrentTrain: epoch  8, batch    58 | loss: 4.0569849CurrentTrain: epoch  8, batch    59 | loss: 4.0309634CurrentTrain: epoch  8, batch    60 | loss: 4.0501719CurrentTrain: epoch  8, batch    61 | loss: 4.0581641CurrentTrain: epoch  8, batch    62 | loss: 4.0438452CurrentTrain: epoch  9, batch     0 | loss: 4.0861015CurrentTrain: epoch  9, batch     1 | loss: 4.0128059CurrentTrain: epoch  9, batch     2 | loss: 4.0775986CurrentTrain: epoch  9, batch     3 | loss: 4.0743332CurrentTrain: epoch  9, batch     4 | loss: 4.0189662CurrentTrain: epoch  9, batch     5 | loss: 4.0483994CurrentTrain: epoch  9, batch     6 | loss: 4.0066466CurrentTrain: epoch  9, batch     7 | loss: 4.0249372CurrentTrain: epoch  9, batch     8 | loss: 4.0509796CurrentTrain: epoch  9, batch     9 | loss: 4.0493565CurrentTrain: epoch  9, batch    10 | loss: 4.0984674CurrentTrain: epoch  9, batch    11 | loss: 4.0469408CurrentTrain: epoch  9, batch    12 | loss: 4.0554571CurrentTrain: epoch  9, batch    13 | loss: 4.0802622CurrentTrain: epoch  9, batch    14 | loss: 4.0942955CurrentTrain: epoch  9, batch    15 | loss: 4.0849876CurrentTrain: epoch  9, batch    16 | loss: 4.0569830CurrentTrain: epoch  9, batch    17 | loss: 4.1017022CurrentTrain: epoch  9, batch    18 | loss: 4.0029435CurrentTrain: epoch  9, batch    19 | loss: 4.1846662CurrentTrain: epoch  9, batch    20 | loss: 4.0916624CurrentTrain: epoch  9, batch    21 | loss: 4.0179448CurrentTrain: epoch  9, batch    22 | loss: 4.0037413CurrentTrain: epoch  9, batch    23 | loss: 4.0710669CurrentTrain: epoch  9, batch    24 | loss: 3.9895995CurrentTrain: epoch  9, batch    25 | loss: 4.0243449CurrentTrain: epoch  9, batch    26 | loss: 4.0629444CurrentTrain: epoch  9, batch    27 | loss: 4.0339227CurrentTrain: epoch  9, batch    28 | loss: 4.0330935CurrentTrain: epoch  9, batch    29 | loss: 4.0479493CurrentTrain: epoch  9, batch    30 | loss: 4.0467219CurrentTrain: epoch  9, batch    31 | loss: 4.0272546CurrentTrain: epoch  9, batch    32 | loss: 4.0401492CurrentTrain: epoch  9, batch    33 | loss: 4.0160213CurrentTrain: epoch  9, batch    34 | loss: 4.0193491CurrentTrain: epoch  9, batch    35 | loss: 4.0581102CurrentTrain: epoch  9, batch    36 | loss: 4.0398374CurrentTrain: epoch  9, batch    37 | loss: 4.0552416CurrentTrain: epoch  9, batch    38 | loss: 4.0213346CurrentTrain: epoch  9, batch    39 | loss: 4.0273333CurrentTrain: epoch  9, batch    40 | loss: 3.9986882CurrentTrain: epoch  9, batch    41 | loss: 4.0138755CurrentTrain: epoch  9, batch    42 | loss: 4.0207696CurrentTrain: epoch  9, batch    43 | loss: 4.0226450CurrentTrain: epoch  9, batch    44 | loss: 4.0418148CurrentTrain: epoch  9, batch    45 | loss: 4.0381875CurrentTrain: epoch  9, batch    46 | loss: 4.0531683CurrentTrain: epoch  9, batch    47 | loss: 4.0246305CurrentTrain: epoch  9, batch    48 | loss: 4.0119429CurrentTrain: epoch  9, batch    49 | loss: 3.9927974CurrentTrain: epoch  9, batch    50 | loss: 4.0164776CurrentTrain: epoch  9, batch    51 | loss: 4.0231237CurrentTrain: epoch  9, batch    52 | loss: 4.0190468CurrentTrain: epoch  9, batch    53 | loss: 4.0306973CurrentTrain: epoch  9, batch    54 | loss: 4.0138617CurrentTrain: epoch  9, batch    55 | loss: 3.9788706CurrentTrain: epoch  9, batch    56 | loss: 4.0451393CurrentTrain: epoch  9, batch    57 | loss: 4.0368657CurrentTrain: epoch  9, batch    58 | loss: 4.0136571CurrentTrain: epoch  9, batch    59 | loss: 4.0636907CurrentTrain: epoch  9, batch    60 | loss: 4.0867562CurrentTrain: epoch  9, batch    61 | loss: 4.0202837CurrentTrain: epoch  9, batch    62 | loss: 3.9944105
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: while the milanese painting scene was dominated by neoclassic painters andrea appiani and luigi sabatelli , giovannni migliara stayed with the historical themes and medieval subtlety of romanticism .
Head Entity: andrea appiani
Tail Entity: neoclassic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the surrealist movement gained prominence, with artists like salvador dalí and max ernst pushing the boundaries of imagination and reality.  
Head Entity: salvador dalí  
Tail Entity: surrealist  

Relation: movement  
Context: the beat generation was a literary movement that emerged in the 1950s, characterized by writers such as jack kerouac and allen ginsberg who challenged conventional norms.  
Head Entity: jack kerouac  
Tail Entity: beat generation  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The historic town of Bath is famous for its Roman baths and is situated within the unitary authority of Bath and North East Somerset.  
Head Entity: Bath  
Tail Entity: Bath and North East Somerset  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the attendees as the prime minister of canada, outlining his government's priorities for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: although joining x - factor offered polaris the chance to reunite with havok , their relationship remained largely unresolved .
Head Entity: havok
Tail Entity: x - factor
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of hard work and dedication, Sarah was finally accepted into the prestigious National Art Society, where she could showcase her talent alongside other renowned artists.  
Head Entity: Sarah  
Tail Entity: National Art Society  

Relation: member of  
Context: The band announced that they would be performing at the annual music festival, marking their return to the stage as members of the popular rock group, The Echoes.  
Head Entity: The Echoes  
Tail Entity: annual music festival  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the belleville turnpike bridge ( also known as the rutgers street bridge ) crosses the passaic river , connecting belleville to north arlington .
Head Entity: belleville turnpike bridge
Tail Entity: passaic river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge crosses the golden gate strait, linking san francisco to marin county.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the london bridge crosses the river thames, providing a route between the city of london and southwark.  
Head Entity: london bridge  
Tail Entity: river thames  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the river flows through the teatree waterhole and comet towards its confluence with the nogoa river to form the mackenzie river .
Head Entity: mackenzie river
Tail Entity: nogoa river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The clear waters of the Missouri River merge with the Mississippi River, creating a vast network of waterways.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: As the sun sets, the small creek flows into the larger lake, providing a vital source of water for the surrounding ecosystem.  
Head Entity: creek  
Tail Entity: lake  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: the result was widely interpreted as a personal rebuke to then chief minister shane stone .
Head Entity: shane stone
Tail Entity: chief minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After serving as the mayor for over a decade, John Smith announced his retirement from the position.  
Head Entity: John Smith  
Tail Entity: mayor  

Relation: position held  
Context: The former president of the organization, Maria Lopez, was recognized for her contributions during her tenure.  
Head Entity: Maria Lopez  
Tail Entity: president
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: released by looking glass studios in 1998 and powered by their own in - house developed dark engine , " thief : the dark project " was considered by many to be a revolutionary game .
Head Entity: thief : the dark project
Tail Entity: looking glass studios
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular game "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and released in 2015, receiving critical acclaim for its storytelling and open-world design.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Fortnite," a battle royale game that took the world by storm, was developed by Epic Games and launched in 2017, quickly becoming a cultural phenomenon.  
Head Entity: Fortnite  
Tail Entity: Epic Games  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: neptune is the second studio album by the london - based band the duke spirit and the last with the original lead guitarist , dan higgins .
Head Entity: the duke spirit
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in silicon valley by a group of innovative engineers looking to revolutionize the industry.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was established in new york city, where they quickly gained a following and changed the music scene.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: " shining time station " won a number of awards and significantly increased the popularity of the " thomas " media franchise in the united states .
Head Entity: shining time station
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish "sushi" is traditionally associated with Japan and has gained immense popularity worldwide.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic brand "Guinness" is known for its rich stout beer, which originated in Ireland and is now enjoyed globally.  
Head Entity: Guinness  
Tail Entity: Ireland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
cur_acc:  ['0.9524']
his_acc:  ['0.9524']
CurrentTrain: epoch  0, batch     0 | loss: 7.4974923CurrentTrain: epoch  0, batch     1 | loss: 8.5202293CurrentTrain: epoch  0, batch     2 | loss: 7.0148621CurrentTrain: epoch  0, batch     3 | loss: 7.3921161CurrentTrain: epoch  1, batch     0 | loss: 7.3319364CurrentTrain: epoch  1, batch     1 | loss: 6.6947145CurrentTrain: epoch  1, batch     2 | loss: 7.0990009CurrentTrain: epoch  1, batch     3 | loss: 5.7481194CurrentTrain: epoch  2, batch     0 | loss: 6.7974477CurrentTrain: epoch  2, batch     1 | loss: 6.3578424CurrentTrain: epoch  2, batch     2 | loss: 5.0814180CurrentTrain: epoch  2, batch     3 | loss: 6.3003111CurrentTrain: epoch  3, batch     0 | loss: 5.5487890CurrentTrain: epoch  3, batch     1 | loss: 6.3407688CurrentTrain: epoch  3, batch     2 | loss: 5.6042109CurrentTrain: epoch  3, batch     3 | loss: 4.2640285CurrentTrain: epoch  4, batch     0 | loss: 5.0246043CurrentTrain: epoch  4, batch     1 | loss: 5.4417686CurrentTrain: epoch  4, batch     2 | loss: 5.1182141CurrentTrain: epoch  4, batch     3 | loss: 4.9197903CurrentTrain: epoch  5, batch     0 | loss: 4.5105643CurrentTrain: epoch  5, batch     1 | loss: 4.6918850CurrentTrain: epoch  5, batch     2 | loss: 4.9317350CurrentTrain: epoch  5, batch     3 | loss: 3.3118978CurrentTrain: epoch  6, batch     0 | loss: 3.8378854CurrentTrain: epoch  6, batch     1 | loss: 4.3676243CurrentTrain: epoch  6, batch     2 | loss: 4.2961063CurrentTrain: epoch  6, batch     3 | loss: 6.6506534CurrentTrain: epoch  7, batch     0 | loss: 3.9111338CurrentTrain: epoch  7, batch     1 | loss: 4.1665735CurrentTrain: epoch  7, batch     2 | loss: 4.3054128CurrentTrain: epoch  7, batch     3 | loss: 3.0347657CurrentTrain: epoch  8, batch     0 | loss: 3.7353346CurrentTrain: epoch  8, batch     1 | loss: 3.4494677CurrentTrain: epoch  8, batch     2 | loss: 4.1839046CurrentTrain: epoch  8, batch     3 | loss: 6.2662396CurrentTrain: epoch  9, batch     0 | loss: 3.7996833CurrentTrain: epoch  9, batch     1 | loss: 3.1577654CurrentTrain: epoch  9, batch     2 | loss: 3.8183861CurrentTrain: epoch  9, batch     3 | loss: 4.6760664
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from all over the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: gordeeva was married to the 1998 olympic men 's singles champion , ilia kulik .
Head Entity: ilia kulik
Tail Entity: 1998 olympic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: The renowned scientist Marie Curie was a key participant of the International Congress of Radiology in 1925.  
Head Entity: Marie Curie  
Tail Entity: International Congress of Radiology  

Relation: participant of  
Context: The famous actor Leonardo DiCaprio was a participant of the 2020 Academy Awards, where he presented the Best Picture award.  
Head Entity: Leonardo DiCaprio  
Tail Entity: 2020 Academy Awards  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed drama series "the crown" was brought to life by the visionary director peter morgan, who skillfully navigated the complexities of the royal family.  
Head Entity: the crown  
Tail Entity: peter morgan  

Relation: director  
Context: in the thrilling action movie "inception," the brilliant director christopher nolan crafted a mind-bending narrative that captivated audiences worldwide.  
Head Entity: inception  
Tail Entity: christopher nolan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game developer, Bungie, announced that it had become an independent studio, having previously been owned by Microsoft.  
Head Entity: Bungie  
Tail Entity: Microsoft  

Relation: owned by  
Context: The luxury fashion brand Gucci, known for its high-end products, is owned by the French conglomerate Kering.  
Head Entity: Gucci  
Tail Entity: Kering  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: remains of taxa that can be confidently assigned to istiodactylidae have been found in the uk and china , in rocks dating from the early cretaceous period ( barremian to aptian stage ) .
Head Entity: barremian
Tail Entity: early cretaceous
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The Great Wall of China is a series of fortifications made of various materials, and it is part of the UNESCO World Heritage Sites.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Sites  

Relation: part of  
Context: The human heart is a vital organ that is part of the circulatory system, responsible for pumping blood throughout the body.  
Head Entity: human heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and now it serves as a gallery showcasing his works, attracting visitors from all over the world.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now thrive.  
Head Entity: old factory  
Tail Entity: startups
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: reading town hall was built in several phases between 1786 and 1897 , although the principal facade was designed by alfred waterhouse in 1875 .
Head Entity: reading town hall
Tail Entity: alfred waterhouse
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the new art museum was designed by renowned architect zaha hadid, whose innovative style has transformed modern architecture.  
Head Entity: new art museum  
Tail Entity: zaha hadid  

Relation: architect  
Context: the iconic sydney opera house was the brainchild of architect jørn utzon, who won the design competition in 1957.  
Head Entity: sydney opera house  
Tail Entity: jørn utzon  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: fantcha was born as francelina durão almeida in mindelo on são vicente island , cape verde .
Head Entity: fantcha
Tail Entity: são vicente island
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york city for several years, john decided to move back to his hometown in los angeles.  
Head Entity: john  
Tail Entity: los angeles  

Relation: residence  
Context: the famous author spent most of her life in a quaint cottage located in the picturesque village of haworth.  
Head Entity: the famous author  
Tail Entity: haworth  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: In the world of opera, Giacomo Puccini is best known for his masterpieces, including "La Bohème" and "Tosca."  
Head Entity: Tosca  
Tail Entity: Giacomo Puccini  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: the postseason tournament concluded with the san francisco 49ers defeating the cincinnati bengals in super bowl xvi , 26–21 , on january 24 , 1982 , at the pontiac silverdome in pontiac , michigan .
Head Entity: super bowl xvi
Tail Entity: pontiac silverdome
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: the annual music festival took place in the vibrant city of new orleans, attracting thousands of visitors from around the world.  
Head Entity: music festival  
Tail Entity: new orleans  

Relation: location  
Context: the historic battle was fought at gettysburg, a pivotal moment in the civil war that changed the course of american history.  
Head Entity: battle  
Tail Entity: gettysburg  
Mixup data size:  200
MixupTrain:  epoch  0, batch     0 | loss: 5.8981843MixupTrain:  epoch  0, batch     1 | loss: 6.1842483MixupTrain:  epoch  0, batch     2 | loss: 5.0835069MixupTrain:  epoch  0, batch     3 | loss: 5.5800970MixupTrain:  epoch  0, batch     4 | loss: 5.1988807MixupTrain:  epoch  0, batch     5 | loss: 4.9728678MixupTrain:  epoch  0, batch     6 | loss: 6.2418954MixupTrain:  epoch  0, batch     7 | loss: 5.4622304MixupTrain:  epoch  0, batch     8 | loss: 4.6103767MixupTrain:  epoch  0, batch     9 | loss: 4.5563313MixupTrain:  epoch  0, batch    10 | loss: 5.1542580MixupTrain:  epoch  0, batch    11 | loss: 4.8922828MixupTrain:  epoch  0, batch    12 | loss: 4.4602493
MemoryTrain:  epoch  0, batch     0 | loss: 4.4771819MemoryTrain:  epoch  0, batch     1 | loss: 3.1774013MemoryTrain:  epoch  0, batch     2 | loss: 3.7345076MemoryTrain:  epoch  0, batch     3 | loss: 5.2219043MemoryTrain:  epoch  1, batch     0 | loss: 4.3788443MemoryTrain:  epoch  1, batch     1 | loss: 3.9451303MemoryTrain:  epoch  1, batch     2 | loss: 3.1446509MemoryTrain:  epoch  1, batch     3 | loss: 3.7768388MemoryTrain:  epoch  2, batch     0 | loss: 3.0140855MemoryTrain:  epoch  2, batch     1 | loss: 3.6854889MemoryTrain:  epoch  2, batch     2 | loss: 3.3379366MemoryTrain:  epoch  2, batch     3 | loss: 2.9387245MemoryTrain:  epoch  3, batch     0 | loss: 2.5491710MemoryTrain:  epoch  3, batch     1 | loss: 2.9750028MemoryTrain:  epoch  3, batch     2 | loss: 2.6510060MemoryTrain:  epoch  3, batch     3 | loss: 2.6370211MemoryTrain:  epoch  4, batch     0 | loss: 2.5888793MemoryTrain:  epoch  4, batch     1 | loss: 2.2635307MemoryTrain:  epoch  4, batch     2 | loss: 2.3337402MemoryTrain:  epoch  4, batch     3 | loss: 2.3312035MemoryTrain:  epoch  5, batch     0 | loss: 2.5930552MemoryTrain:  epoch  5, batch     1 | loss: 1.9528744MemoryTrain:  epoch  5, batch     2 | loss: 1.9842103MemoryTrain:  epoch  5, batch     3 | loss: 2.0568473MemoryTrain:  epoch  6, batch     0 | loss: 2.0273037MemoryTrain:  epoch  6, batch     1 | loss: 2.2391906MemoryTrain:  epoch  6, batch     2 | loss: 1.9080237MemoryTrain:  epoch  6, batch     3 | loss: 1.9124212MemoryTrain:  epoch  7, batch     0 | loss: 2.4985952MemoryTrain:  epoch  7, batch     1 | loss: 1.9634049MemoryTrain:  epoch  7, batch     2 | loss: 1.4678833MemoryTrain:  epoch  7, batch     3 | loss: 1.6119652MemoryTrain:  epoch  8, batch     0 | loss: 1.5334055MemoryTrain:  epoch  8, batch     1 | loss: 1.5296216MemoryTrain:  epoch  8, batch     2 | loss: 1.9484625MemoryTrain:  epoch  8, batch     3 | loss: 2.0674148MemoryTrain:  epoch  9, batch     0 | loss: 1.6529871MemoryTrain:  epoch  9, batch     1 | loss: 1.7853972MemoryTrain:  epoch  9, batch     2 | loss: 1.5987577MemoryTrain:  epoch  9, batch     3 | loss: 1.5070996
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 48.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 65.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 69.79%   [EVAL] batch:   24 | acc: 50.00%,  total acc: 69.00%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 67.07%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 63.17%   [EVAL] batch:   28 | acc: 18.75%,  total acc: 61.64%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 60.62%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 59.07%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 58.98%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 60.04%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 61.03%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 62.14%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 62.85%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 63.51%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 64.31%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 65.06%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 65.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 67.26%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 69.31%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 69.95%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 69.92%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 70.28%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 70.50%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 70.10%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 70.40%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 70.25%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 70.34%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 70.29%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 69.83%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 69.28%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 69.48%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 69.36%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 69.46%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 68.75%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 89.20%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 89.13%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.42%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.81%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 89.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.73%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.02%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.54%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.61%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.06%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.27%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.84%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.17%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.49%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 93.63%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 93.39%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.18%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 93.08%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 92.98%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 93.00%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 93.24%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 93.35%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 92.86%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 91.99%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 91.25%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 90.44%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 89.65%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 88.79%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 88.32%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 88.30%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 88.29%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 88.45%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 88.53%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 88.51%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 88.58%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 88.73%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 88.80%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 88.94%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 89.08%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 89.22%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 89.35%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 88.80%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 88.40%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 87.80%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 87.65%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 87.21%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 86.71%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 86.01%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 85.39%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 84.44%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 83.65%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 83.08%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 82.39%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 81.72%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 81.78%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 82.09%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 82.21%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 82.26%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 82.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 82.49%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 82.60%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 82.87%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 82.98%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 83.29%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 83.31%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 83.30%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 83.22%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 83.31%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 83.08%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 82.89%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 82.83%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 82.87%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 82.64%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 82.63%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 82.51%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 82.24%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 81.87%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 81.61%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 81.61%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 81.50%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 81.35%   
cur_acc:  ['0.9524', '0.6875']
his_acc:  ['0.9524', '0.8135']
CurrentTrain: epoch  0, batch     0 | loss: 5.5822396CurrentTrain: epoch  0, batch     1 | loss: 5.9698839CurrentTrain: epoch  0, batch     2 | loss: 5.7234645CurrentTrain: epoch  0, batch     3 | loss: 8.1284056CurrentTrain: epoch  1, batch     0 | loss: 4.9465523CurrentTrain: epoch  1, batch     1 | loss: 4.7699947CurrentTrain: epoch  1, batch     2 | loss: 4.8484521CurrentTrain: epoch  1, batch     3 | loss: 4.8510799CurrentTrain: epoch  2, batch     0 | loss: 4.8430252CurrentTrain: epoch  2, batch     1 | loss: 4.0352888CurrentTrain: epoch  2, batch     2 | loss: 3.6777158CurrentTrain: epoch  2, batch     3 | loss: 3.6603003CurrentTrain: epoch  3, batch     0 | loss: 3.6763687CurrentTrain: epoch  3, batch     1 | loss: 3.4833169CurrentTrain: epoch  3, batch     2 | loss: 4.0120974CurrentTrain: epoch  3, batch     3 | loss: 3.0314746CurrentTrain: epoch  4, batch     0 | loss: 2.9094806CurrentTrain: epoch  4, batch     1 | loss: 3.7260785CurrentTrain: epoch  4, batch     2 | loss: 3.4272311CurrentTrain: epoch  4, batch     3 | loss: 3.1303761CurrentTrain: epoch  5, batch     0 | loss: 3.6208253CurrentTrain: epoch  5, batch     1 | loss: 3.1901526CurrentTrain: epoch  5, batch     2 | loss: 3.2394814CurrentTrain: epoch  5, batch     3 | loss: 2.6277390CurrentTrain: epoch  6, batch     0 | loss: 3.1069355CurrentTrain: epoch  6, batch     1 | loss: 3.1123662CurrentTrain: epoch  6, batch     2 | loss: 2.7904894CurrentTrain: epoch  6, batch     3 | loss: 2.2106466CurrentTrain: epoch  7, batch     0 | loss: 2.3855894CurrentTrain: epoch  7, batch     1 | loss: 2.6172180CurrentTrain: epoch  7, batch     2 | loss: 2.5298491CurrentTrain: epoch  7, batch     3 | loss: 3.7545114CurrentTrain: epoch  8, batch     0 | loss: 2.8725197CurrentTrain: epoch  8, batch     1 | loss: 2.2049189CurrentTrain: epoch  8, batch     2 | loss: 2.3718820CurrentTrain: epoch  8, batch     3 | loss: 4.6610870CurrentTrain: epoch  9, batch     0 | loss: 2.4404719CurrentTrain: epoch  9, batch     1 | loss: 2.2596722CurrentTrain: epoch  9, batch     2 | loss: 2.5697222CurrentTrain: epoch  9, batch     3 | loss: 2.5366902
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama since 1992, and they have two daughters together.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: darlene released , " darlene mccoy " , on may 8 , 2007 with label emi gospel .
Head Entity: darlene mccoy
Tail Entity: emi gospel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: the band released their latest album, "echoes of the past," under the label universal music group in 2021.  
Head Entity: the band  
Tail Entity: universal music group  

Relation: record label  
Context: after signing a new contract, the artist announced that their upcoming single would be distributed by sony music entertainment.  
Head Entity: the artist  
Tail Entity: sony music entertainment  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: the highway was re - aligned in 1963 to serve the port of vancouver , traveling along the columbia river to ridgefield west of vancouver lake onto a roadway that was to be constructed .
Head Entity: port of vancouver
Tail Entity: columbia river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Miami is famous for its beautiful beaches and vibrant nightlife, situated right along the Atlantic Ocean, making it a popular destination for tourists.  
Head Entity: Miami  
Tail Entity: Atlantic Ocean  

Relation: located in or next to body of water  
Context: The quaint village of Portage is nestled beside the serene waters of Lake Michigan, providing stunning views and a peaceful atmosphere for its residents.  
Head Entity: Portage  
Tail Entity: Lake Michigan  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired Instagram, which now operates as a subsidiary under the social media giant.  
Head Entity: Facebook  
Tail Entity: Instagram  

Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to retain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the speaker proudly mentioned that she had recently obtained her citizenship in canada, where she had lived for over a decade.  
Head Entity: the speaker  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: buck moved to austin in the mid-1970s and joined the blues rock group , the fabulous thunderbirds , along with keith ferguson , jimmie vaughan , and kim wilson .
Head Entity: the fabulous thunderbirds
Tail Entity: blues rock
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the album "thriller" by michael jackson is often regarded as a landmark in pop music history, blending elements of rock, funk, and soul.  
Head Entity: michael jackson  
Tail Entity: pop  

Relation: genre  
Context: the film "inception," directed by christopher nolan, is a complex narrative that combines science fiction with psychological thriller elements.  
Head Entity: inception  
Tail Entity: science fiction  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: the club is named after the ave river , which flows through the town and into the atlantic ocean .
Head Entity: ave river
Tail Entity: atlantic ocean
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: The Mississippi River meanders through several states before finally emptying into the Gulf of Mexico.  
Head Entity: Mississippi River  
Tail Entity: Gulf of Mexico  

Relation: mouth of the watercourse  
Context: The Thames River runs through London and eventually reaches the North Sea, providing a vital waterway for the city.  
Head Entity: Thames River  
Tail Entity: North Sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club san diego wave fc in the nwsl.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA Champions League was won by Chelsea, who defeated Manchester City in the final held in Porto.  
Head Entity: 2021  
Tail Entity: UEFA Champions League  

Relation: sports season of league or competition  
Context: The 2019 Cricket World Cup took place in England and Wales, featuring ten teams competing for the title.  
Head Entity: 2019  
Tail Entity: Cricket World Cup  
Mixup data size:  258
MixupTrain:  epoch  0, batch     0 | loss: 3.8531518MixupTrain:  epoch  0, batch     1 | loss: 4.0859369MixupTrain:  epoch  0, batch     2 | loss: 3.1859782MixupTrain:  epoch  0, batch     3 | loss: 3.4157311MixupTrain:  epoch  0, batch     4 | loss: 3.2690942MixupTrain:  epoch  0, batch     5 | loss: 2.7595645MixupTrain:  epoch  0, batch     6 | loss: 3.4510039MixupTrain:  epoch  0, batch     7 | loss: 3.1769528MixupTrain:  epoch  0, batch     8 | loss: 3.2751693MixupTrain:  epoch  0, batch     9 | loss: 2.9194367MixupTrain:  epoch  0, batch    10 | loss: 2.9374501MixupTrain:  epoch  0, batch    11 | loss: 2.9380379MixupTrain:  epoch  0, batch    12 | loss: 3.3438921MixupTrain:  epoch  0, batch    13 | loss: 2.8422763MixupTrain:  epoch  0, batch    14 | loss: 2.6126389MixupTrain:  epoch  0, batch    15 | loss: 3.1212218MixupTrain:  epoch  0, batch    16 | loss: 2.8362620
MemoryTrain:  epoch  0, batch     0 | loss: 3.7453523MemoryTrain:  epoch  0, batch     1 | loss: 2.5198092MemoryTrain:  epoch  0, batch     2 | loss: 3.7209144MemoryTrain:  epoch  0, batch     3 | loss: 2.8088398MemoryTrain:  epoch  0, batch     4 | loss: 3.1870799MemoryTrain:  epoch  0, batch     5 | loss: 4.1443992MemoryTrain:  epoch  1, batch     0 | loss: 3.7920241MemoryTrain:  epoch  1, batch     1 | loss: 2.4668579MemoryTrain:  epoch  1, batch     2 | loss: 3.1918466MemoryTrain:  epoch  1, batch     3 | loss: 2.9793899MemoryTrain:  epoch  1, batch     4 | loss: 2.9728575MemoryTrain:  epoch  1, batch     5 | loss: 1.8982651MemoryTrain:  epoch  2, batch     0 | loss: 2.2894964MemoryTrain:  epoch  2, batch     1 | loss: 2.1324568MemoryTrain:  epoch  2, batch     2 | loss: 2.2421467MemoryTrain:  epoch  2, batch     3 | loss: 2.9054883MemoryTrain:  epoch  2, batch     4 | loss: 2.6609712MemoryTrain:  epoch  2, batch     5 | loss: 2.5327785MemoryTrain:  epoch  3, batch     0 | loss: 1.6351676MemoryTrain:  epoch  3, batch     1 | loss: 2.0543287MemoryTrain:  epoch  3, batch     2 | loss: 2.6310859MemoryTrain:  epoch  3, batch     3 | loss: 2.4042363MemoryTrain:  epoch  3, batch     4 | loss: 2.3453650MemoryTrain:  epoch  3, batch     5 | loss: 2.1988590MemoryTrain:  epoch  4, batch     0 | loss: 2.3138566MemoryTrain:  epoch  4, batch     1 | loss: 2.2665849MemoryTrain:  epoch  4, batch     2 | loss: 1.8373647MemoryTrain:  epoch  4, batch     3 | loss: 1.7594438MemoryTrain:  epoch  4, batch     4 | loss: 2.5031600MemoryTrain:  epoch  4, batch     5 | loss: 1.5821100MemoryTrain:  epoch  5, batch     0 | loss: 2.1411328MemoryTrain:  epoch  5, batch     1 | loss: 1.8872635MemoryTrain:  epoch  5, batch     2 | loss: 1.7309052MemoryTrain:  epoch  5, batch     3 | loss: 2.1968017MemoryTrain:  epoch  5, batch     4 | loss: 1.8397050MemoryTrain:  epoch  5, batch     5 | loss: 1.5176313MemoryTrain:  epoch  6, batch     0 | loss: 1.8379784MemoryTrain:  epoch  6, batch     1 | loss: 1.5781953MemoryTrain:  epoch  6, batch     2 | loss: 1.8540173MemoryTrain:  epoch  6, batch     3 | loss: 1.8249364MemoryTrain:  epoch  6, batch     4 | loss: 1.6767756MemoryTrain:  epoch  6, batch     5 | loss: 1.5527009MemoryTrain:  epoch  7, batch     0 | loss: 1.4833534MemoryTrain:  epoch  7, batch     1 | loss: 1.7696658MemoryTrain:  epoch  7, batch     2 | loss: 1.6072462MemoryTrain:  epoch  7, batch     3 | loss: 1.5149510MemoryTrain:  epoch  7, batch     4 | loss: 1.4913927MemoryTrain:  epoch  7, batch     5 | loss: 1.4964405MemoryTrain:  epoch  8, batch     0 | loss: 1.5526342MemoryTrain:  epoch  8, batch     1 | loss: 1.4001855MemoryTrain:  epoch  8, batch     2 | loss: 1.5036292MemoryTrain:  epoch  8, batch     3 | loss: 1.5534031MemoryTrain:  epoch  8, batch     4 | loss: 1.6417770MemoryTrain:  epoch  8, batch     5 | loss: 1.2998375MemoryTrain:  epoch  9, batch     0 | loss: 1.6086704MemoryTrain:  epoch  9, batch     1 | loss: 1.9169742MemoryTrain:  epoch  9, batch     2 | loss: 1.3055520MemoryTrain:  epoch  9, batch     3 | loss: 1.4618866MemoryTrain:  epoch  9, batch     4 | loss: 1.5500896MemoryTrain:  epoch  9, batch     5 | loss: 1.7619632
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 94.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 90.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 86.31%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 80.77%   [EVAL] batch:   26 | acc: 56.25%,  total acc: 79.86%   [EVAL] batch:   27 | acc: 43.75%,  total acc: 78.57%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 76.94%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 75.62%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 73.79%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 73.48%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 73.35%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 73.21%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 73.26%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 73.48%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 73.52%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 73.40%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 73.59%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 73.48%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 73.51%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 73.84%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 73.58%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 72.64%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 71.47%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 70.21%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 69.53%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 68.62%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 68.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.71%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 84.06%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.37%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.68%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 88.82%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.88%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.97%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 90.20%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 90.49%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 90.56%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 90.69%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 90.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 90.69%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.68%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 90.74%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 90.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.74%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 90.57%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 90.52%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 90.68%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 90.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 90.88%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 90.83%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 90.28%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 89.16%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 88.17%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 87.31%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 86.38%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 85.29%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 84.78%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 84.55%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 84.60%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 84.46%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 84.50%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 84.21%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 84.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 84.50%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 84.62%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 84.65%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 84.77%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 84.95%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 84.38%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 83.66%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 82.96%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 82.50%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 81.90%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 81.25%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 80.47%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 79.92%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 79.31%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 78.57%   [EVAL] batch:   91 | acc: 31.25%,  total acc: 78.06%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 77.42%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 76.80%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 76.97%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 77.21%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 77.38%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 77.61%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 77.71%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 77.81%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 77.97%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 78.34%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 78.49%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 78.63%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 79.05%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 79.07%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 79.15%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 79.24%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 79.09%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 78.95%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 78.97%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 79.09%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 79.01%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 79.03%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 78.94%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 78.70%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 78.46%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 78.28%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 78.25%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 78.23%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 78.10%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 78.40%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 78.56%   [EVAL] batch:  128 | acc: 93.75%,  total acc: 78.68%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 78.80%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 79.12%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 79.29%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 79.31%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 79.37%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 79.47%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 79.53%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 79.50%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 79.42%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 79.52%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 79.45%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:  143 | acc: 93.75%,  total acc: 79.56%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 79.40%   [EVAL] batch:  145 | acc: 62.50%,  total acc: 79.28%   [EVAL] batch:  146 | acc: 56.25%,  total acc: 79.12%   [EVAL] batch:  147 | acc: 50.00%,  total acc: 78.93%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 78.86%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 78.56%   [EVAL] batch:  151 | acc: 56.25%,  total acc: 78.41%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 78.19%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 77.88%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 77.62%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 77.24%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 77.15%   [EVAL] batch:  157 | acc: 75.00%,  total acc: 77.14%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:  159 | acc: 68.75%,  total acc: 77.03%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 77.02%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 77.04%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 77.03%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 76.98%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 77.01%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 76.96%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 76.95%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 77.01%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 76.92%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 76.65%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 76.32%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 75.94%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 75.72%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 75.43%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 75.21%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 75.36%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 75.63%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 75.90%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 76.43%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 76.55%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 76.65%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 76.77%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 76.63%   
cur_acc:  ['0.9524', '0.6875', '0.7371']
his_acc:  ['0.9524', '0.8135', '0.7663']
CurrentTrain: epoch  0, batch     0 | loss: 6.9973202CurrentTrain: epoch  0, batch     1 | loss: 6.1312442CurrentTrain: epoch  0, batch     2 | loss: 6.3876166CurrentTrain: epoch  0, batch     3 | loss: 5.0513964CurrentTrain: epoch  1, batch     0 | loss: 5.9032187CurrentTrain: epoch  1, batch     1 | loss: 5.4564347CurrentTrain: epoch  1, batch     2 | loss: 5.1495891CurrentTrain: epoch  1, batch     3 | loss: 7.8312039CurrentTrain: epoch  2, batch     0 | loss: 4.9823542CurrentTrain: epoch  2, batch     1 | loss: 4.8434935CurrentTrain: epoch  2, batch     2 | loss: 5.1216731CurrentTrain: epoch  2, batch     3 | loss: 5.2574520CurrentTrain: epoch  3, batch     0 | loss: 4.1031837CurrentTrain: epoch  3, batch     1 | loss: 4.2498026CurrentTrain: epoch  3, batch     2 | loss: 4.9712281CurrentTrain: epoch  3, batch     3 | loss: 7.1969385CurrentTrain: epoch  4, batch     0 | loss: 4.5916252CurrentTrain: epoch  4, batch     1 | loss: 3.5762186CurrentTrain: epoch  4, batch     2 | loss: 4.4125404CurrentTrain: epoch  4, batch     3 | loss: 4.6112695CurrentTrain: epoch  5, batch     0 | loss: 4.1490707CurrentTrain: epoch  5, batch     1 | loss: 3.6792521CurrentTrain: epoch  5, batch     2 | loss: 4.5638342CurrentTrain: epoch  5, batch     3 | loss: 5.4986567CurrentTrain: epoch  6, batch     0 | loss: 4.5056100CurrentTrain: epoch  6, batch     1 | loss: 3.3639860CurrentTrain: epoch  6, batch     2 | loss: 3.5740619CurrentTrain: epoch  6, batch     3 | loss: 5.9087334CurrentTrain: epoch  7, batch     0 | loss: 3.7461953CurrentTrain: epoch  7, batch     1 | loss: 3.9385872CurrentTrain: epoch  7, batch     2 | loss: 3.0089700CurrentTrain: epoch  7, batch     3 | loss: 3.7907391CurrentTrain: epoch  8, batch     0 | loss: 3.6288576CurrentTrain: epoch  8, batch     1 | loss: 3.4234953CurrentTrain: epoch  8, batch     2 | loss: 3.3223543CurrentTrain: epoch  8, batch     3 | loss: 1.9251213CurrentTrain: epoch  9, batch     0 | loss: 2.8838418CurrentTrain: epoch  9, batch     1 | loss: 3.4152832CurrentTrain: epoch  9, batch     2 | loss: 3.1960177CurrentTrain: epoch  9, batch     3 | loss: 3.6412473
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: in 2004 the catalan government gave him the george cross .
Head Entity: george cross
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union will affect all member states.  
Head Entity: environmental regulations  
Tail Entity: European Union  

Relation: applies to jurisdiction  
Context: The Supreme Court's ruling has significant implications for the state of California's water rights.  
Head Entity: Supreme Court  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift performed her hit single "Shake It Off" at the Grammy Awards, captivating the audience with her energetic stage presence.  
Head Entity: Shake It Off  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: an emd gp49 is a 4-axle diesel locomotive built by general motors electro - motive division .
Head Entity: emd gp49
Tail Entity: general motors electro - motive division
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: the iphone 13 is a smartphone designed and marketed by apple inc.  
Head Entity: iphone 13  
Tail Entity: apple inc.  

Relation: manufacturer  
Context: the model s is an all-electric sedan produced by tesla, inc.  
Head Entity: model s  
Tail Entity: tesla, inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: norway was represented in the eurovision song contest 2005 by the song " in my dreams " performed by wig wam .
Head Entity: eurovision song contest 2005
Tail Entity: wig wam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: the united states was represented in the 2020 summer olympics by a team of over 600 athletes competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: united states  

Relation: participant  
Context: the annual tech conference featured several startups, including a presentation by innovative solutions, which showcased their latest product.  
Head Entity: annual tech conference  
Tail Entity: innovative solutions  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: This game runs smoothly on multiple operating systems, such as Linux, Windows 10, and the latest version of macOS.  
Head Entity: game  
Tail Entity: Windows 10  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: swartz served as swedish prime minister for only seven months before being forced from power after the swedish elections of 1917 and replaced by the liberal government of nils edén .
Head Entity: swedish elections of 1917
Tail Entity: nils edén
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: in the recent municipal elections, the voters of springfield chose jane doe as their new mayor, marking a significant shift in the city's leadership.  
Head Entity: municipal elections  
Tail Entity: jane doe  

Relation: successful candidate  
Context: after a closely contested race, the citizens of riverdale elected john smith as their representative in the state assembly, defeating the incumbent.  
Head Entity: state assembly elections  
Tail Entity: john smith  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce loyalty to each other, having grown up under the same roof and shared the same parents.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it was evident that both Clara and her brother, James, inherited their parents' artistic talents, showcasing their skills in painting and music.  
Head Entity: Clara  
Tail Entity: James  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: the united states navy is known for its naval operations and maritime security.  
Head Entity: united states navy  
Tail Entity: naval operations  

Relation: military branch  
Context: general patton served in the united states army during world war ii.  
Head Entity: general patton  
Tail Entity: united states army  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: developed on the east bank of the river near the former native village of otstonwakin , the borough is named for andrew montour , the french / native american and son of madame montour .
Head Entity: madame montour
Tail Entity: andrew montour
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: In the small town of Maplewood, Sarah Johnson is known for her dedication to her family, especially her son, Michael, who is a talented musician.  
Head Entity: Sarah Johnson  
Tail Entity: Michael  

Relation: child  
Context: The famous author, J.K. Rowling, often shares stories about her daughter, Jessica, who inspired many characters in her books.  
Head Entity: J.K. Rowling  
Tail Entity: Jessica  
Mixup data size:  319
MixupTrain:  epoch  0, batch     0 | loss: 2.9651421MixupTrain:  epoch  0, batch     1 | loss: 3.3176965MixupTrain:  epoch  0, batch     2 | loss: 3.0846514MixupTrain:  epoch  0, batch     3 | loss: 2.6358394MixupTrain:  epoch  0, batch     4 | loss: 3.3130883MixupTrain:  epoch  0, batch     5 | loss: 2.6975917MixupTrain:  epoch  0, batch     6 | loss: 2.7424403MixupTrain:  epoch  0, batch     7 | loss: 3.0849390MixupTrain:  epoch  0, batch     8 | loss: 2.4098077MixupTrain:  epoch  0, batch     9 | loss: 2.6453523MixupTrain:  epoch  0, batch    10 | loss: 2.8147978MixupTrain:  epoch  0, batch    11 | loss: 2.6055751MixupTrain:  epoch  0, batch    12 | loss: 2.4207046MixupTrain:  epoch  0, batch    13 | loss: 2.2118025MixupTrain:  epoch  0, batch    14 | loss: 2.4558671MixupTrain:  epoch  0, batch    15 | loss: 2.3277634MixupTrain:  epoch  0, batch    16 | loss: 2.2600116MixupTrain:  epoch  0, batch    17 | loss: 2.6173953MixupTrain:  epoch  0, batch    18 | loss: 2.5552825MixupTrain:  epoch  0, batch    19 | loss: 2.2177998
MemoryTrain:  epoch  0, batch     0 | loss: 2.5902808MemoryTrain:  epoch  0, batch     1 | loss: 3.0529380MemoryTrain:  epoch  0, batch     2 | loss: 2.0437157MemoryTrain:  epoch  0, batch     3 | loss: 2.7919834MemoryTrain:  epoch  0, batch     4 | loss: 2.5108402MemoryTrain:  epoch  0, batch     5 | loss: 3.1074700MemoryTrain:  epoch  0, batch     6 | loss: 2.5780780MemoryTrain:  epoch  0, batch     7 | loss: 2.5046506MemoryTrain:  epoch  1, batch     0 | loss: 2.5592012MemoryTrain:  epoch  1, batch     1 | loss: 2.1269996MemoryTrain:  epoch  1, batch     2 | loss: 2.0728617MemoryTrain:  epoch  1, batch     3 | loss: 2.0195837MemoryTrain:  epoch  1, batch     4 | loss: 3.0357189MemoryTrain:  epoch  1, batch     5 | loss: 2.0491076MemoryTrain:  epoch  1, batch     6 | loss: 2.2034440MemoryTrain:  epoch  1, batch     7 | loss: 2.0743127MemoryTrain:  epoch  2, batch     0 | loss: 2.1475880MemoryTrain:  epoch  2, batch     1 | loss: 2.4416804MemoryTrain:  epoch  2, batch     2 | loss: 1.7934247MemoryTrain:  epoch  2, batch     3 | loss: 1.7923056MemoryTrain:  epoch  2, batch     4 | loss: 1.8528936MemoryTrain:  epoch  2, batch     5 | loss: 1.4581352MemoryTrain:  epoch  2, batch     6 | loss: 2.0001435MemoryTrain:  epoch  2, batch     7 | loss: 1.5979996MemoryTrain:  epoch  3, batch     0 | loss: 1.7364553MemoryTrain:  epoch  3, batch     1 | loss: 1.9173690MemoryTrain:  epoch  3, batch     2 | loss: 1.6805222MemoryTrain:  epoch  3, batch     3 | loss: 1.4288304MemoryTrain:  epoch  3, batch     4 | loss: 1.8858616MemoryTrain:  epoch  3, batch     5 | loss: 1.8212539MemoryTrain:  epoch  3, batch     6 | loss: 2.1060395MemoryTrain:  epoch  3, batch     7 | loss: 1.5407051MemoryTrain:  epoch  4, batch     0 | loss: 2.0523787MemoryTrain:  epoch  4, batch     1 | loss: 1.7156751MemoryTrain:  epoch  4, batch     2 | loss: 1.5234993MemoryTrain:  epoch  4, batch     3 | loss: 1.6259708MemoryTrain:  epoch  4, batch     4 | loss: 1.6519217MemoryTrain:  epoch  4, batch     5 | loss: 1.7389221MemoryTrain:  epoch  4, batch     6 | loss: 1.4365215MemoryTrain:  epoch  4, batch     7 | loss: 1.5063132MemoryTrain:  epoch  5, batch     0 | loss: 1.5882375MemoryTrain:  epoch  5, batch     1 | loss: 1.5431218MemoryTrain:  epoch  5, batch     2 | loss: 1.4724112MemoryTrain:  epoch  5, batch     3 | loss: 1.5703740MemoryTrain:  epoch  5, batch     4 | loss: 1.5618396MemoryTrain:  epoch  5, batch     5 | loss: 1.7218533MemoryTrain:  epoch  5, batch     6 | loss: 1.4762502MemoryTrain:  epoch  5, batch     7 | loss: 1.4105612MemoryTrain:  epoch  6, batch     0 | loss: 1.3757424MemoryTrain:  epoch  6, batch     1 | loss: 1.4265709MemoryTrain:  epoch  6, batch     2 | loss: 1.5757006MemoryTrain:  epoch  6, batch     3 | loss: 1.6261427MemoryTrain:  epoch  6, batch     4 | loss: 1.5163515MemoryTrain:  epoch  6, batch     5 | loss: 1.5012014MemoryTrain:  epoch  6, batch     6 | loss: 1.3719614MemoryTrain:  epoch  6, batch     7 | loss: 1.3982780MemoryTrain:  epoch  7, batch     0 | loss: 1.3510350MemoryTrain:  epoch  7, batch     1 | loss: 1.4008688MemoryTrain:  epoch  7, batch     2 | loss: 1.3469892MemoryTrain:  epoch  7, batch     3 | loss: 1.2988720MemoryTrain:  epoch  7, batch     4 | loss: 1.5241410MemoryTrain:  epoch  7, batch     5 | loss: 1.4962951MemoryTrain:  epoch  7, batch     6 | loss: 1.5253655MemoryTrain:  epoch  7, batch     7 | loss: 1.5025882MemoryTrain:  epoch  8, batch     0 | loss: 1.3668664MemoryTrain:  epoch  8, batch     1 | loss: 1.4458313MemoryTrain:  epoch  8, batch     2 | loss: 1.3605357MemoryTrain:  epoch  8, batch     3 | loss: 1.4240198MemoryTrain:  epoch  8, batch     4 | loss: 1.2573278MemoryTrain:  epoch  8, batch     5 | loss: 1.3123982MemoryTrain:  epoch  8, batch     6 | loss: 1.4456277MemoryTrain:  epoch  8, batch     7 | loss: 1.5714867MemoryTrain:  epoch  9, batch     0 | loss: 1.3161755MemoryTrain:  epoch  9, batch     1 | loss: 1.2789898MemoryTrain:  epoch  9, batch     2 | loss: 1.3624709MemoryTrain:  epoch  9, batch     3 | loss: 1.3062289MemoryTrain:  epoch  9, batch     4 | loss: 1.4100592MemoryTrain:  epoch  9, batch     5 | loss: 1.3145711MemoryTrain:  epoch  9, batch     6 | loss: 1.3373479MemoryTrain:  epoch  9, batch     7 | loss: 1.3479483
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 84.87%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 85.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.34%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 87.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.26%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.60%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 88.93%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.24%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.53%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 89.64%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 89.74%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 89.38%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 89.02%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 89.14%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 88.78%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 88.33%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 87.50%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 87.23%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 86.85%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 85.97%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 85.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.03%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.18%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.32%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 86.23%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 86.36%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 86.50%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 85.96%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 85.02%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 84.11%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 83.23%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 82.68%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 82.06%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 81.15%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 80.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.01%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 85.54%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.76%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.15%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 86.35%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.70%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.35%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 87.65%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 87.65%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 87.64%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 87.10%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 86.85%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 86.73%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 86.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 86.40%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 86.42%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.56%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 86.69%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 86.48%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 86.51%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 86.31%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 86.44%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 86.56%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 86.69%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 86.21%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 85.25%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 84.42%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 83.81%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 83.02%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 82.26%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 81.97%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 81.79%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 81.87%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 82.11%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 82.09%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 82.08%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 82.32%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 82.39%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 82.45%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 82.52%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 82.58%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 82.79%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 82.16%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 81.63%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 81.03%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 80.66%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 80.16%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 79.67%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 78.91%   [EVAL] batch:   88 | acc: 18.75%,  total acc: 78.23%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 77.64%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 76.85%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 76.29%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 75.60%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 74.93%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 75.70%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 75.76%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 75.81%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 75.93%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 76.10%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 76.33%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 76.50%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 76.67%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 76.89%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 77.04%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 77.06%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 77.10%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 77.14%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 77.23%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 76.99%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 76.70%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 76.47%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 76.51%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 76.34%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 76.27%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 76.10%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 75.89%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 75.62%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 75.46%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 75.46%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 75.40%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 75.30%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 75.35%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 75.25%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 75.20%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 74.85%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 74.71%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 74.67%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 74.81%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 75.14%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 75.23%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 75.32%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 75.41%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 75.50%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 75.49%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 75.54%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 75.58%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 75.66%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 75.69%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 75.56%   [EVAL] batch:  145 | acc: 62.50%,  total acc: 75.47%   [EVAL] batch:  146 | acc: 43.75%,  total acc: 75.26%   [EVAL] batch:  147 | acc: 43.75%,  total acc: 75.04%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 74.92%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 74.83%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 74.59%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 74.38%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 74.06%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 73.78%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 73.51%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 73.12%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 73.01%   [EVAL] batch:  157 | acc: 68.75%,  total acc: 72.98%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 72.96%   [EVAL] batch:  159 | acc: 68.75%,  total acc: 72.93%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 72.94%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 72.99%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 73.04%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 73.13%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 73.22%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 73.31%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 73.32%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 73.40%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 73.37%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 73.16%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 72.84%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 72.53%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 72.33%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 72.05%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 71.86%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 72.33%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 72.49%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 72.79%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 72.91%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 73.02%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 73.06%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 73.29%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 73.36%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 73.41%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 73.42%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 73.49%   [EVAL] batch:  191 | acc: 87.50%,  total acc: 73.57%   [EVAL] batch:  192 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 73.68%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 73.82%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 73.83%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 73.86%   [EVAL] batch:  198 | acc: 100.00%,  total acc: 73.99%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 74.06%   [EVAL] batch:  200 | acc: 93.75%,  total acc: 74.16%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 74.20%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 74.20%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 74.23%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 74.33%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 74.39%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:  207 | acc: 87.50%,  total acc: 74.55%   [EVAL] batch:  208 | acc: 75.00%,  total acc: 74.55%   [EVAL] batch:  209 | acc: 75.00%,  total acc: 74.55%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 74.88%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 74.97%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 75.09%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 75.17%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.29%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 75.46%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 75.79%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 75.90%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 76.11%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 76.16%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 76.24%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 76.21%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 76.26%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 76.33%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 76.35%   [EVAL] batch:  231 | acc: 56.25%,  total acc: 76.27%   [EVAL] batch:  232 | acc: 56.25%,  total acc: 76.18%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 76.20%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 76.14%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 76.09%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 76.00%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 76.12%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 76.22%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 76.27%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 76.32%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 76.43%   [EVAL] batch:  244 | acc: 31.25%,  total acc: 76.25%   [EVAL] batch:  245 | acc: 25.00%,  total acc: 76.04%   [EVAL] batch:  246 | acc: 31.25%,  total acc: 75.86%   [EVAL] batch:  247 | acc: 37.50%,  total acc: 75.71%   [EVAL] batch:  248 | acc: 50.00%,  total acc: 75.60%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 75.50%   
cur_acc:  ['0.9524', '0.6875', '0.7371', '0.8115']
his_acc:  ['0.9524', '0.8135', '0.7663', '0.7550']
CurrentTrain: epoch  0, batch     0 | loss: 5.0092964CurrentTrain: epoch  0, batch     1 | loss: 5.4778790CurrentTrain: epoch  0, batch     2 | loss: 6.2335701CurrentTrain: epoch  0, batch     3 | loss: 5.4570785CurrentTrain: epoch  1, batch     0 | loss: 4.8603830CurrentTrain: epoch  1, batch     1 | loss: 4.6307240CurrentTrain: epoch  1, batch     2 | loss: 4.7072268CurrentTrain: epoch  1, batch     3 | loss: 4.7977686CurrentTrain: epoch  2, batch     0 | loss: 5.0541015CurrentTrain: epoch  2, batch     1 | loss: 4.5708737CurrentTrain: epoch  2, batch     2 | loss: 3.9752948CurrentTrain: epoch  2, batch     3 | loss: 3.8996286CurrentTrain: epoch  3, batch     0 | loss: 4.0525064CurrentTrain: epoch  3, batch     1 | loss: 4.1064625CurrentTrain: epoch  3, batch     2 | loss: 3.8860688CurrentTrain: epoch  3, batch     3 | loss: 3.8601356CurrentTrain: epoch  4, batch     0 | loss: 3.4162788CurrentTrain: epoch  4, batch     1 | loss: 3.1907635CurrentTrain: epoch  4, batch     2 | loss: 3.8105447CurrentTrain: epoch  4, batch     3 | loss: 5.0496898CurrentTrain: epoch  5, batch     0 | loss: 2.7290947CurrentTrain: epoch  5, batch     1 | loss: 4.1517687CurrentTrain: epoch  5, batch     2 | loss: 3.0712934CurrentTrain: epoch  5, batch     3 | loss: 1.8420280CurrentTrain: epoch  6, batch     0 | loss: 3.0441728CurrentTrain: epoch  6, batch     1 | loss: 3.1593845CurrentTrain: epoch  6, batch     2 | loss: 3.0135357CurrentTrain: epoch  6, batch     3 | loss: 3.7377102CurrentTrain: epoch  7, batch     0 | loss: 3.2212930CurrentTrain: epoch  7, batch     1 | loss: 3.1910768CurrentTrain: epoch  7, batch     2 | loss: 2.6332972CurrentTrain: epoch  7, batch     3 | loss: 2.3681071CurrentTrain: epoch  8, batch     0 | loss: 2.9235942CurrentTrain: epoch  8, batch     1 | loss: 2.6803799CurrentTrain: epoch  8, batch     2 | loss: 2.5662160CurrentTrain: epoch  8, batch     3 | loss: 2.2581587CurrentTrain: epoch  9, batch     0 | loss: 2.7036686CurrentTrain: epoch  9, batch     1 | loss: 2.7613137CurrentTrain: epoch  9, batch     2 | loss: 2.3032978CurrentTrain: epoch  9, batch     3 | loss: 3.7345457
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of the United States are the main political divisions, with California being the most populous state in the country.  
Head Entity: United States  
Tail Entity: California  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: this contrasts with the common usage of harem as an english loan - word , which implies a female - only enclave or seraglio .
Head Entity: harem
Tail Entity: seraglio
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: Some researchers argue that the term "artificial intelligence" is often used interchangeably with "machine learning," despite the latter being a subset of the former.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  

Relation: said to be the same as  
Context: The term "soda" is frequently considered synonymous with "pop" in various regions, although some people argue there are subtle differences in usage.  
Head Entity: soda  
Tail Entity: pop  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: the filming was primarily held at pollachi . it fetched pawan kalyan the filmfare award for best actor – telugu award at 59th filmfare awards south .
Head Entity: filmfare award for best actor – telugu
Tail Entity: pawan kalyan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: in the thrilling finale of the championship, the underdog team triumphed, securing the title and making history as they celebrated their victory at the national sports arena. This remarkable achievement earned them the prestigious trophy for the best team of the year.  
Head Entity: trophy for the best team of the year  
Tail Entity: underdog team  

Relation: winner  
Context: during the annual music awards, the talented singer captivated the audience with her powerful performance, ultimately leading her to win the coveted title of best new artist. The event was a celebration of emerging talent in the industry.  
Head Entity: title of best new artist  
Tail Entity: talented singer  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the railroad car they were deported in was attached to the end of the last train out of drancy which also carried drancy commandant ss hauptsturmführer alois brunner and other german military personnel .
Head Entity: alois brunner
Tail Entity: hauptsturmführer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: General John Smith was awarded the title of Major General for his exceptional leadership during the conflict, which significantly contributed to the victory of his battalion.  
Head Entity: John Smith  
Tail Entity: Major General  

Relation: military rank  
Context: During the ceremony, Colonel Jane Doe was recognized for her outstanding service and was promoted to the rank of Brigadier General, a position she had long aspired to achieve.  
Head Entity: Jane Doe  
Tail Entity: Brigadier General  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: there are seven different nations that are allied or in conflict : prior to its north american release , " vanguard bandits " was titled " detonator gauntlet " by working designs .
Head Entity: vanguard bandits
Tail Entity: working designs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by the acclaimed author was released by Penguin Random House, a well-known publishing house that has produced numerous bestsellers.  
Head Entity: latest novel  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: After years of hard work, the independent game developer finally secured a deal with Devolver Digital, which is recognized for publishing innovative indie games.  
Head Entity: independent game developer  
Tail Entity: Devolver Digital  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " choose you " and " homesick " were released as the album 's second and third singles , respectively , and each attained moderate chart success .
Head Entity: choose you
Tail Entity: homesick
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter delves into the backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: The opening act of the concert was a local band, followed by a well-known pop artist who energized the crowd.  
Head Entity: local band  
Tail Entity: well-known pop artist  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: since then they had been under the supervision of valerand poullain , formerly john calvin 's successor as minister of the french congregation in strasbourg .
Head Entity: john calvin
Tail Entity: strasbourg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: After several years of hard work, the team moved their operations to a new facility in Austin, Texas, to better serve their clients.  
Head Entity: the team  
Tail Entity: Austin, Texas  

Relation: work location  
Context: The renowned artist spent a significant part of her career creating masterpieces in her studio located in the heart of Paris.  
Head Entity: the renowned artist  
Tail Entity: Paris  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme activity.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a former professional athlete, now works as a sports commentator for major networks.  
Head Entity: john smith  
Tail Entity: sports commentator  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: the george boxley cabin , davenport - bradfield house , and sheridan downtown commercial historic district are listed on the national register of historic places .
Head Entity: sheridan downtown commercial historic district
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: the ancient city of petra in jordan is recognized as a unesco world heritage site due to its archaeological significance.  
Head Entity: ancient city of petra  
Tail Entity: unesco world heritage site  

Relation: heritage designation  
Context: the great barrier reef, known for its stunning marine biodiversity, was designated a world heritage site by unesco in 1981.  
Head Entity: great barrier reef  
Tail Entity: world heritage site
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: ada adini ( 1855 – february 1924 ) was an american operatic soprano who had an active international career from 1876 up into the first decade of the 20th century .
Head Entity: ada adini
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12, 1935 – september 6, 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27, 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: soprano  
Mixup data size:  378
MixupTrain:  epoch  0, batch     0 | loss: 1.9585213MixupTrain:  epoch  0, batch     1 | loss: 2.8296795MixupTrain:  epoch  0, batch     2 | loss: 2.9219676MixupTrain:  epoch  0, batch     3 | loss: 2.2131546MixupTrain:  epoch  0, batch     4 | loss: 2.3019513MixupTrain:  epoch  0, batch     5 | loss: 2.2652658MixupTrain:  epoch  0, batch     6 | loss: 2.0247619MixupTrain:  epoch  0, batch     7 | loss: 2.7357308MixupTrain:  epoch  0, batch     8 | loss: 2.2901126MixupTrain:  epoch  0, batch     9 | loss: 2.8784809MixupTrain:  epoch  0, batch    10 | loss: 2.4540622MixupTrain:  epoch  0, batch    11 | loss: 2.1654328MixupTrain:  epoch  0, batch    12 | loss: 2.3537342MixupTrain:  epoch  0, batch    13 | loss: 2.1116092MixupTrain:  epoch  0, batch    14 | loss: 2.4419776MixupTrain:  epoch  0, batch    15 | loss: 2.2855288MixupTrain:  epoch  0, batch    16 | loss: 2.4433714MixupTrain:  epoch  0, batch    17 | loss: 1.8661070MixupTrain:  epoch  0, batch    18 | loss: 1.8250376MixupTrain:  epoch  0, batch    19 | loss: 2.0938636MixupTrain:  epoch  0, batch    20 | loss: 2.2497647MixupTrain:  epoch  0, batch    21 | loss: 2.2618987MixupTrain:  epoch  0, batch    22 | loss: 2.3232983MixupTrain:  epoch  0, batch    23 | loss: 1.9071051
MemoryTrain:  epoch  0, batch     0 | loss: 2.0743027MemoryTrain:  epoch  0, batch     1 | loss: 1.9812422MemoryTrain:  epoch  0, batch     2 | loss: 2.5863523MemoryTrain:  epoch  0, batch     3 | loss: 2.9355733MemoryTrain:  epoch  0, batch     4 | loss: 2.5329990MemoryTrain:  epoch  0, batch     5 | loss: 1.9418087MemoryTrain:  epoch  0, batch     6 | loss: 1.9576663MemoryTrain:  epoch  0, batch     7 | loss: 2.4122043MemoryTrain:  epoch  0, batch     8 | loss: 2.3574846MemoryTrain:  epoch  0, batch     9 | loss: 2.0043230MemoryTrain:  epoch  1, batch     0 | loss: 2.1296463MemoryTrain:  epoch  1, batch     1 | loss: 2.2328598MemoryTrain:  epoch  1, batch     2 | loss: 1.7785101MemoryTrain:  epoch  1, batch     3 | loss: 2.0962057MemoryTrain:  epoch  1, batch     4 | loss: 1.8416643MemoryTrain:  epoch  1, batch     5 | loss: 1.5801387MemoryTrain:  epoch  1, batch     6 | loss: 2.2783430MemoryTrain:  epoch  1, batch     7 | loss: 1.4960420MemoryTrain:  epoch  1, batch     8 | loss: 1.4250567MemoryTrain:  epoch  1, batch     9 | loss: 2.6168296MemoryTrain:  epoch  2, batch     0 | loss: 1.5787352MemoryTrain:  epoch  2, batch     1 | loss: 1.7539946MemoryTrain:  epoch  2, batch     2 | loss: 1.3780110MemoryTrain:  epoch  2, batch     3 | loss: 1.5548184MemoryTrain:  epoch  2, batch     4 | loss: 1.4958562MemoryTrain:  epoch  2, batch     5 | loss: 1.6078131MemoryTrain:  epoch  2, batch     6 | loss: 1.5438271MemoryTrain:  epoch  2, batch     7 | loss: 1.5367843MemoryTrain:  epoch  2, batch     8 | loss: 1.8608183MemoryTrain:  epoch  2, batch     9 | loss: 2.6428556MemoryTrain:  epoch  3, batch     0 | loss: 1.7729346MemoryTrain:  epoch  3, batch     1 | loss: 1.4181206MemoryTrain:  epoch  3, batch     2 | loss: 1.8344524MemoryTrain:  epoch  3, batch     3 | loss: 1.2868609MemoryTrain:  epoch  3, batch     4 | loss: 1.5451386MemoryTrain:  epoch  3, batch     5 | loss: 1.3246272MemoryTrain:  epoch  3, batch     6 | loss: 1.7808222MemoryTrain:  epoch  3, batch     7 | loss: 1.5355809MemoryTrain:  epoch  3, batch     8 | loss: 1.5973017MemoryTrain:  epoch  3, batch     9 | loss: 1.3679714MemoryTrain:  epoch  4, batch     0 | loss: 1.6502094MemoryTrain:  epoch  4, batch     1 | loss: 1.2794530MemoryTrain:  epoch  4, batch     2 | loss: 1.5236030MemoryTrain:  epoch  4, batch     3 | loss: 1.6689709MemoryTrain:  epoch  4, batch     4 | loss: 1.5137217MemoryTrain:  epoch  4, batch     5 | loss: 1.6182560MemoryTrain:  epoch  4, batch     6 | loss: 1.3117318MemoryTrain:  epoch  4, batch     7 | loss: 1.3839312MemoryTrain:  epoch  4, batch     8 | loss: 1.3290914MemoryTrain:  epoch  4, batch     9 | loss: 1.6733723MemoryTrain:  epoch  5, batch     0 | loss: 1.3004729MemoryTrain:  epoch  5, batch     1 | loss: 1.5130670MemoryTrain:  epoch  5, batch     2 | loss: 1.3232226MemoryTrain:  epoch  5, batch     3 | loss: 1.3980999MemoryTrain:  epoch  5, batch     4 | loss: 1.4490821MemoryTrain:  epoch  5, batch     5 | loss: 1.4357181MemoryTrain:  epoch  5, batch     6 | loss: 1.6058314MemoryTrain:  epoch  5, batch     7 | loss: 1.2904112MemoryTrain:  epoch  5, batch     8 | loss: 1.4383689MemoryTrain:  epoch  5, batch     9 | loss: 1.5939980MemoryTrain:  epoch  6, batch     0 | loss: 1.3643209MemoryTrain:  epoch  6, batch     1 | loss: 1.2753377MemoryTrain:  epoch  6, batch     2 | loss: 1.3090154MemoryTrain:  epoch  6, batch     3 | loss: 1.3993137MemoryTrain:  epoch  6, batch     4 | loss: 1.3363125MemoryTrain:  epoch  6, batch     5 | loss: 1.3665714MemoryTrain:  epoch  6, batch     6 | loss: 1.3174689MemoryTrain:  epoch  6, batch     7 | loss: 1.8183743MemoryTrain:  epoch  6, batch     8 | loss: 1.3595155MemoryTrain:  epoch  6, batch     9 | loss: 1.5947466MemoryTrain:  epoch  7, batch     0 | loss: 1.2771878MemoryTrain:  epoch  7, batch     1 | loss: 1.4243222MemoryTrain:  epoch  7, batch     2 | loss: 1.2708601MemoryTrain:  epoch  7, batch     3 | loss: 1.5343988MemoryTrain:  epoch  7, batch     4 | loss: 1.3826158MemoryTrain:  epoch  7, batch     5 | loss: 1.3470011MemoryTrain:  epoch  7, batch     6 | loss: 1.3487097MemoryTrain:  epoch  7, batch     7 | loss: 1.2884581MemoryTrain:  epoch  7, batch     8 | loss: 1.2851949MemoryTrain:  epoch  7, batch     9 | loss: 1.5557232MemoryTrain:  epoch  8, batch     0 | loss: 1.4718971MemoryTrain:  epoch  8, batch     1 | loss: 1.3056564MemoryTrain:  epoch  8, batch     2 | loss: 1.3052170MemoryTrain:  epoch  8, batch     3 | loss: 1.2897513MemoryTrain:  epoch  8, batch     4 | loss: 1.3392028MemoryTrain:  epoch  8, batch     5 | loss: 1.3058734MemoryTrain:  epoch  8, batch     6 | loss: 1.3204761MemoryTrain:  epoch  8, batch     7 | loss: 1.3000797MemoryTrain:  epoch  8, batch     8 | loss: 1.2608864MemoryTrain:  epoch  8, batch     9 | loss: 1.3198546MemoryTrain:  epoch  9, batch     0 | loss: 1.2336581MemoryTrain:  epoch  9, batch     1 | loss: 1.3852705MemoryTrain:  epoch  9, batch     2 | loss: 1.2735872MemoryTrain:  epoch  9, batch     3 | loss: 1.2992884MemoryTrain:  epoch  9, batch     4 | loss: 1.3245766MemoryTrain:  epoch  9, batch     5 | loss: 1.2856698MemoryTrain:  epoch  9, batch     6 | loss: 1.4223286MemoryTrain:  epoch  9, batch     7 | loss: 1.2955954MemoryTrain:  epoch  9, batch     8 | loss: 1.2738132MemoryTrain:  epoch  9, batch     9 | loss: 1.2458537
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 61.25%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 61.33%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 61.03%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 60.07%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 61.84%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 63.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 67.93%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 70.43%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 70.37%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 70.04%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 70.21%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 70.36%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 70.51%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 70.27%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 67.86%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 66.49%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 65.20%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 66.72%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 67.23%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 68.02%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 68.47%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 68.61%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 69.02%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 69.02%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 69.27%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 69.52%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 71.58%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.46%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 73.81%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 74.15%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 75.00%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 77.45%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.86%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 77.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.39%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.65%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 84.54%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.94%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.31%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 85.52%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 85.57%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 85.61%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.65%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 85.14%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 84.78%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 84.18%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 83.46%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 82.91%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 82.00%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 81.86%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 81.73%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 81.60%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 80.80%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 80.58%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 80.48%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 80.50%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 80.72%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 80.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 81.35%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 80.95%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 79.98%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 79.33%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 78.79%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 78.17%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 77.48%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 77.36%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 77.32%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 77.55%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 77.69%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 77.83%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 77.87%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 78.00%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 78.21%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 78.17%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 78.48%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 78.67%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 78.43%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 77.86%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 77.31%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 77.06%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 76.67%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 76.15%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 75.43%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 74.65%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 73.96%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 73.15%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 72.55%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 71.84%   [EVAL] batch:   93 | acc: 6.25%,  total acc: 71.14%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 71.55%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 71.84%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 72.07%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 72.16%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 72.19%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 72.40%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 72.61%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 72.82%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 73.02%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 73.31%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 72.80%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 72.36%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 71.76%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 71.51%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 71.04%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 70.69%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 70.45%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 70.33%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 70.47%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 70.25%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 70.18%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 70.06%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 69.90%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 69.73%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 69.62%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 69.56%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 69.61%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 69.55%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 69.54%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 69.63%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 69.38%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 69.47%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 69.60%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 69.96%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 70.17%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 70.26%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 70.29%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 70.28%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 70.13%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 69.95%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 69.81%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 69.71%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 69.62%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 69.48%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 69.22%   [EVAL] batch:  146 | acc: 37.50%,  total acc: 69.01%   [EVAL] batch:  147 | acc: 25.00%,  total acc: 68.71%   [EVAL] batch:  148 | acc: 37.50%,  total acc: 68.50%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 68.42%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 68.21%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 68.05%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 67.73%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 67.53%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 67.34%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 67.07%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 67.00%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 66.97%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 66.98%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 66.91%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 67.01%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 67.10%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 67.31%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 67.39%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 67.44%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 67.60%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 67.60%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 67.39%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 67.11%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 66.82%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 66.65%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 66.42%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 66.25%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 66.63%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 67.15%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 67.48%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 67.55%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 67.66%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 67.94%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 68.08%   [EVAL] batch:  187 | acc: 93.75%,  total acc: 68.22%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 68.25%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 68.26%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 68.29%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:  192 | acc: 87.50%,  total acc: 68.43%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 68.46%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 68.53%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 68.53%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 68.53%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 68.62%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 68.69%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 68.81%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 68.87%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 68.87%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 68.96%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 69.09%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:  207 | acc: 75.00%,  total acc: 69.26%   [EVAL] batch:  208 | acc: 68.75%,  total acc: 69.26%   [EVAL] batch:  209 | acc: 68.75%,  total acc: 69.26%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 69.34%   [EVAL] batch:  211 | acc: 93.75%,  total acc: 69.46%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 69.54%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 69.88%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 70.13%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 70.21%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 71.05%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 71.12%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 71.11%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 71.18%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 71.28%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 71.32%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 71.31%   [EVAL] batch:  232 | acc: 56.25%,  total acc: 71.24%   [EVAL] batch:  233 | acc: 75.00%,  total acc: 71.26%   [EVAL] batch:  234 | acc: 56.25%,  total acc: 71.20%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 71.16%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 71.07%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 71.23%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 71.42%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 71.49%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 71.55%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 71.57%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 71.43%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 71.27%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 71.15%   [EVAL] batch:  247 | acc: 37.50%,  total acc: 71.02%   [EVAL] batch:  248 | acc: 43.75%,  total acc: 70.91%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 70.85%   [EVAL] batch:  250 | acc: 50.00%,  total acc: 70.77%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 70.73%   [EVAL] batch:  252 | acc: 68.75%,  total acc: 70.73%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 70.62%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 70.59%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 70.63%   [EVAL] batch:  256 | acc: 43.75%,  total acc: 70.53%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 70.54%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 70.54%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 70.48%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 70.40%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 70.37%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 70.39%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 70.34%   [EVAL] batch:  264 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 70.28%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 70.22%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 70.13%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 70.30%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 70.41%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 70.60%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 70.69%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 70.80%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 70.81%   [EVAL] batch:  276 | acc: 68.75%,  total acc: 70.80%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 70.80%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 70.77%   [EVAL] batch:  279 | acc: 75.00%,  total acc: 70.78%   [EVAL] batch:  280 | acc: 75.00%,  total acc: 70.80%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 70.81%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 70.78%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 70.60%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 70.48%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 70.30%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 70.12%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 70.16%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 70.22%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 70.28%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 70.34%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 70.44%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 70.49%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 70.51%   [EVAL] batch:  295 | acc: 87.50%,  total acc: 70.57%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 70.56%   [EVAL] batch:  297 | acc: 81.25%,  total acc: 70.60%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 70.63%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 70.71%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 70.81%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.24%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 71.41%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 71.48%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 71.75%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 71.69%   
cur_acc:  ['0.9524', '0.6875', '0.7371', '0.8115', '0.7500']
his_acc:  ['0.9524', '0.8135', '0.7663', '0.7550', '0.7169']
CurrentTrain: epoch  0, batch     0 | loss: 5.6946688CurrentTrain: epoch  0, batch     1 | loss: 6.3083067CurrentTrain: epoch  0, batch     2 | loss: 6.3401175CurrentTrain: epoch  0, batch     3 | loss: 5.2549696CurrentTrain: epoch  1, batch     0 | loss: 4.9647627CurrentTrain: epoch  1, batch     1 | loss: 4.4788628CurrentTrain: epoch  1, batch     2 | loss: 5.1223783CurrentTrain: epoch  1, batch     3 | loss: 8.4800568CurrentTrain: epoch  2, batch     0 | loss: 4.4235253CurrentTrain: epoch  2, batch     1 | loss: 4.2779636CurrentTrain: epoch  2, batch     2 | loss: 5.4911942CurrentTrain: epoch  2, batch     3 | loss: 2.6292429CurrentTrain: epoch  3, batch     0 | loss: 4.9794974CurrentTrain: epoch  3, batch     1 | loss: 4.0267558CurrentTrain: epoch  3, batch     2 | loss: 4.9036884CurrentTrain: epoch  3, batch     3 | loss: 2.9007604CurrentTrain: epoch  4, batch     0 | loss: 3.9406631CurrentTrain: epoch  4, batch     1 | loss: 4.7831459CurrentTrain: epoch  4, batch     2 | loss: 3.7896643CurrentTrain: epoch  4, batch     3 | loss: 3.9446580CurrentTrain: epoch  5, batch     0 | loss: 4.0177135CurrentTrain: epoch  5, batch     1 | loss: 3.8649254CurrentTrain: epoch  5, batch     2 | loss: 3.9512706CurrentTrain: epoch  5, batch     3 | loss: 2.6110673CurrentTrain: epoch  6, batch     0 | loss: 3.0388463CurrentTrain: epoch  6, batch     1 | loss: 4.0496669CurrentTrain: epoch  6, batch     2 | loss: 3.4008989CurrentTrain: epoch  6, batch     3 | loss: 6.0695744CurrentTrain: epoch  7, batch     0 | loss: 3.6726871CurrentTrain: epoch  7, batch     1 | loss: 3.6979566CurrentTrain: epoch  7, batch     2 | loss: 2.7086835CurrentTrain: epoch  7, batch     3 | loss: 1.9807128CurrentTrain: epoch  8, batch     0 | loss: 2.8524098CurrentTrain: epoch  8, batch     1 | loss: 3.0419664CurrentTrain: epoch  8, batch     2 | loss: 3.8155818CurrentTrain: epoch  8, batch     3 | loss: 3.7232456CurrentTrain: epoch  9, batch     0 | loss: 3.2079382CurrentTrain: epoch  9, batch     1 | loss: 3.6164484CurrentTrain: epoch  9, batch     2 | loss: 2.7005701CurrentTrain: epoch  9, batch     3 | loss: 2.2998455
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: the " tetrabiblos " was largely responsible for laying down the basic precepts of renaissance astrology , webster ( 1979 ) p.276 .
Head Entity: tetrabiblos
Tail Entity: astrology
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: In her groundbreaking book, "The Second Sex," Simone de Beauvoir explores the role of women in society and the concept of femininity, which has influenced feminist theory significantly.  
Head Entity: The Second Sex  
Tail Entity: feminist theory  

Relation: main subject  
Context: The documentary "Planet Earth" showcases the beauty and diversity of the natural world, focusing on various ecosystems and the species that inhabit them.  
Head Entity: Planet Earth  
Tail Entity: natural world  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: the two teams had met in two previous matches , including in the 2006 world cup group stage , won by brazil 1–0 .
Head Entity: 2006 world cup
Tail Entity: brazil
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: In the 2020 UEFA European Championship, Italy and England faced off in a thrilling final that ended in a penalty shootout.  
Head Entity: 2020 UEFA European Championship  
Tail Entity: Italy  

Relation: participating team  
Context: The 2018 FIFA World Cup saw France and Croatia compete for the prestigious trophy, with France emerging victorious after a 4-2 scoreline.  
Head Entity: 2018 FIFA World Cup  
Tail Entity: France  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: in 1971 , chris and pat joined ratchell with bassist howard messer and former steppenwolf guitarist larry byrom .
Head Entity: ratchell
Tail Entity: larry byrom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The car is equipped with a powerful engine, which includes a turbocharger and a high-performance exhaust system.  
Head Entity: the car  
Tail Entity: turbocharger  

Relation: has part  
Context: The smartphone features a high-resolution camera that consists of a lens, a sensor, and an image processor.  
Head Entity: the smartphone  
Tail Entity: lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: on march 2015 , cube entertainment launched the multi - national girl group clc including original members seunghee , yujin , seungyeon , sorn and yeeun .
Head Entity: clc
Tail Entity: girl group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone brand Apple released its latest model, the iPhone 14, which features advanced technology and improved camera capabilities.  
Head Entity: iPhone 14  
Tail Entity: smartphone  

Relation: instance of  
Context: The famous painting "Starry Night" was created by the artist Vincent van Gogh and is considered a masterpiece of post-impressionism.  
Head Entity: Starry Night  
Tail Entity: painting  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in the state of california, and it includes the famous yosemite national park, which is located in the eastern part of the range.  
Head Entity: sierra nevada  
Tail Entity: yosemite national park  

Relation: mountain range  
Context: the appalachian mountains ( ) extend from the canadian province of newfoundland and labrador down to alabama, making it one of the longest mountain ranges in north america.  
Head Entity: appalachian mountains  
Tail Entity: alabama  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: future films include " mcqueen " , ian bonhote ’s documentary about the fashion designer alexander mcqueen , brad anderson ’s thriller " beirut " , and mark pellington ’s drama " nostalgia " .
Head Entity: nostalgia
Tail Entity: mark pellington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the upcoming movie "the great adventure" is penned by renowned screenwriter jessica taylor, who has previously won multiple awards for her work in the industry.  
Head Entity: the great adventure  
Tail Entity: jessica taylor  

Relation: screenwriter  
Context: in the latest film festival, "the lost city" received accolades for its screenplay, which was crafted by the talented screenwriter, robert lang.  
Head Entity: the lost city  
Tail Entity: robert lang  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, focusing on advanced materials.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: baianism is a term applied to the theology of catholic theologian michael baius ( 1513 - 1589 ) .
Head Entity: michael baius
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the mosque is a place of worship for followers of islam, where they gather for prayers and community events.  
Head Entity: mosque  
Tail Entity: islam  

Relation: religion  
Context: the teachings of buddha form the foundation of buddhism, which emphasizes mindfulness and compassion.  
Head Entity: buddha  
Tail Entity: buddhism  
Mixup data size:  438
MixupTrain:  epoch  0, batch     0 | loss: 2.6962254MixupTrain:  epoch  0, batch     1 | loss: 2.0792038MixupTrain:  epoch  0, batch     2 | loss: 2.3465844MixupTrain:  epoch  0, batch     3 | loss: 2.1131041MixupTrain:  epoch  0, batch     4 | loss: 2.0554202MixupTrain:  epoch  0, batch     5 | loss: 2.6995933MixupTrain:  epoch  0, batch     6 | loss: 2.3328440MixupTrain:  epoch  0, batch     7 | loss: 2.4720216MixupTrain:  epoch  0, batch     8 | loss: 2.6729858MixupTrain:  epoch  0, batch     9 | loss: 2.3635685MixupTrain:  epoch  0, batch    10 | loss: 2.3122834MixupTrain:  epoch  0, batch    11 | loss: 2.2788634MixupTrain:  epoch  0, batch    12 | loss: 2.3653227MixupTrain:  epoch  0, batch    13 | loss: 2.4051202MixupTrain:  epoch  0, batch    14 | loss: 2.3712615MixupTrain:  epoch  0, batch    15 | loss: 2.1382371MixupTrain:  epoch  0, batch    16 | loss: 1.8369302MixupTrain:  epoch  0, batch    17 | loss: 1.8400072MixupTrain:  epoch  0, batch    18 | loss: 1.7796688MixupTrain:  epoch  0, batch    19 | loss: 1.9677019MixupTrain:  epoch  0, batch    20 | loss: 1.9922490MixupTrain:  epoch  0, batch    21 | loss: 2.3513687MixupTrain:  epoch  0, batch    22 | loss: 1.6448125MixupTrain:  epoch  0, batch    23 | loss: 1.5891123MixupTrain:  epoch  0, batch    24 | loss: 1.9509036MixupTrain:  epoch  0, batch    25 | loss: 2.0190688MixupTrain:  epoch  0, batch    26 | loss: 1.7352646MixupTrain:  epoch  0, batch    27 | loss: 2.0767506
MemoryTrain:  epoch  0, batch     0 | loss: 1.8115048MemoryTrain:  epoch  0, batch     1 | loss: 2.3350756MemoryTrain:  epoch  0, batch     2 | loss: 2.3688822MemoryTrain:  epoch  0, batch     3 | loss: 2.1287184MemoryTrain:  epoch  0, batch     4 | loss: 2.2214251MemoryTrain:  epoch  0, batch     5 | loss: 1.9682336MemoryTrain:  epoch  0, batch     6 | loss: 2.3108552MemoryTrain:  epoch  0, batch     7 | loss: 2.0594065MemoryTrain:  epoch  0, batch     8 | loss: 2.5726309MemoryTrain:  epoch  0, batch     9 | loss: 2.1499104MemoryTrain:  epoch  0, batch    10 | loss: 2.4874337MemoryTrain:  epoch  0, batch    11 | loss: 2.0882325MemoryTrain:  epoch  1, batch     0 | loss: 1.6445209MemoryTrain:  epoch  1, batch     1 | loss: 1.6295099MemoryTrain:  epoch  1, batch     2 | loss: 1.3567870MemoryTrain:  epoch  1, batch     3 | loss: 1.8547788MemoryTrain:  epoch  1, batch     4 | loss: 1.7800320MemoryTrain:  epoch  1, batch     5 | loss: 2.0749500MemoryTrain:  epoch  1, batch     6 | loss: 2.0385189MemoryTrain:  epoch  1, batch     7 | loss: 1.9228392MemoryTrain:  epoch  1, batch     8 | loss: 2.2534320MemoryTrain:  epoch  1, batch     9 | loss: 2.0052519MemoryTrain:  epoch  1, batch    10 | loss: 1.8150760MemoryTrain:  epoch  1, batch    11 | loss: 2.0807192MemoryTrain:  epoch  2, batch     0 | loss: 1.8227799MemoryTrain:  epoch  2, batch     1 | loss: 1.7559133MemoryTrain:  epoch  2, batch     2 | loss: 1.3910235MemoryTrain:  epoch  2, batch     3 | loss: 1.4300754MemoryTrain:  epoch  2, batch     4 | loss: 1.3847392MemoryTrain:  epoch  2, batch     5 | loss: 2.1106176MemoryTrain:  epoch  2, batch     6 | loss: 2.0156016MemoryTrain:  epoch  2, batch     7 | loss: 1.4306390MemoryTrain:  epoch  2, batch     8 | loss: 2.0754988MemoryTrain:  epoch  2, batch     9 | loss: 1.3721581MemoryTrain:  epoch  2, batch    10 | loss: 1.6252247MemoryTrain:  epoch  2, batch    11 | loss: 1.1913878MemoryTrain:  epoch  3, batch     0 | loss: 2.1094646MemoryTrain:  epoch  3, batch     1 | loss: 1.4385846MemoryTrain:  epoch  3, batch     2 | loss: 1.3824625MemoryTrain:  epoch  3, batch     3 | loss: 1.4081755MemoryTrain:  epoch  3, batch     4 | loss: 2.3559005MemoryTrain:  epoch  3, batch     5 | loss: 1.3790016MemoryTrain:  epoch  3, batch     6 | loss: 1.4591944MemoryTrain:  epoch  3, batch     7 | loss: 1.6280653MemoryTrain:  epoch  3, batch     8 | loss: 1.3622514MemoryTrain:  epoch  3, batch     9 | loss: 1.4152577MemoryTrain:  epoch  3, batch    10 | loss: 1.4219410MemoryTrain:  epoch  3, batch    11 | loss: 1.1595204MemoryTrain:  epoch  4, batch     0 | loss: 1.6924208MemoryTrain:  epoch  4, batch     1 | loss: 1.3035851MemoryTrain:  epoch  4, batch     2 | loss: 1.6194642MemoryTrain:  epoch  4, batch     3 | loss: 1.3116112MemoryTrain:  epoch  4, batch     4 | loss: 1.6341350MemoryTrain:  epoch  4, batch     5 | loss: 1.7084901MemoryTrain:  epoch  4, batch     6 | loss: 1.5043906MemoryTrain:  epoch  4, batch     7 | loss: 1.4242381MemoryTrain:  epoch  4, batch     8 | loss: 1.2372212MemoryTrain:  epoch  4, batch     9 | loss: 1.6698966MemoryTrain:  epoch  4, batch    10 | loss: 1.4055531MemoryTrain:  epoch  4, batch    11 | loss: 1.3489535MemoryTrain:  epoch  5, batch     0 | loss: 1.7937303MemoryTrain:  epoch  5, batch     1 | loss: 1.6089065MemoryTrain:  epoch  5, batch     2 | loss: 1.9715753MemoryTrain:  epoch  5, batch     3 | loss: 1.4348332MemoryTrain:  epoch  5, batch     4 | loss: 1.2099681MemoryTrain:  epoch  5, batch     5 | loss: 1.3228159MemoryTrain:  epoch  5, batch     6 | loss: 1.3779166MemoryTrain:  epoch  5, batch     7 | loss: 1.3684828MemoryTrain:  epoch  5, batch     8 | loss: 1.4735872MemoryTrain:  epoch  5, batch     9 | loss: 1.2927337MemoryTrain:  epoch  5, batch    10 | loss: 1.5079870MemoryTrain:  epoch  5, batch    11 | loss: 1.2830677MemoryTrain:  epoch  6, batch     0 | loss: 1.4384811MemoryTrain:  epoch  6, batch     1 | loss: 1.6424500MemoryTrain:  epoch  6, batch     2 | loss: 1.3440477MemoryTrain:  epoch  6, batch     3 | loss: 1.3303905MemoryTrain:  epoch  6, batch     4 | loss: 1.5802135MemoryTrain:  epoch  6, batch     5 | loss: 1.3241391MemoryTrain:  epoch  6, batch     6 | loss: 1.3268819MemoryTrain:  epoch  6, batch     7 | loss: 1.2693956MemoryTrain:  epoch  6, batch     8 | loss: 1.4555180MemoryTrain:  epoch  6, batch     9 | loss: 1.3489968MemoryTrain:  epoch  6, batch    10 | loss: 1.4727876MemoryTrain:  epoch  6, batch    11 | loss: 1.3634728MemoryTrain:  epoch  7, batch     0 | loss: 1.3347189MemoryTrain:  epoch  7, batch     1 | loss: 1.4834023MemoryTrain:  epoch  7, batch     2 | loss: 1.2790892MemoryTrain:  epoch  7, batch     3 | loss: 1.3467103MemoryTrain:  epoch  7, batch     4 | loss: 1.3177831MemoryTrain:  epoch  7, batch     5 | loss: 1.5686884MemoryTrain:  epoch  7, batch     6 | loss: 1.5970426MemoryTrain:  epoch  7, batch     7 | loss: 1.4185225MemoryTrain:  epoch  7, batch     8 | loss: 1.2458191MemoryTrain:  epoch  7, batch     9 | loss: 1.2124530MemoryTrain:  epoch  7, batch    10 | loss: 1.4726117MemoryTrain:  epoch  7, batch    11 | loss: 1.2778449MemoryTrain:  epoch  8, batch     0 | loss: 1.3657272MemoryTrain:  epoch  8, batch     1 | loss: 1.4247744MemoryTrain:  epoch  8, batch     2 | loss: 1.2974942MemoryTrain:  epoch  8, batch     3 | loss: 1.3519347MemoryTrain:  epoch  8, batch     4 | loss: 1.2765956MemoryTrain:  epoch  8, batch     5 | loss: 1.2596779MemoryTrain:  epoch  8, batch     6 | loss: 1.2931004MemoryTrain:  epoch  8, batch     7 | loss: 1.4090760MemoryTrain:  epoch  8, batch     8 | loss: 1.5326281MemoryTrain:  epoch  8, batch     9 | loss: 1.2407900MemoryTrain:  epoch  8, batch    10 | loss: 1.2331262MemoryTrain:  epoch  8, batch    11 | loss: 1.2447013MemoryTrain:  epoch  9, batch     0 | loss: 1.2331583MemoryTrain:  epoch  9, batch     1 | loss: 1.3277491MemoryTrain:  epoch  9, batch     2 | loss: 1.2723231MemoryTrain:  epoch  9, batch     3 | loss: 1.3879719MemoryTrain:  epoch  9, batch     4 | loss: 1.2478777MemoryTrain:  epoch  9, batch     5 | loss: 1.2225835MemoryTrain:  epoch  9, batch     6 | loss: 1.3532176MemoryTrain:  epoch  9, batch     7 | loss: 1.2821853MemoryTrain:  epoch  9, batch     8 | loss: 1.3472428MemoryTrain:  epoch  9, batch     9 | loss: 1.3569584MemoryTrain:  epoch  9, batch    10 | loss: 1.3907712MemoryTrain:  epoch  9, batch    11 | loss: 1.1912272
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 66.18%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 62.85%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 60.86%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 31.25%,  total acc: 68.27%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 66.44%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 64.96%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 64.01%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 61.49%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 62.11%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 63.07%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 66.55%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 67.27%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 67.31%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 67.66%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 67.53%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 67.41%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 67.44%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 67.90%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 69.02%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 69.66%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 70.15%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 70.79%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 71.18%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 72.04%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 72.09%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 72.35%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 72.19%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 72.44%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 72.38%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 71.92%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 76.14%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 75.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 81.99%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 83.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.22%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 84.45%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.52%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.74%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 84.80%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 84.31%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 83.83%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 83.24%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 82.68%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 82.14%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 81.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 81.37%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 80.90%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 80.44%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 79.66%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 79.46%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 79.39%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 79.09%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 78.92%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 79.06%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 79.30%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 79.23%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 78.57%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 77.34%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 76.35%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 75.19%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 74.07%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 73.07%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 72.55%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 72.59%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 72.80%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 73.09%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 73.29%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 73.31%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 73.42%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 73.21%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 73.16%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 73.10%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 72.97%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 73.15%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 72.48%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 71.99%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 71.35%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 70.96%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 70.49%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 70.04%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 69.32%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 68.54%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 67.99%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 67.24%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 66.71%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 66.06%   [EVAL] batch:   93 | acc: 6.25%,  total acc: 65.43%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 65.53%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 65.69%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 66.20%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 66.35%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 66.44%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 66.97%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 67.17%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 67.37%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 67.62%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 67.76%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 67.25%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 66.86%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 66.31%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 66.10%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 65.68%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 65.43%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 65.24%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 65.16%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 65.36%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 65.28%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 65.31%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 65.23%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 65.10%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 64.98%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 64.86%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 64.79%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 64.87%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 64.85%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 64.88%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 65.01%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 65.04%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 64.92%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 65.05%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 65.08%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 65.29%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 65.88%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 66.08%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 66.30%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 66.32%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 66.21%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 66.05%   [EVAL] batch:  141 | acc: 56.25%,  total acc: 65.98%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 65.91%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 65.89%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 65.82%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 65.58%   [EVAL] batch:  146 | acc: 31.25%,  total acc: 65.35%   [EVAL] batch:  147 | acc: 37.50%,  total acc: 65.16%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 65.02%   [EVAL] batch:  149 | acc: 50.00%,  total acc: 64.92%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 64.69%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 64.51%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 64.34%   [EVAL] batch:  153 | acc: 18.75%,  total acc: 64.04%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 63.91%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 63.66%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 63.61%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 63.57%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 63.52%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 63.48%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 63.55%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 63.46%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 63.50%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 63.53%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 63.67%   [EVAL] batch:  165 | acc: 87.50%,  total acc: 63.82%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 63.88%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 64.09%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 63.86%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 63.60%   [EVAL] batch:  171 | acc: 12.50%,  total acc: 63.30%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 63.15%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 62.93%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 62.79%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 63.21%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 63.78%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 63.98%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 64.11%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 64.17%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 64.30%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 64.46%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 64.71%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 64.79%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 64.78%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 64.80%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 64.86%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 64.91%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 64.90%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 64.88%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 64.97%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 64.96%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 64.91%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 65.03%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 65.08%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 65.16%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 65.17%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 65.19%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 65.21%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 65.13%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 65.15%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 65.17%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 65.04%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 64.90%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 64.71%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 64.64%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 64.45%   [EVAL] batch:  211 | acc: 56.25%,  total acc: 64.42%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 64.44%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 64.57%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 64.76%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 65.15%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.08%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 66.18%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 66.27%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 66.28%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 66.40%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 66.59%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 66.62%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 66.52%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 66.51%   [EVAL] batch:  234 | acc: 43.75%,  total acc: 66.41%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 66.39%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 66.27%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 66.33%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 66.42%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 66.62%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 66.71%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 66.80%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 66.80%   [EVAL] batch:  244 | acc: 43.75%,  total acc: 66.71%   [EVAL] batch:  245 | acc: 37.50%,  total acc: 66.59%   [EVAL] batch:  246 | acc: 25.00%,  total acc: 66.42%   [EVAL] batch:  247 | acc: 37.50%,  total acc: 66.31%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 66.14%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:  250 | acc: 37.50%,  total acc: 65.96%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 65.92%   [EVAL] batch:  252 | acc: 56.25%,  total acc: 65.88%   [EVAL] batch:  253 | acc: 50.00%,  total acc: 65.82%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 65.76%   [EVAL] batch:  255 | acc: 56.25%,  total acc: 65.72%   [EVAL] batch:  256 | acc: 37.50%,  total acc: 65.61%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 65.65%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 65.66%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 65.54%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 65.55%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 65.52%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 65.46%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 65.42%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 65.25%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 65.17%   [EVAL] batch:  267 | acc: 25.00%,  total acc: 65.02%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 65.08%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 65.19%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 65.54%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 65.65%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 65.81%   [EVAL] batch:  276 | acc: 75.00%,  total acc: 65.84%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 65.83%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 65.84%   [EVAL] batch:  279 | acc: 81.25%,  total acc: 65.89%   [EVAL] batch:  280 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 65.98%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 65.99%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 65.80%   [EVAL] batch:  284 | acc: 31.25%,  total acc: 65.68%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 65.54%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 65.37%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 65.41%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 65.51%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 65.58%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 65.66%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 65.75%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 65.78%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 65.86%   [EVAL] batch:  294 | acc: 75.00%,  total acc: 65.89%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 65.94%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 65.95%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 65.98%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 66.03%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 66.12%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 66.44%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 66.53%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.64%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 67.43%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 67.42%   [EVAL] batch:  314 | acc: 62.50%,  total acc: 67.40%   [EVAL] batch:  315 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:  316 | acc: 81.25%,  total acc: 67.51%   [EVAL] batch:  317 | acc: 93.75%,  total acc: 67.59%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 67.61%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 67.78%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 67.84%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:  325 | acc: 12.50%,  total acc: 67.89%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 67.74%   [EVAL] batch:  327 | acc: 25.00%,  total acc: 67.61%   [EVAL] batch:  328 | acc: 12.50%,  total acc: 67.44%   [EVAL] batch:  329 | acc: 12.50%,  total acc: 67.27%   [EVAL] batch:  330 | acc: 0.00%,  total acc: 67.07%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 67.07%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 67.54%   [EVAL] batch:  337 | acc: 68.75%,  total acc: 67.55%   [EVAL] batch:  338 | acc: 18.75%,  total acc: 67.40%   [EVAL] batch:  339 | acc: 25.00%,  total acc: 67.28%   [EVAL] batch:  340 | acc: 25.00%,  total acc: 67.16%   [EVAL] batch:  341 | acc: 37.50%,  total acc: 67.07%   [EVAL] batch:  342 | acc: 25.00%,  total acc: 66.95%   [EVAL] batch:  343 | acc: 43.75%,  total acc: 66.88%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.97%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 67.40%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 67.42%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 67.44%   [EVAL] batch:  353 | acc: 56.25%,  total acc: 67.41%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 67.45%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 67.51%   [EVAL] batch:  357 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  358 | acc: 87.50%,  total acc: 67.65%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 67.82%   [EVAL] batch:  362 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 67.89%   [EVAL] batch:  364 | acc: 87.50%,  total acc: 67.95%   [EVAL] batch:  365 | acc: 81.25%,  total acc: 67.98%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 68.00%   [EVAL] batch:  367 | acc: 87.50%,  total acc: 68.05%   [EVAL] batch:  368 | acc: 81.25%,  total acc: 68.09%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 68.14%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:  371 | acc: 75.00%,  total acc: 68.21%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 68.20%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 68.23%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 68.27%   
cur_acc:  ['0.9524', '0.6875', '0.7371', '0.8115', '0.7500', '0.7192']
his_acc:  ['0.9524', '0.8135', '0.7663', '0.7550', '0.7169', '0.6827']
CurrentTrain: epoch  0, batch     0 | loss: 5.2434435CurrentTrain: epoch  0, batch     1 | loss: 4.9497442CurrentTrain: epoch  0, batch     2 | loss: 4.8164554CurrentTrain: epoch  0, batch     3 | loss: 4.0709925CurrentTrain: epoch  1, batch     0 | loss: 4.0971661CurrentTrain: epoch  1, batch     1 | loss: 3.8472004CurrentTrain: epoch  1, batch     2 | loss: 3.4985642CurrentTrain: epoch  1, batch     3 | loss: 2.7573328CurrentTrain: epoch  2, batch     0 | loss: 3.5157766CurrentTrain: epoch  2, batch     1 | loss: 3.0086417CurrentTrain: epoch  2, batch     2 | loss: 3.0438910CurrentTrain: epoch  2, batch     3 | loss: 2.0740972CurrentTrain: epoch  3, batch     0 | loss: 2.2371552CurrentTrain: epoch  3, batch     1 | loss: 3.4061098CurrentTrain: epoch  3, batch     2 | loss: 2.7894609CurrentTrain: epoch  3, batch     3 | loss: 1.9876328CurrentTrain: epoch  4, batch     0 | loss: 2.5197656CurrentTrain: epoch  4, batch     1 | loss: 2.5525341CurrentTrain: epoch  4, batch     2 | loss: 2.8144922CurrentTrain: epoch  4, batch     3 | loss: 2.1548548CurrentTrain: epoch  5, batch     0 | loss: 2.2675581CurrentTrain: epoch  5, batch     1 | loss: 2.5946655CurrentTrain: epoch  5, batch     2 | loss: 2.6172676CurrentTrain: epoch  5, batch     3 | loss: 2.1322227CurrentTrain: epoch  6, batch     0 | loss: 2.3284502CurrentTrain: epoch  6, batch     1 | loss: 2.5257940CurrentTrain: epoch  6, batch     2 | loss: 2.0432591CurrentTrain: epoch  6, batch     3 | loss: 2.0301788CurrentTrain: epoch  7, batch     0 | loss: 1.9474425CurrentTrain: epoch  7, batch     1 | loss: 2.3093190CurrentTrain: epoch  7, batch     2 | loss: 2.0988135CurrentTrain: epoch  7, batch     3 | loss: 2.1209581CurrentTrain: epoch  8, batch     0 | loss: 2.1763232CurrentTrain: epoch  8, batch     1 | loss: 2.1316693CurrentTrain: epoch  8, batch     2 | loss: 1.9170423CurrentTrain: epoch  8, batch     3 | loss: 1.8514135CurrentTrain: epoch  9, batch     0 | loss: 1.9659342CurrentTrain: epoch  9, batch     1 | loss: 1.9871341CurrentTrain: epoch  9, batch     2 | loss: 1.9430014CurrentTrain: epoch  9, batch     3 | loss: 1.7738374
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: elizabeth warren is a prominent member of the democratic party, advocating for progressive policies and reforms.  
Head Entity: elizabeth warren  
Tail Entity: democratic party  

Relation: member of political party  
Context: during his tenure in the senate, john mccain was a long-time member of the republican party, known for his bipartisan approach.  
Head Entity: john mccain  
Tail Entity: republican party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film is based on the novel "the great gatsby" by f. scott fitzgerald, capturing the essence of the roaring twenties.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: this musical is inspired by the play "hamlet" written by william shakespeare, reimagining the classic tragedy in a modern setting.  
Head Entity: hamlet  
Tail Entity: william shakespeare  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: another was " the death of nelson " by daniel maclise , a large wall painting in the royal gallery of the palace of westminster .
Head Entity: daniel maclise
Tail Entity: the death of nelson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: one of his most famous pieces is "the persistence of memory," a surreal painting that features melting clocks and explores the concept of time.  
Head Entity: salvador dalí  
Tail Entity: the persistence of memory  

Relation: notable work  
Context: her acclaimed novel "to kill a mockingbird" addresses serious themes of racial injustice and moral growth in the American South.  
Head Entity: harper lee  
Tail Entity: to kill a mockingbird  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the startup saw a significant increase in investment and growth opportunities.  
Head Entity: startup  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: vallicula is a genus of ctenophora in family coeloplanidae , containing a single species , vallicula multiformis .
Head Entity: coeloplanidae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The species Panthera leo belongs to the genus Panthera, which is part of the family Felidae.  
Head Entity: Panthera  
Tail Entity: genus  

Relation: taxon rank  
Context: The plant species Quercus robur is classified under the genus Quercus in the family Fagaceae.  
Head Entity: Quercus  
Tail Entity: genus  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in artificial intelligence and currently works at the mit media lab, focusing on machine learning applications.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. rajesh kumar as the chief epidemiologist, where he will oversee global health initiatives related to infectious diseases.  
Head Entity: dr. rajesh kumar  
Tail Entity: epidemiology  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: wjet erie , pennsylvania was his first official radio job outside of duties performed in the u.s. air force .
Head Entity: wjet
Tail Entity: erie , pennsylvania
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: kxyz radio serves the community of springfield, providing news and entertainment to its listeners.  
Head Entity: kxyz  
Tail Entity: springfield  

Relation: licensed to broadcast to  
Context: the station wabc is known for its extensive coverage of new york city events and local news.  
Head Entity: wabc  
Tail Entity: new york city  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: hd 32518 b is an extrasolar planet which orbits the k - type giant star hd 32518 , located approximately 383 light years away in the constellation camelopardalis .
Head Entity: hd 32518
Tail Entity: camelopardalis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the star betelgeuse is located in the constellation orion and is one of the brightest stars in the night sky.  
Head Entity: betelgeuse  
Tail Entity: orion  

Relation: constellation  
Context: the andromeda galaxy is visible in the constellation andromeda, which is named after a princess in Greek mythology.  
Head Entity: andromeda galaxy  
Tail Entity: andromeda  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Mixup data size:  499
MixupTrain:  epoch  0, batch     0 | loss: 1.5842522MixupTrain:  epoch  0, batch     1 | loss: 1.8911166MixupTrain:  epoch  0, batch     2 | loss: 1.8595367MixupTrain:  epoch  0, batch     3 | loss: 1.8029070MixupTrain:  epoch  0, batch     4 | loss: 1.4234730MixupTrain:  epoch  0, batch     5 | loss: 1.6695188MixupTrain:  epoch  0, batch     6 | loss: 1.9617776MixupTrain:  epoch  0, batch     7 | loss: 1.8720421MixupTrain:  epoch  0, batch     8 | loss: 1.7460694MixupTrain:  epoch  0, batch     9 | loss: 1.6588036MixupTrain:  epoch  0, batch    10 | loss: 1.5596606MixupTrain:  epoch  0, batch    11 | loss: 1.7673606MixupTrain:  epoch  0, batch    12 | loss: 1.6433438MixupTrain:  epoch  0, batch    13 | loss: 1.7159784MixupTrain:  epoch  0, batch    14 | loss: 1.7667867MixupTrain:  epoch  0, batch    15 | loss: 1.3422524MixupTrain:  epoch  0, batch    16 | loss: 1.8716662MixupTrain:  epoch  0, batch    17 | loss: 1.7462426MixupTrain:  epoch  0, batch    18 | loss: 1.8484930MixupTrain:  epoch  0, batch    19 | loss: 1.6772540MixupTrain:  epoch  0, batch    20 | loss: 1.6575708MixupTrain:  epoch  0, batch    21 | loss: 1.6035192MixupTrain:  epoch  0, batch    22 | loss: 1.6511493MixupTrain:  epoch  0, batch    23 | loss: 1.5580223MixupTrain:  epoch  0, batch    24 | loss: 1.6437083MixupTrain:  epoch  0, batch    25 | loss: 1.9661478MixupTrain:  epoch  0, batch    26 | loss: 1.5835918MixupTrain:  epoch  0, batch    27 | loss: 1.9846590MixupTrain:  epoch  0, batch    28 | loss: 1.5084681MixupTrain:  epoch  0, batch    29 | loss: 1.5075741MixupTrain:  epoch  0, batch    30 | loss: 1.5201119MixupTrain:  epoch  0, batch    31 | loss: 1.5698154
MemoryTrain:  epoch  0, batch     0 | loss: 1.4895445MemoryTrain:  epoch  0, batch     1 | loss: 1.7747563MemoryTrain:  epoch  0, batch     2 | loss: 1.8370993MemoryTrain:  epoch  0, batch     3 | loss: 1.6548502MemoryTrain:  epoch  0, batch     4 | loss: 1.5563830MemoryTrain:  epoch  0, batch     5 | loss: 2.2468505MemoryTrain:  epoch  0, batch     6 | loss: 1.8119276MemoryTrain:  epoch  0, batch     7 | loss: 1.7800792MemoryTrain:  epoch  0, batch     8 | loss: 1.8967819MemoryTrain:  epoch  0, batch     9 | loss: 1.5203047MemoryTrain:  epoch  0, batch    10 | loss: 1.9346721MemoryTrain:  epoch  0, batch    11 | loss: 1.8208845MemoryTrain:  epoch  0, batch    12 | loss: 2.0722668MemoryTrain:  epoch  0, batch    13 | loss: 2.4739265MemoryTrain:  epoch  1, batch     0 | loss: 1.5376674MemoryTrain:  epoch  1, batch     1 | loss: 1.8208339MemoryTrain:  epoch  1, batch     2 | loss: 1.3107120MemoryTrain:  epoch  1, batch     3 | loss: 1.6215352MemoryTrain:  epoch  1, batch     4 | loss: 1.5182142MemoryTrain:  epoch  1, batch     5 | loss: 1.3421428MemoryTrain:  epoch  1, batch     6 | loss: 1.4477893MemoryTrain:  epoch  1, batch     7 | loss: 2.0955594MemoryTrain:  epoch  1, batch     8 | loss: 1.5567979MemoryTrain:  epoch  1, batch     9 | loss: 2.0915768MemoryTrain:  epoch  1, batch    10 | loss: 1.3970538MemoryTrain:  epoch  1, batch    11 | loss: 1.5379066MemoryTrain:  epoch  1, batch    12 | loss: 1.6908453MemoryTrain:  epoch  1, batch    13 | loss: 1.2926139MemoryTrain:  epoch  2, batch     0 | loss: 1.3142917MemoryTrain:  epoch  2, batch     1 | loss: 1.5194690MemoryTrain:  epoch  2, batch     2 | loss: 1.5364507MemoryTrain:  epoch  2, batch     3 | loss: 1.4876231MemoryTrain:  epoch  2, batch     4 | loss: 1.4697459MemoryTrain:  epoch  2, batch     5 | loss: 1.4203436MemoryTrain:  epoch  2, batch     6 | loss: 1.3784945MemoryTrain:  epoch  2, batch     7 | loss: 1.2774839MemoryTrain:  epoch  2, batch     8 | loss: 1.4254481MemoryTrain:  epoch  2, batch     9 | loss: 1.2495271MemoryTrain:  epoch  2, batch    10 | loss: 1.4987293MemoryTrain:  epoch  2, batch    11 | loss: 1.3562436MemoryTrain:  epoch  2, batch    12 | loss: 1.6832763MemoryTrain:  epoch  2, batch    13 | loss: 1.6785066MemoryTrain:  epoch  3, batch     0 | loss: 1.4893336MemoryTrain:  epoch  3, batch     1 | loss: 1.3289585MemoryTrain:  epoch  3, batch     2 | loss: 1.4381316MemoryTrain:  epoch  3, batch     3 | loss: 1.3044918MemoryTrain:  epoch  3, batch     4 | loss: 1.3185453MemoryTrain:  epoch  3, batch     5 | loss: 1.3337892MemoryTrain:  epoch  3, batch     6 | loss: 1.2442088MemoryTrain:  epoch  3, batch     7 | loss: 1.3080885MemoryTrain:  epoch  3, batch     8 | loss: 1.4548736MemoryTrain:  epoch  3, batch     9 | loss: 1.3160206MemoryTrain:  epoch  3, batch    10 | loss: 1.2958462MemoryTrain:  epoch  3, batch    11 | loss: 1.2868659MemoryTrain:  epoch  3, batch    12 | loss: 1.3678120MemoryTrain:  epoch  3, batch    13 | loss: 1.1612632MemoryTrain:  epoch  4, batch     0 | loss: 1.2782288MemoryTrain:  epoch  4, batch     1 | loss: 1.3705314MemoryTrain:  epoch  4, batch     2 | loss: 1.2419658MemoryTrain:  epoch  4, batch     3 | loss: 1.2552285MemoryTrain:  epoch  4, batch     4 | loss: 1.2605106MemoryTrain:  epoch  4, batch     5 | loss: 1.3188450MemoryTrain:  epoch  4, batch     6 | loss: 1.3226027MemoryTrain:  epoch  4, batch     7 | loss: 1.3736042MemoryTrain:  epoch  4, batch     8 | loss: 1.3220038MemoryTrain:  epoch  4, batch     9 | loss: 1.2250003MemoryTrain:  epoch  4, batch    10 | loss: 1.3151499MemoryTrain:  epoch  4, batch    11 | loss: 1.2598262MemoryTrain:  epoch  4, batch    12 | loss: 1.3390026MemoryTrain:  epoch  4, batch    13 | loss: 1.1759074MemoryTrain:  epoch  5, batch     0 | loss: 1.2870996MemoryTrain:  epoch  5, batch     1 | loss: 1.2252212MemoryTrain:  epoch  5, batch     2 | loss: 1.3777111MemoryTrain:  epoch  5, batch     3 | loss: 1.2512069MemoryTrain:  epoch  5, batch     4 | loss: 1.3213291MemoryTrain:  epoch  5, batch     5 | loss: 1.2452646MemoryTrain:  epoch  5, batch     6 | loss: 1.2555726MemoryTrain:  epoch  5, batch     7 | loss: 1.2592173MemoryTrain:  epoch  5, batch     8 | loss: 1.2509884MemoryTrain:  epoch  5, batch     9 | loss: 1.2571290MemoryTrain:  epoch  5, batch    10 | loss: 1.3157781MemoryTrain:  epoch  5, batch    11 | loss: 1.2490797MemoryTrain:  epoch  5, batch    12 | loss: 1.2767959MemoryTrain:  epoch  5, batch    13 | loss: 1.3742635MemoryTrain:  epoch  6, batch     0 | loss: 1.3550935MemoryTrain:  epoch  6, batch     1 | loss: 1.2155055MemoryTrain:  epoch  6, batch     2 | loss: 1.2831075MemoryTrain:  epoch  6, batch     3 | loss: 1.3191667MemoryTrain:  epoch  6, batch     4 | loss: 1.2965571MemoryTrain:  epoch  6, batch     5 | loss: 1.2634106MemoryTrain:  epoch  6, batch     6 | loss: 1.3760235MemoryTrain:  epoch  6, batch     7 | loss: 1.2219002MemoryTrain:  epoch  6, batch     8 | loss: 1.2587421MemoryTrain:  epoch  6, batch     9 | loss: 1.2696235MemoryTrain:  epoch  6, batch    10 | loss: 1.2066333MemoryTrain:  epoch  6, batch    11 | loss: 1.2187824MemoryTrain:  epoch  6, batch    12 | loss: 1.2725458MemoryTrain:  epoch  6, batch    13 | loss: 1.3310601MemoryTrain:  epoch  7, batch     0 | loss: 1.2606852MemoryTrain:  epoch  7, batch     1 | loss: 1.2370436MemoryTrain:  epoch  7, batch     2 | loss: 1.3279533MemoryTrain:  epoch  7, batch     3 | loss: 1.2481215MemoryTrain:  epoch  7, batch     4 | loss: 1.2271349MemoryTrain:  epoch  7, batch     5 | loss: 1.2474997MemoryTrain:  epoch  7, batch     6 | loss: 1.2390583MemoryTrain:  epoch  7, batch     7 | loss: 1.3705559MemoryTrain:  epoch  7, batch     8 | loss: 1.1976904MemoryTrain:  epoch  7, batch     9 | loss: 1.1814032MemoryTrain:  epoch  7, batch    10 | loss: 1.2235677MemoryTrain:  epoch  7, batch    11 | loss: 1.2757025MemoryTrain:  epoch  7, batch    12 | loss: 1.2281531MemoryTrain:  epoch  7, batch    13 | loss: 1.5980858MemoryTrain:  epoch  8, batch     0 | loss: 1.2043586MemoryTrain:  epoch  8, batch     1 | loss: 1.2955530MemoryTrain:  epoch  8, batch     2 | loss: 1.3137137MemoryTrain:  epoch  8, batch     3 | loss: 1.2550595MemoryTrain:  epoch  8, batch     4 | loss: 1.2855005MemoryTrain:  epoch  8, batch     5 | loss: 1.2385070MemoryTrain:  epoch  8, batch     6 | loss: 1.2083850MemoryTrain:  epoch  8, batch     7 | loss: 1.3421080MemoryTrain:  epoch  8, batch     8 | loss: 1.2252945MemoryTrain:  epoch  8, batch     9 | loss: 1.2300739MemoryTrain:  epoch  8, batch    10 | loss: 1.2604307MemoryTrain:  epoch  8, batch    11 | loss: 1.2269315MemoryTrain:  epoch  8, batch    12 | loss: 1.2076347MemoryTrain:  epoch  8, batch    13 | loss: 1.2085090MemoryTrain:  epoch  9, batch     0 | loss: 1.2521191MemoryTrain:  epoch  9, batch     1 | loss: 1.1976234MemoryTrain:  epoch  9, batch     2 | loss: 1.2443621MemoryTrain:  epoch  9, batch     3 | loss: 1.2450197MemoryTrain:  epoch  9, batch     4 | loss: 1.2924386MemoryTrain:  epoch  9, batch     5 | loss: 1.2944002MemoryTrain:  epoch  9, batch     6 | loss: 1.2389065MemoryTrain:  epoch  9, batch     7 | loss: 1.2150158MemoryTrain:  epoch  9, batch     8 | loss: 1.2096617MemoryTrain:  epoch  9, batch     9 | loss: 1.2254388MemoryTrain:  epoch  9, batch    10 | loss: 1.1981163MemoryTrain:  epoch  9, batch    11 | loss: 1.2471282MemoryTrain:  epoch  9, batch    12 | loss: 1.2108958MemoryTrain:  epoch  9, batch    13 | loss: 1.1731477
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 87.89%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 86.84%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 85.36%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 85.07%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 84.46%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 84.21%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.37%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 86.05%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.22%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 86.53%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.82%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 87.37%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.63%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 87.87%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 87.86%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 87.85%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 87.96%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 87.84%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 88.06%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 88.16%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 88.25%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 88.35%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 88.73%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 88.81%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 88.19%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 69.60%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 69.27%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.49%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 75.98%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 77.39%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 77.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 78.30%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 78.89%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.97%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 80.79%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 80.95%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.39%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 80.83%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 80.30%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 80.05%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 79.43%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 78.95%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 78.38%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 78.07%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 77.55%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 77.16%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 77.01%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 76.97%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 76.62%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 76.48%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 76.67%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 76.84%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 76.81%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 76.39%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 75.20%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 74.23%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 73.11%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 72.01%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 71.05%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 70.56%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 70.62%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 70.86%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 71.40%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 71.54%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 71.75%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 71.79%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 71.75%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 71.71%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 71.68%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 71.64%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 71.34%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 70.86%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 70.39%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 69.93%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 69.62%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 69.18%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 68.54%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 67.77%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 67.22%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 66.48%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 65.83%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 65.19%   [EVAL] batch:   93 | acc: 6.25%,  total acc: 64.56%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 64.67%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 64.91%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 65.08%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 65.31%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 65.40%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 65.31%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 65.59%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 65.81%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 66.55%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 66.34%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 65.97%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 65.82%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 65.57%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 65.38%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 65.13%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 64.95%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 65.09%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 64.96%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 64.99%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 64.92%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 64.79%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 64.62%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 64.45%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 64.38%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 64.52%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 64.50%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:  126 | acc: 87.50%,  total acc: 64.76%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 64.89%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 64.83%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 64.81%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 64.89%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 65.10%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 65.37%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 65.53%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 65.65%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 66.06%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 66.17%   [EVAL] batch:  138 | acc: 62.50%,  total acc: 66.14%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 66.07%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 65.91%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 65.76%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 65.60%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 65.58%   [EVAL] batch:  144 | acc: 37.50%,  total acc: 65.39%   [EVAL] batch:  145 | acc: 31.25%,  total acc: 65.15%   [EVAL] batch:  146 | acc: 31.25%,  total acc: 64.92%   [EVAL] batch:  147 | acc: 37.50%,  total acc: 64.74%   [EVAL] batch:  148 | acc: 37.50%,  total acc: 64.56%   [EVAL] batch:  149 | acc: 37.50%,  total acc: 64.38%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 64.11%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 63.94%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 63.68%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 63.35%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 63.02%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 62.70%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 62.66%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 62.62%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 62.54%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 62.54%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 62.62%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 62.54%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 62.58%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 62.61%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 62.69%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 62.76%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 62.87%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 62.98%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 62.98%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 62.76%   [EVAL] batch:  170 | acc: 12.50%,  total acc: 62.46%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 62.21%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 62.03%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 61.82%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 61.68%   [EVAL] batch:  175 | acc: 93.75%,  total acc: 61.86%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 62.08%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 62.29%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 62.67%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 62.88%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 63.05%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 63.11%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 63.25%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 63.41%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 63.58%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 63.70%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 63.80%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 63.79%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 63.85%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 63.94%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 64.03%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 64.12%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 64.11%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 64.20%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 64.19%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 64.15%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 64.27%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 64.32%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 64.41%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 64.37%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 64.36%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 64.35%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 64.25%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 64.27%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 64.25%   [EVAL] batch:  207 | acc: 37.50%,  total acc: 64.12%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 63.94%   [EVAL] batch:  209 | acc: 43.75%,  total acc: 63.84%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 63.66%   [EVAL] batch:  211 | acc: 56.25%,  total acc: 63.62%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 63.70%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 63.81%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 63.95%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 64.23%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 64.33%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 64.47%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 65.51%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 65.61%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 65.72%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 65.84%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 65.95%   [EVAL] batch:  232 | acc: 50.00%,  total acc: 65.88%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 65.87%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 65.85%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 65.86%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 65.82%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 65.91%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 66.00%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 66.21%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 66.30%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 66.38%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 66.39%   [EVAL] batch:  244 | acc: 31.25%,  total acc: 66.25%   [EVAL] batch:  245 | acc: 18.75%,  total acc: 66.06%   [EVAL] batch:  246 | acc: 25.00%,  total acc: 65.89%   [EVAL] batch:  247 | acc: 18.75%,  total acc: 65.70%   [EVAL] batch:  248 | acc: 31.25%,  total acc: 65.56%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 65.48%   [EVAL] batch:  250 | acc: 37.50%,  total acc: 65.36%   [EVAL] batch:  251 | acc: 56.25%,  total acc: 65.33%   [EVAL] batch:  252 | acc: 62.50%,  total acc: 65.32%   [EVAL] batch:  253 | acc: 43.75%,  total acc: 65.23%   [EVAL] batch:  254 | acc: 50.00%,  total acc: 65.17%   [EVAL] batch:  255 | acc: 62.50%,  total acc: 65.16%   [EVAL] batch:  256 | acc: 37.50%,  total acc: 65.05%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 65.09%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 65.11%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 65.07%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 64.99%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 64.97%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 64.91%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 64.88%   [EVAL] batch:  265 | acc: 18.75%,  total acc: 64.71%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 64.65%   [EVAL] batch:  267 | acc: 25.00%,  total acc: 64.51%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 64.57%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 64.63%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 64.87%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 64.97%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 65.05%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 65.24%   [EVAL] batch:  276 | acc: 68.75%,  total acc: 65.25%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 65.22%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 65.19%   [EVAL] batch:  279 | acc: 68.75%,  total acc: 65.20%   [EVAL] batch:  280 | acc: 62.50%,  total acc: 65.19%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 65.23%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 65.19%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 65.01%   [EVAL] batch:  284 | acc: 37.50%,  total acc: 64.91%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 64.75%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 64.53%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 64.56%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 64.55%   [EVAL] batch:  289 | acc: 75.00%,  total acc: 64.59%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 64.63%   [EVAL] batch:  291 | acc: 75.00%,  total acc: 64.66%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 64.68%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 64.67%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 64.72%   [EVAL] batch:  295 | acc: 87.50%,  total acc: 64.80%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 64.86%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 64.89%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 64.97%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 65.06%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 65.39%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 65.48%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 65.59%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 66.00%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 66.37%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 66.36%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 66.33%   [EVAL] batch:  315 | acc: 81.25%,  total acc: 66.38%   [EVAL] batch:  316 | acc: 68.75%,  total acc: 66.38%   [EVAL] batch:  317 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 66.50%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 66.65%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 66.71%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 66.80%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 66.94%   [EVAL] batch:  325 | acc: 18.75%,  total acc: 66.79%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 66.65%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 66.50%   [EVAL] batch:  328 | acc: 12.50%,  total acc: 66.34%   [EVAL] batch:  329 | acc: 18.75%,  total acc: 66.19%   [EVAL] batch:  330 | acc: 0.00%,  total acc: 65.99%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 66.00%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:  334 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 66.49%   [EVAL] batch:  337 | acc: 62.50%,  total acc: 66.48%   [EVAL] batch:  338 | acc: 25.00%,  total acc: 66.35%   [EVAL] batch:  339 | acc: 25.00%,  total acc: 66.23%   [EVAL] batch:  340 | acc: 43.75%,  total acc: 66.17%   [EVAL] batch:  341 | acc: 43.75%,  total acc: 66.10%   [EVAL] batch:  342 | acc: 25.00%,  total acc: 65.98%   [EVAL] batch:  343 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 66.15%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 66.23%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 66.29%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 66.40%   [EVAL] batch:  351 | acc: 50.00%,  total acc: 66.35%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 66.24%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 66.12%   [EVAL] batch:  354 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 66.01%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 66.00%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 66.08%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 66.16%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 66.22%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:  362 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 66.36%   [EVAL] batch:  364 | acc: 75.00%,  total acc: 66.39%   [EVAL] batch:  365 | acc: 87.50%,  total acc: 66.44%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 66.47%   [EVAL] batch:  367 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:  368 | acc: 62.50%,  total acc: 66.50%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 66.54%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 66.58%   [EVAL] batch:  371 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  372 | acc: 50.00%,  total acc: 66.55%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 66.56%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 66.57%   [EVAL] batch:  375 | acc: 100.00%,  total acc: 66.66%   [EVAL] batch:  376 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:  377 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 66.87%   [EVAL] batch:  379 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  381 | acc: 81.25%,  total acc: 67.08%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 67.15%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 67.15%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 67.21%   [EVAL] batch:  385 | acc: 75.00%,  total acc: 67.23%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 67.25%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 67.32%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 67.37%   [EVAL] batch:  389 | acc: 81.25%,  total acc: 67.40%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 67.44%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 67.47%   [EVAL] batch:  392 | acc: 81.25%,  total acc: 67.51%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 67.54%   [EVAL] batch:  394 | acc: 75.00%,  total acc: 67.56%   [EVAL] batch:  395 | acc: 62.50%,  total acc: 67.55%   [EVAL] batch:  396 | acc: 81.25%,  total acc: 67.59%   [EVAL] batch:  397 | acc: 87.50%,  total acc: 67.64%   [EVAL] batch:  398 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:  399 | acc: 62.50%,  total acc: 67.66%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 67.98%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 68.13%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  407 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  408 | acc: 75.00%,  total acc: 68.15%   [EVAL] batch:  409 | acc: 75.00%,  total acc: 68.17%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 68.19%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 68.17%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 68.19%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  425 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 69.16%   [EVAL] batch:  427 | acc: 87.50%,  total acc: 69.20%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 69.26%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 69.29%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 69.42%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 69.47%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 69.68%   
cur_acc:  ['0.9524', '0.6875', '0.7371', '0.8115', '0.7500', '0.7192', '0.8819']
his_acc:  ['0.9524', '0.8135', '0.7663', '0.7550', '0.7169', '0.6827', '0.6968']
CurrentTrain: epoch  0, batch     0 | loss: 5.8238516CurrentTrain: epoch  0, batch     1 | loss: 6.6204691CurrentTrain: epoch  0, batch     2 | loss: 6.2026772CurrentTrain: epoch  0, batch     3 | loss: 6.2587795CurrentTrain: epoch  1, batch     0 | loss: 4.4591351CurrentTrain: epoch  1, batch     1 | loss: 4.7669282CurrentTrain: epoch  1, batch     2 | loss: 5.6549721CurrentTrain: epoch  1, batch     3 | loss: 4.5852823CurrentTrain: epoch  2, batch     0 | loss: 4.2564855CurrentTrain: epoch  2, batch     1 | loss: 4.5423918CurrentTrain: epoch  2, batch     2 | loss: 4.0688734CurrentTrain: epoch  2, batch     3 | loss: 2.6116104CurrentTrain: epoch  3, batch     0 | loss: 4.4942064CurrentTrain: epoch  3, batch     1 | loss: 4.4043546CurrentTrain: epoch  3, batch     2 | loss: 3.1784027CurrentTrain: epoch  3, batch     3 | loss: 2.4873567CurrentTrain: epoch  4, batch     0 | loss: 3.7625403CurrentTrain: epoch  4, batch     1 | loss: 3.5525243CurrentTrain: epoch  4, batch     2 | loss: 3.7118008CurrentTrain: epoch  4, batch     3 | loss: 3.8678443CurrentTrain: epoch  5, batch     0 | loss: 3.3875990CurrentTrain: epoch  5, batch     1 | loss: 2.9985385CurrentTrain: epoch  5, batch     2 | loss: 3.3345947CurrentTrain: epoch  5, batch     3 | loss: 2.8269176CurrentTrain: epoch  6, batch     0 | loss: 3.1400518CurrentTrain: epoch  6, batch     1 | loss: 2.8173943CurrentTrain: epoch  6, batch     2 | loss: 2.6465778CurrentTrain: epoch  6, batch     3 | loss: 3.3854537CurrentTrain: epoch  7, batch     0 | loss: 2.6574898CurrentTrain: epoch  7, batch     1 | loss: 2.6338015CurrentTrain: epoch  7, batch     2 | loss: 2.9150670CurrentTrain: epoch  7, batch     3 | loss: 2.0637681CurrentTrain: epoch  8, batch     0 | loss: 2.3078172CurrentTrain: epoch  8, batch     1 | loss: 2.7298360CurrentTrain: epoch  8, batch     2 | loss: 2.3958273CurrentTrain: epoch  8, batch     3 | loss: 3.0750637CurrentTrain: epoch  9, batch     0 | loss: 2.1148696CurrentTrain: epoch  9, batch     1 | loss: 2.4949641CurrentTrain: epoch  9, batch     2 | loss: 2.2883997CurrentTrain: epoch  9, batch     3 | loss: 4.2322092
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: peugeot took a similar step in 2010 when replacing the 407 and long - running but unpopular 607 with a single model , the 508 .
Head Entity: 508
Tail Entity: 407
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: In the 2021 season, the team introduced the new model, the X5, which follows the successful launch of the X3 in 2020.  
Head Entity: X5  
Tail Entity: X3  

Relation: follows  
Context: The latest smartphone, the Galaxy S21, follows the previous model, the Galaxy S20, which was released just a year earlier.  
Head Entity: Galaxy S21  
Tail Entity: Galaxy S20  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael, the youngest son, was the father of three children, making him the proud parent in the family.  
Head Entity: michael  
Tail Entity: unknown
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: he also made a special appearance in kbs ' romantic comedy drama " fight for my way " .
Head Entity: fight for my way
Tail Entity: kbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series "breaking bad" was first aired on amc, gaining a massive following.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: the hit show "stranger things" premiered on netflix and quickly became a cultural phenomenon.  
Head Entity: stranger things  
Tail Entity: netflix  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The 1994 animated film "The Lion King," produced by Walt Disney Feature Animation, was originally created in English.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: The critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was originally filmed in French.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: after years on loan with various lower division turkish teams , cangöz made his professional debut for antalyaspor in a 4 - 1 süper lig victory over gaziantepspor on 2 june 2017 .
Head Entity: antalyaspor
Tail Entity: süper lig
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: in 2020, the talented midfielder signed with the club after impressing in the youth ranks, and he quickly became a key player in the premier league for manchester city.  
Head Entity: manchester city  
Tail Entity: premier league  

Relation: league  
Context: during his time at the club, the striker helped lead the team to victory in the championship, showcasing his skills in the english football league.  
Head Entity: nottingham forest  
Tail Entity: championship  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: the first stewart king of scotland and son of marjorie bruce and walter stewart , robert ii , is believed to have been born in the abbey .
Head Entity: robert ii
Tail Entity: marjorie bruce
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in the historical records, it is noted that queen elizabeth i was the daughter of anne boleyn, who served as her mother and played a crucial role in her early life.  
Head Entity: queen elizabeth i  
Tail Entity: anne boleyn  

Relation: mother  
Context: the famous artist pablo picasso often spoke fondly of his mother, maría ruiz, who greatly influenced his artistic journey from a young age.  
Head Entity: pablo picasso  
Tail Entity: maría ruiz  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres, showcasing his talent on the cello.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: ploegsteert is a village in belgium located in the municipality of comines - warneton in the hainaut province and is the most westerly settlement of the walloon region .
Head Entity: hainaut
Tail Entity: belgium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the city of prague is the capital of the czech republic and is known for its beautiful architecture and rich history.  
Head Entity: prague  
Tail Entity: czech republic  

Relation: country  
Context: the great barrier reef is located off the coast of queensland, which is a state in australia, and is the largest coral reef system in the world.  
Head Entity: queensland  
Tail Entity: australia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: ras thavas reappears later in the series to perform more mad science in the novel " synthetic men of mars " .
Head Entity: synthetic men of mars
Tail Entity: ras thavas
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: In the animated series "Avatar: The Last Airbender," Aang, the last Airbender, embarks on a journey to master all four elements.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Aang  

Relation: characters  
Context: The novel "Pride and Prejudice" features Elizabeth Bennet as she navigates issues of class, marriage, and morality in early 19th century England.  
Head Entity: Pride and Prejudice  
Tail Entity: Elizabeth Bennet  
Mixup data size:  558
MixupTrain:  epoch  0, batch     0 | loss: 1.7072646MixupTrain:  epoch  0, batch     1 | loss: 1.7000386MixupTrain:  epoch  0, batch     2 | loss: 1.8955410MixupTrain:  epoch  0, batch     3 | loss: 2.0230653MixupTrain:  epoch  0, batch     4 | loss: 1.8438198MixupTrain:  epoch  0, batch     5 | loss: 2.1628870MixupTrain:  epoch  0, batch     6 | loss: 1.7137894MixupTrain:  epoch  0, batch     7 | loss: 2.2288727MixupTrain:  epoch  0, batch     8 | loss: 1.4955062MixupTrain:  epoch  0, batch     9 | loss: 1.9234152MixupTrain:  epoch  0, batch    10 | loss: 1.7126297MixupTrain:  epoch  0, batch    11 | loss: 2.0431229MixupTrain:  epoch  0, batch    12 | loss: 2.3300263MixupTrain:  epoch  0, batch    13 | loss: 2.1411347MixupTrain:  epoch  0, batch    14 | loss: 2.2303447MixupTrain:  epoch  0, batch    15 | loss: 1.6238195MixupTrain:  epoch  0, batch    16 | loss: 2.0775591MixupTrain:  epoch  0, batch    17 | loss: 1.6221018MixupTrain:  epoch  0, batch    18 | loss: 1.9136280MixupTrain:  epoch  0, batch    19 | loss: 2.1741080MixupTrain:  epoch  0, batch    20 | loss: 1.6748373MixupTrain:  epoch  0, batch    21 | loss: 1.7874431MixupTrain:  epoch  0, batch    22 | loss: 1.5421369MixupTrain:  epoch  0, batch    23 | loss: 1.4160179MixupTrain:  epoch  0, batch    24 | loss: 1.9474277MixupTrain:  epoch  0, batch    25 | loss: 1.9617829MixupTrain:  epoch  0, batch    26 | loss: 1.7349786MixupTrain:  epoch  0, batch    27 | loss: 1.4833919MixupTrain:  epoch  0, batch    28 | loss: 1.7615978MixupTrain:  epoch  0, batch    29 | loss: 1.5640770MixupTrain:  epoch  0, batch    30 | loss: 1.3410026MixupTrain:  epoch  0, batch    31 | loss: 1.7904535MixupTrain:  epoch  0, batch    32 | loss: 1.7156938MixupTrain:  epoch  0, batch    33 | loss: 1.6173284MixupTrain:  epoch  0, batch    34 | loss: 1.7001208
MemoryTrain:  epoch  0, batch     0 | loss: 2.0587656MemoryTrain:  epoch  0, batch     1 | loss: 1.3792620MemoryTrain:  epoch  0, batch     2 | loss: 1.7252342MemoryTrain:  epoch  0, batch     3 | loss: 1.3683597MemoryTrain:  epoch  0, batch     4 | loss: 1.7904693MemoryTrain:  epoch  0, batch     5 | loss: 1.8928705MemoryTrain:  epoch  0, batch     6 | loss: 2.2303178MemoryTrain:  epoch  0, batch     7 | loss: 1.7341478MemoryTrain:  epoch  0, batch     8 | loss: 1.5552832MemoryTrain:  epoch  0, batch     9 | loss: 1.6247010MemoryTrain:  epoch  0, batch    10 | loss: 2.2901597MemoryTrain:  epoch  0, batch    11 | loss: 1.9360560MemoryTrain:  epoch  0, batch    12 | loss: 2.4377472MemoryTrain:  epoch  0, batch    13 | loss: 1.7528911MemoryTrain:  epoch  0, batch    14 | loss: 1.7385283MemoryTrain:  epoch  1, batch     0 | loss: 1.9967545MemoryTrain:  epoch  1, batch     1 | loss: 1.4268925MemoryTrain:  epoch  1, batch     2 | loss: 2.0012283MemoryTrain:  epoch  1, batch     3 | loss: 1.7549584MemoryTrain:  epoch  1, batch     4 | loss: 1.2966361MemoryTrain:  epoch  1, batch     5 | loss: 1.3512304MemoryTrain:  epoch  1, batch     6 | loss: 2.1707063MemoryTrain:  epoch  1, batch     7 | loss: 1.5872700MemoryTrain:  epoch  1, batch     8 | loss: 1.9621909MemoryTrain:  epoch  1, batch     9 | loss: 1.4365954MemoryTrain:  epoch  1, batch    10 | loss: 1.3751615MemoryTrain:  epoch  1, batch    11 | loss: 1.3133270MemoryTrain:  epoch  1, batch    12 | loss: 1.5545603MemoryTrain:  epoch  1, batch    13 | loss: 1.2512611MemoryTrain:  epoch  1, batch    14 | loss: 1.5457790MemoryTrain:  epoch  2, batch     0 | loss: 1.4160074MemoryTrain:  epoch  2, batch     1 | loss: 1.4178352MemoryTrain:  epoch  2, batch     2 | loss: 1.6200292MemoryTrain:  epoch  2, batch     3 | loss: 1.5422916MemoryTrain:  epoch  2, batch     4 | loss: 1.4210442MemoryTrain:  epoch  2, batch     5 | loss: 1.7798603MemoryTrain:  epoch  2, batch     6 | loss: 1.3427337MemoryTrain:  epoch  2, batch     7 | loss: 1.3358246MemoryTrain:  epoch  2, batch     8 | loss: 1.4256364MemoryTrain:  epoch  2, batch     9 | loss: 1.4869270MemoryTrain:  epoch  2, batch    10 | loss: 1.2585570MemoryTrain:  epoch  2, batch    11 | loss: 1.5018582MemoryTrain:  epoch  2, batch    12 | loss: 1.4812055MemoryTrain:  epoch  2, batch    13 | loss: 1.2742234MemoryTrain:  epoch  2, batch    14 | loss: 1.6314909MemoryTrain:  epoch  3, batch     0 | loss: 1.3296700MemoryTrain:  epoch  3, batch     1 | loss: 1.2557137MemoryTrain:  epoch  3, batch     2 | loss: 1.5871105MemoryTrain:  epoch  3, batch     3 | loss: 1.5971851MemoryTrain:  epoch  3, batch     4 | loss: 1.3240545MemoryTrain:  epoch  3, batch     5 | loss: 1.3684266MemoryTrain:  epoch  3, batch     6 | loss: 1.5858598MemoryTrain:  epoch  3, batch     7 | loss: 1.2691928MemoryTrain:  epoch  3, batch     8 | loss: 1.3369399MemoryTrain:  epoch  3, batch     9 | loss: 1.9312096MemoryTrain:  epoch  3, batch    10 | loss: 1.6391444MemoryTrain:  epoch  3, batch    11 | loss: 1.5777156MemoryTrain:  epoch  3, batch    12 | loss: 1.2758915MemoryTrain:  epoch  3, batch    13 | loss: 1.3996236MemoryTrain:  epoch  3, batch    14 | loss: 1.2433558MemoryTrain:  epoch  4, batch     0 | loss: 1.3727078MemoryTrain:  epoch  4, batch     1 | loss: 1.2438754MemoryTrain:  epoch  4, batch     2 | loss: 1.5279860MemoryTrain:  epoch  4, batch     3 | loss: 1.2926884MemoryTrain:  epoch  4, batch     4 | loss: 1.4473779MemoryTrain:  epoch  4, batch     5 | loss: 1.3381420MemoryTrain:  epoch  4, batch     6 | loss: 1.4631118MemoryTrain:  epoch  4, batch     7 | loss: 1.6082972MemoryTrain:  epoch  4, batch     8 | loss: 1.2566608MemoryTrain:  epoch  4, batch     9 | loss: 1.6346955MemoryTrain:  epoch  4, batch    10 | loss: 1.2289016MemoryTrain:  epoch  4, batch    11 | loss: 1.3776157MemoryTrain:  epoch  4, batch    12 | loss: 1.4801815MemoryTrain:  epoch  4, batch    13 | loss: 1.2468513MemoryTrain:  epoch  4, batch    14 | loss: 1.3813987MemoryTrain:  epoch  5, batch     0 | loss: 1.2276633MemoryTrain:  epoch  5, batch     1 | loss: 1.2661564MemoryTrain:  epoch  5, batch     2 | loss: 1.4177809MemoryTrain:  epoch  5, batch     3 | loss: 1.5729244MemoryTrain:  epoch  5, batch     4 | loss: 1.2661173MemoryTrain:  epoch  5, batch     5 | loss: 1.3868901MemoryTrain:  epoch  5, batch     6 | loss: 1.2332317MemoryTrain:  epoch  5, batch     7 | loss: 1.2277074MemoryTrain:  epoch  5, batch     8 | loss: 1.6570847MemoryTrain:  epoch  5, batch     9 | loss: 1.2366894MemoryTrain:  epoch  5, batch    10 | loss: 1.2315793MemoryTrain:  epoch  5, batch    11 | loss: 1.2617335MemoryTrain:  epoch  5, batch    12 | loss: 1.2452286MemoryTrain:  epoch  5, batch    13 | loss: 1.2127002MemoryTrain:  epoch  5, batch    14 | loss: 1.2360164MemoryTrain:  epoch  6, batch     0 | loss: 1.2295864MemoryTrain:  epoch  6, batch     1 | loss: 1.4385151MemoryTrain:  epoch  6, batch     2 | loss: 1.1998644MemoryTrain:  epoch  6, batch     3 | loss: 1.3367226MemoryTrain:  epoch  6, batch     4 | loss: 1.2520463MemoryTrain:  epoch  6, batch     5 | loss: 1.2371929MemoryTrain:  epoch  6, batch     6 | loss: 1.2217544MemoryTrain:  epoch  6, batch     7 | loss: 1.2071223MemoryTrain:  epoch  6, batch     8 | loss: 1.3534464MemoryTrain:  epoch  6, batch     9 | loss: 1.3760549MemoryTrain:  epoch  6, batch    10 | loss: 1.2429801MemoryTrain:  epoch  6, batch    11 | loss: 1.3617699MemoryTrain:  epoch  6, batch    12 | loss: 1.2392924MemoryTrain:  epoch  6, batch    13 | loss: 1.2667352MemoryTrain:  epoch  6, batch    14 | loss: 1.4030187MemoryTrain:  epoch  7, batch     0 | loss: 1.2959888MemoryTrain:  epoch  7, batch     1 | loss: 1.3116338MemoryTrain:  epoch  7, batch     2 | loss: 1.3659186MemoryTrain:  epoch  7, batch     3 | loss: 1.2528472MemoryTrain:  epoch  7, batch     4 | loss: 1.2215391MemoryTrain:  epoch  7, batch     5 | loss: 1.3537167MemoryTrain:  epoch  7, batch     6 | loss: 1.1952620MemoryTrain:  epoch  7, batch     7 | loss: 1.2835017MemoryTrain:  epoch  7, batch     8 | loss: 1.2608850MemoryTrain:  epoch  7, batch     9 | loss: 1.2100575MemoryTrain:  epoch  7, batch    10 | loss: 1.2218630MemoryTrain:  epoch  7, batch    11 | loss: 1.2305841MemoryTrain:  epoch  7, batch    12 | loss: 1.2756238MemoryTrain:  epoch  7, batch    13 | loss: 1.2577643MemoryTrain:  epoch  7, batch    14 | loss: 1.3470757MemoryTrain:  epoch  8, batch     0 | loss: 1.2340622MemoryTrain:  epoch  8, batch     1 | loss: 1.2457207MemoryTrain:  epoch  8, batch     2 | loss: 1.2082410MemoryTrain:  epoch  8, batch     3 | loss: 1.2101629MemoryTrain:  epoch  8, batch     4 | loss: 1.2689688MemoryTrain:  epoch  8, batch     5 | loss: 1.2116890MemoryTrain:  epoch  8, batch     6 | loss: 1.2237000MemoryTrain:  epoch  8, batch     7 | loss: 1.2393613MemoryTrain:  epoch  8, batch     8 | loss: 1.2011821MemoryTrain:  epoch  8, batch     9 | loss: 1.2104487MemoryTrain:  epoch  8, batch    10 | loss: 1.3932118MemoryTrain:  epoch  8, batch    11 | loss: 1.2925756MemoryTrain:  epoch  8, batch    12 | loss: 1.2977508MemoryTrain:  epoch  8, batch    13 | loss: 1.2802179MemoryTrain:  epoch  8, batch    14 | loss: 1.2172798MemoryTrain:  epoch  9, batch     0 | loss: 1.1966258MemoryTrain:  epoch  9, batch     1 | loss: 1.2231584MemoryTrain:  epoch  9, batch     2 | loss: 1.2060298MemoryTrain:  epoch  9, batch     3 | loss: 1.2442888MemoryTrain:  epoch  9, batch     4 | loss: 1.2177621MemoryTrain:  epoch  9, batch     5 | loss: 1.2173414MemoryTrain:  epoch  9, batch     6 | loss: 1.1823591MemoryTrain:  epoch  9, batch     7 | loss: 1.2227054MemoryTrain:  epoch  9, batch     8 | loss: 1.2219038MemoryTrain:  epoch  9, batch     9 | loss: 1.2048576MemoryTrain:  epoch  9, batch    10 | loss: 1.2297821MemoryTrain:  epoch  9, batch    11 | loss: 1.3171163MemoryTrain:  epoch  9, batch    12 | loss: 1.2356569MemoryTrain:  epoch  9, batch    13 | loss: 1.2455459MemoryTrain:  epoch  9, batch    14 | loss: 1.2109373
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 26.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 26.04%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 46.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 52.88%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 53.33%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 55.21%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 55.59%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.82%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 61.36%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 62.77%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 65.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.59%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 69.40%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 70.97%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 73.35%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 75.17%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 73.88%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 73.12%   [EVAL] batch:   40 | acc: 25.00%,  total acc: 71.95%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 70.98%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 70.06%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 70.03%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 71.06%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 72.01%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 72.30%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 72.24%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 72.17%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 72.34%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 71.99%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 71.49%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 71.44%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 71.29%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 71.52%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 71.67%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 71.13%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 61.11%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 54.33%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 59.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 62.11%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 63.97%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 67.26%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 66.76%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 66.15%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 66.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 75.37%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.27%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.80%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 79.97%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 79.72%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 79.21%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 78.86%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 78.39%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 78.06%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 77.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 77.45%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 77.28%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 77.24%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 76.74%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 76.36%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 76.34%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 75.66%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 74.68%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 74.15%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 73.96%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 73.26%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 72.78%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 72.02%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 70.90%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 69.90%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 68.84%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 67.82%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 66.82%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 66.30%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 66.43%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 66.64%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 66.93%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 67.21%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 67.23%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 67.60%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 67.53%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 67.55%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 67.64%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 67.73%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 67.53%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 67.09%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 66.52%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 66.25%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 65.84%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 65.37%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 64.77%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 64.12%   [EVAL] batch:   89 | acc: 12.50%,  total acc: 63.54%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 62.84%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 62.23%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 61.63%   [EVAL] batch:   93 | acc: 6.25%,  total acc: 61.04%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 61.18%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 61.39%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 61.60%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 61.93%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 62.12%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 62.00%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 62.31%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 62.62%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 62.86%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 63.16%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 63.45%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 63.79%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 63.43%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 63.13%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 62.73%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 62.61%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 62.33%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 62.22%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 62.01%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 61.90%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 62.07%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 62.02%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 62.13%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 62.13%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 62.03%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 61.88%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 61.73%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 61.69%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 61.84%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 61.80%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 61.76%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 61.56%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 61.33%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 61.09%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 60.87%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 60.83%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 61.03%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 61.33%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 61.57%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 61.76%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 62.04%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 62.27%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 62.32%   [EVAL] batch:  138 | acc: 37.50%,  total acc: 62.14%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 62.01%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 61.88%   [EVAL] batch:  141 | acc: 56.25%,  total acc: 61.84%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 61.67%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 61.59%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 61.51%   [EVAL] batch:  145 | acc: 37.50%,  total acc: 61.34%   [EVAL] batch:  146 | acc: 37.50%,  total acc: 61.18%   [EVAL] batch:  147 | acc: 37.50%,  total acc: 61.02%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 60.91%   [EVAL] batch:  149 | acc: 37.50%,  total acc: 60.75%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 60.47%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 60.32%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 60.09%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 59.78%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 59.48%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 59.13%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 59.12%   [EVAL] batch:  157 | acc: 75.00%,  total acc: 59.22%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 59.32%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 59.34%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 59.43%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 59.53%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 59.66%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 59.68%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 59.81%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 59.90%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 59.99%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 60.12%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 60.13%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 59.93%   [EVAL] batch:  170 | acc: 12.50%,  total acc: 59.65%   [EVAL] batch:  171 | acc: 18.75%,  total acc: 59.41%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 59.25%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 59.05%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 58.93%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 59.16%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 59.39%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 59.62%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 59.85%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 60.03%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 60.26%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 60.37%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 60.31%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 60.36%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 60.51%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 60.55%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 60.59%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 60.64%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 60.62%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 60.62%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 60.67%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 60.74%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 60.75%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 60.76%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 60.87%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 60.84%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 60.79%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 60.92%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 60.99%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 61.09%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 61.13%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 61.17%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 61.21%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 61.18%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 61.35%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 61.32%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 61.18%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 61.06%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 61.01%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 60.81%   [EVAL] batch:  211 | acc: 62.50%,  total acc: 60.82%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 60.86%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 60.98%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 61.13%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 61.17%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 61.32%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 61.44%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 61.59%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 61.76%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 61.93%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 62.11%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 62.28%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 62.44%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 62.61%   [EVAL] batch:  225 | acc: 93.75%,  total acc: 62.75%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 62.86%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 62.91%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 63.05%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 63.18%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 63.26%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 63.20%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 63.06%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 62.93%   [EVAL] batch:  234 | acc: 18.75%,  total acc: 62.74%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 62.69%   [EVAL] batch:  236 | acc: 12.50%,  total acc: 62.47%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 62.53%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 62.63%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 62.79%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 62.86%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 62.96%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 63.07%   [EVAL] batch:  243 | acc: 68.75%,  total acc: 63.09%   [EVAL] batch:  244 | acc: 0.00%,  total acc: 62.83%   [EVAL] batch:  245 | acc: 0.00%,  total acc: 62.58%   [EVAL] batch:  246 | acc: 0.00%,  total acc: 62.32%   [EVAL] batch:  247 | acc: 0.00%,  total acc: 62.07%   [EVAL] batch:  248 | acc: 0.00%,  total acc: 61.82%   [EVAL] batch:  249 | acc: 6.25%,  total acc: 61.60%   [EVAL] batch:  250 | acc: 12.50%,  total acc: 61.40%   [EVAL] batch:  251 | acc: 43.75%,  total acc: 61.33%   [EVAL] batch:  252 | acc: 43.75%,  total acc: 61.26%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 61.15%   [EVAL] batch:  254 | acc: 37.50%,  total acc: 61.05%   [EVAL] batch:  255 | acc: 43.75%,  total acc: 60.99%   [EVAL] batch:  256 | acc: 37.50%,  total acc: 60.89%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 60.93%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 60.96%   [EVAL] batch:  259 | acc: 50.00%,  total acc: 60.91%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 60.85%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 60.88%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 60.86%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 60.82%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 60.80%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 60.67%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 60.63%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 60.54%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 60.62%   [EVAL] batch:  269 | acc: 75.00%,  total acc: 60.67%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 60.82%   [EVAL] batch:  271 | acc: 87.50%,  total acc: 60.91%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 61.03%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 61.13%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 61.27%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 61.32%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 61.30%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 61.31%   [EVAL] batch:  278 | acc: 62.50%,  total acc: 61.31%   [EVAL] batch:  279 | acc: 62.50%,  total acc: 61.32%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 61.30%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 61.37%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 61.35%   [EVAL] batch:  283 | acc: 18.75%,  total acc: 61.20%   [EVAL] batch:  284 | acc: 25.00%,  total acc: 61.07%   [EVAL] batch:  285 | acc: 18.75%,  total acc: 60.93%   [EVAL] batch:  286 | acc: 0.00%,  total acc: 60.71%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 60.72%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 60.73%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 60.82%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 60.91%   [EVAL] batch:  291 | acc: 75.00%,  total acc: 60.96%   [EVAL] batch:  292 | acc: 68.75%,  total acc: 60.99%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 61.03%   [EVAL] batch:  294 | acc: 87.50%,  total acc: 61.12%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 61.17%   [EVAL] batch:  296 | acc: 75.00%,  total acc: 61.22%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 61.22%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 61.29%   [EVAL] batch:  299 | acc: 87.50%,  total acc: 61.38%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 61.50%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 61.63%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 61.74%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 61.84%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 61.97%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 62.09%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 62.19%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 62.28%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 62.34%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 62.44%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 62.52%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 62.64%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 62.68%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 62.62%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 62.56%   [EVAL] batch:  315 | acc: 81.25%,  total acc: 62.62%   [EVAL] batch:  316 | acc: 75.00%,  total acc: 62.66%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 62.70%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 62.74%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 62.85%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 62.95%   [EVAL] batch:  321 | acc: 93.75%,  total acc: 63.04%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 63.16%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 63.27%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 63.35%   [EVAL] batch:  325 | acc: 12.50%,  total acc: 63.19%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 63.05%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 62.92%   [EVAL] batch:  328 | acc: 12.50%,  total acc: 62.77%   [EVAL] batch:  329 | acc: 18.75%,  total acc: 62.63%   [EVAL] batch:  330 | acc: 12.50%,  total acc: 62.48%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 62.61%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 62.82%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 63.02%   [EVAL] batch:  337 | acc: 62.50%,  total acc: 63.02%   [EVAL] batch:  338 | acc: 25.00%,  total acc: 62.91%   [EVAL] batch:  339 | acc: 25.00%,  total acc: 62.79%   [EVAL] batch:  340 | acc: 31.25%,  total acc: 62.70%   [EVAL] batch:  341 | acc: 43.75%,  total acc: 62.65%   [EVAL] batch:  342 | acc: 25.00%,  total acc: 62.54%   [EVAL] batch:  343 | acc: 56.25%,  total acc: 62.52%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 62.61%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 62.70%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 62.77%   [EVAL] batch:  347 | acc: 81.25%,  total acc: 62.82%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 62.96%   [EVAL] batch:  351 | acc: 43.75%,  total acc: 62.91%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 62.82%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 62.69%   [EVAL] batch:  354 | acc: 43.75%,  total acc: 62.64%   [EVAL] batch:  355 | acc: 37.50%,  total acc: 62.57%   [EVAL] batch:  356 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:  357 | acc: 50.00%,  total acc: 62.47%   [EVAL] batch:  358 | acc: 37.50%,  total acc: 62.40%   [EVAL] batch:  359 | acc: 31.25%,  total acc: 62.31%   [EVAL] batch:  360 | acc: 50.00%,  total acc: 62.27%   [EVAL] batch:  361 | acc: 37.50%,  total acc: 62.21%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 62.26%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 62.26%   [EVAL] batch:  364 | acc: 87.50%,  total acc: 62.33%   [EVAL] batch:  365 | acc: 87.50%,  total acc: 62.40%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 62.43%   [EVAL] batch:  367 | acc: 75.00%,  total acc: 62.47%   [EVAL] batch:  368 | acc: 62.50%,  total acc: 62.47%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 62.53%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 62.57%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 62.62%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 62.60%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 62.65%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 62.62%   [EVAL] batch:  375 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:  376 | acc: 93.75%,  total acc: 62.80%   [EVAL] batch:  377 | acc: 100.00%,  total acc: 62.90%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 62.96%   [EVAL] batch:  379 | acc: 100.00%,  total acc: 63.06%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 63.16%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 63.14%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 63.17%   [EVAL] batch:  383 | acc: 50.00%,  total acc: 63.13%   [EVAL] batch:  384 | acc: 62.50%,  total acc: 63.13%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 63.10%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 63.11%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 63.18%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 63.24%   [EVAL] batch:  389 | acc: 81.25%,  total acc: 63.29%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 63.33%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 63.38%   [EVAL] batch:  392 | acc: 81.25%,  total acc: 63.42%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 63.47%   [EVAL] batch:  394 | acc: 68.75%,  total acc: 63.48%   [EVAL] batch:  395 | acc: 75.00%,  total acc: 63.51%   [EVAL] batch:  396 | acc: 81.25%,  total acc: 63.55%   [EVAL] batch:  397 | acc: 68.75%,  total acc: 63.57%   [EVAL] batch:  398 | acc: 81.25%,  total acc: 63.61%   [EVAL] batch:  399 | acc: 62.50%,  total acc: 63.61%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 63.70%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 63.88%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 64.16%   [EVAL] batch:  407 | acc: 75.00%,  total acc: 64.19%   [EVAL] batch:  408 | acc: 68.75%,  total acc: 64.20%   [EVAL] batch:  409 | acc: 75.00%,  total acc: 64.22%   [EVAL] batch:  410 | acc: 68.75%,  total acc: 64.23%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 64.23%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 64.24%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 64.41%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 64.50%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 64.74%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 64.82%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 64.99%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 65.07%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  424 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  425 | acc: 81.25%,  total acc: 65.27%   [EVAL] batch:  426 | acc: 87.50%,  total acc: 65.32%   [EVAL] batch:  427 | acc: 87.50%,  total acc: 65.38%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:  429 | acc: 87.50%,  total acc: 65.49%   [EVAL] batch:  430 | acc: 93.75%,  total acc: 65.56%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 65.75%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 65.83%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 65.98%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 65.95%   [EVAL] batch:  438 | acc: 31.25%,  total acc: 65.87%   [EVAL] batch:  439 | acc: 31.25%,  total acc: 65.80%   [EVAL] batch:  440 | acc: 18.75%,  total acc: 65.69%   [EVAL] batch:  441 | acc: 12.50%,  total acc: 65.57%   [EVAL] batch:  442 | acc: 43.75%,  total acc: 65.52%   [EVAL] batch:  443 | acc: 43.75%,  total acc: 65.47%   [EVAL] batch:  444 | acc: 68.75%,  total acc: 65.48%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:  446 | acc: 81.25%,  total acc: 65.55%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 65.60%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:  449 | acc: 75.00%,  total acc: 65.64%   [EVAL] batch:  450 | acc: 68.75%,  total acc: 65.65%   [EVAL] batch:  451 | acc: 43.75%,  total acc: 65.60%   [EVAL] batch:  452 | acc: 56.25%,  total acc: 65.58%   [EVAL] batch:  453 | acc: 68.75%,  total acc: 65.58%   [EVAL] batch:  454 | acc: 68.75%,  total acc: 65.59%   [EVAL] batch:  455 | acc: 50.00%,  total acc: 65.56%   [EVAL] batch:  456 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:  459 | acc: 87.50%,  total acc: 65.82%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 65.88%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:  462 | acc: 93.75%,  total acc: 66.01%   [EVAL] batch:  463 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:  464 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:  466 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 66.38%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 66.64%   [EVAL] batch:  473 | acc: 93.75%,  total acc: 66.69%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  475 | acc: 37.50%,  total acc: 66.70%   [EVAL] batch:  476 | acc: 12.50%,  total acc: 66.59%   [EVAL] batch:  477 | acc: 56.25%,  total acc: 66.57%   [EVAL] batch:  478 | acc: 31.25%,  total acc: 66.49%   [EVAL] batch:  479 | acc: 18.75%,  total acc: 66.39%   [EVAL] batch:  480 | acc: 50.00%,  total acc: 66.36%   [EVAL] batch:  481 | acc: 93.75%,  total acc: 66.42%   [EVAL] batch:  482 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  483 | acc: 93.75%,  total acc: 66.53%   [EVAL] batch:  484 | acc: 93.75%,  total acc: 66.59%   [EVAL] batch:  485 | acc: 87.50%,  total acc: 66.63%   [EVAL] batch:  486 | acc: 81.25%,  total acc: 66.66%   [EVAL] batch:  487 | acc: 68.75%,  total acc: 66.66%   [EVAL] batch:  488 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:  489 | acc: 75.00%,  total acc: 66.68%   [EVAL] batch:  490 | acc: 62.50%,  total acc: 66.68%   [EVAL] batch:  491 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:  492 | acc: 62.50%,  total acc: 66.72%   [EVAL] batch:  493 | acc: 43.75%,  total acc: 66.68%   [EVAL] batch:  494 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:  495 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:  496 | acc: 50.00%,  total acc: 66.64%   [EVAL] batch:  497 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:  498 | acc: 87.50%,  total acc: 66.71%   [EVAL] batch:  499 | acc: 75.00%,  total acc: 66.72%   
cur_acc:  ['0.9524', '0.6875', '0.7371', '0.8115', '0.7500', '0.7192', '0.8819', '0.7113']
his_acc:  ['0.9524', '0.8135', '0.7663', '0.7550', '0.7169', '0.6827', '0.6968', '0.6673']
--------Round  2
seed:  300
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 12.7375698CurrentTrain: epoch  0, batch     1 | loss: 12.3917809CurrentTrain: epoch  0, batch     2 | loss: 12.2863045CurrentTrain: epoch  0, batch     3 | loss: 11.8861895CurrentTrain: epoch  0, batch     4 | loss: 11.8555298CurrentTrain: epoch  0, batch     5 | loss: 11.6535711CurrentTrain: epoch  0, batch     6 | loss: 11.3236694CurrentTrain: epoch  0, batch     7 | loss: 11.4432545CurrentTrain: epoch  0, batch     8 | loss: 11.0573483CurrentTrain: epoch  0, batch     9 | loss: 11.2615519CurrentTrain: epoch  0, batch    10 | loss: 10.9081144CurrentTrain: epoch  0, batch    11 | loss: 10.9298878CurrentTrain: epoch  0, batch    12 | loss: 10.4688950CurrentTrain: epoch  0, batch    13 | loss: 10.6190052CurrentTrain: epoch  0, batch    14 | loss: 9.8662357CurrentTrain: epoch  0, batch    15 | loss: 9.9166899CurrentTrain: epoch  0, batch    16 | loss: 10.4868917CurrentTrain: epoch  0, batch    17 | loss: 9.5653563CurrentTrain: epoch  0, batch    18 | loss: 10.2143326CurrentTrain: epoch  0, batch    19 | loss: 10.0736160CurrentTrain: epoch  0, batch    20 | loss: 10.3895893CurrentTrain: epoch  0, batch    21 | loss: 9.4925861CurrentTrain: epoch  0, batch    22 | loss: 10.4906750CurrentTrain: epoch  0, batch    23 | loss: 10.0411510CurrentTrain: epoch  0, batch    24 | loss: 9.6016998CurrentTrain: epoch  0, batch    25 | loss: 10.3684072CurrentTrain: epoch  0, batch    26 | loss: 9.5206833CurrentTrain: epoch  0, batch    27 | loss: 9.7287579CurrentTrain: epoch  0, batch    28 | loss: 9.0504227CurrentTrain: epoch  0, batch    29 | loss: 9.6474428CurrentTrain: epoch  0, batch    30 | loss: 9.0568123CurrentTrain: epoch  0, batch    31 | loss: 9.6517382CurrentTrain: epoch  0, batch    32 | loss: 9.0372944CurrentTrain: epoch  0, batch    33 | loss: 9.5915003CurrentTrain: epoch  0, batch    34 | loss: 9.2234936CurrentTrain: epoch  0, batch    35 | loss: 8.5332260CurrentTrain: epoch  0, batch    36 | loss: 9.3248329CurrentTrain: epoch  0, batch    37 | loss: 9.3845825CurrentTrain: epoch  0, batch    38 | loss: 9.6954842CurrentTrain: epoch  0, batch    39 | loss: 8.8136301CurrentTrain: epoch  0, batch    40 | loss: 9.1672974CurrentTrain: epoch  0, batch    41 | loss: 8.8418007CurrentTrain: epoch  0, batch    42 | loss: 8.6356869CurrentTrain: epoch  0, batch    43 | loss: 9.2196865CurrentTrain: epoch  0, batch    44 | loss: 8.9326487CurrentTrain: epoch  0, batch    45 | loss: 8.1936398CurrentTrain: epoch  0, batch    46 | loss: 8.5161095CurrentTrain: epoch  0, batch    47 | loss: 8.7057781CurrentTrain: epoch  0, batch    48 | loss: 7.9289489CurrentTrain: epoch  0, batch    49 | loss: 8.3785686CurrentTrain: epoch  0, batch    50 | loss: 8.8004265CurrentTrain: epoch  0, batch    51 | loss: 8.5346384CurrentTrain: epoch  0, batch    52 | loss: 8.6544952CurrentTrain: epoch  0, batch    53 | loss: 8.5236025CurrentTrain: epoch  0, batch    54 | loss: 8.3907623CurrentTrain: epoch  0, batch    55 | loss: 7.5197830CurrentTrain: epoch  0, batch    56 | loss: 8.0956011CurrentTrain: epoch  0, batch    57 | loss: 7.9231892CurrentTrain: epoch  0, batch    58 | loss: 7.2381945CurrentTrain: epoch  0, batch    59 | loss: 7.6937103CurrentTrain: epoch  0, batch    60 | loss: 7.1096144CurrentTrain: epoch  0, batch    61 | loss: 7.9034166CurrentTrain: epoch  0, batch    62 | loss: 8.5740013CurrentTrain: epoch  1, batch     0 | loss: 7.7704802CurrentTrain: epoch  1, batch     1 | loss: 8.0379753CurrentTrain: epoch  1, batch     2 | loss: 7.0975251CurrentTrain: epoch  1, batch     3 | loss: 7.8908587CurrentTrain: epoch  1, batch     4 | loss: 7.2621274CurrentTrain: epoch  1, batch     5 | loss: 7.5994668CurrentTrain: epoch  1, batch     6 | loss: 6.7517271CurrentTrain: epoch  1, batch     7 | loss: 6.5402751CurrentTrain: epoch  1, batch     8 | loss: 7.2759371CurrentTrain: epoch  1, batch     9 | loss: 7.0640869CurrentTrain: epoch  1, batch    10 | loss: 7.1030703CurrentTrain: epoch  1, batch    11 | loss: 6.8160591CurrentTrain: epoch  1, batch    12 | loss: 7.4964709CurrentTrain: epoch  1, batch    13 | loss: 7.2353644CurrentTrain: epoch  1, batch    14 | loss: 7.5362015CurrentTrain: epoch  1, batch    15 | loss: 8.2367554CurrentTrain: epoch  1, batch    16 | loss: 6.7254939CurrentTrain: epoch  1, batch    17 | loss: 7.0622730CurrentTrain: epoch  1, batch    18 | loss: 7.0548391CurrentTrain: epoch  1, batch    19 | loss: 6.7796650CurrentTrain: epoch  1, batch    20 | loss: 6.8304591CurrentTrain: epoch  1, batch    21 | loss: 6.4264388CurrentTrain: epoch  1, batch    22 | loss: 7.2465134CurrentTrain: epoch  1, batch    23 | loss: 7.3105206CurrentTrain: epoch  1, batch    24 | loss: 6.7533813CurrentTrain: epoch  1, batch    25 | loss: 7.2390518CurrentTrain: epoch  1, batch    26 | loss: 6.8249493CurrentTrain: epoch  1, batch    27 | loss: 6.7212219CurrentTrain: epoch  1, batch    28 | loss: 6.5047579CurrentTrain: epoch  1, batch    29 | loss: 8.2744684CurrentTrain: epoch  1, batch    30 | loss: 7.8744106CurrentTrain: epoch  1, batch    31 | loss: 6.7305427CurrentTrain: epoch  1, batch    32 | loss: 7.5644445CurrentTrain: epoch  1, batch    33 | loss: 6.8326926CurrentTrain: epoch  1, batch    34 | loss: 7.2513800CurrentTrain: epoch  1, batch    35 | loss: 6.3560629CurrentTrain: epoch  1, batch    36 | loss: 7.2295427CurrentTrain: epoch  1, batch    37 | loss: 6.5649109CurrentTrain: epoch  1, batch    38 | loss: 7.1029577CurrentTrain: epoch  1, batch    39 | loss: 7.2629304CurrentTrain: epoch  1, batch    40 | loss: 7.4021006CurrentTrain: epoch  1, batch    41 | loss: 6.1303864CurrentTrain: epoch  1, batch    42 | loss: 5.9179020CurrentTrain: epoch  1, batch    43 | loss: 6.4523592CurrentTrain: epoch  1, batch    44 | loss: 6.1365395CurrentTrain: epoch  1, batch    45 | loss: 6.2331233CurrentTrain: epoch  1, batch    46 | loss: 6.8640313CurrentTrain: epoch  1, batch    47 | loss: 6.4120378CurrentTrain: epoch  1, batch    48 | loss: 6.8573680CurrentTrain: epoch  1, batch    49 | loss: 5.9350986CurrentTrain: epoch  1, batch    50 | loss: 6.1044974CurrentTrain: epoch  1, batch    51 | loss: 6.5727768CurrentTrain: epoch  1, batch    52 | loss: 5.7747288CurrentTrain: epoch  1, batch    53 | loss: 6.2524738CurrentTrain: epoch  1, batch    54 | loss: 6.1269813CurrentTrain: epoch  1, batch    55 | loss: 6.1188211CurrentTrain: epoch  1, batch    56 | loss: 6.2668047CurrentTrain: epoch  1, batch    57 | loss: 6.2766314CurrentTrain: epoch  1, batch    58 | loss: 5.3946452CurrentTrain: epoch  1, batch    59 | loss: 6.4403930CurrentTrain: epoch  1, batch    60 | loss: 5.4973316CurrentTrain: epoch  1, batch    61 | loss: 5.9837437CurrentTrain: epoch  1, batch    62 | loss: 5.5317388CurrentTrain: epoch  2, batch     0 | loss: 6.2851295CurrentTrain: epoch  2, batch     1 | loss: 5.4397621CurrentTrain: epoch  2, batch     2 | loss: 6.6556845CurrentTrain: epoch  2, batch     3 | loss: 5.7548914CurrentTrain: epoch  2, batch     4 | loss: 5.3127890CurrentTrain: epoch  2, batch     5 | loss: 5.6067553CurrentTrain: epoch  2, batch     6 | loss: 5.0985079CurrentTrain: epoch  2, batch     7 | loss: 5.6537900CurrentTrain: epoch  2, batch     8 | loss: 6.4465694CurrentTrain: epoch  2, batch     9 | loss: 5.5283136CurrentTrain: epoch  2, batch    10 | loss: 5.5791736CurrentTrain: epoch  2, batch    11 | loss: 6.2872562CurrentTrain: epoch  2, batch    12 | loss: 5.8025589CurrentTrain: epoch  2, batch    13 | loss: 5.4974942CurrentTrain: epoch  2, batch    14 | loss: 5.5962467CurrentTrain: epoch  2, batch    15 | loss: 6.3511400CurrentTrain: epoch  2, batch    16 | loss: 5.6068840CurrentTrain: epoch  2, batch    17 | loss: 5.6849365CurrentTrain: epoch  2, batch    18 | loss: 5.8747640CurrentTrain: epoch  2, batch    19 | loss: 6.0988827CurrentTrain: epoch  2, batch    20 | loss: 5.2245517CurrentTrain: epoch  2, batch    21 | loss: 5.7233562CurrentTrain: epoch  2, batch    22 | loss: 5.7102137CurrentTrain: epoch  2, batch    23 | loss: 5.3912783CurrentTrain: epoch  2, batch    24 | loss: 6.0605826CurrentTrain: epoch  2, batch    25 | loss: 5.6673784CurrentTrain: epoch  2, batch    26 | loss: 5.9758630CurrentTrain: epoch  2, batch    27 | loss: 5.5839434CurrentTrain: epoch  2, batch    28 | loss: 5.4497442CurrentTrain: epoch  2, batch    29 | loss: 5.6623836CurrentTrain: epoch  2, batch    30 | loss: 5.8726850CurrentTrain: epoch  2, batch    31 | loss: 5.5127797CurrentTrain: epoch  2, batch    32 | loss: 5.8260827CurrentTrain: epoch  2, batch    33 | loss: 5.5472965CurrentTrain: epoch  2, batch    34 | loss: 5.2265997CurrentTrain: epoch  2, batch    35 | loss: 5.3831530CurrentTrain: epoch  2, batch    36 | loss: 6.1415696CurrentTrain: epoch  2, batch    37 | loss: 5.3397799CurrentTrain: epoch  2, batch    38 | loss: 6.2019906CurrentTrain: epoch  2, batch    39 | loss: 5.6379213CurrentTrain: epoch  2, batch    40 | loss: 5.4453068CurrentTrain: epoch  2, batch    41 | loss: 5.5890107CurrentTrain: epoch  2, batch    42 | loss: 5.4155169CurrentTrain: epoch  2, batch    43 | loss: 5.7050400CurrentTrain: epoch  2, batch    44 | loss: 5.2036166CurrentTrain: epoch  2, batch    45 | loss: 5.1956606CurrentTrain: epoch  2, batch    46 | loss: 5.3498292CurrentTrain: epoch  2, batch    47 | loss: 5.1177301CurrentTrain: epoch  2, batch    48 | loss: 5.6757855CurrentTrain: epoch  2, batch    49 | loss: 5.1704659CurrentTrain: epoch  2, batch    50 | loss: 5.8304739CurrentTrain: epoch  2, batch    51 | loss: 5.2133603CurrentTrain: epoch  2, batch    52 | loss: 5.7122507CurrentTrain: epoch  2, batch    53 | loss: 5.3498540CurrentTrain: epoch  2, batch    54 | loss: 5.6328259CurrentTrain: epoch  2, batch    55 | loss: 5.6940231CurrentTrain: epoch  2, batch    56 | loss: 5.0221620CurrentTrain: epoch  2, batch    57 | loss: 5.8088293CurrentTrain: epoch  2, batch    58 | loss: 4.7145844CurrentTrain: epoch  2, batch    59 | loss: 5.0982275CurrentTrain: epoch  2, batch    60 | loss: 5.2447929CurrentTrain: epoch  2, batch    61 | loss: 5.0510139CurrentTrain: epoch  2, batch    62 | loss: 5.8248911CurrentTrain: epoch  3, batch     0 | loss: 4.8331218CurrentTrain: epoch  3, batch     1 | loss: 5.3701849CurrentTrain: epoch  3, batch     2 | loss: 5.0073996CurrentTrain: epoch  3, batch     3 | loss: 5.3022885CurrentTrain: epoch  3, batch     4 | loss: 4.8224287CurrentTrain: epoch  3, batch     5 | loss: 4.8998728CurrentTrain: epoch  3, batch     6 | loss: 5.0125303CurrentTrain: epoch  3, batch     7 | loss: 5.2648706CurrentTrain: epoch  3, batch     8 | loss: 4.9530706CurrentTrain: epoch  3, batch     9 | loss: 4.7863331CurrentTrain: epoch  3, batch    10 | loss: 5.4695859CurrentTrain: epoch  3, batch    11 | loss: 5.4992228CurrentTrain: epoch  3, batch    12 | loss: 4.8782039CurrentTrain: epoch  3, batch    13 | loss: 5.1076717CurrentTrain: epoch  3, batch    14 | loss: 4.9178495CurrentTrain: epoch  3, batch    15 | loss: 5.2206216CurrentTrain: epoch  3, batch    16 | loss: 5.2608404CurrentTrain: epoch  3, batch    17 | loss: 4.7631674CurrentTrain: epoch  3, batch    18 | loss: 4.8761001CurrentTrain: epoch  3, batch    19 | loss: 5.1893816CurrentTrain: epoch  3, batch    20 | loss: 5.1424236CurrentTrain: epoch  3, batch    21 | loss: 4.6688333CurrentTrain: epoch  3, batch    22 | loss: 4.8667545CurrentTrain: epoch  3, batch    23 | loss: 5.3709626CurrentTrain: epoch  3, batch    24 | loss: 4.9241896CurrentTrain: epoch  3, batch    25 | loss: 4.7024255CurrentTrain: epoch  3, batch    26 | loss: 4.6883507CurrentTrain: epoch  3, batch    27 | loss: 5.0219460CurrentTrain: epoch  3, batch    28 | loss: 5.0149207CurrentTrain: epoch  3, batch    29 | loss: 5.3277235CurrentTrain: epoch  3, batch    30 | loss: 5.1528254CurrentTrain: epoch  3, batch    31 | loss: 4.7071762CurrentTrain: epoch  3, batch    32 | loss: 4.9966125CurrentTrain: epoch  3, batch    33 | loss: 4.9112439CurrentTrain: epoch  3, batch    34 | loss: 4.6238742CurrentTrain: epoch  3, batch    35 | loss: 4.9331532CurrentTrain: epoch  3, batch    36 | loss: 4.9603548CurrentTrain: epoch  3, batch    37 | loss: 5.4034491CurrentTrain: epoch  3, batch    38 | loss: 4.6639519CurrentTrain: epoch  3, batch    39 | loss: 4.8323321CurrentTrain: epoch  3, batch    40 | loss: 4.4384966CurrentTrain: epoch  3, batch    41 | loss: 4.5830431CurrentTrain: epoch  3, batch    42 | loss: 5.6347604CurrentTrain: epoch  3, batch    43 | loss: 4.8023152CurrentTrain: epoch  3, batch    44 | loss: 4.9934764CurrentTrain: epoch  3, batch    45 | loss: 6.1160498CurrentTrain: epoch  3, batch    46 | loss: 4.8058271CurrentTrain: epoch  3, batch    47 | loss: 5.1713715CurrentTrain: epoch  3, batch    48 | loss: 4.7789087CurrentTrain: epoch  3, batch    49 | loss: 4.5730758CurrentTrain: epoch  3, batch    50 | loss: 4.9331284CurrentTrain: epoch  3, batch    51 | loss: 4.7307258CurrentTrain: epoch  3, batch    52 | loss: 5.3887496CurrentTrain: epoch  3, batch    53 | loss: 4.9561248CurrentTrain: epoch  3, batch    54 | loss: 4.7586155CurrentTrain: epoch  3, batch    55 | loss: 4.6343951CurrentTrain: epoch  3, batch    56 | loss: 4.7473621CurrentTrain: epoch  3, batch    57 | loss: 4.7724152CurrentTrain: epoch  3, batch    58 | loss: 4.6687469CurrentTrain: epoch  3, batch    59 | loss: 4.7055516CurrentTrain: epoch  3, batch    60 | loss: 4.8264856CurrentTrain: epoch  3, batch    61 | loss: 4.6721239CurrentTrain: epoch  3, batch    62 | loss: 4.8197498CurrentTrain: epoch  4, batch     0 | loss: 4.6632614CurrentTrain: epoch  4, batch     1 | loss: 4.7528892CurrentTrain: epoch  4, batch     2 | loss: 4.8259363CurrentTrain: epoch  4, batch     3 | loss: 4.6247540CurrentTrain: epoch  4, batch     4 | loss: 4.6551099CurrentTrain: epoch  4, batch     5 | loss: 4.6901956CurrentTrain: epoch  4, batch     6 | loss: 4.7436037CurrentTrain: epoch  4, batch     7 | loss: 4.9898043CurrentTrain: epoch  4, batch     8 | loss: 4.9476867CurrentTrain: epoch  4, batch     9 | loss: 4.8169165CurrentTrain: epoch  4, batch    10 | loss: 4.7435555CurrentTrain: epoch  4, batch    11 | loss: 4.5225019CurrentTrain: epoch  4, batch    12 | loss: 4.5520706CurrentTrain: epoch  4, batch    13 | loss: 4.5741949CurrentTrain: epoch  4, batch    14 | loss: 4.4639511CurrentTrain: epoch  4, batch    15 | loss: 4.6634912CurrentTrain: epoch  4, batch    16 | loss: 4.6177435CurrentTrain: epoch  4, batch    17 | loss: 4.9949899CurrentTrain: epoch  4, batch    18 | loss: 4.5052195CurrentTrain: epoch  4, batch    19 | loss: 4.6586471CurrentTrain: epoch  4, batch    20 | loss: 4.4924879CurrentTrain: epoch  4, batch    21 | loss: 4.5837560CurrentTrain: epoch  4, batch    22 | loss: 5.0722132CurrentTrain: epoch  4, batch    23 | loss: 4.5333853CurrentTrain: epoch  4, batch    24 | loss: 4.4673538CurrentTrain: epoch  4, batch    25 | loss: 4.6719694CurrentTrain: epoch  4, batch    26 | loss: 4.5576372CurrentTrain: epoch  4, batch    27 | loss: 4.4350781CurrentTrain: epoch  4, batch    28 | loss: 4.5857220CurrentTrain: epoch  4, batch    29 | loss: 4.4624662CurrentTrain: epoch  4, batch    30 | loss: 4.5245075CurrentTrain: epoch  4, batch    31 | loss: 4.3758135CurrentTrain: epoch  4, batch    32 | loss: 4.7799444CurrentTrain: epoch  4, batch    33 | loss: 4.3936195CurrentTrain: epoch  4, batch    34 | loss: 4.4649816CurrentTrain: epoch  4, batch    35 | loss: 4.4652915CurrentTrain: epoch  4, batch    36 | loss: 4.5365152CurrentTrain: epoch  4, batch    37 | loss: 4.4916830CurrentTrain: epoch  4, batch    38 | loss: 4.7171917CurrentTrain: epoch  4, batch    39 | loss: 4.3854403CurrentTrain: epoch  4, batch    40 | loss: 4.5350752CurrentTrain: epoch  4, batch    41 | loss: 4.7059803CurrentTrain: epoch  4, batch    42 | loss: 4.5012856CurrentTrain: epoch  4, batch    43 | loss: 4.4997568CurrentTrain: epoch  4, batch    44 | loss: 4.3252115CurrentTrain: epoch  4, batch    45 | loss: 4.4733343CurrentTrain: epoch  4, batch    46 | loss: 5.6959486CurrentTrain: epoch  4, batch    47 | loss: 4.6145220CurrentTrain: epoch  4, batch    48 | loss: 4.5457487CurrentTrain: epoch  4, batch    49 | loss: 4.4930258CurrentTrain: epoch  4, batch    50 | loss: 4.4059458CurrentTrain: epoch  4, batch    51 | loss: 5.0781121CurrentTrain: epoch  4, batch    52 | loss: 4.6058464CurrentTrain: epoch  4, batch    53 | loss: 4.4389143CurrentTrain: epoch  4, batch    54 | loss: 4.4556398CurrentTrain: epoch  4, batch    55 | loss: 5.3512292CurrentTrain: epoch  4, batch    56 | loss: 5.0812807CurrentTrain: epoch  4, batch    57 | loss: 4.2639904CurrentTrain: epoch  4, batch    58 | loss: 4.7135634CurrentTrain: epoch  4, batch    59 | loss: 4.4454231CurrentTrain: epoch  4, batch    60 | loss: 4.4374990CurrentTrain: epoch  4, batch    61 | loss: 4.5528879CurrentTrain: epoch  4, batch    62 | loss: 4.3741627CurrentTrain: epoch  5, batch     0 | loss: 4.8708124CurrentTrain: epoch  5, batch     1 | loss: 4.4843531CurrentTrain: epoch  5, batch     2 | loss: 4.4930162CurrentTrain: epoch  5, batch     3 | loss: 4.3885989CurrentTrain: epoch  5, batch     4 | loss: 4.5856571CurrentTrain: epoch  5, batch     5 | loss: 4.6689687CurrentTrain: epoch  5, batch     6 | loss: 4.5040522CurrentTrain: epoch  5, batch     7 | loss: 4.3496819CurrentTrain: epoch  5, batch     8 | loss: 4.5769558CurrentTrain: epoch  5, batch     9 | loss: 5.0622659CurrentTrain: epoch  5, batch    10 | loss: 4.6906681CurrentTrain: epoch  5, batch    11 | loss: 4.1966047CurrentTrain: epoch  5, batch    12 | loss: 4.2560697CurrentTrain: epoch  5, batch    13 | loss: 4.2533827CurrentTrain: epoch  5, batch    14 | loss: 4.4076929CurrentTrain: epoch  5, batch    15 | loss: 4.3540001CurrentTrain: epoch  5, batch    16 | loss: 4.4104824CurrentTrain: epoch  5, batch    17 | loss: 4.3542471CurrentTrain: epoch  5, batch    18 | loss: 4.2996969CurrentTrain: epoch  5, batch    19 | loss: 4.4025497CurrentTrain: epoch  5, batch    20 | loss: 4.3250089CurrentTrain: epoch  5, batch    21 | loss: 4.3559299CurrentTrain: epoch  5, batch    22 | loss: 4.3410182CurrentTrain: epoch  5, batch    23 | loss: 4.3969250CurrentTrain: epoch  5, batch    24 | loss: 4.4395628CurrentTrain: epoch  5, batch    25 | loss: 4.8037500CurrentTrain: epoch  5, batch    26 | loss: 4.3497276CurrentTrain: epoch  5, batch    27 | loss: 4.4008198CurrentTrain: epoch  5, batch    28 | loss: 4.3505363CurrentTrain: epoch  5, batch    29 | loss: 4.2768221CurrentTrain: epoch  5, batch    30 | loss: 4.3711076CurrentTrain: epoch  5, batch    31 | loss: 4.4733872CurrentTrain: epoch  5, batch    32 | loss: 4.3010397CurrentTrain: epoch  5, batch    33 | loss: 4.3753157CurrentTrain: epoch  5, batch    34 | loss: 4.3835430CurrentTrain: epoch  5, batch    35 | loss: 4.2526555CurrentTrain: epoch  5, batch    36 | loss: 4.4214544CurrentTrain: epoch  5, batch    37 | loss: 4.4063745CurrentTrain: epoch  5, batch    38 | loss: 4.5398946CurrentTrain: epoch  5, batch    39 | loss: 4.3138270CurrentTrain: epoch  5, batch    40 | loss: 4.3714852CurrentTrain: epoch  5, batch    41 | loss: 4.1463432CurrentTrain: epoch  5, batch    42 | loss: 4.2412648CurrentTrain: epoch  5, batch    43 | loss: 4.4520388CurrentTrain: epoch  5, batch    44 | loss: 4.2843018CurrentTrain: epoch  5, batch    45 | loss: 4.2533154CurrentTrain: epoch  5, batch    46 | loss: 4.3083334CurrentTrain: epoch  5, batch    47 | loss: 4.2503753CurrentTrain: epoch  5, batch    48 | loss: 4.2624187CurrentTrain: epoch  5, batch    49 | loss: 4.4826469CurrentTrain: epoch  5, batch    50 | loss: 4.3136840CurrentTrain: epoch  5, batch    51 | loss: 4.3308268CurrentTrain: epoch  5, batch    52 | loss: 4.2659936CurrentTrain: epoch  5, batch    53 | loss: 4.2947783CurrentTrain: epoch  5, batch    54 | loss: 4.3901610CurrentTrain: epoch  5, batch    55 | loss: 4.7127938CurrentTrain: epoch  5, batch    56 | loss: 4.1848392CurrentTrain: epoch  5, batch    57 | loss: 4.4320374CurrentTrain: epoch  5, batch    58 | loss: 4.3544164CurrentTrain: epoch  5, batch    59 | loss: 4.1969452CurrentTrain: epoch  5, batch    60 | loss: 4.1898603CurrentTrain: epoch  5, batch    61 | loss: 4.1981907CurrentTrain: epoch  5, batch    62 | loss: 4.2307391CurrentTrain: epoch  6, batch     0 | loss: 4.1914206CurrentTrain: epoch  6, batch     1 | loss: 4.3057971CurrentTrain: epoch  6, batch     2 | loss: 4.1799560CurrentTrain: epoch  6, batch     3 | loss: 4.2155151CurrentTrain: epoch  6, batch     4 | loss: 4.2219625CurrentTrain: epoch  6, batch     5 | loss: 4.2317324CurrentTrain: epoch  6, batch     6 | loss: 4.2648058CurrentTrain: epoch  6, batch     7 | loss: 4.3341074CurrentTrain: epoch  6, batch     8 | loss: 4.2707000CurrentTrain: epoch  6, batch     9 | loss: 4.1984901CurrentTrain: epoch  6, batch    10 | loss: 4.2469177CurrentTrain: epoch  6, batch    11 | loss: 4.1588211CurrentTrain: epoch  6, batch    12 | loss: 4.4845953CurrentTrain: epoch  6, batch    13 | loss: 4.2442150CurrentTrain: epoch  6, batch    14 | loss: 4.2235813CurrentTrain: epoch  6, batch    15 | loss: 4.1589479CurrentTrain: epoch  6, batch    16 | loss: 4.2184620CurrentTrain: epoch  6, batch    17 | loss: 4.1938992CurrentTrain: epoch  6, batch    18 | loss: 4.2310066CurrentTrain: epoch  6, batch    19 | loss: 4.2301769CurrentTrain: epoch  6, batch    20 | loss: 4.2371030CurrentTrain: epoch  6, batch    21 | loss: 4.2007809CurrentTrain: epoch  6, batch    22 | loss: 4.1458836CurrentTrain: epoch  6, batch    23 | loss: 4.2505994CurrentTrain: epoch  6, batch    24 | loss: 4.2731934CurrentTrain: epoch  6, batch    25 | loss: 4.2264085CurrentTrain: epoch  6, batch    26 | loss: 4.1603184CurrentTrain: epoch  6, batch    27 | loss: 4.2360430CurrentTrain: epoch  6, batch    28 | loss: 4.1140213CurrentTrain: epoch  6, batch    29 | loss: 4.1695395CurrentTrain: epoch  6, batch    30 | loss: 4.2184038CurrentTrain: epoch  6, batch    31 | loss: 4.3328977CurrentTrain: epoch  6, batch    32 | loss: 4.2253251CurrentTrain: epoch  6, batch    33 | loss: 4.1802149CurrentTrain: epoch  6, batch    34 | loss: 4.2436342CurrentTrain: epoch  6, batch    35 | loss: 4.3803949CurrentTrain: epoch  6, batch    36 | loss: 4.1953902CurrentTrain: epoch  6, batch    37 | loss: 4.4516525CurrentTrain: epoch  6, batch    38 | loss: 4.1480455CurrentTrain: epoch  6, batch    39 | loss: 4.1722946CurrentTrain: epoch  6, batch    40 | loss: 4.2022667CurrentTrain: epoch  6, batch    41 | loss: 4.2073927CurrentTrain: epoch  6, batch    42 | loss: 4.1352572CurrentTrain: epoch  6, batch    43 | loss: 4.0839291CurrentTrain: epoch  6, batch    44 | loss: 4.1805263CurrentTrain: epoch  6, batch    45 | loss: 4.3903847CurrentTrain: epoch  6, batch    46 | loss: 4.1795397CurrentTrain: epoch  6, batch    47 | loss: 4.2741060CurrentTrain: epoch  6, batch    48 | loss: 4.2022743CurrentTrain: epoch  6, batch    49 | loss: 4.1912913CurrentTrain: epoch  6, batch    50 | loss: 4.2899108CurrentTrain: epoch  6, batch    51 | loss: 4.2163992CurrentTrain: epoch  6, batch    52 | loss: 4.2882919CurrentTrain: epoch  6, batch    53 | loss: 4.2831802CurrentTrain: epoch  6, batch    54 | loss: 4.1621370CurrentTrain: epoch  6, batch    55 | loss: 4.6010985CurrentTrain: epoch  6, batch    56 | loss: 4.1743407CurrentTrain: epoch  6, batch    57 | loss: 4.1318822CurrentTrain: epoch  6, batch    58 | loss: 4.1789141CurrentTrain: epoch  6, batch    59 | loss: 4.4475470CurrentTrain: epoch  6, batch    60 | loss: 4.2571864CurrentTrain: epoch  6, batch    61 | loss: 4.2846532CurrentTrain: epoch  6, batch    62 | loss: 4.2179003CurrentTrain: epoch  7, batch     0 | loss: 4.1514969CurrentTrain: epoch  7, batch     1 | loss: 4.1540804CurrentTrain: epoch  7, batch     2 | loss: 4.1601639CurrentTrain: epoch  7, batch     3 | loss: 4.1392226CurrentTrain: epoch  7, batch     4 | loss: 4.0936861CurrentTrain: epoch  7, batch     5 | loss: 4.1582479CurrentTrain: epoch  7, batch     6 | loss: 4.1140113CurrentTrain: epoch  7, batch     7 | loss: 4.2120485CurrentTrain: epoch  7, batch     8 | loss: 4.1222601CurrentTrain: epoch  7, batch     9 | loss: 4.2098632CurrentTrain: epoch  7, batch    10 | loss: 4.2198400CurrentTrain: epoch  7, batch    11 | loss: 4.1605363CurrentTrain: epoch  7, batch    12 | loss: 4.0969973CurrentTrain: epoch  7, batch    13 | loss: 4.2063313CurrentTrain: epoch  7, batch    14 | loss: 4.0813680CurrentTrain: epoch  7, batch    15 | loss: 4.0810246CurrentTrain: epoch  7, batch    16 | loss: 4.1578064CurrentTrain: epoch  7, batch    17 | loss: 4.1101446CurrentTrain: epoch  7, batch    18 | loss: 4.1557107CurrentTrain: epoch  7, batch    19 | loss: 4.1871262CurrentTrain: epoch  7, batch    20 | loss: 4.2157226CurrentTrain: epoch  7, batch    21 | loss: 4.1392632CurrentTrain: epoch  7, batch    22 | loss: 4.2212982CurrentTrain: epoch  7, batch    23 | loss: 4.1276608CurrentTrain: epoch  7, batch    24 | loss: 4.1273403CurrentTrain: epoch  7, batch    25 | loss: 4.3729763CurrentTrain: epoch  7, batch    26 | loss: 4.1796503CurrentTrain: epoch  7, batch    27 | loss: 4.1850438CurrentTrain: epoch  7, batch    28 | loss: 4.1250000CurrentTrain: epoch  7, batch    29 | loss: 4.0790548CurrentTrain: epoch  7, batch    30 | loss: 4.1641269CurrentTrain: epoch  7, batch    31 | loss: 4.1030445CurrentTrain: epoch  7, batch    32 | loss: 4.1055861CurrentTrain: epoch  7, batch    33 | loss: 4.0549874CurrentTrain: epoch  7, batch    34 | loss: 4.0855718CurrentTrain: epoch  7, batch    35 | loss: 4.0927391CurrentTrain: epoch  7, batch    36 | loss: 4.1348076CurrentTrain: epoch  7, batch    37 | loss: 4.1907549CurrentTrain: epoch  7, batch    38 | loss: 4.1061573CurrentTrain: epoch  7, batch    39 | loss: 4.1480818CurrentTrain: epoch  7, batch    40 | loss: 4.0903311CurrentTrain: epoch  7, batch    41 | loss: 4.1320601CurrentTrain: epoch  7, batch    42 | loss: 4.1343555CurrentTrain: epoch  7, batch    43 | loss: 4.1219110CurrentTrain: epoch  7, batch    44 | loss: 4.2409868CurrentTrain: epoch  7, batch    45 | loss: 4.0836740CurrentTrain: epoch  7, batch    46 | loss: 4.1464453CurrentTrain: epoch  7, batch    47 | loss: 4.0354533CurrentTrain: epoch  7, batch    48 | loss: 4.1439290CurrentTrain: epoch  7, batch    49 | loss: 4.1961417CurrentTrain: epoch  7, batch    50 | loss: 4.1573749CurrentTrain: epoch  7, batch    51 | loss: 4.0802007CurrentTrain: epoch  7, batch    52 | loss: 4.1268435CurrentTrain: epoch  7, batch    53 | loss: 4.0884361CurrentTrain: epoch  7, batch    54 | loss: 4.0999079CurrentTrain: epoch  7, batch    55 | loss: 4.0260401CurrentTrain: epoch  7, batch    56 | loss: 4.1310821CurrentTrain: epoch  7, batch    57 | loss: 4.1010919CurrentTrain: epoch  7, batch    58 | loss: 4.0824995CurrentTrain: epoch  7, batch    59 | loss: 4.3359723CurrentTrain: epoch  7, batch    60 | loss: 4.0619011CurrentTrain: epoch  7, batch    61 | loss: 4.0937614CurrentTrain: epoch  7, batch    62 | loss: 4.0790815CurrentTrain: epoch  8, batch     0 | loss: 4.1199894CurrentTrain: epoch  8, batch     1 | loss: 4.1492395CurrentTrain: epoch  8, batch     2 | loss: 4.0854392CurrentTrain: epoch  8, batch     3 | loss: 4.0715961CurrentTrain: epoch  8, batch     4 | loss: 4.0619159CurrentTrain: epoch  8, batch     5 | loss: 4.1288142CurrentTrain: epoch  8, batch     6 | loss: 4.1012168CurrentTrain: epoch  8, batch     7 | loss: 4.1107264CurrentTrain: epoch  8, batch     8 | loss: 4.1442957CurrentTrain: epoch  8, batch     9 | loss: 4.0827255CurrentTrain: epoch  8, batch    10 | loss: 4.0250502CurrentTrain: epoch  8, batch    11 | loss: 4.0772552CurrentTrain: epoch  8, batch    12 | loss: 4.0689178CurrentTrain: epoch  8, batch    13 | loss: 4.2400107CurrentTrain: epoch  8, batch    14 | loss: 4.0646091CurrentTrain: epoch  8, batch    15 | loss: 4.0977449CurrentTrain: epoch  8, batch    16 | loss: 4.1247282CurrentTrain: epoch  8, batch    17 | loss: 4.0549598CurrentTrain: epoch  8, batch    18 | loss: 4.0731831CurrentTrain: epoch  8, batch    19 | loss: 4.1279435CurrentTrain: epoch  8, batch    20 | loss: 4.1670637CurrentTrain: epoch  8, batch    21 | loss: 4.1187930CurrentTrain: epoch  8, batch    22 | loss: 4.0417776CurrentTrain: epoch  8, batch    23 | loss: 4.1055779CurrentTrain: epoch  8, batch    24 | loss: 4.1452441CurrentTrain: epoch  8, batch    25 | loss: 4.1300831CurrentTrain: epoch  8, batch    26 | loss: 4.0945315CurrentTrain: epoch  8, batch    27 | loss: 4.0125504CurrentTrain: epoch  8, batch    28 | loss: 4.0793171CurrentTrain: epoch  8, batch    29 | loss: 4.1085496CurrentTrain: epoch  8, batch    30 | loss: 4.0690570CurrentTrain: epoch  8, batch    31 | loss: 4.0894594CurrentTrain: epoch  8, batch    32 | loss: 4.0843368CurrentTrain: epoch  8, batch    33 | loss: 4.1400967CurrentTrain: epoch  8, batch    34 | loss: 4.0751777CurrentTrain: epoch  8, batch    35 | loss: 4.0628080CurrentTrain: epoch  8, batch    36 | loss: 4.0845046CurrentTrain: epoch  8, batch    37 | loss: 4.0962157CurrentTrain: epoch  8, batch    38 | loss: 4.1040211CurrentTrain: epoch  8, batch    39 | loss: 4.0364962CurrentTrain: epoch  8, batch    40 | loss: 4.0480280CurrentTrain: epoch  8, batch    41 | loss: 4.0706635CurrentTrain: epoch  8, batch    42 | loss: 4.5657129CurrentTrain: epoch  8, batch    43 | loss: 4.0777669CurrentTrain: epoch  8, batch    44 | loss: 4.1463380CurrentTrain: epoch  8, batch    45 | loss: 4.0448651CurrentTrain: epoch  8, batch    46 | loss: 4.2367435CurrentTrain: epoch  8, batch    47 | loss: 4.1416163CurrentTrain: epoch  8, batch    48 | loss: 4.0884042CurrentTrain: epoch  8, batch    49 | loss: 4.0610809CurrentTrain: epoch  8, batch    50 | loss: 4.2731090CurrentTrain: epoch  8, batch    51 | loss: 4.1005125CurrentTrain: epoch  8, batch    52 | loss: 4.0462008CurrentTrain: epoch  8, batch    53 | loss: 4.0926628CurrentTrain: epoch  8, batch    54 | loss: 4.0799208CurrentTrain: epoch  8, batch    55 | loss: 4.0978422CurrentTrain: epoch  8, batch    56 | loss: 4.0686216CurrentTrain: epoch  8, batch    57 | loss: 4.1448832CurrentTrain: epoch  8, batch    58 | loss: 4.0729895CurrentTrain: epoch  8, batch    59 | loss: 4.0747271CurrentTrain: epoch  8, batch    60 | loss: 4.0643129CurrentTrain: epoch  8, batch    61 | loss: 4.0702424CurrentTrain: epoch  8, batch    62 | loss: 4.1510696CurrentTrain: epoch  9, batch     0 | loss: 4.0843301CurrentTrain: epoch  9, batch     1 | loss: 4.0797300CurrentTrain: epoch  9, batch     2 | loss: 4.0580206CurrentTrain: epoch  9, batch     3 | loss: 4.0914574CurrentTrain: epoch  9, batch     4 | loss: 4.0529060CurrentTrain: epoch  9, batch     5 | loss: 4.0478926CurrentTrain: epoch  9, batch     6 | loss: 4.0707998CurrentTrain: epoch  9, batch     7 | loss: 4.0584707CurrentTrain: epoch  9, batch     8 | loss: 4.0419722CurrentTrain: epoch  9, batch     9 | loss: 4.0196829CurrentTrain: epoch  9, batch    10 | loss: 4.0508857CurrentTrain: epoch  9, batch    11 | loss: 4.0953798CurrentTrain: epoch  9, batch    12 | loss: 4.1362114CurrentTrain: epoch  9, batch    13 | loss: 4.0554070CurrentTrain: epoch  9, batch    14 | loss: 4.1134901CurrentTrain: epoch  9, batch    15 | loss: 4.0384846CurrentTrain: epoch  9, batch    16 | loss: 4.0596218CurrentTrain: epoch  9, batch    17 | loss: 4.0555615CurrentTrain: epoch  9, batch    18 | loss: 4.0376558CurrentTrain: epoch  9, batch    19 | loss: 4.0737891CurrentTrain: epoch  9, batch    20 | loss: 4.0979300CurrentTrain: epoch  9, batch    21 | loss: 4.1346507CurrentTrain: epoch  9, batch    22 | loss: 4.0677752CurrentTrain: epoch  9, batch    23 | loss: 4.0301843CurrentTrain: epoch  9, batch    24 | loss: 4.1046524CurrentTrain: epoch  9, batch    25 | loss: 4.4135790CurrentTrain: epoch  9, batch    26 | loss: 4.1385803CurrentTrain: epoch  9, batch    27 | loss: 4.1078224CurrentTrain: epoch  9, batch    28 | loss: 4.0369225CurrentTrain: epoch  9, batch    29 | loss: 4.0709543CurrentTrain: epoch  9, batch    30 | loss: 4.0885625CurrentTrain: epoch  9, batch    31 | loss: 4.0433898CurrentTrain: epoch  9, batch    32 | loss: 4.0792408CurrentTrain: epoch  9, batch    33 | loss: 4.0323343CurrentTrain: epoch  9, batch    34 | loss: 4.0577335CurrentTrain: epoch  9, batch    35 | loss: 4.1087198CurrentTrain: epoch  9, batch    36 | loss: 4.1248422CurrentTrain: epoch  9, batch    37 | loss: 4.0437908CurrentTrain: epoch  9, batch    38 | loss: 4.0204244CurrentTrain: epoch  9, batch    39 | loss: 4.0961757CurrentTrain: epoch  9, batch    40 | loss: 4.0285692CurrentTrain: epoch  9, batch    41 | loss: 4.0464535CurrentTrain: epoch  9, batch    42 | loss: 4.0705109CurrentTrain: epoch  9, batch    43 | loss: 4.0770512CurrentTrain: epoch  9, batch    44 | loss: 4.0814910CurrentTrain: epoch  9, batch    45 | loss: 3.9976451CurrentTrain: epoch  9, batch    46 | loss: 4.0644722CurrentTrain: epoch  9, batch    47 | loss: 4.0206366CurrentTrain: epoch  9, batch    48 | loss: 4.0765667CurrentTrain: epoch  9, batch    49 | loss: 4.0789518CurrentTrain: epoch  9, batch    50 | loss: 4.0845718CurrentTrain: epoch  9, batch    51 | loss: 4.0221529CurrentTrain: epoch  9, batch    52 | loss: 4.0443726CurrentTrain: epoch  9, batch    53 | loss: 4.0539742CurrentTrain: epoch  9, batch    54 | loss: 4.0635080CurrentTrain: epoch  9, batch    55 | loss: 4.0523839CurrentTrain: epoch  9, batch    56 | loss: 4.0412493CurrentTrain: epoch  9, batch    57 | loss: 4.0430503CurrentTrain: epoch  9, batch    58 | loss: 4.0459795CurrentTrain: epoch  9, batch    59 | loss: 4.0711126CurrentTrain: epoch  9, batch    60 | loss: 4.0164871CurrentTrain: epoch  9, batch    61 | loss: 4.0711050CurrentTrain: epoch  9, batch    62 | loss: 4.0118408
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: born in copenhagen , as a young man bang became interested in surrealism as practised by giorgio de chirico , max ernst and yves tanguy .
Head Entity: yves tanguy
Tail Entity: surrealism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the artist was heavily influenced by the cubist movement, which was pioneered by pablo picasso and georges braque.  
Head Entity: pablo picasso  
Tail Entity: cubist movement  

Relation: movement  
Context: the philosopher's ideas were central to the existentialist movement, which gained prominence in the mid-20th century through the works of thinkers like jean-paul sartre and simone de beauvoir.  
Head Entity: jean-paul sartre  
Tail Entity: existentialist movement  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: faldu was born on 1 august 1957 to a family of farmer in kalavad , jamnagar district , bombay state ( present - day gujarat ) .
Head Entity: kalavad
Tail Entity: jamnagar district
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of Springfield is known for its beautiful parks and vibrant community, situated in the heart of Sangamon County, Illinois.  
Head Entity: Springfield  
Tail Entity: Sangamon County  

Relation: located in the administrative territorial entity  
Context: The historic town of Williamsburg is a popular tourist destination, located within the boundaries of James City County, Virginia.  
Head Entity: Williamsburg  
Tail Entity: James City County  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: it was while playing guitar with the negative ponies that stoermer met brandon flowers and dave keuning , who were performing as an early incarnation of the killers .
Head Entity: dave keuning
Tail Entity: the killers
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of dedication and hard work, Sarah was finally accepted into the prestigious New York Ballet, where she would perform alongside some of the most talented dancers in the world.  
Head Entity: Sarah  
Tail Entity: New York Ballet  

Relation: member of  
Context: The renowned jazz musician was known for his incredible improvisation skills and was a long-time member of the legendary Miles Davis Quintet, influencing countless artists in the genre.  
Head Entity: the renowned jazz musician  
Tail Entity: Miles Davis Quintet  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County and allowing vehicles to cross over the waterway.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a passage for trains to cross under the sea between England and France.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the cucuveanu river is a tributary of the argova river in romania .
Head Entity: argova river
Tail Entity: cucuveanu river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: the missouri river is a tributary of the mississippi river, flowing through several states in the united states.  
Head Entity: mississippi river  
Tail Entity: missouri river  

Relation: tributary  
Context: the thames river serves as a tributary to the river medway in southeastern england.  
Head Entity: river medway  
Tail Entity: thames river  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: his father robert docking and grandfather george docking both served as governor of kansas .
Head Entity: george docking
Tail Entity: governor of kansas
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After a successful career in politics, Sarah Thompson was appointed as the new mayor of Springfield.  
Head Entity: Sarah Thompson  
Tail Entity: mayor of Springfield  

Relation: position held  
Context: John Smith, a renowned scientist, was the director of the National Institute of Health for over a decade.  
Head Entity: John Smith  
Tail Entity: director of the National Institute of Health  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: " jedi outcast " was developed by raven software and powered by the i d tech 3 game engine .
Head Entity: jedi outcast
Tail Entity: raven software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and has received numerous awards for its storytelling and gameplay.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Minecraft" was developed by Mojang Studios and has become one of the best-selling video games of all time.  
Head Entity: Minecraft  
Tail Entity: Mojang Studios  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: bed of gold is the debut ep of san francisco - based indie rock band lovelikefire .
Head Entity: lovelikefire
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in the vibrant city of austin, texas, known for its innovation and creativity.  
Head Entity: tech startup  
Tail Entity: austin, texas  

Relation: location of formation  
Context: the famous rock band was established in the bustling streets of new york city, where they found their unique sound.  
Head Entity: rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: during the same month , the band supported kings of leon 's tour of the united states , and coldplay 's tour of the united kingdom .
Head Entity: kings of leon
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous Italian dish, pizza, has gained popularity worldwide, but its roots can be traced back to Naples, Italy.  
Head Entity: pizza  
Tail Entity: Italy  

Relation: country of origin  
Context: The iconic brand, Rolex, is renowned for its luxury watches, which are crafted in Switzerland, known for its precision and quality in watchmaking.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.76%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.76%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.80%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.77%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.04%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.42%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.76%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.92%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.22%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.76%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.76%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.80%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.77%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.04%   
cur_acc:  ['0.9504']
his_acc:  ['0.9504']
CurrentTrain: epoch  0, batch     0 | loss: 7.5848622CurrentTrain: epoch  0, batch     1 | loss: 6.8752251CurrentTrain: epoch  0, batch     2 | loss: 5.6071434CurrentTrain: epoch  0, batch     3 | loss: 8.0463858CurrentTrain: epoch  1, batch     0 | loss: 5.8297057CurrentTrain: epoch  1, batch     1 | loss: 5.8731456CurrentTrain: epoch  1, batch     2 | loss: 7.1373234CurrentTrain: epoch  1, batch     3 | loss: 6.7713890CurrentTrain: epoch  2, batch     0 | loss: 5.1597605CurrentTrain: epoch  2, batch     1 | loss: 5.2799826CurrentTrain: epoch  2, batch     2 | loss: 5.8121033CurrentTrain: epoch  2, batch     3 | loss: 5.4242663CurrentTrain: epoch  3, batch     0 | loss: 5.5457120CurrentTrain: epoch  3, batch     1 | loss: 4.3501949CurrentTrain: epoch  3, batch     2 | loss: 4.9358983CurrentTrain: epoch  3, batch     3 | loss: 6.4352493CurrentTrain: epoch  4, batch     0 | loss: 4.2088728CurrentTrain: epoch  4, batch     1 | loss: 4.3813152CurrentTrain: epoch  4, batch     2 | loss: 5.1077700CurrentTrain: epoch  4, batch     3 | loss: 3.0940735CurrentTrain: epoch  5, batch     0 | loss: 3.9461131CurrentTrain: epoch  5, batch     1 | loss: 5.0099311CurrentTrain: epoch  5, batch     2 | loss: 4.3682642CurrentTrain: epoch  5, batch     3 | loss: 4.4747252CurrentTrain: epoch  6, batch     0 | loss: 4.6335421CurrentTrain: epoch  6, batch     1 | loss: 3.8147955CurrentTrain: epoch  6, batch     2 | loss: 4.5709982CurrentTrain: epoch  6, batch     3 | loss: 4.6772690CurrentTrain: epoch  7, batch     0 | loss: 4.6905546CurrentTrain: epoch  7, batch     1 | loss: 3.9313164CurrentTrain: epoch  7, batch     2 | loss: 3.7471507CurrentTrain: epoch  7, batch     3 | loss: 2.2471042CurrentTrain: epoch  8, batch     0 | loss: 4.0023651CurrentTrain: epoch  8, batch     1 | loss: 3.8440337CurrentTrain: epoch  8, batch     2 | loss: 4.0477495CurrentTrain: epoch  8, batch     3 | loss: 3.1599383CurrentTrain: epoch  9, batch     0 | loss: 3.4796598CurrentTrain: epoch  9, batch     1 | loss: 3.7620621CurrentTrain: epoch  9, batch     2 | loss: 4.2957001CurrentTrain: epoch  9, batch     3 | loss: 3.1830344
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Civil Code governs various aspects of civil law within the state of California, including contracts and property rights.  
Head Entity: California Civil Code  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The band Coldplay is known for their hit song "Viva La Vida," which showcases their unique sound and style.  
Head Entity: Viva La Vida  
Tail Entity: Coldplay  

Relation: performer  
Context: Beyoncé delivered a stunning performance at the Super Bowl halftime show, captivating millions of viewers worldwide.  
Head Entity: Super Bowl halftime show  
Tail Entity: Beyoncé  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: norway was represented in the eurovision song contest 2005 by the song " in my dreams " performed by wig wam .
Head Entity: eurovision song contest 2005
Tail Entity: wig wam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: the united states was represented in the 2020 summer olympics by a team of over 600 athletes competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: united states  

Relation: participant  
Context: the annual tech conference featured several startups, including a presentation by innovative solutions, which showcased their latest product.  
Head Entity: annual tech conference  
Tail Entity: innovative solutions  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete was classified under the Olympic weightlifting category, allowing him to compete at the international level.  
Head Entity: Olympic weightlifting  
Tail Entity: international level  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested race, the independent candidate managed to secure a significant number of votes, ultimately leading to their victory over the long-standing incumbent mayor.  
Head Entity: closely contested race  
Tail Entity: independent candidate  

Relation: successful candidate  
Context: In the recent parliamentary elections, the opposition party's leader emerged victorious, marking a significant shift in the political landscape of the region.  
Head Entity: parliamentary elections  
Tail Entity: opposition party's leader  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce loyalty to each other, having grown up under the same roof and shared the same parents.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it was evident that both Clara and her brother, James, inherited their parents' artistic talents, showcasing their skills in painting and music.  
Head Entity: Clara  
Tail Entity: James  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: the united states navy is known for its naval operations and maritime security.  
Head Entity: united states navy  
Tail Entity: naval operations  

Relation: military branch  
Context: general patton served in the united states army during world war ii.  
Head Entity: general patton  
Tail Entity: united states army  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: mukesh is married to nita ambani and has two sons , anant and akash , and a daughter , isha .
Head Entity: nita ambani
Tail Entity: akash
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: elon musk has six children, including a son named x ae a-xii and a daughter named exa dark sidereal.  
Head Entity: elon musk  
Tail Entity: x ae a-xii  

Relation: child  
Context: j.k. rowling is the mother of three children, including a daughter named jessica and a son named david.  
Head Entity: j.k. rowling  
Tail Entity: jessica  
Mixup data size:  199
MixupTrain:  epoch  0, batch     0 | loss: 6.4560835MixupTrain:  epoch  0, batch     1 | loss: 5.8745409MixupTrain:  epoch  0, batch     2 | loss: 5.5543694MixupTrain:  epoch  0, batch     3 | loss: 5.5302027MixupTrain:  epoch  0, batch     4 | loss: 5.3000025MixupTrain:  epoch  0, batch     5 | loss: 5.6969690MixupTrain:  epoch  0, batch     6 | loss: 5.2865824MixupTrain:  epoch  0, batch     7 | loss: 4.9198154MixupTrain:  epoch  0, batch     8 | loss: 5.2763467MixupTrain:  epoch  0, batch     9 | loss: 4.9752052MixupTrain:  epoch  0, batch    10 | loss: 4.8599114MixupTrain:  epoch  0, batch    11 | loss: 4.4963994MixupTrain:  epoch  0, batch    12 | loss: 4.1666261
MemoryTrain:  epoch  0, batch     0 | loss: 4.2152233MemoryTrain:  epoch  0, batch     1 | loss: 4.3362765MemoryTrain:  epoch  0, batch     2 | loss: 5.6141405MemoryTrain:  epoch  0, batch     3 | loss: 4.9897628MemoryTrain:  epoch  1, batch     0 | loss: 3.8959358MemoryTrain:  epoch  1, batch     1 | loss: 3.7107744MemoryTrain:  epoch  1, batch     2 | loss: 4.6587982MemoryTrain:  epoch  1, batch     3 | loss: 3.6348600MemoryTrain:  epoch  2, batch     0 | loss: 2.7707849MemoryTrain:  epoch  2, batch     1 | loss: 2.8086500MemoryTrain:  epoch  2, batch     2 | loss: 4.1548004MemoryTrain:  epoch  2, batch     3 | loss: 3.6509566MemoryTrain:  epoch  3, batch     0 | loss: 3.0692277MemoryTrain:  epoch  3, batch     1 | loss: 3.1945934MemoryTrain:  epoch  3, batch     2 | loss: 2.5943122MemoryTrain:  epoch  3, batch     3 | loss: 2.7018745MemoryTrain:  epoch  4, batch     0 | loss: 2.8321631MemoryTrain:  epoch  4, batch     1 | loss: 2.2659359MemoryTrain:  epoch  4, batch     2 | loss: 2.5890121MemoryTrain:  epoch  4, batch     3 | loss: 2.2085726MemoryTrain:  epoch  5, batch     0 | loss: 3.0087690MemoryTrain:  epoch  5, batch     1 | loss: 2.1009955MemoryTrain:  epoch  5, batch     2 | loss: 2.5766997MemoryTrain:  epoch  5, batch     3 | loss: 2.3470285MemoryTrain:  epoch  6, batch     0 | loss: 2.7195587MemoryTrain:  epoch  6, batch     1 | loss: 2.2309322MemoryTrain:  epoch  6, batch     2 | loss: 2.2308974MemoryTrain:  epoch  6, batch     3 | loss: 2.1675072MemoryTrain:  epoch  7, batch     0 | loss: 2.2219236MemoryTrain:  epoch  7, batch     1 | loss: 2.0218501MemoryTrain:  epoch  7, batch     2 | loss: 1.9541469MemoryTrain:  epoch  7, batch     3 | loss: 2.3177826MemoryTrain:  epoch  8, batch     0 | loss: 2.2412412MemoryTrain:  epoch  8, batch     1 | loss: 1.9971756MemoryTrain:  epoch  8, batch     2 | loss: 1.8007390MemoryTrain:  epoch  8, batch     3 | loss: 2.1202009MemoryTrain:  epoch  9, batch     0 | loss: 2.0741391MemoryTrain:  epoch  9, batch     1 | loss: 1.7814939MemoryTrain:  epoch  9, batch     2 | loss: 1.8188528MemoryTrain:  epoch  9, batch     3 | loss: 1.9363728
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 79.76%   [EVAL] batch:   21 | acc: 50.00%,  total acc: 78.41%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.85%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.35%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 82.86%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.78%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 84.29%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 83.54%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 83.63%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.87%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 83.81%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 83.15%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 82.71%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 82.68%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 82.14%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 82.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 82.48%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 82.78%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 82.75%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 82.95%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 82.92%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 82.02%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 81.36%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 80.83%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 80.00%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 79.00%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 77.92%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 76.98%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 92.56%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 92.61%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 92.12%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 92.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 92.59%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.56%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 94.09%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 94.08%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.51%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.62%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 94.46%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 94.17%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 93.89%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 93.22%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 92.71%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 92.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 92.55%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.57%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.50%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.52%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.54%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 92.46%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 92.48%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 92.40%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 92.52%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 92.54%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 92.26%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 91.89%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 91.54%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 91.19%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 90.95%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 90.81%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 90.67%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 90.58%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 90.54%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 90.50%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 90.29%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 90.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 90.22%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 90.27%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 90.31%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 90.43%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 90.24%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 89.68%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 89.06%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 88.60%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 88.08%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 88.00%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 88.00%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 88.13%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 88.26%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 88.52%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 88.70%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 88.93%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 89.05%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 89.16%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 89.27%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 89.42%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 89.34%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 89.08%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 89.00%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 89.05%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 88.97%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 88.90%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 88.60%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 88.53%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 88.35%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 88.23%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 88.00%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 88.05%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 88.05%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 88.09%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 88.09%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 88.08%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 87.92%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 87.34%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 87.09%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 86.73%   [EVAL] batch:  122 | acc: 18.75%,  total acc: 86.18%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 85.53%   [EVAL] batch:  124 | acc: 31.25%,  total acc: 85.10%   
cur_acc:  ['0.9504', '0.7698']
his_acc:  ['0.9504', '0.8510']
CurrentTrain: epoch  0, batch     0 | loss: 6.2175384CurrentTrain: epoch  0, batch     1 | loss: 6.2899084CurrentTrain: epoch  0, batch     2 | loss: 6.3128023CurrentTrain: epoch  0, batch     3 | loss: 5.0225725CurrentTrain: epoch  1, batch     0 | loss: 5.2741442CurrentTrain: epoch  1, batch     1 | loss: 5.1333456CurrentTrain: epoch  1, batch     2 | loss: 5.4523745CurrentTrain: epoch  1, batch     3 | loss: 6.2156210CurrentTrain: epoch  2, batch     0 | loss: 4.7996392CurrentTrain: epoch  2, batch     1 | loss: 5.1226196CurrentTrain: epoch  2, batch     2 | loss: 3.7848501CurrentTrain: epoch  2, batch     3 | loss: 2.6492529CurrentTrain: epoch  3, batch     0 | loss: 4.3672624CurrentTrain: epoch  3, batch     1 | loss: 3.8865957CurrentTrain: epoch  3, batch     2 | loss: 4.2075157CurrentTrain: epoch  3, batch     3 | loss: 4.7445970CurrentTrain: epoch  4, batch     0 | loss: 4.1477556CurrentTrain: epoch  4, batch     1 | loss: 3.2440462CurrentTrain: epoch  4, batch     2 | loss: 4.2334847CurrentTrain: epoch  4, batch     3 | loss: 7.0196624CurrentTrain: epoch  5, batch     0 | loss: 3.4009438CurrentTrain: epoch  5, batch     1 | loss: 3.3535409CurrentTrain: epoch  5, batch     2 | loss: 3.2941799CurrentTrain: epoch  5, batch     3 | loss: 2.8374000CurrentTrain: epoch  6, batch     0 | loss: 3.5515904CurrentTrain: epoch  6, batch     1 | loss: 2.9072828CurrentTrain: epoch  6, batch     2 | loss: 2.7837093CurrentTrain: epoch  6, batch     3 | loss: 5.8392081CurrentTrain: epoch  7, batch     0 | loss: 2.9928541CurrentTrain: epoch  7, batch     1 | loss: 2.7829826CurrentTrain: epoch  7, batch     2 | loss: 2.7564216CurrentTrain: epoch  7, batch     3 | loss: 2.5037968CurrentTrain: epoch  8, batch     0 | loss: 2.5761561CurrentTrain: epoch  8, batch     1 | loss: 2.7079539CurrentTrain: epoch  8, batch     2 | loss: 2.7630413CurrentTrain: epoch  8, batch     3 | loss: 1.7829758CurrentTrain: epoch  9, batch     0 | loss: 2.5196490CurrentTrain: epoch  9, batch     1 | loss: 2.5226483CurrentTrain: epoch  9, batch     2 | loss: 2.4875989CurrentTrain: epoch  9, batch     3 | loss: 2.1154795
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, has been married to michelle obama since 1992, and they have two daughters together.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: " i 'm a cuckoo " was belle & sebastian 's second single from " dear catastrophe waitress " , released on rough trade records in 2004 .
Head Entity: belle & sebastian
Tail Entity: rough trade records
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Random Access Memories" by Daft Punk was released under the Columbia Records label in 2013.  
Head Entity: Daft Punk  
Tail Entity: Columbia Records  

Relation: record label  
Context: Taylor Swift's latest album "Evermore" was produced by her own label, Republic Records, and released in 2020.  
Head Entity: Taylor Swift  
Tail Entity: Republic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: cohn loaned hayworth to metro - goldwyn - mayer to appear in " susan and god " opposite joan crawford .
Head Entity: susan and god
Tail Entity: metro - goldwyn - mayer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: The film was released by Universal Pictures, which is known for its extensive catalog of classic movies.  
Head Entity: Universal Pictures  
Tail Entity: classic movies  

Relation: distributor  
Context: The music album was distributed by Sony Music, reaching audiences worldwide and topping the charts.  
Head Entity: Sony Music  
Tail Entity: music album  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: the highway was re - aligned in 1963 to serve the port of vancouver , traveling along the columbia river to ridgefield west of vancouver lake onto a roadway that was to be constructed .
Head Entity: port of vancouver
Tail Entity: columbia river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Miami is famous for its beautiful beaches and vibrant nightlife, situated right along the Atlantic Ocean, making it a popular destination for tourists.  
Head Entity: Miami  
Tail Entity: Atlantic Ocean  

Relation: located in or next to body of water  
Context: The quaint village of Port Isaac is nestled on the rugged coastline of Cornwall, directly adjacent to the picturesque harbor that opens into the Celtic Sea.  
Head Entity: Port Isaac  
Tail Entity: Celtic Sea  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Sample 1:  
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Sample 2:  
Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to retain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the speaker proudly mentioned that she had recently obtained her citizenship in canada, where she had lived for over a decade.  
Head Entity: the speaker  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: he is also a part of the atlanta - based hip - hop / r&b;/soul musical collective the dungeon family .
Head Entity: the dungeon family
Tail Entity: hip - hop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the band is known for their unique blend of rock and electronic music, often categorized under the genre of synth-pop.  
Head Entity: the band  
Tail Entity: synth-pop  

Relation: genre  
Context: she has made significant contributions to the world of classical music, particularly in the genre of opera.  
Head Entity: she  
Tail Entity: opera  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: in the west , the rivers rib , ash and stort flow south from the hundred parishes to meet the lea and then the thames .
Head Entity: rib
Tail Entity: lea
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: The river Seine flows through Paris and eventually empties into the English Channel, marking its mouth.  
Head Entity: Seine  
Tail Entity: English Channel  

Relation: mouth of the watercourse  
Context: The Mississippi River travels a long distance before reaching its mouth at the Gulf of Mexico, where it meets the sea.  
Head Entity: Mississippi River  
Tail Entity: Gulf of Mexico  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: tarun dey is an indian football defender who played for india in the 1984 asian cup .
Head Entity: tarun dey
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: lebron james is a professional basketball forward known for his exceptional skills and leadership on the court, currently playing for the los angeles lakers.  
Head Entity: lebron james  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: serena williams is a renowned tennis player who has dominated women's singles and doubles, showcasing her talent as a top-ranked player in the sport.  
Head Entity: serena williams  
Tail Entity: player  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
Mixup data size:  259
MixupTrain:  epoch  0, batch     0 | loss: 3.5445569MixupTrain:  epoch  0, batch     1 | loss: 3.3353721MixupTrain:  epoch  0, batch     2 | loss: 3.9437698MixupTrain:  epoch  0, batch     3 | loss: 3.8809165MixupTrain:  epoch  0, batch     4 | loss: 3.5877866MixupTrain:  epoch  0, batch     5 | loss: 3.6342353MixupTrain:  epoch  0, batch     6 | loss: 3.4517297MixupTrain:  epoch  0, batch     7 | loss: 3.5591317MixupTrain:  epoch  0, batch     8 | loss: 3.9453464MixupTrain:  epoch  0, batch     9 | loss: 3.2412071MixupTrain:  epoch  0, batch    10 | loss: 2.8793276MixupTrain:  epoch  0, batch    11 | loss: 3.0244570MixupTrain:  epoch  0, batch    12 | loss: 3.6657197MixupTrain:  epoch  0, batch    13 | loss: 3.8489188MixupTrain:  epoch  0, batch    14 | loss: 2.6724804MixupTrain:  epoch  0, batch    15 | loss: 3.2407401MixupTrain:  epoch  0, batch    16 | loss: 3.1979790
MemoryTrain:  epoch  0, batch     0 | loss: 3.1660452MemoryTrain:  epoch  0, batch     1 | loss: 3.1072221MemoryTrain:  epoch  0, batch     2 | loss: 4.5686712MemoryTrain:  epoch  0, batch     3 | loss: 4.2398906MemoryTrain:  epoch  0, batch     4 | loss: 3.7064381MemoryTrain:  epoch  0, batch     5 | loss: 3.1262360MemoryTrain:  epoch  1, batch     0 | loss: 3.3493602MemoryTrain:  epoch  1, batch     1 | loss: 3.9780450MemoryTrain:  epoch  1, batch     2 | loss: 3.5886714MemoryTrain:  epoch  1, batch     3 | loss: 3.0248277MemoryTrain:  epoch  1, batch     4 | loss: 2.5880308MemoryTrain:  epoch  1, batch     5 | loss: 2.6564531MemoryTrain:  epoch  2, batch     0 | loss: 2.9159102MemoryTrain:  epoch  2, batch     1 | loss: 2.1627882MemoryTrain:  epoch  2, batch     2 | loss: 3.0848370MemoryTrain:  epoch  2, batch     3 | loss: 2.7357581MemoryTrain:  epoch  2, batch     4 | loss: 3.0506001MemoryTrain:  epoch  2, batch     5 | loss: 2.2469079MemoryTrain:  epoch  3, batch     0 | loss: 2.8690863MemoryTrain:  epoch  3, batch     1 | loss: 2.7513154MemoryTrain:  epoch  3, batch     2 | loss: 2.1014113MemoryTrain:  epoch  3, batch     3 | loss: 2.3125911MemoryTrain:  epoch  3, batch     4 | loss: 2.2827706MemoryTrain:  epoch  3, batch     5 | loss: 2.5284405MemoryTrain:  epoch  4, batch     0 | loss: 2.3075695MemoryTrain:  epoch  4, batch     1 | loss: 2.5968552MemoryTrain:  epoch  4, batch     2 | loss: 2.0802338MemoryTrain:  epoch  4, batch     3 | loss: 2.3401184MemoryTrain:  epoch  4, batch     4 | loss: 2.5754313MemoryTrain:  epoch  4, batch     5 | loss: 2.0841091MemoryTrain:  epoch  5, batch     0 | loss: 2.1464448MemoryTrain:  epoch  5, batch     1 | loss: 2.1335177MemoryTrain:  epoch  5, batch     2 | loss: 2.0847478MemoryTrain:  epoch  5, batch     3 | loss: 2.3679879MemoryTrain:  epoch  5, batch     4 | loss: 1.7751558MemoryTrain:  epoch  5, batch     5 | loss: 1.8924454MemoryTrain:  epoch  6, batch     0 | loss: 1.5827849MemoryTrain:  epoch  6, batch     1 | loss: 2.2526681MemoryTrain:  epoch  6, batch     2 | loss: 1.8732464MemoryTrain:  epoch  6, batch     3 | loss: 1.8182395MemoryTrain:  epoch  6, batch     4 | loss: 1.8473091MemoryTrain:  epoch  6, batch     5 | loss: 1.6634206MemoryTrain:  epoch  7, batch     0 | loss: 1.6855130MemoryTrain:  epoch  7, batch     1 | loss: 1.8356762MemoryTrain:  epoch  7, batch     2 | loss: 1.8472909MemoryTrain:  epoch  7, batch     3 | loss: 1.8136616MemoryTrain:  epoch  7, batch     4 | loss: 1.7710540MemoryTrain:  epoch  7, batch     5 | loss: 1.4767703MemoryTrain:  epoch  8, batch     0 | loss: 1.6411879MemoryTrain:  epoch  8, batch     1 | loss: 1.5210688MemoryTrain:  epoch  8, batch     2 | loss: 1.5271082MemoryTrain:  epoch  8, batch     3 | loss: 1.6584024MemoryTrain:  epoch  8, batch     4 | loss: 1.5652325MemoryTrain:  epoch  8, batch     5 | loss: 1.9080528MemoryTrain:  epoch  9, batch     0 | loss: 1.4146113MemoryTrain:  epoch  9, batch     1 | loss: 1.5703474MemoryTrain:  epoch  9, batch     2 | loss: 1.6011312MemoryTrain:  epoch  9, batch     3 | loss: 1.6370616MemoryTrain:  epoch  9, batch     4 | loss: 1.5781485MemoryTrain:  epoch  9, batch     5 | loss: 1.3326167
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 88.69%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 88.32%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 88.80%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 89.00%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 88.46%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 88.19%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 87.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 87.29%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 84.19%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 83.39%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 83.16%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 82.60%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 82.24%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 81.89%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 81.88%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 81.40%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 80.95%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 80.81%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 80.82%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 80.00%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 79.48%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 78.32%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 77.04%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 76.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 76.96%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 77.83%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 78.24%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 78.64%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 79.63%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 79.98%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 80.31%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 80.64%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 80.46%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 90.22%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 90.36%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 90.74%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.16%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.73%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 91.80%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 91.18%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 91.07%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 90.80%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 90.71%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 90.79%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.72%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 91.62%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 91.39%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 91.30%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 90.82%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 90.49%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 90.69%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 90.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 90.38%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 90.45%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 90.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 90.40%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 90.46%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 90.41%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 90.36%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 90.42%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 90.47%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 90.42%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 90.38%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 90.33%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 90.10%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 89.96%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 89.83%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 89.89%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 89.76%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 89.73%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 89.52%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 89.41%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 89.13%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 89.02%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 89.00%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 88.98%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 88.88%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 88.70%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 88.77%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 88.67%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 88.81%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 88.64%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 88.10%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 87.43%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 86.91%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 86.34%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 86.14%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 86.15%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 86.31%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 86.75%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 86.97%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 87.24%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 87.37%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 87.63%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 87.75%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 87.81%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 87.87%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 87.68%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 87.62%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 87.68%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 87.62%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 87.32%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 87.04%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 86.70%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 86.59%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 86.32%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 85.99%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 85.95%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 85.96%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 86.05%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 86.02%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 85.87%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 85.26%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 84.66%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 83.97%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 83.33%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 82.66%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 82.10%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 82.19%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 82.28%   [EVAL] batch:  127 | acc: 93.75%,  total acc: 82.37%   [EVAL] batch:  128 | acc: 87.50%,  total acc: 82.41%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 82.36%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 82.49%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 82.58%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 82.74%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 82.78%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 82.89%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 82.93%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 82.82%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 82.89%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 82.92%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 82.95%   [EVAL] batch:  143 | acc: 100.00%,  total acc: 83.07%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 83.02%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 83.05%   [EVAL] batch:  146 | acc: 87.50%,  total acc: 83.08%   [EVAL] batch:  147 | acc: 81.25%,  total acc: 83.07%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 83.18%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 83.25%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 83.20%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 83.18%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 83.17%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 83.10%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 82.93%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 82.84%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 82.71%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 82.55%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 82.38%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 82.34%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 82.21%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 82.13%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 82.05%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 82.05%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 81.93%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 81.81%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 81.77%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 81.77%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 81.54%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 81.40%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 81.07%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 81.00%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 80.68%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 80.50%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 80.61%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 80.72%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 80.94%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 81.15%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 81.32%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 81.42%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 81.52%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 81.62%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 81.72%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 81.55%   
cur_acc:  ['0.9504', '0.7698', '0.8046']
his_acc:  ['0.9504', '0.8510', '0.8155']
CurrentTrain: epoch  0, batch     0 | loss: 6.4707956CurrentTrain: epoch  0, batch     1 | loss: 5.0002160CurrentTrain: epoch  0, batch     2 | loss: 7.1603923CurrentTrain: epoch  0, batch     3 | loss: 4.6598029CurrentTrain: epoch  1, batch     0 | loss: 5.9719906CurrentTrain: epoch  1, batch     1 | loss: 4.5054817CurrentTrain: epoch  1, batch     2 | loss: 4.7984095CurrentTrain: epoch  1, batch     3 | loss: 6.0644217CurrentTrain: epoch  2, batch     0 | loss: 4.2500839CurrentTrain: epoch  2, batch     1 | loss: 4.5581002CurrentTrain: epoch  2, batch     2 | loss: 5.1828375CurrentTrain: epoch  2, batch     3 | loss: 2.2763524CurrentTrain: epoch  3, batch     0 | loss: 3.8220561CurrentTrain: epoch  3, batch     1 | loss: 4.5806561CurrentTrain: epoch  3, batch     2 | loss: 4.1763749CurrentTrain: epoch  3, batch     3 | loss: 4.3348236CurrentTrain: epoch  4, batch     0 | loss: 3.7791476CurrentTrain: epoch  4, batch     1 | loss: 3.9490669CurrentTrain: epoch  4, batch     2 | loss: 3.7472880CurrentTrain: epoch  4, batch     3 | loss: 3.7665598CurrentTrain: epoch  5, batch     0 | loss: 3.6444731CurrentTrain: epoch  5, batch     1 | loss: 3.6473112CurrentTrain: epoch  5, batch     2 | loss: 3.2277040CurrentTrain: epoch  5, batch     3 | loss: 4.1132092CurrentTrain: epoch  6, batch     0 | loss: 3.8468001CurrentTrain: epoch  6, batch     1 | loss: 2.9219635CurrentTrain: epoch  6, batch     2 | loss: 3.5829413CurrentTrain: epoch  6, batch     3 | loss: 2.9303319CurrentTrain: epoch  7, batch     0 | loss: 3.5526643CurrentTrain: epoch  7, batch     1 | loss: 2.8119063CurrentTrain: epoch  7, batch     2 | loss: 3.4986835CurrentTrain: epoch  7, batch     3 | loss: 2.5451982CurrentTrain: epoch  8, batch     0 | loss: 3.3134475CurrentTrain: epoch  8, batch     1 | loss: 2.6661978CurrentTrain: epoch  8, batch     2 | loss: 3.2621877CurrentTrain: epoch  8, batch     3 | loss: 2.3254786CurrentTrain: epoch  9, batch     0 | loss: 3.1120820CurrentTrain: epoch  9, batch     1 | loss: 3.2904191CurrentTrain: epoch  9, batch     2 | loss: 2.1346312CurrentTrain: epoch  9, batch     3 | loss: 1.9807484
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: the nikon d5500 features other improvements over its predecessor nikon d5300 .
Head Entity: nikon d5500
Tail Entity: nikon d5300
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: in the series of popular smartphones, the iphone 12 follows the iphone 11, which was released the previous year.  
Head Entity: iphone 12  
Tail Entity: iphone 11  

Relation: follows  
Context: the latest installment in the series, the harry potter and the deathly hallows, follows the previous book, harry potter and the half-blood prince.  
Head Entity: harry potter and the deathly hallows  
Tail Entity: harry potter and the half-blood prince  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: alphonse louis pierre pyrame de candolle ( 28 october 18064 april 1893 ) was a french - swiss botanist , the son of the swiss botanist augustin pyramus de candolle .
Head Entity: alphonse louis pierre pyrame de candolle
Tail Entity: augustin pyramus de candolle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: charles darwin ( 12 february 1809 – 19 april 1882 ) was an english naturalist and geologist, best known for his contributions to the science of evolution. he was the son of robert darwin, a wealthy society doctor.  
Head Entity: charles darwin  
Tail Entity: robert darwin  

Relation: father  
Context: barack obama ( born august 4, 1961 ) is an american attorney and politician who served as the 44th president of the united states from 2009 to 2017. he is the son of barack obama sr., a kenyan economist.  
Head Entity: barack obama  
Tail Entity: barack obama sr.  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon when it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: that same year saran made a special appearance in a scene in the kannada film " arasu " .
Head Entity: arasu
Tail Entity: kannada
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The critically acclaimed movie "Parasite" was primarily filmed in Korean and received international recognition.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Doraemon" is originally produced in Japanese and has been dubbed in several languages worldwide.  
Head Entity: Doraemon  
Tail Entity: Japanese  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: the first stewart king of scotland and son of marjorie bruce and walter stewart , robert ii , is believed to have been born in the abbey .
Head Entity: robert ii
Tail Entity: marjorie bruce
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in the historical records, it is noted that queen elizabeth i was the daughter of anne boleyn, who served as her mother during her early years.  
Head Entity: queen elizabeth i  
Tail Entity: anne boleyn  

Relation: mother  
Context: the famous artist pablo picasso often spoke fondly of his mother, maría ruiz, who greatly influenced his early artistic development.  
Head Entity: pablo picasso  
Tail Entity: maría ruiz  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: the type locality given is " fan - si - pan mountains , tonkin , indo - china " ( mount fansipan , sa pa district , nghệ an province , northwestern vietnam ) .
Head Entity: sa pa district
Tail Entity: vietnam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the capital city of the country is located in the central region, known for its rich history and cultural heritage, particularly in the area of traditional crafts.  
Head Entity: capital city  
Tail Entity: vietnam  

Relation: country  
Context: the famous ancient ruins are situated in a region that was once the heart of a powerful empire, now recognized as a UNESCO World Heritage site.  
Head Entity: ancient ruins  
Tail Entity: cambodia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Mixup data size:  318
MixupTrain:  epoch  0, batch     0 | loss: 2.5071714MixupTrain:  epoch  0, batch     1 | loss: 2.5867540MixupTrain:  epoch  0, batch     2 | loss: 2.8490367MixupTrain:  epoch  0, batch     3 | loss: 2.9812548MixupTrain:  epoch  0, batch     4 | loss: 2.1229362MixupTrain:  epoch  0, batch     5 | loss: 2.2518691MixupTrain:  epoch  0, batch     6 | loss: 2.5039824MixupTrain:  epoch  0, batch     7 | loss: 2.5187764MixupTrain:  epoch  0, batch     8 | loss: 2.3458791MixupTrain:  epoch  0, batch     9 | loss: 2.4741543MixupTrain:  epoch  0, batch    10 | loss: 2.9432797MixupTrain:  epoch  0, batch    11 | loss: 2.6111015MixupTrain:  epoch  0, batch    12 | loss: 2.6565326MixupTrain:  epoch  0, batch    13 | loss: 2.4955127MixupTrain:  epoch  0, batch    14 | loss: 2.3789289MixupTrain:  epoch  0, batch    15 | loss: 2.6664131MixupTrain:  epoch  0, batch    16 | loss: 2.4161519MixupTrain:  epoch  0, batch    17 | loss: 2.2986095MixupTrain:  epoch  0, batch    18 | loss: 2.0799187MixupTrain:  epoch  0, batch    19 | loss: 2.4718209
MemoryTrain:  epoch  0, batch     0 | loss: 1.7229520MemoryTrain:  epoch  0, batch     1 | loss: 2.0254736MemoryTrain:  epoch  0, batch     2 | loss: 2.1409750MemoryTrain:  epoch  0, batch     3 | loss: 2.4136765MemoryTrain:  epoch  0, batch     4 | loss: 2.4497802MemoryTrain:  epoch  0, batch     5 | loss: 2.6515951MemoryTrain:  epoch  0, batch     6 | loss: 2.4770749MemoryTrain:  epoch  0, batch     7 | loss: 2.8476100MemoryTrain:  epoch  1, batch     0 | loss: 2.1853299MemoryTrain:  epoch  1, batch     1 | loss: 2.3177919MemoryTrain:  epoch  1, batch     2 | loss: 2.0013211MemoryTrain:  epoch  1, batch     3 | loss: 1.8862513MemoryTrain:  epoch  1, batch     4 | loss: 2.3588278MemoryTrain:  epoch  1, batch     5 | loss: 1.8285525MemoryTrain:  epoch  1, batch     6 | loss: 2.0908418MemoryTrain:  epoch  1, batch     7 | loss: 1.5970235MemoryTrain:  epoch  2, batch     0 | loss: 1.7829716MemoryTrain:  epoch  2, batch     1 | loss: 1.8573585MemoryTrain:  epoch  2, batch     2 | loss: 1.9750566MemoryTrain:  epoch  2, batch     3 | loss: 1.7262566MemoryTrain:  epoch  2, batch     4 | loss: 1.7739503MemoryTrain:  epoch  2, batch     5 | loss: 1.5784596MemoryTrain:  epoch  2, batch     6 | loss: 1.3881705MemoryTrain:  epoch  2, batch     7 | loss: 1.8969355MemoryTrain:  epoch  3, batch     0 | loss: 1.6883159MemoryTrain:  epoch  3, batch     1 | loss: 1.7362564MemoryTrain:  epoch  3, batch     2 | loss: 1.5011508MemoryTrain:  epoch  3, batch     3 | loss: 1.5753450MemoryTrain:  epoch  3, batch     4 | loss: 1.3907658MemoryTrain:  epoch  3, batch     5 | loss: 1.6965966MemoryTrain:  epoch  3, batch     6 | loss: 1.5850290MemoryTrain:  epoch  3, batch     7 | loss: 1.5364244MemoryTrain:  epoch  4, batch     0 | loss: 1.5780354MemoryTrain:  epoch  4, batch     1 | loss: 1.5517621MemoryTrain:  epoch  4, batch     2 | loss: 1.6882808MemoryTrain:  epoch  4, batch     3 | loss: 1.4887810MemoryTrain:  epoch  4, batch     4 | loss: 1.5790349MemoryTrain:  epoch  4, batch     5 | loss: 1.5255417MemoryTrain:  epoch  4, batch     6 | loss: 1.3456817MemoryTrain:  epoch  4, batch     7 | loss: 1.3087378MemoryTrain:  epoch  5, batch     0 | loss: 1.6256738MemoryTrain:  epoch  5, batch     1 | loss: 1.5411160MemoryTrain:  epoch  5, batch     2 | loss: 1.4347293MemoryTrain:  epoch  5, batch     3 | loss: 1.3263872MemoryTrain:  epoch  5, batch     4 | loss: 1.2992893MemoryTrain:  epoch  5, batch     5 | loss: 1.4671247MemoryTrain:  epoch  5, batch     6 | loss: 1.4343038MemoryTrain:  epoch  5, batch     7 | loss: 1.5468835MemoryTrain:  epoch  6, batch     0 | loss: 1.4415436MemoryTrain:  epoch  6, batch     1 | loss: 1.3518581MemoryTrain:  epoch  6, batch     2 | loss: 1.3882601MemoryTrain:  epoch  6, batch     3 | loss: 1.3287102MemoryTrain:  epoch  6, batch     4 | loss: 1.4616058MemoryTrain:  epoch  6, batch     5 | loss: 1.4337797MemoryTrain:  epoch  6, batch     6 | loss: 1.3520775MemoryTrain:  epoch  6, batch     7 | loss: 1.3613265MemoryTrain:  epoch  7, batch     0 | loss: 1.2822294MemoryTrain:  epoch  7, batch     1 | loss: 1.3695674MemoryTrain:  epoch  7, batch     2 | loss: 1.3277177MemoryTrain:  epoch  7, batch     3 | loss: 1.3903129MemoryTrain:  epoch  7, batch     4 | loss: 1.2647266MemoryTrain:  epoch  7, batch     5 | loss: 1.3334823MemoryTrain:  epoch  7, batch     6 | loss: 1.4153746MemoryTrain:  epoch  7, batch     7 | loss: 1.4190941MemoryTrain:  epoch  8, batch     0 | loss: 1.3704641MemoryTrain:  epoch  8, batch     1 | loss: 1.3429840MemoryTrain:  epoch  8, batch     2 | loss: 1.3060408MemoryTrain:  epoch  8, batch     3 | loss: 1.3593529MemoryTrain:  epoch  8, batch     4 | loss: 1.3778402MemoryTrain:  epoch  8, batch     5 | loss: 1.3330189MemoryTrain:  epoch  8, batch     6 | loss: 1.3089955MemoryTrain:  epoch  8, batch     7 | loss: 1.3821347MemoryTrain:  epoch  9, batch     0 | loss: 1.2948322MemoryTrain:  epoch  9, batch     1 | loss: 1.3106403MemoryTrain:  epoch  9, batch     2 | loss: 1.3428679MemoryTrain:  epoch  9, batch     3 | loss: 1.3271325MemoryTrain:  epoch  9, batch     4 | loss: 1.3247185MemoryTrain:  epoch  9, batch     5 | loss: 1.3092327MemoryTrain:  epoch  9, batch     6 | loss: 1.3581502MemoryTrain:  epoch  9, batch     7 | loss: 1.2883635
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 67.01%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 66.45%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.30%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 80.70%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.42%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 81.76%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 81.58%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 80.61%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 79.38%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 77.90%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 76.64%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 75.58%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 74.86%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 75.68%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 76.91%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 77.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 77.21%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 77.16%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 76.65%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 76.74%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 76.93%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 76.67%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 76.43%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 76.62%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 76.69%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 76.77%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 76.95%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 77.32%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 76.69%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.85%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 88.07%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 87.13%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 87.14%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 86.98%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 86.66%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 86.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.18%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.23%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 88.07%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 87.92%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 87.77%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 87.11%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 87.37%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 87.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.25%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 87.26%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.38%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 87.62%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.61%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 87.39%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 86.75%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 86.33%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 86.35%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 86.27%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 85.99%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 85.91%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 85.84%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 85.58%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 85.51%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 85.35%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 85.29%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 85.24%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 85.27%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 85.04%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 84.81%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 84.67%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 84.54%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 84.50%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 84.54%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 84.58%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 84.46%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 84.49%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 84.45%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 84.65%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 84.45%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 83.89%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 83.26%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 82.79%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 82.49%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 82.26%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 82.39%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 82.58%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.78%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 83.44%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 83.96%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 84.12%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 84.28%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 84.44%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 84.53%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 84.62%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 84.47%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 84.44%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 84.52%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 84.49%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 84.05%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 83.51%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 83.03%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 82.50%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 82.21%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 81.81%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 81.58%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 81.63%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 81.84%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 81.89%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 81.94%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 81.83%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 81.15%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 80.73%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 80.12%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 79.47%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 78.83%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 78.25%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 78.27%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 78.30%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 78.22%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 78.00%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 77.88%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 77.91%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 78.03%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.20%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 78.31%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 78.38%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 78.45%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 78.60%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 78.67%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 78.51%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 78.39%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 78.41%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 78.48%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 78.45%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 78.52%   [EVAL] batch:  144 | acc: 81.25%,  total acc: 78.53%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 78.60%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.70%   [EVAL] batch:  147 | acc: 87.50%,  total acc: 78.76%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.90%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.00%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 79.06%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 79.07%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 79.08%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 79.18%   [EVAL] batch:  154 | acc: 62.50%,  total acc: 79.07%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 79.01%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 78.86%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 78.68%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 78.46%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 78.28%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 78.14%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 77.89%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 77.76%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 77.71%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 77.65%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 77.60%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 77.54%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 77.53%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 77.55%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 77.39%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 77.27%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 77.00%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 76.95%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 76.69%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 76.54%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 76.80%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 77.06%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 77.19%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 77.44%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 77.49%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 77.58%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 77.64%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 77.69%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 77.77%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 77.79%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 77.78%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 77.76%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 77.75%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 77.67%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 77.59%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 77.45%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 77.44%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 77.30%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 77.28%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 77.37%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 77.36%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 77.34%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 77.27%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 77.04%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 77.03%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 76.90%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 76.83%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 76.73%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 76.78%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 76.89%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 77.00%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 77.19%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 77.38%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 77.48%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 77.69%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 77.79%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 78.10%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 78.20%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 78.24%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 78.31%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 78.38%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:  225 | acc: 50.00%,  total acc: 78.35%   [EVAL] batch:  226 | acc: 25.00%,  total acc: 78.11%   [EVAL] batch:  227 | acc: 31.25%,  total acc: 77.91%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 77.67%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 77.45%   [EVAL] batch:  230 | acc: 31.25%,  total acc: 77.25%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 77.26%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 77.33%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 77.40%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 77.57%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 77.58%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 77.64%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 77.58%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 77.54%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 77.56%   [EVAL] batch:  242 | acc: 75.00%,  total acc: 77.55%   [EVAL] batch:  243 | acc: 56.25%,  total acc: 77.46%   [EVAL] batch:  244 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 77.54%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 77.51%   [EVAL] batch:  247 | acc: 87.50%,  total acc: 77.55%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 77.61%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 77.65%   
cur_acc:  ['0.9504', '0.7698', '0.8046', '0.7669']
his_acc:  ['0.9504', '0.8510', '0.8155', '0.7765']
CurrentTrain: epoch  0, batch     0 | loss: 5.2377143CurrentTrain: epoch  0, batch     1 | loss: 5.6297984CurrentTrain: epoch  0, batch     2 | loss: 5.2246675CurrentTrain: epoch  0, batch     3 | loss: 4.2176661CurrentTrain: epoch  1, batch     0 | loss: 4.1336861CurrentTrain: epoch  1, batch     1 | loss: 5.0676637CurrentTrain: epoch  1, batch     2 | loss: 3.7794333CurrentTrain: epoch  1, batch     3 | loss: 3.3473816CurrentTrain: epoch  2, batch     0 | loss: 3.3401434CurrentTrain: epoch  2, batch     1 | loss: 4.3955765CurrentTrain: epoch  2, batch     2 | loss: 3.0438809CurrentTrain: epoch  2, batch     3 | loss: 2.3371506CurrentTrain: epoch  3, batch     0 | loss: 3.1232991CurrentTrain: epoch  3, batch     1 | loss: 3.0101202CurrentTrain: epoch  3, batch     2 | loss: 3.8430591CurrentTrain: epoch  3, batch     3 | loss: 2.2141657CurrentTrain: epoch  4, batch     0 | loss: 2.6995397CurrentTrain: epoch  4, batch     1 | loss: 3.7998974CurrentTrain: epoch  4, batch     2 | loss: 2.9616706CurrentTrain: epoch  4, batch     3 | loss: 1.9821671CurrentTrain: epoch  5, batch     0 | loss: 2.4780431CurrentTrain: epoch  5, batch     1 | loss: 2.4480364CurrentTrain: epoch  5, batch     2 | loss: 3.7924204CurrentTrain: epoch  5, batch     3 | loss: 3.8378081CurrentTrain: epoch  6, batch     0 | loss: 2.9460561CurrentTrain: epoch  6, batch     1 | loss: 2.8902087CurrentTrain: epoch  6, batch     2 | loss: 2.5595498CurrentTrain: epoch  6, batch     3 | loss: 3.1488042CurrentTrain: epoch  7, batch     0 | loss: 2.5500488CurrentTrain: epoch  7, batch     1 | loss: 2.8692696CurrentTrain: epoch  7, batch     2 | loss: 2.5626595CurrentTrain: epoch  7, batch     3 | loss: 3.6531096CurrentTrain: epoch  8, batch     0 | loss: 2.4299347CurrentTrain: epoch  8, batch     1 | loss: 3.2113466CurrentTrain: epoch  8, batch     2 | loss: 2.0354071CurrentTrain: epoch  8, batch     3 | loss: 2.1169684CurrentTrain: epoch  9, batch     0 | loss: 2.7286377CurrentTrain: epoch  9, batch     1 | loss: 2.3833766CurrentTrain: epoch  9, batch     2 | loss: 2.6594810CurrentTrain: epoch  9, batch     3 | loss: 2.1132827
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: elizabeth warren is a prominent member of the democratic party, advocating for progressive policies and reforms.  
Head Entity: elizabeth warren  
Tail Entity: democratic party  

Relation: member of political party  
Context: during his tenure, barack obama was a key figure in the democratic party, leading numerous initiatives and campaigns.  
Head Entity: barack obama  
Tail Entity: democratic party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film is based on the novel "the great gatsby" by f. scott fitzgerald, capturing the essence of the roaring twenties.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: this painting draws inspiration from the famous artwork "starry night" created by vincent van gogh, showcasing swirling skies and vibrant colors.  
Head Entity: starry night  
Tail Entity: vincent van gogh  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: in 2011 , zhigarev was elected as a deputy in the state duma , the lower house of the federal assembly of russia , as a representative for the shchelkovsky constituency in moscow .
Head Entity: state duma
Tail Entity: moscow
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: The global technology company, headquartered in Cupertino, California, has been a leader in innovation for over a decade.  
Head Entity: global technology company  
Tail Entity: Cupertino  

Relation: headquarters location  
Context: The non-profit organization moved its headquarters to Austin, Texas, to better serve its growing community of volunteers and supporters.  
Head Entity: non-profit organization  
Tail Entity: Austin  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie was renowned for her groundbreaking research in radioactivity, which laid the foundation for future advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: wjmi ( 99.7 fm ) is a radio station licensed to jackson , mississippi , united states , with a mainstream urban musical format .
Head Entity: wjmi
Tail Entity: jackson , mississippi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: kxyz ( 101.5 fm ) is a radio station licensed to transmit to the city of austin , texas , providing a mix of rock and pop music.  
Head Entity: kxyz  
Tail Entity: austin , texas  

Relation: licensed to broadcast to  
Context: wqrs ( 88.3 fm ) is a public radio station licensed to serve the community of boulder , colorado , focusing on educational programming.  
Head Entity: wqrs  
Tail Entity: boulder , colorado  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion and is one of the brightest nebulae visible to the naked eye.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major, which is known for its prominence in the night sky.  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Mixup data size:  378
MixupTrain:  epoch  0, batch     0 | loss: 1.7784404MixupTrain:  epoch  0, batch     1 | loss: 1.8051099MixupTrain:  epoch  0, batch     2 | loss: 1.9741768MixupTrain:  epoch  0, batch     3 | loss: 1.8796821MixupTrain:  epoch  0, batch     4 | loss: 2.5666613MixupTrain:  epoch  0, batch     5 | loss: 1.7629015MixupTrain:  epoch  0, batch     6 | loss: 1.8522307MixupTrain:  epoch  0, batch     7 | loss: 2.0441440MixupTrain:  epoch  0, batch     8 | loss: 2.1935284MixupTrain:  epoch  0, batch     9 | loss: 2.0166983MixupTrain:  epoch  0, batch    10 | loss: 2.1806879MixupTrain:  epoch  0, batch    11 | loss: 1.5652478MixupTrain:  epoch  0, batch    12 | loss: 2.0908511MixupTrain:  epoch  0, batch    13 | loss: 2.2737115MixupTrain:  epoch  0, batch    14 | loss: 1.6876833MixupTrain:  epoch  0, batch    15 | loss: 1.5117164MixupTrain:  epoch  0, batch    16 | loss: 1.7264729MixupTrain:  epoch  0, batch    17 | loss: 1.9679796MixupTrain:  epoch  0, batch    18 | loss: 1.8806403MixupTrain:  epoch  0, batch    19 | loss: 1.5547587MixupTrain:  epoch  0, batch    20 | loss: 1.9075060MixupTrain:  epoch  0, batch    21 | loss: 1.8591153MixupTrain:  epoch  0, batch    22 | loss: 2.3021866MixupTrain:  epoch  0, batch    23 | loss: 1.5246371
MemoryTrain:  epoch  0, batch     0 | loss: 1.9523807MemoryTrain:  epoch  0, batch     1 | loss: 1.4197482MemoryTrain:  epoch  0, batch     2 | loss: 1.8663759MemoryTrain:  epoch  0, batch     3 | loss: 2.5256653MemoryTrain:  epoch  0, batch     4 | loss: 1.8754334MemoryTrain:  epoch  0, batch     5 | loss: 1.6935139MemoryTrain:  epoch  0, batch     6 | loss: 2.2769690MemoryTrain:  epoch  0, batch     7 | loss: 2.4753828MemoryTrain:  epoch  0, batch     8 | loss: 2.1880319MemoryTrain:  epoch  0, batch     9 | loss: 1.9242804MemoryTrain:  epoch  1, batch     0 | loss: 1.7504014MemoryTrain:  epoch  1, batch     1 | loss: 1.7575886MemoryTrain:  epoch  1, batch     2 | loss: 1.6661977MemoryTrain:  epoch  1, batch     3 | loss: 2.1642084MemoryTrain:  epoch  1, batch     4 | loss: 1.4446898MemoryTrain:  epoch  1, batch     5 | loss: 1.3787665MemoryTrain:  epoch  1, batch     6 | loss: 1.5897764MemoryTrain:  epoch  1, batch     7 | loss: 1.5112875MemoryTrain:  epoch  1, batch     8 | loss: 1.5814829MemoryTrain:  epoch  1, batch     9 | loss: 1.4752522MemoryTrain:  epoch  2, batch     0 | loss: 1.3373425MemoryTrain:  epoch  2, batch     1 | loss: 1.5015054MemoryTrain:  epoch  2, batch     2 | loss: 1.6296902MemoryTrain:  epoch  2, batch     3 | loss: 1.2441194MemoryTrain:  epoch  2, batch     4 | loss: 1.4589219MemoryTrain:  epoch  2, batch     5 | loss: 1.5468686MemoryTrain:  epoch  2, batch     6 | loss: 1.6729773MemoryTrain:  epoch  2, batch     7 | loss: 1.4309092MemoryTrain:  epoch  2, batch     8 | loss: 1.7412423MemoryTrain:  epoch  2, batch     9 | loss: 1.3104730MemoryTrain:  epoch  3, batch     0 | loss: 1.6054791MemoryTrain:  epoch  3, batch     1 | loss: 1.4683611MemoryTrain:  epoch  3, batch     2 | loss: 1.4477158MemoryTrain:  epoch  3, batch     3 | loss: 1.4203318MemoryTrain:  epoch  3, batch     4 | loss: 1.3183944MemoryTrain:  epoch  3, batch     5 | loss: 1.3604378MemoryTrain:  epoch  3, batch     6 | loss: 1.2764641MemoryTrain:  epoch  3, batch     7 | loss: 1.3846886MemoryTrain:  epoch  3, batch     8 | loss: 1.3489914MemoryTrain:  epoch  3, batch     9 | loss: 1.2570164MemoryTrain:  epoch  4, batch     0 | loss: 1.3334626MemoryTrain:  epoch  4, batch     1 | loss: 1.3303699MemoryTrain:  epoch  4, batch     2 | loss: 1.4354641MemoryTrain:  epoch  4, batch     3 | loss: 1.2857053MemoryTrain:  epoch  4, batch     4 | loss: 1.3141062MemoryTrain:  epoch  4, batch     5 | loss: 1.4410784MemoryTrain:  epoch  4, batch     6 | loss: 1.2959232MemoryTrain:  epoch  4, batch     7 | loss: 1.2608374MemoryTrain:  epoch  4, batch     8 | loss: 1.3751533MemoryTrain:  epoch  4, batch     9 | loss: 1.8322635MemoryTrain:  epoch  5, batch     0 | loss: 1.2718470MemoryTrain:  epoch  5, batch     1 | loss: 1.4623761MemoryTrain:  epoch  5, batch     2 | loss: 1.2731084MemoryTrain:  epoch  5, batch     3 | loss: 1.3269316MemoryTrain:  epoch  5, batch     4 | loss: 1.2723371MemoryTrain:  epoch  5, batch     5 | loss: 1.3118119MemoryTrain:  epoch  5, batch     6 | loss: 1.2946930MemoryTrain:  epoch  5, batch     7 | loss: 1.3169162MemoryTrain:  epoch  5, batch     8 | loss: 1.2838264MemoryTrain:  epoch  5, batch     9 | loss: 1.1960224MemoryTrain:  epoch  6, batch     0 | loss: 1.2953193MemoryTrain:  epoch  6, batch     1 | loss: 1.2511038MemoryTrain:  epoch  6, batch     2 | loss: 1.2999718MemoryTrain:  epoch  6, batch     3 | loss: 1.2450523MemoryTrain:  epoch  6, batch     4 | loss: 1.2360176MemoryTrain:  epoch  6, batch     5 | loss: 1.2510957MemoryTrain:  epoch  6, batch     6 | loss: 1.2201862MemoryTrain:  epoch  6, batch     7 | loss: 1.3859029MemoryTrain:  epoch  6, batch     8 | loss: 1.2429925MemoryTrain:  epoch  6, batch     9 | loss: 1.2049558MemoryTrain:  epoch  7, batch     0 | loss: 1.2235711MemoryTrain:  epoch  7, batch     1 | loss: 1.2444324MemoryTrain:  epoch  7, batch     2 | loss: 1.2339805MemoryTrain:  epoch  7, batch     3 | loss: 1.2048156MemoryTrain:  epoch  7, batch     4 | loss: 1.2650062MemoryTrain:  epoch  7, batch     5 | loss: 1.3308182MemoryTrain:  epoch  7, batch     6 | loss: 1.2338825MemoryTrain:  epoch  7, batch     7 | loss: 1.1914420MemoryTrain:  epoch  7, batch     8 | loss: 1.2016213MemoryTrain:  epoch  7, batch     9 | loss: 1.2922497MemoryTrain:  epoch  8, batch     0 | loss: 1.2417812MemoryTrain:  epoch  8, batch     1 | loss: 1.2293791MemoryTrain:  epoch  8, batch     2 | loss: 1.2505310MemoryTrain:  epoch  8, batch     3 | loss: 1.2528952MemoryTrain:  epoch  8, batch     4 | loss: 1.2677512MemoryTrain:  epoch  8, batch     5 | loss: 1.2839457MemoryTrain:  epoch  8, batch     6 | loss: 1.2380018MemoryTrain:  epoch  8, batch     7 | loss: 1.2034235MemoryTrain:  epoch  8, batch     8 | loss: 1.2220639MemoryTrain:  epoch  8, batch     9 | loss: 1.2259244MemoryTrain:  epoch  9, batch     0 | loss: 1.3183702MemoryTrain:  epoch  9, batch     1 | loss: 1.2040110MemoryTrain:  epoch  9, batch     2 | loss: 1.2479796MemoryTrain:  epoch  9, batch     3 | loss: 1.2026219MemoryTrain:  epoch  9, batch     4 | loss: 1.2552316MemoryTrain:  epoch  9, batch     5 | loss: 1.2858191MemoryTrain:  epoch  9, batch     6 | loss: 1.2240133MemoryTrain:  epoch  9, batch     7 | loss: 1.2379847MemoryTrain:  epoch  9, batch     8 | loss: 1.2773678MemoryTrain:  epoch  9, batch     9 | loss: 1.3004913
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 95.31%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 95.00%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 86.56%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 82.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 85.61%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 84.93%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 84.64%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 84.20%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 83.45%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 83.06%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.30%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.67%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 85.03%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.56%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 86.17%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.73%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 86.89%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 87.02%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 87.16%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 87.39%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 87.61%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 88.00%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 87.40%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.42%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 75.82%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.31%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.05%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 80.51%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 80.54%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 80.21%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 79.73%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 79.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.45%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 81.40%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.12%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 82.24%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 82.22%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 81.93%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 81.52%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 81.51%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 81.38%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 81.37%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 81.37%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 81.48%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 81.02%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 81.14%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 80.92%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 80.39%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 80.08%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 80.23%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 80.04%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 79.96%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 79.69%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 79.33%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 79.01%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 78.77%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 78.62%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 78.66%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 78.52%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 78.30%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 78.42%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 78.46%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 78.50%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 78.54%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 78.57%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 78.37%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 78.56%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 79.01%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 78.96%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 78.46%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 77.98%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 77.65%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 77.33%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 77.37%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 77.41%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 77.60%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 77.85%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 77.95%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 78.23%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 78.46%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 78.68%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 79.12%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 79.34%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 79.95%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 80.09%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 80.04%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 80.05%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 80.18%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 80.19%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 79.79%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 79.34%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 79.01%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 78.52%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 78.21%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 77.73%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 77.54%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 77.63%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.83%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 77.91%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 77.99%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 78.07%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 77.99%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 77.34%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 76.96%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 76.38%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 75.76%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 75.15%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 74.60%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 74.75%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 74.66%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 74.61%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 74.37%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 74.33%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 74.53%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 74.81%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 74.91%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 75.05%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 75.32%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 75.18%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 75.13%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 75.22%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 75.31%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 75.39%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 75.39%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 75.47%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:  147 | acc: 87.50%,  total acc: 75.68%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 75.80%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 75.92%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 75.95%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 75.99%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 76.02%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 76.14%   [EVAL] batch:  154 | acc: 81.25%,  total acc: 76.17%   [EVAL] batch:  155 | acc: 75.00%,  total acc: 76.16%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 76.07%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 75.91%   [EVAL] batch:  158 | acc: 37.50%,  total acc: 75.67%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 75.51%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 75.39%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 75.19%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 75.04%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 74.96%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 74.89%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 74.81%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 74.78%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 74.70%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 74.70%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 74.52%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 74.42%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 74.20%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 74.17%   [EVAL] batch:  173 | acc: 37.50%,  total acc: 73.96%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 73.82%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 73.97%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 74.26%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 74.41%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 74.69%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 74.93%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 75.03%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 75.10%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 75.17%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 75.27%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 75.30%   [EVAL] batch:  188 | acc: 43.75%,  total acc: 75.13%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 75.07%   [EVAL] batch:  190 | acc: 56.25%,  total acc: 74.97%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 74.87%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 74.81%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 74.65%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 74.62%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 74.55%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 74.59%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 74.68%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 74.69%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 74.69%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 74.63%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 74.41%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 74.30%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 74.27%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 74.18%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 74.25%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 74.49%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 74.91%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 75.03%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 75.15%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.37%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 75.71%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 75.90%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 75.98%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  225 | acc: 50.00%,  total acc: 76.08%   [EVAL] batch:  226 | acc: 37.50%,  total acc: 75.91%   [EVAL] batch:  227 | acc: 25.00%,  total acc: 75.69%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 75.46%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 75.22%   [EVAL] batch:  230 | acc: 31.25%,  total acc: 75.03%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 75.05%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 75.13%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 75.21%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 75.40%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 75.45%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 75.45%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 75.47%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 75.42%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 75.36%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 75.36%   [EVAL] batch:  242 | acc: 68.75%,  total acc: 75.33%   [EVAL] batch:  243 | acc: 56.25%,  total acc: 75.26%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 75.28%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 75.30%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 75.23%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 75.30%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 75.35%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 75.35%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 75.52%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 75.66%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 75.83%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 75.88%   [EVAL] batch:  257 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 76.06%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 76.11%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 76.10%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 76.12%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 76.16%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 76.21%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 76.25%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 76.27%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 76.29%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 76.26%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 76.23%   [EVAL] batch:  269 | acc: 62.50%,  total acc: 76.18%   [EVAL] batch:  270 | acc: 68.75%,  total acc: 76.15%   [EVAL] batch:  271 | acc: 62.50%,  total acc: 76.10%   [EVAL] batch:  272 | acc: 75.00%,  total acc: 76.10%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 76.09%   [EVAL] batch:  274 | acc: 56.25%,  total acc: 76.02%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 76.11%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 76.45%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 76.53%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 76.53%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 76.55%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 76.50%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 76.49%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 76.46%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 76.39%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 76.37%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 76.45%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 76.53%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 76.77%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 76.83%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 76.91%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 76.98%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 77.06%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 77.14%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 77.22%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:  300 | acc: 81.25%,  total acc: 77.30%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 77.36%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 77.41%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 77.47%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 77.48%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 77.53%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 77.64%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 77.69%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 77.76%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 77.83%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 77.86%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 77.78%   
cur_acc:  ['0.9504', '0.7698', '0.8046', '0.7669', '0.8740']
his_acc:  ['0.9504', '0.8510', '0.8155', '0.7765', '0.7778']
CurrentTrain: epoch  0, batch     0 | loss: 6.2210484CurrentTrain: epoch  0, batch     1 | loss: 5.8374462CurrentTrain: epoch  0, batch     2 | loss: 7.2476382CurrentTrain: epoch  0, batch     3 | loss: 6.3836875CurrentTrain: epoch  1, batch     0 | loss: 5.7452049CurrentTrain: epoch  1, batch     1 | loss: 5.1055264CurrentTrain: epoch  1, batch     2 | loss: 5.7747931CurrentTrain: epoch  1, batch     3 | loss: 3.7805901CurrentTrain: epoch  2, batch     0 | loss: 4.8537054CurrentTrain: epoch  2, batch     1 | loss: 4.5272741CurrentTrain: epoch  2, batch     2 | loss: 4.9207401CurrentTrain: epoch  2, batch     3 | loss: 5.0403223CurrentTrain: epoch  3, batch     0 | loss: 4.2267728CurrentTrain: epoch  3, batch     1 | loss: 4.6942420CurrentTrain: epoch  3, batch     2 | loss: 4.0030589CurrentTrain: epoch  3, batch     3 | loss: 4.6707249CurrentTrain: epoch  4, batch     0 | loss: 3.7711668CurrentTrain: epoch  4, batch     1 | loss: 4.0821896CurrentTrain: epoch  4, batch     2 | loss: 4.0992260CurrentTrain: epoch  4, batch     3 | loss: 3.7404766CurrentTrain: epoch  5, batch     0 | loss: 3.3431852CurrentTrain: epoch  5, batch     1 | loss: 3.4129930CurrentTrain: epoch  5, batch     2 | loss: 4.0053716CurrentTrain: epoch  5, batch     3 | loss: 3.9697542CurrentTrain: epoch  6, batch     0 | loss: 3.6620126CurrentTrain: epoch  6, batch     1 | loss: 3.1497245CurrentTrain: epoch  6, batch     2 | loss: 3.0572793CurrentTrain: epoch  6, batch     3 | loss: 5.9403038CurrentTrain: epoch  7, batch     0 | loss: 3.8772337CurrentTrain: epoch  7, batch     1 | loss: 3.1616609CurrentTrain: epoch  7, batch     2 | loss: 2.7095251CurrentTrain: epoch  7, batch     3 | loss: 2.3229079CurrentTrain: epoch  8, batch     0 | loss: 2.7368073CurrentTrain: epoch  8, batch     1 | loss: 2.7910757CurrentTrain: epoch  8, batch     2 | loss: 3.4171767CurrentTrain: epoch  8, batch     3 | loss: 3.8088326CurrentTrain: epoch  9, batch     0 | loss: 2.9182262CurrentTrain: epoch  9, batch     1 | loss: 2.9081302CurrentTrain: epoch  9, batch     2 | loss: 2.6018589CurrentTrain: epoch  9, batch     3 | loss: 2.8244541
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, also known as cop26, took place in glasgow, scotland, where numerous world leaders gathered to discuss climate action.  
Head Entity: united nations climate change conference  
Tail Entity: cop26  

Relation: participant of  
Context: elon musk, the ceo of spacex, was a key participant in the 2021 met gala, showcasing his vision for sustainable energy and space exploration.  
Head Entity: elon musk  
Tail Entity: 2021 met gala  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: the festival opened with " whiplash " directed by damien chazelle and closed with musical drama " rudderless " directed by william h. macy .
Head Entity: whiplash
Tail Entity: damien chazelle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the critically acclaimed series "breaking bad" was brought to life by director vince gilligan, while the spin-off "better call saul" was directed by peter gould.  
Head Entity: breaking bad  
Tail Entity: vince gilligan  

Relation: director  
Context: the animated feature "spider-man: into the spider-verse" was directed by bob persichetti, peter ramsey, and rodney rothman, showcasing a unique visual style.  
Head Entity: spider-man: into the spider-verse  
Tail Entity: bob persichetti  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: chernoff also reported and anchored for time warner 's now - defunct cnnfn .
Head Entity: cnnfn
Tail Entity: time warner
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: The famous painting "Starry Night" is part of the collection at the Museum of Modern Art, which is owned by the MoMA.  
Head Entity: Museum of Modern Art  
Tail Entity: MoMA  

Relation: owned by  
Context: The iconic brand Nike was founded by Phil Knight and is currently owned by the publicly traded company Nike, Inc.  
Head Entity: Nike, Inc.  
Tail Entity: Nike  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and now it serves as a gallery showcasing his works, attracting visitors from all over the world.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now thrive.  
Head Entity: old factory  
Tail Entity: startups
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: reading town hall was built in several phases between 1786 and 1897 , although the principal facade was designed by alfred waterhouse in 1875 .
Head Entity: reading town hall
Tail Entity: alfred waterhouse
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the new art museum was designed by renowned architect zaha hadid, whose innovative style has transformed modern architecture.  
Head Entity: new art museum  
Tail Entity: zaha hadid  

Relation: architect  
Context: the iconic sydney opera house was the brainchild of architect jørn utzon, who won the design competition in 1957.  
Head Entity: sydney opera house  
Tail Entity: jørn utzon  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a fresh start.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida as a teenager.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: the postseason tournament concluded with the san francisco 49ers defeating the cincinnati bengals in super bowl xvi , 26–21 , on january 24 , 1982 , at the pontiac silverdome in pontiac , michigan .
Head Entity: super bowl xvi
Tail Entity: pontiac silverdome
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: the annual music festival took place in the vibrant city of new orleans, attracting thousands of visitors from around the world.  
Head Entity: music festival  
Tail Entity: new orleans  

Relation: location  
Context: the historic battle was fought at gettysburg, a pivotal moment in the civil war that changed the course of american history.  
Head Entity: battle  
Tail Entity: gettysburg  
Mixup data size:  439
MixupTrain:  epoch  0, batch     0 | loss: 1.6975515MixupTrain:  epoch  0, batch     1 | loss: 1.6131288MixupTrain:  epoch  0, batch     2 | loss: 2.4273871MixupTrain:  epoch  0, batch     3 | loss: 2.4357943MixupTrain:  epoch  0, batch     4 | loss: 1.9813596MixupTrain:  epoch  0, batch     5 | loss: 2.2588978MixupTrain:  epoch  0, batch     6 | loss: 2.1956189MixupTrain:  epoch  0, batch     7 | loss: 1.7605446MixupTrain:  epoch  0, batch     8 | loss: 2.1033373MixupTrain:  epoch  0, batch     9 | loss: 2.3056189MixupTrain:  epoch  0, batch    10 | loss: 2.4118195MixupTrain:  epoch  0, batch    11 | loss: 1.9650215MixupTrain:  epoch  0, batch    12 | loss: 2.1587918MixupTrain:  epoch  0, batch    13 | loss: 2.3669976MixupTrain:  epoch  0, batch    14 | loss: 2.1055061MixupTrain:  epoch  0, batch    15 | loss: 2.0664173MixupTrain:  epoch  0, batch    16 | loss: 1.7853715MixupTrain:  epoch  0, batch    17 | loss: 2.0179963MixupTrain:  epoch  0, batch    18 | loss: 2.0367013MixupTrain:  epoch  0, batch    19 | loss: 1.7554948MixupTrain:  epoch  0, batch    20 | loss: 2.2832779MixupTrain:  epoch  0, batch    21 | loss: 1.7527211MixupTrain:  epoch  0, batch    22 | loss: 1.7290869MixupTrain:  epoch  0, batch    23 | loss: 2.2102767MixupTrain:  epoch  0, batch    24 | loss: 1.8658973MixupTrain:  epoch  0, batch    25 | loss: 1.9574601MixupTrain:  epoch  0, batch    26 | loss: 1.7536344MixupTrain:  epoch  0, batch    27 | loss: 2.1664977
MemoryTrain:  epoch  0, batch     0 | loss: 1.6289324MemoryTrain:  epoch  0, batch     1 | loss: 2.5152483MemoryTrain:  epoch  0, batch     2 | loss: 1.6210532MemoryTrain:  epoch  0, batch     3 | loss: 2.1367433MemoryTrain:  epoch  0, batch     4 | loss: 2.3586154MemoryTrain:  epoch  0, batch     5 | loss: 1.4885375MemoryTrain:  epoch  0, batch     6 | loss: 2.3539650MemoryTrain:  epoch  0, batch     7 | loss: 2.0591614MemoryTrain:  epoch  0, batch     8 | loss: 2.2182503MemoryTrain:  epoch  0, batch     9 | loss: 2.1821024MemoryTrain:  epoch  0, batch    10 | loss: 2.0304499MemoryTrain:  epoch  0, batch    11 | loss: 3.1986227MemoryTrain:  epoch  1, batch     0 | loss: 1.4789212MemoryTrain:  epoch  1, batch     1 | loss: 1.8790107MemoryTrain:  epoch  1, batch     2 | loss: 2.2243893MemoryTrain:  epoch  1, batch     3 | loss: 1.7437351MemoryTrain:  epoch  1, batch     4 | loss: 1.4194553MemoryTrain:  epoch  1, batch     5 | loss: 1.6383735MemoryTrain:  epoch  1, batch     6 | loss: 2.1740286MemoryTrain:  epoch  1, batch     7 | loss: 2.0545850MemoryTrain:  epoch  1, batch     8 | loss: 2.1012592MemoryTrain:  epoch  1, batch     9 | loss: 1.2608097MemoryTrain:  epoch  1, batch    10 | loss: 2.0552926MemoryTrain:  epoch  1, batch    11 | loss: 1.2265778MemoryTrain:  epoch  2, batch     0 | loss: 1.6925197MemoryTrain:  epoch  2, batch     1 | loss: 1.3071771MemoryTrain:  epoch  2, batch     2 | loss: 1.4348460MemoryTrain:  epoch  2, batch     3 | loss: 1.4368284MemoryTrain:  epoch  2, batch     4 | loss: 1.5142016MemoryTrain:  epoch  2, batch     5 | loss: 1.5639110MemoryTrain:  epoch  2, batch     6 | loss: 1.7376457MemoryTrain:  epoch  2, batch     7 | loss: 2.0769849MemoryTrain:  epoch  2, batch     8 | loss: 1.3811209MemoryTrain:  epoch  2, batch     9 | loss: 1.9551253MemoryTrain:  epoch  2, batch    10 | loss: 1.6617380MemoryTrain:  epoch  2, batch    11 | loss: 1.2459314MemoryTrain:  epoch  3, batch     0 | loss: 1.4838772MemoryTrain:  epoch  3, batch     1 | loss: 1.2474564MemoryTrain:  epoch  3, batch     2 | loss: 1.4748183MemoryTrain:  epoch  3, batch     3 | loss: 1.2926812MemoryTrain:  epoch  3, batch     4 | loss: 1.5002577MemoryTrain:  epoch  3, batch     5 | loss: 1.8405898MemoryTrain:  epoch  3, batch     6 | loss: 1.7327893MemoryTrain:  epoch  3, batch     7 | loss: 1.4994552MemoryTrain:  epoch  3, batch     8 | loss: 1.2441379MemoryTrain:  epoch  3, batch     9 | loss: 1.8155999MemoryTrain:  epoch  3, batch    10 | loss: 1.4552920MemoryTrain:  epoch  3, batch    11 | loss: 1.2670437MemoryTrain:  epoch  4, batch     0 | loss: 1.3274895MemoryTrain:  epoch  4, batch     1 | loss: 1.2255543MemoryTrain:  epoch  4, batch     2 | loss: 1.3790451MemoryTrain:  epoch  4, batch     3 | loss: 1.3622968MemoryTrain:  epoch  4, batch     4 | loss: 1.5251391MemoryTrain:  epoch  4, batch     5 | loss: 1.3374562MemoryTrain:  epoch  4, batch     6 | loss: 1.7025948MemoryTrain:  epoch  4, batch     7 | loss: 1.3277212MemoryTrain:  epoch  4, batch     8 | loss: 1.2294819MemoryTrain:  epoch  4, batch     9 | loss: 1.5536463MemoryTrain:  epoch  4, batch    10 | loss: 1.5301666MemoryTrain:  epoch  4, batch    11 | loss: 1.1933722MemoryTrain:  epoch  5, batch     0 | loss: 1.3479029MemoryTrain:  epoch  5, batch     1 | loss: 1.4646969MemoryTrain:  epoch  5, batch     2 | loss: 1.4417052MemoryTrain:  epoch  5, batch     3 | loss: 1.3400288MemoryTrain:  epoch  5, batch     4 | loss: 1.2300475MemoryTrain:  epoch  5, batch     5 | loss: 1.2832901MemoryTrain:  epoch  5, batch     6 | loss: 1.4125004MemoryTrain:  epoch  5, batch     7 | loss: 1.2727484MemoryTrain:  epoch  5, batch     8 | loss: 1.2682738MemoryTrain:  epoch  5, batch     9 | loss: 1.3381964MemoryTrain:  epoch  5, batch    10 | loss: 1.3865807MemoryTrain:  epoch  5, batch    11 | loss: 1.6615369MemoryTrain:  epoch  6, batch     0 | loss: 1.6320715MemoryTrain:  epoch  6, batch     1 | loss: 1.2164700MemoryTrain:  epoch  6, batch     2 | loss: 1.2950637MemoryTrain:  epoch  6, batch     3 | loss: 1.4012542MemoryTrain:  epoch  6, batch     4 | loss: 1.2343373MemoryTrain:  epoch  6, batch     5 | loss: 1.2855692MemoryTrain:  epoch  6, batch     6 | loss: 1.2898496MemoryTrain:  epoch  6, batch     7 | loss: 1.2438363MemoryTrain:  epoch  6, batch     8 | loss: 1.5785906MemoryTrain:  epoch  6, batch     9 | loss: 1.2336006MemoryTrain:  epoch  6, batch    10 | loss: 1.2457078MemoryTrain:  epoch  6, batch    11 | loss: 1.2567782MemoryTrain:  epoch  7, batch     0 | loss: 1.2758250MemoryTrain:  epoch  7, batch     1 | loss: 1.2910780MemoryTrain:  epoch  7, batch     2 | loss: 1.2237296MemoryTrain:  epoch  7, batch     3 | loss: 1.4173679MemoryTrain:  epoch  7, batch     4 | loss: 1.2859583MemoryTrain:  epoch  7, batch     5 | loss: 1.2888491MemoryTrain:  epoch  7, batch     6 | loss: 1.4583647MemoryTrain:  epoch  7, batch     7 | loss: 1.3184770MemoryTrain:  epoch  7, batch     8 | loss: 1.2731090MemoryTrain:  epoch  7, batch     9 | loss: 1.3511274MemoryTrain:  epoch  7, batch    10 | loss: 1.2300568MemoryTrain:  epoch  7, batch    11 | loss: 1.2031653MemoryTrain:  epoch  8, batch     0 | loss: 1.2976985MemoryTrain:  epoch  8, batch     1 | loss: 1.2975533MemoryTrain:  epoch  8, batch     2 | loss: 1.3250477MemoryTrain:  epoch  8, batch     3 | loss: 1.3332304MemoryTrain:  epoch  8, batch     4 | loss: 1.1999807MemoryTrain:  epoch  8, batch     5 | loss: 1.3484499MemoryTrain:  epoch  8, batch     6 | loss: 1.2631469MemoryTrain:  epoch  8, batch     7 | loss: 1.4528720MemoryTrain:  epoch  8, batch     8 | loss: 1.2400150MemoryTrain:  epoch  8, batch     9 | loss: 1.2163409MemoryTrain:  epoch  8, batch    10 | loss: 1.1947825MemoryTrain:  epoch  8, batch    11 | loss: 1.4159250MemoryTrain:  epoch  9, batch     0 | loss: 1.2314677MemoryTrain:  epoch  9, batch     1 | loss: 1.2934842MemoryTrain:  epoch  9, batch     2 | loss: 1.2850033MemoryTrain:  epoch  9, batch     3 | loss: 1.2642248MemoryTrain:  epoch  9, batch     4 | loss: 1.2833232MemoryTrain:  epoch  9, batch     5 | loss: 1.2105629MemoryTrain:  epoch  9, batch     6 | loss: 1.2393749MemoryTrain:  epoch  9, batch     7 | loss: 1.2292157MemoryTrain:  epoch  9, batch     8 | loss: 1.2866765MemoryTrain:  epoch  9, batch     9 | loss: 1.1957548MemoryTrain:  epoch  9, batch    10 | loss: 1.4030838MemoryTrain:  epoch  9, batch    11 | loss: 1.1733236
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 41.67%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 46.43%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 51.56%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 56.94%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 73.03%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 69.32%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 68.48%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 65.75%   [EVAL] batch:   25 | acc: 12.50%,  total acc: 63.70%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 62.04%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 60.71%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 59.05%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 57.92%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 56.05%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 57.39%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 58.09%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 59.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 60.24%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 60.98%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 62.01%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 62.66%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 64.88%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 66.34%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 66.71%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 66.76%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 66.80%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 67.12%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 66.54%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 66.71%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 66.63%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 66.90%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 66.70%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 66.96%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 66.56%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 66.27%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 66.00%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 66.15%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 65.88%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 66.13%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 65.58%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 74.48%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 80.08%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 80.49%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 79.78%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 79.82%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 79.22%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 79.44%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.97%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 81.40%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.69%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.82%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 81.39%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 80.85%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 80.60%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 80.87%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 80.75%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 80.76%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 80.53%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.78%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 80.44%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 80.00%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 80.13%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 79.82%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 79.20%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 78.92%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 79.06%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 79.10%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 78.93%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 78.77%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 78.61%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 78.37%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 78.50%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 78.45%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 78.22%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 78.08%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 77.99%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 77.69%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 77.65%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 77.53%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 77.63%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 77.48%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 77.61%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 77.73%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 77.97%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 77.79%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 77.38%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 77.21%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 77.18%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 77.37%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 77.41%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 77.60%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 77.85%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 77.82%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 77.99%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 78.02%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 78.26%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 78.49%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 78.93%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 79.15%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 79.76%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 79.90%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 79.85%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 79.87%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 80.01%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 79.56%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 79.11%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 78.90%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 78.47%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 78.21%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 77.85%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 77.71%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 77.80%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 78.07%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 78.15%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 78.28%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 78.20%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 77.55%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 77.17%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 76.59%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 75.97%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 75.35%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 74.80%   [EVAL] batch:  125 | acc: 81.25%,  total acc: 74.85%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 74.80%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 74.71%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 74.42%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 74.53%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 74.81%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 74.95%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 75.32%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 75.41%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 75.27%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 75.22%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 75.27%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 75.35%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 75.31%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 75.30%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 75.13%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 75.09%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 74.91%   [EVAL] batch:  147 | acc: 56.25%,  total acc: 74.79%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 74.75%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 74.67%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 74.59%   [EVAL] batch:  151 | acc: 62.50%,  total acc: 74.51%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 74.26%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 74.19%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 73.91%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 73.68%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 73.61%   [EVAL] batch:  157 | acc: 43.75%,  total acc: 73.42%   [EVAL] batch:  158 | acc: 37.50%,  total acc: 73.19%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 73.01%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 72.90%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 72.69%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 72.55%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 72.48%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 72.46%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 72.44%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 72.42%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 72.40%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 72.41%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 72.24%   [EVAL] batch:  170 | acc: 50.00%,  total acc: 72.11%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 71.88%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 71.86%   [EVAL] batch:  173 | acc: 37.50%,  total acc: 71.66%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 71.54%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 71.86%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 72.33%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 72.63%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 72.86%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 72.97%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 73.08%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 73.20%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 73.27%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 73.18%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 73.19%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 73.17%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 73.11%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 73.06%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 72.91%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 72.80%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 72.93%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 72.94%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 72.98%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 72.77%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 72.72%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 72.61%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 72.56%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 72.48%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 72.55%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 73.17%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 73.27%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.52%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 74.32%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 74.41%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 74.53%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 74.56%   [EVAL] batch:  226 | acc: 37.50%,  total acc: 74.39%   [EVAL] batch:  227 | acc: 37.50%,  total acc: 74.23%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 74.02%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 73.78%   [EVAL] batch:  230 | acc: 50.00%,  total acc: 73.67%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 73.73%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 73.82%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 73.90%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 74.10%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 74.21%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 74.21%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 74.22%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 74.14%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 74.07%   [EVAL] batch:  241 | acc: 50.00%,  total acc: 73.97%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 73.89%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 73.74%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 73.73%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 73.61%   [EVAL] batch:  247 | acc: 87.50%,  total acc: 73.66%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 73.72%   [EVAL] batch:  249 | acc: 75.00%,  total acc: 73.72%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 73.91%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 73.99%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 74.04%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 74.12%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 74.15%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 74.18%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 74.16%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 74.16%   [EVAL] batch:  260 | acc: 37.50%,  total acc: 74.02%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 73.95%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 73.98%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 74.03%   [EVAL] batch:  264 | acc: 93.75%,  total acc: 74.10%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 74.13%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 74.16%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 74.14%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 74.12%   [EVAL] batch:  269 | acc: 75.00%,  total acc: 74.12%   [EVAL] batch:  270 | acc: 81.25%,  total acc: 74.15%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 74.13%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 74.11%   [EVAL] batch:  273 | acc: 68.75%,  total acc: 74.09%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 74.00%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 74.56%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 74.56%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 74.58%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 74.54%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 74.54%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 74.50%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 74.43%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 74.41%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 74.59%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 74.76%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 74.89%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 74.98%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 75.15%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 75.44%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 75.50%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 75.56%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 75.64%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 75.75%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 75.79%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 75.85%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.93%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 76.00%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 75.86%   [EVAL] batch:  314 | acc: 37.50%,  total acc: 75.73%   [EVAL] batch:  315 | acc: 37.50%,  total acc: 75.61%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 75.53%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 75.41%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 75.43%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 75.45%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 75.51%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 75.54%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 75.64%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 75.67%   [EVAL] batch:  325 | acc: 100.00%,  total acc: 75.75%   [EVAL] batch:  326 | acc: 87.50%,  total acc: 75.78%   [EVAL] batch:  327 | acc: 87.50%,  total acc: 75.82%   [EVAL] batch:  328 | acc: 87.50%,  total acc: 75.85%   [EVAL] batch:  329 | acc: 93.75%,  total acc: 75.91%   [EVAL] batch:  330 | acc: 93.75%,  total acc: 75.96%   [EVAL] batch:  331 | acc: 37.50%,  total acc: 75.85%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 75.79%   [EVAL] batch:  333 | acc: 37.50%,  total acc: 75.67%   [EVAL] batch:  334 | acc: 56.25%,  total acc: 75.62%   [EVAL] batch:  335 | acc: 43.75%,  total acc: 75.52%   [EVAL] batch:  336 | acc: 31.25%,  total acc: 75.39%   [EVAL] batch:  337 | acc: 25.00%,  total acc: 75.24%   [EVAL] batch:  338 | acc: 12.50%,  total acc: 75.06%   [EVAL] batch:  339 | acc: 25.00%,  total acc: 74.91%   [EVAL] batch:  340 | acc: 6.25%,  total acc: 74.71%   [EVAL] batch:  341 | acc: 25.00%,  total acc: 74.56%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 74.38%   [EVAL] batch:  343 | acc: 18.75%,  total acc: 74.22%   [EVAL] batch:  344 | acc: 87.50%,  total acc: 74.26%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 74.35%   [EVAL] batch:  347 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:  348 | acc: 87.50%,  total acc: 74.46%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 74.52%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 74.57%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:  352 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 74.74%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 74.79%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 74.86%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 74.88%   [EVAL] batch:  357 | acc: 81.25%,  total acc: 74.90%   [EVAL] batch:  358 | acc: 50.00%,  total acc: 74.83%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 74.86%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 74.84%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 74.88%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 74.83%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 74.73%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 74.71%   [EVAL] batch:  365 | acc: 75.00%,  total acc: 74.71%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 74.69%   [EVAL] batch:  367 | acc: 75.00%,  total acc: 74.69%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 74.64%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 74.58%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 74.51%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 74.46%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 74.43%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 74.43%   [EVAL] batch:  374 | acc: 68.75%,  total acc: 74.42%   
cur_acc:  ['0.9504', '0.7698', '0.8046', '0.7669', '0.8740', '0.6558']
his_acc:  ['0.9504', '0.8510', '0.8155', '0.7765', '0.7778', '0.7442']
CurrentTrain: epoch  0, batch     0 | loss: 6.1291714CurrentTrain: epoch  0, batch     1 | loss: 5.2251458CurrentTrain: epoch  0, batch     2 | loss: 6.6657181CurrentTrain: epoch  0, batch     3 | loss: 9.5378780CurrentTrain: epoch  1, batch     0 | loss: 4.6843834CurrentTrain: epoch  1, batch     1 | loss: 5.5530720CurrentTrain: epoch  1, batch     2 | loss: 5.4439650CurrentTrain: epoch  1, batch     3 | loss: 4.3882179CurrentTrain: epoch  2, batch     0 | loss: 5.0617018CurrentTrain: epoch  2, batch     1 | loss: 5.2402072CurrentTrain: epoch  2, batch     2 | loss: 4.2950602CurrentTrain: epoch  2, batch     3 | loss: 3.9612272CurrentTrain: epoch  3, batch     0 | loss: 5.0732069CurrentTrain: epoch  3, batch     1 | loss: 4.0392475CurrentTrain: epoch  3, batch     2 | loss: 4.2355666CurrentTrain: epoch  3, batch     3 | loss: 3.4520383CurrentTrain: epoch  4, batch     0 | loss: 4.4041095CurrentTrain: epoch  4, batch     1 | loss: 3.4145927CurrentTrain: epoch  4, batch     2 | loss: 4.6582685CurrentTrain: epoch  4, batch     3 | loss: 4.5322428CurrentTrain: epoch  5, batch     0 | loss: 4.1199021CurrentTrain: epoch  5, batch     1 | loss: 5.3975296CurrentTrain: epoch  5, batch     2 | loss: 2.9615862CurrentTrain: epoch  5, batch     3 | loss: 2.2862988CurrentTrain: epoch  6, batch     0 | loss: 3.7195563CurrentTrain: epoch  6, batch     1 | loss: 3.7753537CurrentTrain: epoch  6, batch     2 | loss: 3.8159938CurrentTrain: epoch  6, batch     3 | loss: 4.0734339CurrentTrain: epoch  7, batch     0 | loss: 4.4924178CurrentTrain: epoch  7, batch     1 | loss: 2.9297843CurrentTrain: epoch  7, batch     2 | loss: 3.3603036CurrentTrain: epoch  7, batch     3 | loss: 1.8586903CurrentTrain: epoch  8, batch     0 | loss: 3.6917677CurrentTrain: epoch  8, batch     1 | loss: 3.0402524CurrentTrain: epoch  8, batch     2 | loss: 3.9374456CurrentTrain: epoch  8, batch     3 | loss: 1.8540863CurrentTrain: epoch  9, batch     0 | loss: 3.3949122CurrentTrain: epoch  9, batch     1 | loss: 3.8868961CurrentTrain: epoch  9, batch     2 | loss: 2.5669613CurrentTrain: epoch  9, batch     3 | loss: 4.0163341
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the book "sapiens: a brief history of humankind" explores the evolution of human societies and cultures.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: human societies  

Relation: main subject  
Context: the documentary "our planet" showcases the beauty of nature and the impact of climate change on wildlife.  
Head Entity: our planet  
Tail Entity: nature  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: the two teams had met in two previous matches , including in the 2006 world cup group stage , won by brazil 1–0 .
Head Entity: 2006 world cup
Tail Entity: brazil
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: In the 2020 UEFA European Championship, Italy and England faced off in the final, with Italy emerging victorious after a tense penalty shootout.  
Head Entity: 2020 UEFA European Championship  
Tail Entity: Italy  

Relation: participating team  
Context: The 2018 FIFA World Cup saw France and Croatia compete in an exhilarating final, where France claimed their second title.  
Head Entity: 2018 FIFA World Cup  
Tail Entity: France  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states from invasions and raids, and is considered one of the most iconic structures in the world.  
Head Entity: Great Wall of China  
Tail Entity: fortification  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in the state of california, and it includes the famous yosemite national park, which is located within the range.  
Head Entity: sierra nevada  
Tail Entity: yosemite national park  

Relation: mountain range  
Context: the appalachian mountains ( ) extend from the canadian province of quebec down to alabama, making it one of the longest mountain ranges in north america.  
Head Entity: appalachian mountains  
Tail Entity: alabama  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: later that year he had a minor hit film with " landru " , written by françoise sagan and starring charles denner , michèle morgan , danielle darrieux and hildegard knef .
Head Entity: " landru "
Tail Entity: françoise sagan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The acclaimed film "Inception," directed by Christopher Nolan, features a complex narrative crafted by the talented screenwriter, Jonathan Nolan.  
Head Entity: "Inception"  
Tail Entity: Jonathan Nolan  

Relation: screenwriter  
Context: The beloved animated movie "Toy Story" was brought to life through the creative writing of the screenwriter, Joss Whedon, who infused humor and heart into the characters.  
Head Entity: "Toy Story"  
Tail Entity: Joss Whedon  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, focusing on advanced materials and nanotechnology.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as a major center for the roman catholic faith in paris.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the gelug school of tibetan buddhism, advocating for peace and compassion worldwide.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
Mixup data size:  498
MixupTrain:  epoch  0, batch     0 | loss: 2.4659334MixupTrain:  epoch  0, batch     1 | loss: 1.8351678MixupTrain:  epoch  0, batch     2 | loss: 2.5584269MixupTrain:  epoch  0, batch     3 | loss: 1.8787925MixupTrain:  epoch  0, batch     4 | loss: 1.8395945MixupTrain:  epoch  0, batch     5 | loss: 2.4887087MixupTrain:  epoch  0, batch     6 | loss: 2.1215618MixupTrain:  epoch  0, batch     7 | loss: 1.9465043MixupTrain:  epoch  0, batch     8 | loss: 1.9640381MixupTrain:  epoch  0, batch     9 | loss: 1.6360803MixupTrain:  epoch  0, batch    10 | loss: 1.7824060MixupTrain:  epoch  0, batch    11 | loss: 2.1884132MixupTrain:  epoch  0, batch    12 | loss: 2.2408073MixupTrain:  epoch  0, batch    13 | loss: 1.9851254MixupTrain:  epoch  0, batch    14 | loss: 1.9248551MixupTrain:  epoch  0, batch    15 | loss: 2.3989041MixupTrain:  epoch  0, batch    16 | loss: 2.0373405MixupTrain:  epoch  0, batch    17 | loss: 2.5042057MixupTrain:  epoch  0, batch    18 | loss: 2.3908542MixupTrain:  epoch  0, batch    19 | loss: 2.5714425MixupTrain:  epoch  0, batch    20 | loss: 2.0881615MixupTrain:  epoch  0, batch    21 | loss: 1.7302387MixupTrain:  epoch  0, batch    22 | loss: 1.8970262MixupTrain:  epoch  0, batch    23 | loss: 2.4203560MixupTrain:  epoch  0, batch    24 | loss: 2.5635197MixupTrain:  epoch  0, batch    25 | loss: 1.7059028MixupTrain:  epoch  0, batch    26 | loss: 1.9300584MixupTrain:  epoch  0, batch    27 | loss: 1.8123544MixupTrain:  epoch  0, batch    28 | loss: 1.9652149MixupTrain:  epoch  0, batch    29 | loss: 1.9446578MixupTrain:  epoch  0, batch    30 | loss: 1.6147881MixupTrain:  epoch  0, batch    31 | loss: 1.7245978
MemoryTrain:  epoch  0, batch     0 | loss: 2.3758712MemoryTrain:  epoch  0, batch     1 | loss: 2.0528328MemoryTrain:  epoch  0, batch     2 | loss: 2.3617523MemoryTrain:  epoch  0, batch     3 | loss: 2.4749053MemoryTrain:  epoch  0, batch     4 | loss: 1.8529245MemoryTrain:  epoch  0, batch     5 | loss: 1.8405771MemoryTrain:  epoch  0, batch     6 | loss: 1.8295803MemoryTrain:  epoch  0, batch     7 | loss: 2.1602592MemoryTrain:  epoch  0, batch     8 | loss: 2.1634502MemoryTrain:  epoch  0, batch     9 | loss: 2.5398536MemoryTrain:  epoch  0, batch    10 | loss: 1.8958051MemoryTrain:  epoch  0, batch    11 | loss: 2.4807181MemoryTrain:  epoch  0, batch    12 | loss: 1.9353383MemoryTrain:  epoch  0, batch    13 | loss: 2.3987598MemoryTrain:  epoch  1, batch     0 | loss: 1.9853699MemoryTrain:  epoch  1, batch     1 | loss: 1.9737450MemoryTrain:  epoch  1, batch     2 | loss: 1.8449132MemoryTrain:  epoch  1, batch     3 | loss: 2.2510233MemoryTrain:  epoch  1, batch     4 | loss: 1.6197093MemoryTrain:  epoch  1, batch     5 | loss: 2.1537755MemoryTrain:  epoch  1, batch     6 | loss: 2.3700333MemoryTrain:  epoch  1, batch     7 | loss: 1.4126018MemoryTrain:  epoch  1, batch     8 | loss: 2.1406596MemoryTrain:  epoch  1, batch     9 | loss: 2.0313978MemoryTrain:  epoch  1, batch    10 | loss: 1.7507010MemoryTrain:  epoch  1, batch    11 | loss: 1.4743795MemoryTrain:  epoch  1, batch    12 | loss: 1.9635406MemoryTrain:  epoch  1, batch    13 | loss: 1.5912033MemoryTrain:  epoch  2, batch     0 | loss: 2.1615071MemoryTrain:  epoch  2, batch     1 | loss: 1.5499160MemoryTrain:  epoch  2, batch     2 | loss: 1.7823648MemoryTrain:  epoch  2, batch     3 | loss: 1.7180204MemoryTrain:  epoch  2, batch     4 | loss: 1.4281588MemoryTrain:  epoch  2, batch     5 | loss: 1.5511127MemoryTrain:  epoch  2, batch     6 | loss: 2.0219288MemoryTrain:  epoch  2, batch     7 | loss: 1.6540004MemoryTrain:  epoch  2, batch     8 | loss: 1.2732222MemoryTrain:  epoch  2, batch     9 | loss: 1.8741221MemoryTrain:  epoch  2, batch    10 | loss: 1.5563138MemoryTrain:  epoch  2, batch    11 | loss: 1.3976414MemoryTrain:  epoch  2, batch    12 | loss: 1.7643481MemoryTrain:  epoch  2, batch    13 | loss: 1.1478496MemoryTrain:  epoch  3, batch     0 | loss: 1.6025554MemoryTrain:  epoch  3, batch     1 | loss: 1.3464196MemoryTrain:  epoch  3, batch     2 | loss: 1.8833405MemoryTrain:  epoch  3, batch     3 | loss: 1.4697075MemoryTrain:  epoch  3, batch     4 | loss: 1.5592577MemoryTrain:  epoch  3, batch     5 | loss: 1.2367101MemoryTrain:  epoch  3, batch     6 | loss: 1.4365320MemoryTrain:  epoch  3, batch     7 | loss: 2.0468645MemoryTrain:  epoch  3, batch     8 | loss: 1.5593613MemoryTrain:  epoch  3, batch     9 | loss: 1.2471433MemoryTrain:  epoch  3, batch    10 | loss: 1.5541929MemoryTrain:  epoch  3, batch    11 | loss: 1.8166652MemoryTrain:  epoch  3, batch    12 | loss: 1.4628536MemoryTrain:  epoch  3, batch    13 | loss: 1.1976919MemoryTrain:  epoch  4, batch     0 | loss: 1.3051738MemoryTrain:  epoch  4, batch     1 | loss: 1.3739676MemoryTrain:  epoch  4, batch     2 | loss: 1.6356030MemoryTrain:  epoch  4, batch     3 | loss: 1.8352907MemoryTrain:  epoch  4, batch     4 | loss: 1.4035007MemoryTrain:  epoch  4, batch     5 | loss: 1.4780102MemoryTrain:  epoch  4, batch     6 | loss: 1.2609237MemoryTrain:  epoch  4, batch     7 | loss: 1.2145886MemoryTrain:  epoch  4, batch     8 | loss: 1.6241617MemoryTrain:  epoch  4, batch     9 | loss: 1.2757132MemoryTrain:  epoch  4, batch    10 | loss: 1.8575716MemoryTrain:  epoch  4, batch    11 | loss: 1.5464180MemoryTrain:  epoch  4, batch    12 | loss: 1.4285176MemoryTrain:  epoch  4, batch    13 | loss: 1.2076627MemoryTrain:  epoch  5, batch     0 | loss: 1.2753614MemoryTrain:  epoch  5, batch     1 | loss: 1.3541210MemoryTrain:  epoch  5, batch     2 | loss: 1.2847158MemoryTrain:  epoch  5, batch     3 | loss: 1.4500350MemoryTrain:  epoch  5, batch     4 | loss: 1.5875022MemoryTrain:  epoch  5, batch     5 | loss: 1.4970903MemoryTrain:  epoch  5, batch     6 | loss: 1.3092651MemoryTrain:  epoch  5, batch     7 | loss: 1.6171484MemoryTrain:  epoch  5, batch     8 | loss: 1.2703978MemoryTrain:  epoch  5, batch     9 | loss: 1.4935727MemoryTrain:  epoch  5, batch    10 | loss: 1.2543309MemoryTrain:  epoch  5, batch    11 | loss: 1.3838980MemoryTrain:  epoch  5, batch    12 | loss: 1.3244414MemoryTrain:  epoch  5, batch    13 | loss: 1.3482476MemoryTrain:  epoch  6, batch     0 | loss: 1.3055017MemoryTrain:  epoch  6, batch     1 | loss: 1.3840744MemoryTrain:  epoch  6, batch     2 | loss: 1.5208833MemoryTrain:  epoch  6, batch     3 | loss: 1.2899581MemoryTrain:  epoch  6, batch     4 | loss: 1.4270718MemoryTrain:  epoch  6, batch     5 | loss: 1.4202996MemoryTrain:  epoch  6, batch     6 | loss: 1.3197663MemoryTrain:  epoch  6, batch     7 | loss: 1.3894129MemoryTrain:  epoch  6, batch     8 | loss: 1.2795730MemoryTrain:  epoch  6, batch     9 | loss: 1.2704632MemoryTrain:  epoch  6, batch    10 | loss: 1.2793831MemoryTrain:  epoch  6, batch    11 | loss: 1.2986965MemoryTrain:  epoch  6, batch    12 | loss: 1.2290267MemoryTrain:  epoch  6, batch    13 | loss: 1.1834335MemoryTrain:  epoch  7, batch     0 | loss: 1.2128074MemoryTrain:  epoch  7, batch     1 | loss: 1.2018628MemoryTrain:  epoch  7, batch     2 | loss: 1.2735931MemoryTrain:  epoch  7, batch     3 | loss: 1.2653010MemoryTrain:  epoch  7, batch     4 | loss: 1.4673507MemoryTrain:  epoch  7, batch     5 | loss: 1.4969765MemoryTrain:  epoch  7, batch     6 | loss: 1.3765997MemoryTrain:  epoch  7, batch     7 | loss: 1.3394264MemoryTrain:  epoch  7, batch     8 | loss: 1.3100420MemoryTrain:  epoch  7, batch     9 | loss: 1.2470007MemoryTrain:  epoch  7, batch    10 | loss: 1.2938234MemoryTrain:  epoch  7, batch    11 | loss: 1.3637875MemoryTrain:  epoch  7, batch    12 | loss: 1.3387010MemoryTrain:  epoch  7, batch    13 | loss: 1.3248925MemoryTrain:  epoch  8, batch     0 | loss: 1.2893656MemoryTrain:  epoch  8, batch     1 | loss: 1.3090442MemoryTrain:  epoch  8, batch     2 | loss: 1.3587441MemoryTrain:  epoch  8, batch     3 | loss: 1.4456657MemoryTrain:  epoch  8, batch     4 | loss: 1.3684809MemoryTrain:  epoch  8, batch     5 | loss: 1.2280490MemoryTrain:  epoch  8, batch     6 | loss: 1.2848278MemoryTrain:  epoch  8, batch     7 | loss: 1.2937502MemoryTrain:  epoch  8, batch     8 | loss: 1.2571608MemoryTrain:  epoch  8, batch     9 | loss: 1.2319176MemoryTrain:  epoch  8, batch    10 | loss: 1.2243767MemoryTrain:  epoch  8, batch    11 | loss: 1.2737294MemoryTrain:  epoch  8, batch    12 | loss: 1.2389227MemoryTrain:  epoch  8, batch    13 | loss: 1.1618847MemoryTrain:  epoch  9, batch     0 | loss: 1.2763121MemoryTrain:  epoch  9, batch     1 | loss: 1.3986338MemoryTrain:  epoch  9, batch     2 | loss: 1.2295966MemoryTrain:  epoch  9, batch     3 | loss: 1.1812773MemoryTrain:  epoch  9, batch     4 | loss: 1.1856351MemoryTrain:  epoch  9, batch     5 | loss: 1.3578187MemoryTrain:  epoch  9, batch     6 | loss: 1.2849361MemoryTrain:  epoch  9, batch     7 | loss: 1.2737647MemoryTrain:  epoch  9, batch     8 | loss: 1.3115635MemoryTrain:  epoch  9, batch     9 | loss: 1.2543186MemoryTrain:  epoch  9, batch    10 | loss: 1.3202143MemoryTrain:  epoch  9, batch    11 | loss: 1.2396785MemoryTrain:  epoch  9, batch    12 | loss: 1.2522882MemoryTrain:  epoch  9, batch    13 | loss: 1.3217592
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 70.70%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 65.13%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 70.37%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 69.64%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 69.61%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 68.54%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 67.34%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 67.58%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 68.37%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 70.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 71.71%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 71.15%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 71.09%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 70.73%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 70.09%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 69.77%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 70.03%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 69.72%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 69.97%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 69.68%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 69.40%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 69.39%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 69.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 69.24%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 69.35%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 69.58%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 70.20%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 70.39%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 70.58%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 70.97%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 70.94%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 71.31%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 71.27%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 70.83%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 65.87%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 72.79%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 77.01%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.03%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 80.11%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 79.78%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 80.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 80.74%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.47%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.14%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 83.10%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 82.64%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 82.34%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 81.52%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 80.99%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 80.87%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 80.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 80.15%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 80.05%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.31%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 79.98%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 79.55%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 79.28%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 78.66%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 78.18%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 78.07%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 77.82%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 77.68%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 77.54%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 77.31%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 77.37%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 77.33%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 77.02%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 76.90%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 76.88%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 76.85%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 76.48%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 76.46%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 76.44%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 76.50%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 76.40%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 76.22%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 75.96%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 75.87%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 75.70%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 75.69%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 75.53%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 75.08%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 74.48%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 74.12%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 73.62%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 73.42%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 73.37%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 73.60%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 73.63%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 73.78%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 73.72%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 73.87%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 74.41%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 75.44%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.80%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 75.73%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 75.78%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 75.95%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 76.00%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 75.58%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 75.17%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 74.77%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 74.49%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 74.22%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 74.17%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 74.29%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.51%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 74.52%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 74.63%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 74.74%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 74.74%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 74.11%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 73.76%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 73.21%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 72.61%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 72.08%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 71.55%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 71.58%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 71.36%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 71.19%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 70.88%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 70.77%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 70.75%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 70.88%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 71.10%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 71.22%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 71.34%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 71.83%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 71.72%   [EVAL] batch:  139 | acc: 68.75%,  total acc: 71.70%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 71.76%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 71.85%   [EVAL] batch:  143 | acc: 81.25%,  total acc: 71.92%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 71.77%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 71.79%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 71.64%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 71.58%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 71.56%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 71.46%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 71.23%   [EVAL] batch:  151 | acc: 50.00%,  total acc: 71.09%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 70.75%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 70.58%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 70.20%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 69.91%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 69.82%   [EVAL] batch:  157 | acc: 43.75%,  total acc: 69.66%   [EVAL] batch:  158 | acc: 37.50%,  total acc: 69.46%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 69.30%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 69.22%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 69.02%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 68.90%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 68.86%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 68.83%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 68.79%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 68.79%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 68.79%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 68.64%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 68.49%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 68.24%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 68.21%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 67.96%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 67.79%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 68.68%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 69.02%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 69.19%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 69.22%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 69.41%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 69.38%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 69.44%   [EVAL] batch:  190 | acc: 56.25%,  total acc: 69.37%   [EVAL] batch:  191 | acc: 68.75%,  total acc: 69.37%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 69.37%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 69.20%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 69.20%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 69.10%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 69.13%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 69.19%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 69.25%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 69.25%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 69.25%   [EVAL] batch:  201 | acc: 31.25%,  total acc: 69.06%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 69.00%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 68.87%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 68.84%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 68.78%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 68.81%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.11%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 69.23%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 69.37%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 69.42%   [EVAL] batch:  213 | acc: 31.25%,  total acc: 69.25%   [EVAL] batch:  214 | acc: 31.25%,  total acc: 69.07%   [EVAL] batch:  215 | acc: 25.00%,  total acc: 68.87%   [EVAL] batch:  216 | acc: 37.50%,  total acc: 68.72%   [EVAL] batch:  217 | acc: 43.75%,  total acc: 68.61%   [EVAL] batch:  218 | acc: 43.75%,  total acc: 68.49%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 69.25%   [EVAL] batch:  226 | acc: 37.50%,  total acc: 69.11%   [EVAL] batch:  227 | acc: 37.50%,  total acc: 68.97%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 68.78%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 68.59%   [EVAL] batch:  230 | acc: 50.00%,  total acc: 68.51%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 68.62%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 68.72%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 68.99%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 69.07%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 69.09%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 69.09%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 68.98%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 68.93%   [EVAL] batch:  241 | acc: 62.50%,  total acc: 68.90%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 68.85%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:  244 | acc: 43.75%,  total acc: 68.65%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 68.60%   [EVAL] batch:  246 | acc: 43.75%,  total acc: 68.50%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 68.52%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 68.60%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 68.95%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 69.04%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 69.14%   [EVAL] batch:  257 | acc: 56.25%,  total acc: 69.09%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 69.09%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 69.11%   [EVAL] batch:  260 | acc: 31.25%,  total acc: 68.97%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 68.92%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 68.92%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 68.99%   [EVAL] batch:  264 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 69.10%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 69.12%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 69.12%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 69.12%   [EVAL] batch:  269 | acc: 75.00%,  total acc: 69.14%   [EVAL] batch:  270 | acc: 75.00%,  total acc: 69.17%   [EVAL] batch:  271 | acc: 62.50%,  total acc: 69.14%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 69.14%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 69.16%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 69.09%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 69.77%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 69.81%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 69.78%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 69.80%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 69.78%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 69.73%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 69.73%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 70.12%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 70.28%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 70.58%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 70.68%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 70.95%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 71.03%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 71.10%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 71.23%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 71.30%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 71.38%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 71.50%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 71.71%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 71.65%   [EVAL] batch:  313 | acc: 0.00%,  total acc: 71.42%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 71.21%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 70.98%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 70.76%   [EVAL] batch:  317 | acc: 0.00%,  total acc: 70.54%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 70.43%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 70.45%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 70.50%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 70.56%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 70.66%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 70.71%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 70.72%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 70.68%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 70.67%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 70.67%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:  330 | acc: 50.00%,  total acc: 70.56%   [EVAL] batch:  331 | acc: 12.50%,  total acc: 70.39%   [EVAL] batch:  332 | acc: 56.25%,  total acc: 70.35%   [EVAL] batch:  333 | acc: 43.75%,  total acc: 70.27%   [EVAL] batch:  334 | acc: 31.25%,  total acc: 70.15%   [EVAL] batch:  335 | acc: 37.50%,  total acc: 70.05%   [EVAL] batch:  336 | acc: 25.00%,  total acc: 69.92%   [EVAL] batch:  337 | acc: 12.50%,  total acc: 69.75%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 69.56%   [EVAL] batch:  339 | acc: 31.25%,  total acc: 69.45%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 69.24%   [EVAL] batch:  341 | acc: 18.75%,  total acc: 69.10%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 68.91%   [EVAL] batch:  343 | acc: 6.25%,  total acc: 68.73%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 68.77%   [EVAL] batch:  345 | acc: 81.25%,  total acc: 68.80%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 68.86%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.93%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 68.96%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 68.95%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 69.02%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 69.09%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 69.14%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 69.19%   [EVAL] batch:  354 | acc: 93.75%,  total acc: 69.26%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:  357 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:  358 | acc: 62.50%,  total acc: 69.36%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 69.43%   [EVAL] batch:  360 | acc: 75.00%,  total acc: 69.44%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 69.49%   [EVAL] batch:  362 | acc: 31.25%,  total acc: 69.39%   [EVAL] batch:  363 | acc: 37.50%,  total acc: 69.30%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 69.26%   [EVAL] batch:  365 | acc: 68.75%,  total acc: 69.26%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 69.19%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 69.17%   [EVAL] batch:  368 | acc: 56.25%,  total acc: 69.14%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 69.09%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 69.05%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 69.00%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 68.98%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 69.02%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 69.00%   [EVAL] batch:  375 | acc: 56.25%,  total acc: 68.97%   [EVAL] batch:  376 | acc: 43.75%,  total acc: 68.90%   [EVAL] batch:  377 | acc: 62.50%,  total acc: 68.88%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 68.93%   [EVAL] batch:  379 | acc: 87.50%,  total acc: 68.98%   [EVAL] batch:  380 | acc: 81.25%,  total acc: 69.01%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 69.16%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 69.22%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 69.33%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  387 | acc: 68.75%,  total acc: 69.41%   [EVAL] batch:  388 | acc: 25.00%,  total acc: 69.30%   [EVAL] batch:  389 | acc: 31.25%,  total acc: 69.20%   [EVAL] batch:  390 | acc: 18.75%,  total acc: 69.07%   [EVAL] batch:  391 | acc: 37.50%,  total acc: 68.99%   [EVAL] batch:  392 | acc: 31.25%,  total acc: 68.89%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 68.81%   [EVAL] batch:  394 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 69.11%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  400 | acc: 56.25%,  total acc: 69.23%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 69.09%   [EVAL] batch:  402 | acc: 50.00%,  total acc: 69.04%   [EVAL] batch:  403 | acc: 68.75%,  total acc: 69.04%   [EVAL] batch:  404 | acc: 37.50%,  total acc: 68.97%   [EVAL] batch:  405 | acc: 31.25%,  total acc: 68.87%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 68.89%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  408 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 69.09%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 69.13%   [EVAL] batch:  411 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 69.25%   [EVAL] batch:  413 | acc: 50.00%,  total acc: 69.20%   [EVAL] batch:  414 | acc: 68.75%,  total acc: 69.20%   [EVAL] batch:  415 | acc: 56.25%,  total acc: 69.17%   [EVAL] batch:  416 | acc: 43.75%,  total acc: 69.11%   [EVAL] batch:  417 | acc: 56.25%,  total acc: 69.08%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 69.11%   [EVAL] batch:  419 | acc: 56.25%,  total acc: 69.08%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 69.11%   [EVAL] batch:  421 | acc: 56.25%,  total acc: 69.08%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 69.05%   [EVAL] batch:  423 | acc: 68.75%,  total acc: 69.04%   [EVAL] batch:  424 | acc: 56.25%,  total acc: 69.01%   [EVAL] batch:  425 | acc: 75.00%,  total acc: 69.03%   [EVAL] batch:  426 | acc: 75.00%,  total acc: 69.04%   [EVAL] batch:  427 | acc: 81.25%,  total acc: 69.07%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 69.10%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 69.13%   [EVAL] batch:  430 | acc: 81.25%,  total acc: 69.16%   [EVAL] batch:  431 | acc: 81.25%,  total acc: 69.18%   [EVAL] batch:  432 | acc: 81.25%,  total acc: 69.21%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 69.27%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 69.32%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 69.26%   
cur_acc:  ['0.9504', '0.7698', '0.8046', '0.7669', '0.8740', '0.6558', '0.7083']
his_acc:  ['0.9504', '0.8510', '0.8155', '0.7765', '0.7778', '0.7442', '0.6926']
CurrentTrain: epoch  0, batch     0 | loss: 4.8858981CurrentTrain: epoch  0, batch     1 | loss: 4.9600992CurrentTrain: epoch  0, batch     2 | loss: 6.2054434CurrentTrain: epoch  0, batch     3 | loss: 6.7249064CurrentTrain: epoch  1, batch     0 | loss: 4.5831251CurrentTrain: epoch  1, batch     1 | loss: 3.9127903CurrentTrain: epoch  1, batch     2 | loss: 4.3690581CurrentTrain: epoch  1, batch     3 | loss: 3.7623289CurrentTrain: epoch  2, batch     0 | loss: 4.0475316CurrentTrain: epoch  2, batch     1 | loss: 3.2372680CurrentTrain: epoch  2, batch     2 | loss: 4.0430574CurrentTrain: epoch  2, batch     3 | loss: 2.2313440CurrentTrain: epoch  3, batch     0 | loss: 3.3765373CurrentTrain: epoch  3, batch     1 | loss: 3.4222877CurrentTrain: epoch  3, batch     2 | loss: 3.2395260CurrentTrain: epoch  3, batch     3 | loss: 4.6293449CurrentTrain: epoch  4, batch     0 | loss: 3.9325478CurrentTrain: epoch  4, batch     1 | loss: 2.7150209CurrentTrain: epoch  4, batch     2 | loss: 2.7084222CurrentTrain: epoch  4, batch     3 | loss: 3.2122474CurrentTrain: epoch  5, batch     0 | loss: 3.0403957CurrentTrain: epoch  5, batch     1 | loss: 2.8435504CurrentTrain: epoch  5, batch     2 | loss: 2.6255641CurrentTrain: epoch  5, batch     3 | loss: 2.4162910CurrentTrain: epoch  6, batch     0 | loss: 3.0094290CurrentTrain: epoch  6, batch     1 | loss: 2.2936256CurrentTrain: epoch  6, batch     2 | loss: 2.7408428CurrentTrain: epoch  6, batch     3 | loss: 2.7620857CurrentTrain: epoch  7, batch     0 | loss: 2.8751068CurrentTrain: epoch  7, batch     1 | loss: 2.2207375CurrentTrain: epoch  7, batch     2 | loss: 2.4043536CurrentTrain: epoch  7, batch     3 | loss: 2.4544785CurrentTrain: epoch  8, batch     0 | loss: 2.3598707CurrentTrain: epoch  8, batch     1 | loss: 2.3309114CurrentTrain: epoch  8, batch     2 | loss: 2.3299983CurrentTrain: epoch  8, batch     3 | loss: 2.2969911CurrentTrain: epoch  9, batch     0 | loss: 2.3623188CurrentTrain: epoch  9, batch     1 | loss: 2.2536998CurrentTrain: epoch  9, batch     2 | loss: 2.3209105CurrentTrain: epoch  9, batch     3 | loss: 2.0347018
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the sutagao are the chibcha - speaking indios sutagaos indigenous people from the region of fusagasugá , bogotá savanna , cundinamarca , colombia .
Head Entity: colombia
Tail Entity: cundinamarca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The state of California is known for its diverse geography and is divided into several counties, including Los Angeles County, which is the most populous county in the United States.  
Head Entity: California  
Tail Entity: Los Angeles County  

Relation: contains administrative territorial entity  
Context: The country of Japan is made up of several prefectures, with Tokyo being the most famous and densely populated among them.  
Head Entity: Japan  
Tail Entity: Tokyo  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: heinz , is a german given name , a diminutive of heinrich and cognate of the given name henry .
Head Entity: heinrich
Tail Entity: henry
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although experts argue there are key differences.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Documentary prize, highlighting its impactful storytelling.  
Head Entity: Best Documentary prize  
Tail Entity: "Voices of Change"  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a prominent figure in the royal navy, known for his rank of vice admiral during the napoleonic wars, particularly at the battle of trafalgar.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: she went on to the film school at san francisco state university , when she was hired as an artist for an atari game called " electrocop " .
Head Entity: electrocop
Tail Entity: atari
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The renowned author released her latest novel through a well-known publishing house that has been in the industry for decades.  
Head Entity: latest novel  
Tail Entity: publishing house  

Relation: publisher  
Context: After years of hard work, the independent game developer finally secured a deal with a major publisher to distribute their new game worldwide.  
Head Entity: new game  
Tail Entity: major publisher  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: the headquarters of the company is situated in san francisco, where it has been operating since 2010.  
Head Entity: the company  
Tail Entity: san francisco  

Relation: work location  
Context: during her time at the university, she conducted research in boston, which greatly influenced her career.  
Head Entity: she  
Tail Entity: boston  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: Maria Gonzalez is a renowned chef known for her innovative culinary techniques and delicious recipes.  
Head Entity: Maria Gonzalez  
Tail Entity: chef  

Relation: occupation  
Context: Dr. James Smith has dedicated his life to research in the field of astrophysics, contributing significantly to our understanding of black holes.  
Head Entity: Dr. James Smith  
Tail Entity: astrophysicist  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting millions of tourists each year.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti (october 12, 1935 – september 6, 2007) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey (born march 27, 1969) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: soprano  
Mixup data size:  558
MixupTrain:  epoch  0, batch     0 | loss: 2.1879182MixupTrain:  epoch  0, batch     1 | loss: 1.9809330MixupTrain:  epoch  0, batch     2 | loss: 1.6993288MixupTrain:  epoch  0, batch     3 | loss: 1.7712332MixupTrain:  epoch  0, batch     4 | loss: 1.9968726MixupTrain:  epoch  0, batch     5 | loss: 1.5618050MixupTrain:  epoch  0, batch     6 | loss: 1.8343657MixupTrain:  epoch  0, batch     7 | loss: 1.7684349MixupTrain:  epoch  0, batch     8 | loss: 1.6486344MixupTrain:  epoch  0, batch     9 | loss: 1.8087276MixupTrain:  epoch  0, batch    10 | loss: 1.8721603MixupTrain:  epoch  0, batch    11 | loss: 1.6728349MixupTrain:  epoch  0, batch    12 | loss: 1.9169180MixupTrain:  epoch  0, batch    13 | loss: 1.8535908MixupTrain:  epoch  0, batch    14 | loss: 1.6214015MixupTrain:  epoch  0, batch    15 | loss: 1.9572029MixupTrain:  epoch  0, batch    16 | loss: 1.7466744MixupTrain:  epoch  0, batch    17 | loss: 1.7988561MixupTrain:  epoch  0, batch    18 | loss: 1.7390916MixupTrain:  epoch  0, batch    19 | loss: 1.9597290MixupTrain:  epoch  0, batch    20 | loss: 1.9128000MixupTrain:  epoch  0, batch    21 | loss: 1.6868450MixupTrain:  epoch  0, batch    22 | loss: 1.8764324MixupTrain:  epoch  0, batch    23 | loss: 1.9785338MixupTrain:  epoch  0, batch    24 | loss: 1.7957031MixupTrain:  epoch  0, batch    25 | loss: 1.7017391MixupTrain:  epoch  0, batch    26 | loss: 1.5680854MixupTrain:  epoch  0, batch    27 | loss: 1.8194723MixupTrain:  epoch  0, batch    28 | loss: 1.5425868MixupTrain:  epoch  0, batch    29 | loss: 1.8406584MixupTrain:  epoch  0, batch    30 | loss: 1.4652182MixupTrain:  epoch  0, batch    31 | loss: 1.7040313MixupTrain:  epoch  0, batch    32 | loss: 2.0192320MixupTrain:  epoch  0, batch    33 | loss: 2.0294889MixupTrain:  epoch  0, batch    34 | loss: 1.7821898
MemoryTrain:  epoch  0, batch     0 | loss: 1.4943517MemoryTrain:  epoch  0, batch     1 | loss: 2.0922468MemoryTrain:  epoch  0, batch     2 | loss: 1.3968046MemoryTrain:  epoch  0, batch     3 | loss: 1.4733450MemoryTrain:  epoch  0, batch     4 | loss: 1.6844683MemoryTrain:  epoch  0, batch     5 | loss: 2.0920720MemoryTrain:  epoch  0, batch     6 | loss: 1.4635526MemoryTrain:  epoch  0, batch     7 | loss: 1.6845407MemoryTrain:  epoch  0, batch     8 | loss: 1.5271101MemoryTrain:  epoch  0, batch     9 | loss: 2.0037353MemoryTrain:  epoch  0, batch    10 | loss: 2.1404285MemoryTrain:  epoch  0, batch    11 | loss: 1.7239712MemoryTrain:  epoch  0, batch    12 | loss: 2.2141645MemoryTrain:  epoch  0, batch    13 | loss: 2.0363202MemoryTrain:  epoch  0, batch    14 | loss: 2.4323621MemoryTrain:  epoch  1, batch     0 | loss: 1.3888705MemoryTrain:  epoch  1, batch     1 | loss: 1.4976254MemoryTrain:  epoch  1, batch     2 | loss: 1.7195964MemoryTrain:  epoch  1, batch     3 | loss: 1.6768742MemoryTrain:  epoch  1, batch     4 | loss: 1.4928700MemoryTrain:  epoch  1, batch     5 | loss: 2.3516822MemoryTrain:  epoch  1, batch     6 | loss: 1.4318578MemoryTrain:  epoch  1, batch     7 | loss: 2.4830976MemoryTrain:  epoch  1, batch     8 | loss: 1.4164183MemoryTrain:  epoch  1, batch     9 | loss: 1.7842093MemoryTrain:  epoch  1, batch    10 | loss: 1.4161520MemoryTrain:  epoch  1, batch    11 | loss: 1.3310330MemoryTrain:  epoch  1, batch    12 | loss: 1.5300442MemoryTrain:  epoch  1, batch    13 | loss: 1.5823160MemoryTrain:  epoch  1, batch    14 | loss: 1.7253685MemoryTrain:  epoch  2, batch     0 | loss: 1.3844410MemoryTrain:  epoch  2, batch     1 | loss: 1.2693560MemoryTrain:  epoch  2, batch     2 | loss: 1.4677184MemoryTrain:  epoch  2, batch     3 | loss: 1.4391605MemoryTrain:  epoch  2, batch     4 | loss: 1.5965351MemoryTrain:  epoch  2, batch     5 | loss: 1.6657200MemoryTrain:  epoch  2, batch     6 | loss: 1.4794145MemoryTrain:  epoch  2, batch     7 | loss: 1.9968128MemoryTrain:  epoch  2, batch     8 | loss: 1.4008412MemoryTrain:  epoch  2, batch     9 | loss: 1.6475617MemoryTrain:  epoch  2, batch    10 | loss: 1.3801548MemoryTrain:  epoch  2, batch    11 | loss: 1.4780333MemoryTrain:  epoch  2, batch    12 | loss: 1.3340867MemoryTrain:  epoch  2, batch    13 | loss: 1.3356552MemoryTrain:  epoch  2, batch    14 | loss: 1.5278246MemoryTrain:  epoch  3, batch     0 | loss: 1.4584899MemoryTrain:  epoch  3, batch     1 | loss: 1.3310969MemoryTrain:  epoch  3, batch     2 | loss: 1.3594706MemoryTrain:  epoch  3, batch     3 | loss: 1.5290158MemoryTrain:  epoch  3, batch     4 | loss: 1.2117629MemoryTrain:  epoch  3, batch     5 | loss: 1.5338157MemoryTrain:  epoch  3, batch     6 | loss: 1.6177907MemoryTrain:  epoch  3, batch     7 | loss: 1.4296447MemoryTrain:  epoch  3, batch     8 | loss: 1.3665421MemoryTrain:  epoch  3, batch     9 | loss: 1.5054370MemoryTrain:  epoch  3, batch    10 | loss: 1.3966594MemoryTrain:  epoch  3, batch    11 | loss: 1.2930698MemoryTrain:  epoch  3, batch    12 | loss: 1.2094213MemoryTrain:  epoch  3, batch    13 | loss: 1.3730898MemoryTrain:  epoch  3, batch    14 | loss: 1.4256802MemoryTrain:  epoch  4, batch     0 | loss: 1.3741846MemoryTrain:  epoch  4, batch     1 | loss: 1.2200123MemoryTrain:  epoch  4, batch     2 | loss: 1.5193309MemoryTrain:  epoch  4, batch     3 | loss: 1.3532130MemoryTrain:  epoch  4, batch     4 | loss: 1.4840511MemoryTrain:  epoch  4, batch     5 | loss: 1.2285112MemoryTrain:  epoch  4, batch     6 | loss: 1.4266837MemoryTrain:  epoch  4, batch     7 | loss: 1.3670881MemoryTrain:  epoch  4, batch     8 | loss: 1.2309816MemoryTrain:  epoch  4, batch     9 | loss: 1.2586544MemoryTrain:  epoch  4, batch    10 | loss: 1.3736410MemoryTrain:  epoch  4, batch    11 | loss: 1.3116633MemoryTrain:  epoch  4, batch    12 | loss: 1.4713657MemoryTrain:  epoch  4, batch    13 | loss: 1.2376474MemoryTrain:  epoch  4, batch    14 | loss: 1.3705207MemoryTrain:  epoch  5, batch     0 | loss: 1.3282744MemoryTrain:  epoch  5, batch     1 | loss: 1.3060472MemoryTrain:  epoch  5, batch     2 | loss: 1.2812417MemoryTrain:  epoch  5, batch     3 | loss: 1.2163591MemoryTrain:  epoch  5, batch     4 | loss: 1.3058727MemoryTrain:  epoch  5, batch     5 | loss: 1.3841281MemoryTrain:  epoch  5, batch     6 | loss: 1.3838779MemoryTrain:  epoch  5, batch     7 | loss: 1.2566776MemoryTrain:  epoch  5, batch     8 | loss: 1.2738134MemoryTrain:  epoch  5, batch     9 | loss: 1.3100258MemoryTrain:  epoch  5, batch    10 | loss: 1.3569620MemoryTrain:  epoch  5, batch    11 | loss: 1.2089567MemoryTrain:  epoch  5, batch    12 | loss: 1.2932538MemoryTrain:  epoch  5, batch    13 | loss: 1.2151289MemoryTrain:  epoch  5, batch    14 | loss: 1.2404971MemoryTrain:  epoch  6, batch     0 | loss: 1.2749343MemoryTrain:  epoch  6, batch     1 | loss: 1.2377429MemoryTrain:  epoch  6, batch     2 | loss: 1.3975898MemoryTrain:  epoch  6, batch     3 | loss: 1.2250056MemoryTrain:  epoch  6, batch     4 | loss: 1.2316798MemoryTrain:  epoch  6, batch     5 | loss: 1.2605360MemoryTrain:  epoch  6, batch     6 | loss: 1.4616215MemoryTrain:  epoch  6, batch     7 | loss: 1.2788752MemoryTrain:  epoch  6, batch     8 | loss: 1.2688525MemoryTrain:  epoch  6, batch     9 | loss: 1.4363556MemoryTrain:  epoch  6, batch    10 | loss: 1.2063353MemoryTrain:  epoch  6, batch    11 | loss: 1.3123840MemoryTrain:  epoch  6, batch    12 | loss: 1.3458261MemoryTrain:  epoch  6, batch    13 | loss: 1.2076106MemoryTrain:  epoch  6, batch    14 | loss: 1.3015872MemoryTrain:  epoch  7, batch     0 | loss: 1.4469118MemoryTrain:  epoch  7, batch     1 | loss: 1.2380184MemoryTrain:  epoch  7, batch     2 | loss: 1.2476543MemoryTrain:  epoch  7, batch     3 | loss: 1.2518260MemoryTrain:  epoch  7, batch     4 | loss: 1.2436852MemoryTrain:  epoch  7, batch     5 | loss: 1.1869774MemoryTrain:  epoch  7, batch     6 | loss: 1.2539750MemoryTrain:  epoch  7, batch     7 | loss: 1.2497524MemoryTrain:  epoch  7, batch     8 | loss: 1.2736096MemoryTrain:  epoch  7, batch     9 | loss: 1.2152864MemoryTrain:  epoch  7, batch    10 | loss: 1.2476705MemoryTrain:  epoch  7, batch    11 | loss: 1.2868755MemoryTrain:  epoch  7, batch    12 | loss: 1.2105715MemoryTrain:  epoch  7, batch    13 | loss: 1.2029879MemoryTrain:  epoch  7, batch    14 | loss: 1.2315067MemoryTrain:  epoch  8, batch     0 | loss: 1.2641411MemoryTrain:  epoch  8, batch     1 | loss: 1.2294459MemoryTrain:  epoch  8, batch     2 | loss: 1.2241294MemoryTrain:  epoch  8, batch     3 | loss: 1.2852132MemoryTrain:  epoch  8, batch     4 | loss: 1.2534306MemoryTrain:  epoch  8, batch     5 | loss: 1.2567871MemoryTrain:  epoch  8, batch     6 | loss: 1.1907173MemoryTrain:  epoch  8, batch     7 | loss: 1.2529106MemoryTrain:  epoch  8, batch     8 | loss: 1.2076714MemoryTrain:  epoch  8, batch     9 | loss: 1.2497137MemoryTrain:  epoch  8, batch    10 | loss: 1.1665332MemoryTrain:  epoch  8, batch    11 | loss: 1.3093591MemoryTrain:  epoch  8, batch    12 | loss: 1.1965559MemoryTrain:  epoch  8, batch    13 | loss: 1.2597663MemoryTrain:  epoch  8, batch    14 | loss: 1.2659166MemoryTrain:  epoch  9, batch     0 | loss: 1.2079198MemoryTrain:  epoch  9, batch     1 | loss: 1.2282448MemoryTrain:  epoch  9, batch     2 | loss: 1.2114755MemoryTrain:  epoch  9, batch     3 | loss: 1.2219369MemoryTrain:  epoch  9, batch     4 | loss: 1.2122114MemoryTrain:  epoch  9, batch     5 | loss: 1.2162755MemoryTrain:  epoch  9, batch     6 | loss: 1.2081814MemoryTrain:  epoch  9, batch     7 | loss: 1.2453113MemoryTrain:  epoch  9, batch     8 | loss: 1.2109098MemoryTrain:  epoch  9, batch     9 | loss: 1.2286682MemoryTrain:  epoch  9, batch    10 | loss: 1.1971097MemoryTrain:  epoch  9, batch    11 | loss: 1.2652053MemoryTrain:  epoch  9, batch    12 | loss: 1.2412395MemoryTrain:  epoch  9, batch    13 | loss: 1.1905308MemoryTrain:  epoch  9, batch    14 | loss: 1.2491256
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 75.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 78.80%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.25%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 80.05%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   28 | acc: 62.50%,  total acc: 78.88%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 78.96%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 79.03%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 79.30%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 79.04%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 77.95%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 77.70%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 77.80%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 77.72%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 77.81%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 77.44%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 77.83%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 77.91%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 77.84%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 77.78%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 77.99%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 78.19%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 78.26%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 78.44%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 79.95%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 81.36%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 81.57%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 81.78%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 81.98%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 82.07%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 82.36%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 81.85%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 58.33%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 59.13%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 70.74%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 69.57%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 70.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 73.92%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 76.84%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 77.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 77.60%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 78.04%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 78.29%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.38%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 79.73%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 79.91%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 80.09%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 80.11%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 79.44%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 78.80%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 77.79%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 77.21%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 76.53%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 75.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 75.98%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 76.08%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 76.42%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 76.27%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 75.91%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 76.00%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 75.66%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 75.11%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 74.79%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 74.79%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 74.80%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 74.70%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 74.60%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 74.41%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 74.23%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 74.15%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 73.97%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 73.81%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 73.73%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 73.59%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 73.35%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 73.46%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 73.40%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 73.50%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 73.19%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 73.05%   [EVAL] batch:   77 | acc: 50.00%,  total acc: 72.76%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 72.47%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 72.19%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 72.38%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 72.26%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 71.84%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 71.43%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 71.10%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 70.64%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 70.55%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 70.72%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 70.95%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 71.20%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 71.24%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 71.41%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 72.29%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 72.58%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 73.33%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 73.53%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 73.48%   [EVAL] batch:  103 | acc: 81.25%,  total acc: 73.56%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 73.82%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 73.42%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 73.03%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 72.88%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 72.61%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 72.41%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 72.10%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 72.01%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 72.15%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.39%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 72.52%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 72.65%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 72.78%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 72.69%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 72.08%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 71.69%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 71.16%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 70.58%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 70.06%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 69.55%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 69.59%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 69.34%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 69.14%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 68.90%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 68.80%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 68.99%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 69.40%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 69.54%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 70.06%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 69.74%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 69.51%   [EVAL] batch:  140 | acc: 56.25%,  total acc: 69.41%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 69.41%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 69.19%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 68.97%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 68.88%   [EVAL] batch:  145 | acc: 68.75%,  total acc: 68.88%   [EVAL] batch:  146 | acc: 56.25%,  total acc: 68.79%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 68.79%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 68.71%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 68.42%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 68.26%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 67.93%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 67.74%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 67.38%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 67.07%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:  157 | acc: 43.75%,  total acc: 66.81%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 66.52%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 66.50%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 66.32%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 66.22%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 66.20%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 66.21%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 66.19%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 66.21%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 66.18%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 66.24%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 66.10%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 65.97%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 65.73%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 65.72%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 65.52%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 65.39%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 65.59%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 66.35%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 66.70%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 66.81%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 66.89%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 66.94%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 66.89%   [EVAL] batch:  188 | acc: 37.50%,  total acc: 66.73%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 66.45%   [EVAL] batch:  190 | acc: 12.50%,  total acc: 66.16%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 65.98%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 65.77%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 65.66%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 65.64%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 65.56%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 65.61%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 65.69%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 65.70%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 65.72%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:  201 | acc: 25.00%,  total acc: 65.53%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 65.49%   [EVAL] batch:  203 | acc: 43.75%,  total acc: 65.38%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 65.37%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 65.29%   [EVAL] batch:  206 | acc: 75.00%,  total acc: 65.34%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 65.47%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 65.64%   [EVAL] batch:  209 | acc: 87.50%,  total acc: 65.74%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 66.08%   [EVAL] batch:  213 | acc: 43.75%,  total acc: 65.98%   [EVAL] batch:  214 | acc: 37.50%,  total acc: 65.84%   [EVAL] batch:  215 | acc: 31.25%,  total acc: 65.68%   [EVAL] batch:  216 | acc: 43.75%,  total acc: 65.58%   [EVAL] batch:  217 | acc: 43.75%,  total acc: 65.48%   [EVAL] batch:  218 | acc: 43.75%,  total acc: 65.38%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.54%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 65.82%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 66.10%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 66.26%   [EVAL] batch:  226 | acc: 43.75%,  total acc: 66.16%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 66.09%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 65.91%   [EVAL] batch:  229 | acc: 37.50%,  total acc: 65.79%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 65.77%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 65.87%   [EVAL] batch:  232 | acc: 75.00%,  total acc: 65.91%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 65.97%   [EVAL] batch:  234 | acc: 75.00%,  total acc: 66.01%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 66.00%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 65.95%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 65.97%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 66.00%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 65.96%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 65.95%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 65.91%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 65.87%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 65.78%   [EVAL] batch:  244 | acc: 50.00%,  total acc: 65.71%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 65.70%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 65.64%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 65.68%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 65.76%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 65.70%   [EVAL] batch:  250 | acc: 100.00%,  total acc: 65.84%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 66.06%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 66.14%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 66.39%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 66.38%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 66.41%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 66.39%   [EVAL] batch:  260 | acc: 31.25%,  total acc: 66.26%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 66.22%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 66.23%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 66.29%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 66.37%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 66.42%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 66.51%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 66.50%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 66.41%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 66.26%   [EVAL] batch:  271 | acc: 37.50%,  total acc: 66.15%   [EVAL] batch:  272 | acc: 37.50%,  total acc: 66.05%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 65.90%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 65.77%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:  277 | acc: 93.75%,  total acc: 66.12%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 66.47%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 66.50%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 66.48%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 66.48%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 66.44%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 66.45%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 66.80%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 66.89%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 67.09%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 67.79%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 68.11%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 68.45%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 68.71%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 68.65%   [EVAL] batch:  313 | acc: 0.00%,  total acc: 68.43%   [EVAL] batch:  314 | acc: 6.25%,  total acc: 68.23%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 68.02%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 67.80%   [EVAL] batch:  317 | acc: 0.00%,  total acc: 67.59%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 67.48%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 67.64%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 67.77%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 67.83%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 67.85%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 67.81%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 67.82%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 67.82%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 67.77%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 67.73%   [EVAL] batch:  331 | acc: 18.75%,  total acc: 67.58%   [EVAL] batch:  332 | acc: 37.50%,  total acc: 67.49%   [EVAL] batch:  333 | acc: 50.00%,  total acc: 67.44%   [EVAL] batch:  334 | acc: 43.75%,  total acc: 67.37%   [EVAL] batch:  335 | acc: 37.50%,  total acc: 67.28%   [EVAL] batch:  336 | acc: 18.75%,  total acc: 67.14%   [EVAL] batch:  337 | acc: 12.50%,  total acc: 66.97%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 66.80%   [EVAL] batch:  339 | acc: 12.50%,  total acc: 66.64%   [EVAL] batch:  340 | acc: 6.25%,  total acc: 66.46%   [EVAL] batch:  341 | acc: 0.00%,  total acc: 66.26%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 66.09%   [EVAL] batch:  343 | acc: 12.50%,  total acc: 65.93%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 65.98%   [EVAL] batch:  345 | acc: 81.25%,  total acc: 66.02%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 66.08%   [EVAL] batch:  347 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 66.22%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 66.21%   [EVAL] batch:  350 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 66.37%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 66.45%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 66.53%   [EVAL] batch:  354 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  355 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 66.67%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 66.57%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 66.45%   [EVAL] batch:  359 | acc: 6.25%,  total acc: 66.28%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 66.17%   [EVAL] batch:  361 | acc: 18.75%,  total acc: 66.04%   [EVAL] batch:  362 | acc: 18.75%,  total acc: 65.91%   [EVAL] batch:  363 | acc: 31.25%,  total acc: 65.81%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 65.79%   [EVAL] batch:  365 | acc: 68.75%,  total acc: 65.80%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 65.75%   [EVAL] batch:  367 | acc: 68.75%,  total acc: 65.76%   [EVAL] batch:  368 | acc: 50.00%,  total acc: 65.72%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 65.68%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 65.60%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 65.56%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 65.55%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 65.59%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 65.58%   [EVAL] batch:  375 | acc: 62.50%,  total acc: 65.58%   [EVAL] batch:  376 | acc: 56.25%,  total acc: 65.55%   [EVAL] batch:  377 | acc: 62.50%,  total acc: 65.54%   [EVAL] batch:  378 | acc: 68.75%,  total acc: 65.55%   [EVAL] batch:  379 | acc: 56.25%,  total acc: 65.53%   [EVAL] batch:  380 | acc: 75.00%,  total acc: 65.55%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 65.61%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 65.65%   [EVAL] batch:  383 | acc: 62.50%,  total acc: 65.64%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:  385 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:  386 | acc: 56.25%,  total acc: 65.60%   [EVAL] batch:  387 | acc: 43.75%,  total acc: 65.54%   [EVAL] batch:  388 | acc: 18.75%,  total acc: 65.42%   [EVAL] batch:  389 | acc: 31.25%,  total acc: 65.34%   [EVAL] batch:  390 | acc: 18.75%,  total acc: 65.22%   [EVAL] batch:  391 | acc: 31.25%,  total acc: 65.13%   [EVAL] batch:  392 | acc: 25.00%,  total acc: 65.03%   [EVAL] batch:  393 | acc: 31.25%,  total acc: 64.94%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 65.02%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 65.35%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 65.42%   [EVAL] batch:  400 | acc: 37.50%,  total acc: 65.35%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 65.27%   [EVAL] batch:  402 | acc: 50.00%,  total acc: 65.23%   [EVAL] batch:  403 | acc: 50.00%,  total acc: 65.19%   [EVAL] batch:  404 | acc: 31.25%,  total acc: 65.11%   [EVAL] batch:  405 | acc: 31.25%,  total acc: 65.02%   [EVAL] batch:  406 | acc: 75.00%,  total acc: 65.05%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 65.12%   [EVAL] batch:  408 | acc: 100.00%,  total acc: 65.20%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 65.27%   [EVAL] batch:  410 | acc: 87.50%,  total acc: 65.33%   [EVAL] batch:  411 | acc: 93.75%,  total acc: 65.40%   [EVAL] batch:  412 | acc: 87.50%,  total acc: 65.45%   [EVAL] batch:  413 | acc: 62.50%,  total acc: 65.44%   [EVAL] batch:  414 | acc: 68.75%,  total acc: 65.45%   [EVAL] batch:  415 | acc: 56.25%,  total acc: 65.43%   [EVAL] batch:  416 | acc: 37.50%,  total acc: 65.36%   [EVAL] batch:  417 | acc: 50.00%,  total acc: 65.33%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 65.36%   [EVAL] batch:  419 | acc: 50.00%,  total acc: 65.33%   [EVAL] batch:  420 | acc: 56.25%,  total acc: 65.31%   [EVAL] batch:  421 | acc: 56.25%,  total acc: 65.28%   [EVAL] batch:  422 | acc: 50.00%,  total acc: 65.25%   [EVAL] batch:  423 | acc: 56.25%,  total acc: 65.23%   [EVAL] batch:  424 | acc: 56.25%,  total acc: 65.21%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 65.20%   [EVAL] batch:  426 | acc: 75.00%,  total acc: 65.22%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 65.23%   [EVAL] batch:  428 | acc: 81.25%,  total acc: 65.27%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 65.29%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 65.31%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 65.34%   [EVAL] batch:  432 | acc: 81.25%,  total acc: 65.37%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 65.45%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 65.55%   [EVAL] batch:  437 | acc: 62.50%,  total acc: 65.54%   [EVAL] batch:  438 | acc: 81.25%,  total acc: 65.58%   [EVAL] batch:  439 | acc: 56.25%,  total acc: 65.55%   [EVAL] batch:  440 | acc: 75.00%,  total acc: 65.58%   [EVAL] batch:  441 | acc: 81.25%,  total acc: 65.61%   [EVAL] batch:  442 | acc: 75.00%,  total acc: 65.63%   [EVAL] batch:  443 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:  444 | acc: 93.75%,  total acc: 65.69%   [EVAL] batch:  445 | acc: 68.75%,  total acc: 65.70%   [EVAL] batch:  446 | acc: 68.75%,  total acc: 65.70%   [EVAL] batch:  447 | acc: 75.00%,  total acc: 65.72%   [EVAL] batch:  448 | acc: 37.50%,  total acc: 65.66%   [EVAL] batch:  449 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  450 | acc: 87.50%,  total acc: 65.78%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 65.82%   [EVAL] batch:  452 | acc: 75.00%,  total acc: 65.84%   [EVAL] batch:  453 | acc: 75.00%,  total acc: 65.86%   [EVAL] batch:  454 | acc: 75.00%,  total acc: 65.88%   [EVAL] batch:  455 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:  456 | acc: 93.75%,  total acc: 66.01%   [EVAL] batch:  457 | acc: 87.50%,  total acc: 66.06%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  459 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 66.26%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  462 | acc: 81.25%,  total acc: 66.36%   [EVAL] batch:  463 | acc: 75.00%,  total acc: 66.38%   [EVAL] batch:  464 | acc: 68.75%,  total acc: 66.38%   [EVAL] batch:  465 | acc: 81.25%,  total acc: 66.42%   [EVAL] batch:  466 | acc: 68.75%,  total acc: 66.42%   [EVAL] batch:  467 | acc: 81.25%,  total acc: 66.45%   [EVAL] batch:  468 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:  469 | acc: 75.00%,  total acc: 66.50%   [EVAL] batch:  470 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:  471 | acc: 68.75%,  total acc: 66.53%   [EVAL] batch:  472 | acc: 62.50%,  total acc: 66.52%   [EVAL] batch:  473 | acc: 75.00%,  total acc: 66.53%   [EVAL] batch:  474 | acc: 62.50%,  total acc: 66.53%   [EVAL] batch:  475 | acc: 75.00%,  total acc: 66.54%   [EVAL] batch:  476 | acc: 81.25%,  total acc: 66.57%   [EVAL] batch:  477 | acc: 68.75%,  total acc: 66.58%   [EVAL] batch:  478 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:  479 | acc: 81.25%,  total acc: 66.65%   [EVAL] batch:  480 | acc: 68.75%,  total acc: 66.66%   [EVAL] batch:  481 | acc: 87.50%,  total acc: 66.70%   [EVAL] batch:  482 | acc: 75.00%,  total acc: 66.72%   [EVAL] batch:  483 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  484 | acc: 81.25%,  total acc: 66.82%   [EVAL] batch:  485 | acc: 81.25%,  total acc: 66.85%   [EVAL] batch:  486 | acc: 87.50%,  total acc: 66.89%   [EVAL] batch:  487 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  488 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 67.09%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:  491 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  492 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:  493 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  494 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:  496 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  497 | acc: 81.25%,  total acc: 67.56%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:  499 | acc: 100.00%,  total acc: 67.69%   
cur_acc:  ['0.9504', '0.7698', '0.8046', '0.7669', '0.8740', '0.6558', '0.7083', '0.8185']
his_acc:  ['0.9504', '0.8510', '0.8155', '0.7765', '0.7778', '0.7442', '0.6926', '0.6769']
--------Round  3
seed:  400
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.0163383CurrentTrain: epoch  0, batch     1 | loss: 12.6199055CurrentTrain: epoch  0, batch     2 | loss: 12.0659122CurrentTrain: epoch  0, batch     3 | loss: 12.0403185CurrentTrain: epoch  0, batch     4 | loss: 11.8768311CurrentTrain: epoch  0, batch     5 | loss: 11.7116613CurrentTrain: epoch  0, batch     6 | loss: 11.7157192CurrentTrain: epoch  0, batch     7 | loss: 11.3147545CurrentTrain: epoch  0, batch     8 | loss: 11.4094534CurrentTrain: epoch  0, batch     9 | loss: 11.2277946CurrentTrain: epoch  0, batch    10 | loss: 11.0203552CurrentTrain: epoch  0, batch    11 | loss: 10.5956554CurrentTrain: epoch  0, batch    12 | loss: 10.9211311CurrentTrain: epoch  0, batch    13 | loss: 10.7034130CurrentTrain: epoch  0, batch    14 | loss: 10.5737906CurrentTrain: epoch  0, batch    15 | loss: 10.5752964CurrentTrain: epoch  0, batch    16 | loss: 10.7940025CurrentTrain: epoch  0, batch    17 | loss: 10.5792618CurrentTrain: epoch  0, batch    18 | loss: 10.9380417CurrentTrain: epoch  0, batch    19 | loss: 10.1757383CurrentTrain: epoch  0, batch    20 | loss: 10.2315102CurrentTrain: epoch  0, batch    21 | loss: 10.3067427CurrentTrain: epoch  0, batch    22 | loss: 10.5523834CurrentTrain: epoch  0, batch    23 | loss: 10.5063620CurrentTrain: epoch  0, batch    24 | loss: 9.7297564CurrentTrain: epoch  0, batch    25 | loss: 10.0788116CurrentTrain: epoch  0, batch    26 | loss: 10.0636282CurrentTrain: epoch  0, batch    27 | loss: 9.4162531CurrentTrain: epoch  0, batch    28 | loss: 10.0759029CurrentTrain: epoch  0, batch    29 | loss: 9.3998032CurrentTrain: epoch  0, batch    30 | loss: 9.1157265CurrentTrain: epoch  0, batch    31 | loss: 9.8245325CurrentTrain: epoch  0, batch    32 | loss: 9.2154570CurrentTrain: epoch  0, batch    33 | loss: 9.6751699CurrentTrain: epoch  0, batch    34 | loss: 10.3828888CurrentTrain: epoch  0, batch    35 | loss: 9.6718035CurrentTrain: epoch  0, batch    36 | loss: 9.3169308CurrentTrain: epoch  0, batch    37 | loss: 9.3295126CurrentTrain: epoch  0, batch    38 | loss: 9.3993340CurrentTrain: epoch  0, batch    39 | loss: 8.8432589CurrentTrain: epoch  0, batch    40 | loss: 9.3864031CurrentTrain: epoch  0, batch    41 | loss: 8.8405275CurrentTrain: epoch  0, batch    42 | loss: 9.6477613CurrentTrain: epoch  0, batch    43 | loss: 9.1011858CurrentTrain: epoch  0, batch    44 | loss: 8.4170284CurrentTrain: epoch  0, batch    45 | loss: 9.7504997CurrentTrain: epoch  0, batch    46 | loss: 9.0205469CurrentTrain: epoch  0, batch    47 | loss: 9.0162640CurrentTrain: epoch  0, batch    48 | loss: 8.7220411CurrentTrain: epoch  0, batch    49 | loss: 8.7937851CurrentTrain: epoch  0, batch    50 | loss: 8.1781006CurrentTrain: epoch  0, batch    51 | loss: 8.4636955CurrentTrain: epoch  0, batch    52 | loss: 8.9256325CurrentTrain: epoch  0, batch    53 | loss: 7.6919208CurrentTrain: epoch  0, batch    54 | loss: 8.2043610CurrentTrain: epoch  0, batch    55 | loss: 8.5009098CurrentTrain: epoch  0, batch    56 | loss: 8.1992073CurrentTrain: epoch  0, batch    57 | loss: 8.5525846CurrentTrain: epoch  0, batch    58 | loss: 8.6235027CurrentTrain: epoch  0, batch    59 | loss: 8.9326553CurrentTrain: epoch  0, batch    60 | loss: 7.3446383CurrentTrain: epoch  0, batch    61 | loss: 7.6555519CurrentTrain: epoch  0, batch    62 | loss: 9.2307558CurrentTrain: epoch  1, batch     0 | loss: 7.9879603CurrentTrain: epoch  1, batch     1 | loss: 8.0525990CurrentTrain: epoch  1, batch     2 | loss: 7.0878687CurrentTrain: epoch  1, batch     3 | loss: 8.1120319CurrentTrain: epoch  1, batch     4 | loss: 8.0961752CurrentTrain: epoch  1, batch     5 | loss: 7.6915245CurrentTrain: epoch  1, batch     6 | loss: 7.1398592CurrentTrain: epoch  1, batch     7 | loss: 7.1556044CurrentTrain: epoch  1, batch     8 | loss: 7.4782257CurrentTrain: epoch  1, batch     9 | loss: 8.3168697CurrentTrain: epoch  1, batch    10 | loss: 7.0727654CurrentTrain: epoch  1, batch    11 | loss: 7.3950396CurrentTrain: epoch  1, batch    12 | loss: 7.3605399CurrentTrain: epoch  1, batch    13 | loss: 8.0578642CurrentTrain: epoch  1, batch    14 | loss: 7.4617534CurrentTrain: epoch  1, batch    15 | loss: 8.6566992CurrentTrain: epoch  1, batch    16 | loss: 7.0673656CurrentTrain: epoch  1, batch    17 | loss: 7.5902052CurrentTrain: epoch  1, batch    18 | loss: 7.7627311CurrentTrain: epoch  1, batch    19 | loss: 7.1827917CurrentTrain: epoch  1, batch    20 | loss: 7.6774087CurrentTrain: epoch  1, batch    21 | loss: 7.4480209CurrentTrain: epoch  1, batch    22 | loss: 7.0330477CurrentTrain: epoch  1, batch    23 | loss: 6.9588623CurrentTrain: epoch  1, batch    24 | loss: 6.8513927CurrentTrain: epoch  1, batch    25 | loss: 6.4217920CurrentTrain: epoch  1, batch    26 | loss: 6.7420459CurrentTrain: epoch  1, batch    27 | loss: 6.9646845CurrentTrain: epoch  1, batch    28 | loss: 6.6440148CurrentTrain: epoch  1, batch    29 | loss: 7.1710587CurrentTrain: epoch  1, batch    30 | loss: 6.9588919CurrentTrain: epoch  1, batch    31 | loss: 7.0668430CurrentTrain: epoch  1, batch    32 | loss: 6.6530962CurrentTrain: epoch  1, batch    33 | loss: 7.1381364CurrentTrain: epoch  1, batch    34 | loss: 6.8076553CurrentTrain: epoch  1, batch    35 | loss: 7.5922327CurrentTrain: epoch  1, batch    36 | loss: 7.0507717CurrentTrain: epoch  1, batch    37 | loss: 7.2319298CurrentTrain: epoch  1, batch    38 | loss: 6.5506759CurrentTrain: epoch  1, batch    39 | loss: 6.1676712CurrentTrain: epoch  1, batch    40 | loss: 6.4682131CurrentTrain: epoch  1, batch    41 | loss: 6.5197330CurrentTrain: epoch  1, batch    42 | loss: 6.4888239CurrentTrain: epoch  1, batch    43 | loss: 6.7314525CurrentTrain: epoch  1, batch    44 | loss: 5.9978232CurrentTrain: epoch  1, batch    45 | loss: 6.3152504CurrentTrain: epoch  1, batch    46 | loss: 7.5495934CurrentTrain: epoch  1, batch    47 | loss: 6.3168917CurrentTrain: epoch  1, batch    48 | loss: 6.4961138CurrentTrain: epoch  1, batch    49 | loss: 5.9808931CurrentTrain: epoch  1, batch    50 | loss: 6.5988693CurrentTrain: epoch  1, batch    51 | loss: 6.5492983CurrentTrain: epoch  1, batch    52 | loss: 5.8307743CurrentTrain: epoch  1, batch    53 | loss: 6.0652308CurrentTrain: epoch  1, batch    54 | loss: 6.1071124CurrentTrain: epoch  1, batch    55 | loss: 6.0025845CurrentTrain: epoch  1, batch    56 | loss: 6.3767405CurrentTrain: epoch  1, batch    57 | loss: 5.6210580CurrentTrain: epoch  1, batch    58 | loss: 6.4323883CurrentTrain: epoch  1, batch    59 | loss: 7.1498237CurrentTrain: epoch  1, batch    60 | loss: 5.8531494CurrentTrain: epoch  1, batch    61 | loss: 6.7285686CurrentTrain: epoch  1, batch    62 | loss: 5.0404530CurrentTrain: epoch  2, batch     0 | loss: 5.4454432CurrentTrain: epoch  2, batch     1 | loss: 4.9075985CurrentTrain: epoch  2, batch     2 | loss: 6.3146763CurrentTrain: epoch  2, batch     3 | loss: 6.5142512CurrentTrain: epoch  2, batch     4 | loss: 6.2288685CurrentTrain: epoch  2, batch     5 | loss: 6.2334175CurrentTrain: epoch  2, batch     6 | loss: 5.9763336CurrentTrain: epoch  2, batch     7 | loss: 6.5718465CurrentTrain: epoch  2, batch     8 | loss: 5.7546291CurrentTrain: epoch  2, batch     9 | loss: 6.2642736CurrentTrain: epoch  2, batch    10 | loss: 5.7182574CurrentTrain: epoch  2, batch    11 | loss: 6.9638443CurrentTrain: epoch  2, batch    12 | loss: 6.0211391CurrentTrain: epoch  2, batch    13 | loss: 5.4548316CurrentTrain: epoch  2, batch    14 | loss: 5.9079914CurrentTrain: epoch  2, batch    15 | loss: 5.8308740CurrentTrain: epoch  2, batch    16 | loss: 5.8758178CurrentTrain: epoch  2, batch    17 | loss: 5.6289186CurrentTrain: epoch  2, batch    18 | loss: 5.9335093CurrentTrain: epoch  2, batch    19 | loss: 5.9112482CurrentTrain: epoch  2, batch    20 | loss: 5.9476185CurrentTrain: epoch  2, batch    21 | loss: 5.5129890CurrentTrain: epoch  2, batch    22 | loss: 5.9723134CurrentTrain: epoch  2, batch    23 | loss: 4.8421016CurrentTrain: epoch  2, batch    24 | loss: 5.8800306CurrentTrain: epoch  2, batch    25 | loss: 5.6323175CurrentTrain: epoch  2, batch    26 | loss: 5.6983461CurrentTrain: epoch  2, batch    27 | loss: 6.0546923CurrentTrain: epoch  2, batch    28 | loss: 4.8972812CurrentTrain: epoch  2, batch    29 | loss: 5.9092054CurrentTrain: epoch  2, batch    30 | loss: 5.5169172CurrentTrain: epoch  2, batch    31 | loss: 5.6554718CurrentTrain: epoch  2, batch    32 | loss: 5.7843008CurrentTrain: epoch  2, batch    33 | loss: 5.8972001CurrentTrain: epoch  2, batch    34 | loss: 5.4559469CurrentTrain: epoch  2, batch    35 | loss: 5.1755171CurrentTrain: epoch  2, batch    36 | loss: 6.3438158CurrentTrain: epoch  2, batch    37 | loss: 5.5228128CurrentTrain: epoch  2, batch    38 | loss: 5.9130793CurrentTrain: epoch  2, batch    39 | loss: 5.6892157CurrentTrain: epoch  2, batch    40 | loss: 5.8517613CurrentTrain: epoch  2, batch    41 | loss: 5.4226789CurrentTrain: epoch  2, batch    42 | loss: 6.0207844CurrentTrain: epoch  2, batch    43 | loss: 5.4998798CurrentTrain: epoch  2, batch    44 | loss: 5.9154634CurrentTrain: epoch  2, batch    45 | loss: 5.8236847CurrentTrain: epoch  2, batch    46 | loss: 6.3937573CurrentTrain: epoch  2, batch    47 | loss: 5.4377441CurrentTrain: epoch  2, batch    48 | loss: 5.5710521CurrentTrain: epoch  2, batch    49 | loss: 5.0997224CurrentTrain: epoch  2, batch    50 | loss: 4.9223881CurrentTrain: epoch  2, batch    51 | loss: 5.5958815CurrentTrain: epoch  2, batch    52 | loss: 5.6638579CurrentTrain: epoch  2, batch    53 | loss: 4.9979072CurrentTrain: epoch  2, batch    54 | loss: 5.4874077CurrentTrain: epoch  2, batch    55 | loss: 5.4113307CurrentTrain: epoch  2, batch    56 | loss: 5.9852219CurrentTrain: epoch  2, batch    57 | loss: 5.4816155CurrentTrain: epoch  2, batch    58 | loss: 5.9823399CurrentTrain: epoch  2, batch    59 | loss: 5.4865499CurrentTrain: epoch  2, batch    60 | loss: 5.7388287CurrentTrain: epoch  2, batch    61 | loss: 4.9262161CurrentTrain: epoch  2, batch    62 | loss: 5.0331478CurrentTrain: epoch  3, batch     0 | loss: 6.3029137CurrentTrain: epoch  3, batch     1 | loss: 5.7216387CurrentTrain: epoch  3, batch     2 | loss: 5.0312481CurrentTrain: epoch  3, batch     3 | loss: 4.8524761CurrentTrain: epoch  3, batch     4 | loss: 5.5363026CurrentTrain: epoch  3, batch     5 | loss: 5.1026130CurrentTrain: epoch  3, batch     6 | loss: 5.1273217CurrentTrain: epoch  3, batch     7 | loss: 5.1390800CurrentTrain: epoch  3, batch     8 | loss: 4.9871058CurrentTrain: epoch  3, batch     9 | loss: 5.0237207CurrentTrain: epoch  3, batch    10 | loss: 5.0749788CurrentTrain: epoch  3, batch    11 | loss: 5.3320212CurrentTrain: epoch  3, batch    12 | loss: 5.1734877CurrentTrain: epoch  3, batch    13 | loss: 4.8679562CurrentTrain: epoch  3, batch    14 | loss: 5.0801640CurrentTrain: epoch  3, batch    15 | loss: 5.5650396CurrentTrain: epoch  3, batch    16 | loss: 5.3742208CurrentTrain: epoch  3, batch    17 | loss: 5.6085558CurrentTrain: epoch  3, batch    18 | loss: 5.5561571CurrentTrain: epoch  3, batch    19 | loss: 6.1174564CurrentTrain: epoch  3, batch    20 | loss: 5.2580628CurrentTrain: epoch  3, batch    21 | loss: 5.1812811CurrentTrain: epoch  3, batch    22 | loss: 5.1566429CurrentTrain: epoch  3, batch    23 | loss: 4.9755640CurrentTrain: epoch  3, batch    24 | loss: 5.3557119CurrentTrain: epoch  3, batch    25 | loss: 5.0741925CurrentTrain: epoch  3, batch    26 | loss: 5.0615997CurrentTrain: epoch  3, batch    27 | loss: 5.1230707CurrentTrain: epoch  3, batch    28 | loss: 5.0109453CurrentTrain: epoch  3, batch    29 | loss: 4.8408637CurrentTrain: epoch  3, batch    30 | loss: 4.8909459CurrentTrain: epoch  3, batch    31 | loss: 4.8581867CurrentTrain: epoch  3, batch    32 | loss: 5.0653358CurrentTrain: epoch  3, batch    33 | loss: 4.8782420CurrentTrain: epoch  3, batch    34 | loss: 5.1278591CurrentTrain: epoch  3, batch    35 | loss: 4.9610806CurrentTrain: epoch  3, batch    36 | loss: 5.0037055CurrentTrain: epoch  3, batch    37 | loss: 5.4498491CurrentTrain: epoch  3, batch    38 | loss: 4.8497562CurrentTrain: epoch  3, batch    39 | loss: 5.3052073CurrentTrain: epoch  3, batch    40 | loss: 5.2236443CurrentTrain: epoch  3, batch    41 | loss: 5.2712879CurrentTrain: epoch  3, batch    42 | loss: 4.8263354CurrentTrain: epoch  3, batch    43 | loss: 5.1508207CurrentTrain: epoch  3, batch    44 | loss: 4.7250090CurrentTrain: epoch  3, batch    45 | loss: 4.8379955CurrentTrain: epoch  3, batch    46 | loss: 4.6312809CurrentTrain: epoch  3, batch    47 | loss: 4.7047396CurrentTrain: epoch  3, batch    48 | loss: 4.7200289CurrentTrain: epoch  3, batch    49 | loss: 5.0705032CurrentTrain: epoch  3, batch    50 | loss: 4.9980288CurrentTrain: epoch  3, batch    51 | loss: 4.5091114CurrentTrain: epoch  3, batch    52 | loss: 5.0454359CurrentTrain: epoch  3, batch    53 | loss: 5.0011635CurrentTrain: epoch  3, batch    54 | loss: 4.7571702CurrentTrain: epoch  3, batch    55 | loss: 4.8129802CurrentTrain: epoch  3, batch    56 | loss: 4.6012855CurrentTrain: epoch  3, batch    57 | loss: 4.9763546CurrentTrain: epoch  3, batch    58 | loss: 4.9819260CurrentTrain: epoch  3, batch    59 | loss: 4.5328007CurrentTrain: epoch  3, batch    60 | loss: 4.9037757CurrentTrain: epoch  3, batch    61 | loss: 4.4719691CurrentTrain: epoch  3, batch    62 | loss: 4.4731388CurrentTrain: epoch  4, batch     0 | loss: 4.6557159CurrentTrain: epoch  4, batch     1 | loss: 4.5732646CurrentTrain: epoch  4, batch     2 | loss: 4.5830159CurrentTrain: epoch  4, batch     3 | loss: 5.2998857CurrentTrain: epoch  4, batch     4 | loss: 4.8397975CurrentTrain: epoch  4, batch     5 | loss: 4.3901906CurrentTrain: epoch  4, batch     6 | loss: 4.5829554CurrentTrain: epoch  4, batch     7 | loss: 4.5000620CurrentTrain: epoch  4, batch     8 | loss: 4.7744551CurrentTrain: epoch  4, batch     9 | loss: 4.6752682CurrentTrain: epoch  4, batch    10 | loss: 4.6147223CurrentTrain: epoch  4, batch    11 | loss: 4.5848150CurrentTrain: epoch  4, batch    12 | loss: 4.6820555CurrentTrain: epoch  4, batch    13 | loss: 4.4903717CurrentTrain: epoch  4, batch    14 | loss: 4.6498337CurrentTrain: epoch  4, batch    15 | loss: 4.6735868CurrentTrain: epoch  4, batch    16 | loss: 5.0803051CurrentTrain: epoch  4, batch    17 | loss: 4.6287327CurrentTrain: epoch  4, batch    18 | loss: 5.5040083CurrentTrain: epoch  4, batch    19 | loss: 5.0829663CurrentTrain: epoch  4, batch    20 | loss: 4.7849865CurrentTrain: epoch  4, batch    21 | loss: 4.7332354CurrentTrain: epoch  4, batch    22 | loss: 4.6845446CurrentTrain: epoch  4, batch    23 | loss: 5.1012058CurrentTrain: epoch  4, batch    24 | loss: 4.5878587CurrentTrain: epoch  4, batch    25 | loss: 4.6816211CurrentTrain: epoch  4, batch    26 | loss: 4.8145752CurrentTrain: epoch  4, batch    27 | loss: 4.5949354CurrentTrain: epoch  4, batch    28 | loss: 4.4955673CurrentTrain: epoch  4, batch    29 | loss: 4.5668268CurrentTrain: epoch  4, batch    30 | loss: 4.4923868CurrentTrain: epoch  4, batch    31 | loss: 4.6007223CurrentTrain: epoch  4, batch    32 | loss: 4.4224377CurrentTrain: epoch  4, batch    33 | loss: 4.5180526CurrentTrain: epoch  4, batch    34 | loss: 4.7888808CurrentTrain: epoch  4, batch    35 | loss: 4.7077165CurrentTrain: epoch  4, batch    36 | loss: 4.4855580CurrentTrain: epoch  4, batch    37 | loss: 4.5417657CurrentTrain: epoch  4, batch    38 | loss: 4.7252569CurrentTrain: epoch  4, batch    39 | loss: 4.5979476CurrentTrain: epoch  4, batch    40 | loss: 5.1830559CurrentTrain: epoch  4, batch    41 | loss: 4.3749223CurrentTrain: epoch  4, batch    42 | loss: 4.5182323CurrentTrain: epoch  4, batch    43 | loss: 4.5562153CurrentTrain: epoch  4, batch    44 | loss: 4.3744497CurrentTrain: epoch  4, batch    45 | loss: 4.9379973CurrentTrain: epoch  4, batch    46 | loss: 4.4636011CurrentTrain: epoch  4, batch    47 | loss: 4.5049038CurrentTrain: epoch  4, batch    48 | loss: 4.7212591CurrentTrain: epoch  4, batch    49 | loss: 4.5256844CurrentTrain: epoch  4, batch    50 | loss: 4.3687172CurrentTrain: epoch  4, batch    51 | loss: 4.8821826CurrentTrain: epoch  4, batch    52 | loss: 4.5441489CurrentTrain: epoch  4, batch    53 | loss: 4.7711105CurrentTrain: epoch  4, batch    54 | loss: 4.4265985CurrentTrain: epoch  4, batch    55 | loss: 4.5676975CurrentTrain: epoch  4, batch    56 | loss: 4.4339113CurrentTrain: epoch  4, batch    57 | loss: 4.3820200CurrentTrain: epoch  4, batch    58 | loss: 4.4971147CurrentTrain: epoch  4, batch    59 | loss: 4.5237112CurrentTrain: epoch  4, batch    60 | loss: 4.3650055CurrentTrain: epoch  4, batch    61 | loss: 4.7597218CurrentTrain: epoch  4, batch    62 | loss: 6.1370821CurrentTrain: epoch  5, batch     0 | loss: 4.6162891CurrentTrain: epoch  5, batch     1 | loss: 4.4431658CurrentTrain: epoch  5, batch     2 | loss: 4.4501176CurrentTrain: epoch  5, batch     3 | loss: 4.4677620CurrentTrain: epoch  5, batch     4 | loss: 4.8876553CurrentTrain: epoch  5, batch     5 | loss: 4.8273168CurrentTrain: epoch  5, batch     6 | loss: 4.4210038CurrentTrain: epoch  5, batch     7 | loss: 4.3456850CurrentTrain: epoch  5, batch     8 | loss: 4.2868052CurrentTrain: epoch  5, batch     9 | loss: 4.4469519CurrentTrain: epoch  5, batch    10 | loss: 4.3676329CurrentTrain: epoch  5, batch    11 | loss: 4.5220051CurrentTrain: epoch  5, batch    12 | loss: 4.4897709CurrentTrain: epoch  5, batch    13 | loss: 4.3677130CurrentTrain: epoch  5, batch    14 | loss: 4.7754126CurrentTrain: epoch  5, batch    15 | loss: 4.4896665CurrentTrain: epoch  5, batch    16 | loss: 4.5273061CurrentTrain: epoch  5, batch    17 | loss: 4.3024125CurrentTrain: epoch  5, batch    18 | loss: 4.3640485CurrentTrain: epoch  5, batch    19 | loss: 4.5582809CurrentTrain: epoch  5, batch    20 | loss: 4.6912231CurrentTrain: epoch  5, batch    21 | loss: 4.1878343CurrentTrain: epoch  5, batch    22 | loss: 5.0525026CurrentTrain: epoch  5, batch    23 | loss: 4.2951250CurrentTrain: epoch  5, batch    24 | loss: 4.2718611CurrentTrain: epoch  5, batch    25 | loss: 4.4471188CurrentTrain: epoch  5, batch    26 | loss: 4.2767057CurrentTrain: epoch  5, batch    27 | loss: 4.4349551CurrentTrain: epoch  5, batch    28 | loss: 4.3625484CurrentTrain: epoch  5, batch    29 | loss: 4.5278549CurrentTrain: epoch  5, batch    30 | loss: 4.5796638CurrentTrain: epoch  5, batch    31 | loss: 4.9794617CurrentTrain: epoch  5, batch    32 | loss: 4.3698750CurrentTrain: epoch  5, batch    33 | loss: 4.3616138CurrentTrain: epoch  5, batch    34 | loss: 4.2850790CurrentTrain: epoch  5, batch    35 | loss: 4.3145657CurrentTrain: epoch  5, batch    36 | loss: 4.2724962CurrentTrain: epoch  5, batch    37 | loss: 4.3011656CurrentTrain: epoch  5, batch    38 | loss: 4.2503157CurrentTrain: epoch  5, batch    39 | loss: 4.4485044CurrentTrain: epoch  5, batch    40 | loss: 4.3382912CurrentTrain: epoch  5, batch    41 | loss: 4.2659211CurrentTrain: epoch  5, batch    42 | loss: 4.5288086CurrentTrain: epoch  5, batch    43 | loss: 4.2693639CurrentTrain: epoch  5, batch    44 | loss: 4.2490177CurrentTrain: epoch  5, batch    45 | loss: 4.2710314CurrentTrain: epoch  5, batch    46 | loss: 4.2728200CurrentTrain: epoch  5, batch    47 | loss: 4.6554918CurrentTrain: epoch  5, batch    48 | loss: 4.4824314CurrentTrain: epoch  5, batch    49 | loss: 4.3666315CurrentTrain: epoch  5, batch    50 | loss: 4.3702631CurrentTrain: epoch  5, batch    51 | loss: 4.3142304CurrentTrain: epoch  5, batch    52 | loss: 4.3498263CurrentTrain: epoch  5, batch    53 | loss: 4.1862283CurrentTrain: epoch  5, batch    54 | loss: 4.3526607CurrentTrain: epoch  5, batch    55 | loss: 4.3916883CurrentTrain: epoch  5, batch    56 | loss: 4.1678643CurrentTrain: epoch  5, batch    57 | loss: 4.2739911CurrentTrain: epoch  5, batch    58 | loss: 4.4282579CurrentTrain: epoch  5, batch    59 | loss: 4.2351685CurrentTrain: epoch  5, batch    60 | loss: 4.2072854CurrentTrain: epoch  5, batch    61 | loss: 4.3151298CurrentTrain: epoch  5, batch    62 | loss: 4.1833243CurrentTrain: epoch  6, batch     0 | loss: 4.1997261CurrentTrain: epoch  6, batch     1 | loss: 4.5483751CurrentTrain: epoch  6, batch     2 | loss: 4.3387227CurrentTrain: epoch  6, batch     3 | loss: 4.2568321CurrentTrain: epoch  6, batch     4 | loss: 4.1629438CurrentTrain: epoch  6, batch     5 | loss: 4.2245402CurrentTrain: epoch  6, batch     6 | loss: 4.2529659CurrentTrain: epoch  6, batch     7 | loss: 4.2715220CurrentTrain: epoch  6, batch     8 | loss: 4.2247143CurrentTrain: epoch  6, batch     9 | loss: 4.3152542CurrentTrain: epoch  6, batch    10 | loss: 4.2049809CurrentTrain: epoch  6, batch    11 | loss: 4.2584529CurrentTrain: epoch  6, batch    12 | loss: 4.3013659CurrentTrain: epoch  6, batch    13 | loss: 4.2746205CurrentTrain: epoch  6, batch    14 | loss: 4.2286959CurrentTrain: epoch  6, batch    15 | loss: 4.2050200CurrentTrain: epoch  6, batch    16 | loss: 4.2240801CurrentTrain: epoch  6, batch    17 | loss: 4.2684155CurrentTrain: epoch  6, batch    18 | loss: 4.2682362CurrentTrain: epoch  6, batch    19 | loss: 4.3323679CurrentTrain: epoch  6, batch    20 | loss: 4.3209991CurrentTrain: epoch  6, batch    21 | loss: 4.3182249CurrentTrain: epoch  6, batch    22 | loss: 4.2125921CurrentTrain: epoch  6, batch    23 | loss: 4.1995311CurrentTrain: epoch  6, batch    24 | loss: 4.3042164CurrentTrain: epoch  6, batch    25 | loss: 4.1721926CurrentTrain: epoch  6, batch    26 | loss: 4.2230606CurrentTrain: epoch  6, batch    27 | loss: 4.0922346CurrentTrain: epoch  6, batch    28 | loss: 4.2518711CurrentTrain: epoch  6, batch    29 | loss: 4.2361364CurrentTrain: epoch  6, batch    30 | loss: 4.2389393CurrentTrain: epoch  6, batch    31 | loss: 4.1700511CurrentTrain: epoch  6, batch    32 | loss: 4.2079763CurrentTrain: epoch  6, batch    33 | loss: 4.1918688CurrentTrain: epoch  6, batch    34 | loss: 4.2089133CurrentTrain: epoch  6, batch    35 | loss: 4.1478100CurrentTrain: epoch  6, batch    36 | loss: 4.2722807CurrentTrain: epoch  6, batch    37 | loss: 4.4093790CurrentTrain: epoch  6, batch    38 | loss: 4.2784891CurrentTrain: epoch  6, batch    39 | loss: 4.1558704CurrentTrain: epoch  6, batch    40 | loss: 4.1625533CurrentTrain: epoch  6, batch    41 | loss: 4.1899495CurrentTrain: epoch  6, batch    42 | loss: 4.3058548CurrentTrain: epoch  6, batch    43 | loss: 4.2684093CurrentTrain: epoch  6, batch    44 | loss: 4.6100454CurrentTrain: epoch  6, batch    45 | loss: 4.1536198CurrentTrain: epoch  6, batch    46 | loss: 4.2276030CurrentTrain: epoch  6, batch    47 | loss: 4.1755733CurrentTrain: epoch  6, batch    48 | loss: 4.2767191CurrentTrain: epoch  6, batch    49 | loss: 4.0517817CurrentTrain: epoch  6, batch    50 | loss: 4.2241964CurrentTrain: epoch  6, batch    51 | loss: 4.1424618CurrentTrain: epoch  6, batch    52 | loss: 4.2819443CurrentTrain: epoch  6, batch    53 | loss: 4.1762838CurrentTrain: epoch  6, batch    54 | loss: 4.2179060CurrentTrain: epoch  6, batch    55 | loss: 4.3106499CurrentTrain: epoch  6, batch    56 | loss: 4.1704974CurrentTrain: epoch  6, batch    57 | loss: 4.1281013CurrentTrain: epoch  6, batch    58 | loss: 4.1252289CurrentTrain: epoch  6, batch    59 | loss: 4.1823907CurrentTrain: epoch  6, batch    60 | loss: 4.1213398CurrentTrain: epoch  6, batch    61 | loss: 4.1490664CurrentTrain: epoch  6, batch    62 | loss: 4.0871601CurrentTrain: epoch  7, batch     0 | loss: 4.1010799CurrentTrain: epoch  7, batch     1 | loss: 4.1361580CurrentTrain: epoch  7, batch     2 | loss: 4.1655388CurrentTrain: epoch  7, batch     3 | loss: 4.1649618CurrentTrain: epoch  7, batch     4 | loss: 4.1123571CurrentTrain: epoch  7, batch     5 | loss: 4.1500578CurrentTrain: epoch  7, batch     6 | loss: 4.1575961CurrentTrain: epoch  7, batch     7 | loss: 4.1696606CurrentTrain: epoch  7, batch     8 | loss: 4.1688795CurrentTrain: epoch  7, batch     9 | loss: 4.1771545CurrentTrain: epoch  7, batch    10 | loss: 4.3963723CurrentTrain: epoch  7, batch    11 | loss: 4.1789608CurrentTrain: epoch  7, batch    12 | loss: 4.2169571CurrentTrain: epoch  7, batch    13 | loss: 4.1523190CurrentTrain: epoch  7, batch    14 | loss: 4.1040277CurrentTrain: epoch  7, batch    15 | loss: 4.1130247CurrentTrain: epoch  7, batch    16 | loss: 4.1500554CurrentTrain: epoch  7, batch    17 | loss: 4.1159039CurrentTrain: epoch  7, batch    18 | loss: 4.0675788CurrentTrain: epoch  7, batch    19 | loss: 4.1458225CurrentTrain: epoch  7, batch    20 | loss: 4.1239519CurrentTrain: epoch  7, batch    21 | loss: 4.1087470CurrentTrain: epoch  7, batch    22 | loss: 4.1581497CurrentTrain: epoch  7, batch    23 | loss: 4.1472502CurrentTrain: epoch  7, batch    24 | loss: 4.1958704CurrentTrain: epoch  7, batch    25 | loss: 4.1474137CurrentTrain: epoch  7, batch    26 | loss: 4.1595812CurrentTrain: epoch  7, batch    27 | loss: 4.1296039CurrentTrain: epoch  7, batch    28 | loss: 4.1229744CurrentTrain: epoch  7, batch    29 | loss: 4.1445699CurrentTrain: epoch  7, batch    30 | loss: 4.1619930CurrentTrain: epoch  7, batch    31 | loss: 4.1267996CurrentTrain: epoch  7, batch    32 | loss: 4.1388278CurrentTrain: epoch  7, batch    33 | loss: 4.1448669CurrentTrain: epoch  7, batch    34 | loss: 4.0878649CurrentTrain: epoch  7, batch    35 | loss: 4.0868707CurrentTrain: epoch  7, batch    36 | loss: 4.1011586CurrentTrain: epoch  7, batch    37 | loss: 4.1261625CurrentTrain: epoch  7, batch    38 | loss: 4.1025777CurrentTrain: epoch  7, batch    39 | loss: 4.2630191CurrentTrain: epoch  7, batch    40 | loss: 4.1435232CurrentTrain: epoch  7, batch    41 | loss: 4.2468104CurrentTrain: epoch  7, batch    42 | loss: 4.1045246CurrentTrain: epoch  7, batch    43 | loss: 4.0600600CurrentTrain: epoch  7, batch    44 | loss: 4.0662184CurrentTrain: epoch  7, batch    45 | loss: 4.1640120CurrentTrain: epoch  7, batch    46 | loss: 4.0871325CurrentTrain: epoch  7, batch    47 | loss: 4.1231055CurrentTrain: epoch  7, batch    48 | loss: 4.1133871CurrentTrain: epoch  7, batch    49 | loss: 4.0700130CurrentTrain: epoch  7, batch    50 | loss: 4.0802460CurrentTrain: epoch  7, batch    51 | loss: 4.0527763CurrentTrain: epoch  7, batch    52 | loss: 4.0881052CurrentTrain: epoch  7, batch    53 | loss: 4.0739756CurrentTrain: epoch  7, batch    54 | loss: 4.1297054CurrentTrain: epoch  7, batch    55 | loss: 4.1258292CurrentTrain: epoch  7, batch    56 | loss: 4.0912414CurrentTrain: epoch  7, batch    57 | loss: 4.0931654CurrentTrain: epoch  7, batch    58 | loss: 4.0938740CurrentTrain: epoch  7, batch    59 | loss: 4.1178732CurrentTrain: epoch  7, batch    60 | loss: 3.9950352CurrentTrain: epoch  7, batch    61 | loss: 4.1191854CurrentTrain: epoch  7, batch    62 | loss: 4.0728960CurrentTrain: epoch  8, batch     0 | loss: 4.0347185CurrentTrain: epoch  8, batch     1 | loss: 4.1392326CurrentTrain: epoch  8, batch     2 | loss: 4.0185556CurrentTrain: epoch  8, batch     3 | loss: 4.1204948CurrentTrain: epoch  8, batch     4 | loss: 4.0855937CurrentTrain: epoch  8, batch     5 | loss: 4.0570641CurrentTrain: epoch  8, batch     6 | loss: 4.0716877CurrentTrain: epoch  8, batch     7 | loss: 4.0814586CurrentTrain: epoch  8, batch     8 | loss: 4.0935001CurrentTrain: epoch  8, batch     9 | loss: 4.0804472CurrentTrain: epoch  8, batch    10 | loss: 4.1365490CurrentTrain: epoch  8, batch    11 | loss: 4.0994873CurrentTrain: epoch  8, batch    12 | loss: 4.1165018CurrentTrain: epoch  8, batch    13 | loss: 4.1160135CurrentTrain: epoch  8, batch    14 | loss: 4.0616169CurrentTrain: epoch  8, batch    15 | loss: 4.0917645CurrentTrain: epoch  8, batch    16 | loss: 4.0564280CurrentTrain: epoch  8, batch    17 | loss: 4.1034164CurrentTrain: epoch  8, batch    18 | loss: 4.0925159CurrentTrain: epoch  8, batch    19 | loss: 4.0804758CurrentTrain: epoch  8, batch    20 | loss: 3.9945593CurrentTrain: epoch  8, batch    21 | loss: 4.0735455CurrentTrain: epoch  8, batch    22 | loss: 4.1108756CurrentTrain: epoch  8, batch    23 | loss: 4.0840435CurrentTrain: epoch  8, batch    24 | loss: 4.0545988CurrentTrain: epoch  8, batch    25 | loss: 4.0614214CurrentTrain: epoch  8, batch    26 | loss: 4.0657911CurrentTrain: epoch  8, batch    27 | loss: 4.0842695CurrentTrain: epoch  8, batch    28 | loss: 4.0179901CurrentTrain: epoch  8, batch    29 | loss: 4.1218529CurrentTrain: epoch  8, batch    30 | loss: 4.0218077CurrentTrain: epoch  8, batch    31 | loss: 4.1185269CurrentTrain: epoch  8, batch    32 | loss: 4.0568333CurrentTrain: epoch  8, batch    33 | loss: 4.0477395CurrentTrain: epoch  8, batch    34 | loss: 4.0923657CurrentTrain: epoch  8, batch    35 | loss: 4.1413498CurrentTrain: epoch  8, batch    36 | loss: 4.1129713CurrentTrain: epoch  8, batch    37 | loss: 4.0107069CurrentTrain: epoch  8, batch    38 | loss: 4.0190964CurrentTrain: epoch  8, batch    39 | loss: 4.1144495CurrentTrain: epoch  8, batch    40 | loss: 4.1125631CurrentTrain: epoch  8, batch    41 | loss: 4.0480323CurrentTrain: epoch  8, batch    42 | loss: 4.1411824CurrentTrain: epoch  8, batch    43 | loss: 4.0858355CurrentTrain: epoch  8, batch    44 | loss: 4.0816565CurrentTrain: epoch  8, batch    45 | loss: 4.0515556CurrentTrain: epoch  8, batch    46 | loss: 4.0877295CurrentTrain: epoch  8, batch    47 | loss: 4.0704093CurrentTrain: epoch  8, batch    48 | loss: 4.0653114CurrentTrain: epoch  8, batch    49 | loss: 4.0346823CurrentTrain: epoch  8, batch    50 | loss: 4.0570421CurrentTrain: epoch  8, batch    51 | loss: 4.0269356CurrentTrain: epoch  8, batch    52 | loss: 4.1165462CurrentTrain: epoch  8, batch    53 | loss: 4.1023836CurrentTrain: epoch  8, batch    54 | loss: 4.0718656CurrentTrain: epoch  8, batch    55 | loss: 4.1047935CurrentTrain: epoch  8, batch    56 | loss: 4.0182238CurrentTrain: epoch  8, batch    57 | loss: 4.0639119CurrentTrain: epoch  8, batch    58 | loss: 4.0357046CurrentTrain: epoch  8, batch    59 | loss: 4.0684872CurrentTrain: epoch  8, batch    60 | loss: 4.0099335CurrentTrain: epoch  8, batch    61 | loss: 4.0572243CurrentTrain: epoch  8, batch    62 | loss: 4.0461006CurrentTrain: epoch  9, batch     0 | loss: 4.0029459CurrentTrain: epoch  9, batch     1 | loss: 4.0228820CurrentTrain: epoch  9, batch     2 | loss: 4.0711746CurrentTrain: epoch  9, batch     3 | loss: 4.1111612CurrentTrain: epoch  9, batch     4 | loss: 4.0515847CurrentTrain: epoch  9, batch     5 | loss: 4.0397844CurrentTrain: epoch  9, batch     6 | loss: 4.0330095CurrentTrain: epoch  9, batch     7 | loss: 4.0333700CurrentTrain: epoch  9, batch     8 | loss: 4.0721488CurrentTrain: epoch  9, batch     9 | loss: 4.0279179CurrentTrain: epoch  9, batch    10 | loss: 4.0350451CurrentTrain: epoch  9, batch    11 | loss: 4.0466766CurrentTrain: epoch  9, batch    12 | loss: 4.0539227CurrentTrain: epoch  9, batch    13 | loss: 4.0531425CurrentTrain: epoch  9, batch    14 | loss: 4.0215445CurrentTrain: epoch  9, batch    15 | loss: 4.0484800CurrentTrain: epoch  9, batch    16 | loss: 4.0989938CurrentTrain: epoch  9, batch    17 | loss: 4.0514755CurrentTrain: epoch  9, batch    18 | loss: 4.0472946CurrentTrain: epoch  9, batch    19 | loss: 3.9942024CurrentTrain: epoch  9, batch    20 | loss: 4.0391331CurrentTrain: epoch  9, batch    21 | loss: 4.0472417CurrentTrain: epoch  9, batch    22 | loss: 4.0140572CurrentTrain: epoch  9, batch    23 | loss: 4.0429153CurrentTrain: epoch  9, batch    24 | loss: 4.0817146CurrentTrain: epoch  9, batch    25 | loss: 4.0624733CurrentTrain: epoch  9, batch    26 | loss: 4.0351276CurrentTrain: epoch  9, batch    27 | loss: 4.0890388CurrentTrain: epoch  9, batch    28 | loss: 4.0098138CurrentTrain: epoch  9, batch    29 | loss: 4.0356112CurrentTrain: epoch  9, batch    30 | loss: 4.0307255CurrentTrain: epoch  9, batch    31 | loss: 3.9856801CurrentTrain: epoch  9, batch    32 | loss: 4.0960884CurrentTrain: epoch  9, batch    33 | loss: 4.0704517CurrentTrain: epoch  9, batch    34 | loss: 4.0412312CurrentTrain: epoch  9, batch    35 | loss: 4.0162067CurrentTrain: epoch  9, batch    36 | loss: 4.0505533CurrentTrain: epoch  9, batch    37 | loss: 4.0411754CurrentTrain: epoch  9, batch    38 | loss: 4.0267277CurrentTrain: epoch  9, batch    39 | loss: 4.0357656CurrentTrain: epoch  9, batch    40 | loss: 4.0260754CurrentTrain: epoch  9, batch    41 | loss: 4.0607376CurrentTrain: epoch  9, batch    42 | loss: 4.0371218CurrentTrain: epoch  9, batch    43 | loss: 4.0122709CurrentTrain: epoch  9, batch    44 | loss: 4.0240102CurrentTrain: epoch  9, batch    45 | loss: 4.0936379CurrentTrain: epoch  9, batch    46 | loss: 4.0184832CurrentTrain: epoch  9, batch    47 | loss: 4.0845137CurrentTrain: epoch  9, batch    48 | loss: 4.0504594CurrentTrain: epoch  9, batch    49 | loss: 4.0533590CurrentTrain: epoch  9, batch    50 | loss: 4.0286517CurrentTrain: epoch  9, batch    51 | loss: 4.0625648CurrentTrain: epoch  9, batch    52 | loss: 4.0370636CurrentTrain: epoch  9, batch    53 | loss: 4.0891809CurrentTrain: epoch  9, batch    54 | loss: 4.0363798CurrentTrain: epoch  9, batch    55 | loss: 4.0694332CurrentTrain: epoch  9, batch    56 | loss: 4.0508866CurrentTrain: epoch  9, batch    57 | loss: 4.0384955CurrentTrain: epoch  9, batch    58 | loss: 4.0822439CurrentTrain: epoch  9, batch    59 | loss: 4.0134645CurrentTrain: epoch  9, batch    60 | loss: 4.0556998CurrentTrain: epoch  9, batch    61 | loss: 4.0424232CurrentTrain: epoch  9, batch    62 | loss: 4.0788822
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: while the milanese painting scene was dominated by neoclassic painters andrea appiani and luigi sabatelli , giovannni migliara stayed with the historical themes and medieval subtlety of romanticism .
Head Entity: andrea appiani
Tail Entity: neoclassic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: the surrealist movement, which emerged in the early 20th century, was significantly influenced by the works of artists like salvador dalí and max ernst, who explored the unconscious mind through their art.  
Head Entity: salvador dalí  
Tail Entity: surrealist  

Relation: movement  
Context: the beat generation was a literary movement that emerged in the 1950s, characterized by writers such as jack kerouac and allen ginsberg, who challenged conventional norms and explored new forms of expression.  
Head Entity: jack kerouac  
Tail Entity: beat generation  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: faldu was born on 1 august 1957 to a family of farmer in kalavad , jamnagar district , bombay state ( present - day gujarat ) .
Head Entity: kalavad
Tail Entity: jamnagar district
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of Springfield is known for its beautiful parks and vibrant community, situated in the heart of Sangamon County, Illinois.  
Head Entity: Springfield  
Tail Entity: Sangamon County  

Relation: located in the administrative territorial entity  
Context: The historic town of Williamsburg is a popular tourist destination, located within the boundaries of James City County, Virginia.  
Head Entity: Williamsburg  
Tail Entity: James City County  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: he is an ancestor of querl dox , brainiac 5 of the legion of super - heroes .
Head Entity: brainiac 5
Tail Entity: legion of super - heroes
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: She has been a prominent figure in the local art scene and is a member of the downtown art collective.  
Head Entity: She  
Tail Entity: downtown art collective  

Relation: member of  
Context: The band has gained popularity over the years, and John is a long-time member of the rock group.  
Head Entity: John  
Tail Entity: rock group  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County and allowing vehicles to cross over the waterway.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a rail link between the UK and mainland Europe.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the municipality contains the lower reaches of the maracanã river , which is joined by the caripi river before widening into maracanã bay and emptying into the atlantic ocean beside maiandeua island .
Head Entity: maracanã river
Tail Entity: caripi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Amazon River, one of the largest rivers in the world, receives numerous tributaries, including the Madeira River, which significantly contributes to its flow.  
Head Entity: Amazon River  
Tail Entity: Madeira River  

Relation: tributary  
Context: The Mississippi River is fed by several smaller rivers, with the Missouri River being one of its major tributaries that enhances its water volume.  
Head Entity: Mississippi River  
Tail Entity: Missouri River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: the result was widely interpreted as a personal rebuke to then chief minister shane stone .
Head Entity: shane stone
Tail Entity: chief minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After serving as the mayor for over a decade, John Smith announced his retirement from the position.  
Head Entity: John Smith  
Tail Entity: mayor  

Relation: position held  
Context: The former president of the organization, Maria Lopez, was praised for her leadership during her tenure.  
Head Entity: Maria Lopez  
Tail Entity: president
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: released by looking glass studios in 1998 and powered by their own in - house developed dark engine , " thief : the dark project " was considered by many to be a revolutionary game .
Head Entity: thief : the dark project
Tail Entity: looking glass studios
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: The popular game "The Witcher 3: Wild Hunt" was developed by CD Projekt Red, a studio known for its commitment to quality and storytelling.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Fortnite," a battle royale game that took the world by storm, was developed by Epic Games, which is also famous for its Unreal Engine technology.  
Head Entity: Fortnite  
Tail Entity: Epic Games  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: neptune is the second studio album by the london - based band the duke spirit and the last with the original lead guitarist , dan higgins .
Head Entity: the duke spirit
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in silicon valley by a group of innovative engineers looking to revolutionize the industry.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous rock band was established in new york city, where they quickly gained a following and changed the music scene.  
Head Entity: the famous rock band  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: during the same month , the band supported kings of leon 's tour of the united states , and coldplay 's tour of the united kingdom .
Head Entity: kings of leon
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous Italian dish, pizza, has gained popularity worldwide, but its roots can be traced back to Naples, Italy.  
Head Entity: pizza  
Tail Entity: Italy  

Relation: country of origin  
Context: The iconic brand, Rolex, is renowned for its luxury watches, which are crafted in Switzerland, known for its precision and quality in watchmaking.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.35%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.45%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.35%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.45%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 95.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
cur_acc:  ['0.9524']
his_acc:  ['0.9524']
CurrentTrain: epoch  0, batch     0 | loss: 7.8943725CurrentTrain: epoch  0, batch     1 | loss: 6.9180565CurrentTrain: epoch  0, batch     2 | loss: 7.2132220CurrentTrain: epoch  0, batch     3 | loss: 4.6009197CurrentTrain: epoch  1, batch     0 | loss: 6.7501907CurrentTrain: epoch  1, batch     1 | loss: 6.6359982CurrentTrain: epoch  1, batch     2 | loss: 6.2727442CurrentTrain: epoch  1, batch     3 | loss: 5.4995012CurrentTrain: epoch  2, batch     0 | loss: 5.5703516CurrentTrain: epoch  2, batch     1 | loss: 5.3562317CurrentTrain: epoch  2, batch     2 | loss: 5.6872144CurrentTrain: epoch  2, batch     3 | loss: 5.4189954CurrentTrain: epoch  3, batch     0 | loss: 5.6874437CurrentTrain: epoch  3, batch     1 | loss: 4.8185987CurrentTrain: epoch  3, batch     2 | loss: 4.8261337CurrentTrain: epoch  3, batch     3 | loss: 2.8866830CurrentTrain: epoch  4, batch     0 | loss: 4.9987926CurrentTrain: epoch  4, batch     1 | loss: 4.9065204CurrentTrain: epoch  4, batch     2 | loss: 4.0699606CurrentTrain: epoch  4, batch     3 | loss: 3.2667127CurrentTrain: epoch  5, batch     0 | loss: 4.4402065CurrentTrain: epoch  5, batch     1 | loss: 4.5096202CurrentTrain: epoch  5, batch     2 | loss: 4.5770831CurrentTrain: epoch  5, batch     3 | loss: 3.4589410CurrentTrain: epoch  6, batch     0 | loss: 4.4036255CurrentTrain: epoch  6, batch     1 | loss: 4.7067065CurrentTrain: epoch  6, batch     2 | loss: 3.9709716CurrentTrain: epoch  6, batch     3 | loss: 2.0988100CurrentTrain: epoch  7, batch     0 | loss: 4.3812003CurrentTrain: epoch  7, batch     1 | loss: 3.3087802CurrentTrain: epoch  7, batch     2 | loss: 3.9763150CurrentTrain: epoch  7, batch     3 | loss: 6.1657257CurrentTrain: epoch  8, batch     0 | loss: 4.0517063CurrentTrain: epoch  8, batch     1 | loss: 3.6716805CurrentTrain: epoch  8, batch     2 | loss: 3.2682781CurrentTrain: epoch  8, batch     3 | loss: 4.4396667CurrentTrain: epoch  9, batch     0 | loss: 3.1506281CurrentTrain: epoch  9, batch     1 | loss: 4.0802774CurrentTrain: epoch  9, batch     2 | loss: 2.8969245CurrentTrain: epoch  9, batch     3 | loss: 6.0384769
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: bryson 's best known work is his 1985 book " evil angels " which chronicles the story of lindy chamberlain 's trial for murder , following the death of her baby daughter , azaria .
Head Entity: evil angels
Tail Entity: death
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: in her groundbreaking 2010 book "the immortal life of henrietta lacks," rebecca skloot explores the life of henrietta lacks and the impact of her cells on medical research.  
Head Entity: the immortal life of henrietta lacks  
Tail Entity: henrietta lacks  

Relation: main subject  
Context: "sapiens: a brief history of humankind" by yuval noah harari examines the history and impact of homo sapiens from the emergence of our species to the present day.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: homo sapiens  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The United States women's national soccer team has consistently performed well in international competitions, winning the FIFA Women's World Cup multiple times, including in 2019 where they defeated the Netherlands 2-0.  
Head Entity: FIFA Women's World Cup  
Tail Entity: Netherlands  

Relation: participating team  
Context: During the 2021 Tokyo Olympics, the Australian basketball team showcased their skills, ultimately facing the United States in the semifinals, where they lost 97-78.  
Head Entity: Tokyo Olympics  
Tail Entity: United States  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: at state level , the parliament of victoria consists of the legislative assembly ( the lower house ) and the legislative council ( the upper house ) .
Head Entity: parliament of victoria
Tail Entity: legislative council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The human body is composed of various systems, including the circulatory system, which is essential for transporting blood throughout the body.  
Head Entity: human body  
Tail Entity: circulatory system  

Relation: has part  
Context: The solar system includes several celestial bodies, with Earth being one of the planets that orbits the Sun.  
Head Entity: solar system  
Tail Entity: Earth  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director, which was awarded to Guillermo del Toro.  
Head Entity: Guillermo del Toro  
Tail Entity: best director  

Relation: nominated for  
Context: In 2020, the popular series "Succession" was nominated for several Emmy Awards, showcasing its critical acclaim and audience popularity.  
Head Entity: Succession  
Tail Entity: Emmy Awards  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry with its touch interface and app ecosystem.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect Chinese states from invasions and raids, and is considered one of the most iconic structures in the world.  
Head Entity: Great Wall of China  
Tail Entity: fortification
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the majestic peaks of the sierra nevada ( ) are known for their stunning beauty and are part of the larger cascade range .  
Head Entity: sierra nevada  
Tail Entity: cascade range  

Relation: mountain range  
Context: the famous everest ( ) is the highest peak in the himalayas and is part of the greater himalayan range .  
Head Entity: everest  
Tail Entity: greater himalayan range  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: anders matthesen has also released several cds with his radio material , in addition to the animated movie " terkel in trouble " , based on one of these .
Head Entity: terkel in trouble
Tail Entity: anders matthesen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the acclaimed film "inception" features a complex narrative that was intricately crafted by its talented screenwriter, who is known for his unique storytelling style.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: screenwriter  
Context: the beloved animated feature "finding nemo" was brought to life through the creative vision of its screenwriter, whose work has captivated audiences of all ages.  
Head Entity: finding nemo  
Tail Entity: andrew stanton  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily presented in English, captivating audiences worldwide.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is celebrated for its intricate storytelling and is originally written in Spanish, reflecting the culture of Latin America.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, focusing on advanced materials and nanotechnology.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as a major center for the roman catholic faith in paris.  
Head Entity: notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the practice of tibetan buddhism, advocating for peace and compassion around the world.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
Mixup data size:  198
MixupTrain:  epoch  0, batch     0 | loss: 6.4525665MixupTrain:  epoch  0, batch     1 | loss: 5.9923651MixupTrain:  epoch  0, batch     2 | loss: 6.0146324MixupTrain:  epoch  0, batch     3 | loss: 5.7970589MixupTrain:  epoch  0, batch     4 | loss: 5.6275329MixupTrain:  epoch  0, batch     5 | loss: 5.2820225MixupTrain:  epoch  0, batch     6 | loss: 5.2060330MixupTrain:  epoch  0, batch     7 | loss: 5.1777755MixupTrain:  epoch  0, batch     8 | loss: 4.5920968MixupTrain:  epoch  0, batch     9 | loss: 4.4332074MixupTrain:  epoch  0, batch    10 | loss: 4.7574645MixupTrain:  epoch  0, batch    11 | loss: 4.3003507MixupTrain:  epoch  0, batch    12 | loss: 3.6327941
MemoryTrain:  epoch  0, batch     0 | loss: 3.3817329MemoryTrain:  epoch  0, batch     1 | loss: 3.8660097MemoryTrain:  epoch  0, batch     2 | loss: 3.6576264MemoryTrain:  epoch  0, batch     3 | loss: 3.4192894MemoryTrain:  epoch  1, batch     0 | loss: 2.7117529MemoryTrain:  epoch  1, batch     1 | loss: 3.8233380MemoryTrain:  epoch  1, batch     2 | loss: 3.0455492MemoryTrain:  epoch  1, batch     3 | loss: 2.7657771MemoryTrain:  epoch  2, batch     0 | loss: 3.2560449MemoryTrain:  epoch  2, batch     1 | loss: 2.1582720MemoryTrain:  epoch  2, batch     2 | loss: 2.3735642MemoryTrain:  epoch  2, batch     3 | loss: 1.7648611MemoryTrain:  epoch  3, batch     0 | loss: 1.5809604MemoryTrain:  epoch  3, batch     1 | loss: 2.1907878MemoryTrain:  epoch  3, batch     2 | loss: 2.8779292MemoryTrain:  epoch  3, batch     3 | loss: 1.9220216MemoryTrain:  epoch  4, batch     0 | loss: 1.9042149MemoryTrain:  epoch  4, batch     1 | loss: 1.7129960MemoryTrain:  epoch  4, batch     2 | loss: 1.9531174MemoryTrain:  epoch  4, batch     3 | loss: 2.3459866MemoryTrain:  epoch  5, batch     0 | loss: 1.7620596MemoryTrain:  epoch  5, batch     1 | loss: 1.6792953MemoryTrain:  epoch  5, batch     2 | loss: 1.9390280MemoryTrain:  epoch  5, batch     3 | loss: 1.8475590MemoryTrain:  epoch  6, batch     0 | loss: 1.6739372MemoryTrain:  epoch  6, batch     1 | loss: 2.2630615MemoryTrain:  epoch  6, batch     2 | loss: 1.3631127MemoryTrain:  epoch  6, batch     3 | loss: 1.6601615MemoryTrain:  epoch  7, batch     0 | loss: 1.6032681MemoryTrain:  epoch  7, batch     1 | loss: 1.7545516MemoryTrain:  epoch  7, batch     2 | loss: 1.7081594MemoryTrain:  epoch  7, batch     3 | loss: 1.5874259MemoryTrain:  epoch  8, batch     0 | loss: 1.6028693MemoryTrain:  epoch  8, batch     1 | loss: 1.6718265MemoryTrain:  epoch  8, batch     2 | loss: 1.4702311MemoryTrain:  epoch  8, batch     3 | loss: 1.5758443MemoryTrain:  epoch  9, batch     0 | loss: 1.5120260MemoryTrain:  epoch  9, batch     1 | loss: 1.6466010MemoryTrain:  epoch  9, batch     2 | loss: 1.2978537MemoryTrain:  epoch  9, batch     3 | loss: 1.5373151
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 61.46%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 59.87%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 61.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 56.25%,  total acc: 68.29%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 67.86%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 68.10%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 67.54%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 68.16%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 68.94%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 69.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 70.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 71.01%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 73.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 75.15%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 75.73%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 76.53%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 76.63%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 76.86%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 76.82%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 76.91%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 77.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 77.45%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 77.64%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 77.83%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 78.24%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 78.64%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 78.79%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 78.95%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 78.99%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 79.13%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 78.96%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 79.10%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 78.83%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 78.17%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 90.77%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 90.91%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 91.30%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.59%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 92.46%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 93.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.57%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.57%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.92%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 93.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 94.07%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.22%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 94.19%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.44%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 94.43%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 94.41%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 94.40%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.52%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 94.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 94.49%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.47%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.32%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 94.31%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.30%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 93.97%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 93.96%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.96%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 93.75%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 93.16%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 92.69%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 92.35%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 92.10%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 92.03%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 92.17%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 92.01%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 92.12%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 92.23%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 92.33%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 91.12%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 90.10%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 89.26%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 88.21%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 87.11%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 86.11%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 85.90%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 86.07%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 86.24%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 86.32%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 86.48%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 86.58%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 86.24%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 85.97%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 85.78%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 85.53%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 85.08%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 85.04%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 85.29%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 85.31%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 85.40%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 85.54%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 85.83%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 85.97%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 86.10%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 86.24%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 86.37%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 86.52%   [EVAL] batch:  108 | acc: 87.50%,  total acc: 86.53%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 86.53%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 86.43%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 86.38%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 86.40%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 86.53%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 86.65%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 86.71%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 86.66%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 86.67%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 86.63%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 86.48%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 86.39%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 86.30%   
cur_acc:  ['0.9524', '0.7817']
his_acc:  ['0.9524', '0.8630']
CurrentTrain: epoch  0, batch     0 | loss: 5.9704156CurrentTrain: epoch  0, batch     1 | loss: 5.4886761CurrentTrain: epoch  0, batch     2 | loss: 6.0366020CurrentTrain: epoch  0, batch     3 | loss: 5.7646303CurrentTrain: epoch  1, batch     0 | loss: 4.9991341CurrentTrain: epoch  1, batch     1 | loss: 5.9370904CurrentTrain: epoch  1, batch     2 | loss: 4.7435265CurrentTrain: epoch  1, batch     3 | loss: 2.6187108CurrentTrain: epoch  2, batch     0 | loss: 4.0767360CurrentTrain: epoch  2, batch     1 | loss: 4.5216632CurrentTrain: epoch  2, batch     2 | loss: 4.6973896CurrentTrain: epoch  2, batch     3 | loss: 3.3011467CurrentTrain: epoch  3, batch     0 | loss: 4.1391764CurrentTrain: epoch  3, batch     1 | loss: 4.0275693CurrentTrain: epoch  3, batch     2 | loss: 4.2622457CurrentTrain: epoch  3, batch     3 | loss: 5.4387922CurrentTrain: epoch  4, batch     0 | loss: 3.5641153CurrentTrain: epoch  4, batch     1 | loss: 3.9856892CurrentTrain: epoch  4, batch     2 | loss: 3.8333974CurrentTrain: epoch  4, batch     3 | loss: 3.6125791CurrentTrain: epoch  5, batch     0 | loss: 3.8803873CurrentTrain: epoch  5, batch     1 | loss: 3.3126469CurrentTrain: epoch  5, batch     2 | loss: 3.0311491CurrentTrain: epoch  5, batch     3 | loss: 5.0540428CurrentTrain: epoch  6, batch     0 | loss: 3.6811326CurrentTrain: epoch  6, batch     1 | loss: 3.3078647CurrentTrain: epoch  6, batch     2 | loss: 3.5272446CurrentTrain: epoch  6, batch     3 | loss: 1.8190297CurrentTrain: epoch  7, batch     0 | loss: 3.4353685CurrentTrain: epoch  7, batch     1 | loss: 3.1749296CurrentTrain: epoch  7, batch     2 | loss: 3.6577473CurrentTrain: epoch  7, batch     3 | loss: 1.9237103CurrentTrain: epoch  8, batch     0 | loss: 2.8665714CurrentTrain: epoch  8, batch     1 | loss: 2.9705858CurrentTrain: epoch  8, batch     2 | loss: 3.7257969CurrentTrain: epoch  8, batch     3 | loss: 3.2701390CurrentTrain: epoch  9, batch     0 | loss: 3.2114224CurrentTrain: epoch  9, batch     1 | loss: 2.9610748CurrentTrain: epoch  9, batch     2 | loss: 3.3168530CurrentTrain: epoch  9, batch     3 | loss: 2.1107149
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: peugeot took a similar step in 2010 when replacing the 407 and long - running but unpopular 607 with a single model , the 508 .
Head Entity: 508
Tail Entity: 407
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: In the 2021 season, the team introduced the new model, the X5, which follows the successful launch of the X3 in 2020.  
Head Entity: X5  
Tail Entity: X3  

Relation: follows  
Context: The latest smartphone, the Galaxy S21, follows the previous model, the Galaxy S20, which was released just a year earlier.  
Head Entity: Galaxy S21  
Tail Entity: Galaxy S20  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: in 1992 , with grand ceremony , the orioles began their season in a brand new ballpark , oriole park at camden yards , and thus retiring memorial stadium in the major league baseball world .
Head Entity: memorial stadium
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: The 2020 Summer Olympics, originally scheduled to be held in Tokyo, were postponed due to the COVID-19 pandemic, but the athletes continued to train for their respective sports.  
Head Entity: athletes  
Tail Entity: Olympics  

Relation: sport  
Context: After years of dedication and hard work, she finally qualified for the national swimming championships, showcasing her talent in the sport.  
Head Entity: national swimming championships  
Tail Entity: swimming  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael, the youngest son, was the father of three children, making him the proud dad of the next generation.  
Head Entity: michael  
Tail Entity: unknown father (context does not specify)  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon when it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English and became a global phenomenon.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was originally filmed in French and received numerous awards for its unique storytelling.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: the league champions for the fifth time in their history ( and the second season running ) were taunton town , but runners - up mangotsfield united took promotion to the southern league .
Head Entity: mangotsfield united
Tail Entity: southern league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful season, the team secured their spot in the premier league, showcasing their talent and determination throughout the matches, while their rivals struggled to keep up.  
Head Entity: the team  
Tail Entity: premier league  

Relation: league  
Context: Following their victory in the championship, the players celebrated their promotion to the national league, marking a significant achievement in their careers.  
Head Entity: the players  
Tail Entity: national league  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: retrieved 29 september 2010 . the daughter of actress xenia desni , tamara desni was born in berlin .
Head Entity: tamara desni
Tail Entity: xenia desni
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: on 15th march 2015, the famous singer and actress, jennifer lopez, celebrated her birthday with her children, emme and max, who are her pride and joy.  
Head Entity: emme  
Tail Entity: jennifer lopez  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much the children adored their mother, elena, as they shared stories and laughter together.  
Head Entity: elena  
Tail Entity: children  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site, symbolizing the country's historical strength.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, who navigates societal expectations and her feelings for mr. darcy.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Mixup data size:  258
MixupTrain:  epoch  0, batch     0 | loss: 3.0753509MixupTrain:  epoch  0, batch     1 | loss: 3.2106425MixupTrain:  epoch  0, batch     2 | loss: 2.6926630MixupTrain:  epoch  0, batch     3 | loss: 2.5020549MixupTrain:  epoch  0, batch     4 | loss: 2.8742936MixupTrain:  epoch  0, batch     5 | loss: 2.9894551MixupTrain:  epoch  0, batch     6 | loss: 3.2048895MixupTrain:  epoch  0, batch     7 | loss: 2.9826961MixupTrain:  epoch  0, batch     8 | loss: 3.3988315MixupTrain:  epoch  0, batch     9 | loss: 2.7554820MixupTrain:  epoch  0, batch    10 | loss: 2.9270653MixupTrain:  epoch  0, batch    11 | loss: 2.9903932MixupTrain:  epoch  0, batch    12 | loss: 3.1370862MixupTrain:  epoch  0, batch    13 | loss: 3.1399798MixupTrain:  epoch  0, batch    14 | loss: 2.4155659MixupTrain:  epoch  0, batch    15 | loss: 2.9330474MixupTrain:  epoch  0, batch    16 | loss: 1.9643825
MemoryTrain:  epoch  0, batch     0 | loss: 2.0480146MemoryTrain:  epoch  0, batch     1 | loss: 2.5227976MemoryTrain:  epoch  0, batch     2 | loss: 3.3558497MemoryTrain:  epoch  0, batch     3 | loss: 3.2590137MemoryTrain:  epoch  0, batch     4 | loss: 3.5804503MemoryTrain:  epoch  0, batch     5 | loss: 3.4760034MemoryTrain:  epoch  1, batch     0 | loss: 2.9369392MemoryTrain:  epoch  1, batch     1 | loss: 2.7834549MemoryTrain:  epoch  1, batch     2 | loss: 3.1561322MemoryTrain:  epoch  1, batch     3 | loss: 1.8379766MemoryTrain:  epoch  1, batch     4 | loss: 2.7419665MemoryTrain:  epoch  1, batch     5 | loss: 1.8745707MemoryTrain:  epoch  2, batch     0 | loss: 2.4202442MemoryTrain:  epoch  2, batch     1 | loss: 2.9528141MemoryTrain:  epoch  2, batch     2 | loss: 2.2430253MemoryTrain:  epoch  2, batch     3 | loss: 1.9501157MemoryTrain:  epoch  2, batch     4 | loss: 2.4207087MemoryTrain:  epoch  2, batch     5 | loss: 1.5359060MemoryTrain:  epoch  3, batch     0 | loss: 2.5695109MemoryTrain:  epoch  3, batch     1 | loss: 2.0378501MemoryTrain:  epoch  3, batch     2 | loss: 2.1776347MemoryTrain:  epoch  3, batch     3 | loss: 1.8541821MemoryTrain:  epoch  3, batch     4 | loss: 1.8850335MemoryTrain:  epoch  3, batch     5 | loss: 2.6965082MemoryTrain:  epoch  4, batch     0 | loss: 1.3885295MemoryTrain:  epoch  4, batch     1 | loss: 2.4251451MemoryTrain:  epoch  4, batch     2 | loss: 2.6178188MemoryTrain:  epoch  4, batch     3 | loss: 1.9932272MemoryTrain:  epoch  4, batch     4 | loss: 2.1246669MemoryTrain:  epoch  4, batch     5 | loss: 1.4522908MemoryTrain:  epoch  5, batch     0 | loss: 2.5018003MemoryTrain:  epoch  5, batch     1 | loss: 1.6617119MemoryTrain:  epoch  5, batch     2 | loss: 1.4013706MemoryTrain:  epoch  5, batch     3 | loss: 2.4705911MemoryTrain:  epoch  5, batch     4 | loss: 1.7622632MemoryTrain:  epoch  5, batch     5 | loss: 1.3505970MemoryTrain:  epoch  6, batch     0 | loss: 2.2217329MemoryTrain:  epoch  6, batch     1 | loss: 1.6772022MemoryTrain:  epoch  6, batch     2 | loss: 1.4814925MemoryTrain:  epoch  6, batch     3 | loss: 1.8347185MemoryTrain:  epoch  6, batch     4 | loss: 1.6254904MemoryTrain:  epoch  6, batch     5 | loss: 1.3201903MemoryTrain:  epoch  7, batch     0 | loss: 1.6002846MemoryTrain:  epoch  7, batch     1 | loss: 1.4313037MemoryTrain:  epoch  7, batch     2 | loss: 1.4910753MemoryTrain:  epoch  7, batch     3 | loss: 1.7471881MemoryTrain:  epoch  7, batch     4 | loss: 1.6638539MemoryTrain:  epoch  7, batch     5 | loss: 1.6777527MemoryTrain:  epoch  8, batch     0 | loss: 1.3951623MemoryTrain:  epoch  8, batch     1 | loss: 1.3699033MemoryTrain:  epoch  8, batch     2 | loss: 1.7329094MemoryTrain:  epoch  8, batch     3 | loss: 1.5984484MemoryTrain:  epoch  8, batch     4 | loss: 1.4347975MemoryTrain:  epoch  8, batch     5 | loss: 1.7015381MemoryTrain:  epoch  9, batch     0 | loss: 1.7585044MemoryTrain:  epoch  9, batch     1 | loss: 1.5360808MemoryTrain:  epoch  9, batch     2 | loss: 1.4687203MemoryTrain:  epoch  9, batch     3 | loss: 1.5357518MemoryTrain:  epoch  9, batch     4 | loss: 1.2996438MemoryTrain:  epoch  9, batch     5 | loss: 1.2277042
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 73.05%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 73.03%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 74.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 76.42%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 77.45%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 78.39%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 79.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 79.57%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 80.09%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 80.58%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 80.82%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 81.04%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 82.72%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 83.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 83.88%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 84.13%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 84.06%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 83.54%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 82.85%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 83.70%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 84.04%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 84.57%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 84.75%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 84.68%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 84.13%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 83.73%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 83.56%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 83.64%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 83.26%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 82.89%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 82.87%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 82.63%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 82.71%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 82.68%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 82.96%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 82.34%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 81.70%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.53%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 85.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.02%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.34%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.76%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.03%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 90.13%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 90.70%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.92%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 90.84%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.05%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 91.17%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 91.22%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.28%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.45%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 91.70%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.74%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 91.56%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 90.62%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 90.04%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 89.79%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 89.14%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 88.71%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 87.79%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 86.92%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 86.84%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 86.38%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 86.03%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 85.78%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 85.89%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 86.00%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 86.32%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 85.36%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 84.33%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 83.57%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 82.59%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 81.56%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 80.56%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 80.49%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 80.72%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 81.10%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 81.32%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 81.47%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 81.53%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 81.18%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 80.90%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 80.70%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 80.50%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 80.11%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 79.92%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 80.13%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 80.27%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 80.41%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 80.55%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.74%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.94%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 81.06%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 81.07%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 81.13%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 81.19%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 81.31%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 81.37%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 81.02%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 80.67%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 80.33%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 80.00%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 79.73%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 79.30%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 79.37%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 79.33%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 79.51%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 79.58%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 79.70%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 79.82%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 79.83%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 79.95%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 80.01%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 80.02%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 80.03%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 80.09%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 80.15%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 80.11%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 80.02%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 79.98%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 79.99%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 79.95%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 79.72%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 79.65%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 79.71%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 79.77%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 79.87%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 79.97%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 79.89%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 79.72%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 79.60%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 79.34%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 79.31%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 79.28%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 79.21%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 79.59%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 79.87%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.96%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 80.05%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 80.14%   [EVAL] batch:  152 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 80.28%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 80.32%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 80.37%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 80.49%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 80.70%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 80.78%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 80.86%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 80.94%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 81.02%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 81.10%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 81.10%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 80.99%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 80.95%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 80.84%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 80.92%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 81.10%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 81.21%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 81.32%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 81.39%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 81.46%   [EVAL] batch:  175 | acc: 81.25%,  total acc: 81.46%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 81.32%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 81.21%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 81.18%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 81.22%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 81.11%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 81.01%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 81.01%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 80.94%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 80.98%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 80.98%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 81.08%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 80.88%   
cur_acc:  ['0.9524', '0.7817', '0.8234']
his_acc:  ['0.9524', '0.8630', '0.8088']
CurrentTrain: epoch  0, batch     0 | loss: 4.9875836CurrentTrain: epoch  0, batch     1 | loss: 6.8230109CurrentTrain: epoch  0, batch     2 | loss: 5.9625626CurrentTrain: epoch  0, batch     3 | loss: 4.3538961CurrentTrain: epoch  1, batch     0 | loss: 5.3379459CurrentTrain: epoch  1, batch     1 | loss: 5.7038002CurrentTrain: epoch  1, batch     2 | loss: 4.2339334CurrentTrain: epoch  1, batch     3 | loss: 3.9343624CurrentTrain: epoch  2, batch     0 | loss: 4.4908428CurrentTrain: epoch  2, batch     1 | loss: 4.9661951CurrentTrain: epoch  2, batch     2 | loss: 4.8175402CurrentTrain: epoch  2, batch     3 | loss: 1.7675726CurrentTrain: epoch  3, batch     0 | loss: 4.6707358CurrentTrain: epoch  3, batch     1 | loss: 4.9088526CurrentTrain: epoch  3, batch     2 | loss: 3.7170367CurrentTrain: epoch  3, batch     3 | loss: 4.5755086CurrentTrain: epoch  4, batch     0 | loss: 3.7541566CurrentTrain: epoch  4, batch     1 | loss: 4.1734810CurrentTrain: epoch  4, batch     2 | loss: 4.3346710CurrentTrain: epoch  4, batch     3 | loss: 4.2908750CurrentTrain: epoch  5, batch     0 | loss: 3.2965219CurrentTrain: epoch  5, batch     1 | loss: 3.9823627CurrentTrain: epoch  5, batch     2 | loss: 3.4259214CurrentTrain: epoch  5, batch     3 | loss: 4.5769343CurrentTrain: epoch  6, batch     0 | loss: 3.4284358CurrentTrain: epoch  6, batch     1 | loss: 4.0790195CurrentTrain: epoch  6, batch     2 | loss: 2.9400456CurrentTrain: epoch  6, batch     3 | loss: 5.2406139CurrentTrain: epoch  7, batch     0 | loss: 2.9142058CurrentTrain: epoch  7, batch     1 | loss: 3.4859490CurrentTrain: epoch  7, batch     2 | loss: 3.6044807CurrentTrain: epoch  7, batch     3 | loss: 1.8292346CurrentTrain: epoch  8, batch     0 | loss: 3.1248322CurrentTrain: epoch  8, batch     1 | loss: 2.8538420CurrentTrain: epoch  8, batch     2 | loss: 2.9312532CurrentTrain: epoch  8, batch     3 | loss: 2.9475126CurrentTrain: epoch  9, batch     0 | loss: 2.5197682CurrentTrain: epoch  9, batch     1 | loss: 3.4872532CurrentTrain: epoch  9, batch     2 | loss: 2.5958228CurrentTrain: epoch  9, batch     3 | loss: 3.1906700
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift performed her hit single "Shake It Off" at the Grammy Awards, captivating the audience with her energetic stage presence.  
Head Entity: Shake It Off  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that has been produced by Tesla, Inc. since 2012, showcasing cutting-edge technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: at the 2016 summer olympics , heath and schofield won the silver k-2 200 event , finishing behind spain 's saúl craviotto and cristian toro .
Head Entity: 2016 summer olympics
Tail Entity: spain
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: during the 2020 presidential election, the democratic party nominated joe biden as their candidate, while the republican party supported donald trump.  
Head Entity: 2020 presidential election  
Tail Entity: democratic party  

Relation: participant  
Context: in the 2022 world cup, france emerged as the champion, defeating argentina in a thrilling final match.  
Head Entity: 2022 world cup  
Tail Entity: france  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: This game runs smoothly on multiple operating systems, such as Linux, Windows 10, and the latest version of macOS.  
Head Entity: game  
Tail Entity: Windows 10  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters dash, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters dash  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the local political landscape.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring change and transparency to the office.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce loyalty to each other, having grown up under the same roof and shared the same parents.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it became clear that both Anna and her brother, Mark, had inherited their parents' artistic talents, showcasing their skills in painting and music.  
Head Entity: Anna  
Tail Entity: Mark  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: sergeant johnson served in the united states marine corps during his military career.  
Head Entity: sergeant johnson  
Tail Entity: united states marine corps  

Relation: military branch  
Context: general smith was a prominent figure in the royal air force, leading several key missions.  
Head Entity: general smith  
Tail Entity: royal air force  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
Mixup data size:  319
MixupTrain:  epoch  0, batch     0 | loss: 2.3230813MixupTrain:  epoch  0, batch     1 | loss: 3.0040064MixupTrain:  epoch  0, batch     2 | loss: 3.1786462MixupTrain:  epoch  0, batch     3 | loss: 3.2284333MixupTrain:  epoch  0, batch     4 | loss: 3.0909986MixupTrain:  epoch  0, batch     5 | loss: 2.3035876MixupTrain:  epoch  0, batch     6 | loss: 2.3755106MixupTrain:  epoch  0, batch     7 | loss: 2.6354914MixupTrain:  epoch  0, batch     8 | loss: 3.2650188MixupTrain:  epoch  0, batch     9 | loss: 2.9607738MixupTrain:  epoch  0, batch    10 | loss: 3.2644651MixupTrain:  epoch  0, batch    11 | loss: 2.3946684MixupTrain:  epoch  0, batch    12 | loss: 2.8197813MixupTrain:  epoch  0, batch    13 | loss: 2.6825658MixupTrain:  epoch  0, batch    14 | loss: 2.8507532MixupTrain:  epoch  0, batch    15 | loss: 2.3988274MixupTrain:  epoch  0, batch    16 | loss: 2.1472891MixupTrain:  epoch  0, batch    17 | loss: 2.6888317MixupTrain:  epoch  0, batch    18 | loss: 3.0060242MixupTrain:  epoch  0, batch    19 | loss: 2.0180179
MemoryTrain:  epoch  0, batch     0 | loss: 2.3078184MemoryTrain:  epoch  0, batch     1 | loss: 2.5630336MemoryTrain:  epoch  0, batch     2 | loss: 3.0919309MemoryTrain:  epoch  0, batch     3 | loss: 3.0155139MemoryTrain:  epoch  0, batch     4 | loss: 2.8606524MemoryTrain:  epoch  0, batch     5 | loss: 2.4835925MemoryTrain:  epoch  0, batch     6 | loss: 3.3959341MemoryTrain:  epoch  0, batch     7 | loss: 3.5626898MemoryTrain:  epoch  1, batch     0 | loss: 2.1980653MemoryTrain:  epoch  1, batch     1 | loss: 3.1385212MemoryTrain:  epoch  1, batch     2 | loss: 2.4136090MemoryTrain:  epoch  1, batch     3 | loss: 2.3853998MemoryTrain:  epoch  1, batch     4 | loss: 3.1100297MemoryTrain:  epoch  1, batch     5 | loss: 2.7925632MemoryTrain:  epoch  1, batch     6 | loss: 1.7945129MemoryTrain:  epoch  1, batch     7 | loss: 2.4531407MemoryTrain:  epoch  2, batch     0 | loss: 1.9675303MemoryTrain:  epoch  2, batch     1 | loss: 2.2671559MemoryTrain:  epoch  2, batch     2 | loss: 2.8986354MemoryTrain:  epoch  2, batch     3 | loss: 1.9176428MemoryTrain:  epoch  2, batch     4 | loss: 2.5220199MemoryTrain:  epoch  2, batch     5 | loss: 2.0196414MemoryTrain:  epoch  2, batch     6 | loss: 1.8417954MemoryTrain:  epoch  2, batch     7 | loss: 1.9625753MemoryTrain:  epoch  3, batch     0 | loss: 2.1887622MemoryTrain:  epoch  3, batch     1 | loss: 2.3261142MemoryTrain:  epoch  3, batch     2 | loss: 1.8431156MemoryTrain:  epoch  3, batch     3 | loss: 2.2495403MemoryTrain:  epoch  3, batch     4 | loss: 1.8616070MemoryTrain:  epoch  3, batch     5 | loss: 1.6934979MemoryTrain:  epoch  3, batch     6 | loss: 1.6358612MemoryTrain:  epoch  3, batch     7 | loss: 2.1331458MemoryTrain:  epoch  4, batch     0 | loss: 1.9227012MemoryTrain:  epoch  4, batch     1 | loss: 2.5928299MemoryTrain:  epoch  4, batch     2 | loss: 1.7450035MemoryTrain:  epoch  4, batch     3 | loss: 1.7745476MemoryTrain:  epoch  4, batch     4 | loss: 1.5732152MemoryTrain:  epoch  4, batch     5 | loss: 2.0068076MemoryTrain:  epoch  4, batch     6 | loss: 1.9218774MemoryTrain:  epoch  4, batch     7 | loss: 1.3394821MemoryTrain:  epoch  5, batch     0 | loss: 1.7166569MemoryTrain:  epoch  5, batch     1 | loss: 2.2681379MemoryTrain:  epoch  5, batch     2 | loss: 1.6353633MemoryTrain:  epoch  5, batch     3 | loss: 1.8552619MemoryTrain:  epoch  5, batch     4 | loss: 1.7819408MemoryTrain:  epoch  5, batch     5 | loss: 1.6571530MemoryTrain:  epoch  5, batch     6 | loss: 1.5151238MemoryTrain:  epoch  5, batch     7 | loss: 1.6068981MemoryTrain:  epoch  6, batch     0 | loss: 1.6256274MemoryTrain:  epoch  6, batch     1 | loss: 2.0029411MemoryTrain:  epoch  6, batch     2 | loss: 1.4776666MemoryTrain:  epoch  6, batch     3 | loss: 1.5652843MemoryTrain:  epoch  6, batch     4 | loss: 1.6758332MemoryTrain:  epoch  6, batch     5 | loss: 1.5914767MemoryTrain:  epoch  6, batch     6 | loss: 1.3887665MemoryTrain:  epoch  6, batch     7 | loss: 1.3359871MemoryTrain:  epoch  7, batch     0 | loss: 1.6106375MemoryTrain:  epoch  7, batch     1 | loss: 1.7118847MemoryTrain:  epoch  7, batch     2 | loss: 1.4771695MemoryTrain:  epoch  7, batch     3 | loss: 1.4278874MemoryTrain:  epoch  7, batch     4 | loss: 1.4799027MemoryTrain:  epoch  7, batch     5 | loss: 1.3870282MemoryTrain:  epoch  7, batch     6 | loss: 1.6291577MemoryTrain:  epoch  7, batch     7 | loss: 1.2610263MemoryTrain:  epoch  8, batch     0 | loss: 1.3735125MemoryTrain:  epoch  8, batch     1 | loss: 1.3585215MemoryTrain:  epoch  8, batch     2 | loss: 1.6561695MemoryTrain:  epoch  8, batch     3 | loss: 1.3495433MemoryTrain:  epoch  8, batch     4 | loss: 1.3307551MemoryTrain:  epoch  8, batch     5 | loss: 1.5801930MemoryTrain:  epoch  8, batch     6 | loss: 1.4820862MemoryTrain:  epoch  8, batch     7 | loss: 1.4114871MemoryTrain:  epoch  9, batch     0 | loss: 1.3963633MemoryTrain:  epoch  9, batch     1 | loss: 1.4805659MemoryTrain:  epoch  9, batch     2 | loss: 1.2880039MemoryTrain:  epoch  9, batch     3 | loss: 1.4005468MemoryTrain:  epoch  9, batch     4 | loss: 1.2708666MemoryTrain:  epoch  9, batch     5 | loss: 1.4003789MemoryTrain:  epoch  9, batch     6 | loss: 1.4383954MemoryTrain:  epoch  9, batch     7 | loss: 1.5733986
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 56.25%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.32%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 79.60%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 80.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 80.73%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 81.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 82.34%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 82.32%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 82.59%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.85%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 82.67%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 81.81%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 80.43%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 79.52%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 79.30%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 78.44%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 78.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 78.31%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 78.61%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 78.77%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 78.82%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 79.09%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 79.35%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 79.06%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 78.77%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 77.97%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 77.08%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 76.23%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 75.50%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 74.60%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.55%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.95%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 87.14%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.84%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 87.99%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.30%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 88.59%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 88.72%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.10%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 89.17%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 88.86%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 88.43%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 88.02%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.27%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 88.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.11%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 88.10%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 88.31%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 88.18%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 87.72%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 86.75%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 86.12%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 85.62%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 84.84%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 84.07%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 83.93%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 83.50%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 82.88%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 82.86%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 82.46%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 82.17%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 81.97%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 81.34%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 80.99%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 80.30%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 79.45%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 79.31%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 79.00%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 77.96%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 77.03%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 76.28%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 75.40%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 74.45%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 73.53%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 73.55%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 74.41%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 74.71%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 75.07%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 74.79%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 74.44%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 74.18%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 73.91%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 73.39%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 73.20%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 73.70%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 73.90%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.06%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 75.18%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 75.88%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 75.58%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 75.17%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 74.77%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 74.43%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 74.10%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 73.72%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 73.67%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 73.52%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 73.59%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 73.60%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 73.66%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 73.68%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 73.74%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 73.91%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 74.07%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 74.13%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 74.14%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 74.24%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 74.45%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 74.36%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 74.11%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 74.02%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 73.93%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 73.89%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 73.62%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 73.63%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 73.73%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 73.83%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 73.98%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 74.13%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 74.27%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 74.14%   [EVAL] batch:  138 | acc: 37.50%,  total acc: 73.88%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 73.57%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 73.36%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 72.98%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 72.90%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 72.79%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.30%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 73.88%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 74.01%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:  154 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 74.56%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 74.72%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.12%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 75.23%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 75.27%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 75.08%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 74.77%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 74.59%   [EVAL] batch:  166 | acc: 50.00%,  total acc: 74.44%   [EVAL] batch:  167 | acc: 31.25%,  total acc: 74.18%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 74.15%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 74.30%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 74.53%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 74.64%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 74.75%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 74.82%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 74.79%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 74.72%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 74.68%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 74.65%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 74.69%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 74.52%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 74.42%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 74.35%   [EVAL] batch:  183 | acc: 62.50%,  total acc: 74.29%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 74.36%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 74.46%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 74.57%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 74.60%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 74.57%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 74.54%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 74.64%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 74.64%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 74.71%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 74.78%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 74.81%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 74.75%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 74.81%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 74.87%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 74.91%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 74.94%   [EVAL] batch:  201 | acc: 93.75%,  total acc: 75.03%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 75.06%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 75.06%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 75.15%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 75.24%   [EVAL] batch:  206 | acc: 50.00%,  total acc: 75.12%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 74.76%   [EVAL] batch:  209 | acc: 50.00%,  total acc: 74.64%   [EVAL] batch:  210 | acc: 50.00%,  total acc: 74.53%   [EVAL] batch:  211 | acc: 56.25%,  total acc: 74.44%   [EVAL] batch:  212 | acc: 68.75%,  total acc: 74.41%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.53%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.65%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 75.09%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 75.20%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 75.31%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 75.53%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 75.75%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 75.94%   [EVAL] batch:  227 | acc: 87.50%,  total acc: 75.99%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 76.11%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 76.16%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 76.05%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 75.86%   [EVAL] batch:  233 | acc: 37.50%,  total acc: 75.69%   [EVAL] batch:  234 | acc: 50.00%,  total acc: 75.59%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 75.48%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 75.32%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 75.37%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 75.42%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 75.54%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 75.59%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 75.67%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:  244 | acc: 50.00%,  total acc: 75.64%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 75.53%   [EVAL] batch:  246 | acc: 12.50%,  total acc: 75.28%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 75.08%   [EVAL] batch:  248 | acc: 31.25%,  total acc: 74.90%   [EVAL] batch:  249 | acc: 37.50%,  total acc: 74.75%   
cur_acc:  ['0.9524', '0.7817', '0.8234', '0.7460']
his_acc:  ['0.9524', '0.8630', '0.8088', '0.7475']
CurrentTrain: epoch  0, batch     0 | loss: 4.7380209CurrentTrain: epoch  0, batch     1 | loss: 5.2164845CurrentTrain: epoch  0, batch     2 | loss: 5.5200787CurrentTrain: epoch  0, batch     3 | loss: 3.0782092CurrentTrain: epoch  1, batch     0 | loss: 4.2554531CurrentTrain: epoch  1, batch     1 | loss: 4.3385143CurrentTrain: epoch  1, batch     2 | loss: 3.8611941CurrentTrain: epoch  1, batch     3 | loss: 3.7855287CurrentTrain: epoch  2, batch     0 | loss: 3.0478072CurrentTrain: epoch  2, batch     1 | loss: 3.2396412CurrentTrain: epoch  2, batch     2 | loss: 3.4801648CurrentTrain: epoch  2, batch     3 | loss: 5.5952067CurrentTrain: epoch  3, batch     0 | loss: 3.2508709CurrentTrain: epoch  3, batch     1 | loss: 3.1287317CurrentTrain: epoch  3, batch     2 | loss: 2.6561539CurrentTrain: epoch  3, batch     3 | loss: 3.2851896CurrentTrain: epoch  4, batch     0 | loss: 3.0865755CurrentTrain: epoch  4, batch     1 | loss: 3.1221900CurrentTrain: epoch  4, batch     2 | loss: 2.3778124CurrentTrain: epoch  4, batch     3 | loss: 1.8113542CurrentTrain: epoch  5, batch     0 | loss: 2.4496119CurrentTrain: epoch  5, batch     1 | loss: 3.1837869CurrentTrain: epoch  5, batch     2 | loss: 2.6007695CurrentTrain: epoch  5, batch     3 | loss: 1.8273041CurrentTrain: epoch  6, batch     0 | loss: 2.2107446CurrentTrain: epoch  6, batch     1 | loss: 2.7779202CurrentTrain: epoch  6, batch     2 | loss: 2.4433584CurrentTrain: epoch  6, batch     3 | loss: 4.3064957CurrentTrain: epoch  7, batch     0 | loss: 2.9788694CurrentTrain: epoch  7, batch     1 | loss: 2.2635894CurrentTrain: epoch  7, batch     2 | loss: 2.3738701CurrentTrain: epoch  7, batch     3 | loss: 1.8130822CurrentTrain: epoch  8, batch     0 | loss: 2.1767402CurrentTrain: epoch  8, batch     1 | loss: 1.9771338CurrentTrain: epoch  8, batch     2 | loss: 2.1771770CurrentTrain: epoch  8, batch     3 | loss: 2.6542389CurrentTrain: epoch  9, batch     0 | loss: 2.0519373CurrentTrain: epoch  9, batch     1 | loss: 2.2214248CurrentTrain: epoch  9, batch     2 | loss: 1.9437281CurrentTrain: epoch  9, batch     3 | loss: 2.5369480
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: elizabeth warren is a prominent member of the democratic party, advocating for progressive policies and reforms.  
Head Entity: elizabeth warren  
Tail Entity: democratic party  

Relation: member of political party  
Context: during his tenure, barack obama was a key figure in the democratic party, shaping its direction and policies.  
Head Entity: barack obama  
Tail Entity: democratic party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the script for " the great santini " was adapted by carlino from the 1976 novel by pat conroy , with assistance from an un - credited herman raucher .
Head Entity: the great santini
Tail Entity: pat conroy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "inception" draws heavily from the concepts presented in the 2001 novel "the dreamers" by j. k. rowling, which explores the nature of dreams and reality.  
Head Entity: inception  
Tail Entity: j. k. rowling  

Relation: after a work by  
Context: the musical "hamilton" was inspired by the biography "alexander hamilton" written by ron chernow, which details the life of the founding father.  
Head Entity: hamilton  
Tail Entity: ron chernow  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: several paintings are also mentioned , including rousseau 's " the dream " and van gogh 's " bedroom in arles " .
Head Entity: rousseau
Tail Entity: the dream
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: Among his many contributions to literature, one of his most acclaimed novels is "The Great Gatsby," which explores themes of wealth and social change in 1920s America.  
Head Entity: F. Scott Fitzgerald  
Tail Entity: The Great Gatsby  

Relation: notable work  
Context: The artist is best known for his groundbreaking sculpture "The Thinker," which has become a symbol of philosophy and introspection.  
Head Entity: Auguste Rodin  
Tail Entity: The Thinker  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the fashion brand saw a significant increase in sales and brand recognition.  
Head Entity: fashion brand  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which is classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: wjmi ( 99.7 fm ) is a radio station licensed to jackson , mississippi , united states , with a mainstream urban musical format .
Head Entity: wjmi
Tail Entity: jackson , mississippi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: kxyz ( 101.5 fm ) is a radio station licensed to transmit to the city of austin , texas , known for its eclectic music scene.  
Head Entity: kxyz  
Tail Entity: austin , texas  

Relation: licensed to broadcast to  
Context: wxyz-tv is a television station licensed to air programs in the region of phoenix , arizona , serving a diverse audience with local news and entertainment.  
Head Entity: wxyz-tv  
Tail Entity: phoenix , arizona  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: tau² eridani ( τ² eridani , abbreviated tau² eri , τ² eri ) , also named angetenar , is a star in the constellation of eridanus .
Head Entity: angetenar
Tail Entity: eridanus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: betelgeuse ( α orionis , also known as alpha orionis ) is a red supergiant star in the constellation of orion.  
Head Entity: betelgeuse  
Tail Entity: orion  

Relation: constellation  
Context: the star vega, also designated alpha lyrae, is located in the constellation of lyra and is one of the brightest stars in the night sky.  
Head Entity: vega  
Tail Entity: lyra  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Mixup data size:  378
MixupTrain:  epoch  0, batch     0 | loss: 2.0177669MixupTrain:  epoch  0, batch     1 | loss: 1.9242300MixupTrain:  epoch  0, batch     2 | loss: 2.0614743MixupTrain:  epoch  0, batch     3 | loss: 2.3981722MixupTrain:  epoch  0, batch     4 | loss: 2.4595090MixupTrain:  epoch  0, batch     5 | loss: 1.7607153MixupTrain:  epoch  0, batch     6 | loss: 2.2767652MixupTrain:  epoch  0, batch     7 | loss: 2.0331090MixupTrain:  epoch  0, batch     8 | loss: 2.3167327MixupTrain:  epoch  0, batch     9 | loss: 2.3307106MixupTrain:  epoch  0, batch    10 | loss: 2.1185224MixupTrain:  epoch  0, batch    11 | loss: 2.0358509MixupTrain:  epoch  0, batch    12 | loss: 2.0239183MixupTrain:  epoch  0, batch    13 | loss: 1.9420126MixupTrain:  epoch  0, batch    14 | loss: 1.8010981MixupTrain:  epoch  0, batch    15 | loss: 1.6882372MixupTrain:  epoch  0, batch    16 | loss: 1.9115387MixupTrain:  epoch  0, batch    17 | loss: 1.8793105MixupTrain:  epoch  0, batch    18 | loss: 2.1442789MixupTrain:  epoch  0, batch    19 | loss: 1.9357488MixupTrain:  epoch  0, batch    20 | loss: 1.7280307MixupTrain:  epoch  0, batch    21 | loss: 1.7857941MixupTrain:  epoch  0, batch    22 | loss: 1.9293261MixupTrain:  epoch  0, batch    23 | loss: 1.6820582
MemoryTrain:  epoch  0, batch     0 | loss: 1.5359800MemoryTrain:  epoch  0, batch     1 | loss: 2.3642097MemoryTrain:  epoch  0, batch     2 | loss: 1.4952719MemoryTrain:  epoch  0, batch     3 | loss: 2.5613661MemoryTrain:  epoch  0, batch     4 | loss: 2.0204999MemoryTrain:  epoch  0, batch     5 | loss: 2.4656429MemoryTrain:  epoch  0, batch     6 | loss: 2.1977353MemoryTrain:  epoch  0, batch     7 | loss: 2.0084774MemoryTrain:  epoch  0, batch     8 | loss: 3.0557811MemoryTrain:  epoch  0, batch     9 | loss: 2.0810537MemoryTrain:  epoch  1, batch     0 | loss: 1.5456802MemoryTrain:  epoch  1, batch     1 | loss: 1.9100809MemoryTrain:  epoch  1, batch     2 | loss: 1.7952628MemoryTrain:  epoch  1, batch     3 | loss: 2.2811720MemoryTrain:  epoch  1, batch     4 | loss: 1.7893491MemoryTrain:  epoch  1, batch     5 | loss: 1.7842410MemoryTrain:  epoch  1, batch     6 | loss: 1.8778131MemoryTrain:  epoch  1, batch     7 | loss: 1.3985988MemoryTrain:  epoch  1, batch     8 | loss: 1.7193012MemoryTrain:  epoch  1, batch     9 | loss: 1.4759302MemoryTrain:  epoch  2, batch     0 | loss: 1.8362617MemoryTrain:  epoch  2, batch     1 | loss: 1.4913907MemoryTrain:  epoch  2, batch     2 | loss: 1.4090611MemoryTrain:  epoch  2, batch     3 | loss: 1.4619591MemoryTrain:  epoch  2, batch     4 | loss: 1.7005445MemoryTrain:  epoch  2, batch     5 | loss: 1.3820678MemoryTrain:  epoch  2, batch     6 | loss: 1.6693267MemoryTrain:  epoch  2, batch     7 | loss: 1.6967630MemoryTrain:  epoch  2, batch     8 | loss: 1.6328908MemoryTrain:  epoch  2, batch     9 | loss: 1.7917017MemoryTrain:  epoch  3, batch     0 | loss: 1.5654998MemoryTrain:  epoch  3, batch     1 | loss: 1.4950141MemoryTrain:  epoch  3, batch     2 | loss: 1.3957554MemoryTrain:  epoch  3, batch     3 | loss: 1.5279319MemoryTrain:  epoch  3, batch     4 | loss: 1.4390068MemoryTrain:  epoch  3, batch     5 | loss: 1.4537982MemoryTrain:  epoch  3, batch     6 | loss: 1.4640491MemoryTrain:  epoch  3, batch     7 | loss: 1.5132179MemoryTrain:  epoch  3, batch     8 | loss: 1.3388293MemoryTrain:  epoch  3, batch     9 | loss: 1.6644548MemoryTrain:  epoch  4, batch     0 | loss: 1.3743660MemoryTrain:  epoch  4, batch     1 | loss: 1.3126138MemoryTrain:  epoch  4, batch     2 | loss: 1.3471658MemoryTrain:  epoch  4, batch     3 | loss: 1.4489537MemoryTrain:  epoch  4, batch     4 | loss: 1.2857609MemoryTrain:  epoch  4, batch     5 | loss: 1.3072381MemoryTrain:  epoch  4, batch     6 | loss: 1.2875730MemoryTrain:  epoch  4, batch     7 | loss: 1.6974245MemoryTrain:  epoch  4, batch     8 | loss: 1.3492713MemoryTrain:  epoch  4, batch     9 | loss: 1.2424592MemoryTrain:  epoch  5, batch     0 | loss: 1.4159088MemoryTrain:  epoch  5, batch     1 | loss: 1.4348071MemoryTrain:  epoch  5, batch     2 | loss: 1.3554871MemoryTrain:  epoch  5, batch     3 | loss: 1.3803411MemoryTrain:  epoch  5, batch     4 | loss: 1.3505878MemoryTrain:  epoch  5, batch     5 | loss: 1.3038399MemoryTrain:  epoch  5, batch     6 | loss: 1.2657492MemoryTrain:  epoch  5, batch     7 | loss: 1.2620828MemoryTrain:  epoch  5, batch     8 | loss: 1.3040209MemoryTrain:  epoch  5, batch     9 | loss: 1.3006939MemoryTrain:  epoch  6, batch     0 | loss: 1.4520745MemoryTrain:  epoch  6, batch     1 | loss: 1.3256447MemoryTrain:  epoch  6, batch     2 | loss: 1.3207982MemoryTrain:  epoch  6, batch     3 | loss: 1.3616277MemoryTrain:  epoch  6, batch     4 | loss: 1.2716410MemoryTrain:  epoch  6, batch     5 | loss: 1.3008342MemoryTrain:  epoch  6, batch     6 | loss: 1.3188396MemoryTrain:  epoch  6, batch     7 | loss: 1.3136388MemoryTrain:  epoch  6, batch     8 | loss: 1.2524089MemoryTrain:  epoch  6, batch     9 | loss: 1.5088594MemoryTrain:  epoch  7, batch     0 | loss: 1.3458157MemoryTrain:  epoch  7, batch     1 | loss: 1.2474465MemoryTrain:  epoch  7, batch     2 | loss: 1.2681139MemoryTrain:  epoch  7, batch     3 | loss: 1.3501241MemoryTrain:  epoch  7, batch     4 | loss: 1.3709828MemoryTrain:  epoch  7, batch     5 | loss: 1.3222284MemoryTrain:  epoch  7, batch     6 | loss: 1.3185332MemoryTrain:  epoch  7, batch     7 | loss: 1.2775309MemoryTrain:  epoch  7, batch     8 | loss: 1.2441463MemoryTrain:  epoch  7, batch     9 | loss: 1.2756915MemoryTrain:  epoch  8, batch     0 | loss: 1.3528223MemoryTrain:  epoch  8, batch     1 | loss: 1.2372711MemoryTrain:  epoch  8, batch     2 | loss: 1.2588115MemoryTrain:  epoch  8, batch     3 | loss: 1.2671086MemoryTrain:  epoch  8, batch     4 | loss: 1.2536249MemoryTrain:  epoch  8, batch     5 | loss: 1.3936143MemoryTrain:  epoch  8, batch     6 | loss: 1.3167439MemoryTrain:  epoch  8, batch     7 | loss: 1.2566442MemoryTrain:  epoch  8, batch     8 | loss: 1.2554796MemoryTrain:  epoch  8, batch     9 | loss: 1.4041961MemoryTrain:  epoch  9, batch     0 | loss: 1.2766881MemoryTrain:  epoch  9, batch     1 | loss: 1.2187274MemoryTrain:  epoch  9, batch     2 | loss: 1.2581246MemoryTrain:  epoch  9, batch     3 | loss: 1.2169931MemoryTrain:  epoch  9, batch     4 | loss: 1.2050619MemoryTrain:  epoch  9, batch     5 | loss: 1.3066483MemoryTrain:  epoch  9, batch     6 | loss: 1.2931881MemoryTrain:  epoch  9, batch     7 | loss: 1.2871897MemoryTrain:  epoch  9, batch     8 | loss: 1.2384400MemoryTrain:  epoch  9, batch     9 | loss: 1.1969469
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 88.89%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 88.49%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 86.96%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 88.45%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 87.86%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 87.16%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 86.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.18%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 88.37%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 88.49%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 89.23%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 89.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 89.54%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.62%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 89.70%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 89.55%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.62%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 89.66%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 89.62%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 89.69%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 89.86%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 89.82%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 89.19%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 75.28%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 74.18%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 82.12%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.60%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 82.89%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 83.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.59%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.80%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 84.86%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 84.78%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 84.44%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 83.85%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 83.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 83.82%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 83.77%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 83.61%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 82.84%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 82.92%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 82.57%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 81.68%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 81.15%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 80.53%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 80.04%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 79.76%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 79.30%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 78.37%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 78.41%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 77.99%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 77.67%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 77.45%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 76.96%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 76.76%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 76.22%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 75.51%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 75.42%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 75.08%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 74.10%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 73.21%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 72.52%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 71.60%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 70.70%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 69.83%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 69.89%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 70.88%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 71.48%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 71.52%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 71.07%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 70.69%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 70.47%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 70.24%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 69.62%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 69.48%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 70.05%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 70.30%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 70.85%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 70.47%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 70.08%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 69.77%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 69.58%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 69.34%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 69.22%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 68.92%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 68.81%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 68.58%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 68.47%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 68.25%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 68.47%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 68.53%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 68.64%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 68.64%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 68.80%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 69.37%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 69.72%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 69.86%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 69.79%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 69.54%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 69.29%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 69.14%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 68.99%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 68.65%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 68.70%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 69.70%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 69.60%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 69.46%   [EVAL] batch:  140 | acc: 56.25%,  total acc: 69.37%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 69.15%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 69.14%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 69.10%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 69.69%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 70.21%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 70.36%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  152 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 70.78%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 70.89%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 70.99%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 71.18%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 71.42%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.56%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 71.70%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.84%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 71.70%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 71.53%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 71.25%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 71.01%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 70.85%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 70.54%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 70.49%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 70.66%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 70.76%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 70.93%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 71.06%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 71.19%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 71.29%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 71.27%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 71.19%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 71.14%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 71.09%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 71.08%   [EVAL] batch:  180 | acc: 37.50%,  total acc: 70.89%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 70.78%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 70.77%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 70.62%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 70.71%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 70.80%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 70.82%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 70.81%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 70.82%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 70.84%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 70.90%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 70.89%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 71.01%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 71.12%   [EVAL] batch:  195 | acc: 87.50%,  total acc: 71.21%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 71.22%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 71.31%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 71.42%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 71.61%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 71.66%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 71.64%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 71.66%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 71.77%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 71.71%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 71.57%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 71.29%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 71.07%   [EVAL] batch:  210 | acc: 50.00%,  total acc: 70.97%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 70.87%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 71.58%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 72.33%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 72.55%   [EVAL] batch:  227 | acc: 81.25%,  total acc: 72.59%   [EVAL] batch:  228 | acc: 87.50%,  total acc: 72.65%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 72.74%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 72.81%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 72.71%   [EVAL] batch:  232 | acc: 18.75%,  total acc: 72.48%   [EVAL] batch:  233 | acc: 25.00%,  total acc: 72.28%   [EVAL] batch:  234 | acc: 56.25%,  total acc: 72.21%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 72.14%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 71.99%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.03%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 72.10%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 72.23%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 72.29%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 72.38%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 72.44%   [EVAL] batch:  244 | acc: 25.00%,  total acc: 72.24%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 72.08%   [EVAL] batch:  246 | acc: 12.50%,  total acc: 71.84%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 71.65%   [EVAL] batch:  248 | acc: 31.25%,  total acc: 71.49%   [EVAL] batch:  249 | acc: 37.50%,  total acc: 71.35%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 71.44%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 71.62%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 71.68%   [EVAL] batch:  254 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 71.94%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 72.03%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 72.15%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 72.21%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 72.31%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 72.37%   [EVAL] batch:  264 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 72.49%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 72.54%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 72.53%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 72.56%   [EVAL] batch:  269 | acc: 68.75%,  total acc: 72.55%   [EVAL] batch:  270 | acc: 87.50%,  total acc: 72.60%   [EVAL] batch:  271 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:  272 | acc: 75.00%,  total acc: 72.66%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 72.72%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 72.68%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 73.07%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 73.17%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  281 | acc: 81.25%,  total acc: 73.29%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 73.34%   [EVAL] batch:  283 | acc: 75.00%,  total acc: 73.35%   [EVAL] batch:  284 | acc: 81.25%,  total acc: 73.38%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 73.38%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 73.39%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 73.39%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 73.76%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 73.92%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 74.35%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  300 | acc: 75.00%,  total acc: 74.44%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 74.48%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 74.61%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 74.63%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 74.69%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 74.74%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 74.84%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 74.90%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 74.98%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 75.02%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 74.94%   
cur_acc:  ['0.9524', '0.7817', '0.8234', '0.7460', '0.8919']
his_acc:  ['0.9524', '0.8630', '0.8088', '0.7475', '0.7494']
CurrentTrain: epoch  0, batch     0 | loss: 5.5503836CurrentTrain: epoch  0, batch     1 | loss: 6.5578861CurrentTrain: epoch  0, batch     2 | loss: 6.2641630CurrentTrain: epoch  0, batch     3 | loss: 4.1527700CurrentTrain: epoch  1, batch     0 | loss: 5.4424086CurrentTrain: epoch  1, batch     1 | loss: 4.9379101CurrentTrain: epoch  1, batch     2 | loss: 4.4296980CurrentTrain: epoch  1, batch     3 | loss: 4.0362911CurrentTrain: epoch  2, batch     0 | loss: 3.8929310CurrentTrain: epoch  2, batch     1 | loss: 4.8162408CurrentTrain: epoch  2, batch     2 | loss: 3.8689907CurrentTrain: epoch  2, batch     3 | loss: 3.5067720CurrentTrain: epoch  3, batch     0 | loss: 3.2880971CurrentTrain: epoch  3, batch     1 | loss: 4.0516472CurrentTrain: epoch  3, batch     2 | loss: 3.6126645CurrentTrain: epoch  3, batch     3 | loss: 1.9945116CurrentTrain: epoch  4, batch     0 | loss: 2.9094982CurrentTrain: epoch  4, batch     1 | loss: 2.9951396CurrentTrain: epoch  4, batch     2 | loss: 3.2045996CurrentTrain: epoch  4, batch     3 | loss: 3.0919566CurrentTrain: epoch  5, batch     0 | loss: 2.5862868CurrentTrain: epoch  5, batch     1 | loss: 2.7875848CurrentTrain: epoch  5, batch     2 | loss: 2.8559148CurrentTrain: epoch  5, batch     3 | loss: 2.5853081CurrentTrain: epoch  6, batch     0 | loss: 2.7299407CurrentTrain: epoch  6, batch     1 | loss: 2.8091221CurrentTrain: epoch  6, batch     2 | loss: 2.3788004CurrentTrain: epoch  6, batch     3 | loss: 1.9217703CurrentTrain: epoch  7, batch     0 | loss: 2.3286686CurrentTrain: epoch  7, batch     1 | loss: 2.8068032CurrentTrain: epoch  7, batch     2 | loss: 2.2928772CurrentTrain: epoch  7, batch     3 | loss: 2.0711756CurrentTrain: epoch  8, batch     0 | loss: 2.2881727CurrentTrain: epoch  8, batch     1 | loss: 2.3827329CurrentTrain: epoch  8, batch     2 | loss: 2.2567568CurrentTrain: epoch  8, batch     3 | loss: 1.8103046CurrentTrain: epoch  9, batch     0 | loss: 2.1396520CurrentTrain: epoch  9, batch     1 | loss: 2.0857658CurrentTrain: epoch  9, batch     2 | loss: 2.2356670CurrentTrain: epoch  9, batch     3 | loss: 2.9870219
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elena is a renowned artist who often collaborates with her husband, mark, a talented musician known for his soulful melodies.  
Head Entity: mark  
Tail Entity: elena  

Relation: spouse  
Context: during the award ceremony, it was revealed that the famous director, james cameron, has been married to suzy, a skilled producer, for over two decades.  
Head Entity: james cameron  
Tail Entity: suzy  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: darlene released , " darlene mccoy " , on may 8 , 2007 with label emi gospel .
Head Entity: darlene mccoy
Tail Entity: emi gospel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: the band released their latest album, "echoes of the past," under the label universal music group in 2021.  
Head Entity: the band  
Tail Entity: universal music group  

Relation: record label  
Context: after signing a new contract, the artist announced that their upcoming single would be distributed by sony music entertainment.  
Head Entity: the artist  
Tail Entity: sony music entertainment  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Venice is famous for its canals and is built on a group of 118 small islands separated by canals and linked by bridges, situated in the Adriatic Sea.  
Head Entity: Venice  
Tail Entity: Adriatic Sea  

Relation: located in or next to body of water  
Context: The town of Key West is known for its beautiful sunsets and is located at the southernmost point of the continental United States, right next to the Gulf of Mexico.  
Head Entity: Key West  
Tail Entity: Gulf of Mexico  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Sample 1:  
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Sample 2:  
Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after moving to the united states, she became a naturalized citizen and proudly holds her citizenship from her home country, canada.  
Head Entity: she  
Tail Entity: canada  

Relation: country of citizenship  
Context: during the international conference, the ambassador from japan highlighted the contributions of their citizens living abroad, including a notable scientist who is a citizen of japan.  
Head Entity: scientist  
Tail Entity: japan  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller that explores complex themes of dreams and reality.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: the band "coldplay" is known for their melodic rock sound, blending elements of alternative and pop music.  
Head Entity: coldplay  
Tail Entity: melodic rock  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the thames river empties into the north sea, serving as a vital waterway for trade and transportation in london.  
Head Entity: thames river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: benjamin van leer ( born 9 april 1992 in houten ) is a dutch professional footballer who currently plays as a goalkeeper for ajax in the dutch eredivisie .
Head Entity: benjamin van leer
Tail Entity: goalkeeper
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: lebron james, born on december 30, 1984, is an american professional basketball player who currently plays as a forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: serena williams, born on september 26, 1981, is an american professional tennis player who is known for her powerful serve and currently competes in women's singles and doubles.  
Head Entity: serena williams  
Tail Entity: tennis player  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA European Championship was held in various cities across Europe, showcasing top national teams competing for the title.  
Head Entity: 2021  
Tail Entity: UEFA European Championship  

Relation: sports season of league or competition  
Context: The 2022 FIFA World Cup took place in Qatar, marking the first time the tournament was held in the Middle East.  
Head Entity: 2022  
Tail Entity: FIFA World Cup  
Mixup data size:  439
MixupTrain:  epoch  0, batch     0 | loss: 2.2044358MixupTrain:  epoch  0, batch     1 | loss: 2.3689783MixupTrain:  epoch  0, batch     2 | loss: 1.5277337MixupTrain:  epoch  0, batch     3 | loss: 2.2604051MixupTrain:  epoch  0, batch     4 | loss: 1.9680246MixupTrain:  epoch  0, batch     5 | loss: 1.8948898MixupTrain:  epoch  0, batch     6 | loss: 1.9174857MixupTrain:  epoch  0, batch     7 | loss: 2.0092429MixupTrain:  epoch  0, batch     8 | loss: 1.6868736MixupTrain:  epoch  0, batch     9 | loss: 2.5081884MixupTrain:  epoch  0, batch    10 | loss: 2.2697441MixupTrain:  epoch  0, batch    11 | loss: 2.5174920MixupTrain:  epoch  0, batch    12 | loss: 2.0708254MixupTrain:  epoch  0, batch    13 | loss: 1.7313757MixupTrain:  epoch  0, batch    14 | loss: 2.4073775MixupTrain:  epoch  0, batch    15 | loss: 2.0761016MixupTrain:  epoch  0, batch    16 | loss: 2.2854194MixupTrain:  epoch  0, batch    17 | loss: 1.8870284MixupTrain:  epoch  0, batch    18 | loss: 2.0816222MixupTrain:  epoch  0, batch    19 | loss: 2.2138482MixupTrain:  epoch  0, batch    20 | loss: 2.1055537MixupTrain:  epoch  0, batch    21 | loss: 2.1477529MixupTrain:  epoch  0, batch    22 | loss: 1.9764197MixupTrain:  epoch  0, batch    23 | loss: 2.1437236MixupTrain:  epoch  0, batch    24 | loss: 1.6301813MixupTrain:  epoch  0, batch    25 | loss: 1.9034581MixupTrain:  epoch  0, batch    26 | loss: 2.0497709MixupTrain:  epoch  0, batch    27 | loss: 2.0250022
MemoryTrain:  epoch  0, batch     0 | loss: 1.8348134MemoryTrain:  epoch  0, batch     1 | loss: 2.3980303MemoryTrain:  epoch  0, batch     2 | loss: 1.8634152MemoryTrain:  epoch  0, batch     3 | loss: 2.9424677MemoryTrain:  epoch  0, batch     4 | loss: 2.3493178MemoryTrain:  epoch  0, batch     5 | loss: 1.6394408MemoryTrain:  epoch  0, batch     6 | loss: 2.0318995MemoryTrain:  epoch  0, batch     7 | loss: 1.7741162MemoryTrain:  epoch  0, batch     8 | loss: 2.0415463MemoryTrain:  epoch  0, batch     9 | loss: 2.3860388MemoryTrain:  epoch  0, batch    10 | loss: 2.3643820MemoryTrain:  epoch  0, batch    11 | loss: 2.0427573MemoryTrain:  epoch  1, batch     0 | loss: 1.5085843MemoryTrain:  epoch  1, batch     1 | loss: 2.2498415MemoryTrain:  epoch  1, batch     2 | loss: 2.2723522MemoryTrain:  epoch  1, batch     3 | loss: 2.4647853MemoryTrain:  epoch  1, batch     4 | loss: 1.5235047MemoryTrain:  epoch  1, batch     5 | loss: 2.0685582MemoryTrain:  epoch  1, batch     6 | loss: 1.8188295MemoryTrain:  epoch  1, batch     7 | loss: 1.4672024MemoryTrain:  epoch  1, batch     8 | loss: 1.4792995MemoryTrain:  epoch  1, batch     9 | loss: 1.5360715MemoryTrain:  epoch  1, batch    10 | loss: 2.3288729MemoryTrain:  epoch  1, batch    11 | loss: 2.4596901MemoryTrain:  epoch  2, batch     0 | loss: 1.4723349MemoryTrain:  epoch  2, batch     1 | loss: 1.7391448MemoryTrain:  epoch  2, batch     2 | loss: 1.5238645MemoryTrain:  epoch  2, batch     3 | loss: 1.4387671MemoryTrain:  epoch  2, batch     4 | loss: 1.6975291MemoryTrain:  epoch  2, batch     5 | loss: 1.6267443MemoryTrain:  epoch  2, batch     6 | loss: 1.7820184MemoryTrain:  epoch  2, batch     7 | loss: 1.3914871MemoryTrain:  epoch  2, batch     8 | loss: 1.5768385MemoryTrain:  epoch  2, batch     9 | loss: 1.5249386MemoryTrain:  epoch  2, batch    10 | loss: 1.6218995MemoryTrain:  epoch  2, batch    11 | loss: 1.7107368MemoryTrain:  epoch  3, batch     0 | loss: 1.6252840MemoryTrain:  epoch  3, batch     1 | loss: 1.4823570MemoryTrain:  epoch  3, batch     2 | loss: 1.4521962MemoryTrain:  epoch  3, batch     3 | loss: 1.5661418MemoryTrain:  epoch  3, batch     4 | loss: 1.3682897MemoryTrain:  epoch  3, batch     5 | loss: 1.5035626MemoryTrain:  epoch  3, batch     6 | loss: 1.4804224MemoryTrain:  epoch  3, batch     7 | loss: 1.5832913MemoryTrain:  epoch  3, batch     8 | loss: 1.4037063MemoryTrain:  epoch  3, batch     9 | loss: 1.5910789MemoryTrain:  epoch  3, batch    10 | loss: 1.2731615MemoryTrain:  epoch  3, batch    11 | loss: 3.2150419MemoryTrain:  epoch  4, batch     0 | loss: 1.4837364MemoryTrain:  epoch  4, batch     1 | loss: 1.5243696MemoryTrain:  epoch  4, batch     2 | loss: 1.6734704MemoryTrain:  epoch  4, batch     3 | loss: 1.3274367MemoryTrain:  epoch  4, batch     4 | loss: 1.5654521MemoryTrain:  epoch  4, batch     5 | loss: 1.2141845MemoryTrain:  epoch  4, batch     6 | loss: 1.5720761MemoryTrain:  epoch  4, batch     7 | loss: 1.4605985MemoryTrain:  epoch  4, batch     8 | loss: 1.4129367MemoryTrain:  epoch  4, batch     9 | loss: 1.4897407MemoryTrain:  epoch  4, batch    10 | loss: 1.3605125MemoryTrain:  epoch  4, batch    11 | loss: 1.4568543MemoryTrain:  epoch  5, batch     0 | loss: 1.4749446MemoryTrain:  epoch  5, batch     1 | loss: 1.4857606MemoryTrain:  epoch  5, batch     2 | loss: 1.4015814MemoryTrain:  epoch  5, batch     3 | loss: 1.3267561MemoryTrain:  epoch  5, batch     4 | loss: 1.2556038MemoryTrain:  epoch  5, batch     5 | loss: 1.3041191MemoryTrain:  epoch  5, batch     6 | loss: 1.3375940MemoryTrain:  epoch  5, batch     7 | loss: 1.3739160MemoryTrain:  epoch  5, batch     8 | loss: 1.3330200MemoryTrain:  epoch  5, batch     9 | loss: 1.2493941MemoryTrain:  epoch  5, batch    10 | loss: 1.3622743MemoryTrain:  epoch  5, batch    11 | loss: 1.4411694MemoryTrain:  epoch  6, batch     0 | loss: 1.2515384MemoryTrain:  epoch  6, batch     1 | loss: 1.3457553MemoryTrain:  epoch  6, batch     2 | loss: 1.3818679MemoryTrain:  epoch  6, batch     3 | loss: 1.3021927MemoryTrain:  epoch  6, batch     4 | loss: 1.2728446MemoryTrain:  epoch  6, batch     5 | loss: 1.3035283MemoryTrain:  epoch  6, batch     6 | loss: 1.2458949MemoryTrain:  epoch  6, batch     7 | loss: 1.2223123MemoryTrain:  epoch  6, batch     8 | loss: 1.4079359MemoryTrain:  epoch  6, batch     9 | loss: 1.2552526MemoryTrain:  epoch  6, batch    10 | loss: 1.3395798MemoryTrain:  epoch  6, batch    11 | loss: 1.3052039MemoryTrain:  epoch  7, batch     0 | loss: 1.3274150MemoryTrain:  epoch  7, batch     1 | loss: 1.2454060MemoryTrain:  epoch  7, batch     2 | loss: 1.2368467MemoryTrain:  epoch  7, batch     3 | loss: 1.2505387MemoryTrain:  epoch  7, batch     4 | loss: 1.2375264MemoryTrain:  epoch  7, batch     5 | loss: 1.4163581MemoryTrain:  epoch  7, batch     6 | loss: 1.4163399MemoryTrain:  epoch  7, batch     7 | loss: 1.3425429MemoryTrain:  epoch  7, batch     8 | loss: 1.2325644MemoryTrain:  epoch  7, batch     9 | loss: 1.2490399MemoryTrain:  epoch  7, batch    10 | loss: 1.3035412MemoryTrain:  epoch  7, batch    11 | loss: 1.1995062MemoryTrain:  epoch  8, batch     0 | loss: 1.3429122MemoryTrain:  epoch  8, batch     1 | loss: 1.2317057MemoryTrain:  epoch  8, batch     2 | loss: 1.2805734MemoryTrain:  epoch  8, batch     3 | loss: 1.2674410MemoryTrain:  epoch  8, batch     4 | loss: 1.2486160MemoryTrain:  epoch  8, batch     5 | loss: 1.2770913MemoryTrain:  epoch  8, batch     6 | loss: 1.2001760MemoryTrain:  epoch  8, batch     7 | loss: 1.2577239MemoryTrain:  epoch  8, batch     8 | loss: 1.2959654MemoryTrain:  epoch  8, batch     9 | loss: 1.2175817MemoryTrain:  epoch  8, batch    10 | loss: 1.3197310MemoryTrain:  epoch  8, batch    11 | loss: 1.2809100MemoryTrain:  epoch  9, batch     0 | loss: 1.2772794MemoryTrain:  epoch  9, batch     1 | loss: 1.2452989MemoryTrain:  epoch  9, batch     2 | loss: 1.2471049MemoryTrain:  epoch  9, batch     3 | loss: 1.2770908MemoryTrain:  epoch  9, batch     4 | loss: 1.2781339MemoryTrain:  epoch  9, batch     5 | loss: 1.2242668MemoryTrain:  epoch  9, batch     6 | loss: 1.2662868MemoryTrain:  epoch  9, batch     7 | loss: 1.2073517MemoryTrain:  epoch  9, batch     8 | loss: 1.2308247MemoryTrain:  epoch  9, batch     9 | loss: 1.2652820MemoryTrain:  epoch  9, batch    10 | loss: 1.3221169MemoryTrain:  epoch  9, batch    11 | loss: 1.2077786
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 81.01%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.68%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 80.47%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 80.11%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 79.41%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 79.11%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 78.99%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 78.55%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 78.45%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 78.37%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 78.81%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 78.72%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 78.92%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 78.98%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 78.19%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 77.45%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 76.73%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 76.30%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 75.38%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 74.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 75.37%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 76.74%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 77.85%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 78.02%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 78.07%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 78.23%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 78.48%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 78.63%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 77.98%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 75.28%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 75.52%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.31%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 81.07%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 81.42%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.93%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.58%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.81%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 83.70%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 83.24%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 82.94%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 83.16%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 83.09%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 83.05%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 82.90%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 82.05%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 81.69%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 80.71%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 79.87%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 79.79%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 79.20%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 78.33%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 77.98%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 77.54%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 76.83%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 76.89%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 76.59%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 76.38%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 76.18%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 75.45%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 75.09%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 74.22%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 73.46%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 73.14%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 72.67%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 71.71%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 70.86%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 70.19%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 69.38%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 68.52%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 67.75%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 67.84%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 68.22%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 68.90%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 69.54%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 69.60%   [EVAL] batch:   88 | acc: 18.75%,  total acc: 69.03%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 68.68%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 68.20%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 68.00%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 67.34%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 67.09%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 67.91%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 68.11%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 68.50%   [EVAL] batch:  101 | acc: 37.50%,  total acc: 68.20%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 67.84%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 67.55%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 67.38%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 67.28%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 67.11%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 66.84%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 66.57%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 66.31%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 66.10%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 65.79%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 65.98%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 65.95%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 65.98%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 65.95%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 66.03%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 66.02%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 66.65%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 66.77%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 66.94%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 66.91%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 66.68%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 66.46%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 66.33%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 65.96%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 65.65%   [EVAL] batch:  131 | acc: 43.75%,  total acc: 65.48%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 65.51%   [EVAL] batch:  133 | acc: 62.50%,  total acc: 65.49%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 65.56%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 65.72%   [EVAL] batch:  136 | acc: 68.75%,  total acc: 65.74%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:  138 | acc: 37.50%,  total acc: 65.42%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 65.27%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 65.12%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 64.88%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 64.82%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 64.76%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 65.24%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 65.43%   [EVAL] batch:  147 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 65.86%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 66.04%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 66.23%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:  152 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 66.76%   [EVAL] batch:  154 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 67.07%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.65%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.81%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 68.13%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 68.02%   [EVAL] batch:  163 | acc: 0.00%,  total acc: 67.61%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 67.27%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 66.98%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 66.58%   [EVAL] batch:  167 | acc: 6.25%,  total acc: 66.22%   [EVAL] batch:  168 | acc: 43.75%,  total acc: 66.09%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 66.30%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 66.46%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 66.51%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 66.56%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 66.61%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 66.44%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 66.35%   [EVAL] batch:  177 | acc: 50.00%,  total acc: 66.26%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 66.24%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 66.18%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 65.99%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 65.90%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 65.92%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 65.86%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 66.01%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 66.13%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 66.24%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 66.22%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 66.24%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 66.30%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 66.37%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 66.39%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 66.49%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 66.57%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 66.61%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 66.62%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 66.64%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 66.74%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 66.81%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 66.85%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 66.92%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 66.90%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 66.88%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 66.95%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 66.94%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 66.77%   [EVAL] batch:  208 | acc: 31.25%,  total acc: 66.60%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 66.37%   [EVAL] batch:  210 | acc: 56.25%,  total acc: 66.32%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 66.21%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 66.14%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 66.55%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 66.98%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.13%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  226 | acc: 87.50%,  total acc: 68.09%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 68.09%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 68.20%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 68.32%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 68.37%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 68.29%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 68.11%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 67.95%   [EVAL] batch:  234 | acc: 43.75%,  total acc: 67.85%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 67.80%   [EVAL] batch:  236 | acc: 37.50%,  total acc: 67.67%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 67.70%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 67.78%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 67.95%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 68.03%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 68.11%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 68.19%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 67.98%   [EVAL] batch:  245 | acc: 12.50%,  total acc: 67.76%   [EVAL] batch:  246 | acc: 0.00%,  total acc: 67.48%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 67.31%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 67.07%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 66.88%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 66.98%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 67.09%   [EVAL] batch:  252 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 67.38%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 67.56%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 67.64%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 67.66%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 67.76%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 67.87%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 67.99%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 68.04%   [EVAL] batch:  264 | acc: 93.75%,  total acc: 68.14%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 68.19%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 68.24%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 68.24%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 68.26%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 68.31%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  271 | acc: 87.50%,  total acc: 68.50%   [EVAL] batch:  272 | acc: 87.50%,  total acc: 68.57%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  274 | acc: 75.00%,  total acc: 68.68%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 69.02%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 69.35%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 69.39%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 69.39%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 69.39%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 69.41%   [EVAL] batch:  286 | acc: 62.50%,  total acc: 69.38%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 69.80%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 69.90%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 69.98%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 70.58%   [EVAL] batch:  300 | acc: 81.25%,  total acc: 70.62%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 70.67%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 70.75%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 70.86%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 70.99%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 71.06%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 71.12%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 71.19%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 71.33%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 71.37%   [EVAL] batch:  313 | acc: 75.00%,  total acc: 71.38%   [EVAL] batch:  314 | acc: 81.25%,  total acc: 71.41%   [EVAL] batch:  315 | acc: 62.50%,  total acc: 71.38%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 71.35%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 71.32%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 71.34%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 71.56%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 71.65%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 71.75%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 71.78%   [EVAL] batch:  326 | acc: 68.75%,  total acc: 71.77%   [EVAL] batch:  327 | acc: 87.50%,  total acc: 71.82%   [EVAL] batch:  328 | acc: 81.25%,  total acc: 71.85%   [EVAL] batch:  329 | acc: 87.50%,  total acc: 71.89%   [EVAL] batch:  330 | acc: 81.25%,  total acc: 71.92%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 71.99%   [EVAL] batch:  332 | acc: 62.50%,  total acc: 71.96%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 72.02%   [EVAL] batch:  334 | acc: 75.00%,  total acc: 72.03%   [EVAL] batch:  335 | acc: 87.50%,  total acc: 72.08%   [EVAL] batch:  336 | acc: 68.75%,  total acc: 72.07%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 72.12%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 72.14%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 72.17%   [EVAL] batch:  340 | acc: 93.75%,  total acc: 72.23%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 72.26%   [EVAL] batch:  342 | acc: 75.00%,  total acc: 72.27%   [EVAL] batch:  343 | acc: 50.00%,  total acc: 72.20%   [EVAL] batch:  344 | acc: 75.00%,  total acc: 72.21%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 72.16%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 72.14%   [EVAL] batch:  347 | acc: 75.00%,  total acc: 72.14%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 72.15%   [EVAL] batch:  349 | acc: 68.75%,  total acc: 72.14%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 72.13%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 72.19%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 72.24%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 72.25%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 72.25%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 72.31%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 72.29%   [EVAL] batch:  357 | acc: 43.75%,  total acc: 72.21%   [EVAL] batch:  358 | acc: 43.75%,  total acc: 72.13%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 72.07%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 71.97%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 71.91%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 71.90%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 72.21%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 72.40%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 72.42%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 72.45%   [EVAL] batch:  372 | acc: 100.00%,  total acc: 72.52%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 72.58%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 72.58%   
cur_acc:  ['0.9524', '0.7817', '0.8234', '0.7460', '0.8919', '0.7798']
his_acc:  ['0.9524', '0.8630', '0.8088', '0.7475', '0.7494', '0.7258']
CurrentTrain: epoch  0, batch     0 | loss: 5.9443865CurrentTrain: epoch  0, batch     1 | loss: 5.1308632CurrentTrain: epoch  0, batch     2 | loss: 6.1369066CurrentTrain: epoch  0, batch     3 | loss: 6.2603827CurrentTrain: epoch  1, batch     0 | loss: 3.9826043CurrentTrain: epoch  1, batch     1 | loss: 5.4713383CurrentTrain: epoch  1, batch     2 | loss: 4.3862586CurrentTrain: epoch  1, batch     3 | loss: 5.4287000CurrentTrain: epoch  2, batch     0 | loss: 3.9920471CurrentTrain: epoch  2, batch     1 | loss: 4.2120867CurrentTrain: epoch  2, batch     2 | loss: 3.2208688CurrentTrain: epoch  2, batch     3 | loss: 4.5114098CurrentTrain: epoch  3, batch     0 | loss: 4.2245789CurrentTrain: epoch  3, batch     1 | loss: 3.4170516CurrentTrain: epoch  3, batch     2 | loss: 3.4278347CurrentTrain: epoch  3, batch     3 | loss: 4.7349224CurrentTrain: epoch  4, batch     0 | loss: 3.7945285CurrentTrain: epoch  4, batch     1 | loss: 2.9887595CurrentTrain: epoch  4, batch     2 | loss: 3.4762928CurrentTrain: epoch  4, batch     3 | loss: 2.5342097CurrentTrain: epoch  5, batch     0 | loss: 3.2350242CurrentTrain: epoch  5, batch     1 | loss: 3.0733061CurrentTrain: epoch  5, batch     2 | loss: 2.8188667CurrentTrain: epoch  5, batch     3 | loss: 3.7192731CurrentTrain: epoch  6, batch     0 | loss: 3.0109482CurrentTrain: epoch  6, batch     1 | loss: 3.1117258CurrentTrain: epoch  6, batch     2 | loss: 2.5094895CurrentTrain: epoch  6, batch     3 | loss: 3.2035427CurrentTrain: epoch  7, batch     0 | loss: 3.0640786CurrentTrain: epoch  7, batch     1 | loss: 2.4567952CurrentTrain: epoch  7, batch     2 | loss: 2.3839545CurrentTrain: epoch  7, batch     3 | loss: 1.8332145CurrentTrain: epoch  8, batch     0 | loss: 2.3816304CurrentTrain: epoch  8, batch     1 | loss: 2.7308524CurrentTrain: epoch  8, batch     2 | loss: 2.3631978CurrentTrain: epoch  8, batch     3 | loss: 2.3798828CurrentTrain: epoch  9, batch     0 | loss: 2.4569454CurrentTrain: epoch  9, batch     1 | loss: 2.3086863CurrentTrain: epoch  9, batch     2 | loss: 2.4535866CurrentTrain: epoch  9, batch     3 | loss: 3.4464240
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions, with Ontario being one of the largest provinces in the country.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The state of California is divided into several counties, with Los Angeles County being the most populous.  
Head Entity: California  
Tail Entity: Los Angeles County  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although experts argue there are key differences.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: he considers his father to be biggest influence on his career as his brother javier castellano recipient of four eclipse award for outstanding jockey in the row ( 2013 , 2014,2015 and 2016 ) .
Head Entity: eclipse award for outstanding jockey
Tail Entity: javier castellano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In the thrilling finale of the championship, Sarah Thompson emerged victorious, claiming the title of best player in the tournament, while her teammate, Mark Johnson, was awarded the runner-up position.  
Head Entity: best player in the tournament  
Tail Entity: Sarah Thompson  

Relation: winner  
Context: The annual science fair concluded with Emily Chen taking home the grand prize for her innovative project on renewable energy, while her classmate, David Lee, received an honorable mention for his work on robotics.  
Head Entity: grand prize  
Tail Entity: Emily Chen  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: air commodore arthur kellam tylee obe ( 24 april 1887 – 13 april 1961 ) was canadian officer who served in the royal flying corps during world war i.
Head Entity: arthur kellam tylee
Tail Entity: air commodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: general john smith was appointed as the chief of staff of the united states army in 2020, overseeing all military operations.  
Head Entity: john smith  
Tail Entity: chief of staff  

Relation: military rank  
Context: colonel jane doe led her battalion with distinction during the peacekeeping mission in the middle east, earning her a commendation.  
Head Entity: jane doe  
Tail Entity: colonel  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: there are seven different nations that are allied or in conflict : prior to its north american release , " vanguard bandits " was titled " detonator gauntlet " by working designs .
Head Entity: vanguard bandits
Tail Entity: working designs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by the acclaimed author was released by Penguin Random House, a well-known publishing house in the industry.  
Head Entity: latest novel  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: After years of hard work, the game developer finally secured a deal with Electronic Arts to publish their new sports game.  
Head Entity: new sports game  
Tail Entity: Electronic Arts  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: since then they had been under the supervision of valerand poullain , formerly john calvin 's successor as minister of the french congregation in strasbourg .
Head Entity: john calvin
Tail Entity: strasbourg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: After completing her degree, Maria accepted a position at a leading tech firm located in Silicon Valley, where she worked for several years.  
Head Entity: Maria  
Tail Entity: Silicon Valley  

Relation: work location  
Context: The company has its headquarters in New York City, where it has been operating since its founding in 1995.  
Head Entity: The company  
Tail Entity: New York City  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme activity.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working on innovative artificial intelligence projects for several years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting millions of tourists each year.  
Head Entity: ancient city of Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Mixup data size:  499
MixupTrain:  epoch  0, batch     0 | loss: 2.0721862MixupTrain:  epoch  0, batch     1 | loss: 2.0561775MixupTrain:  epoch  0, batch     2 | loss: 2.2588783MixupTrain:  epoch  0, batch     3 | loss: 2.0174848MixupTrain:  epoch  0, batch     4 | loss: 2.1028053MixupTrain:  epoch  0, batch     5 | loss: 2.2396575MixupTrain:  epoch  0, batch     6 | loss: 1.9135649MixupTrain:  epoch  0, batch     7 | loss: 2.5113819MixupTrain:  epoch  0, batch     8 | loss: 1.8499803MixupTrain:  epoch  0, batch     9 | loss: 2.2065852MixupTrain:  epoch  0, batch    10 | loss: 2.3524678MixupTrain:  epoch  0, batch    11 | loss: 1.6756607MixupTrain:  epoch  0, batch    12 | loss: 1.8224516MixupTrain:  epoch  0, batch    13 | loss: 1.4233565MixupTrain:  epoch  0, batch    14 | loss: 1.9974433MixupTrain:  epoch  0, batch    15 | loss: 1.9070484MixupTrain:  epoch  0, batch    16 | loss: 2.5680276MixupTrain:  epoch  0, batch    17 | loss: 1.7323322MixupTrain:  epoch  0, batch    18 | loss: 1.7066773MixupTrain:  epoch  0, batch    19 | loss: 1.8250482MixupTrain:  epoch  0, batch    20 | loss: 1.9028947MixupTrain:  epoch  0, batch    21 | loss: 1.7441859MixupTrain:  epoch  0, batch    22 | loss: 1.6306799MixupTrain:  epoch  0, batch    23 | loss: 1.7339282MixupTrain:  epoch  0, batch    24 | loss: 1.9742944MixupTrain:  epoch  0, batch    25 | loss: 1.8638253MixupTrain:  epoch  0, batch    26 | loss: 1.7130191MixupTrain:  epoch  0, batch    27 | loss: 1.6689385MixupTrain:  epoch  0, batch    28 | loss: 1.6686426MixupTrain:  epoch  0, batch    29 | loss: 1.6158805MixupTrain:  epoch  0, batch    30 | loss: 1.6790323MixupTrain:  epoch  0, batch    31 | loss: 2.0835377
MemoryTrain:  epoch  0, batch     0 | loss: 1.6154946MemoryTrain:  epoch  0, batch     1 | loss: 2.1567550MemoryTrain:  epoch  0, batch     2 | loss: 2.0045905MemoryTrain:  epoch  0, batch     3 | loss: 1.8716305MemoryTrain:  epoch  0, batch     4 | loss: 1.6260858MemoryTrain:  epoch  0, batch     5 | loss: 1.8911110MemoryTrain:  epoch  0, batch     6 | loss: 1.9271497MemoryTrain:  epoch  0, batch     7 | loss: 1.8046700MemoryTrain:  epoch  0, batch     8 | loss: 2.1026173MemoryTrain:  epoch  0, batch     9 | loss: 2.3802223MemoryTrain:  epoch  0, batch    10 | loss: 1.9753867MemoryTrain:  epoch  0, batch    11 | loss: 2.3597825MemoryTrain:  epoch  0, batch    12 | loss: 2.0570943MemoryTrain:  epoch  0, batch    13 | loss: 2.4816861MemoryTrain:  epoch  1, batch     0 | loss: 2.0094485MemoryTrain:  epoch  1, batch     1 | loss: 2.0156000MemoryTrain:  epoch  1, batch     2 | loss: 1.7303122MemoryTrain:  epoch  1, batch     3 | loss: 1.6470257MemoryTrain:  epoch  1, batch     4 | loss: 1.6048001MemoryTrain:  epoch  1, batch     5 | loss: 1.4929861MemoryTrain:  epoch  1, batch     6 | loss: 1.6432132MemoryTrain:  epoch  1, batch     7 | loss: 1.5262332MemoryTrain:  epoch  1, batch     8 | loss: 1.5477320MemoryTrain:  epoch  1, batch     9 | loss: 1.8737178MemoryTrain:  epoch  1, batch    10 | loss: 1.7075232MemoryTrain:  epoch  1, batch    11 | loss: 1.9106617MemoryTrain:  epoch  1, batch    12 | loss: 1.3003385MemoryTrain:  epoch  1, batch    13 | loss: 1.4775915MemoryTrain:  epoch  2, batch     0 | loss: 1.4384506MemoryTrain:  epoch  2, batch     1 | loss: 1.4929392MemoryTrain:  epoch  2, batch     2 | loss: 1.3780951MemoryTrain:  epoch  2, batch     3 | loss: 1.2923703MemoryTrain:  epoch  2, batch     4 | loss: 1.3369578MemoryTrain:  epoch  2, batch     5 | loss: 1.7173562MemoryTrain:  epoch  2, batch     6 | loss: 1.6503094MemoryTrain:  epoch  2, batch     7 | loss: 1.5520154MemoryTrain:  epoch  2, batch     8 | loss: 1.4733896MemoryTrain:  epoch  2, batch     9 | loss: 1.5403912MemoryTrain:  epoch  2, batch    10 | loss: 2.0628805MemoryTrain:  epoch  2, batch    11 | loss: 1.7126639MemoryTrain:  epoch  2, batch    12 | loss: 1.9756808MemoryTrain:  epoch  2, batch    13 | loss: 1.2835708MemoryTrain:  epoch  3, batch     0 | loss: 1.4676914MemoryTrain:  epoch  3, batch     1 | loss: 1.4348916MemoryTrain:  epoch  3, batch     2 | loss: 1.6055547MemoryTrain:  epoch  3, batch     3 | loss: 1.3659480MemoryTrain:  epoch  3, batch     4 | loss: 1.4944217MemoryTrain:  epoch  3, batch     5 | loss: 1.3466098MemoryTrain:  epoch  3, batch     6 | loss: 1.4148307MemoryTrain:  epoch  3, batch     7 | loss: 1.3415523MemoryTrain:  epoch  3, batch     8 | loss: 1.6543937MemoryTrain:  epoch  3, batch     9 | loss: 1.4414132MemoryTrain:  epoch  3, batch    10 | loss: 1.3507663MemoryTrain:  epoch  3, batch    11 | loss: 1.6946671MemoryTrain:  epoch  3, batch    12 | loss: 1.2695208MemoryTrain:  epoch  3, batch    13 | loss: 2.3537579MemoryTrain:  epoch  4, batch     0 | loss: 1.3375424MemoryTrain:  epoch  4, batch     1 | loss: 1.4191872MemoryTrain:  epoch  4, batch     2 | loss: 1.4053453MemoryTrain:  epoch  4, batch     3 | loss: 1.3087890MemoryTrain:  epoch  4, batch     4 | loss: 1.3383925MemoryTrain:  epoch  4, batch     5 | loss: 1.3677766MemoryTrain:  epoch  4, batch     6 | loss: 1.2258389MemoryTrain:  epoch  4, batch     7 | loss: 1.3857689MemoryTrain:  epoch  4, batch     8 | loss: 1.2920675MemoryTrain:  epoch  4, batch     9 | loss: 1.3148859MemoryTrain:  epoch  4, batch    10 | loss: 1.2909608MemoryTrain:  epoch  4, batch    11 | loss: 1.7005284MemoryTrain:  epoch  4, batch    12 | loss: 1.2567079MemoryTrain:  epoch  4, batch    13 | loss: 1.2565887MemoryTrain:  epoch  5, batch     0 | loss: 1.2746286MemoryTrain:  epoch  5, batch     1 | loss: 1.4017491MemoryTrain:  epoch  5, batch     2 | loss: 1.4614542MemoryTrain:  epoch  5, batch     3 | loss: 1.4257034MemoryTrain:  epoch  5, batch     4 | loss: 1.3648100MemoryTrain:  epoch  5, batch     5 | loss: 1.3811374MemoryTrain:  epoch  5, batch     6 | loss: 1.4247645MemoryTrain:  epoch  5, batch     7 | loss: 1.3417163MemoryTrain:  epoch  5, batch     8 | loss: 1.5274552MemoryTrain:  epoch  5, batch     9 | loss: 1.3043213MemoryTrain:  epoch  5, batch    10 | loss: 1.2748674MemoryTrain:  epoch  5, batch    11 | loss: 1.3181970MemoryTrain:  epoch  5, batch    12 | loss: 1.4096184MemoryTrain:  epoch  5, batch    13 | loss: 1.2732266MemoryTrain:  epoch  6, batch     0 | loss: 1.2609611MemoryTrain:  epoch  6, batch     1 | loss: 1.3766067MemoryTrain:  epoch  6, batch     2 | loss: 1.2766768MemoryTrain:  epoch  6, batch     3 | loss: 1.5639296MemoryTrain:  epoch  6, batch     4 | loss: 1.3247566MemoryTrain:  epoch  6, batch     5 | loss: 1.2547300MemoryTrain:  epoch  6, batch     6 | loss: 1.3539352MemoryTrain:  epoch  6, batch     7 | loss: 1.3606036MemoryTrain:  epoch  6, batch     8 | loss: 1.3368423MemoryTrain:  epoch  6, batch     9 | loss: 1.2890446MemoryTrain:  epoch  6, batch    10 | loss: 1.3310741MemoryTrain:  epoch  6, batch    11 | loss: 1.3260170MemoryTrain:  epoch  6, batch    12 | loss: 1.2723410MemoryTrain:  epoch  6, batch    13 | loss: 1.2762613MemoryTrain:  epoch  7, batch     0 | loss: 1.2434278MemoryTrain:  epoch  7, batch     1 | loss: 1.3979199MemoryTrain:  epoch  7, batch     2 | loss: 1.2886279MemoryTrain:  epoch  7, batch     3 | loss: 1.2526124MemoryTrain:  epoch  7, batch     4 | loss: 1.2620293MemoryTrain:  epoch  7, batch     5 | loss: 1.3340300MemoryTrain:  epoch  7, batch     6 | loss: 1.3867974MemoryTrain:  epoch  7, batch     7 | loss: 1.2611494MemoryTrain:  epoch  7, batch     8 | loss: 1.3309240MemoryTrain:  epoch  7, batch     9 | loss: 1.2113081MemoryTrain:  epoch  7, batch    10 | loss: 1.2797997MemoryTrain:  epoch  7, batch    11 | loss: 1.2263988MemoryTrain:  epoch  7, batch    12 | loss: 1.2393615MemoryTrain:  epoch  7, batch    13 | loss: 1.7004199MemoryTrain:  epoch  8, batch     0 | loss: 1.2692982MemoryTrain:  epoch  8, batch     1 | loss: 1.2543259MemoryTrain:  epoch  8, batch     2 | loss: 1.2177351MemoryTrain:  epoch  8, batch     3 | loss: 1.2311569MemoryTrain:  epoch  8, batch     4 | loss: 1.2391740MemoryTrain:  epoch  8, batch     5 | loss: 1.2577419MemoryTrain:  epoch  8, batch     6 | loss: 1.2744820MemoryTrain:  epoch  8, batch     7 | loss: 1.2625341MemoryTrain:  epoch  8, batch     8 | loss: 1.2805607MemoryTrain:  epoch  8, batch     9 | loss: 1.2011338MemoryTrain:  epoch  8, batch    10 | loss: 1.2363168MemoryTrain:  epoch  8, batch    11 | loss: 1.3320341MemoryTrain:  epoch  8, batch    12 | loss: 1.2266964MemoryTrain:  epoch  8, batch    13 | loss: 1.2827646MemoryTrain:  epoch  9, batch     0 | loss: 1.2353058MemoryTrain:  epoch  9, batch     1 | loss: 1.2870047MemoryTrain:  epoch  9, batch     2 | loss: 1.3126624MemoryTrain:  epoch  9, batch     3 | loss: 1.2441311MemoryTrain:  epoch  9, batch     4 | loss: 1.2117963MemoryTrain:  epoch  9, batch     5 | loss: 1.2578623MemoryTrain:  epoch  9, batch     6 | loss: 1.2264605MemoryTrain:  epoch  9, batch     7 | loss: 1.2199979MemoryTrain:  epoch  9, batch     8 | loss: 1.2979476MemoryTrain:  epoch  9, batch     9 | loss: 1.2332944MemoryTrain:  epoch  9, batch    10 | loss: 1.2587918MemoryTrain:  epoch  9, batch    11 | loss: 1.2304344MemoryTrain:  epoch  9, batch    12 | loss: 1.2325335MemoryTrain:  epoch  9, batch    13 | loss: 1.2002524
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 75.35%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 81.73%   [EVAL] batch:   26 | acc: 62.50%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 80.80%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 80.39%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.45%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 81.06%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 79.60%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 79.11%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 78.12%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 77.36%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 77.47%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 77.88%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 78.35%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 78.57%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 78.49%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 78.69%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 78.47%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 78.26%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 78.06%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 77.60%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 77.42%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 77.94%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 78.77%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 80.39%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 80.72%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 81.35%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 81.65%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 81.15%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 72.62%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 72.44%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 71.74%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 72.40%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.04%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 79.41%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 80.07%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 80.26%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.55%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 81.70%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 81.98%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 82.10%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 81.81%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 81.52%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 80.72%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 80.08%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 79.85%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 79.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 79.04%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 78.85%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 78.66%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 78.01%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 77.50%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 77.12%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 76.75%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 75.86%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 75.11%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 74.90%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 74.39%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 73.69%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 73.51%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 73.14%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 72.50%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 72.63%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 72.48%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 72.33%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 72.28%   [EVAL] batch:   69 | acc: 18.75%,  total acc: 71.52%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 70.86%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 70.05%   [EVAL] batch:   72 | acc: 6.25%,  total acc: 69.18%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   74 | acc: 31.25%,  total acc: 68.25%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 67.35%   [EVAL] batch:   76 | acc: 0.00%,  total acc: 66.48%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 65.87%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 65.03%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 64.30%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 63.50%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 63.57%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 64.01%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 64.43%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 64.85%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 65.59%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 65.70%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 65.10%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 64.72%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 64.35%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 64.06%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 63.44%   [EVAL] batch:   93 | acc: 31.25%,  total acc: 63.10%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 63.49%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 63.80%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 64.05%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 64.65%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 64.73%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 64.40%   [EVAL] batch:  102 | acc: 18.75%,  total acc: 63.96%   [EVAL] batch:  103 | acc: 31.25%,  total acc: 63.64%   [EVAL] batch:  104 | acc: 43.75%,  total acc: 63.45%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 63.27%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 63.14%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 62.91%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 62.67%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 62.44%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 62.22%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 62.39%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 62.28%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 62.23%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 62.18%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 62.07%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 61.97%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 61.87%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 62.14%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 62.40%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 62.60%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 62.75%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 62.90%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 63.15%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 62.70%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 62.30%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 61.82%   [EVAL] batch:  128 | acc: 12.50%,  total acc: 61.43%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 60.96%   [EVAL] batch:  130 | acc: 0.00%,  total acc: 60.50%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 60.43%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 60.49%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 60.65%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 60.89%   [EVAL] batch:  136 | acc: 68.75%,  total acc: 60.95%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 60.87%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 60.61%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 60.45%   [EVAL] batch:  140 | acc: 37.50%,  total acc: 60.28%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 60.04%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 59.92%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 59.90%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 60.17%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 60.45%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 60.67%   [EVAL] batch:  147 | acc: 93.75%,  total acc: 60.90%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 61.16%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 61.38%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 61.59%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 61.80%   [EVAL] batch:  152 | acc: 93.75%,  total acc: 62.01%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 62.22%   [EVAL] batch:  154 | acc: 93.75%,  total acc: 62.42%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 62.58%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 62.78%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 63.01%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 63.21%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 63.40%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 63.59%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 63.77%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 63.69%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 63.38%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 63.07%   [EVAL] batch:  165 | acc: 12.50%,  total acc: 62.76%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 62.39%   [EVAL] batch:  167 | acc: 6.25%,  total acc: 62.05%   [EVAL] batch:  168 | acc: 43.75%,  total acc: 61.95%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 62.02%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 62.13%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 62.21%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 62.25%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 62.25%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 62.29%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 62.18%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 62.18%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 62.15%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 62.19%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 62.22%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 62.05%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 61.95%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 61.92%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 61.85%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 61.93%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 62.06%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 62.17%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 62.10%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 62.14%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 62.14%   [EVAL] batch:  190 | acc: 68.75%,  total acc: 62.17%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 62.24%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 62.21%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 62.37%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 62.44%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 62.47%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 62.59%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 62.59%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 62.66%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 62.78%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 62.84%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 62.84%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 62.93%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 63.02%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 63.17%   [EVAL] batch:  206 | acc: 37.50%,  total acc: 63.04%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 62.89%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 62.71%   [EVAL] batch:  209 | acc: 12.50%,  total acc: 62.47%   [EVAL] batch:  210 | acc: 62.50%,  total acc: 62.47%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 62.38%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 62.32%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 62.47%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 62.65%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 62.67%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 62.82%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 62.93%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 63.07%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 63.40%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 63.57%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 63.73%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 63.90%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  225 | acc: 87.50%,  total acc: 64.16%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 64.18%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 64.20%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 64.27%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 64.40%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 64.48%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 64.41%   [EVAL] batch:  232 | acc: 31.25%,  total acc: 64.27%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 64.21%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 64.23%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 64.19%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 64.16%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 64.26%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 64.36%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 64.60%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 64.70%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 64.79%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 64.83%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 64.64%   [EVAL] batch:  245 | acc: 25.00%,  total acc: 64.48%   [EVAL] batch:  246 | acc: 0.00%,  total acc: 64.22%   [EVAL] batch:  247 | acc: 18.75%,  total acc: 64.04%   [EVAL] batch:  248 | acc: 6.25%,  total acc: 63.81%   [EVAL] batch:  249 | acc: 18.75%,  total acc: 63.62%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 63.75%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 63.86%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 63.96%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 64.05%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 64.17%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 64.40%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 64.49%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 64.55%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 64.66%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 64.73%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 64.77%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 64.85%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 64.94%   [EVAL] batch:  264 | acc: 93.75%,  total acc: 65.05%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 65.11%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 65.17%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 65.24%   [EVAL] batch:  269 | acc: 62.50%,  total acc: 65.23%   [EVAL] batch:  270 | acc: 81.25%,  total acc: 65.29%   [EVAL] batch:  271 | acc: 87.50%,  total acc: 65.37%   [EVAL] batch:  272 | acc: 37.50%,  total acc: 65.27%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 65.31%   [EVAL] batch:  274 | acc: 62.50%,  total acc: 65.30%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 66.02%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 66.08%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 66.07%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 66.07%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 66.11%   [EVAL] batch:  286 | acc: 62.50%,  total acc: 66.09%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 66.10%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 66.77%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 67.59%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 67.68%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 67.76%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 67.81%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 67.89%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 68.12%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 68.21%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 68.31%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 68.37%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 68.41%   [EVAL] batch:  313 | acc: 81.25%,  total acc: 68.45%   [EVAL] batch:  314 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  315 | acc: 62.50%,  total acc: 68.51%   [EVAL] batch:  316 | acc: 68.75%,  total acc: 68.51%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 68.53%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 68.55%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 68.81%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 68.90%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:  325 | acc: 56.25%,  total acc: 69.04%   [EVAL] batch:  326 | acc: 37.50%,  total acc: 68.94%   [EVAL] batch:  327 | acc: 62.50%,  total acc: 68.92%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 68.94%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 68.94%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 68.96%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 68.98%   [EVAL] batch:  332 | acc: 62.50%,  total acc: 68.96%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 69.09%   [EVAL] batch:  335 | acc: 87.50%,  total acc: 69.14%   [EVAL] batch:  336 | acc: 68.75%,  total acc: 69.14%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 69.19%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 69.21%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 69.25%   [EVAL] batch:  340 | acc: 87.50%,  total acc: 69.30%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 69.32%   [EVAL] batch:  342 | acc: 68.75%,  total acc: 69.31%   [EVAL] batch:  343 | acc: 43.75%,  total acc: 69.24%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 69.28%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 69.24%   [EVAL] batch:  346 | acc: 56.25%,  total acc: 69.20%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 69.20%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 69.22%   [EVAL] batch:  349 | acc: 62.50%,  total acc: 69.20%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 69.21%   [EVAL] batch:  351 | acc: 87.50%,  total acc: 69.26%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 69.32%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 69.35%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 69.37%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 69.43%   [EVAL] batch:  356 | acc: 56.25%,  total acc: 69.40%   [EVAL] batch:  357 | acc: 37.50%,  total acc: 69.31%   [EVAL] batch:  358 | acc: 37.50%,  total acc: 69.22%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 69.17%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 69.08%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 69.03%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 69.01%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 69.26%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 69.43%   [EVAL] batch:  368 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 69.54%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 69.58%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 69.62%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 69.67%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 69.74%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 69.75%   [EVAL] batch:  375 | acc: 75.00%,  total acc: 69.76%   [EVAL] batch:  376 | acc: 87.50%,  total acc: 69.81%   [EVAL] batch:  377 | acc: 87.50%,  total acc: 69.86%   [EVAL] batch:  378 | acc: 68.75%,  total acc: 69.85%   [EVAL] batch:  379 | acc: 81.25%,  total acc: 69.88%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 69.94%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 69.99%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 69.99%   [EVAL] batch:  384 | acc: 68.75%,  total acc: 69.98%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 69.92%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 69.93%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 69.99%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 69.97%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 69.95%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 69.93%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 69.96%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 70.01%   [EVAL] batch:  393 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  400 | acc: 81.25%,  total acc: 70.53%   [EVAL] batch:  401 | acc: 62.50%,  total acc: 70.51%   [EVAL] batch:  402 | acc: 75.00%,  total acc: 70.52%   [EVAL] batch:  403 | acc: 68.75%,  total acc: 70.51%   [EVAL] batch:  404 | acc: 93.75%,  total acc: 70.57%   [EVAL] batch:  405 | acc: 87.50%,  total acc: 70.61%   [EVAL] batch:  406 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:  407 | acc: 68.75%,  total acc: 70.66%   [EVAL] batch:  408 | acc: 31.25%,  total acc: 70.57%   [EVAL] batch:  409 | acc: 62.50%,  total acc: 70.55%   [EVAL] batch:  410 | acc: 43.75%,  total acc: 70.48%   [EVAL] batch:  411 | acc: 50.00%,  total acc: 70.43%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 70.46%   [EVAL] batch:  413 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 70.56%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 70.60%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 70.65%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 70.69%   [EVAL] batch:  419 | acc: 68.75%,  total acc: 70.68%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 70.68%   [EVAL] batch:  421 | acc: 68.75%,  total acc: 70.68%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 70.64%   [EVAL] batch:  423 | acc: 68.75%,  total acc: 70.64%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 70.66%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 70.80%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 71.17%   [EVAL] batch:  433 | acc: 100.00%,  total acc: 71.24%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 71.44%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 71.39%   
cur_acc:  ['0.9524', '0.7817', '0.8234', '0.7460', '0.8919', '0.7798', '0.8115']
his_acc:  ['0.9524', '0.8630', '0.8088', '0.7475', '0.7494', '0.7258', '0.7139']
CurrentTrain: epoch  0, batch     0 | loss: 6.3719149CurrentTrain: epoch  0, batch     1 | loss: 6.3980422CurrentTrain: epoch  0, batch     2 | loss: 6.8001432CurrentTrain: epoch  0, batch     3 | loss: 5.1252880CurrentTrain: epoch  1, batch     0 | loss: 6.5640049CurrentTrain: epoch  1, batch     1 | loss: 5.8912621CurrentTrain: epoch  1, batch     2 | loss: 4.8611937CurrentTrain: epoch  1, batch     3 | loss: 3.8547187CurrentTrain: epoch  2, batch     0 | loss: 5.0629687CurrentTrain: epoch  2, batch     1 | loss: 4.4441566CurrentTrain: epoch  2, batch     2 | loss: 4.7307835CurrentTrain: epoch  2, batch     3 | loss: 6.5836368CurrentTrain: epoch  3, batch     0 | loss: 4.2430992CurrentTrain: epoch  3, batch     1 | loss: 3.9105618CurrentTrain: epoch  3, batch     2 | loss: 5.1620340CurrentTrain: epoch  3, batch     3 | loss: 7.2333517CurrentTrain: epoch  4, batch     0 | loss: 3.8025174CurrentTrain: epoch  4, batch     1 | loss: 5.0066838CurrentTrain: epoch  4, batch     2 | loss: 3.5344350CurrentTrain: epoch  4, batch     3 | loss: 3.4483917CurrentTrain: epoch  5, batch     0 | loss: 3.6235590CurrentTrain: epoch  5, batch     1 | loss: 3.5327227CurrentTrain: epoch  5, batch     2 | loss: 3.9125166CurrentTrain: epoch  5, batch     3 | loss: 2.5443783CurrentTrain: epoch  6, batch     0 | loss: 3.6462078CurrentTrain: epoch  6, batch     1 | loss: 3.1423860CurrentTrain: epoch  6, batch     2 | loss: 3.4030159CurrentTrain: epoch  6, batch     3 | loss: 2.3444250CurrentTrain: epoch  7, batch     0 | loss: 2.8851910CurrentTrain: epoch  7, batch     1 | loss: 3.5553932CurrentTrain: epoch  7, batch     2 | loss: 3.1413372CurrentTrain: epoch  7, batch     3 | loss: 2.1251254CurrentTrain: epoch  8, batch     0 | loss: 2.6350114CurrentTrain: epoch  8, batch     1 | loss: 3.0521550CurrentTrain: epoch  8, batch     2 | loss: 3.2267861CurrentTrain: epoch  8, batch     3 | loss: 1.9623101CurrentTrain: epoch  9, batch     0 | loss: 3.1157939CurrentTrain: epoch  9, batch     1 | loss: 2.5625024CurrentTrain: epoch  9, batch     2 | loss: 2.6817086CurrentTrain: epoch  9, batch     3 | loss: 1.7837043
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found on the slopes of the majestic Andes mountains, providing a stunning backdrop for visitors.  
Head Entity: Andes mountains  
Tail Entity: ancient ruins  

Relation: located on terrain feature  
Context: The village is nestled in the valley of the great Mississippi River, surrounded by lush greenery and wildlife.  
Head Entity: Mississippi River  
Tail Entity: village  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, also known as cop26, took place in glasgow, scotland, where numerous world leaders gathered to discuss climate action.  
Head Entity: united nations climate change conference  
Tail Entity: cop26  

Relation: participant of  
Context: elon musk, the ceo of spacex, was a key participant in the 2021 met gala, showcasing his vision for sustainable energy and space exploration.  
Head Entity: elon musk  
Tail Entity: 2021 met gala  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed director christopher nolan is known for his work on the blockbuster film "inception," which was released in 2010.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: director  
Context: in the latest season of the popular TV series "the crown," directed by peter morgan, viewers are treated to a deeper exploration of the royal family's history.  
Head Entity: the crown  
Tail Entity: peter morgan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game developer, Epic Games, announced that it had acquired the rights to the popular game franchise, Gears of War, which was previously owned by Microsoft.  
Head Entity: Epic Games  
Tail Entity: Microsoft  

Relation: owned by  
Context: The luxury fashion brand Gucci, known for its high-end products, is a subsidiary of the Kering Group, which owns several other prestigious labels.  
Head Entity: Gucci  
Tail Entity: Kering Group  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and it now serves as a gallery showcasing his work, attracting visitors from all over the world.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant co-working space, housing several startups and freelancers who thrive in the creative environment.  
Head Entity: old factory  
Tail Entity: several startups  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: reading town hall was built in several phases between 1786 and 1897 , although the principal facade was designed by alfred waterhouse in 1875 .
Head Entity: reading town hall
Tail Entity: alfred waterhouse
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the new art museum was designed by renowned architect zaha hadid, whose innovative style has transformed modern architecture.  
Head Entity: new art museum  
Tail Entity: zaha hadid  

Relation: architect  
Context: the iconic sydney opera house was the brainchild of architect jørn utzon, who won the design competition in 1957.  
Head Entity: sydney opera house  
Tail Entity: jørn utzon  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: fantcha was born as francelina durão almeida in mindelo on são vicente island , cape verde .
Head Entity: fantcha
Tail Entity: são vicente island
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york city for several years, john decided to move back to his hometown in los angeles.  
Head Entity: john  
Tail Entity: los angeles  

Relation: residence  
Context: the famous author spent most of her life in a quaint cottage located in the picturesque village of haworth, england.  
Head Entity: the famous author  
Tail Entity: haworth
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the small town of Gettysburg, Pennsylvania, which is now a popular tourist destination.  
Head Entity: historic battle  
Tail Entity: Gettysburg, Pennsylvania  
Mixup data size:  558
MixupTrain:  epoch  0, batch     0 | loss: 2.5323214MixupTrain:  epoch  0, batch     1 | loss: 2.2064185MixupTrain:  epoch  0, batch     2 | loss: 2.2361924MixupTrain:  epoch  0, batch     3 | loss: 2.2706995MixupTrain:  epoch  0, batch     4 | loss: 1.9890022MixupTrain:  epoch  0, batch     5 | loss: 1.8562227MixupTrain:  epoch  0, batch     6 | loss: 1.7389836MixupTrain:  epoch  0, batch     7 | loss: 1.9737810MixupTrain:  epoch  0, batch     8 | loss: 2.0380278MixupTrain:  epoch  0, batch     9 | loss: 1.6910149MixupTrain:  epoch  0, batch    10 | loss: 1.8197857MixupTrain:  epoch  0, batch    11 | loss: 1.8886027MixupTrain:  epoch  0, batch    12 | loss: 2.1945418MixupTrain:  epoch  0, batch    13 | loss: 2.1868203MixupTrain:  epoch  0, batch    14 | loss: 1.8998684MixupTrain:  epoch  0, batch    15 | loss: 2.0814478MixupTrain:  epoch  0, batch    16 | loss: 1.9598017MixupTrain:  epoch  0, batch    17 | loss: 1.8060301MixupTrain:  epoch  0, batch    18 | loss: 1.9880471MixupTrain:  epoch  0, batch    19 | loss: 1.6760773MixupTrain:  epoch  0, batch    20 | loss: 2.2189170MixupTrain:  epoch  0, batch    21 | loss: 1.7988596MixupTrain:  epoch  0, batch    22 | loss: 1.8260282MixupTrain:  epoch  0, batch    23 | loss: 1.5725837MixupTrain:  epoch  0, batch    24 | loss: 2.0160849MixupTrain:  epoch  0, batch    25 | loss: 2.3011400MixupTrain:  epoch  0, batch    26 | loss: 2.2934976MixupTrain:  epoch  0, batch    27 | loss: 2.4307047MixupTrain:  epoch  0, batch    28 | loss: 1.8569834MixupTrain:  epoch  0, batch    29 | loss: 2.0695662MixupTrain:  epoch  0, batch    30 | loss: 1.9056641MixupTrain:  epoch  0, batch    31 | loss: 1.8873368MixupTrain:  epoch  0, batch    32 | loss: 2.0480692MixupTrain:  epoch  0, batch    33 | loss: 1.8096181MixupTrain:  epoch  0, batch    34 | loss: 1.8304362
MemoryTrain:  epoch  0, batch     0 | loss: 1.6522677MemoryTrain:  epoch  0, batch     1 | loss: 1.5687695MemoryTrain:  epoch  0, batch     2 | loss: 1.8970554MemoryTrain:  epoch  0, batch     3 | loss: 2.1561530MemoryTrain:  epoch  0, batch     4 | loss: 1.8214765MemoryTrain:  epoch  0, batch     5 | loss: 2.5700181MemoryTrain:  epoch  0, batch     6 | loss: 1.6645280MemoryTrain:  epoch  0, batch     7 | loss: 2.6877198MemoryTrain:  epoch  0, batch     8 | loss: 1.8503690MemoryTrain:  epoch  0, batch     9 | loss: 2.4517550MemoryTrain:  epoch  0, batch    10 | loss: 1.8429748MemoryTrain:  epoch  0, batch    11 | loss: 2.1351433MemoryTrain:  epoch  0, batch    12 | loss: 2.2352386MemoryTrain:  epoch  0, batch    13 | loss: 2.0521145MemoryTrain:  epoch  0, batch    14 | loss: 3.3748899MemoryTrain:  epoch  1, batch     0 | loss: 1.4333334MemoryTrain:  epoch  1, batch     1 | loss: 2.7501419MemoryTrain:  epoch  1, batch     2 | loss: 1.7911537MemoryTrain:  epoch  1, batch     3 | loss: 1.7845703MemoryTrain:  epoch  1, batch     4 | loss: 2.3132570MemoryTrain:  epoch  1, batch     5 | loss: 1.7369379MemoryTrain:  epoch  1, batch     6 | loss: 1.5063906MemoryTrain:  epoch  1, batch     7 | loss: 2.0266495MemoryTrain:  epoch  1, batch     8 | loss: 1.4755220MemoryTrain:  epoch  1, batch     9 | loss: 1.9595838MemoryTrain:  epoch  1, batch    10 | loss: 1.4752285MemoryTrain:  epoch  1, batch    11 | loss: 2.1790335MemoryTrain:  epoch  1, batch    12 | loss: 1.4918052MemoryTrain:  epoch  1, batch    13 | loss: 2.1497421MemoryTrain:  epoch  1, batch    14 | loss: 2.4897497MemoryTrain:  epoch  2, batch     0 | loss: 2.0968060MemoryTrain:  epoch  2, batch     1 | loss: 1.8101172MemoryTrain:  epoch  2, batch     2 | loss: 1.6473317MemoryTrain:  epoch  2, batch     3 | loss: 1.4378302MemoryTrain:  epoch  2, batch     4 | loss: 1.5524487MemoryTrain:  epoch  2, batch     5 | loss: 2.0077584MemoryTrain:  epoch  2, batch     6 | loss: 1.3628898MemoryTrain:  epoch  2, batch     7 | loss: 1.7780877MemoryTrain:  epoch  2, batch     8 | loss: 1.9682961MemoryTrain:  epoch  2, batch     9 | loss: 1.7190659MemoryTrain:  epoch  2, batch    10 | loss: 2.2026422MemoryTrain:  epoch  2, batch    11 | loss: 1.5474613MemoryTrain:  epoch  2, batch    12 | loss: 1.5949990MemoryTrain:  epoch  2, batch    13 | loss: 1.2613921MemoryTrain:  epoch  2, batch    14 | loss: 1.2676281MemoryTrain:  epoch  3, batch     0 | loss: 1.6575141MemoryTrain:  epoch  3, batch     1 | loss: 1.2850633MemoryTrain:  epoch  3, batch     2 | loss: 1.3626409MemoryTrain:  epoch  3, batch     3 | loss: 1.4378116MemoryTrain:  epoch  3, batch     4 | loss: 1.7387605MemoryTrain:  epoch  3, batch     5 | loss: 1.2362342MemoryTrain:  epoch  3, batch     6 | loss: 1.4400593MemoryTrain:  epoch  3, batch     7 | loss: 1.8308593MemoryTrain:  epoch  3, batch     8 | loss: 1.3665457MemoryTrain:  epoch  3, batch     9 | loss: 1.5317693MemoryTrain:  epoch  3, batch    10 | loss: 1.7438767MemoryTrain:  epoch  3, batch    11 | loss: 1.5765164MemoryTrain:  epoch  3, batch    12 | loss: 1.6527157MemoryTrain:  epoch  3, batch    13 | loss: 1.6064138MemoryTrain:  epoch  3, batch    14 | loss: 1.5062511MemoryTrain:  epoch  4, batch     0 | loss: 1.6735938MemoryTrain:  epoch  4, batch     1 | loss: 1.4369829MemoryTrain:  epoch  4, batch     2 | loss: 1.2977111MemoryTrain:  epoch  4, batch     3 | loss: 1.4152570MemoryTrain:  epoch  4, batch     4 | loss: 1.5491507MemoryTrain:  epoch  4, batch     5 | loss: 1.5309303MemoryTrain:  epoch  4, batch     6 | loss: 1.3128148MemoryTrain:  epoch  4, batch     7 | loss: 1.3653400MemoryTrain:  epoch  4, batch     8 | loss: 1.6255453MemoryTrain:  epoch  4, batch     9 | loss: 1.5090895MemoryTrain:  epoch  4, batch    10 | loss: 1.4365139MemoryTrain:  epoch  4, batch    11 | loss: 1.3547289MemoryTrain:  epoch  4, batch    12 | loss: 1.3046262MemoryTrain:  epoch  4, batch    13 | loss: 1.6310564MemoryTrain:  epoch  4, batch    14 | loss: 1.4135482MemoryTrain:  epoch  5, batch     0 | loss: 1.4417366MemoryTrain:  epoch  5, batch     1 | loss: 1.4598687MemoryTrain:  epoch  5, batch     2 | loss: 1.2438238MemoryTrain:  epoch  5, batch     3 | loss: 1.2868524MemoryTrain:  epoch  5, batch     4 | loss: 1.2712545MemoryTrain:  epoch  5, batch     5 | loss: 1.2238475MemoryTrain:  epoch  5, batch     6 | loss: 1.3538452MemoryTrain:  epoch  5, batch     7 | loss: 1.2736657MemoryTrain:  epoch  5, batch     8 | loss: 1.6264421MemoryTrain:  epoch  5, batch     9 | loss: 1.3149449MemoryTrain:  epoch  5, batch    10 | loss: 1.5619128MemoryTrain:  epoch  5, batch    11 | loss: 1.2166450MemoryTrain:  epoch  5, batch    12 | loss: 1.7745899MemoryTrain:  epoch  5, batch    13 | loss: 1.3686107MemoryTrain:  epoch  5, batch    14 | loss: 1.4725461MemoryTrain:  epoch  6, batch     0 | loss: 1.4598185MemoryTrain:  epoch  6, batch     1 | loss: 1.2132514MemoryTrain:  epoch  6, batch     2 | loss: 1.3117005MemoryTrain:  epoch  6, batch     3 | loss: 1.1927267MemoryTrain:  epoch  6, batch     4 | loss: 1.4321134MemoryTrain:  epoch  6, batch     5 | loss: 1.2555321MemoryTrain:  epoch  6, batch     6 | loss: 1.3277553MemoryTrain:  epoch  6, batch     7 | loss: 1.2992327MemoryTrain:  epoch  6, batch     8 | loss: 1.3015444MemoryTrain:  epoch  6, batch     9 | loss: 1.2382699MemoryTrain:  epoch  6, batch    10 | loss: 1.2297260MemoryTrain:  epoch  6, batch    11 | loss: 1.6739588MemoryTrain:  epoch  6, batch    12 | loss: 1.9680036MemoryTrain:  epoch  6, batch    13 | loss: 1.2215557MemoryTrain:  epoch  6, batch    14 | loss: 1.1973305MemoryTrain:  epoch  7, batch     0 | loss: 1.4978403MemoryTrain:  epoch  7, batch     1 | loss: 1.4581823MemoryTrain:  epoch  7, batch     2 | loss: 1.2305754MemoryTrain:  epoch  7, batch     3 | loss: 1.2462907MemoryTrain:  epoch  7, batch     4 | loss: 1.2229866MemoryTrain:  epoch  7, batch     5 | loss: 1.3944387MemoryTrain:  epoch  7, batch     6 | loss: 1.3786163MemoryTrain:  epoch  7, batch     7 | loss: 1.2806966MemoryTrain:  epoch  7, batch     8 | loss: 1.2800581MemoryTrain:  epoch  7, batch     9 | loss: 1.2424877MemoryTrain:  epoch  7, batch    10 | loss: 1.4687073MemoryTrain:  epoch  7, batch    11 | loss: 1.4735889MemoryTrain:  epoch  7, batch    12 | loss: 1.2030259MemoryTrain:  epoch  7, batch    13 | loss: 1.2024430MemoryTrain:  epoch  7, batch    14 | loss: 1.2065630MemoryTrain:  epoch  8, batch     0 | loss: 1.3844445MemoryTrain:  epoch  8, batch     1 | loss: 1.2452571MemoryTrain:  epoch  8, batch     2 | loss: 1.2429026MemoryTrain:  epoch  8, batch     3 | loss: 1.2555761MemoryTrain:  epoch  8, batch     4 | loss: 1.4679399MemoryTrain:  epoch  8, batch     5 | loss: 1.3887577MemoryTrain:  epoch  8, batch     6 | loss: 1.2170109MemoryTrain:  epoch  8, batch     7 | loss: 1.2134197MemoryTrain:  epoch  8, batch     8 | loss: 1.3818057MemoryTrain:  epoch  8, batch     9 | loss: 1.2402081MemoryTrain:  epoch  8, batch    10 | loss: 1.2214820MemoryTrain:  epoch  8, batch    11 | loss: 1.3343768MemoryTrain:  epoch  8, batch    12 | loss: 1.3246273MemoryTrain:  epoch  8, batch    13 | loss: 1.2489574MemoryTrain:  epoch  8, batch    14 | loss: 1.2404492MemoryTrain:  epoch  9, batch     0 | loss: 1.2292098MemoryTrain:  epoch  9, batch     1 | loss: 1.3170087MemoryTrain:  epoch  9, batch     2 | loss: 1.2692496MemoryTrain:  epoch  9, batch     3 | loss: 1.3242629MemoryTrain:  epoch  9, batch     4 | loss: 1.2844486MemoryTrain:  epoch  9, batch     5 | loss: 1.2782049MemoryTrain:  epoch  9, batch     6 | loss: 1.2975061MemoryTrain:  epoch  9, batch     7 | loss: 1.2624953MemoryTrain:  epoch  9, batch     8 | loss: 1.2185957MemoryTrain:  epoch  9, batch     9 | loss: 1.2671030MemoryTrain:  epoch  9, batch    10 | loss: 1.3427780MemoryTrain:  epoch  9, batch    11 | loss: 1.4314133MemoryTrain:  epoch  9, batch    12 | loss: 1.2522105MemoryTrain:  epoch  9, batch    13 | loss: 1.2413177MemoryTrain:  epoch  9, batch    14 | loss: 1.2208316
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 15.00%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 20.54%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 28.12%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 36.11%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 40.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 47.40%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 50.48%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 55.42%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 57.42%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 58.82%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 60.53%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 59.69%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 58.33%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 58.52%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 58.42%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 58.07%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 58.25%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 56.25%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 54.40%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 52.90%   [EVAL] batch:   28 | acc: 12.50%,  total acc: 51.51%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 50.00%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 48.39%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 48.63%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 51.29%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 52.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 53.65%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 54.39%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 55.43%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 57.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 58.23%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 58.93%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 59.88%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 60.65%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 60.28%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 60.05%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 59.57%   [EVAL] batch:   47 | acc: 25.00%,  total acc: 58.85%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 58.93%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 58.13%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 57.72%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 57.81%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 58.02%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 58.10%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 58.18%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 58.48%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 58.11%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 57.97%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 57.73%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 58.13%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 58.30%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 58.77%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 58.23%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 70.92%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 71.61%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 72.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 78.49%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.39%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 78.65%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 78.89%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 79.11%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.65%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 80.49%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 80.65%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 80.96%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 81.11%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 80.83%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 80.71%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 80.19%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 79.56%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 79.46%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 78.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 78.80%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 78.49%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 78.42%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 78.01%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 77.39%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 77.46%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 76.86%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 75.86%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 75.11%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 74.39%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 73.79%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 73.61%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 73.34%   [EVAL] batch:   64 | acc: 31.25%,  total acc: 72.69%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 72.82%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 72.57%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 72.43%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 72.28%   [EVAL] batch:   69 | acc: 25.00%,  total acc: 71.61%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 71.21%   [EVAL] batch:   71 | acc: 12.50%,  total acc: 70.40%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 69.61%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 69.26%   [EVAL] batch:   74 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 67.85%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 67.05%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 66.43%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 65.59%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 64.77%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 63.97%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 64.02%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:   84 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 66.12%   [EVAL] batch:   88 | acc: 18.75%,  total acc: 65.59%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 65.21%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 64.77%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 64.47%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 63.91%   [EVAL] batch:   93 | acc: 37.50%,  total acc: 63.63%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 63.95%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 64.26%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 64.50%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 64.73%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 64.79%   [EVAL] batch:  101 | acc: 6.25%,  total acc: 64.22%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 63.59%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 62.98%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 62.38%   [EVAL] batch:  105 | acc: 0.00%,  total acc: 61.79%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 61.74%   [EVAL] batch:  107 | acc: 50.00%,  total acc: 61.63%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 61.53%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 61.36%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 61.32%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 61.16%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 61.39%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 61.35%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 61.52%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 61.48%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 61.59%   [EVAL] batch:  117 | acc: 56.25%,  total acc: 61.55%   [EVAL] batch:  118 | acc: 50.00%,  total acc: 61.45%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 61.72%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 61.98%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 62.19%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 62.30%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 62.45%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 62.70%   [EVAL] batch:  125 | acc: 12.50%,  total acc: 62.30%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 61.91%   [EVAL] batch:  127 | acc: 6.25%,  total acc: 61.47%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 61.05%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 60.58%   [EVAL] batch:  130 | acc: 0.00%,  total acc: 60.11%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 60.04%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 60.10%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 60.17%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 60.28%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 60.52%   [EVAL] batch:  136 | acc: 68.75%,  total acc: 60.58%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 60.51%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 60.25%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 60.00%   [EVAL] batch:  140 | acc: 43.75%,  total acc: 59.88%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 59.64%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 59.57%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 59.51%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 59.78%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 60.06%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 60.29%   [EVAL] batch:  147 | acc: 93.75%,  total acc: 60.52%   [EVAL] batch:  148 | acc: 93.75%,  total acc: 60.74%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 60.92%   [EVAL] batch:  150 | acc: 93.75%,  total acc: 61.13%   [EVAL] batch:  151 | acc: 93.75%,  total acc: 61.35%   [EVAL] batch:  152 | acc: 93.75%,  total acc: 61.56%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 61.73%   [EVAL] batch:  154 | acc: 87.50%,  total acc: 61.90%   [EVAL] batch:  155 | acc: 87.50%,  total acc: 62.06%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 62.26%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 62.70%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 62.89%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 63.08%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 63.27%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 63.19%   [EVAL] batch:  163 | acc: 12.50%,  total acc: 62.88%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 62.61%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 62.35%   [EVAL] batch:  166 | acc: 0.00%,  total acc: 61.98%   [EVAL] batch:  167 | acc: 6.25%,  total acc: 61.64%   [EVAL] batch:  168 | acc: 43.75%,  total acc: 61.54%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 61.65%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 61.77%   [EVAL] batch:  171 | acc: 93.75%,  total acc: 61.95%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 61.99%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 61.96%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 62.00%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 61.97%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 62.01%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 61.97%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 62.01%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 62.01%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 61.84%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 61.71%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 61.68%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 61.55%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 61.69%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 61.83%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 61.90%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 61.87%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 61.94%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 61.97%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 62.04%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 62.11%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 62.11%   [EVAL] batch:  193 | acc: 93.75%,  total acc: 62.27%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 62.37%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 62.37%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 62.40%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 62.53%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 62.59%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 62.66%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 62.69%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 62.72%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 62.65%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 62.62%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 62.65%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 62.74%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 62.59%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 62.44%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 62.23%   [EVAL] batch:  209 | acc: 6.25%,  total acc: 61.96%   [EVAL] batch:  210 | acc: 43.75%,  total acc: 61.88%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 61.79%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 61.74%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 61.89%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 62.06%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 62.15%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 62.33%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 62.44%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 62.61%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 63.12%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 63.61%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 63.85%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 63.90%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 64.03%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 64.16%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 64.23%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 64.17%   [EVAL] batch:  232 | acc: 18.75%,  total acc: 63.98%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 63.89%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 63.88%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 63.85%   [EVAL] batch:  236 | acc: 56.25%,  total acc: 63.82%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 63.87%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 63.96%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.11%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 64.16%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 64.26%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 64.35%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 64.40%   [EVAL] batch:  244 | acc: 18.75%,  total acc: 64.21%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 64.08%   [EVAL] batch:  246 | acc: 0.00%,  total acc: 63.82%   [EVAL] batch:  247 | acc: 18.75%,  total acc: 63.63%   [EVAL] batch:  248 | acc: 12.50%,  total acc: 63.43%   [EVAL] batch:  249 | acc: 25.00%,  total acc: 63.28%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 63.40%   [EVAL] batch:  251 | acc: 93.75%,  total acc: 63.52%   [EVAL] batch:  252 | acc: 100.00%,  total acc: 63.66%   [EVAL] batch:  253 | acc: 87.50%,  total acc: 63.75%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 63.87%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 64.01%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 64.03%   [EVAL] batch:  257 | acc: 93.75%,  total acc: 64.15%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 64.19%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 64.28%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 64.27%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 64.46%   [EVAL] batch:  264 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 64.70%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 64.74%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 64.78%   [EVAL] batch:  269 | acc: 37.50%,  total acc: 64.68%   [EVAL] batch:  270 | acc: 37.50%,  total acc: 64.58%   [EVAL] batch:  271 | acc: 50.00%,  total acc: 64.52%   [EVAL] batch:  272 | acc: 31.25%,  total acc: 64.40%   [EVAL] batch:  273 | acc: 62.50%,  total acc: 64.39%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 64.27%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 65.05%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 65.11%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 65.12%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 65.13%   [EVAL] batch:  285 | acc: 75.00%,  total acc: 65.17%   [EVAL] batch:  286 | acc: 62.50%,  total acc: 65.16%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 65.17%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 65.84%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 66.68%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 66.77%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 66.86%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 66.91%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 67.15%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 67.21%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 67.30%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 67.47%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 67.51%   [EVAL] batch:  313 | acc: 75.00%,  total acc: 67.54%   [EVAL] batch:  314 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:  315 | acc: 62.50%,  total acc: 67.62%   [EVAL] batch:  316 | acc: 50.00%,  total acc: 67.57%   [EVAL] batch:  317 | acc: 68.75%,  total acc: 67.57%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 67.59%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 67.78%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 67.84%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 68.10%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 68.08%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 68.02%   [EVAL] batch:  327 | acc: 62.50%,  total acc: 68.01%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 67.99%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 67.99%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 67.99%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 68.02%   [EVAL] batch:  332 | acc: 62.50%,  total acc: 68.00%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 68.08%   [EVAL] batch:  334 | acc: 81.25%,  total acc: 68.12%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 68.19%   [EVAL] batch:  336 | acc: 75.00%,  total acc: 68.21%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 68.18%   [EVAL] batch:  338 | acc: 56.25%,  total acc: 68.14%   [EVAL] batch:  339 | acc: 50.00%,  total acc: 68.09%   [EVAL] batch:  340 | acc: 62.50%,  total acc: 68.07%   [EVAL] batch:  341 | acc: 50.00%,  total acc: 68.02%   [EVAL] batch:  342 | acc: 43.75%,  total acc: 67.95%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 67.84%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 67.84%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 67.77%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 67.76%   [EVAL] batch:  347 | acc: 68.75%,  total acc: 67.76%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 67.77%   [EVAL] batch:  349 | acc: 56.25%,  total acc: 67.73%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 67.75%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 67.88%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 67.92%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 67.96%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 68.03%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 68.01%   [EVAL] batch:  357 | acc: 37.50%,  total acc: 67.93%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 67.83%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 67.78%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 67.69%   [EVAL] batch:  361 | acc: 43.75%,  total acc: 67.63%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 67.61%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 68.05%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 68.21%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 68.26%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 68.42%   [EVAL] batch:  375 | acc: 43.75%,  total acc: 68.35%   [EVAL] batch:  376 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:  377 | acc: 87.50%,  total acc: 68.45%   [EVAL] batch:  378 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 68.49%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 68.54%   [EVAL] batch:  382 | acc: 75.00%,  total acc: 68.55%   [EVAL] batch:  383 | acc: 81.25%,  total acc: 68.59%   [EVAL] batch:  384 | acc: 68.75%,  total acc: 68.59%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 68.52%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 68.52%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 68.59%   [EVAL] batch:  388 | acc: 62.50%,  total acc: 68.57%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 68.57%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 68.56%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 68.57%   [EVAL] batch:  392 | acc: 81.25%,  total acc: 68.61%   [EVAL] batch:  393 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 69.11%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 69.09%   [EVAL] batch:  401 | acc: 43.75%,  total acc: 69.03%   [EVAL] batch:  402 | acc: 68.75%,  total acc: 69.03%   [EVAL] batch:  403 | acc: 56.25%,  total acc: 69.00%   [EVAL] batch:  404 | acc: 75.00%,  total acc: 69.01%   [EVAL] batch:  405 | acc: 75.00%,  total acc: 69.03%   [EVAL] batch:  406 | acc: 81.25%,  total acc: 69.06%   [EVAL] batch:  407 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:  408 | acc: 62.50%,  total acc: 69.04%   [EVAL] batch:  409 | acc: 56.25%,  total acc: 69.01%   [EVAL] batch:  410 | acc: 50.00%,  total acc: 68.96%   [EVAL] batch:  411 | acc: 43.75%,  total acc: 68.90%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 68.93%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 68.93%   [EVAL] batch:  414 | acc: 68.75%,  total acc: 68.93%   [EVAL] batch:  415 | acc: 68.75%,  total acc: 68.93%   [EVAL] batch:  416 | acc: 62.50%,  total acc: 68.91%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 68.91%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 68.94%   [EVAL] batch:  419 | acc: 62.50%,  total acc: 68.93%   [EVAL] batch:  420 | acc: 68.75%,  total acc: 68.93%   [EVAL] batch:  421 | acc: 56.25%,  total acc: 68.90%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 68.87%   [EVAL] batch:  423 | acc: 62.50%,  total acc: 68.85%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 69.43%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 69.48%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 69.69%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 69.66%   [EVAL] batch:  438 | acc: 12.50%,  total acc: 69.53%   [EVAL] batch:  439 | acc: 31.25%,  total acc: 69.45%   [EVAL] batch:  440 | acc: 6.25%,  total acc: 69.30%   [EVAL] batch:  441 | acc: 12.50%,  total acc: 69.17%   [EVAL] batch:  442 | acc: 12.50%,  total acc: 69.05%   [EVAL] batch:  443 | acc: 31.25%,  total acc: 68.96%   [EVAL] batch:  444 | acc: 68.75%,  total acc: 68.96%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 69.02%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 69.06%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 69.10%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 69.11%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 69.14%   [EVAL] batch:  450 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 69.23%   [EVAL] batch:  452 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:  453 | acc: 81.25%,  total acc: 69.30%   [EVAL] batch:  454 | acc: 87.50%,  total acc: 69.34%   [EVAL] batch:  455 | acc: 87.50%,  total acc: 69.38%   [EVAL] batch:  456 | acc: 37.50%,  total acc: 69.31%   [EVAL] batch:  457 | acc: 43.75%,  total acc: 69.25%   [EVAL] batch:  458 | acc: 37.50%,  total acc: 69.19%   [EVAL] batch:  459 | acc: 68.75%,  total acc: 69.18%   [EVAL] batch:  460 | acc: 62.50%,  total acc: 69.17%   [EVAL] batch:  461 | acc: 37.50%,  total acc: 69.10%   [EVAL] batch:  462 | acc: 43.75%,  total acc: 69.05%   [EVAL] batch:  463 | acc: 0.00%,  total acc: 68.90%   [EVAL] batch:  464 | acc: 18.75%,  total acc: 68.79%   [EVAL] batch:  465 | acc: 0.00%,  total acc: 68.64%   [EVAL] batch:  466 | acc: 12.50%,  total acc: 68.52%   [EVAL] batch:  467 | acc: 6.25%,  total acc: 68.39%   [EVAL] batch:  468 | acc: 12.50%,  total acc: 68.27%   [EVAL] batch:  469 | acc: 87.50%,  total acc: 68.31%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 68.43%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:  473 | acc: 87.50%,  total acc: 68.53%   [EVAL] batch:  474 | acc: 81.25%,  total acc: 68.55%   [EVAL] batch:  475 | acc: 93.75%,  total acc: 68.61%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  477 | acc: 93.75%,  total acc: 68.71%   [EVAL] batch:  478 | acc: 93.75%,  total acc: 68.76%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 68.82%   [EVAL] batch:  480 | acc: 100.00%,  total acc: 68.88%   [EVAL] batch:  481 | acc: 75.00%,  total acc: 68.89%   [EVAL] batch:  482 | acc: 31.25%,  total acc: 68.81%   [EVAL] batch:  483 | acc: 50.00%,  total acc: 68.78%   [EVAL] batch:  484 | acc: 37.50%,  total acc: 68.71%   [EVAL] batch:  485 | acc: 43.75%,  total acc: 68.66%   [EVAL] batch:  486 | acc: 31.25%,  total acc: 68.58%   [EVAL] batch:  487 | acc: 31.25%,  total acc: 68.51%   [EVAL] batch:  488 | acc: 43.75%,  total acc: 68.46%   [EVAL] batch:  489 | acc: 62.50%,  total acc: 68.44%   [EVAL] batch:  490 | acc: 81.25%,  total acc: 68.47%   [EVAL] batch:  491 | acc: 50.00%,  total acc: 68.43%   [EVAL] batch:  492 | acc: 75.00%,  total acc: 68.45%   [EVAL] batch:  493 | acc: 56.25%,  total acc: 68.42%   [EVAL] batch:  494 | acc: 43.75%,  total acc: 68.37%   [EVAL] batch:  495 | acc: 43.75%,  total acc: 68.32%   [EVAL] batch:  496 | acc: 56.25%,  total acc: 68.30%   [EVAL] batch:  497 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  498 | acc: 81.25%,  total acc: 68.36%   [EVAL] batch:  499 | acc: 62.50%,  total acc: 68.35%   
cur_acc:  ['0.9524', '0.7817', '0.8234', '0.7460', '0.8919', '0.7798', '0.8115', '0.5823']
his_acc:  ['0.9524', '0.8630', '0.8088', '0.7475', '0.7494', '0.7258', '0.7139', '0.6835']
--------Round  4
seed:  500
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.1497774CurrentTrain: epoch  0, batch     1 | loss: 12.8847179CurrentTrain: epoch  0, batch     2 | loss: 12.7183037CurrentTrain: epoch  0, batch     3 | loss: 12.1352015CurrentTrain: epoch  0, batch     4 | loss: 11.8647289CurrentTrain: epoch  0, batch     5 | loss: 11.7850208CurrentTrain: epoch  0, batch     6 | loss: 11.8987312CurrentTrain: epoch  0, batch     7 | loss: 11.6864042CurrentTrain: epoch  0, batch     8 | loss: 11.5640965CurrentTrain: epoch  0, batch     9 | loss: 11.4650822CurrentTrain: epoch  0, batch    10 | loss: 11.3667812CurrentTrain: epoch  0, batch    11 | loss: 11.1108742CurrentTrain: epoch  0, batch    12 | loss: 10.8872032CurrentTrain: epoch  0, batch    13 | loss: 10.7633591CurrentTrain: epoch  0, batch    14 | loss: 10.6143045CurrentTrain: epoch  0, batch    15 | loss: 10.9401407CurrentTrain: epoch  0, batch    16 | loss: 10.4048481CurrentTrain: epoch  0, batch    17 | loss: 10.3641491CurrentTrain: epoch  0, batch    18 | loss: 9.7633133CurrentTrain: epoch  0, batch    19 | loss: 9.9507122CurrentTrain: epoch  0, batch    20 | loss: 10.4358253CurrentTrain: epoch  0, batch    21 | loss: 9.9626274CurrentTrain: epoch  0, batch    22 | loss: 10.2313576CurrentTrain: epoch  0, batch    23 | loss: 10.2497311CurrentTrain: epoch  0, batch    24 | loss: 10.1853333CurrentTrain: epoch  0, batch    25 | loss: 10.1147041CurrentTrain: epoch  0, batch    26 | loss: 9.9152451CurrentTrain: epoch  0, batch    27 | loss: 9.8703213CurrentTrain: epoch  0, batch    28 | loss: 9.7083797CurrentTrain: epoch  0, batch    29 | loss: 9.4682360CurrentTrain: epoch  0, batch    30 | loss: 9.8254690CurrentTrain: epoch  0, batch    31 | loss: 9.5883045CurrentTrain: epoch  0, batch    32 | loss: 9.6324530CurrentTrain: epoch  0, batch    33 | loss: 8.7312317CurrentTrain: epoch  0, batch    34 | loss: 10.0909290CurrentTrain: epoch  0, batch    35 | loss: 9.3206682CurrentTrain: epoch  0, batch    36 | loss: 9.5621176CurrentTrain: epoch  0, batch    37 | loss: 8.7280159CurrentTrain: epoch  0, batch    38 | loss: 9.1212711CurrentTrain: epoch  0, batch    39 | loss: 8.9585972CurrentTrain: epoch  0, batch    40 | loss: 8.7595615CurrentTrain: epoch  0, batch    41 | loss: 8.8644428CurrentTrain: epoch  0, batch    42 | loss: 9.6569767CurrentTrain: epoch  0, batch    43 | loss: 8.8515358CurrentTrain: epoch  0, batch    44 | loss: 9.1018085CurrentTrain: epoch  0, batch    45 | loss: 8.9864140CurrentTrain: epoch  0, batch    46 | loss: 8.1524410CurrentTrain: epoch  0, batch    47 | loss: 8.5849571CurrentTrain: epoch  0, batch    48 | loss: 8.2727108CurrentTrain: epoch  0, batch    49 | loss: 9.1095047CurrentTrain: epoch  0, batch    50 | loss: 8.0587826CurrentTrain: epoch  0, batch    51 | loss: 8.3226309CurrentTrain: epoch  0, batch    52 | loss: 8.5126677CurrentTrain: epoch  0, batch    53 | loss: 8.8381290CurrentTrain: epoch  0, batch    54 | loss: 9.1049461CurrentTrain: epoch  0, batch    55 | loss: 9.1565018CurrentTrain: epoch  0, batch    56 | loss: 7.3593478CurrentTrain: epoch  0, batch    57 | loss: 7.5327892CurrentTrain: epoch  0, batch    58 | loss: 8.0319538CurrentTrain: epoch  0, batch    59 | loss: 7.8839297CurrentTrain: epoch  0, batch    60 | loss: 8.4961967CurrentTrain: epoch  0, batch    61 | loss: 7.4137850CurrentTrain: epoch  0, batch    62 | loss: 7.4783473CurrentTrain: epoch  1, batch     0 | loss: 8.1861563CurrentTrain: epoch  1, batch     1 | loss: 7.2239962CurrentTrain: epoch  1, batch     2 | loss: 6.7522845CurrentTrain: epoch  1, batch     3 | loss: 8.4837475CurrentTrain: epoch  1, batch     4 | loss: 8.3629827CurrentTrain: epoch  1, batch     5 | loss: 7.1071815CurrentTrain: epoch  1, batch     6 | loss: 7.6110325CurrentTrain: epoch  1, batch     7 | loss: 7.0047827CurrentTrain: epoch  1, batch     8 | loss: 7.8920202CurrentTrain: epoch  1, batch     9 | loss: 7.2610807CurrentTrain: epoch  1, batch    10 | loss: 7.3549547CurrentTrain: epoch  1, batch    11 | loss: 7.4849863CurrentTrain: epoch  1, batch    12 | loss: 7.1387930CurrentTrain: epoch  1, batch    13 | loss: 7.2327871CurrentTrain: epoch  1, batch    14 | loss: 7.5527954CurrentTrain: epoch  1, batch    15 | loss: 8.0038090CurrentTrain: epoch  1, batch    16 | loss: 7.8219233CurrentTrain: epoch  1, batch    17 | loss: 7.7027736CurrentTrain: epoch  1, batch    18 | loss: 7.2647791CurrentTrain: epoch  1, batch    19 | loss: 7.5604839CurrentTrain: epoch  1, batch    20 | loss: 7.7305284CurrentTrain: epoch  1, batch    21 | loss: 7.0903997CurrentTrain: epoch  1, batch    22 | loss: 7.8814511CurrentTrain: epoch  1, batch    23 | loss: 7.2206717CurrentTrain: epoch  1, batch    24 | loss: 7.2564440CurrentTrain: epoch  1, batch    25 | loss: 6.1007991CurrentTrain: epoch  1, batch    26 | loss: 6.9468317CurrentTrain: epoch  1, batch    27 | loss: 7.1019549CurrentTrain: epoch  1, batch    28 | loss: 6.6693420CurrentTrain: epoch  1, batch    29 | loss: 6.6368790CurrentTrain: epoch  1, batch    30 | loss: 6.6624217CurrentTrain: epoch  1, batch    31 | loss: 6.8184948CurrentTrain: epoch  1, batch    32 | loss: 6.6118202CurrentTrain: epoch  1, batch    33 | loss: 6.5621519CurrentTrain: epoch  1, batch    34 | loss: 7.5792680CurrentTrain: epoch  1, batch    35 | loss: 6.7236104CurrentTrain: epoch  1, batch    36 | loss: 6.4960465CurrentTrain: epoch  1, batch    37 | loss: 6.9332156CurrentTrain: epoch  1, batch    38 | loss: 6.3499088CurrentTrain: epoch  1, batch    39 | loss: 5.9644380CurrentTrain: epoch  1, batch    40 | loss: 6.6988249CurrentTrain: epoch  1, batch    41 | loss: 6.3404975CurrentTrain: epoch  1, batch    42 | loss: 6.3644629CurrentTrain: epoch  1, batch    43 | loss: 6.4015770CurrentTrain: epoch  1, batch    44 | loss: 6.6287985CurrentTrain: epoch  1, batch    45 | loss: 6.5553775CurrentTrain: epoch  1, batch    46 | loss: 6.7580976CurrentTrain: epoch  1, batch    47 | loss: 6.9719939CurrentTrain: epoch  1, batch    48 | loss: 6.1830997CurrentTrain: epoch  1, batch    49 | loss: 6.4600182CurrentTrain: epoch  1, batch    50 | loss: 7.4707298CurrentTrain: epoch  1, batch    51 | loss: 6.7659235CurrentTrain: epoch  1, batch    52 | loss: 5.7194910CurrentTrain: epoch  1, batch    53 | loss: 7.0507193CurrentTrain: epoch  1, batch    54 | loss: 6.3165627CurrentTrain: epoch  1, batch    55 | loss: 5.8510737CurrentTrain: epoch  1, batch    56 | loss: 7.0843849CurrentTrain: epoch  1, batch    57 | loss: 7.0076776CurrentTrain: epoch  1, batch    58 | loss: 6.5388288CurrentTrain: epoch  1, batch    59 | loss: 6.3594604CurrentTrain: epoch  1, batch    60 | loss: 7.0032811CurrentTrain: epoch  1, batch    61 | loss: 6.2989521CurrentTrain: epoch  1, batch    62 | loss: 6.5745397CurrentTrain: epoch  2, batch     0 | loss: 6.4527826CurrentTrain: epoch  2, batch     1 | loss: 6.2240334CurrentTrain: epoch  2, batch     2 | loss: 6.1594281CurrentTrain: epoch  2, batch     3 | loss: 5.8620510CurrentTrain: epoch  2, batch     4 | loss: 6.6649132CurrentTrain: epoch  2, batch     5 | loss: 5.8353004CurrentTrain: epoch  2, batch     6 | loss: 6.5063820CurrentTrain: epoch  2, batch     7 | loss: 5.6624422CurrentTrain: epoch  2, batch     8 | loss: 5.6434774CurrentTrain: epoch  2, batch     9 | loss: 5.7052984CurrentTrain: epoch  2, batch    10 | loss: 6.0902295CurrentTrain: epoch  2, batch    11 | loss: 6.2178931CurrentTrain: epoch  2, batch    12 | loss: 5.9046631CurrentTrain: epoch  2, batch    13 | loss: 6.2051983CurrentTrain: epoch  2, batch    14 | loss: 5.9090567CurrentTrain: epoch  2, batch    15 | loss: 6.2309990CurrentTrain: epoch  2, batch    16 | loss: 5.7221179CurrentTrain: epoch  2, batch    17 | loss: 5.2093844CurrentTrain: epoch  2, batch    18 | loss: 5.5788903CurrentTrain: epoch  2, batch    19 | loss: 5.5213633CurrentTrain: epoch  2, batch    20 | loss: 5.4679956CurrentTrain: epoch  2, batch    21 | loss: 5.9447584CurrentTrain: epoch  2, batch    22 | loss: 6.0339532CurrentTrain: epoch  2, batch    23 | loss: 5.8306832CurrentTrain: epoch  2, batch    24 | loss: 5.7717848CurrentTrain: epoch  2, batch    25 | loss: 6.2072272CurrentTrain: epoch  2, batch    26 | loss: 5.4807100CurrentTrain: epoch  2, batch    27 | loss: 6.1172543CurrentTrain: epoch  2, batch    28 | loss: 5.6032524CurrentTrain: epoch  2, batch    29 | loss: 5.9736176CurrentTrain: epoch  2, batch    30 | loss: 5.7601709CurrentTrain: epoch  2, batch    31 | loss: 5.2796326CurrentTrain: epoch  2, batch    32 | loss: 6.4143410CurrentTrain: epoch  2, batch    33 | loss: 5.7397990CurrentTrain: epoch  2, batch    34 | loss: 5.7122293CurrentTrain: epoch  2, batch    35 | loss: 6.2193441CurrentTrain: epoch  2, batch    36 | loss: 5.3021145CurrentTrain: epoch  2, batch    37 | loss: 5.8339767CurrentTrain: epoch  2, batch    38 | loss: 5.2428451CurrentTrain: epoch  2, batch    39 | loss: 5.3104992CurrentTrain: epoch  2, batch    40 | loss: 5.4559865CurrentTrain: epoch  2, batch    41 | loss: 5.6375179CurrentTrain: epoch  2, batch    42 | loss: 4.8446932CurrentTrain: epoch  2, batch    43 | loss: 5.4192162CurrentTrain: epoch  2, batch    44 | loss: 5.3110170CurrentTrain: epoch  2, batch    45 | loss: 5.4885554CurrentTrain: epoch  2, batch    46 | loss: 5.6764097CurrentTrain: epoch  2, batch    47 | loss: 5.4367256CurrentTrain: epoch  2, batch    48 | loss: 4.9975677CurrentTrain: epoch  2, batch    49 | loss: 5.2368460CurrentTrain: epoch  2, batch    50 | loss: 5.4157114CurrentTrain: epoch  2, batch    51 | loss: 6.0181661CurrentTrain: epoch  2, batch    52 | loss: 5.3258100CurrentTrain: epoch  2, batch    53 | loss: 5.0197349CurrentTrain: epoch  2, batch    54 | loss: 5.0001583CurrentTrain: epoch  2, batch    55 | loss: 5.8020535CurrentTrain: epoch  2, batch    56 | loss: 5.9295964CurrentTrain: epoch  2, batch    57 | loss: 6.0543385CurrentTrain: epoch  2, batch    58 | loss: 6.5180469CurrentTrain: epoch  2, batch    59 | loss: 5.6217299CurrentTrain: epoch  2, batch    60 | loss: 5.4285288CurrentTrain: epoch  2, batch    61 | loss: 5.2506371CurrentTrain: epoch  2, batch    62 | loss: 4.7212114CurrentTrain: epoch  3, batch     0 | loss: 5.1655025CurrentTrain: epoch  3, batch     1 | loss: 5.4050665CurrentTrain: epoch  3, batch     2 | loss: 5.9129696CurrentTrain: epoch  3, batch     3 | loss: 6.7268701CurrentTrain: epoch  3, batch     4 | loss: 5.1511545CurrentTrain: epoch  3, batch     5 | loss: 5.6114502CurrentTrain: epoch  3, batch     6 | loss: 5.3671541CurrentTrain: epoch  3, batch     7 | loss: 5.0360885CurrentTrain: epoch  3, batch     8 | loss: 4.8865304CurrentTrain: epoch  3, batch     9 | loss: 5.4480014CurrentTrain: epoch  3, batch    10 | loss: 4.6825600CurrentTrain: epoch  3, batch    11 | loss: 5.2120924CurrentTrain: epoch  3, batch    12 | loss: 4.8736153CurrentTrain: epoch  3, batch    13 | loss: 5.4507875CurrentTrain: epoch  3, batch    14 | loss: 4.9906406CurrentTrain: epoch  3, batch    15 | loss: 5.6238742CurrentTrain: epoch  3, batch    16 | loss: 5.0153942CurrentTrain: epoch  3, batch    17 | loss: 4.8551712CurrentTrain: epoch  3, batch    18 | loss: 5.1458502CurrentTrain: epoch  3, batch    19 | loss: 4.8467817CurrentTrain: epoch  3, batch    20 | loss: 5.2456975CurrentTrain: epoch  3, batch    21 | loss: 5.1337223CurrentTrain: epoch  3, batch    22 | loss: 4.9386058CurrentTrain: epoch  3, batch    23 | loss: 5.3072314CurrentTrain: epoch  3, batch    24 | loss: 4.9230490CurrentTrain: epoch  3, batch    25 | loss: 4.6965194CurrentTrain: epoch  3, batch    26 | loss: 5.5223351CurrentTrain: epoch  3, batch    27 | loss: 4.8334312CurrentTrain: epoch  3, batch    28 | loss: 5.1108351CurrentTrain: epoch  3, batch    29 | loss: 4.6988926CurrentTrain: epoch  3, batch    30 | loss: 4.8203773CurrentTrain: epoch  3, batch    31 | loss: 5.2883043CurrentTrain: epoch  3, batch    32 | loss: 4.6733036CurrentTrain: epoch  3, batch    33 | loss: 4.8529248CurrentTrain: epoch  3, batch    34 | loss: 4.7320700CurrentTrain: epoch  3, batch    35 | loss: 5.0426273CurrentTrain: epoch  3, batch    36 | loss: 4.5960217CurrentTrain: epoch  3, batch    37 | loss: 4.4906683CurrentTrain: epoch  3, batch    38 | loss: 5.0913219CurrentTrain: epoch  3, batch    39 | loss: 4.8620706CurrentTrain: epoch  3, batch    40 | loss: 4.4776320CurrentTrain: epoch  3, batch    41 | loss: 4.7542048CurrentTrain: epoch  3, batch    42 | loss: 4.4646320CurrentTrain: epoch  3, batch    43 | loss: 4.6110020CurrentTrain: epoch  3, batch    44 | loss: 5.2974424CurrentTrain: epoch  3, batch    45 | loss: 4.6681709CurrentTrain: epoch  3, batch    46 | loss: 4.5864129CurrentTrain: epoch  3, batch    47 | loss: 4.5391560CurrentTrain: epoch  3, batch    48 | loss: 4.6921749CurrentTrain: epoch  3, batch    49 | loss: 5.0596199CurrentTrain: epoch  3, batch    50 | loss: 4.7459021CurrentTrain: epoch  3, batch    51 | loss: 4.8058805CurrentTrain: epoch  3, batch    52 | loss: 5.0182900CurrentTrain: epoch  3, batch    53 | loss: 4.7975092CurrentTrain: epoch  3, batch    54 | loss: 5.1291494CurrentTrain: epoch  3, batch    55 | loss: 4.8974628CurrentTrain: epoch  3, batch    56 | loss: 4.7942009CurrentTrain: epoch  3, batch    57 | loss: 4.8226967CurrentTrain: epoch  3, batch    58 | loss: 5.0042939CurrentTrain: epoch  3, batch    59 | loss: 4.5823655CurrentTrain: epoch  3, batch    60 | loss: 4.5736303CurrentTrain: epoch  3, batch    61 | loss: 4.6286421CurrentTrain: epoch  3, batch    62 | loss: 4.3953743CurrentTrain: epoch  4, batch     0 | loss: 4.6566253CurrentTrain: epoch  4, batch     1 | loss: 4.7392201CurrentTrain: epoch  4, batch     2 | loss: 4.5139990CurrentTrain: epoch  4, batch     3 | loss: 4.4695106CurrentTrain: epoch  4, batch     4 | loss: 4.6974506CurrentTrain: epoch  4, batch     5 | loss: 4.6966381CurrentTrain: epoch  4, batch     6 | loss: 4.6680980CurrentTrain: epoch  4, batch     7 | loss: 4.5867238CurrentTrain: epoch  4, batch     8 | loss: 4.4790230CurrentTrain: epoch  4, batch     9 | loss: 4.5629787CurrentTrain: epoch  4, batch    10 | loss: 4.5403299CurrentTrain: epoch  4, batch    11 | loss: 4.5539956CurrentTrain: epoch  4, batch    12 | loss: 5.0616264CurrentTrain: epoch  4, batch    13 | loss: 4.4145670CurrentTrain: epoch  4, batch    14 | loss: 5.0673199CurrentTrain: epoch  4, batch    15 | loss: 4.8285494CurrentTrain: epoch  4, batch    16 | loss: 4.5429344CurrentTrain: epoch  4, batch    17 | loss: 4.5027628CurrentTrain: epoch  4, batch    18 | loss: 4.5208731CurrentTrain: epoch  4, batch    19 | loss: 4.5365229CurrentTrain: epoch  4, batch    20 | loss: 4.6473246CurrentTrain: epoch  4, batch    21 | loss: 4.4468908CurrentTrain: epoch  4, batch    22 | loss: 4.6580296CurrentTrain: epoch  4, batch    23 | loss: 4.4781151CurrentTrain: epoch  4, batch    24 | loss: 4.4025068CurrentTrain: epoch  4, batch    25 | loss: 4.4935589CurrentTrain: epoch  4, batch    26 | loss: 4.4924817CurrentTrain: epoch  4, batch    27 | loss: 4.3720288CurrentTrain: epoch  4, batch    28 | loss: 4.5252619CurrentTrain: epoch  4, batch    29 | loss: 4.5694628CurrentTrain: epoch  4, batch    30 | loss: 4.3529797CurrentTrain: epoch  4, batch    31 | loss: 4.6974020CurrentTrain: epoch  4, batch    32 | loss: 4.5323191CurrentTrain: epoch  4, batch    33 | loss: 4.4414015CurrentTrain: epoch  4, batch    34 | loss: 4.2300344CurrentTrain: epoch  4, batch    35 | loss: 4.7350793CurrentTrain: epoch  4, batch    36 | loss: 4.4091644CurrentTrain: epoch  4, batch    37 | loss: 4.5703926CurrentTrain: epoch  4, batch    38 | loss: 4.6066318CurrentTrain: epoch  4, batch    39 | loss: 5.0312309CurrentTrain: epoch  4, batch    40 | loss: 4.5990334CurrentTrain: epoch  4, batch    41 | loss: 4.3931727CurrentTrain: epoch  4, batch    42 | loss: 4.3830538CurrentTrain: epoch  4, batch    43 | loss: 4.5469804CurrentTrain: epoch  4, batch    44 | loss: 4.3449526CurrentTrain: epoch  4, batch    45 | loss: 4.3718224CurrentTrain: epoch  4, batch    46 | loss: 4.3238430CurrentTrain: epoch  4, batch    47 | loss: 4.3682933CurrentTrain: epoch  4, batch    48 | loss: 4.7626438CurrentTrain: epoch  4, batch    49 | loss: 4.6107230CurrentTrain: epoch  4, batch    50 | loss: 4.6776724CurrentTrain: epoch  4, batch    51 | loss: 4.2470837CurrentTrain: epoch  4, batch    52 | loss: 4.5495424CurrentTrain: epoch  4, batch    53 | loss: 4.3374071CurrentTrain: epoch  4, batch    54 | loss: 4.6287661CurrentTrain: epoch  4, batch    55 | loss: 4.3818445CurrentTrain: epoch  4, batch    56 | loss: 4.3755698CurrentTrain: epoch  4, batch    57 | loss: 4.1664591CurrentTrain: epoch  4, batch    58 | loss: 4.3318663CurrentTrain: epoch  4, batch    59 | loss: 4.2853446CurrentTrain: epoch  4, batch    60 | loss: 4.6107502CurrentTrain: epoch  4, batch    61 | loss: 4.3251801CurrentTrain: epoch  4, batch    62 | loss: 4.2804384CurrentTrain: epoch  5, batch     0 | loss: 4.4342217CurrentTrain: epoch  5, batch     1 | loss: 4.4779930CurrentTrain: epoch  5, batch     2 | loss: 4.4395795CurrentTrain: epoch  5, batch     3 | loss: 4.3147817CurrentTrain: epoch  5, batch     4 | loss: 4.4757919CurrentTrain: epoch  5, batch     5 | loss: 4.4006710CurrentTrain: epoch  5, batch     6 | loss: 4.2941012CurrentTrain: epoch  5, batch     7 | loss: 4.4300828CurrentTrain: epoch  5, batch     8 | loss: 4.3439517CurrentTrain: epoch  5, batch     9 | loss: 4.3977594CurrentTrain: epoch  5, batch    10 | loss: 4.3441844CurrentTrain: epoch  5, batch    11 | loss: 4.2755432CurrentTrain: epoch  5, batch    12 | loss: 4.3184605CurrentTrain: epoch  5, batch    13 | loss: 4.4078560CurrentTrain: epoch  5, batch    14 | loss: 4.3564634CurrentTrain: epoch  5, batch    15 | loss: 4.3155217CurrentTrain: epoch  5, batch    16 | loss: 4.2861567CurrentTrain: epoch  5, batch    17 | loss: 4.7089520CurrentTrain: epoch  5, batch    18 | loss: 4.3902178CurrentTrain: epoch  5, batch    19 | loss: 4.4260044CurrentTrain: epoch  5, batch    20 | loss: 4.2621822CurrentTrain: epoch  5, batch    21 | loss: 4.4003539CurrentTrain: epoch  5, batch    22 | loss: 4.3615656CurrentTrain: epoch  5, batch    23 | loss: 4.3182454CurrentTrain: epoch  5, batch    24 | loss: 4.3402362CurrentTrain: epoch  5, batch    25 | loss: 4.3831196CurrentTrain: epoch  5, batch    26 | loss: 4.2475481CurrentTrain: epoch  5, batch    27 | loss: 4.3373699CurrentTrain: epoch  5, batch    28 | loss: 4.3614173CurrentTrain: epoch  5, batch    29 | loss: 4.1888361CurrentTrain: epoch  5, batch    30 | loss: 4.2256227CurrentTrain: epoch  5, batch    31 | loss: 4.3064413CurrentTrain: epoch  5, batch    32 | loss: 4.2572083CurrentTrain: epoch  5, batch    33 | loss: 4.1958208CurrentTrain: epoch  5, batch    34 | loss: 4.5994134CurrentTrain: epoch  5, batch    35 | loss: 4.3092480CurrentTrain: epoch  5, batch    36 | loss: 4.2971487CurrentTrain: epoch  5, batch    37 | loss: 4.3493185CurrentTrain: epoch  5, batch    38 | loss: 4.2462540CurrentTrain: epoch  5, batch    39 | loss: 4.3033028CurrentTrain: epoch  5, batch    40 | loss: 4.3416233CurrentTrain: epoch  5, batch    41 | loss: 4.2762070CurrentTrain: epoch  5, batch    42 | loss: 4.4687400CurrentTrain: epoch  5, batch    43 | loss: 4.1753216CurrentTrain: epoch  5, batch    44 | loss: 4.4449654CurrentTrain: epoch  5, batch    45 | loss: 4.2848325CurrentTrain: epoch  5, batch    46 | loss: 4.4420872CurrentTrain: epoch  5, batch    47 | loss: 4.4246559CurrentTrain: epoch  5, batch    48 | loss: 4.3105464CurrentTrain: epoch  5, batch    49 | loss: 4.3235998CurrentTrain: epoch  5, batch    50 | loss: 4.2215014CurrentTrain: epoch  5, batch    51 | loss: 4.2971087CurrentTrain: epoch  5, batch    52 | loss: 4.2265759CurrentTrain: epoch  5, batch    53 | loss: 4.1578064CurrentTrain: epoch  5, batch    54 | loss: 4.1868196CurrentTrain: epoch  5, batch    55 | loss: 4.7508163CurrentTrain: epoch  5, batch    56 | loss: 4.1709042CurrentTrain: epoch  5, batch    57 | loss: 4.3132524CurrentTrain: epoch  5, batch    58 | loss: 4.3633494CurrentTrain: epoch  5, batch    59 | loss: 4.2477584CurrentTrain: epoch  5, batch    60 | loss: 4.3858166CurrentTrain: epoch  5, batch    61 | loss: 4.2617650CurrentTrain: epoch  5, batch    62 | loss: 4.2232647CurrentTrain: epoch  6, batch     0 | loss: 4.4026904CurrentTrain: epoch  6, batch     1 | loss: 4.2507162CurrentTrain: epoch  6, batch     2 | loss: 4.2043676CurrentTrain: epoch  6, batch     3 | loss: 4.1695719CurrentTrain: epoch  6, batch     4 | loss: 4.1972313CurrentTrain: epoch  6, batch     5 | loss: 4.3427057CurrentTrain: epoch  6, batch     6 | loss: 4.2391243CurrentTrain: epoch  6, batch     7 | loss: 4.2003412CurrentTrain: epoch  6, batch     8 | loss: 4.2148175CurrentTrain: epoch  6, batch     9 | loss: 4.2465825CurrentTrain: epoch  6, batch    10 | loss: 4.2330680CurrentTrain: epoch  6, batch    11 | loss: 4.1949072CurrentTrain: epoch  6, batch    12 | loss: 4.2780070CurrentTrain: epoch  6, batch    13 | loss: 4.0673685CurrentTrain: epoch  6, batch    14 | loss: 4.2744341CurrentTrain: epoch  6, batch    15 | loss: 4.2212110CurrentTrain: epoch  6, batch    16 | loss: 4.1199646CurrentTrain: epoch  6, batch    17 | loss: 4.4323525CurrentTrain: epoch  6, batch    18 | loss: 4.3111391CurrentTrain: epoch  6, batch    19 | loss: 4.1544075CurrentTrain: epoch  6, batch    20 | loss: 4.5397282CurrentTrain: epoch  6, batch    21 | loss: 4.0836115CurrentTrain: epoch  6, batch    22 | loss: 4.1813107CurrentTrain: epoch  6, batch    23 | loss: 4.3305225CurrentTrain: epoch  6, batch    24 | loss: 4.2397046CurrentTrain: epoch  6, batch    25 | loss: 4.2705688CurrentTrain: epoch  6, batch    26 | loss: 4.1863070CurrentTrain: epoch  6, batch    27 | loss: 4.1749620CurrentTrain: epoch  6, batch    28 | loss: 4.2438369CurrentTrain: epoch  6, batch    29 | loss: 4.1810260CurrentTrain: epoch  6, batch    30 | loss: 4.1142492CurrentTrain: epoch  6, batch    31 | loss: 4.0952220CurrentTrain: epoch  6, batch    32 | loss: 4.2235665CurrentTrain: epoch  6, batch    33 | loss: 4.1740103CurrentTrain: epoch  6, batch    34 | loss: 4.2715759CurrentTrain: epoch  6, batch    35 | loss: 4.2607574CurrentTrain: epoch  6, batch    36 | loss: 4.2282100CurrentTrain: epoch  6, batch    37 | loss: 4.1425991CurrentTrain: epoch  6, batch    38 | loss: 4.1184335CurrentTrain: epoch  6, batch    39 | loss: 4.2198377CurrentTrain: epoch  6, batch    40 | loss: 4.2208557CurrentTrain: epoch  6, batch    41 | loss: 4.3649111CurrentTrain: epoch  6, batch    42 | loss: 4.1746912CurrentTrain: epoch  6, batch    43 | loss: 4.2020144CurrentTrain: epoch  6, batch    44 | loss: 4.1602087CurrentTrain: epoch  6, batch    45 | loss: 4.2443247CurrentTrain: epoch  6, batch    46 | loss: 4.1818190CurrentTrain: epoch  6, batch    47 | loss: 4.2530699CurrentTrain: epoch  6, batch    48 | loss: 4.2213116CurrentTrain: epoch  6, batch    49 | loss: 4.2022085CurrentTrain: epoch  6, batch    50 | loss: 4.5123067CurrentTrain: epoch  6, batch    51 | loss: 4.2976742CurrentTrain: epoch  6, batch    52 | loss: 4.1605330CurrentTrain: epoch  6, batch    53 | loss: 4.1489582CurrentTrain: epoch  6, batch    54 | loss: 4.0756264CurrentTrain: epoch  6, batch    55 | loss: 4.1762066CurrentTrain: epoch  6, batch    56 | loss: 4.4428186CurrentTrain: epoch  6, batch    57 | loss: 4.1449552CurrentTrain: epoch  6, batch    58 | loss: 4.1829815CurrentTrain: epoch  6, batch    59 | loss: 4.1779346CurrentTrain: epoch  6, batch    60 | loss: 4.1467304CurrentTrain: epoch  6, batch    61 | loss: 4.0479441CurrentTrain: epoch  6, batch    62 | loss: 4.0650854CurrentTrain: epoch  7, batch     0 | loss: 4.1812820CurrentTrain: epoch  7, batch     1 | loss: 4.1112771CurrentTrain: epoch  7, batch     2 | loss: 4.1567287CurrentTrain: epoch  7, batch     3 | loss: 4.1946349CurrentTrain: epoch  7, batch     4 | loss: 4.1162014CurrentTrain: epoch  7, batch     5 | loss: 4.1471801CurrentTrain: epoch  7, batch     6 | loss: 4.1595373CurrentTrain: epoch  7, batch     7 | loss: 4.0952640CurrentTrain: epoch  7, batch     8 | loss: 4.0828238CurrentTrain: epoch  7, batch     9 | loss: 4.0863543CurrentTrain: epoch  7, batch    10 | loss: 4.1546774CurrentTrain: epoch  7, batch    11 | loss: 4.1401024CurrentTrain: epoch  7, batch    12 | loss: 4.1446595CurrentTrain: epoch  7, batch    13 | loss: 4.0584106CurrentTrain: epoch  7, batch    14 | loss: 4.1493263CurrentTrain: epoch  7, batch    15 | loss: 4.1086092CurrentTrain: epoch  7, batch    16 | loss: 4.0743294CurrentTrain: epoch  7, batch    17 | loss: 4.0843401CurrentTrain: epoch  7, batch    18 | loss: 4.1091256CurrentTrain: epoch  7, batch    19 | loss: 4.0666528CurrentTrain: epoch  7, batch    20 | loss: 4.1610003CurrentTrain: epoch  7, batch    21 | loss: 4.1515050CurrentTrain: epoch  7, batch    22 | loss: 4.1344242CurrentTrain: epoch  7, batch    23 | loss: 4.1110301CurrentTrain: epoch  7, batch    24 | loss: 4.0845356CurrentTrain: epoch  7, batch    25 | loss: 4.0665174CurrentTrain: epoch  7, batch    26 | loss: 4.1198359CurrentTrain: epoch  7, batch    27 | loss: 4.2237849CurrentTrain: epoch  7, batch    28 | loss: 4.0855436CurrentTrain: epoch  7, batch    29 | loss: 4.0991898CurrentTrain: epoch  7, batch    30 | loss: 4.1121025CurrentTrain: epoch  7, batch    31 | loss: 4.0973687CurrentTrain: epoch  7, batch    32 | loss: 4.0721941CurrentTrain: epoch  7, batch    33 | loss: 4.1235075CurrentTrain: epoch  7, batch    34 | loss: 4.1294608CurrentTrain: epoch  7, batch    35 | loss: 4.1218405CurrentTrain: epoch  7, batch    36 | loss: 4.1076236CurrentTrain: epoch  7, batch    37 | loss: 4.1625676CurrentTrain: epoch  7, batch    38 | loss: 4.1158795CurrentTrain: epoch  7, batch    39 | loss: 4.0743766CurrentTrain: epoch  7, batch    40 | loss: 4.1099873CurrentTrain: epoch  7, batch    41 | loss: 4.1405916CurrentTrain: epoch  7, batch    42 | loss: 4.1472182CurrentTrain: epoch  7, batch    43 | loss: 4.1135731CurrentTrain: epoch  7, batch    44 | loss: 4.1262703CurrentTrain: epoch  7, batch    45 | loss: 4.1129513CurrentTrain: epoch  7, batch    46 | loss: 4.0800037CurrentTrain: epoch  7, batch    47 | loss: 4.1213207CurrentTrain: epoch  7, batch    48 | loss: 4.1066322CurrentTrain: epoch  7, batch    49 | loss: 4.0643301CurrentTrain: epoch  7, batch    50 | loss: 4.0862427CurrentTrain: epoch  7, batch    51 | loss: 4.1338358CurrentTrain: epoch  7, batch    52 | loss: 4.1074400CurrentTrain: epoch  7, batch    53 | loss: 4.1543813CurrentTrain: epoch  7, batch    54 | loss: 4.1101303CurrentTrain: epoch  7, batch    55 | loss: 4.1373167CurrentTrain: epoch  7, batch    56 | loss: 4.0499573CurrentTrain: epoch  7, batch    57 | loss: 4.4470177CurrentTrain: epoch  7, batch    58 | loss: 4.2927408CurrentTrain: epoch  7, batch    59 | loss: 4.1029100CurrentTrain: epoch  7, batch    60 | loss: 4.1054940CurrentTrain: epoch  7, batch    61 | loss: 4.0753260CurrentTrain: epoch  7, batch    62 | loss: 4.0792227CurrentTrain: epoch  8, batch     0 | loss: 4.0624356CurrentTrain: epoch  8, batch     1 | loss: 4.0880404CurrentTrain: epoch  8, batch     2 | loss: 4.0885925CurrentTrain: epoch  8, batch     3 | loss: 4.0868058CurrentTrain: epoch  8, batch     4 | loss: 4.0889940CurrentTrain: epoch  8, batch     5 | loss: 4.0417418CurrentTrain: epoch  8, batch     6 | loss: 4.1103859CurrentTrain: epoch  8, batch     7 | loss: 4.1065307CurrentTrain: epoch  8, batch     8 | loss: 4.1628451CurrentTrain: epoch  8, batch     9 | loss: 4.0540328CurrentTrain: epoch  8, batch    10 | loss: 4.1193175CurrentTrain: epoch  8, batch    11 | loss: 4.1363306CurrentTrain: epoch  8, batch    12 | loss: 4.0501943CurrentTrain: epoch  8, batch    13 | loss: 4.0877552CurrentTrain: epoch  8, batch    14 | loss: 4.0923834CurrentTrain: epoch  8, batch    15 | loss: 4.0642939CurrentTrain: epoch  8, batch    16 | loss: 4.2172885CurrentTrain: epoch  8, batch    17 | loss: 4.0638409CurrentTrain: epoch  8, batch    18 | loss: 4.1061893CurrentTrain: epoch  8, batch    19 | loss: 4.0335741CurrentTrain: epoch  8, batch    20 | loss: 4.1270790CurrentTrain: epoch  8, batch    21 | loss: 4.0913744CurrentTrain: epoch  8, batch    22 | loss: 4.0585051CurrentTrain: epoch  8, batch    23 | loss: 3.9776490CurrentTrain: epoch  8, batch    24 | loss: 4.0659075CurrentTrain: epoch  8, batch    25 | loss: 4.1082511CurrentTrain: epoch  8, batch    26 | loss: 4.1156263CurrentTrain: epoch  8, batch    27 | loss: 4.0585523CurrentTrain: epoch  8, batch    28 | loss: 4.0404162CurrentTrain: epoch  8, batch    29 | loss: 4.0020390CurrentTrain: epoch  8, batch    30 | loss: 4.0547771CurrentTrain: epoch  8, batch    31 | loss: 4.0481205CurrentTrain: epoch  8, batch    32 | loss: 4.0757751CurrentTrain: epoch  8, batch    33 | loss: 4.1222048CurrentTrain: epoch  8, batch    34 | loss: 4.1053467CurrentTrain: epoch  8, batch    35 | loss: 4.0566940CurrentTrain: epoch  8, batch    36 | loss: 4.1181259CurrentTrain: epoch  8, batch    37 | loss: 4.0971050CurrentTrain: epoch  8, batch    38 | loss: 4.0319242CurrentTrain: epoch  8, batch    39 | loss: 4.0810213CurrentTrain: epoch  8, batch    40 | loss: 4.0514679CurrentTrain: epoch  8, batch    41 | loss: 4.0909357CurrentTrain: epoch  8, batch    42 | loss: 4.0781255CurrentTrain: epoch  8, batch    43 | loss: 4.0324984CurrentTrain: epoch  8, batch    44 | loss: 4.1442947CurrentTrain: epoch  8, batch    45 | loss: 4.0715337CurrentTrain: epoch  8, batch    46 | loss: 4.0594740CurrentTrain: epoch  8, batch    47 | loss: 4.0180607CurrentTrain: epoch  8, batch    48 | loss: 4.0213852CurrentTrain: epoch  8, batch    49 | loss: 4.0688143CurrentTrain: epoch  8, batch    50 | loss: 4.0356851CurrentTrain: epoch  8, batch    51 | loss: 4.0548396CurrentTrain: epoch  8, batch    52 | loss: 4.0775557CurrentTrain: epoch  8, batch    53 | loss: 4.0861206CurrentTrain: epoch  8, batch    54 | loss: 4.1350384CurrentTrain: epoch  8, batch    55 | loss: 4.0190730CurrentTrain: epoch  8, batch    56 | loss: 4.0955524CurrentTrain: epoch  8, batch    57 | loss: 4.1077881CurrentTrain: epoch  8, batch    58 | loss: 4.0929403CurrentTrain: epoch  8, batch    59 | loss: 4.0608697CurrentTrain: epoch  8, batch    60 | loss: 4.0470634CurrentTrain: epoch  8, batch    61 | loss: 4.0144587CurrentTrain: epoch  8, batch    62 | loss: 4.0763569CurrentTrain: epoch  9, batch     0 | loss: 4.0519652CurrentTrain: epoch  9, batch     1 | loss: 4.0212965CurrentTrain: epoch  9, batch     2 | loss: 4.0616283CurrentTrain: epoch  9, batch     3 | loss: 4.0829792CurrentTrain: epoch  9, batch     4 | loss: 4.0406218CurrentTrain: epoch  9, batch     5 | loss: 4.0333810CurrentTrain: epoch  9, batch     6 | loss: 4.0537348CurrentTrain: epoch  9, batch     7 | loss: 4.0533228CurrentTrain: epoch  9, batch     8 | loss: 4.0385981CurrentTrain: epoch  9, batch     9 | loss: 4.0751925CurrentTrain: epoch  9, batch    10 | loss: 4.0907016CurrentTrain: epoch  9, batch    11 | loss: 4.0679216CurrentTrain: epoch  9, batch    12 | loss: 4.0536795CurrentTrain: epoch  9, batch    13 | loss: 4.0351996CurrentTrain: epoch  9, batch    14 | loss: 4.0268750CurrentTrain: epoch  9, batch    15 | loss: 4.0274801CurrentTrain: epoch  9, batch    16 | loss: 4.0497856CurrentTrain: epoch  9, batch    17 | loss: 4.0478549CurrentTrain: epoch  9, batch    18 | loss: 4.0235391CurrentTrain: epoch  9, batch    19 | loss: 4.0086021CurrentTrain: epoch  9, batch    20 | loss: 4.0201831CurrentTrain: epoch  9, batch    21 | loss: 4.1566868CurrentTrain: epoch  9, batch    22 | loss: 4.0339098CurrentTrain: epoch  9, batch    23 | loss: 4.0627465CurrentTrain: epoch  9, batch    24 | loss: 3.9837017CurrentTrain: epoch  9, batch    25 | loss: 4.0507736CurrentTrain: epoch  9, batch    26 | loss: 3.9806893CurrentTrain: epoch  9, batch    27 | loss: 4.0646482CurrentTrain: epoch  9, batch    28 | loss: 4.0182247CurrentTrain: epoch  9, batch    29 | loss: 4.0742579CurrentTrain: epoch  9, batch    30 | loss: 4.0718050CurrentTrain: epoch  9, batch    31 | loss: 4.0573874CurrentTrain: epoch  9, batch    32 | loss: 4.0613165CurrentTrain: epoch  9, batch    33 | loss: 4.0464125CurrentTrain: epoch  9, batch    34 | loss: 4.0887308CurrentTrain: epoch  9, batch    35 | loss: 4.0386953CurrentTrain: epoch  9, batch    36 | loss: 4.0039949CurrentTrain: epoch  9, batch    37 | loss: 4.0055108CurrentTrain: epoch  9, batch    38 | loss: 4.0758305CurrentTrain: epoch  9, batch    39 | loss: 4.0716538CurrentTrain: epoch  9, batch    40 | loss: 4.0220184CurrentTrain: epoch  9, batch    41 | loss: 3.9825187CurrentTrain: epoch  9, batch    42 | loss: 4.0279942CurrentTrain: epoch  9, batch    43 | loss: 4.0421634CurrentTrain: epoch  9, batch    44 | loss: 4.0528908CurrentTrain: epoch  9, batch    45 | loss: 4.0061402CurrentTrain: epoch  9, batch    46 | loss: 4.0679464CurrentTrain: epoch  9, batch    47 | loss: 4.0353537CurrentTrain: epoch  9, batch    48 | loss: 4.0609460CurrentTrain: epoch  9, batch    49 | loss: 4.0037656CurrentTrain: epoch  9, batch    50 | loss: 4.0437245CurrentTrain: epoch  9, batch    51 | loss: 4.0335760CurrentTrain: epoch  9, batch    52 | loss: 4.1092281CurrentTrain: epoch  9, batch    53 | loss: 4.0141201CurrentTrain: epoch  9, batch    54 | loss: 3.9950099CurrentTrain: epoch  9, batch    55 | loss: 4.0689106CurrentTrain: epoch  9, batch    56 | loss: 4.0317116CurrentTrain: epoch  9, batch    57 | loss: 4.9714422CurrentTrain: epoch  9, batch    58 | loss: 4.4004660CurrentTrain: epoch  9, batch    59 | loss: 4.5477438CurrentTrain: epoch  9, batch    60 | loss: 3.9809153CurrentTrain: epoch  9, batch    61 | loss: 4.0218759CurrentTrain: epoch  9, batch    62 | loss: 4.5981216
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the theme of the loss of identity in crowds in this painting can be seen as a precursor to expressionist painting , particularly the work of james ensor .
Head Entity: james ensor
Tail Entity: expressionist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The surrealist movement, characterized by its exploration of the unconscious mind, was significantly influenced by the works of Salvador Dalí.  
Head Entity: Salvador Dalí  
Tail Entity: surrealist  

Relation: movement  
Context: The emergence of the romantic movement in literature was marked by the profound impact of poets like William Wordsworth and Samuel Taylor Coleridge.  
Head Entity: William Wordsworth  
Tail Entity: romantic  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The historic town of Bath is famous for its Roman baths and is situated within the unitary authority of Bath and North East Somerset in England.  
Head Entity: Bath  
Tail Entity: Bath and North East Somerset  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: in october 1913 , jessie fauset wrote to joel spingarn , who was deeply involved with the naacp , explaining lee ’s situation .
Head Entity: joel spingarn
Tail Entity: naacp
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: during her time at the university, sarah joined the debate team, where she honed her skills and made lasting friendships with fellow members.  
Head Entity: sarah  
Tail Entity: debate team  

Relation: member of  
Context: after years of dedication to the sport, michael was proud to be recognized as a member of the national soccer team, representing his country in international competitions.  
Head Entity: michael  
Tail Entity: national soccer team  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the solkan bridge ( , ) is a arch bridge over the soča river near nova gorica in western slovenia ( by railway terminology it is a viaduct ) .
Head Entity: solkan bridge
Tail Entity: soča river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge ( , ) spans the golden gate strait, the entrance to san francisco bay from the pacific ocean.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the channel tunnel ( , ) runs beneath the english channel, connecting the united kingdom with mainland europe.  
Head Entity: channel tunnel  
Tail Entity: english channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the babocomari river is a major tributary of the upper san pedro river in southeastern arizona .
Head Entity: san pedro river
Tail Entity: babocomari river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: the missouri river is the longest tributary of the mississippi river, flowing through several states in the united states.  
Head Entity: mississippi river  
Tail Entity: missouri river  

Relation: tributary  
Context: the thames river has numerous tributaries, including the river colne, which contributes to its flow in london.  
Head Entity: thames river  
Tail Entity: river colne  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: democratic candidates include former attorney general drew edmondson and former state senator connie johnson .
Head Entity: drew edmondson
Tail Entity: attorney general
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: during his tenure, former president barack obama implemented several key policies that shaped the nation.  
Head Entity: barack obama  
Tail Entity: president  

Relation: position held  
Context: the board of directors announced that jane smith has been appointed as the new chief executive officer of the company.  
Head Entity: jane smith  
Tail Entity: chief executive officer  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: " jedi outcast " was developed by raven software and powered by the i d tech 3 game engine .
Head Entity: jedi outcast
Tail Entity: raven software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and has received numerous awards for its storytelling and gameplay.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Minecraft" was developed by Mojang Studios and has become one of the best-selling video games of all time.  
Head Entity: Minecraft  
Tail Entity: Mojang Studios  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: morbid angel was formed in 1983 in tampa , florida by guitarist trey azagthoth and drummer / vocalist mike browning .
Head Entity: morbid angel
Tail Entity: tampa , florida
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the band coldplay was formed in 1996 at university college london, where the members met while studying.  
Head Entity: coldplay  
Tail Entity: university college london  

Relation: location of formation  
Context: the popular rock group green day originated in 1986 in berkeley, california, when the members were still teenagers.  
Head Entity: green day  
Tail Entity: berkeley, california  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: during the same month , the band supported kings of leon 's tour of the united states , and coldplay 's tour of the united kingdom .
Head Entity: kings of leon
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous Italian dish, pizza, has gained popularity worldwide, but its roots can be traced back to Naples, Italy.  
Head Entity: pizza  
Tail Entity: Italy  

Relation: country of origin  
Context: The iconic brand, Rolex, is renowned for its luxury watches, which are crafted in Switzerland, known for its precision and quality in watchmaking.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.11%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.18%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.80%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.87%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 95.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.80%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.77%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.04%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.79%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.36%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 94.60%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.79%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.44%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.93%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.02%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.20%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.11%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.18%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.91%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.80%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.87%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 95.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.80%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.77%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.04%   
cur_acc:  ['0.9504']
his_acc:  ['0.9504']
CurrentTrain: epoch  0, batch     0 | loss: 6.6454353CurrentTrain: epoch  0, batch     1 | loss: 5.8237824CurrentTrain: epoch  0, batch     2 | loss: 5.7595987CurrentTrain: epoch  0, batch     3 | loss: 6.5461292CurrentTrain: epoch  1, batch     0 | loss: 5.3493376CurrentTrain: epoch  1, batch     1 | loss: 5.8535299CurrentTrain: epoch  1, batch     2 | loss: 4.6879349CurrentTrain: epoch  1, batch     3 | loss: 2.1812778CurrentTrain: epoch  2, batch     0 | loss: 3.9675598CurrentTrain: epoch  2, batch     1 | loss: 3.3686616CurrentTrain: epoch  2, batch     2 | loss: 4.1500444CurrentTrain: epoch  2, batch     3 | loss: 2.0236535CurrentTrain: epoch  3, batch     0 | loss: 3.0910804CurrentTrain: epoch  3, batch     1 | loss: 3.3812981CurrentTrain: epoch  3, batch     2 | loss: 3.1195846CurrentTrain: epoch  3, batch     3 | loss: 2.5998933CurrentTrain: epoch  4, batch     0 | loss: 3.2118578CurrentTrain: epoch  4, batch     1 | loss: 2.8194108CurrentTrain: epoch  4, batch     2 | loss: 2.9794538CurrentTrain: epoch  4, batch     3 | loss: 3.3024008CurrentTrain: epoch  5, batch     0 | loss: 3.8177850CurrentTrain: epoch  5, batch     1 | loss: 2.5316694CurrentTrain: epoch  5, batch     2 | loss: 2.4568729CurrentTrain: epoch  5, batch     3 | loss: 2.0377679CurrentTrain: epoch  6, batch     0 | loss: 2.5476346CurrentTrain: epoch  6, batch     1 | loss: 2.5161753CurrentTrain: epoch  6, batch     2 | loss: 2.5741160CurrentTrain: epoch  6, batch     3 | loss: 5.9202824CurrentTrain: epoch  7, batch     0 | loss: 2.4718685CurrentTrain: epoch  7, batch     1 | loss: 2.7238588CurrentTrain: epoch  7, batch     2 | loss: 2.2636909CurrentTrain: epoch  7, batch     3 | loss: 2.5562172CurrentTrain: epoch  8, batch     0 | loss: 2.0622644CurrentTrain: epoch  8, batch     1 | loss: 2.2979162CurrentTrain: epoch  8, batch     2 | loss: 2.4959979CurrentTrain: epoch  8, batch     3 | loss: 2.2725487CurrentTrain: epoch  9, batch     0 | loss: 2.3195627CurrentTrain: epoch  9, batch     1 | loss: 1.9483116CurrentTrain: epoch  9, batch     2 | loss: 2.1107547CurrentTrain: epoch  9, batch     3 | loss: 2.6416006
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor, she became a prominent figure in the Democratic Party, advocating for social justice and community development.  
Head Entity: she  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: During his tenure in the Senate, he was known for his strong support of the Republican Party's policies on tax reform and national security.  
Head Entity: he  
Tail Entity: Republican Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the original story of real life escape of betty mahmoody is depicted in the movie " not without my daughter " which itself was based on betty mahmoody 's book of the same name .
Head Entity: not without my daughter
Tail Entity: betty mahmoody
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from f. scott fitzgerald's classic novel, capturing the essence of the roaring twenties and the complexities of love and ambition.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the animated feature "the lion king" was inspired by shakespeare's play "hamlet," incorporating themes of betrayal, revenge, and the struggle for power.  
Head Entity: the lion king  
Tail Entity: shakespeare
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the group named itself after the popular children 's book harold and the purple crayon by crockett johnson , whose protagonist , harold , uses a purple crayon to draw his imagination into reality .
Head Entity: crockett johnson
Tail Entity: harold and the purple crayon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: The renowned physicist Albert Einstein is best known for his groundbreaking theory of relativity, which revolutionized our understanding of space, time, and gravity.  
Head Entity: Albert Einstein  
Tail Entity: theory of relativity  

Relation: notable work  
Context: The famous author J.K. Rowling gained worldwide fame with her Harry Potter series, which has captivated millions of readers and inspired a successful film franchise.  
Head Entity: J.K. Rowling  
Tail Entity: Harry Potter series  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: The global headquarters of the tech giant is located in Silicon Valley, where it has been a major player in the industry for over two decades.  
Head Entity: tech giant  
Tail Entity: Silicon Valley  

Relation: headquarters location  
Context: After years of expansion, the non-profit organization finally established its main office in the heart of New York City, allowing for better outreach and collaboration.  
Head Entity: non-profit organization  
Tail Entity: New York City  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie was renowned for her groundbreaking research in radioactivity, which laid the foundation for future advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: wjet erie , pennsylvania was his first official radio job outside of duties performed in the u.s. air force .
Head Entity: wjet
Tail Entity: erie , pennsylvania
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: kxyz radio serves the community of springfield, providing news and entertainment to its listeners.  
Head Entity: kxyz radio  
Tail Entity: springfield  

Relation: licensed to broadcast to  
Context: the station wabc is known for its extensive coverage of new york city events and news.  
Head Entity: wabc  
Tail Entity: new york city  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion and is one of the brightest nebulae visible to the naked eye.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star betelgeuse is a prominent feature in the constellation orion, known for its reddish hue.  
Head Entity: betelgeuse  
Tail Entity: orion  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Mixup data size:  198
MixupTrain:  epoch  0, batch     0 | loss: 6.0626486MixupTrain:  epoch  0, batch     1 | loss: 5.7594191MixupTrain:  epoch  0, batch     2 | loss: 5.7866250MixupTrain:  epoch  0, batch     3 | loss: 6.2724675MixupTrain:  epoch  0, batch     4 | loss: 5.9292001MixupTrain:  epoch  0, batch     5 | loss: 6.6720243MixupTrain:  epoch  0, batch     6 | loss: 5.7525728MixupTrain:  epoch  0, batch     7 | loss: 5.1234582MixupTrain:  epoch  0, batch     8 | loss: 5.3500114MixupTrain:  epoch  0, batch     9 | loss: 5.0171345MixupTrain:  epoch  0, batch    10 | loss: 4.6177482MixupTrain:  epoch  0, batch    11 | loss: 3.9225920MixupTrain:  epoch  0, batch    12 | loss: 4.4817351
MemoryTrain:  epoch  0, batch     0 | loss: 4.7683592MemoryTrain:  epoch  0, batch     1 | loss: 4.4445043MemoryTrain:  epoch  0, batch     2 | loss: 3.5281944MemoryTrain:  epoch  0, batch     3 | loss: 4.3558588MemoryTrain:  epoch  1, batch     0 | loss: 3.3831310MemoryTrain:  epoch  1, batch     1 | loss: 3.4579868MemoryTrain:  epoch  1, batch     2 | loss: 4.0567036MemoryTrain:  epoch  1, batch     3 | loss: 3.9834573MemoryTrain:  epoch  2, batch     0 | loss: 2.6886981MemoryTrain:  epoch  2, batch     1 | loss: 2.6483619MemoryTrain:  epoch  2, batch     2 | loss: 2.7774167MemoryTrain:  epoch  2, batch     3 | loss: 3.2400436MemoryTrain:  epoch  3, batch     0 | loss: 2.0248132MemoryTrain:  epoch  3, batch     1 | loss: 2.3833675MemoryTrain:  epoch  3, batch     2 | loss: 1.9524443MemoryTrain:  epoch  3, batch     3 | loss: 2.7144411MemoryTrain:  epoch  4, batch     0 | loss: 1.7149904MemoryTrain:  epoch  4, batch     1 | loss: 1.7499578MemoryTrain:  epoch  4, batch     2 | loss: 2.6900191MemoryTrain:  epoch  4, batch     3 | loss: 1.8383760MemoryTrain:  epoch  5, batch     0 | loss: 1.9842525MemoryTrain:  epoch  5, batch     1 | loss: 1.4989920MemoryTrain:  epoch  5, batch     2 | loss: 2.0374105MemoryTrain:  epoch  5, batch     3 | loss: 1.9742372MemoryTrain:  epoch  6, batch     0 | loss: 1.6681192MemoryTrain:  epoch  6, batch     1 | loss: 2.0210783MemoryTrain:  epoch  6, batch     2 | loss: 1.9170728MemoryTrain:  epoch  6, batch     3 | loss: 1.6371465MemoryTrain:  epoch  7, batch     0 | loss: 2.2577691MemoryTrain:  epoch  7, batch     1 | loss: 1.4913715MemoryTrain:  epoch  7, batch     2 | loss: 1.6748936MemoryTrain:  epoch  7, batch     3 | loss: 1.7641596MemoryTrain:  epoch  8, batch     0 | loss: 1.9021071MemoryTrain:  epoch  8, batch     1 | loss: 1.6422958MemoryTrain:  epoch  8, batch     2 | loss: 1.6167164MemoryTrain:  epoch  8, batch     3 | loss: 1.7806658MemoryTrain:  epoch  9, batch     0 | loss: 1.6173556MemoryTrain:  epoch  9, batch     1 | loss: 1.5613427MemoryTrain:  epoch  9, batch     2 | loss: 1.6477294MemoryTrain:  epoch  9, batch     3 | loss: 1.4212332
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 91.02%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 89.47%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 77.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.60%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 82.03%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 82.39%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 82.35%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.68%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 82.60%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 82.89%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 84.88%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.09%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 85.73%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 86.04%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 86.76%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.90%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.03%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 87.16%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 87.39%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 87.61%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 87.61%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 88.01%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 88.10%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 87.50%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 82.59%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.93%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 86.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 86.08%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 85.60%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.83%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.15%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.86%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.13%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.85%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 91.28%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.48%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 91.58%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 91.49%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.41%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.58%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 91.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.79%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 91.71%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.75%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 91.55%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 91.14%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 90.96%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 91.01%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 90.95%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 91.43%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 91.47%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 91.31%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 91.15%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 91.10%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 90.86%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 90.99%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.37%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 91.49%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 91.61%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 91.64%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 91.53%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 91.51%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 91.30%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 91.13%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 90.85%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 90.21%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 89.43%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 88.90%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 88.30%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 87.72%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 87.57%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 87.71%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 87.85%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 87.98%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 88.11%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 88.36%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 88.29%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 88.35%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 88.34%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 88.20%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 88.26%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 88.19%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 88.30%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 88.42%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 88.53%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 88.80%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 88.90%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 89.00%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 89.11%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 89.30%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 89.40%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 89.44%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 89.36%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 89.40%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 89.49%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 89.48%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 89.51%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 89.55%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 89.64%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 89.62%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 89.65%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 89.74%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 89.82%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 89.85%   
cur_acc:  ['0.9504', '0.8750']
his_acc:  ['0.9504', '0.8985']
CurrentTrain: epoch  0, batch     0 | loss: 6.4387774CurrentTrain: epoch  0, batch     1 | loss: 7.7600026CurrentTrain: epoch  0, batch     2 | loss: 6.7486277CurrentTrain: epoch  0, batch     3 | loss: 5.5037613CurrentTrain: epoch  1, batch     0 | loss: 5.6480432CurrentTrain: epoch  1, batch     1 | loss: 6.4222665CurrentTrain: epoch  1, batch     2 | loss: 6.4227214CurrentTrain: epoch  1, batch     3 | loss: 6.5694704CurrentTrain: epoch  2, batch     0 | loss: 5.3153419CurrentTrain: epoch  2, batch     1 | loss: 5.6719356CurrentTrain: epoch  2, batch     2 | loss: 5.2205658CurrentTrain: epoch  2, batch     3 | loss: 3.3730531CurrentTrain: epoch  3, batch     0 | loss: 4.0131607CurrentTrain: epoch  3, batch     1 | loss: 5.0847473CurrentTrain: epoch  3, batch     2 | loss: 4.6896381CurrentTrain: epoch  3, batch     3 | loss: 7.6541619CurrentTrain: epoch  4, batch     0 | loss: 4.8011770CurrentTrain: epoch  4, batch     1 | loss: 4.1084909CurrentTrain: epoch  4, batch     2 | loss: 4.6219945CurrentTrain: epoch  4, batch     3 | loss: 3.9623299CurrentTrain: epoch  5, batch     0 | loss: 4.0537615CurrentTrain: epoch  5, batch     1 | loss: 4.6480985CurrentTrain: epoch  5, batch     2 | loss: 4.0550175CurrentTrain: epoch  5, batch     3 | loss: 3.6733253CurrentTrain: epoch  6, batch     0 | loss: 3.6140966CurrentTrain: epoch  6, batch     1 | loss: 3.8393931CurrentTrain: epoch  6, batch     2 | loss: 4.4974704CurrentTrain: epoch  6, batch     3 | loss: 3.4261501CurrentTrain: epoch  7, batch     0 | loss: 3.0304060CurrentTrain: epoch  7, batch     1 | loss: 3.4684594CurrentTrain: epoch  7, batch     2 | loss: 4.7659173CurrentTrain: epoch  7, batch     3 | loss: 2.5945137CurrentTrain: epoch  8, batch     0 | loss: 3.8737562CurrentTrain: epoch  8, batch     1 | loss: 3.5266397CurrentTrain: epoch  8, batch     2 | loss: 3.1334953CurrentTrain: epoch  8, batch     3 | loss: 3.0107086CurrentTrain: epoch  9, batch     0 | loss: 3.9242101CurrentTrain: epoch  9, batch     1 | loss: 3.0560844CurrentTrain: epoch  9, batch     2 | loss: 3.2238073CurrentTrain: epoch  9, batch     3 | loss: 2.4210980
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and organizations come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy, with many fans attending the matches.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: " runaway reptar " was written by ali marie matheson and jon cooksey and directed by john holmquist and jim duffy .
Head Entity: runaway reptar
Tail Entity: john holmquist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: "Inception" was directed by Christopher Nolan, who is known for his innovative storytelling and visual effects.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: director  
Context: The acclaimed stage play "Hamilton" was directed by Thomas Kail, bringing Lin-Manuel Miranda's vision to life on Broadway.  
Head Entity: Hamilton  
Tail Entity: Thomas Kail  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game developer, Bungie, announced that it had become an independent studio, having previously been owned by Microsoft.  
Head Entity: Bungie  
Tail Entity: Microsoft  

Relation: owned by  
Context: The luxury fashion brand Gucci, known for its high-end products, is owned by the French conglomerate Kering.  
Head Entity: Gucci  
Tail Entity: Kering  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: remains of taxa that can be confidently assigned to istiodactylidae have been found in the uk and china , in rocks dating from the early cretaceous period ( barremian to aptian stage ) .
Head Entity: barremian
Tail Entity: early cretaceous
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The Great Wall of China is a series of fortifications made of various materials, and it is part of the UNESCO World Heritage Sites.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Sites  

Relation: part of  
Context: The human heart is a vital organ that is part of the circulatory system, responsible for pumping blood throughout the body.  
Head Entity: human heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and now it serves as a gallery showcasing his works, attracting visitors from all over the world.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now thrive.  
Head Entity: old factory  
Tail Entity: startups
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: reading town hall was built in several phases between 1786 and 1897 , although the principal facade was designed by alfred waterhouse in 1875 .
Head Entity: reading town hall
Tail Entity: alfred waterhouse
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the new art museum was designed by renowned architect zaha hadid, whose innovative style has transformed modern architecture.  
Head Entity: new art museum  
Tail Entity: zaha hadid  

Relation: architect  
Context: the iconic sydney opera house was the brainchild of architect jørn utzon, who won the design competition in 1957.  
Head Entity: sydney opera house  
Tail Entity: jørn utzon  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: fantcha was born as francelina durão almeida in mindelo on são vicente island , cape verde .
Head Entity: fantcha
Tail Entity: são vicente island
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york city for several years, john decided to move back to his hometown in los angeles.  
Head Entity: john  
Tail Entity: los angeles  

Relation: residence  
Context: the famous author spent most of her life in a quaint cottage located in the picturesque village of haworth.  
Head Entity: the famous author  
Tail Entity: haworth  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the banks of the Potomac River, a significant site in American history.  
Head Entity: historic battle  
Tail Entity: Potomac River  
Mixup data size:  259
MixupTrain:  epoch  0, batch     0 | loss: 3.4592272MixupTrain:  epoch  0, batch     1 | loss: 3.1824425MixupTrain:  epoch  0, batch     2 | loss: 5.0263167MixupTrain:  epoch  0, batch     3 | loss: 3.1150545MixupTrain:  epoch  0, batch     4 | loss: 3.8897323MixupTrain:  epoch  0, batch     5 | loss: 3.6696144MixupTrain:  epoch  0, batch     6 | loss: 3.5056166MixupTrain:  epoch  0, batch     7 | loss: 3.6951502MixupTrain:  epoch  0, batch     8 | loss: 3.6414593MixupTrain:  epoch  0, batch     9 | loss: 3.1591431MixupTrain:  epoch  0, batch    10 | loss: 4.0596476MixupTrain:  epoch  0, batch    11 | loss: 2.9258092MixupTrain:  epoch  0, batch    12 | loss: 2.8835803MixupTrain:  epoch  0, batch    13 | loss: 3.7099208MixupTrain:  epoch  0, batch    14 | loss: 4.3242704MixupTrain:  epoch  0, batch    15 | loss: 2.5959574MixupTrain:  epoch  0, batch    16 | loss: 2.2816510
MemoryTrain:  epoch  0, batch     0 | loss: 2.8584228MemoryTrain:  epoch  0, batch     1 | loss: 2.5441937MemoryTrain:  epoch  0, batch     2 | loss: 3.6488359MemoryTrain:  epoch  0, batch     3 | loss: 3.2503276MemoryTrain:  epoch  0, batch     4 | loss: 3.5533285MemoryTrain:  epoch  0, batch     5 | loss: 3.2826831MemoryTrain:  epoch  1, batch     0 | loss: 2.7974968MemoryTrain:  epoch  1, batch     1 | loss: 2.2153838MemoryTrain:  epoch  1, batch     2 | loss: 3.0605211MemoryTrain:  epoch  1, batch     3 | loss: 2.3199434MemoryTrain:  epoch  1, batch     4 | loss: 2.7674754MemoryTrain:  epoch  1, batch     5 | loss: 2.0131800MemoryTrain:  epoch  2, batch     0 | loss: 2.4228587MemoryTrain:  epoch  2, batch     1 | loss: 1.6721398MemoryTrain:  epoch  2, batch     2 | loss: 2.4639645MemoryTrain:  epoch  2, batch     3 | loss: 2.7176714MemoryTrain:  epoch  2, batch     4 | loss: 2.2124057MemoryTrain:  epoch  2, batch     5 | loss: 1.8867788MemoryTrain:  epoch  3, batch     0 | loss: 2.1759622MemoryTrain:  epoch  3, batch     1 | loss: 1.8770994MemoryTrain:  epoch  3, batch     2 | loss: 2.3674293MemoryTrain:  epoch  3, batch     3 | loss: 1.9588797MemoryTrain:  epoch  3, batch     4 | loss: 2.4496126MemoryTrain:  epoch  3, batch     5 | loss: 1.8691345MemoryTrain:  epoch  4, batch     0 | loss: 2.1208875MemoryTrain:  epoch  4, batch     1 | loss: 1.8856456MemoryTrain:  epoch  4, batch     2 | loss: 2.6392233MemoryTrain:  epoch  4, batch     3 | loss: 1.7501674MemoryTrain:  epoch  4, batch     4 | loss: 1.7052307MemoryTrain:  epoch  4, batch     5 | loss: 1.7997429MemoryTrain:  epoch  5, batch     0 | loss: 2.0822663MemoryTrain:  epoch  5, batch     1 | loss: 1.6958289MemoryTrain:  epoch  5, batch     2 | loss: 1.7855117MemoryTrain:  epoch  5, batch     3 | loss: 2.2023017MemoryTrain:  epoch  5, batch     4 | loss: 1.5788490MemoryTrain:  epoch  5, batch     5 | loss: 1.6498755MemoryTrain:  epoch  6, batch     0 | loss: 2.0481696MemoryTrain:  epoch  6, batch     1 | loss: 1.5157320MemoryTrain:  epoch  6, batch     2 | loss: 1.8010070MemoryTrain:  epoch  6, batch     3 | loss: 1.8378096MemoryTrain:  epoch  6, batch     4 | loss: 1.5860615MemoryTrain:  epoch  6, batch     5 | loss: 1.8409576MemoryTrain:  epoch  7, batch     0 | loss: 1.8090358MemoryTrain:  epoch  7, batch     1 | loss: 1.6140828MemoryTrain:  epoch  7, batch     2 | loss: 1.7167612MemoryTrain:  epoch  7, batch     3 | loss: 1.5047357MemoryTrain:  epoch  7, batch     4 | loss: 1.6459072MemoryTrain:  epoch  7, batch     5 | loss: 1.6362740MemoryTrain:  epoch  8, batch     0 | loss: 1.6342396MemoryTrain:  epoch  8, batch     1 | loss: 1.4931693MemoryTrain:  epoch  8, batch     2 | loss: 1.7313293MemoryTrain:  epoch  8, batch     3 | loss: 1.5903566MemoryTrain:  epoch  8, batch     4 | loss: 1.4493514MemoryTrain:  epoch  8, batch     5 | loss: 1.4841814MemoryTrain:  epoch  9, batch     0 | loss: 1.6588744MemoryTrain:  epoch  9, batch     1 | loss: 1.4563655MemoryTrain:  epoch  9, batch     2 | loss: 1.4502339MemoryTrain:  epoch  9, batch     3 | loss: 1.5920079MemoryTrain:  epoch  9, batch     4 | loss: 1.4523165MemoryTrain:  epoch  9, batch     5 | loss: 1.4915107
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 53.91%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 59.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 74.34%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 73.44%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 72.32%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 72.73%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 72.55%   [EVAL] batch:   23 | acc: 43.75%,  total acc: 71.35%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 12.50%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 67.59%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 65.85%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 64.44%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 63.54%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 61.69%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 61.72%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 62.69%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 63.24%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 64.11%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 65.20%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 66.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 68.30%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 69.60%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 70.52%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 70.88%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 71.22%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 71.68%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 71.57%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 71.39%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 71.23%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 71.30%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 71.36%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 71.54%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 71.05%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 70.69%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 70.02%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 69.67%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 69.76%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 68.95%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 79.89%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.18%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.23%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 88.61%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 88.72%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 88.56%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 88.41%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.65%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 88.88%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 88.85%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 88.82%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.92%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 88.54%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 88.18%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 88.17%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 88.05%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 88.04%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 88.14%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 88.32%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 88.59%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 88.38%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 88.27%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 88.26%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 88.15%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 88.33%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 88.50%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 88.21%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 87.94%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 87.85%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 87.67%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 87.25%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 87.17%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 87.01%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 86.78%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 86.47%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 86.17%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 85.96%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 85.67%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 84.94%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 84.30%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 83.60%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 83.28%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 82.61%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 82.53%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 82.72%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 83.29%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 83.64%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 83.62%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 83.72%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 83.70%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 83.55%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 83.52%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 83.44%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 83.60%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 83.76%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 83.92%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 84.23%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 84.32%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 84.61%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 84.75%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 84.89%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 85.02%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 85.20%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 85.27%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 85.34%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 85.36%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 85.38%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 85.45%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 85.57%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 85.59%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 86.00%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 85.57%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 85.29%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 84.86%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 84.69%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 84.42%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 84.11%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 84.07%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 84.19%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 84.26%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 84.33%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 84.35%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 84.33%   [EVAL] batch:  138 | acc: 100.00%,  total acc: 84.44%   [EVAL] batch:  139 | acc: 87.50%,  total acc: 84.46%   [EVAL] batch:  140 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:  141 | acc: 93.75%,  total acc: 84.64%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 84.62%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 84.46%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 84.27%   [EVAL] batch:  145 | acc: 50.00%,  total acc: 84.03%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 84.01%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 83.91%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 83.64%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 83.54%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 83.07%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 82.73%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 82.31%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 81.94%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 81.65%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 81.17%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 81.05%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 81.13%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 81.13%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 81.21%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 81.21%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 81.33%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 81.36%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 81.48%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 81.59%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 81.55%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 81.66%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 81.73%   [EVAL] batch:  169 | acc: 93.75%,  total acc: 81.80%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 81.83%   [EVAL] batch:  171 | acc: 87.50%,  total acc: 81.87%   [EVAL] batch:  172 | acc: 87.50%,  total acc: 81.90%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 81.97%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 82.00%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 81.82%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 81.71%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 81.56%   [EVAL] batch:  179 | acc: 75.00%,  total acc: 81.53%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 81.53%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 81.32%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 81.15%   [EVAL] batch:  183 | acc: 31.25%,  total acc: 80.88%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 80.81%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 80.65%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 80.61%   [EVAL] batch:  187 | acc: 18.75%,  total acc: 80.29%   
cur_acc:  ['0.9504', '0.8750', '0.6895']
his_acc:  ['0.9504', '0.8985', '0.8029']
CurrentTrain: epoch  0, batch     0 | loss: 5.1245484CurrentTrain: epoch  0, batch     1 | loss: 5.8143148CurrentTrain: epoch  0, batch     2 | loss: 6.2509856CurrentTrain: epoch  0, batch     3 | loss: 4.4726009CurrentTrain: epoch  1, batch     0 | loss: 5.0992184CurrentTrain: epoch  1, batch     1 | loss: 4.6618385CurrentTrain: epoch  1, batch     2 | loss: 4.7180943CurrentTrain: epoch  1, batch     3 | loss: 6.9613624CurrentTrain: epoch  2, batch     0 | loss: 4.4222727CurrentTrain: epoch  2, batch     1 | loss: 4.5729451CurrentTrain: epoch  2, batch     2 | loss: 4.0612249CurrentTrain: epoch  2, batch     3 | loss: 6.7225652CurrentTrain: epoch  3, batch     0 | loss: 3.9314263CurrentTrain: epoch  3, batch     1 | loss: 3.9407091CurrentTrain: epoch  3, batch     2 | loss: 4.2108307CurrentTrain: epoch  3, batch     3 | loss: 5.2731333CurrentTrain: epoch  4, batch     0 | loss: 3.4793348CurrentTrain: epoch  4, batch     1 | loss: 3.6548302CurrentTrain: epoch  4, batch     2 | loss: 4.0489197CurrentTrain: epoch  4, batch     3 | loss: 2.0195427CurrentTrain: epoch  5, batch     0 | loss: 3.2170405CurrentTrain: epoch  5, batch     1 | loss: 3.4048972CurrentTrain: epoch  5, batch     2 | loss: 3.9869142CurrentTrain: epoch  5, batch     3 | loss: 2.8607643CurrentTrain: epoch  6, batch     0 | loss: 3.1984968CurrentTrain: epoch  6, batch     1 | loss: 2.9467583CurrentTrain: epoch  6, batch     2 | loss: 3.8371253CurrentTrain: epoch  6, batch     3 | loss: 3.1348538CurrentTrain: epoch  7, batch     0 | loss: 3.4866824CurrentTrain: epoch  7, batch     1 | loss: 2.5993609CurrentTrain: epoch  7, batch     2 | loss: 3.1713533CurrentTrain: epoch  7, batch     3 | loss: 3.0624053CurrentTrain: epoch  8, batch     0 | loss: 3.2131577CurrentTrain: epoch  8, batch     1 | loss: 2.6804726CurrentTrain: epoch  8, batch     2 | loss: 2.9168518CurrentTrain: epoch  8, batch     3 | loss: 3.2893984CurrentTrain: epoch  9, batch     0 | loss: 2.9682770CurrentTrain: epoch  9, batch     1 | loss: 2.6904554CurrentTrain: epoch  9, batch     2 | loss: 2.3940301CurrentTrain: epoch  9, batch     3 | loss: 2.2761979
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of Australia are the major political divisions, with New South Wales being the most populous state in the country.  
Head Entity: Australia  
Tail Entity: New South Wales  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "soda" and "pop" are said to be the same as each other, though this varies by region.  
Head Entity: soda  
Tail Entity: pop  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: he considers his father to be biggest influence on his career as his brother javier castellano recipient of four eclipse award for outstanding jockey in the row ( 2013 , 2014,2015 and 2016 ) .
Head Entity: eclipse award for outstanding jockey
Tail Entity: javier castellano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In the thrilling finale of the championship, Sarah Thompson emerged victorious, claiming the title of best player in the tournament, while her teammate, Mark Johnson, was awarded the runner-up position.  
Head Entity: best player in the tournament  
Tail Entity: Sarah Thompson  

Relation: winner  
Context: The annual science fair concluded with Emily Chen taking home the grand prize for her innovative project on renewable energy, while her classmate, David Lee, received an honorable mention for his work on robotics.  
Head Entity: grand prize  
Tail Entity: Emily Chen  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the railroad car they were deported in was attached to the end of the last train out of drancy which also carried drancy commandant ss hauptsturmführer alois brunner and other german military personnel .
Head Entity: alois brunner
Tail Entity: hauptsturmführer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: General John Smith was awarded the title of Major General for his exceptional leadership during the conflict, which significantly contributed to the victory of his battalion.  
Head Entity: John Smith  
Tail Entity: Major General  

Relation: military rank  
Context: During the ceremony, Colonel Jane Doe was recognized for her outstanding service and was promoted to the rank of Brigadier General, a position she had long aspired to achieve.  
Head Entity: Jane Doe  
Tail Entity: Brigadier General  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: there are seven different nations that are allied or in conflict : prior to its north american release , " vanguard bandits " was titled " detonator gauntlet " by working designs .
Head Entity: vanguard bandits
Tail Entity: working designs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by the acclaimed author was released by Penguin Random House, a well-known publishing house in the industry.  
Head Entity: latest novel  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: After years of hard work, the game was finally published by Electronic Arts, bringing excitement to gamers worldwide.  
Head Entity: game  
Tail Entity: Electronic Arts  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " choose you " and " homesick " were released as the album 's second and third singles , respectively , and each attained moderate chart success .
Head Entity: choose you
Tail Entity: homesick
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter delves into the backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: The opening act of the concert was electrifying, followed by a stunning performance from the headliner.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: he was also associated with robert wilkinson in producing " londina illustrata " , an illustrated account of ancient buildings in london and westminster in two volumes ( 1819–25 ) .
Head Entity: robert wilkinson
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in Silicon Valley, where they have been developing innovative technology solutions for over a decade.  
Head Entity: the company  
Tail Entity: Silicon Valley  

Relation: work location  
Context: During her time at the university, she conducted research in various laboratories located in Cambridge, contributing significantly to her field of study.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working on innovative artificial intelligence projects for several years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: the george boxley cabin , davenport - bradfield house , and sheridan downtown commercial historic district are listed on the national register of historic places .
Head Entity: sheridan downtown commercial historic district
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: the ancient city of petra in jordan is recognized as a unesco world heritage site due to its archaeological significance.  
Head Entity: ancient city of petra  
Tail Entity: unesco world heritage site  

Relation: heritage designation  
Context: the great barrier reef, known for its stunning marine biodiversity, was designated a world heritage site by unesco in 1981.  
Head Entity: great barrier reef  
Tail Entity: world heritage site
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: ada adini ( 1855 – february 1924 ) was an american operatic soprano who had an active international career from 1876 up into the first decade of the 20th century .
Head Entity: ada adini
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12, 1935 – september 6, 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27, 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: soprano  
Mixup data size:  318
MixupTrain:  epoch  0, batch     0 | loss: 2.5383247MixupTrain:  epoch  0, batch     1 | loss: 2.4289778MixupTrain:  epoch  0, batch     2 | loss: 2.7351407MixupTrain:  epoch  0, batch     3 | loss: 2.3664782MixupTrain:  epoch  0, batch     4 | loss: 3.3542190MixupTrain:  epoch  0, batch     5 | loss: 2.4446290MixupTrain:  epoch  0, batch     6 | loss: 2.5734106MixupTrain:  epoch  0, batch     7 | loss: 2.4486535MixupTrain:  epoch  0, batch     8 | loss: 2.7798724MixupTrain:  epoch  0, batch     9 | loss: 2.4430125MixupTrain:  epoch  0, batch    10 | loss: 2.5896214MixupTrain:  epoch  0, batch    11 | loss: 2.5328314MixupTrain:  epoch  0, batch    12 | loss: 3.0118820MixupTrain:  epoch  0, batch    13 | loss: 2.7076881MixupTrain:  epoch  0, batch    14 | loss: 2.3745909MixupTrain:  epoch  0, batch    15 | loss: 2.6837541MixupTrain:  epoch  0, batch    16 | loss: 2.2935751MixupTrain:  epoch  0, batch    17 | loss: 2.0841413MixupTrain:  epoch  0, batch    18 | loss: 2.3322561MixupTrain:  epoch  0, batch    19 | loss: 2.1082271
MemoryTrain:  epoch  0, batch     0 | loss: 2.1627645MemoryTrain:  epoch  0, batch     1 | loss: 3.0021677MemoryTrain:  epoch  0, batch     2 | loss: 2.5216522MemoryTrain:  epoch  0, batch     3 | loss: 2.7808363MemoryTrain:  epoch  0, batch     4 | loss: 2.5884047MemoryTrain:  epoch  0, batch     5 | loss: 3.0300164MemoryTrain:  epoch  0, batch     6 | loss: 1.7655085MemoryTrain:  epoch  0, batch     7 | loss: 3.5902514MemoryTrain:  epoch  1, batch     0 | loss: 2.4185991MemoryTrain:  epoch  1, batch     1 | loss: 2.5232022MemoryTrain:  epoch  1, batch     2 | loss: 1.9147253MemoryTrain:  epoch  1, batch     3 | loss: 2.6788831MemoryTrain:  epoch  1, batch     4 | loss: 2.1169853MemoryTrain:  epoch  1, batch     5 | loss: 2.3583558MemoryTrain:  epoch  1, batch     6 | loss: 1.9299769MemoryTrain:  epoch  1, batch     7 | loss: 2.2823138MemoryTrain:  epoch  2, batch     0 | loss: 1.9203385MemoryTrain:  epoch  2, batch     1 | loss: 2.0871358MemoryTrain:  epoch  2, batch     2 | loss: 2.0217316MemoryTrain:  epoch  2, batch     3 | loss: 1.7826899MemoryTrain:  epoch  2, batch     4 | loss: 2.1354473MemoryTrain:  epoch  2, batch     5 | loss: 2.0848699MemoryTrain:  epoch  2, batch     6 | loss: 1.5668534MemoryTrain:  epoch  2, batch     7 | loss: 1.4822750MemoryTrain:  epoch  3, batch     0 | loss: 1.9654312MemoryTrain:  epoch  3, batch     1 | loss: 1.7931696MemoryTrain:  epoch  3, batch     2 | loss: 1.7119849MemoryTrain:  epoch  3, batch     3 | loss: 1.7261844MemoryTrain:  epoch  3, batch     4 | loss: 1.4798400MemoryTrain:  epoch  3, batch     5 | loss: 1.6172448MemoryTrain:  epoch  3, batch     6 | loss: 1.9149160MemoryTrain:  epoch  3, batch     7 | loss: 1.6004418MemoryTrain:  epoch  4, batch     0 | loss: 1.6970022MemoryTrain:  epoch  4, batch     1 | loss: 1.4280989MemoryTrain:  epoch  4, batch     2 | loss: 1.6386423MemoryTrain:  epoch  4, batch     3 | loss: 1.6830583MemoryTrain:  epoch  4, batch     4 | loss: 1.6043708MemoryTrain:  epoch  4, batch     5 | loss: 1.4998100MemoryTrain:  epoch  4, batch     6 | loss: 1.7713076MemoryTrain:  epoch  4, batch     7 | loss: 2.1345601MemoryTrain:  epoch  5, batch     0 | loss: 1.9137940MemoryTrain:  epoch  5, batch     1 | loss: 1.6349564MemoryTrain:  epoch  5, batch     2 | loss: 1.7650383MemoryTrain:  epoch  5, batch     3 | loss: 1.3314428MemoryTrain:  epoch  5, batch     4 | loss: 1.5908993MemoryTrain:  epoch  5, batch     5 | loss: 1.4277722MemoryTrain:  epoch  5, batch     6 | loss: 1.6295509MemoryTrain:  epoch  5, batch     7 | loss: 1.4589797MemoryTrain:  epoch  6, batch     0 | loss: 1.7780608MemoryTrain:  epoch  6, batch     1 | loss: 1.6839664MemoryTrain:  epoch  6, batch     2 | loss: 1.3756006MemoryTrain:  epoch  6, batch     3 | loss: 1.5373909MemoryTrain:  epoch  6, batch     4 | loss: 1.6590631MemoryTrain:  epoch  6, batch     5 | loss: 1.4446809MemoryTrain:  epoch  6, batch     6 | loss: 1.4885250MemoryTrain:  epoch  6, batch     7 | loss: 1.2455246MemoryTrain:  epoch  7, batch     0 | loss: 1.5411152MemoryTrain:  epoch  7, batch     1 | loss: 1.4940487MemoryTrain:  epoch  7, batch     2 | loss: 1.3589903MemoryTrain:  epoch  7, batch     3 | loss: 1.5247747MemoryTrain:  epoch  7, batch     4 | loss: 1.4539452MemoryTrain:  epoch  7, batch     5 | loss: 1.4224224MemoryTrain:  epoch  7, batch     6 | loss: 1.4435928MemoryTrain:  epoch  7, batch     7 | loss: 1.9936659MemoryTrain:  epoch  8, batch     0 | loss: 1.6909268MemoryTrain:  epoch  8, batch     1 | loss: 1.5419406MemoryTrain:  epoch  8, batch     2 | loss: 1.4106812MemoryTrain:  epoch  8, batch     3 | loss: 1.5664463MemoryTrain:  epoch  8, batch     4 | loss: 1.2570394MemoryTrain:  epoch  8, batch     5 | loss: 1.3331896MemoryTrain:  epoch  8, batch     6 | loss: 1.5022867MemoryTrain:  epoch  8, batch     7 | loss: 1.3022931MemoryTrain:  epoch  9, batch     0 | loss: 1.3328307MemoryTrain:  epoch  9, batch     1 | loss: 1.4558703MemoryTrain:  epoch  9, batch     2 | loss: 1.3400664MemoryTrain:  epoch  9, batch     3 | loss: 1.2867888MemoryTrain:  epoch  9, batch     4 | loss: 1.4004036MemoryTrain:  epoch  9, batch     5 | loss: 1.4922588MemoryTrain:  epoch  9, batch     6 | loss: 1.2520416MemoryTrain:  epoch  9, batch     7 | loss: 1.3844168
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 42.97%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 47.22%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 54.33%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 57.08%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 58.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 59.19%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 63.16%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 69.02%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 71.39%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 71.30%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 71.34%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 72.38%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 72.46%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 72.35%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 70.77%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 70.18%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 69.10%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 67.74%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 67.43%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 66.99%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 66.72%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 66.16%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 65.77%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 65.55%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 65.20%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 65.42%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 65.82%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 66.02%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 66.33%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 69.10%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 71.12%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 71.50%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 72.52%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 78.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.11%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.28%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 84.74%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 85.81%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 87.20%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 87.21%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 87.22%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 87.08%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 86.55%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 86.04%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 85.68%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 85.46%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 84.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 84.68%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 84.62%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 84.55%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 84.03%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 83.52%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 83.37%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 83.51%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 83.69%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 84.12%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 84.52%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 84.47%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 84.42%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 84.47%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 84.33%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 84.56%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 84.64%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 84.60%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 84.55%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 84.50%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 84.25%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 84.13%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 83.97%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 83.70%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 83.67%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 83.56%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 83.23%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 82.38%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 81.47%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 80.74%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 80.23%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 79.45%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 79.33%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 79.95%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 80.38%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 80.52%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 80.39%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 80.47%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 80.28%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 80.10%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 80.05%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 79.81%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 80.01%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 80.40%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 80.77%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 80.90%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 81.07%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 81.42%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 81.59%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 81.76%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 82.07%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 82.17%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 82.33%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 82.37%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 82.52%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 82.62%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 82.76%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 82.80%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 82.94%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 83.08%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 83.22%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 83.30%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 82.84%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 82.48%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 82.08%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 81.83%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 81.59%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 81.30%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 81.30%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 81.53%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 81.62%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 81.66%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 81.66%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 81.74%   [EVAL] batch:  139 | acc: 87.50%,  total acc: 81.79%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 81.83%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 81.87%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 81.91%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 81.77%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 81.55%   [EVAL] batch:  145 | acc: 43.75%,  total acc: 81.29%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 81.21%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 81.08%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 80.83%   [EVAL] batch:  149 | acc: 56.25%,  total acc: 80.67%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 80.17%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 79.81%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 79.41%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 78.94%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 78.55%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 78.04%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 77.87%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 77.97%   [EVAL] batch:  158 | acc: 75.00%,  total acc: 77.95%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.05%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 78.07%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 78.22%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 78.28%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 78.50%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 78.48%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 78.70%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 78.64%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 78.55%   [EVAL] batch:  171 | acc: 50.00%,  total acc: 78.38%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 78.32%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 78.38%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 78.18%   [EVAL] batch:  175 | acc: 56.25%,  total acc: 78.05%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 78.00%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 77.88%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 77.90%   [EVAL] batch:  179 | acc: 81.25%,  total acc: 77.92%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 77.94%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 77.82%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 77.66%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 77.48%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 77.47%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 77.32%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 77.37%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 77.13%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 76.88%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 76.71%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 76.51%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 76.30%   [EVAL] batch:  192 | acc: 25.00%,  total acc: 76.04%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 75.93%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 75.90%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 75.86%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 75.92%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 75.69%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 75.81%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 75.80%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 75.83%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 75.80%   [EVAL] batch:  204 | acc: 87.50%,  total acc: 75.85%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 75.94%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 76.03%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 76.26%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 76.34%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 76.42%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 76.53%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:  213 | acc: 75.00%,  total acc: 76.55%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 76.57%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:  217 | acc: 93.75%,  total acc: 76.63%   [EVAL] batch:  218 | acc: 68.75%,  total acc: 76.60%   [EVAL] batch:  219 | acc: 75.00%,  total acc: 76.59%   [EVAL] batch:  220 | acc: 31.25%,  total acc: 76.39%   [EVAL] batch:  221 | acc: 43.75%,  total acc: 76.24%   [EVAL] batch:  222 | acc: 37.50%,  total acc: 76.07%   [EVAL] batch:  223 | acc: 25.00%,  total acc: 75.84%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 75.72%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 75.58%   [EVAL] batch:  226 | acc: 43.75%,  total acc: 75.44%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 75.33%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 75.27%   [EVAL] batch:  229 | acc: 50.00%,  total acc: 75.16%   [EVAL] batch:  230 | acc: 50.00%,  total acc: 75.05%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:  232 | acc: 68.75%,  total acc: 74.97%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:  234 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 75.05%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 75.34%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 75.41%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 75.72%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:  245 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 75.96%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 76.06%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 76.15%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 76.25%   
cur_acc:  ['0.9504', '0.8750', '0.6895', '0.7252']
his_acc:  ['0.9504', '0.8985', '0.8029', '0.7625']
CurrentTrain: epoch  0, batch     0 | loss: 5.8650823CurrentTrain: epoch  0, batch     1 | loss: 5.7599430CurrentTrain: epoch  0, batch     2 | loss: 5.6500120CurrentTrain: epoch  0, batch     3 | loss: 6.6665850CurrentTrain: epoch  1, batch     0 | loss: 4.4462199CurrentTrain: epoch  1, batch     1 | loss: 5.4729528CurrentTrain: epoch  1, batch     2 | loss: 5.7287760CurrentTrain: epoch  1, batch     3 | loss: 3.8322916CurrentTrain: epoch  2, batch     0 | loss: 4.3173823CurrentTrain: epoch  2, batch     1 | loss: 4.4826050CurrentTrain: epoch  2, batch     2 | loss: 5.8075752CurrentTrain: epoch  2, batch     3 | loss: 4.8056912CurrentTrain: epoch  3, batch     0 | loss: 4.1799941CurrentTrain: epoch  3, batch     1 | loss: 3.8938959CurrentTrain: epoch  3, batch     2 | loss: 5.1541729CurrentTrain: epoch  3, batch     3 | loss: 3.4856691CurrentTrain: epoch  4, batch     0 | loss: 4.1287661CurrentTrain: epoch  4, batch     1 | loss: 4.4142265CurrentTrain: epoch  4, batch     2 | loss: 3.8761888CurrentTrain: epoch  4, batch     3 | loss: 1.9471936CurrentTrain: epoch  5, batch     0 | loss: 3.1906872CurrentTrain: epoch  5, batch     1 | loss: 3.7006214CurrentTrain: epoch  5, batch     2 | loss: 4.4911470CurrentTrain: epoch  5, batch     3 | loss: 6.1284981CurrentTrain: epoch  6, batch     0 | loss: 3.9606984CurrentTrain: epoch  6, batch     1 | loss: 3.7773573CurrentTrain: epoch  6, batch     2 | loss: 3.2294345CurrentTrain: epoch  6, batch     3 | loss: 3.6802354CurrentTrain: epoch  7, batch     0 | loss: 3.2255507CurrentTrain: epoch  7, batch     1 | loss: 3.0052776CurrentTrain: epoch  7, batch     2 | loss: 3.9256248CurrentTrain: epoch  7, batch     3 | loss: 1.9771887CurrentTrain: epoch  8, batch     0 | loss: 3.0740647CurrentTrain: epoch  8, batch     1 | loss: 2.8475473CurrentTrain: epoch  8, batch     2 | loss: 3.3363035CurrentTrain: epoch  8, batch     3 | loss: 1.9261875CurrentTrain: epoch  9, batch     0 | loss: 2.5337501CurrentTrain: epoch  9, batch     1 | loss: 3.0063210CurrentTrain: epoch  9, batch     2 | loss: 2.9459743CurrentTrain: epoch  9, batch     3 | loss: 3.2149143
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift performed her hit single "Shake It Off" at the Grammy Awards, captivating the audience with her energetic stage presence.  
Head Entity: Shake It Off  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the next full - frame digital slrs were the canon eos-1ds of late 2002 , followed by kodak 's dcs pro 14n in 2003 .
Head Entity: dcs pro 14n
Tail Entity: kodak
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The latest smartphone model, the Galaxy S21, was released by Samsung in January 2021, showcasing advanced camera technology and performance.  
Head Entity: Galaxy S21  
Tail Entity: Samsung  

Relation: manufacturer  
Context: The iconic Model T was produced by Ford Motor Company, revolutionizing the automotive industry in the early 20th century.  
Head Entity: Model T  
Tail Entity: Ford Motor Company  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: norway was represented in the eurovision song contest 2005 by the song " in my dreams " performed by wig wam .
Head Entity: eurovision song contest 2005
Tail Entity: wig wam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: the united states was represented in the 2020 summer olympics by a team of over 600 athletes competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: united states  

Relation: participant  
Context: the world health organization organized a global conference in 2021, where numerous countries participated to discuss pandemic preparedness.  
Head Entity: global conference 2021  
Tail Entity: world health organization  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: " robowar " is a similar game that was released later on the macintosh , and is now available for both mac os and windows .
Head Entity: robowar
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: "The latest version of Adobe Photoshop is compatible with both Windows 10 and macOS Monterey."  
Head Entity: Adobe Photoshop  
Tail Entity: Windows 10  

Relation: operating system  
Context: "Minecraft runs smoothly on various platforms, including Linux and the latest version of Windows."  
Head Entity: Minecraft  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete was classified under the FIS Alpine Skiing World Cup, allowing him to compete at the highest level.  
Head Entity: FIS Alpine Skiing World Cup  
Tail Entity: highest level  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: swartz served as swedish prime minister for only seven months before being forced from power after the swedish elections of 1917 and replaced by the liberal government of nils edén .
Head Entity: swedish elections of 1917
Tail Entity: nils edén
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: in the recent municipal elections, the voters of springfield chose jane doe as their new mayor, marking a significant shift in the city's leadership.  
Head Entity: municipal elections  
Tail Entity: jane doe  

Relation: successful candidate  
Context: after a closely contested race, the citizens of riverdale elected john smith as their representative in the state assembly, defeating the incumbent.  
Head Entity: state assembly election  
Tail Entity: john smith  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce loyalty to each other, having grown up under the same roof and shared the same parents.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it was evident that both Clara and her brother, James, inherited their parents' artistic talents, showcasing their skills in painting and music.  
Head Entity: Clara  
Tail Entity: James  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: dennis chalker is a retired navy seal , inventor and author who has written six books about the united states navy seals .
Head Entity: dennis chalker
Tail Entity: united states navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: the general served in the air force for over two decades before retiring and taking on a role in civilian leadership.  
Head Entity: the general  
Tail Entity: air force  

Relation: military branch  
Context: after completing his training, the soldier was assigned to the army's special forces unit, where he excelled in various missions.  
Head Entity: the soldier  
Tail Entity: army  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom, who often embark on thrilling quests together.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902 and later became a subject of much speculation.  
Head Entity: albert einstein  
Tail Entity: lieserl  
Mixup data size:  378
MixupTrain:  epoch  0, batch     0 | loss: 2.7413527MixupTrain:  epoch  0, batch     1 | loss: 2.4052231MixupTrain:  epoch  0, batch     2 | loss: 2.1485286MixupTrain:  epoch  0, batch     3 | loss: 2.8708039MixupTrain:  epoch  0, batch     4 | loss: 2.5141052MixupTrain:  epoch  0, batch     5 | loss: 2.5859663MixupTrain:  epoch  0, batch     6 | loss: 1.9969739MixupTrain:  epoch  0, batch     7 | loss: 2.1740825MixupTrain:  epoch  0, batch     8 | loss: 2.3945099MixupTrain:  epoch  0, batch     9 | loss: 2.3012986MixupTrain:  epoch  0, batch    10 | loss: 2.4683419MixupTrain:  epoch  0, batch    11 | loss: 2.4097615MixupTrain:  epoch  0, batch    12 | loss: 2.4249710MixupTrain:  epoch  0, batch    13 | loss: 1.9622503MixupTrain:  epoch  0, batch    14 | loss: 2.3596931MixupTrain:  epoch  0, batch    15 | loss: 2.0772002MixupTrain:  epoch  0, batch    16 | loss: 2.0599100MixupTrain:  epoch  0, batch    17 | loss: 2.0063702MixupTrain:  epoch  0, batch    18 | loss: 2.5674879MixupTrain:  epoch  0, batch    19 | loss: 2.3746157MixupTrain:  epoch  0, batch    20 | loss: 2.3498668MixupTrain:  epoch  0, batch    21 | loss: 2.1791247MixupTrain:  epoch  0, batch    22 | loss: 1.8686189MixupTrain:  epoch  0, batch    23 | loss: 1.9778361
MemoryTrain:  epoch  0, batch     0 | loss: 1.7877932MemoryTrain:  epoch  0, batch     1 | loss: 1.9039688MemoryTrain:  epoch  0, batch     2 | loss: 2.6343660MemoryTrain:  epoch  0, batch     3 | loss: 1.9255803MemoryTrain:  epoch  0, batch     4 | loss: 2.1510673MemoryTrain:  epoch  0, batch     5 | loss: 2.3978915MemoryTrain:  epoch  0, batch     6 | loss: 2.8213983MemoryTrain:  epoch  0, batch     7 | loss: 2.6981645MemoryTrain:  epoch  0, batch     8 | loss: 2.5143182MemoryTrain:  epoch  0, batch     9 | loss: 3.5070198MemoryTrain:  epoch  1, batch     0 | loss: 2.0619109MemoryTrain:  epoch  1, batch     1 | loss: 2.4203382MemoryTrain:  epoch  1, batch     2 | loss: 1.6052234MemoryTrain:  epoch  1, batch     3 | loss: 1.9512321MemoryTrain:  epoch  1, batch     4 | loss: 2.0811689MemoryTrain:  epoch  1, batch     5 | loss: 2.3760293MemoryTrain:  epoch  1, batch     6 | loss: 1.4778324MemoryTrain:  epoch  1, batch     7 | loss: 1.6217784MemoryTrain:  epoch  1, batch     8 | loss: 2.2226021MemoryTrain:  epoch  1, batch     9 | loss: 1.3523223MemoryTrain:  epoch  2, batch     0 | loss: 2.0175281MemoryTrain:  epoch  2, batch     1 | loss: 1.8066318MemoryTrain:  epoch  2, batch     2 | loss: 1.7606347MemoryTrain:  epoch  2, batch     3 | loss: 1.6365498MemoryTrain:  epoch  2, batch     4 | loss: 1.8202322MemoryTrain:  epoch  2, batch     5 | loss: 1.6094795MemoryTrain:  epoch  2, batch     6 | loss: 1.6097019MemoryTrain:  epoch  2, batch     7 | loss: 1.4579062MemoryTrain:  epoch  2, batch     8 | loss: 1.7206001MemoryTrain:  epoch  2, batch     9 | loss: 1.3721871MemoryTrain:  epoch  3, batch     0 | loss: 1.4001194MemoryTrain:  epoch  3, batch     1 | loss: 1.5567023MemoryTrain:  epoch  3, batch     2 | loss: 1.6847302MemoryTrain:  epoch  3, batch     3 | loss: 1.7717316MemoryTrain:  epoch  3, batch     4 | loss: 1.4065876MemoryTrain:  epoch  3, batch     5 | loss: 1.5823131MemoryTrain:  epoch  3, batch     6 | loss: 1.6589712MemoryTrain:  epoch  3, batch     7 | loss: 1.5348721MemoryTrain:  epoch  3, batch     8 | loss: 1.5649395MemoryTrain:  epoch  3, batch     9 | loss: 1.7420105MemoryTrain:  epoch  4, batch     0 | loss: 1.5903962MemoryTrain:  epoch  4, batch     1 | loss: 1.2837342MemoryTrain:  epoch  4, batch     2 | loss: 1.5148623MemoryTrain:  epoch  4, batch     3 | loss: 1.4223135MemoryTrain:  epoch  4, batch     4 | loss: 1.4175655MemoryTrain:  epoch  4, batch     5 | loss: 1.4584794MemoryTrain:  epoch  4, batch     6 | loss: 1.5841410MemoryTrain:  epoch  4, batch     7 | loss: 1.3238184MemoryTrain:  epoch  4, batch     8 | loss: 1.7129710MemoryTrain:  epoch  4, batch     9 | loss: 1.2160289MemoryTrain:  epoch  5, batch     0 | loss: 1.4143245MemoryTrain:  epoch  5, batch     1 | loss: 1.2793617MemoryTrain:  epoch  5, batch     2 | loss: 1.4311539MemoryTrain:  epoch  5, batch     3 | loss: 1.6403399MemoryTrain:  epoch  5, batch     4 | loss: 1.4558163MemoryTrain:  epoch  5, batch     5 | loss: 1.4495356MemoryTrain:  epoch  5, batch     6 | loss: 1.3050764MemoryTrain:  epoch  5, batch     7 | loss: 1.4299008MemoryTrain:  epoch  5, batch     8 | loss: 1.5258260MemoryTrain:  epoch  5, batch     9 | loss: 1.3654217MemoryTrain:  epoch  6, batch     0 | loss: 1.4021339MemoryTrain:  epoch  6, batch     1 | loss: 1.3727754MemoryTrain:  epoch  6, batch     2 | loss: 1.4504921MemoryTrain:  epoch  6, batch     3 | loss: 1.2879345MemoryTrain:  epoch  6, batch     4 | loss: 1.3631024MemoryTrain:  epoch  6, batch     5 | loss: 1.2857440MemoryTrain:  epoch  6, batch     6 | loss: 1.3007004MemoryTrain:  epoch  6, batch     7 | loss: 1.5215974MemoryTrain:  epoch  6, batch     8 | loss: 1.4778972MemoryTrain:  epoch  6, batch     9 | loss: 1.6272668MemoryTrain:  epoch  7, batch     0 | loss: 1.3086033MemoryTrain:  epoch  7, batch     1 | loss: 1.3279855MemoryTrain:  epoch  7, batch     2 | loss: 1.2822604MemoryTrain:  epoch  7, batch     3 | loss: 1.2998875MemoryTrain:  epoch  7, batch     4 | loss: 1.2435116MemoryTrain:  epoch  7, batch     5 | loss: 1.4901714MemoryTrain:  epoch  7, batch     6 | loss: 1.4643382MemoryTrain:  epoch  7, batch     7 | loss: 1.3232071MemoryTrain:  epoch  7, batch     8 | loss: 1.3329060MemoryTrain:  epoch  7, batch     9 | loss: 1.3117613MemoryTrain:  epoch  8, batch     0 | loss: 1.2705181MemoryTrain:  epoch  8, batch     1 | loss: 1.2919601MemoryTrain:  epoch  8, batch     2 | loss: 1.2981284MemoryTrain:  epoch  8, batch     3 | loss: 1.3980918MemoryTrain:  epoch  8, batch     4 | loss: 1.2201736MemoryTrain:  epoch  8, batch     5 | loss: 1.2597694MemoryTrain:  epoch  8, batch     6 | loss: 1.3272548MemoryTrain:  epoch  8, batch     7 | loss: 1.4340997MemoryTrain:  epoch  8, batch     8 | loss: 1.2259926MemoryTrain:  epoch  8, batch     9 | loss: 1.2407801MemoryTrain:  epoch  9, batch     0 | loss: 1.3441627MemoryTrain:  epoch  9, batch     1 | loss: 1.2914083MemoryTrain:  epoch  9, batch     2 | loss: 1.2297755MemoryTrain:  epoch  9, batch     3 | loss: 1.2633091MemoryTrain:  epoch  9, batch     4 | loss: 1.3177868MemoryTrain:  epoch  9, batch     5 | loss: 1.2821755MemoryTrain:  epoch  9, batch     6 | loss: 1.3018302MemoryTrain:  epoch  9, batch     7 | loss: 1.2170265MemoryTrain:  epoch  9, batch     8 | loss: 1.3318592MemoryTrain:  epoch  9, batch     9 | loss: 1.2653413
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 85.20%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.19%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 89.22%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 88.87%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.84%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.95%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 88.64%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 87.78%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 86.96%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 86.30%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 85.81%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 85.20%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 84.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.80%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.98%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.14%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 85.07%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 85.27%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 85.09%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 84.91%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 84.75%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 84.32%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 83.87%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 83.23%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.24%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.74%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 81.64%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.20%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 82.72%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 83.51%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 83.95%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 85.37%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 85.57%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 85.61%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 85.37%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 85.00%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 84.24%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 83.24%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 82.55%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 82.14%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 81.00%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 80.89%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 80.90%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 80.56%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 80.23%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 80.02%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 80.04%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 80.17%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 80.30%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 80.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 80.84%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 81.15%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 81.35%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 81.35%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 81.35%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 81.53%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 81.53%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 81.80%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 82.05%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 81.95%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 81.94%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 81.93%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 81.84%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 81.92%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 81.83%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 81.74%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 81.65%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 81.41%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 81.41%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 81.33%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 80.95%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 80.12%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 79.24%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 78.53%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 78.05%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 77.23%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 77.06%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 77.32%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 77.75%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 78.29%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 78.39%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 78.22%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 78.06%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 78.03%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 77.75%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 77.97%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 78.40%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 78.81%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 78.95%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 79.15%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 79.34%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 79.72%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 79.90%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 80.02%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 80.03%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 79.88%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 79.95%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 79.96%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 79.91%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 79.93%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 79.94%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 80.10%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 80.17%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 80.33%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 80.49%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 80.75%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 80.36%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 80.02%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 79.74%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 79.51%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 79.28%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 79.01%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 78.98%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 78.99%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 79.15%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 79.26%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 79.37%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 79.43%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 79.44%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 79.45%   [EVAL] batch:  139 | acc: 87.50%,  total acc: 79.51%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 79.57%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 79.62%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 79.59%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 79.34%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 79.14%   [EVAL] batch:  145 | acc: 25.00%,  total acc: 78.77%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 78.74%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 78.72%   [EVAL] batch:  148 | acc: 43.75%,  total acc: 78.48%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 78.38%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 77.90%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 77.55%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 77.21%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 76.75%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 76.41%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 75.96%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 75.84%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.95%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 76.02%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.13%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 76.16%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.27%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 76.38%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 76.45%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 76.59%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 76.80%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 77.07%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 76.99%   [EVAL] batch:  170 | acc: 68.75%,  total acc: 76.94%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 76.74%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 76.59%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 76.62%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 76.39%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 76.03%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 75.78%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 75.56%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 75.38%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 75.21%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 75.14%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 74.93%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 74.80%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 74.59%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 74.59%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 74.46%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 74.53%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 74.27%   [EVAL] batch:  188 | acc: 50.00%,  total acc: 74.14%   [EVAL] batch:  189 | acc: 50.00%,  total acc: 74.01%   [EVAL] batch:  190 | acc: 62.50%,  total acc: 73.95%   [EVAL] batch:  191 | acc: 68.75%,  total acc: 73.93%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 73.83%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 73.68%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 73.62%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 73.60%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 73.60%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 73.64%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 73.46%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 73.54%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 73.48%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 73.46%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 73.38%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 73.35%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 73.42%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 73.52%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 73.65%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 73.77%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 73.87%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 74.18%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 74.21%   [EVAL] batch:  214 | acc: 81.25%,  total acc: 74.24%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 74.25%   [EVAL] batch:  216 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 74.28%   [EVAL] batch:  218 | acc: 62.50%,  total acc: 74.23%   [EVAL] batch:  219 | acc: 68.75%,  total acc: 74.20%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 73.98%   [EVAL] batch:  221 | acc: 37.50%,  total acc: 73.82%   [EVAL] batch:  222 | acc: 31.25%,  total acc: 73.63%   [EVAL] batch:  223 | acc: 25.00%,  total acc: 73.41%   [EVAL] batch:  224 | acc: 50.00%,  total acc: 73.31%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 73.17%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 73.13%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 73.08%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 73.06%   [EVAL] batch:  229 | acc: 62.50%,  total acc: 73.02%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 72.94%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 72.93%   [EVAL] batch:  232 | acc: 62.50%,  total acc: 72.88%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:  234 | acc: 75.00%,  total acc: 72.93%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 72.93%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 73.08%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 73.28%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 73.37%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 73.59%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:  245 | acc: 93.75%,  total acc: 73.88%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.17%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:  250 | acc: 81.25%,  total acc: 74.30%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 74.33%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 74.36%   [EVAL] batch:  254 | acc: 87.50%,  total acc: 74.41%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 74.54%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 74.54%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 74.57%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 74.59%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 74.62%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 74.62%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 74.71%   [EVAL] batch:  263 | acc: 93.75%,  total acc: 74.79%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 74.81%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 74.86%   [EVAL] batch:  266 | acc: 93.75%,  total acc: 74.93%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 75.05%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 75.07%   [EVAL] batch:  270 | acc: 68.75%,  total acc: 75.05%   [EVAL] batch:  271 | acc: 81.25%,  total acc: 75.07%   [EVAL] batch:  272 | acc: 81.25%,  total acc: 75.09%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 75.16%   [EVAL] batch:  274 | acc: 75.00%,  total acc: 75.16%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 75.95%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 76.03%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 76.11%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 76.32%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 76.34%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 76.33%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 76.37%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 76.43%   [EVAL] batch:  293 | acc: 75.00%,  total acc: 76.42%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 76.33%   [EVAL] batch:  295 | acc: 50.00%,  total acc: 76.25%   [EVAL] batch:  296 | acc: 56.25%,  total acc: 76.18%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 76.13%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 76.07%   [EVAL] batch:  299 | acc: 56.25%,  total acc: 76.00%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 76.06%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 76.18%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 76.19%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 76.29%   [EVAL] batch:  306 | acc: 75.00%,  total acc: 76.28%   [EVAL] batch:  307 | acc: 75.00%,  total acc: 76.28%   [EVAL] batch:  308 | acc: 75.00%,  total acc: 76.27%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 76.23%   [EVAL] batch:  310 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:  311 | acc: 56.25%,  total acc: 76.18%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 76.08%   
cur_acc:  ['0.9504', '0.8750', '0.6895', '0.7252', '0.8323']
his_acc:  ['0.9504', '0.8985', '0.8029', '0.7625', '0.7608']
CurrentTrain: epoch  0, batch     0 | loss: 6.0401235CurrentTrain: epoch  0, batch     1 | loss: 7.7011347CurrentTrain: epoch  0, batch     2 | loss: 6.0896378CurrentTrain: epoch  0, batch     3 | loss: 4.0933561CurrentTrain: epoch  1, batch     0 | loss: 5.8208780CurrentTrain: epoch  1, batch     1 | loss: 5.1322451CurrentTrain: epoch  1, batch     2 | loss: 5.1541395CurrentTrain: epoch  1, batch     3 | loss: 4.7966585CurrentTrain: epoch  2, batch     0 | loss: 5.3012819CurrentTrain: epoch  2, batch     1 | loss: 4.0986366CurrentTrain: epoch  2, batch     2 | loss: 4.5653696CurrentTrain: epoch  2, batch     3 | loss: 2.8558602CurrentTrain: epoch  3, batch     0 | loss: 4.7369308CurrentTrain: epoch  3, batch     1 | loss: 3.1627145CurrentTrain: epoch  3, batch     2 | loss: 4.4972086CurrentTrain: epoch  3, batch     3 | loss: 3.8344646CurrentTrain: epoch  4, batch     0 | loss: 3.4996650CurrentTrain: epoch  4, batch     1 | loss: 4.8093171CurrentTrain: epoch  4, batch     2 | loss: 3.6098022CurrentTrain: epoch  4, batch     3 | loss: 3.1919429CurrentTrain: epoch  5, batch     0 | loss: 3.7335074CurrentTrain: epoch  5, batch     1 | loss: 3.4988112CurrentTrain: epoch  5, batch     2 | loss: 3.4722469CurrentTrain: epoch  5, batch     3 | loss: 4.7877746CurrentTrain: epoch  6, batch     0 | loss: 4.4054074CurrentTrain: epoch  6, batch     1 | loss: 3.0474911CurrentTrain: epoch  6, batch     2 | loss: 3.0056169CurrentTrain: epoch  6, batch     3 | loss: 2.7226102CurrentTrain: epoch  7, batch     0 | loss: 3.1651287CurrentTrain: epoch  7, batch     1 | loss: 3.1238325CurrentTrain: epoch  7, batch     2 | loss: 2.6899600CurrentTrain: epoch  7, batch     3 | loss: 2.1584392CurrentTrain: epoch  8, batch     0 | loss: 2.8099370CurrentTrain: epoch  8, batch     1 | loss: 2.9878836CurrentTrain: epoch  8, batch     2 | loss: 3.0404248CurrentTrain: epoch  8, batch     3 | loss: 2.0123961CurrentTrain: epoch  9, batch     0 | loss: 2.7694013CurrentTrain: epoch  9, batch     1 | loss: 2.6787231CurrentTrain: epoch  9, batch     2 | loss: 2.4747005CurrentTrain: epoch  9, batch     3 | loss: 2.0552309
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: She has been a professional player in tennis since she was a teenager and has won several championships.  
Head Entity: She  
Tail Entity: tennis  

Relation: sport  
Context: The city is known for its vibrant soccer culture, with numerous teams competing in various leagues.  
Head Entity: The city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: alphonse louis pierre pyrame de candolle ( 28 october 18064 april 1893 ) was a french - swiss botanist , the son of the swiss botanist augustin pyramus de candolle .
Head Entity: alphonse louis pierre pyrame de candolle
Tail Entity: augustin pyramus de candolle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: charles darwin ( 12 february 1809 – 19 april 1882 ) was an english naturalist and geologist, best known for his contributions to the science of evolution. he was the son of robert darwin, a wealthy society doctor.  
Head Entity: charles darwin  
Tail Entity: robert darwin  

Relation: father  
Context: barack obama ( born august 4, 1961 ) is an american politician and attorney who served as the 44th president of the united states from 2009 to 2017. he is the son of barack obama sr., a kenyan economist.  
Head Entity: barack obama  
Tail Entity: barack obama sr.  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon when it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" received critical acclaim and was originally produced in Korean, showcasing the talents of its director and cast.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Avatar: The Last Airbender" was created in English and has gained a massive following worldwide.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: after years on loan with various lower division turkish teams , cangöz made his professional debut for antalyaspor in a 4 - 1 süper lig victory over gaziantepspor on 2 june 2017 .
Head Entity: antalyaspor
Tail Entity: süper lig
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: in 2020, the talented midfielder signed with the club after impressing in the youth ranks, and he quickly became a key player in the premier league, helping his team secure a top-four finish.  
Head Entity: premier league  
Tail Entity: club  

Relation: league  
Context: during his time at the club, he contributed significantly to their success in the championship, leading them to promotion to the top tier of english football.  
Head Entity: championship  
Tail Entity: club  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: other teammates included dominique leray and élisabeth riffiod , whose son boris diaw currently plays in the nba for the spurs ( 2014 ) .
Head Entity: boris diaw
Tail Entity: élisabeth riffiod
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in a recent interview, actress jennifer garner spoke fondly of her children, including violet affleck, whom she describes as a wonderful daughter.  
Head Entity: violet affleck  
Tail Entity: jennifer garner  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much emily loved spending time with her mother, sarah connor, who always supported her dreams.  
Head Entity: emily  
Tail Entity: sarah connor  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres, showcasing his talent on the cello.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Mixup data size:  438
MixupTrain:  epoch  0, batch     0 | loss: 2.3285736MixupTrain:  epoch  0, batch     1 | loss: 2.6050928MixupTrain:  epoch  0, batch     2 | loss: 2.1829561MixupTrain:  epoch  0, batch     3 | loss: 1.9533463MixupTrain:  epoch  0, batch     4 | loss: 2.0834151MixupTrain:  epoch  0, batch     5 | loss: 2.2038244MixupTrain:  epoch  0, batch     6 | loss: 2.2768633MixupTrain:  epoch  0, batch     7 | loss: 2.2666981MixupTrain:  epoch  0, batch     8 | loss: 1.9305551MixupTrain:  epoch  0, batch     9 | loss: 2.1531992MixupTrain:  epoch  0, batch    10 | loss: 2.7461247MixupTrain:  epoch  0, batch    11 | loss: 1.9786578MixupTrain:  epoch  0, batch    12 | loss: 1.7228975MixupTrain:  epoch  0, batch    13 | loss: 1.9485143MixupTrain:  epoch  0, batch    14 | loss: 2.5730418MixupTrain:  epoch  0, batch    15 | loss: 2.5997901MixupTrain:  epoch  0, batch    16 | loss: 2.2123113MixupTrain:  epoch  0, batch    17 | loss: 1.9929609MixupTrain:  epoch  0, batch    18 | loss: 2.0819687MixupTrain:  epoch  0, batch    19 | loss: 2.4814615MixupTrain:  epoch  0, batch    20 | loss: 1.6947823MixupTrain:  epoch  0, batch    21 | loss: 2.0200066MixupTrain:  epoch  0, batch    22 | loss: 2.0125306MixupTrain:  epoch  0, batch    23 | loss: 2.6288479MixupTrain:  epoch  0, batch    24 | loss: 1.8301145MixupTrain:  epoch  0, batch    25 | loss: 2.2352153MixupTrain:  epoch  0, batch    26 | loss: 1.7952603MixupTrain:  epoch  0, batch    27 | loss: 1.6377020
MemoryTrain:  epoch  0, batch     0 | loss: 2.0535479MemoryTrain:  epoch  0, batch     1 | loss: 2.1117482MemoryTrain:  epoch  0, batch     2 | loss: 1.7925003MemoryTrain:  epoch  0, batch     3 | loss: 2.5890794MemoryTrain:  epoch  0, batch     4 | loss: 2.0113025MemoryTrain:  epoch  0, batch     5 | loss: 2.0341909MemoryTrain:  epoch  0, batch     6 | loss: 1.9471880MemoryTrain:  epoch  0, batch     7 | loss: 1.6780258MemoryTrain:  epoch  0, batch     8 | loss: 2.3572302MemoryTrain:  epoch  0, batch     9 | loss: 2.9060612MemoryTrain:  epoch  0, batch    10 | loss: 2.3763208MemoryTrain:  epoch  0, batch    11 | loss: 1.7704146MemoryTrain:  epoch  1, batch     0 | loss: 1.6498326MemoryTrain:  epoch  1, batch     1 | loss: 1.7512748MemoryTrain:  epoch  1, batch     2 | loss: 2.2295556MemoryTrain:  epoch  1, batch     3 | loss: 1.3975402MemoryTrain:  epoch  1, batch     4 | loss: 1.7598603MemoryTrain:  epoch  1, batch     5 | loss: 2.0944443MemoryTrain:  epoch  1, batch     6 | loss: 1.8783251MemoryTrain:  epoch  1, batch     7 | loss: 1.9683083MemoryTrain:  epoch  1, batch     8 | loss: 2.0062737MemoryTrain:  epoch  1, batch     9 | loss: 1.7114400MemoryTrain:  epoch  1, batch    10 | loss: 1.9753850MemoryTrain:  epoch  1, batch    11 | loss: 1.4315608MemoryTrain:  epoch  2, batch     0 | loss: 1.3652985MemoryTrain:  epoch  2, batch     1 | loss: 1.8816133MemoryTrain:  epoch  2, batch     2 | loss: 1.4196429MemoryTrain:  epoch  2, batch     3 | loss: 1.7329894MemoryTrain:  epoch  2, batch     4 | loss: 1.6343327MemoryTrain:  epoch  2, batch     5 | loss: 1.5860045MemoryTrain:  epoch  2, batch     6 | loss: 1.3559678MemoryTrain:  epoch  2, batch     7 | loss: 1.7725890MemoryTrain:  epoch  2, batch     8 | loss: 1.3825196MemoryTrain:  epoch  2, batch     9 | loss: 1.5295527MemoryTrain:  epoch  2, batch    10 | loss: 1.7589617MemoryTrain:  epoch  2, batch    11 | loss: 1.3362793MemoryTrain:  epoch  3, batch     0 | loss: 1.6287134MemoryTrain:  epoch  3, batch     1 | loss: 1.3340459MemoryTrain:  epoch  3, batch     2 | loss: 1.6034673MemoryTrain:  epoch  3, batch     3 | loss: 1.3044895MemoryTrain:  epoch  3, batch     4 | loss: 1.4342902MemoryTrain:  epoch  3, batch     5 | loss: 1.5361593MemoryTrain:  epoch  3, batch     6 | loss: 1.5759397MemoryTrain:  epoch  3, batch     7 | loss: 1.3625576MemoryTrain:  epoch  3, batch     8 | loss: 1.4230938MemoryTrain:  epoch  3, batch     9 | loss: 1.4521202MemoryTrain:  epoch  3, batch    10 | loss: 1.3494432MemoryTrain:  epoch  3, batch    11 | loss: 1.5045507MemoryTrain:  epoch  4, batch     0 | loss: 1.5924363MemoryTrain:  epoch  4, batch     1 | loss: 1.2538621MemoryTrain:  epoch  4, batch     2 | loss: 1.4998128MemoryTrain:  epoch  4, batch     3 | loss: 1.2433587MemoryTrain:  epoch  4, batch     4 | loss: 1.5823572MemoryTrain:  epoch  4, batch     5 | loss: 1.4187368MemoryTrain:  epoch  4, batch     6 | loss: 1.2740288MemoryTrain:  epoch  4, batch     7 | loss: 1.3986468MemoryTrain:  epoch  4, batch     8 | loss: 1.5977985MemoryTrain:  epoch  4, batch     9 | loss: 1.5520819MemoryTrain:  epoch  4, batch    10 | loss: 1.2533839MemoryTrain:  epoch  4, batch    11 | loss: 1.2818112MemoryTrain:  epoch  5, batch     0 | loss: 1.3019996MemoryTrain:  epoch  5, batch     1 | loss: 1.7832596MemoryTrain:  epoch  5, batch     2 | loss: 1.4065342MemoryTrain:  epoch  5, batch     3 | loss: 1.2651284MemoryTrain:  epoch  5, batch     4 | loss: 1.2944344MemoryTrain:  epoch  5, batch     5 | loss: 1.3521290MemoryTrain:  epoch  5, batch     6 | loss: 1.3822542MemoryTrain:  epoch  5, batch     7 | loss: 1.3068576MemoryTrain:  epoch  5, batch     8 | loss: 1.2411797MemoryTrain:  epoch  5, batch     9 | loss: 1.3098025MemoryTrain:  epoch  5, batch    10 | loss: 1.2959681MemoryTrain:  epoch  5, batch    11 | loss: 1.1726325MemoryTrain:  epoch  6, batch     0 | loss: 1.2353967MemoryTrain:  epoch  6, batch     1 | loss: 1.3362234MemoryTrain:  epoch  6, batch     2 | loss: 1.3318040MemoryTrain:  epoch  6, batch     3 | loss: 1.4160188MemoryTrain:  epoch  6, batch     4 | loss: 1.3594403MemoryTrain:  epoch  6, batch     5 | loss: 1.3032719MemoryTrain:  epoch  6, batch     6 | loss: 1.3414398MemoryTrain:  epoch  6, batch     7 | loss: 1.4118477MemoryTrain:  epoch  6, batch     8 | loss: 1.3235272MemoryTrain:  epoch  6, batch     9 | loss: 1.2572098MemoryTrain:  epoch  6, batch    10 | loss: 1.2218195MemoryTrain:  epoch  6, batch    11 | loss: 1.1650224MemoryTrain:  epoch  7, batch     0 | loss: 1.3206544MemoryTrain:  epoch  7, batch     1 | loss: 1.2923830MemoryTrain:  epoch  7, batch     2 | loss: 1.2552817MemoryTrain:  epoch  7, batch     3 | loss: 1.3629321MemoryTrain:  epoch  7, batch     4 | loss: 1.2426459MemoryTrain:  epoch  7, batch     5 | loss: 1.1998990MemoryTrain:  epoch  7, batch     6 | loss: 1.3068585MemoryTrain:  epoch  7, batch     7 | loss: 1.2794018MemoryTrain:  epoch  7, batch     8 | loss: 1.3191082MemoryTrain:  epoch  7, batch     9 | loss: 1.3207033MemoryTrain:  epoch  7, batch    10 | loss: 1.2381711MemoryTrain:  epoch  7, batch    11 | loss: 1.2842150MemoryTrain:  epoch  8, batch     0 | loss: 1.3943007MemoryTrain:  epoch  8, batch     1 | loss: 1.3162444MemoryTrain:  epoch  8, batch     2 | loss: 1.2965184MemoryTrain:  epoch  8, batch     3 | loss: 1.2946974MemoryTrain:  epoch  8, batch     4 | loss: 1.2234885MemoryTrain:  epoch  8, batch     5 | loss: 1.2949998MemoryTrain:  epoch  8, batch     6 | loss: 1.1932263MemoryTrain:  epoch  8, batch     7 | loss: 1.2900966MemoryTrain:  epoch  8, batch     8 | loss: 1.2583064MemoryTrain:  epoch  8, batch     9 | loss: 1.2306141MemoryTrain:  epoch  8, batch    10 | loss: 1.2923460MemoryTrain:  epoch  8, batch    11 | loss: 1.2335865MemoryTrain:  epoch  9, batch     0 | loss: 1.2237926MemoryTrain:  epoch  9, batch     1 | loss: 1.2715030MemoryTrain:  epoch  9, batch     2 | loss: 1.2844958MemoryTrain:  epoch  9, batch     3 | loss: 1.2728689MemoryTrain:  epoch  9, batch     4 | loss: 1.2491879MemoryTrain:  epoch  9, batch     5 | loss: 1.2865641MemoryTrain:  epoch  9, batch     6 | loss: 1.2987175MemoryTrain:  epoch  9, batch     7 | loss: 1.2295269MemoryTrain:  epoch  9, batch     8 | loss: 1.2588620MemoryTrain:  epoch  9, batch     9 | loss: 1.2972097MemoryTrain:  epoch  9, batch    10 | loss: 1.2638695MemoryTrain:  epoch  9, batch    11 | loss: 1.2503741
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 34.38%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 44.53%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 48.61%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 52.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 58.65%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 59.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 59.56%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 61.46%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 63.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 67.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.17%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 76.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 77.39%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 77.86%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 78.30%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 78.89%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 78.78%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 78.59%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 78.20%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 77.83%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 77.18%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 77.27%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.59%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 79.21%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 79.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 79.53%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 79.21%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 79.01%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 78.82%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 78.98%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 78.35%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 77.96%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 78.02%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 77.86%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 77.71%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 77.66%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 77.38%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 63.07%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 73.81%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 73.58%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.80%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.49%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.43%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 82.43%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 84.45%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 84.23%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 83.89%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 83.15%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 82.31%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 81.77%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 81.51%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 80.38%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 80.27%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 80.17%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 80.19%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 79.75%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 79.43%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 79.24%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 78.95%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 78.45%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 78.28%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 78.44%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 78.48%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 78.33%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 78.37%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 78.52%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 78.56%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 78.69%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 78.73%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 79.08%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 78.39%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 77.64%   [EVAL] batch:   71 | acc: 25.00%,  total acc: 76.91%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 76.03%   [EVAL] batch:   73 | acc: 6.25%,  total acc: 75.08%   [EVAL] batch:   74 | acc: 12.50%,  total acc: 74.25%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 74.26%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 74.27%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 74.28%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 74.05%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 73.98%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 74.00%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 73.55%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 72.74%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 71.95%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 71.25%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 70.71%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 70.04%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 69.96%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 70.88%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 71.74%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 71.71%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 71.81%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 71.71%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 71.62%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 71.72%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 71.44%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 71.72%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 72.80%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 73.00%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 73.50%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 73.74%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 74.21%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 74.45%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 74.34%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 74.46%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 74.52%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 74.41%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 74.47%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 74.53%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 74.85%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 75.05%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 75.15%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 74.75%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 74.41%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 74.08%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 73.89%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 73.62%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 73.53%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 73.68%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 74.03%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 74.18%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 74.23%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 74.19%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 74.11%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 74.16%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 74.16%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 74.13%   [EVAL] batch:  143 | acc: 43.75%,  total acc: 73.91%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 73.75%   [EVAL] batch:  145 | acc: 37.50%,  total acc: 73.50%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 73.51%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 73.28%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 73.21%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 72.76%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 72.41%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 72.02%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 71.59%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 71.25%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 70.79%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 70.70%   [EVAL] batch:  157 | acc: 87.50%,  total acc: 70.81%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 70.87%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 71.08%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 71.14%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 71.24%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 71.34%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 71.52%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 71.65%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 71.71%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:  169 | acc: 62.50%,  total acc: 71.99%   [EVAL] batch:  170 | acc: 50.00%,  total acc: 71.86%   [EVAL] batch:  171 | acc: 50.00%,  total acc: 71.73%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 71.60%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 71.66%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 71.50%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 71.16%   [EVAL] batch:  176 | acc: 25.00%,  total acc: 70.90%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 70.68%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 70.53%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 70.38%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 70.30%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 70.12%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 70.01%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 69.84%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 69.83%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 69.72%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 69.82%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 69.61%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 69.54%   [EVAL] batch:  189 | acc: 56.25%,  total acc: 69.47%   [EVAL] batch:  190 | acc: 56.25%,  total acc: 69.40%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 69.47%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 69.33%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 69.23%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 69.20%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 69.13%   [EVAL] batch:  196 | acc: 50.00%,  total acc: 69.04%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 69.10%   [EVAL] batch:  198 | acc: 18.75%,  total acc: 68.84%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 68.97%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 68.91%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 68.87%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 68.84%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 68.78%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 68.81%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 68.90%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 69.46%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:  212 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  213 | acc: 81.25%,  total acc: 69.80%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 69.83%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 69.85%   [EVAL] batch:  216 | acc: 75.00%,  total acc: 69.87%   [EVAL] batch:  217 | acc: 87.50%,  total acc: 69.95%   [EVAL] batch:  218 | acc: 62.50%,  total acc: 69.92%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 69.83%   [EVAL] batch:  220 | acc: 12.50%,  total acc: 69.57%   [EVAL] batch:  221 | acc: 6.25%,  total acc: 69.28%   [EVAL] batch:  222 | acc: 12.50%,  total acc: 69.03%   [EVAL] batch:  223 | acc: 0.00%,  total acc: 68.72%   [EVAL] batch:  224 | acc: 6.25%,  total acc: 68.44%   [EVAL] batch:  225 | acc: 50.00%,  total acc: 68.36%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 68.36%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 68.34%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 68.34%   [EVAL] batch:  229 | acc: 62.50%,  total acc: 68.32%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 68.26%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 68.27%   [EVAL] batch:  232 | acc: 56.25%,  total acc: 68.21%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 68.22%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 68.19%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 68.17%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 68.25%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 68.30%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 68.67%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 68.80%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 69.16%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 69.21%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 69.31%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 69.35%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:  250 | acc: 81.25%,  total acc: 69.65%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 69.67%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 69.74%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 69.69%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 69.68%   [EVAL] batch:  255 | acc: 93.75%,  total acc: 69.78%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 69.84%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 69.89%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 69.93%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 69.95%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 69.97%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 69.97%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 70.15%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 70.17%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 70.21%   [EVAL] batch:  266 | acc: 100.00%,  total acc: 70.32%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 70.47%   [EVAL] batch:  269 | acc: 75.00%,  total acc: 70.49%   [EVAL] batch:  270 | acc: 75.00%,  total acc: 70.50%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 70.50%   [EVAL] batch:  272 | acc: 75.00%,  total acc: 70.51%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 70.53%   [EVAL] batch:  274 | acc: 75.00%,  total acc: 70.55%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.76%   [EVAL] batch:  277 | acc: 93.75%,  total acc: 70.84%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 71.05%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 71.46%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 71.85%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 71.91%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 71.96%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 71.97%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 72.02%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 72.10%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 72.07%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 71.93%   [EVAL] batch:  295 | acc: 25.00%,  total acc: 71.77%   [EVAL] batch:  296 | acc: 37.50%,  total acc: 71.65%   [EVAL] batch:  297 | acc: 62.50%,  total acc: 71.62%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 71.51%   [EVAL] batch:  299 | acc: 50.00%,  total acc: 71.44%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 71.66%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 71.69%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 71.76%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 71.81%   [EVAL] batch:  306 | acc: 37.50%,  total acc: 71.70%   [EVAL] batch:  307 | acc: 6.25%,  total acc: 71.49%   [EVAL] batch:  308 | acc: 6.25%,  total acc: 71.28%   [EVAL] batch:  309 | acc: 18.75%,  total acc: 71.11%   [EVAL] batch:  310 | acc: 6.25%,  total acc: 70.90%   [EVAL] batch:  311 | acc: 0.00%,  total acc: 70.67%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 70.53%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 70.44%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 70.32%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 70.17%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 70.05%   [EVAL] batch:  317 | acc: 50.00%,  total acc: 69.99%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 69.87%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 69.92%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 69.96%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 69.99%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 70.07%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 70.10%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 70.12%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 70.11%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 70.09%   [EVAL] batch:  327 | acc: 43.75%,  total acc: 70.01%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 70.02%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 70.04%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 70.03%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 70.11%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 70.18%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 70.32%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 70.39%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 70.80%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 70.97%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 71.21%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 71.27%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 71.34%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 71.42%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 71.46%   [EVAL] batch:  351 | acc: 81.25%,  total acc: 71.48%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 71.46%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 71.47%   [EVAL] batch:  354 | acc: 50.00%,  total acc: 71.41%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 71.38%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 71.45%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 71.51%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 71.73%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 71.77%   [EVAL] batch:  362 | acc: 87.50%,  total acc: 71.81%   [EVAL] batch:  363 | acc: 75.00%,  total acc: 71.82%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 71.82%   [EVAL] batch:  365 | acc: 68.75%,  total acc: 71.81%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 71.82%   [EVAL] batch:  367 | acc: 68.75%,  total acc: 71.81%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 71.73%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 71.76%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 71.77%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 71.71%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 71.73%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 71.77%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 71.82%   
cur_acc:  ['0.9504', '0.8750', '0.6895', '0.7252', '0.8323', '0.7738']
his_acc:  ['0.9504', '0.8985', '0.8029', '0.7625', '0.7608', '0.7182']
CurrentTrain: epoch  0, batch     0 | loss: 5.3644543CurrentTrain: epoch  0, batch     1 | loss: 6.1042714CurrentTrain: epoch  0, batch     2 | loss: 5.3767052CurrentTrain: epoch  0, batch     3 | loss: 9.2844954CurrentTrain: epoch  1, batch     0 | loss: 4.6174674CurrentTrain: epoch  1, batch     1 | loss: 4.7035980CurrentTrain: epoch  1, batch     2 | loss: 4.0857625CurrentTrain: epoch  1, batch     3 | loss: 5.5698166CurrentTrain: epoch  2, batch     0 | loss: 4.2929335CurrentTrain: epoch  2, batch     1 | loss: 3.5867841CurrentTrain: epoch  2, batch     2 | loss: 3.3635337CurrentTrain: epoch  2, batch     3 | loss: 3.8276987CurrentTrain: epoch  3, batch     0 | loss: 4.0777082CurrentTrain: epoch  3, batch     1 | loss: 3.3661489CurrentTrain: epoch  3, batch     2 | loss: 3.4570696CurrentTrain: epoch  3, batch     3 | loss: 3.6983576CurrentTrain: epoch  4, batch     0 | loss: 3.0371032CurrentTrain: epoch  4, batch     1 | loss: 3.4768889CurrentTrain: epoch  4, batch     2 | loss: 3.4259875CurrentTrain: epoch  4, batch     3 | loss: 2.1180074CurrentTrain: epoch  5, batch     0 | loss: 2.9832540CurrentTrain: epoch  5, batch     1 | loss: 3.1003048CurrentTrain: epoch  5, batch     2 | loss: 3.0402586CurrentTrain: epoch  5, batch     3 | loss: 2.1994216CurrentTrain: epoch  6, batch     0 | loss: 2.6066017CurrentTrain: epoch  6, batch     1 | loss: 2.8939867CurrentTrain: epoch  6, batch     2 | loss: 2.6514387CurrentTrain: epoch  6, batch     3 | loss: 3.3618298CurrentTrain: epoch  7, batch     0 | loss: 2.6758585CurrentTrain: epoch  7, batch     1 | loss: 2.4866979CurrentTrain: epoch  7, batch     2 | loss: 2.4201870CurrentTrain: epoch  7, batch     3 | loss: 2.1114769CurrentTrain: epoch  8, batch     0 | loss: 2.0926619CurrentTrain: epoch  8, batch     1 | loss: 2.4953814CurrentTrain: epoch  8, batch     2 | loss: 2.2446404CurrentTrain: epoch  8, batch     3 | loss: 2.0968032CurrentTrain: epoch  9, batch     0 | loss: 2.1773472CurrentTrain: epoch  9, batch     1 | loss: 2.1597176CurrentTrain: epoch  9, batch     2 | loss: 2.2317116CurrentTrain: epoch  9, batch     3 | loss: 1.8231630
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elena is a renowned artist who often collaborates with her husband, mark, a talented musician known for his soulful melodies.  
Head Entity: mark  
Tail Entity: elena  

Relation: spouse  
Context: during the award ceremony, it was revealed that the famous director, james cameron, has been married to suzy, a successful producer, for over two decades.  
Head Entity: james cameron  
Tail Entity: suzy  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: " i 'm a cuckoo " was belle & sebastian 's second single from " dear catastrophe waitress " , released on rough trade records in 2004 .
Head Entity: belle & sebastian
Tail Entity: rough trade records
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Random Access Memories" by Daft Punk was released under the Columbia Records label in 2013.  
Head Entity: Daft Punk  
Tail Entity: Columbia Records  

Relation: record label  
Context: Taylor Swift's latest album "Evermore" was produced by her own label, Republic Records, and released in 2020.  
Head Entity: Taylor Swift  
Tail Entity: Republic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: it then crossed the indian ocean , passing st. pierre island , providence atoll and farquhar atoll of seychelles before making landfall in southeastern australia .
Head Entity: farquhar atoll
Tail Entity: indian ocean
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Venice is famous for its canals, which are situated right next to the Adriatic Sea, making it a unique destination for tourists.  
Head Entity: Venice  
Tail Entity: Adriatic Sea  

Relation: located in or next to body of water  
Context: The village of Hvar is nestled on the shores of the Aegean Sea, attracting visitors with its stunning views and vibrant culture.  
Head Entity: Hvar  
Tail Entity: Aegean Sea  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: however , four pairs , including meiliana jauhari and greysia polii of indonesia , and ha jung eun and kim min jung of korea , were disqualified from the competition .
Head Entity: ha jung eun
Tail Entity: korea
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: After years of living abroad, Maria decided to apply for dual citizenship, allowing her to maintain her ties to both her home country and her new life in Canada.  
Head Entity: Maria  
Tail Entity: Canada  

Relation: country of citizenship  
Context: The athlete proudly represented her nation at the Olympics, showcasing her skills and dedication to her country of citizenship, which is Brazil.  
Head Entity: The athlete  
Tail Entity: Brazil  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: he is also a part of the atlanta - based hip - hop / r&b;/soul musical collective the dungeon family .
Head Entity: the dungeon family
Tail Entity: hip - hop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the band is known for their unique blend of rock and electronic music, often categorized under the genre of synth-pop.  
Head Entity: the band  
Tail Entity: synth-pop  

Relation: genre  
Context: she has made significant contributions to the world of classical music, particularly in the genre of opera.  
Head Entity: she  
Tail Entity: opera  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: in the west , the rivers rib , ash and stort flow south from the hundred parishes to meet the lea and then the thames .
Head Entity: rib
Tail Entity: lea
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: The river Seine flows through Paris and eventually empties into the English Channel, making it a vital watercourse in the region.  
Head Entity: Seine  
Tail Entity: English Channel  

Relation: mouth of the watercourse  
Context: The Mississippi River, one of the longest rivers in North America, has its mouth at the Gulf of Mexico, where it contributes to the rich ecosystem of the area.  
Head Entity: Mississippi River  
Tail Entity: Gulf of Mexico  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and club san diego wave fc in the nwsl.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
Mixup data size:  499
MixupTrain:  epoch  0, batch     0 | loss: 2.1899348MixupTrain:  epoch  0, batch     1 | loss: 1.9142590MixupTrain:  epoch  0, batch     2 | loss: 1.7911010MixupTrain:  epoch  0, batch     3 | loss: 1.9032674MixupTrain:  epoch  0, batch     4 | loss: 1.8695815MixupTrain:  epoch  0, batch     5 | loss: 2.0120454MixupTrain:  epoch  0, batch     6 | loss: 2.4046373MixupTrain:  epoch  0, batch     7 | loss: 2.0028247MixupTrain:  epoch  0, batch     8 | loss: 2.1108442MixupTrain:  epoch  0, batch     9 | loss: 1.8039167MixupTrain:  epoch  0, batch    10 | loss: 2.2893946MixupTrain:  epoch  0, batch    11 | loss: 2.2481157MixupTrain:  epoch  0, batch    12 | loss: 2.0117058MixupTrain:  epoch  0, batch    13 | loss: 1.8026248MixupTrain:  epoch  0, batch    14 | loss: 1.9163996MixupTrain:  epoch  0, batch    15 | loss: 1.9793625MixupTrain:  epoch  0, batch    16 | loss: 2.0166869MixupTrain:  epoch  0, batch    17 | loss: 1.9591629MixupTrain:  epoch  0, batch    18 | loss: 2.1339340MixupTrain:  epoch  0, batch    19 | loss: 1.7361025MixupTrain:  epoch  0, batch    20 | loss: 1.9881569MixupTrain:  epoch  0, batch    21 | loss: 1.7436154MixupTrain:  epoch  0, batch    22 | loss: 1.8635359MixupTrain:  epoch  0, batch    23 | loss: 1.8280985MixupTrain:  epoch  0, batch    24 | loss: 2.3075688MixupTrain:  epoch  0, batch    25 | loss: 1.6328883MixupTrain:  epoch  0, batch    26 | loss: 1.7438116MixupTrain:  epoch  0, batch    27 | loss: 1.5184734MixupTrain:  epoch  0, batch    28 | loss: 1.8801825MixupTrain:  epoch  0, batch    29 | loss: 2.0115301MixupTrain:  epoch  0, batch    30 | loss: 2.1975210MixupTrain:  epoch  0, batch    31 | loss: 1.5289520
MemoryTrain:  epoch  0, batch     0 | loss: 1.5759532MemoryTrain:  epoch  0, batch     1 | loss: 1.3441105MemoryTrain:  epoch  0, batch     2 | loss: 2.1309531MemoryTrain:  epoch  0, batch     3 | loss: 2.5122893MemoryTrain:  epoch  0, batch     4 | loss: 2.3320267MemoryTrain:  epoch  0, batch     5 | loss: 1.3775146MemoryTrain:  epoch  0, batch     6 | loss: 2.2405505MemoryTrain:  epoch  0, batch     7 | loss: 1.5751867MemoryTrain:  epoch  0, batch     8 | loss: 1.9597692MemoryTrain:  epoch  0, batch     9 | loss: 2.7306948MemoryTrain:  epoch  0, batch    10 | loss: 2.0835483MemoryTrain:  epoch  0, batch    11 | loss: 2.1553669MemoryTrain:  epoch  0, batch    12 | loss: 2.8269484MemoryTrain:  epoch  0, batch    13 | loss: 1.7185466MemoryTrain:  epoch  1, batch     0 | loss: 1.8448055MemoryTrain:  epoch  1, batch     1 | loss: 2.4040751MemoryTrain:  epoch  1, batch     2 | loss: 1.6924201MemoryTrain:  epoch  1, batch     3 | loss: 1.8790272MemoryTrain:  epoch  1, batch     4 | loss: 1.5335245MemoryTrain:  epoch  1, batch     5 | loss: 1.6754334MemoryTrain:  epoch  1, batch     6 | loss: 2.0598743MemoryTrain:  epoch  1, batch     7 | loss: 1.3097520MemoryTrain:  epoch  1, batch     8 | loss: 1.6450305MemoryTrain:  epoch  1, batch     9 | loss: 1.7490900MemoryTrain:  epoch  1, batch    10 | loss: 1.6487489MemoryTrain:  epoch  1, batch    11 | loss: 2.0670729MemoryTrain:  epoch  1, batch    12 | loss: 1.8863368MemoryTrain:  epoch  1, batch    13 | loss: 1.1911952MemoryTrain:  epoch  2, batch     0 | loss: 1.4979841MemoryTrain:  epoch  2, batch     1 | loss: 2.0289102MemoryTrain:  epoch  2, batch     2 | loss: 1.3257599MemoryTrain:  epoch  2, batch     3 | loss: 2.1706951MemoryTrain:  epoch  2, batch     4 | loss: 1.6780059MemoryTrain:  epoch  2, batch     5 | loss: 1.6916105MemoryTrain:  epoch  2, batch     6 | loss: 1.6245539MemoryTrain:  epoch  2, batch     7 | loss: 1.3306049MemoryTrain:  epoch  2, batch     8 | loss: 1.9694045MemoryTrain:  epoch  2, batch     9 | loss: 1.4380798MemoryTrain:  epoch  2, batch    10 | loss: 2.0449882MemoryTrain:  epoch  2, batch    11 | loss: 1.6545116MemoryTrain:  epoch  2, batch    12 | loss: 1.3203397MemoryTrain:  epoch  2, batch    13 | loss: 1.3340085MemoryTrain:  epoch  3, batch     0 | loss: 1.3159688MemoryTrain:  epoch  3, batch     1 | loss: 1.4466777MemoryTrain:  epoch  3, batch     2 | loss: 1.6125228MemoryTrain:  epoch  3, batch     3 | loss: 1.3281728MemoryTrain:  epoch  3, batch     4 | loss: 1.5068688MemoryTrain:  epoch  3, batch     5 | loss: 2.1117949MemoryTrain:  epoch  3, batch     6 | loss: 1.5108898MemoryTrain:  epoch  3, batch     7 | loss: 1.7319582MemoryTrain:  epoch  3, batch     8 | loss: 1.2419105MemoryTrain:  epoch  3, batch     9 | loss: 1.4897295MemoryTrain:  epoch  3, batch    10 | loss: 1.2874690MemoryTrain:  epoch  3, batch    11 | loss: 1.9255096MemoryTrain:  epoch  3, batch    12 | loss: 1.4461329MemoryTrain:  epoch  3, batch    13 | loss: 1.3824713MemoryTrain:  epoch  4, batch     0 | loss: 1.8038136MemoryTrain:  epoch  4, batch     1 | loss: 1.7105571MemoryTrain:  epoch  4, batch     2 | loss: 1.2470374MemoryTrain:  epoch  4, batch     3 | loss: 1.6826035MemoryTrain:  epoch  4, batch     4 | loss: 1.4306954MemoryTrain:  epoch  4, batch     5 | loss: 1.3857884MemoryTrain:  epoch  4, batch     6 | loss: 1.6210365MemoryTrain:  epoch  4, batch     7 | loss: 1.2453398MemoryTrain:  epoch  4, batch     8 | loss: 1.3237069MemoryTrain:  epoch  4, batch     9 | loss: 1.6000516MemoryTrain:  epoch  4, batch    10 | loss: 1.4129096MemoryTrain:  epoch  4, batch    11 | loss: 1.2736459MemoryTrain:  epoch  4, batch    12 | loss: 1.3409743MemoryTrain:  epoch  4, batch    13 | loss: 1.2351087MemoryTrain:  epoch  5, batch     0 | loss: 1.2974954MemoryTrain:  epoch  5, batch     1 | loss: 1.5506327MemoryTrain:  epoch  5, batch     2 | loss: 1.3628383MemoryTrain:  epoch  5, batch     3 | loss: 1.3416270MemoryTrain:  epoch  5, batch     4 | loss: 1.6651396MemoryTrain:  epoch  5, batch     5 | loss: 1.2996842MemoryTrain:  epoch  5, batch     6 | loss: 1.4412506MemoryTrain:  epoch  5, batch     7 | loss: 1.3946166MemoryTrain:  epoch  5, batch     8 | loss: 1.2504454MemoryTrain:  epoch  5, batch     9 | loss: 1.2573922MemoryTrain:  epoch  5, batch    10 | loss: 1.4126747MemoryTrain:  epoch  5, batch    11 | loss: 1.2601488MemoryTrain:  epoch  5, batch    12 | loss: 1.3916967MemoryTrain:  epoch  5, batch    13 | loss: 1.1799614MemoryTrain:  epoch  6, batch     0 | loss: 1.2946846MemoryTrain:  epoch  6, batch     1 | loss: 1.6508904MemoryTrain:  epoch  6, batch     2 | loss: 1.3799756MemoryTrain:  epoch  6, batch     3 | loss: 1.2423544MemoryTrain:  epoch  6, batch     4 | loss: 1.2546840MemoryTrain:  epoch  6, batch     5 | loss: 1.3304036MemoryTrain:  epoch  6, batch     6 | loss: 1.3099962MemoryTrain:  epoch  6, batch     7 | loss: 1.5233172MemoryTrain:  epoch  6, batch     8 | loss: 1.4576670MemoryTrain:  epoch  6, batch     9 | loss: 1.2442619MemoryTrain:  epoch  6, batch    10 | loss: 1.4190377MemoryTrain:  epoch  6, batch    11 | loss: 1.2408007MemoryTrain:  epoch  6, batch    12 | loss: 1.2108911MemoryTrain:  epoch  6, batch    13 | loss: 1.5451168MemoryTrain:  epoch  7, batch     0 | loss: 1.4054747MemoryTrain:  epoch  7, batch     1 | loss: 1.2216773MemoryTrain:  epoch  7, batch     2 | loss: 1.2531015MemoryTrain:  epoch  7, batch     3 | loss: 1.2858361MemoryTrain:  epoch  7, batch     4 | loss: 1.3323722MemoryTrain:  epoch  7, batch     5 | loss: 1.4292784MemoryTrain:  epoch  7, batch     6 | loss: 1.2324913MemoryTrain:  epoch  7, batch     7 | loss: 1.4036751MemoryTrain:  epoch  7, batch     8 | loss: 1.3242768MemoryTrain:  epoch  7, batch     9 | loss: 1.2092741MemoryTrain:  epoch  7, batch    10 | loss: 1.3770529MemoryTrain:  epoch  7, batch    11 | loss: 1.3497634MemoryTrain:  epoch  7, batch    12 | loss: 1.3011428MemoryTrain:  epoch  7, batch    13 | loss: 1.6053665MemoryTrain:  epoch  8, batch     0 | loss: 1.2252249MemoryTrain:  epoch  8, batch     1 | loss: 1.3469579MemoryTrain:  epoch  8, batch     2 | loss: 1.2529504MemoryTrain:  epoch  8, batch     3 | loss: 1.4239004MemoryTrain:  epoch  8, batch     4 | loss: 1.2771045MemoryTrain:  epoch  8, batch     5 | loss: 1.2324494MemoryTrain:  epoch  8, batch     6 | loss: 1.2361823MemoryTrain:  epoch  8, batch     7 | loss: 1.2201445MemoryTrain:  epoch  8, batch     8 | loss: 1.3154616MemoryTrain:  epoch  8, batch     9 | loss: 1.2376817MemoryTrain:  epoch  8, batch    10 | loss: 1.3337474MemoryTrain:  epoch  8, batch    11 | loss: 1.4307355MemoryTrain:  epoch  8, batch    12 | loss: 1.3149147MemoryTrain:  epoch  8, batch    13 | loss: 1.2122078MemoryTrain:  epoch  9, batch     0 | loss: 1.3059576MemoryTrain:  epoch  9, batch     1 | loss: 1.2657923MemoryTrain:  epoch  9, batch     2 | loss: 1.2167540MemoryTrain:  epoch  9, batch     3 | loss: 1.2800261MemoryTrain:  epoch  9, batch     4 | loss: 1.2229202MemoryTrain:  epoch  9, batch     5 | loss: 1.2756252MemoryTrain:  epoch  9, batch     6 | loss: 1.3220681MemoryTrain:  epoch  9, batch     7 | loss: 1.2640343MemoryTrain:  epoch  9, batch     8 | loss: 1.2365866MemoryTrain:  epoch  9, batch     9 | loss: 1.2302029MemoryTrain:  epoch  9, batch    10 | loss: 1.3258542MemoryTrain:  epoch  9, batch    11 | loss: 1.3221982MemoryTrain:  epoch  9, batch    12 | loss: 1.2967083MemoryTrain:  epoch  9, batch    13 | loss: 1.1931937
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   25 | acc: 25.00%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 76.56%   [EVAL] batch:   28 | acc: 43.75%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 50.00%,  total acc: 74.58%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 72.98%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 72.46%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 71.78%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 71.14%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 70.89%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 70.66%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 70.44%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 70.72%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 70.99%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 71.41%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 71.80%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 72.38%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 72.59%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 71.81%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 71.20%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 70.35%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 70.05%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 69.26%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 68.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 69.24%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.99%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 72.52%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 73.36%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 73.59%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 73.02%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 75.28%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 75.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.55%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.35%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.44%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 81.07%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.42%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 81.59%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 81.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.93%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 83.18%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 83.28%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 83.24%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 82.92%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 82.47%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 81.65%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 81.12%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 80.87%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 80.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 80.02%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 80.05%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 80.31%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 79.86%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 79.43%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 79.24%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 79.06%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 78.66%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 78.50%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 78.54%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 78.59%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 78.43%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 78.47%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 78.61%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 78.65%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 78.88%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 78.92%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 79.20%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 79.05%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 78.25%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 77.62%   [EVAL] batch:   74 | acc: 31.25%,  total acc: 77.00%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 76.97%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 76.95%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 77.00%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 76.82%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 76.88%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 76.85%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 76.52%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 75.68%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 74.85%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 74.12%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 73.55%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 72.84%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 72.73%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 73.63%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.91%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 74.19%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 74.28%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 74.35%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 74.23%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 74.11%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 74.18%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 73.88%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 74.13%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 74.39%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 75.12%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 75.29%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 75.53%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 75.75%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 75.97%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 76.62%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 76.60%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 76.43%   [EVAL] batch:  114 | acc: 87.50%,  total acc: 76.52%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 76.44%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 76.48%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 76.52%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 76.81%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 77.00%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 77.18%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 77.32%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 77.45%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 76.88%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 76.33%   [EVAL] batch:  127 | acc: 18.75%,  total acc: 75.88%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 75.29%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 74.86%   [EVAL] batch:  130 | acc: 18.75%,  total acc: 74.43%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 74.34%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 74.39%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 74.72%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 74.86%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 74.86%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 74.91%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 74.87%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 74.78%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 74.82%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 74.82%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 74.78%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 74.52%   [EVAL] batch:  144 | acc: 25.00%,  total acc: 74.18%   [EVAL] batch:  145 | acc: 12.50%,  total acc: 73.76%   [EVAL] batch:  146 | acc: 37.50%,  total acc: 73.51%   [EVAL] batch:  147 | acc: 18.75%,  total acc: 73.14%   [EVAL] batch:  148 | acc: 12.50%,  total acc: 72.73%   [EVAL] batch:  149 | acc: 6.25%,  total acc: 72.29%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 71.85%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 71.50%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 71.08%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 70.66%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 70.36%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 69.91%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 69.82%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 69.98%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 70.09%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.23%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 70.30%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 70.55%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 70.66%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 70.97%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 71.07%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 71.24%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 71.40%   [EVAL] batch:  170 | acc: 75.00%,  total acc: 71.42%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 71.40%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 71.35%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 71.44%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 71.29%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 70.95%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 70.76%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 70.54%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 70.43%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 70.28%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 70.20%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 70.05%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 69.95%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 69.77%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 69.80%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 69.69%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 69.58%   [EVAL] batch:  188 | acc: 43.75%,  total acc: 69.44%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 69.31%   [EVAL] batch:  190 | acc: 31.25%,  total acc: 69.11%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 69.01%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 68.81%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 68.65%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 68.59%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 68.56%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 68.50%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 68.53%   [EVAL] batch:  198 | acc: 18.75%,  total acc: 68.28%   [EVAL] batch:  199 | acc: 93.75%,  total acc: 68.41%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 68.41%   [EVAL] batch:  201 | acc: 56.25%,  total acc: 68.35%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 68.35%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 68.29%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 68.26%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 68.33%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 68.42%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.72%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 68.84%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 68.96%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 69.13%   [EVAL] batch:  213 | acc: 56.25%,  total acc: 69.07%   [EVAL] batch:  214 | acc: 43.75%,  total acc: 68.95%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 68.98%   [EVAL] batch:  216 | acc: 62.50%,  total acc: 68.95%   [EVAL] batch:  217 | acc: 75.00%,  total acc: 68.98%   [EVAL] batch:  218 | acc: 56.25%,  total acc: 68.92%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 68.86%   [EVAL] batch:  220 | acc: 12.50%,  total acc: 68.61%   [EVAL] batch:  221 | acc: 12.50%,  total acc: 68.36%   [EVAL] batch:  222 | acc: 12.50%,  total acc: 68.11%   [EVAL] batch:  223 | acc: 12.50%,  total acc: 67.86%   [EVAL] batch:  224 | acc: 12.50%,  total acc: 67.61%   [EVAL] batch:  225 | acc: 43.75%,  total acc: 67.51%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 67.46%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 67.38%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 67.36%   [EVAL] batch:  229 | acc: 62.50%,  total acc: 67.34%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 67.29%   [EVAL] batch:  231 | acc: 68.75%,  total acc: 67.30%   [EVAL] batch:  232 | acc: 62.50%,  total acc: 67.27%   [EVAL] batch:  233 | acc: 68.75%,  total acc: 67.28%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 67.26%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 67.21%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 67.30%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 67.38%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 68.29%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 68.45%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 68.70%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 68.73%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 68.82%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 68.77%   [EVAL] batch:  254 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 68.77%   [EVAL] batch:  256 | acc: 87.50%,  total acc: 68.85%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 68.87%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.87%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 68.87%   [EVAL] batch:  260 | acc: 68.75%,  total acc: 68.87%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 68.82%   [EVAL] batch:  262 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 69.01%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 69.06%   [EVAL] batch:  265 | acc: 87.50%,  total acc: 69.13%   [EVAL] batch:  266 | acc: 93.75%,  total acc: 69.22%   [EVAL] batch:  267 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 69.40%   [EVAL] batch:  269 | acc: 68.75%,  total acc: 69.40%   [EVAL] batch:  270 | acc: 56.25%,  total acc: 69.35%   [EVAL] batch:  271 | acc: 81.25%,  total acc: 69.39%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 69.39%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 69.46%   [EVAL] batch:  274 | acc: 68.75%,  total acc: 69.45%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:  277 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 69.87%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 70.30%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 70.40%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 70.81%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 70.89%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 70.95%   [EVAL] batch:  290 | acc: 75.00%,  total acc: 70.96%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 71.02%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 71.10%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 71.09%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 70.95%   [EVAL] batch:  295 | acc: 25.00%,  total acc: 70.80%   [EVAL] batch:  296 | acc: 31.25%,  total acc: 70.66%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 70.62%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 70.51%   [EVAL] batch:  299 | acc: 37.50%,  total acc: 70.40%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 70.55%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 70.63%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 70.66%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 70.74%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 70.79%   [EVAL] batch:  306 | acc: 43.75%,  total acc: 70.70%   [EVAL] batch:  307 | acc: 6.25%,  total acc: 70.50%   [EVAL] batch:  308 | acc: 6.25%,  total acc: 70.29%   [EVAL] batch:  309 | acc: 18.75%,  total acc: 70.12%   [EVAL] batch:  310 | acc: 6.25%,  total acc: 69.92%   [EVAL] batch:  311 | acc: 6.25%,  total acc: 69.71%   [EVAL] batch:  312 | acc: 31.25%,  total acc: 69.59%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 69.53%   [EVAL] batch:  314 | acc: 37.50%,  total acc: 69.42%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 69.28%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 69.18%   [EVAL] batch:  317 | acc: 62.50%,  total acc: 69.16%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 69.10%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 69.16%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 69.20%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 69.24%   [EVAL] batch:  322 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 69.37%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:  325 | acc: 50.00%,  total acc: 69.33%   [EVAL] batch:  326 | acc: 31.25%,  total acc: 69.21%   [EVAL] batch:  327 | acc: 37.50%,  total acc: 69.11%   [EVAL] batch:  328 | acc: 62.50%,  total acc: 69.09%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 69.05%   [EVAL] batch:  330 | acc: 50.00%,  total acc: 69.00%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 69.05%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 69.13%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 69.36%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 69.71%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:  341 | acc: 100.00%,  total acc: 69.88%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:  343 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 70.35%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 70.50%   [EVAL] batch:  350 | acc: 6.25%,  total acc: 70.32%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 70.15%   [EVAL] batch:  352 | acc: 12.50%,  total acc: 69.99%   [EVAL] batch:  353 | acc: 6.25%,  total acc: 69.81%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 69.63%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 69.47%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 69.50%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:  360 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:  361 | acc: 81.25%,  total acc: 69.82%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 69.82%   [EVAL] batch:  363 | acc: 75.00%,  total acc: 69.83%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 69.74%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 69.71%   [EVAL] batch:  366 | acc: 68.75%,  total acc: 69.70%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 69.67%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 69.58%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 69.63%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 69.64%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 69.61%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 69.70%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:  376 | acc: 87.50%,  total acc: 69.81%   [EVAL] batch:  377 | acc: 87.50%,  total acc: 69.86%   [EVAL] batch:  378 | acc: 68.75%,  total acc: 69.85%   [EVAL] batch:  379 | acc: 87.50%,  total acc: 69.90%   [EVAL] batch:  380 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 70.03%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 70.17%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 70.19%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 70.26%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  387 | acc: 68.75%,  total acc: 70.33%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 70.29%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 70.27%   [EVAL] batch:  390 | acc: 75.00%,  total acc: 70.28%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 70.30%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 70.29%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 70.32%   [EVAL] batch:  394 | acc: 68.75%,  total acc: 70.32%   [EVAL] batch:  395 | acc: 81.25%,  total acc: 70.34%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 70.40%   [EVAL] batch:  397 | acc: 81.25%,  total acc: 70.43%   [EVAL] batch:  398 | acc: 87.50%,  total acc: 70.47%   [EVAL] batch:  399 | acc: 81.25%,  total acc: 70.50%   [EVAL] batch:  400 | acc: 25.00%,  total acc: 70.39%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 70.29%   [EVAL] batch:  402 | acc: 25.00%,  total acc: 70.18%   [EVAL] batch:  403 | acc: 43.75%,  total acc: 70.11%   [EVAL] batch:  404 | acc: 50.00%,  total acc: 70.06%   [EVAL] batch:  405 | acc: 25.00%,  total acc: 69.95%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 69.92%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 69.87%   [EVAL] batch:  408 | acc: 50.00%,  total acc: 69.82%   [EVAL] batch:  409 | acc: 62.50%,  total acc: 69.80%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 69.78%   [EVAL] batch:  411 | acc: 62.50%,  total acc: 69.77%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 69.82%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 69.86%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 69.91%   [EVAL] batch:  416 | acc: 75.00%,  total acc: 69.92%   [EVAL] batch:  417 | acc: 93.75%,  total acc: 69.98%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 69.93%   [EVAL] batch:  420 | acc: 43.75%,  total acc: 69.86%   [EVAL] batch:  421 | acc: 31.25%,  total acc: 69.77%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 69.74%   [EVAL] batch:  423 | acc: 31.25%,  total acc: 69.65%   [EVAL] batch:  424 | acc: 37.50%,  total acc: 69.57%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  432 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 70.13%   [EVAL] batch:  434 | acc: 87.50%,  total acc: 70.17%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 70.21%   [EVAL] batch:  436 | acc: 87.50%,  total acc: 70.25%   [EVAL] batch:  437 | acc: 37.50%,  total acc: 70.18%   
cur_acc:  ['0.9504', '0.8750', '0.6895', '0.7252', '0.8323', '0.7738', '0.7302']
his_acc:  ['0.9504', '0.8985', '0.8029', '0.7625', '0.7608', '0.7182', '0.7018']
CurrentTrain: epoch  0, batch     0 | loss: 7.0257721CurrentTrain: epoch  0, batch     1 | loss: 6.3503399CurrentTrain: epoch  0, batch     2 | loss: 5.7793846CurrentTrain: epoch  0, batch     3 | loss: 5.2179108CurrentTrain: epoch  1, batch     0 | loss: 5.9895267CurrentTrain: epoch  1, batch     1 | loss: 4.8831434CurrentTrain: epoch  1, batch     2 | loss: 4.9824524CurrentTrain: epoch  1, batch     3 | loss: 6.3548889CurrentTrain: epoch  2, batch     0 | loss: 5.0323734CurrentTrain: epoch  2, batch     1 | loss: 4.4083805CurrentTrain: epoch  2, batch     2 | loss: 5.1178732CurrentTrain: epoch  2, batch     3 | loss: 2.7352910CurrentTrain: epoch  3, batch     0 | loss: 4.0142808CurrentTrain: epoch  3, batch     1 | loss: 4.8301477CurrentTrain: epoch  3, batch     2 | loss: 3.8633015CurrentTrain: epoch  3, batch     3 | loss: 2.1458015CurrentTrain: epoch  4, batch     0 | loss: 3.6137850CurrentTrain: epoch  4, batch     1 | loss: 3.6633570CurrentTrain: epoch  4, batch     2 | loss: 5.4287095CurrentTrain: epoch  4, batch     3 | loss: 3.8986602CurrentTrain: epoch  5, batch     0 | loss: 3.4851151CurrentTrain: epoch  5, batch     1 | loss: 3.3331981CurrentTrain: epoch  5, batch     2 | loss: 4.6481190CurrentTrain: epoch  5, batch     3 | loss: 6.3912668CurrentTrain: epoch  6, batch     0 | loss: 3.3739691CurrentTrain: epoch  6, batch     1 | loss: 3.6908426CurrentTrain: epoch  6, batch     2 | loss: 4.3104520CurrentTrain: epoch  6, batch     3 | loss: 2.1804726CurrentTrain: epoch  7, batch     0 | loss: 4.0376644CurrentTrain: epoch  7, batch     1 | loss: 3.7003994CurrentTrain: epoch  7, batch     2 | loss: 3.2870507CurrentTrain: epoch  7, batch     3 | loss: 2.4538827CurrentTrain: epoch  8, batch     0 | loss: 3.3675556CurrentTrain: epoch  8, batch     1 | loss: 3.3054705CurrentTrain: epoch  8, batch     2 | loss: 3.2182245CurrentTrain: epoch  8, batch     3 | loss: 4.6855526CurrentTrain: epoch  9, batch     0 | loss: 3.2896624CurrentTrain: epoch  9, batch     1 | loss: 3.9021425CurrentTrain: epoch  9, batch     2 | loss: 2.8691936CurrentTrain: epoch  9, batch     3 | loss: 3.4493241
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: the " tetrabiblos " was largely responsible for laying down the basic precepts of renaissance astrology , webster ( 1979 ) p.276 .
Head Entity: tetrabiblos
Tail Entity: astrology
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: In her groundbreaking book, "The Second Sex," Simone de Beauvoir explores the role of women in society and the concept of femininity, which has influenced feminist theory significantly.  
Head Entity: The Second Sex  
Tail Entity: feminist theory  

Relation: main subject  
Context: The documentary "Planet Earth" showcases the beauty and diversity of the natural world, highlighting the importance of conservation efforts to protect endangered species.  
Head Entity: Planet Earth  
Tail Entity: conservation efforts  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: the two teams had met in two previous matches , including in the 2006 world cup group stage , won by brazil 1–0 .
Head Entity: 2006 world cup
Tail Entity: brazil
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: In the 2020 UEFA European Championship, Italy and England faced off in the final, with Italy emerging victorious after a tense penalty shootout.  
Head Entity: 2020 UEFA European Championship  
Tail Entity: Italy  

Relation: participating team  
Context: The 2018 FIFA World Cup saw France and Croatia compete in an exhilarating final, where France claimed their second title.  
Head Entity: 2018 FIFA World Cup  
Tail Entity: France  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: at state level , the parliament of victoria consists of the legislative assembly ( the lower house ) and the legislative council ( the upper house ) .
Head Entity: parliament of victoria
Tail Entity: legislative council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The human body is composed of various systems, including the circulatory system, which is essential for transporting blood throughout the body.  
Head Entity: human body  
Tail Entity: circulatory system  

Relation: has part  
Context: The solar system includes several celestial bodies, with the Earth being one of the planets that orbits the Sun.  
Head Entity: solar system  
Tail Entity: Earth  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: gyllenhaal was nominated for an academy award for best supporting actress for her performance .
Head Entity: gyllenhaal
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film was nominated for several prestigious awards, including the Golden Globe for Best Motion Picture.  
Head Entity: The film  
Tail Entity: Golden Globe for Best Motion Picture  

Relation: nominated for  
Context: She was nominated for the Grammy Award for Best New Artist after her debut album received critical acclaim.  
Head Entity: She  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: soon after , besteiro became a member of unión general de trabajadores ( ugt ) trade union , and of the spanish socialist workers ' party ( psoe).jackson , gabriel .
Head Entity: unión general de trabajadores
Tail Entity: trade union
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The Great Wall of China is a remarkable feat of engineering and a symbol of Chinese culture, built to protect against invasions and raids. It is often considered one of the most significant historical structures in the world.  
Head Entity: Great Wall of China  
Tail Entity: historical structure  

Relation: instance of  
Context: The Amazon rainforest is known for its vast biodiversity and is often referred to as the lungs of the Earth due to its role in producing oxygen and absorbing carbon dioxide. It is a critical component of the global ecosystem.  
Head Entity: Amazon rainforest  
Tail Entity: ecosystem
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: the valley is confined by the zillertal alps in the northeast and the sarntal alps in the southwest , rising up to the tagewaldhorn peak at .
Head Entity: tagewaldhorn
Tail Entity: sarntal alps
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the majestic peaks of the rocky mountains stretch across several states, with the highest point being mount elbert, which towers over the surrounding landscape.  
Head Entity: mount elbert  
Tail Entity: rocky mountains  

Relation: mountain range  
Context: nestled in the heart of the himalayas, mount everest stands as the tallest mountain in the world, part of the greater himalayan range that attracts climbers from all over the globe.  
Head Entity: mount everest  
Tail Entity: greater himalayan range  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: anders matthesen has also released several cds with his radio material , in addition to the animated movie " terkel in trouble " , based on one of these .
Head Entity: terkel in trouble
Tail Entity: anders matthesen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the acclaimed film "inception" features a complex narrative that keeps viewers on the edge of their seats, thanks to the brilliant writing of its screenwriter, christopher nolan.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: screenwriter  
Context: the heartwarming story of "the pursuit of happyness" was brought to life by the talented screenwriter, steve conrad, whose script captured the struggles and triumphs of the main character.  
Head Entity: the pursuit of happyness  
Tail Entity: steve conrad  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is a cornerstone of Latin American literature and is originally written in Spanish.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: udasi has been an early sect based on the teachings of sri chand ( 1494–1643 ) , the son of guru nanak , the founder and the first guru of sikhism .
Head Entity: guru nanak
Tail Entity: sikhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the baha'i faith is a religion founded by baha'u'llah in the 19th century, emphasizing the spiritual unity of all humankind.  
Head Entity: baha'u'llah  
Tail Entity: baha'i faith  

Relation: religion  
Context: the ancient greeks practiced a polytheistic religion that included a pantheon of gods and goddesses, such as zeus and athena.  
Head Entity: ancient greeks  
Tail Entity: polytheistic religion  
Mixup data size:  560
MixupTrain:  epoch  0, batch     0 | loss: 2.2565060MixupTrain:  epoch  0, batch     1 | loss: 2.3355883MixupTrain:  epoch  0, batch     2 | loss: 1.7371789MixupTrain:  epoch  0, batch     3 | loss: 1.8500066MixupTrain:  epoch  0, batch     4 | loss: 1.9874051MixupTrain:  epoch  0, batch     5 | loss: 1.9371597MixupTrain:  epoch  0, batch     6 | loss: 1.8504981MixupTrain:  epoch  0, batch     7 | loss: 2.0375232MixupTrain:  epoch  0, batch     8 | loss: 2.9401336MixupTrain:  epoch  0, batch     9 | loss: 1.7803334MixupTrain:  epoch  0, batch    10 | loss: 1.8354279MixupTrain:  epoch  0, batch    11 | loss: 2.0548210MixupTrain:  epoch  0, batch    12 | loss: 1.7234144MixupTrain:  epoch  0, batch    13 | loss: 2.1384710MixupTrain:  epoch  0, batch    14 | loss: 1.6348979MixupTrain:  epoch  0, batch    15 | loss: 1.7852318MixupTrain:  epoch  0, batch    16 | loss: 2.1640377MixupTrain:  epoch  0, batch    17 | loss: 1.6948355MixupTrain:  epoch  0, batch    18 | loss: 2.1181894MixupTrain:  epoch  0, batch    19 | loss: 2.0057144MixupTrain:  epoch  0, batch    20 | loss: 1.9464850MixupTrain:  epoch  0, batch    21 | loss: 1.5838496MixupTrain:  epoch  0, batch    22 | loss: 2.0020104MixupTrain:  epoch  0, batch    23 | loss: 1.5670950MixupTrain:  epoch  0, batch    24 | loss: 1.9702256MixupTrain:  epoch  0, batch    25 | loss: 1.4763829MixupTrain:  epoch  0, batch    26 | loss: 1.8352144MixupTrain:  epoch  0, batch    27 | loss: 2.0327667MixupTrain:  epoch  0, batch    28 | loss: 1.8981679MixupTrain:  epoch  0, batch    29 | loss: 2.2195126MixupTrain:  epoch  0, batch    30 | loss: 1.5935381MixupTrain:  epoch  0, batch    31 | loss: 1.7477029MixupTrain:  epoch  0, batch    32 | loss: 1.7309514MixupTrain:  epoch  0, batch    33 | loss: 1.5287662MixupTrain:  epoch  0, batch    34 | loss: 1.7230198
MemoryTrain:  epoch  0, batch     0 | loss: 1.8998598MemoryTrain:  epoch  0, batch     1 | loss: 1.4948360MemoryTrain:  epoch  0, batch     2 | loss: 1.9780864MemoryTrain:  epoch  0, batch     3 | loss: 2.0223584MemoryTrain:  epoch  0, batch     4 | loss: 1.6845076MemoryTrain:  epoch  0, batch     5 | loss: 2.1108532MemoryTrain:  epoch  0, batch     6 | loss: 2.7696269MemoryTrain:  epoch  0, batch     7 | loss: 1.8388553MemoryTrain:  epoch  0, batch     8 | loss: 2.4740207MemoryTrain:  epoch  0, batch     9 | loss: 2.0643134MemoryTrain:  epoch  0, batch    10 | loss: 1.7984329MemoryTrain:  epoch  0, batch    11 | loss: 2.3003674MemoryTrain:  epoch  0, batch    12 | loss: 2.3311865MemoryTrain:  epoch  0, batch    13 | loss: 2.5033288MemoryTrain:  epoch  0, batch    14 | loss: 2.9333591MemoryTrain:  epoch  1, batch     0 | loss: 1.7048973MemoryTrain:  epoch  1, batch     1 | loss: 1.3632094MemoryTrain:  epoch  1, batch     2 | loss: 2.0412621MemoryTrain:  epoch  1, batch     3 | loss: 1.5379614MemoryTrain:  epoch  1, batch     4 | loss: 2.0139518MemoryTrain:  epoch  1, batch     5 | loss: 2.4087780MemoryTrain:  epoch  1, batch     6 | loss: 1.8453357MemoryTrain:  epoch  1, batch     7 | loss: 1.9171233MemoryTrain:  epoch  1, batch     8 | loss: 1.9339857MemoryTrain:  epoch  1, batch     9 | loss: 1.9773754MemoryTrain:  epoch  1, batch    10 | loss: 2.3701353MemoryTrain:  epoch  1, batch    11 | loss: 1.7609355MemoryTrain:  epoch  1, batch    12 | loss: 2.3589683MemoryTrain:  epoch  1, batch    13 | loss: 1.6246822MemoryTrain:  epoch  1, batch    14 | loss: 2.1279597MemoryTrain:  epoch  2, batch     0 | loss: 1.4369771MemoryTrain:  epoch  2, batch     1 | loss: 1.5665703MemoryTrain:  epoch  2, batch     2 | loss: 2.9414883MemoryTrain:  epoch  2, batch     3 | loss: 1.7428309MemoryTrain:  epoch  2, batch     4 | loss: 1.8158299MemoryTrain:  epoch  2, batch     5 | loss: 1.4022521MemoryTrain:  epoch  2, batch     6 | loss: 2.0390115MemoryTrain:  epoch  2, batch     7 | loss: 2.4397440MemoryTrain:  epoch  2, batch     8 | loss: 1.5521998MemoryTrain:  epoch  2, batch     9 | loss: 1.3183796MemoryTrain:  epoch  2, batch    10 | loss: 1.7454927MemoryTrain:  epoch  2, batch    11 | loss: 1.5775678MemoryTrain:  epoch  2, batch    12 | loss: 1.8750062MemoryTrain:  epoch  2, batch    13 | loss: 1.5861660MemoryTrain:  epoch  2, batch    14 | loss: 1.3723836MemoryTrain:  epoch  3, batch     0 | loss: 1.4736006MemoryTrain:  epoch  3, batch     1 | loss: 1.9504583MemoryTrain:  epoch  3, batch     2 | loss: 2.4009244MemoryTrain:  epoch  3, batch     3 | loss: 1.3304117MemoryTrain:  epoch  3, batch     4 | loss: 1.5836208MemoryTrain:  epoch  3, batch     5 | loss: 1.3444747MemoryTrain:  epoch  3, batch     6 | loss: 1.3084617MemoryTrain:  epoch  3, batch     7 | loss: 1.5775613MemoryTrain:  epoch  3, batch     8 | loss: 1.3240407MemoryTrain:  epoch  3, batch     9 | loss: 1.3516779MemoryTrain:  epoch  3, batch    10 | loss: 2.0464411MemoryTrain:  epoch  3, batch    11 | loss: 1.5327345MemoryTrain:  epoch  3, batch    12 | loss: 1.7239709MemoryTrain:  epoch  3, batch    13 | loss: 1.6818390MemoryTrain:  epoch  3, batch    14 | loss: 1.2588506MemoryTrain:  epoch  4, batch     0 | loss: 1.4191573MemoryTrain:  epoch  4, batch     1 | loss: 1.3420448MemoryTrain:  epoch  4, batch     2 | loss: 1.3490515MemoryTrain:  epoch  4, batch     3 | loss: 1.4920020MemoryTrain:  epoch  4, batch     4 | loss: 1.3166087MemoryTrain:  epoch  4, batch     5 | loss: 2.4378967MemoryTrain:  epoch  4, batch     6 | loss: 1.4365761MemoryTrain:  epoch  4, batch     7 | loss: 1.3925164MemoryTrain:  epoch  4, batch     8 | loss: 2.3637240MemoryTrain:  epoch  4, batch     9 | loss: 1.6463325MemoryTrain:  epoch  4, batch    10 | loss: 1.4448725MemoryTrain:  epoch  4, batch    11 | loss: 1.7654290MemoryTrain:  epoch  4, batch    12 | loss: 1.3997600MemoryTrain:  epoch  4, batch    13 | loss: 1.4726484MemoryTrain:  epoch  4, batch    14 | loss: 1.4036212MemoryTrain:  epoch  5, batch     0 | loss: 1.4187355MemoryTrain:  epoch  5, batch     1 | loss: 1.8372126MemoryTrain:  epoch  5, batch     2 | loss: 1.9716246MemoryTrain:  epoch  5, batch     3 | loss: 1.2729003MemoryTrain:  epoch  5, batch     4 | loss: 1.3493012MemoryTrain:  epoch  5, batch     5 | loss: 1.5406506MemoryTrain:  epoch  5, batch     6 | loss: 1.3391616MemoryTrain:  epoch  5, batch     7 | loss: 1.4417827MemoryTrain:  epoch  5, batch     8 | loss: 1.7022947MemoryTrain:  epoch  5, batch     9 | loss: 1.4525772MemoryTrain:  epoch  5, batch    10 | loss: 1.4187508MemoryTrain:  epoch  5, batch    11 | loss: 1.2485362MemoryTrain:  epoch  5, batch    12 | loss: 1.9830041MemoryTrain:  epoch  5, batch    13 | loss: 1.4121519MemoryTrain:  epoch  5, batch    14 | loss: 1.1898857MemoryTrain:  epoch  6, batch     0 | loss: 1.2440015MemoryTrain:  epoch  6, batch     1 | loss: 1.2967634MemoryTrain:  epoch  6, batch     2 | loss: 1.3835232MemoryTrain:  epoch  6, batch     3 | loss: 1.2851222MemoryTrain:  epoch  6, batch     4 | loss: 1.2566648MemoryTrain:  epoch  6, batch     5 | loss: 1.5091300MemoryTrain:  epoch  6, batch     6 | loss: 1.8888308MemoryTrain:  epoch  6, batch     7 | loss: 1.9970198MemoryTrain:  epoch  6, batch     8 | loss: 1.2826645MemoryTrain:  epoch  6, batch     9 | loss: 1.3939056MemoryTrain:  epoch  6, batch    10 | loss: 1.3949174MemoryTrain:  epoch  6, batch    11 | loss: 1.6768579MemoryTrain:  epoch  6, batch    12 | loss: 1.3404919MemoryTrain:  epoch  6, batch    13 | loss: 1.3414880MemoryTrain:  epoch  6, batch    14 | loss: 1.4194753MemoryTrain:  epoch  7, batch     0 | loss: 1.2857223MemoryTrain:  epoch  7, batch     1 | loss: 1.2629265MemoryTrain:  epoch  7, batch     2 | loss: 1.4181937MemoryTrain:  epoch  7, batch     3 | loss: 1.4070561MemoryTrain:  epoch  7, batch     4 | loss: 1.4816573MemoryTrain:  epoch  7, batch     5 | loss: 1.2614914MemoryTrain:  epoch  7, batch     6 | loss: 1.3842232MemoryTrain:  epoch  7, batch     7 | loss: 1.3085219MemoryTrain:  epoch  7, batch     8 | loss: 1.2726598MemoryTrain:  epoch  7, batch     9 | loss: 1.3227156MemoryTrain:  epoch  7, batch    10 | loss: 1.3939579MemoryTrain:  epoch  7, batch    11 | loss: 1.3151765MemoryTrain:  epoch  7, batch    12 | loss: 1.8353524MemoryTrain:  epoch  7, batch    13 | loss: 1.4486222MemoryTrain:  epoch  7, batch    14 | loss: 1.4607482MemoryTrain:  epoch  8, batch     0 | loss: 1.7772874MemoryTrain:  epoch  8, batch     1 | loss: 1.5840361MemoryTrain:  epoch  8, batch     2 | loss: 1.2997917MemoryTrain:  epoch  8, batch     3 | loss: 1.3349977MemoryTrain:  epoch  8, batch     4 | loss: 1.3931228MemoryTrain:  epoch  8, batch     5 | loss: 1.3351890MemoryTrain:  epoch  8, batch     6 | loss: 1.4438860MemoryTrain:  epoch  8, batch     7 | loss: 1.2475438MemoryTrain:  epoch  8, batch     8 | loss: 1.2589090MemoryTrain:  epoch  8, batch     9 | loss: 1.2647529MemoryTrain:  epoch  8, batch    10 | loss: 1.3418298MemoryTrain:  epoch  8, batch    11 | loss: 1.2836244MemoryTrain:  epoch  8, batch    12 | loss: 1.4828256MemoryTrain:  epoch  8, batch    13 | loss: 1.2240963MemoryTrain:  epoch  8, batch    14 | loss: 1.2964039MemoryTrain:  epoch  9, batch     0 | loss: 1.3281206MemoryTrain:  epoch  9, batch     1 | loss: 1.4943807MemoryTrain:  epoch  9, batch     2 | loss: 1.2335610MemoryTrain:  epoch  9, batch     3 | loss: 1.2608335MemoryTrain:  epoch  9, batch     4 | loss: 1.2384243MemoryTrain:  epoch  9, batch     5 | loss: 1.2309234MemoryTrain:  epoch  9, batch     6 | loss: 1.3717792MemoryTrain:  epoch  9, batch     7 | loss: 1.4325664MemoryTrain:  epoch  9, batch     8 | loss: 1.2980137MemoryTrain:  epoch  9, batch     9 | loss: 1.3353294MemoryTrain:  epoch  9, batch    10 | loss: 1.3755964MemoryTrain:  epoch  9, batch    11 | loss: 1.2582147MemoryTrain:  epoch  9, batch    12 | loss: 1.3078743MemoryTrain:  epoch  9, batch    13 | loss: 1.2522745MemoryTrain:  epoch  9, batch    14 | loss: 1.1679494
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 64.58%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 57.81%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 51.39%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 50.33%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 52.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.76%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 56.82%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 58.70%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 61.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 59.62%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 57.87%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 56.03%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 54.96%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 53.54%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 52.22%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 52.54%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 53.79%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 54.96%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 56.07%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 56.94%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 57.94%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 58.88%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 59.62%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 60.47%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 60.67%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 60.57%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 61.19%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 61.93%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 61.25%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 60.60%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 60.64%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 60.29%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 60.08%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 59.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 59.93%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 60.22%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 60.61%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 61.00%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 61.59%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 61.94%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 62.17%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 62.61%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 63.14%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 63.44%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 64.04%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 64.31%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 63.99%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 63.12%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 73.30%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 73.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.52%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 78.86%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 78.93%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 79.34%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 79.73%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 79.93%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.45%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.94%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 81.55%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 81.69%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 81.68%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 81.39%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 80.98%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 80.32%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 79.82%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 79.59%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 78.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 78.55%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 78.61%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 78.89%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 78.47%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 78.07%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 77.90%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 77.52%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 76.62%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 76.06%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 75.72%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 75.30%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 75.30%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 75.49%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 75.58%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 75.85%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 75.93%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 76.36%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 75.71%   [EVAL] batch:   70 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 74.39%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 73.72%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 72.97%   [EVAL] batch:   74 | acc: 18.75%,  total acc: 72.25%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 72.29%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 72.40%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 72.60%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 72.55%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 72.45%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 72.03%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 71.23%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 70.46%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 69.78%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 69.19%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 68.53%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 68.47%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 70.39%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 70.51%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 70.36%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 70.22%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 70.27%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 70.00%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 70.30%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 71.64%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 72.42%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 73.16%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 73.17%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 73.03%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 73.28%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 73.24%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 73.36%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 73.42%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 73.65%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 73.76%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 73.92%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 74.29%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 74.45%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 73.86%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 73.33%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 72.75%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 72.19%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 71.63%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 71.14%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 71.07%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 71.15%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 71.36%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 71.76%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 71.74%   [EVAL] batch:  138 | acc: 37.50%,  total acc: 71.49%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 71.34%   [EVAL] batch:  140 | acc: 62.50%,  total acc: 71.28%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 71.04%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 70.85%   [EVAL] batch:  143 | acc: 12.50%,  total acc: 70.44%   [EVAL] batch:  144 | acc: 18.75%,  total acc: 70.09%   [EVAL] batch:  145 | acc: 6.25%,  total acc: 69.65%   [EVAL] batch:  146 | acc: 25.00%,  total acc: 69.35%   [EVAL] batch:  147 | acc: 18.75%,  total acc: 69.00%   [EVAL] batch:  148 | acc: 12.50%,  total acc: 68.62%   [EVAL] batch:  149 | acc: 6.25%,  total acc: 68.21%   [EVAL] batch:  150 | acc: 6.25%,  total acc: 67.80%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 67.39%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 66.99%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 66.60%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 66.25%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 65.83%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 65.76%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 65.86%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 65.96%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 66.23%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 66.32%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 66.37%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 66.50%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 66.83%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 66.95%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  169 | acc: 68.75%,  total acc: 67.35%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 67.32%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 67.30%   [EVAL] batch:  172 | acc: 62.50%,  total acc: 67.27%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 67.35%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 67.21%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 66.94%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 66.81%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 66.61%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 66.52%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 66.39%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 66.37%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 66.24%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 66.15%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 66.00%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 66.01%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 65.93%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 66.04%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 65.82%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 65.64%   [EVAL] batch:  189 | acc: 50.00%,  total acc: 65.56%   [EVAL] batch:  190 | acc: 31.25%,  total acc: 65.38%   [EVAL] batch:  191 | acc: 43.75%,  total acc: 65.27%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 65.22%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 65.08%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 65.13%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 65.11%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 65.07%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 65.06%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 64.86%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 64.97%   [EVAL] batch:  200 | acc: 37.50%,  total acc: 64.83%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 64.76%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 64.75%   [EVAL] batch:  203 | acc: 37.50%,  total acc: 64.61%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 64.51%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 64.50%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 64.61%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 65.09%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 65.43%   [EVAL] batch:  213 | acc: 62.50%,  total acc: 65.42%   [EVAL] batch:  214 | acc: 62.50%,  total acc: 65.41%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 65.48%   [EVAL] batch:  216 | acc: 62.50%,  total acc: 65.47%   [EVAL] batch:  217 | acc: 75.00%,  total acc: 65.51%   [EVAL] batch:  218 | acc: 62.50%,  total acc: 65.50%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 65.43%   [EVAL] batch:  220 | acc: 12.50%,  total acc: 65.19%   [EVAL] batch:  221 | acc: 18.75%,  total acc: 64.98%   [EVAL] batch:  222 | acc: 12.50%,  total acc: 64.74%   [EVAL] batch:  223 | acc: 12.50%,  total acc: 64.51%   [EVAL] batch:  224 | acc: 6.25%,  total acc: 64.25%   [EVAL] batch:  225 | acc: 37.50%,  total acc: 64.13%   [EVAL] batch:  226 | acc: 56.25%,  total acc: 64.10%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 64.04%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:  229 | acc: 62.50%,  total acc: 64.05%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 64.02%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 64.09%   [EVAL] batch:  232 | acc: 62.50%,  total acc: 64.08%   [EVAL] batch:  233 | acc: 75.00%,  total acc: 64.13%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 64.12%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 64.09%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 64.19%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 64.29%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 64.56%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 64.68%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 64.82%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 65.29%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 65.41%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 65.47%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 65.61%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  250 | acc: 87.50%,  total acc: 65.84%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 65.87%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 65.96%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 65.99%   [EVAL] batch:  254 | acc: 75.00%,  total acc: 66.03%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 66.09%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:  258 | acc: 62.50%,  total acc: 66.17%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 66.18%   [EVAL] batch:  260 | acc: 62.50%,  total acc: 66.16%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 66.13%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 66.19%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 66.23%   [EVAL] batch:  265 | acc: 68.75%,  total acc: 66.24%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 66.27%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 66.38%   [EVAL] batch:  269 | acc: 37.50%,  total acc: 66.27%   [EVAL] batch:  270 | acc: 43.75%,  total acc: 66.19%   [EVAL] batch:  271 | acc: 56.25%,  total acc: 66.15%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 66.12%   [EVAL] batch:  273 | acc: 68.75%,  total acc: 66.13%   [EVAL] batch:  274 | acc: 43.75%,  total acc: 66.05%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:  277 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 66.51%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.63%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  287 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 67.74%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 67.78%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 67.87%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 67.96%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 67.96%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 67.84%   [EVAL] batch:  295 | acc: 25.00%,  total acc: 67.69%   [EVAL] batch:  296 | acc: 25.00%,  total acc: 67.55%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 67.51%   [EVAL] batch:  298 | acc: 31.25%,  total acc: 67.39%   [EVAL] batch:  299 | acc: 37.50%,  total acc: 67.29%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 67.38%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 67.47%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 67.53%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 67.58%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:  306 | acc: 43.75%,  total acc: 67.63%   [EVAL] batch:  307 | acc: 6.25%,  total acc: 67.43%   [EVAL] batch:  308 | acc: 0.00%,  total acc: 67.21%   [EVAL] batch:  309 | acc: 12.50%,  total acc: 67.04%   [EVAL] batch:  310 | acc: 0.00%,  total acc: 66.82%   [EVAL] batch:  311 | acc: 0.00%,  total acc: 66.61%   [EVAL] batch:  312 | acc: 31.25%,  total acc: 66.49%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 66.42%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 66.31%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 66.18%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 66.07%   [EVAL] batch:  317 | acc: 68.75%,  total acc: 66.08%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 65.95%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 66.00%   [EVAL] batch:  320 | acc: 81.25%,  total acc: 66.04%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 66.09%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 66.18%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 66.24%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 66.27%   [EVAL] batch:  325 | acc: 50.00%,  total acc: 66.22%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 66.09%   [EVAL] batch:  327 | acc: 37.50%,  total acc: 66.01%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 65.98%   [EVAL] batch:  329 | acc: 56.25%,  total acc: 65.95%   [EVAL] batch:  330 | acc: 50.00%,  total acc: 65.90%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 65.96%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 66.05%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  334 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 66.42%   [EVAL] batch:  338 | acc: 81.25%,  total acc: 66.46%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 66.53%   [EVAL] batch:  340 | acc: 87.50%,  total acc: 66.59%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 66.63%   [EVAL] batch:  342 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 66.77%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 66.98%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 67.04%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:  349 | acc: 93.75%,  total acc: 67.21%   [EVAL] batch:  350 | acc: 6.25%,  total acc: 67.04%   [EVAL] batch:  351 | acc: 25.00%,  total acc: 66.92%   [EVAL] batch:  352 | acc: 18.75%,  total acc: 66.78%   [EVAL] batch:  353 | acc: 12.50%,  total acc: 66.63%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 66.46%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 66.31%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 66.35%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 66.43%   [EVAL] batch:  358 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  359 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:  360 | acc: 87.50%,  total acc: 66.66%   [EVAL] batch:  361 | acc: 81.25%,  total acc: 66.70%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 66.70%   [EVAL] batch:  363 | acc: 62.50%,  total acc: 66.69%   [EVAL] batch:  364 | acc: 31.25%,  total acc: 66.59%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 66.56%   [EVAL] batch:  366 | acc: 62.50%,  total acc: 66.55%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 66.51%   [EVAL] batch:  368 | acc: 37.50%,  total acc: 66.43%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 66.39%   [EVAL] batch:  370 | acc: 62.50%,  total acc: 66.37%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 66.31%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 66.35%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 66.41%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 66.40%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  376 | acc: 81.25%,  total acc: 66.51%   [EVAL] batch:  377 | acc: 87.50%,  total acc: 66.57%   [EVAL] batch:  378 | acc: 81.25%,  total acc: 66.61%   [EVAL] batch:  379 | acc: 87.50%,  total acc: 66.66%   [EVAL] batch:  380 | acc: 93.75%,  total acc: 66.73%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 66.80%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 67.01%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  387 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:  388 | acc: 50.00%,  total acc: 67.14%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 67.15%   [EVAL] batch:  390 | acc: 75.00%,  total acc: 67.17%   [EVAL] batch:  391 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 67.21%   [EVAL] batch:  394 | acc: 68.75%,  total acc: 67.22%   [EVAL] batch:  395 | acc: 81.25%,  total acc: 67.25%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 67.32%   [EVAL] batch:  397 | acc: 87.50%,  total acc: 67.37%   [EVAL] batch:  398 | acc: 75.00%,  total acc: 67.39%   [EVAL] batch:  399 | acc: 68.75%,  total acc: 67.39%   [EVAL] batch:  400 | acc: 37.50%,  total acc: 67.32%   [EVAL] batch:  401 | acc: 50.00%,  total acc: 67.27%   [EVAL] batch:  402 | acc: 25.00%,  total acc: 67.17%   [EVAL] batch:  403 | acc: 31.25%,  total acc: 67.08%   [EVAL] batch:  404 | acc: 25.00%,  total acc: 66.98%   [EVAL] batch:  405 | acc: 12.50%,  total acc: 66.84%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 66.82%   [EVAL] batch:  407 | acc: 56.25%,  total acc: 66.79%   [EVAL] batch:  408 | acc: 56.25%,  total acc: 66.76%   [EVAL] batch:  409 | acc: 56.25%,  total acc: 66.74%   [EVAL] batch:  410 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 66.72%   [EVAL] batch:  412 | acc: 62.50%,  total acc: 66.71%   [EVAL] batch:  413 | acc: 62.50%,  total acc: 66.70%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 66.73%   [EVAL] batch:  415 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  416 | acc: 68.75%,  total acc: 66.74%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 66.75%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 66.78%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 66.71%   [EVAL] batch:  420 | acc: 43.75%,  total acc: 66.66%   [EVAL] batch:  421 | acc: 25.00%,  total acc: 66.56%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 66.53%   [EVAL] batch:  423 | acc: 31.25%,  total acc: 66.45%   [EVAL] batch:  424 | acc: 43.75%,  total acc: 66.40%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 66.55%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 66.63%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 66.71%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 66.86%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  432 | acc: 81.25%,  total acc: 66.97%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 67.04%   [EVAL] batch:  434 | acc: 87.50%,  total acc: 67.08%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 67.13%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:  437 | acc: 75.00%,  total acc: 67.21%   [EVAL] batch:  438 | acc: 56.25%,  total acc: 67.18%   [EVAL] batch:  439 | acc: 56.25%,  total acc: 67.16%   [EVAL] batch:  440 | acc: 68.75%,  total acc: 67.16%   [EVAL] batch:  441 | acc: 75.00%,  total acc: 67.18%   [EVAL] batch:  442 | acc: 75.00%,  total acc: 67.20%   [EVAL] batch:  443 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 67.22%   [EVAL] batch:  445 | acc: 68.75%,  total acc: 67.22%   [EVAL] batch:  446 | acc: 68.75%,  total acc: 67.23%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:  448 | acc: 93.75%,  total acc: 67.33%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 67.36%   [EVAL] batch:  450 | acc: 0.00%,  total acc: 67.21%   [EVAL] batch:  451 | acc: 0.00%,  total acc: 67.06%   [EVAL] batch:  452 | acc: 12.50%,  total acc: 66.94%   [EVAL] batch:  453 | acc: 6.25%,  total acc: 66.81%   [EVAL] batch:  454 | acc: 0.00%,  total acc: 66.66%   [EVAL] batch:  455 | acc: 6.25%,  total acc: 66.53%   [EVAL] batch:  456 | acc: 68.75%,  total acc: 66.53%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:  461 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  462 | acc: 56.25%,  total acc: 66.86%   [EVAL] batch:  463 | acc: 6.25%,  total acc: 66.73%   [EVAL] batch:  464 | acc: 12.50%,  total acc: 66.61%   [EVAL] batch:  465 | acc: 6.25%,  total acc: 66.48%   [EVAL] batch:  466 | acc: 31.25%,  total acc: 66.41%   [EVAL] batch:  467 | acc: 6.25%,  total acc: 66.28%   [EVAL] batch:  468 | acc: 31.25%,  total acc: 66.20%   [EVAL] batch:  469 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 66.31%   [EVAL] batch:  471 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:  472 | acc: 87.50%,  total acc: 66.40%   [EVAL] batch:  473 | acc: 93.75%,  total acc: 66.46%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  475 | acc: 87.50%,  total acc: 66.57%   [EVAL] batch:  476 | acc: 87.50%,  total acc: 66.61%   [EVAL] batch:  477 | acc: 81.25%,  total acc: 66.64%   [EVAL] batch:  478 | acc: 56.25%,  total acc: 66.62%   [EVAL] batch:  479 | acc: 87.50%,  total acc: 66.67%   [EVAL] batch:  480 | acc: 87.50%,  total acc: 66.71%   [EVAL] batch:  481 | acc: 62.50%,  total acc: 66.70%   [EVAL] batch:  482 | acc: 25.00%,  total acc: 66.61%   [EVAL] batch:  483 | acc: 50.00%,  total acc: 66.58%   [EVAL] batch:  484 | acc: 50.00%,  total acc: 66.55%   [EVAL] batch:  485 | acc: 43.75%,  total acc: 66.50%   [EVAL] batch:  486 | acc: 56.25%,  total acc: 66.48%   [EVAL] batch:  487 | acc: 56.25%,  total acc: 66.46%   [EVAL] batch:  488 | acc: 62.50%,  total acc: 66.45%   [EVAL] batch:  489 | acc: 87.50%,  total acc: 66.49%   [EVAL] batch:  490 | acc: 87.50%,  total acc: 66.54%   [EVAL] batch:  491 | acc: 75.00%,  total acc: 66.55%   [EVAL] batch:  492 | acc: 81.25%,  total acc: 66.58%   [EVAL] batch:  493 | acc: 75.00%,  total acc: 66.60%   [EVAL] batch:  494 | acc: 93.75%,  total acc: 66.65%   [EVAL] batch:  495 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  496 | acc: 81.25%,  total acc: 66.74%   [EVAL] batch:  497 | acc: 93.75%,  total acc: 66.79%   [EVAL] batch:  498 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:  499 | acc: 87.50%,  total acc: 66.88%   
cur_acc:  ['0.9504', '0.8750', '0.6895', '0.7252', '0.8323', '0.7738', '0.7302', '0.6399']
his_acc:  ['0.9504', '0.8985', '0.8029', '0.7625', '0.7608', '0.7182', '0.7018', '0.6687']
--------Round  5
seed:  600
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 12.5413551CurrentTrain: epoch  0, batch     1 | loss: 12.4633293CurrentTrain: epoch  0, batch     2 | loss: 12.1186218CurrentTrain: epoch  0, batch     3 | loss: 12.1417685CurrentTrain: epoch  0, batch     4 | loss: 11.3149204CurrentTrain: epoch  0, batch     5 | loss: 11.1745930CurrentTrain: epoch  0, batch     6 | loss: 11.3960028CurrentTrain: epoch  0, batch     7 | loss: 10.8999014CurrentTrain: epoch  0, batch     8 | loss: 11.0632915CurrentTrain: epoch  0, batch     9 | loss: 10.9676456CurrentTrain: epoch  0, batch    10 | loss: 10.8567753CurrentTrain: epoch  0, batch    11 | loss: 10.7993183CurrentTrain: epoch  0, batch    12 | loss: 10.1524439CurrentTrain: epoch  0, batch    13 | loss: 10.2250338CurrentTrain: epoch  0, batch    14 | loss: 10.5698223CurrentTrain: epoch  0, batch    15 | loss: 10.4198437CurrentTrain: epoch  0, batch    16 | loss: 10.4869642CurrentTrain: epoch  0, batch    17 | loss: 9.9758167CurrentTrain: epoch  0, batch    18 | loss: 9.9229813CurrentTrain: epoch  0, batch    19 | loss: 10.0821829CurrentTrain: epoch  0, batch    20 | loss: 9.9865417CurrentTrain: epoch  0, batch    21 | loss: 10.0910521CurrentTrain: epoch  0, batch    22 | loss: 10.1870756CurrentTrain: epoch  0, batch    23 | loss: 9.6298943CurrentTrain: epoch  0, batch    24 | loss: 10.0356731CurrentTrain: epoch  0, batch    25 | loss: 9.4807091CurrentTrain: epoch  0, batch    26 | loss: 9.5144825CurrentTrain: epoch  0, batch    27 | loss: 9.5250301CurrentTrain: epoch  0, batch    28 | loss: 9.2917051CurrentTrain: epoch  0, batch    29 | loss: 9.5487480CurrentTrain: epoch  0, batch    30 | loss: 8.9619350CurrentTrain: epoch  0, batch    31 | loss: 9.2323227CurrentTrain: epoch  0, batch    32 | loss: 9.1642056CurrentTrain: epoch  0, batch    33 | loss: 9.0101261CurrentTrain: epoch  0, batch    34 | loss: 8.5636616CurrentTrain: epoch  0, batch    35 | loss: 9.2845860CurrentTrain: epoch  0, batch    36 | loss: 9.6425629CurrentTrain: epoch  0, batch    37 | loss: 9.5536880CurrentTrain: epoch  0, batch    38 | loss: 9.0572338CurrentTrain: epoch  0, batch    39 | loss: 9.1801405CurrentTrain: epoch  0, batch    40 | loss: 9.1149311CurrentTrain: epoch  0, batch    41 | loss: 9.2590437CurrentTrain: epoch  0, batch    42 | loss: 8.9242935CurrentTrain: epoch  0, batch    43 | loss: 8.2083549CurrentTrain: epoch  0, batch    44 | loss: 8.8392601CurrentTrain: epoch  0, batch    45 | loss: 9.0996952CurrentTrain: epoch  0, batch    46 | loss: 9.0425396CurrentTrain: epoch  0, batch    47 | loss: 8.3012009CurrentTrain: epoch  0, batch    48 | loss: 8.1725025CurrentTrain: epoch  0, batch    49 | loss: 9.1530247CurrentTrain: epoch  0, batch    50 | loss: 8.3035259CurrentTrain: epoch  0, batch    51 | loss: 8.8161583CurrentTrain: epoch  0, batch    52 | loss: 8.8410664CurrentTrain: epoch  0, batch    53 | loss: 8.0796452CurrentTrain: epoch  0, batch    54 | loss: 8.3058033CurrentTrain: epoch  0, batch    55 | loss: 8.4084120CurrentTrain: epoch  0, batch    56 | loss: 7.2897749CurrentTrain: epoch  0, batch    57 | loss: 7.8801079CurrentTrain: epoch  0, batch    58 | loss: 7.9321527CurrentTrain: epoch  0, batch    59 | loss: 7.7754583CurrentTrain: epoch  0, batch    60 | loss: 8.2476759CurrentTrain: epoch  0, batch    61 | loss: 7.6015534CurrentTrain: epoch  0, batch    62 | loss: 6.8703451CurrentTrain: epoch  1, batch     0 | loss: 8.1142197CurrentTrain: epoch  1, batch     1 | loss: 7.6789813CurrentTrain: epoch  1, batch     2 | loss: 7.6335907CurrentTrain: epoch  1, batch     3 | loss: 7.8540182CurrentTrain: epoch  1, batch     4 | loss: 6.3869133CurrentTrain: epoch  1, batch     5 | loss: 8.1255655CurrentTrain: epoch  1, batch     6 | loss: 6.5367413CurrentTrain: epoch  1, batch     7 | loss: 7.5707440CurrentTrain: epoch  1, batch     8 | loss: 7.3160100CurrentTrain: epoch  1, batch     9 | loss: 7.9883804CurrentTrain: epoch  1, batch    10 | loss: 7.5580711CurrentTrain: epoch  1, batch    11 | loss: 7.5571957CurrentTrain: epoch  1, batch    12 | loss: 6.9015603CurrentTrain: epoch  1, batch    13 | loss: 6.6254330CurrentTrain: epoch  1, batch    14 | loss: 6.9210658CurrentTrain: epoch  1, batch    15 | loss: 7.6339707CurrentTrain: epoch  1, batch    16 | loss: 7.1581159CurrentTrain: epoch  1, batch    17 | loss: 8.8041954CurrentTrain: epoch  1, batch    18 | loss: 6.8817163CurrentTrain: epoch  1, batch    19 | loss: 7.2875528CurrentTrain: epoch  1, batch    20 | loss: 7.3888278CurrentTrain: epoch  1, batch    21 | loss: 7.0350556CurrentTrain: epoch  1, batch    22 | loss: 6.8115239CurrentTrain: epoch  1, batch    23 | loss: 6.8246737CurrentTrain: epoch  1, batch    24 | loss: 7.1095667CurrentTrain: epoch  1, batch    25 | loss: 6.8053589CurrentTrain: epoch  1, batch    26 | loss: 7.0387669CurrentTrain: epoch  1, batch    27 | loss: 7.0062475CurrentTrain: epoch  1, batch    28 | loss: 6.4405055CurrentTrain: epoch  1, batch    29 | loss: 6.9366255CurrentTrain: epoch  1, batch    30 | loss: 6.1248856CurrentTrain: epoch  1, batch    31 | loss: 6.8225899CurrentTrain: epoch  1, batch    32 | loss: 6.6376643CurrentTrain: epoch  1, batch    33 | loss: 7.6227388CurrentTrain: epoch  1, batch    34 | loss: 6.8592768CurrentTrain: epoch  1, batch    35 | loss: 6.5402813CurrentTrain: epoch  1, batch    36 | loss: 6.8043613CurrentTrain: epoch  1, batch    37 | loss: 6.6218319CurrentTrain: epoch  1, batch    38 | loss: 7.2071996CurrentTrain: epoch  1, batch    39 | loss: 6.1384087CurrentTrain: epoch  1, batch    40 | loss: 5.6079431CurrentTrain: epoch  1, batch    41 | loss: 5.6669436CurrentTrain: epoch  1, batch    42 | loss: 7.0639372CurrentTrain: epoch  1, batch    43 | loss: 6.8855286CurrentTrain: epoch  1, batch    44 | loss: 6.2670326CurrentTrain: epoch  1, batch    45 | loss: 6.0369892CurrentTrain: epoch  1, batch    46 | loss: 6.2983017CurrentTrain: epoch  1, batch    47 | loss: 7.2019396CurrentTrain: epoch  1, batch    48 | loss: 6.5509806CurrentTrain: epoch  1, batch    49 | loss: 6.2315307CurrentTrain: epoch  1, batch    50 | loss: 6.9890728CurrentTrain: epoch  1, batch    51 | loss: 6.4481134CurrentTrain: epoch  1, batch    52 | loss: 7.6210442CurrentTrain: epoch  1, batch    53 | loss: 6.0861712CurrentTrain: epoch  1, batch    54 | loss: 6.9446526CurrentTrain: epoch  1, batch    55 | loss: 6.1039147CurrentTrain: epoch  1, batch    56 | loss: 5.5099306CurrentTrain: epoch  1, batch    57 | loss: 6.1720037CurrentTrain: epoch  1, batch    58 | loss: 5.6401124CurrentTrain: epoch  1, batch    59 | loss: 6.4503927CurrentTrain: epoch  1, batch    60 | loss: 5.6848607CurrentTrain: epoch  1, batch    61 | loss: 6.2032542CurrentTrain: epoch  1, batch    62 | loss: 7.0859489CurrentTrain: epoch  2, batch     0 | loss: 5.8457017CurrentTrain: epoch  2, batch     1 | loss: 5.9228325CurrentTrain: epoch  2, batch     2 | loss: 6.1305952CurrentTrain: epoch  2, batch     3 | loss: 5.9457612CurrentTrain: epoch  2, batch     4 | loss: 5.7915092CurrentTrain: epoch  2, batch     5 | loss: 6.0936737CurrentTrain: epoch  2, batch     6 | loss: 5.6430044CurrentTrain: epoch  2, batch     7 | loss: 5.9252663CurrentTrain: epoch  2, batch     8 | loss: 7.1059146CurrentTrain: epoch  2, batch     9 | loss: 5.6664948CurrentTrain: epoch  2, batch    10 | loss: 6.4867768CurrentTrain: epoch  2, batch    11 | loss: 5.9273558CurrentTrain: epoch  2, batch    12 | loss: 6.6321363CurrentTrain: epoch  2, batch    13 | loss: 5.7038584CurrentTrain: epoch  2, batch    14 | loss: 5.8800240CurrentTrain: epoch  2, batch    15 | loss: 5.8434825CurrentTrain: epoch  2, batch    16 | loss: 6.2002916CurrentTrain: epoch  2, batch    17 | loss: 6.2848263CurrentTrain: epoch  2, batch    18 | loss: 5.4849186CurrentTrain: epoch  2, batch    19 | loss: 5.6575222CurrentTrain: epoch  2, batch    20 | loss: 5.2055874CurrentTrain: epoch  2, batch    21 | loss: 6.0855904CurrentTrain: epoch  2, batch    22 | loss: 5.2468338CurrentTrain: epoch  2, batch    23 | loss: 6.1079760CurrentTrain: epoch  2, batch    24 | loss: 4.9485345CurrentTrain: epoch  2, batch    25 | loss: 5.5273910CurrentTrain: epoch  2, batch    26 | loss: 6.2750301CurrentTrain: epoch  2, batch    27 | loss: 5.6898847CurrentTrain: epoch  2, batch    28 | loss: 5.6784420CurrentTrain: epoch  2, batch    29 | loss: 6.3279176CurrentTrain: epoch  2, batch    30 | loss: 5.4411030CurrentTrain: epoch  2, batch    31 | loss: 5.2049665CurrentTrain: epoch  2, batch    32 | loss: 5.3922606CurrentTrain: epoch  2, batch    33 | loss: 5.4268007CurrentTrain: epoch  2, batch    34 | loss: 5.7881765CurrentTrain: epoch  2, batch    35 | loss: 5.3703823CurrentTrain: epoch  2, batch    36 | loss: 6.1690035CurrentTrain: epoch  2, batch    37 | loss: 5.0955791CurrentTrain: epoch  2, batch    38 | loss: 5.1445856CurrentTrain: epoch  2, batch    39 | loss: 5.5533667CurrentTrain: epoch  2, batch    40 | loss: 5.8558764CurrentTrain: epoch  2, batch    41 | loss: 6.9648967CurrentTrain: epoch  2, batch    42 | loss: 5.5899129CurrentTrain: epoch  2, batch    43 | loss: 5.0020571CurrentTrain: epoch  2, batch    44 | loss: 5.5334225CurrentTrain: epoch  2, batch    45 | loss: 5.4627819CurrentTrain: epoch  2, batch    46 | loss: 6.2493019CurrentTrain: epoch  2, batch    47 | loss: 5.6292262CurrentTrain: epoch  2, batch    48 | loss: 5.4132175CurrentTrain: epoch  2, batch    49 | loss: 4.9476147CurrentTrain: epoch  2, batch    50 | loss: 4.7995386CurrentTrain: epoch  2, batch    51 | loss: 5.7640858CurrentTrain: epoch  2, batch    52 | loss: 4.8763161CurrentTrain: epoch  2, batch    53 | loss: 5.1424475CurrentTrain: epoch  2, batch    54 | loss: 5.4028873CurrentTrain: epoch  2, batch    55 | loss: 5.2489386CurrentTrain: epoch  2, batch    56 | loss: 5.4193230CurrentTrain: epoch  2, batch    57 | loss: 5.8169017CurrentTrain: epoch  2, batch    58 | loss: 5.8886919CurrentTrain: epoch  2, batch    59 | loss: 4.8617287CurrentTrain: epoch  2, batch    60 | loss: 4.9697599CurrentTrain: epoch  2, batch    61 | loss: 4.9261889CurrentTrain: epoch  2, batch    62 | loss: 5.0503998CurrentTrain: epoch  3, batch     0 | loss: 4.9948177CurrentTrain: epoch  3, batch     1 | loss: 5.0825815CurrentTrain: epoch  3, batch     2 | loss: 5.5607195CurrentTrain: epoch  3, batch     3 | loss: 4.9236155CurrentTrain: epoch  3, batch     4 | loss: 5.2076778CurrentTrain: epoch  3, batch     5 | loss: 5.1386418CurrentTrain: epoch  3, batch     6 | loss: 5.0110402CurrentTrain: epoch  3, batch     7 | loss: 5.0020962CurrentTrain: epoch  3, batch     8 | loss: 5.0791492CurrentTrain: epoch  3, batch     9 | loss: 5.0873451CurrentTrain: epoch  3, batch    10 | loss: 4.7276268CurrentTrain: epoch  3, batch    11 | loss: 4.7285194CurrentTrain: epoch  3, batch    12 | loss: 4.9439225CurrentTrain: epoch  3, batch    13 | loss: 5.1084371CurrentTrain: epoch  3, batch    14 | loss: 4.9786081CurrentTrain: epoch  3, batch    15 | loss: 4.7954531CurrentTrain: epoch  3, batch    16 | loss: 5.2179365CurrentTrain: epoch  3, batch    17 | loss: 5.0491810CurrentTrain: epoch  3, batch    18 | loss: 4.9271874CurrentTrain: epoch  3, batch    19 | loss: 5.0173426CurrentTrain: epoch  3, batch    20 | loss: 5.3259902CurrentTrain: epoch  3, batch    21 | loss: 5.2878609CurrentTrain: epoch  3, batch    22 | loss: 5.0036802CurrentTrain: epoch  3, batch    23 | loss: 5.1917419CurrentTrain: epoch  3, batch    24 | loss: 5.0586901CurrentTrain: epoch  3, batch    25 | loss: 4.7634659CurrentTrain: epoch  3, batch    26 | loss: 4.6876240CurrentTrain: epoch  3, batch    27 | loss: 4.9121952CurrentTrain: epoch  3, batch    28 | loss: 4.5586753CurrentTrain: epoch  3, batch    29 | loss: 4.8699903CurrentTrain: epoch  3, batch    30 | loss: 4.9433761CurrentTrain: epoch  3, batch    31 | loss: 5.1525326CurrentTrain: epoch  3, batch    32 | loss: 4.8916950CurrentTrain: epoch  3, batch    33 | loss: 4.8760595CurrentTrain: epoch  3, batch    34 | loss: 4.7931852CurrentTrain: epoch  3, batch    35 | loss: 4.8014483CurrentTrain: epoch  3, batch    36 | loss: 5.1738310CurrentTrain: epoch  3, batch    37 | loss: 5.2141194CurrentTrain: epoch  3, batch    38 | loss: 4.7733121CurrentTrain: epoch  3, batch    39 | loss: 4.5867972CurrentTrain: epoch  3, batch    40 | loss: 5.0039201CurrentTrain: epoch  3, batch    41 | loss: 4.7418289CurrentTrain: epoch  3, batch    42 | loss: 4.7487850CurrentTrain: epoch  3, batch    43 | loss: 4.5379601CurrentTrain: epoch  3, batch    44 | loss: 4.6866875CurrentTrain: epoch  3, batch    45 | loss: 4.6387997CurrentTrain: epoch  3, batch    46 | loss: 5.2530069CurrentTrain: epoch  3, batch    47 | loss: 4.5636635CurrentTrain: epoch  3, batch    48 | loss: 4.6188374CurrentTrain: epoch  3, batch    49 | loss: 4.7465272CurrentTrain: epoch  3, batch    50 | loss: 5.3219805CurrentTrain: epoch  3, batch    51 | loss: 5.1843405CurrentTrain: epoch  3, batch    52 | loss: 5.1141205CurrentTrain: epoch  3, batch    53 | loss: 4.4478650CurrentTrain: epoch  3, batch    54 | loss: 4.5227785CurrentTrain: epoch  3, batch    55 | loss: 5.5917630CurrentTrain: epoch  3, batch    56 | loss: 5.2839899CurrentTrain: epoch  3, batch    57 | loss: 4.5202494CurrentTrain: epoch  3, batch    58 | loss: 5.1288662CurrentTrain: epoch  3, batch    59 | loss: 5.3555570CurrentTrain: epoch  3, batch    60 | loss: 5.2099519CurrentTrain: epoch  3, batch    61 | loss: 4.7291341CurrentTrain: epoch  3, batch    62 | loss: 4.6376042CurrentTrain: epoch  4, batch     0 | loss: 5.0759282CurrentTrain: epoch  4, batch     1 | loss: 4.6325207CurrentTrain: epoch  4, batch     2 | loss: 4.5964909CurrentTrain: epoch  4, batch     3 | loss: 4.9926801CurrentTrain: epoch  4, batch     4 | loss: 4.7738028CurrentTrain: epoch  4, batch     5 | loss: 4.8640718CurrentTrain: epoch  4, batch     6 | loss: 4.7412024CurrentTrain: epoch  4, batch     7 | loss: 5.0426292CurrentTrain: epoch  4, batch     8 | loss: 4.7642326CurrentTrain: epoch  4, batch     9 | loss: 4.4234996CurrentTrain: epoch  4, batch    10 | loss: 5.0816121CurrentTrain: epoch  4, batch    11 | loss: 4.6856022CurrentTrain: epoch  4, batch    12 | loss: 4.6196151CurrentTrain: epoch  4, batch    13 | loss: 4.6181283CurrentTrain: epoch  4, batch    14 | loss: 4.5474296CurrentTrain: epoch  4, batch    15 | loss: 5.1649489CurrentTrain: epoch  4, batch    16 | loss: 4.6105123CurrentTrain: epoch  4, batch    17 | loss: 4.4597106CurrentTrain: epoch  4, batch    18 | loss: 4.4525709CurrentTrain: epoch  4, batch    19 | loss: 4.4163680CurrentTrain: epoch  4, batch    20 | loss: 4.4582434CurrentTrain: epoch  4, batch    21 | loss: 4.7870817CurrentTrain: epoch  4, batch    22 | loss: 4.5401707CurrentTrain: epoch  4, batch    23 | loss: 4.4898634CurrentTrain: epoch  4, batch    24 | loss: 5.2260747CurrentTrain: epoch  4, batch    25 | loss: 4.5005431CurrentTrain: epoch  4, batch    26 | loss: 4.5897098CurrentTrain: epoch  4, batch    27 | loss: 4.4372401CurrentTrain: epoch  4, batch    28 | loss: 5.0824890CurrentTrain: epoch  4, batch    29 | loss: 4.6287098CurrentTrain: epoch  4, batch    30 | loss: 4.4467182CurrentTrain: epoch  4, batch    31 | loss: 4.6672735CurrentTrain: epoch  4, batch    32 | loss: 4.4562192CurrentTrain: epoch  4, batch    33 | loss: 4.6198683CurrentTrain: epoch  4, batch    34 | loss: 4.5052643CurrentTrain: epoch  4, batch    35 | loss: 4.6818438CurrentTrain: epoch  4, batch    36 | loss: 4.6621332CurrentTrain: epoch  4, batch    37 | loss: 4.6632218CurrentTrain: epoch  4, batch    38 | loss: 4.4698825CurrentTrain: epoch  4, batch    39 | loss: 4.5510807CurrentTrain: epoch  4, batch    40 | loss: 4.5763755CurrentTrain: epoch  4, batch    41 | loss: 4.4152102CurrentTrain: epoch  4, batch    42 | loss: 4.4536247CurrentTrain: epoch  4, batch    43 | loss: 4.3423667CurrentTrain: epoch  4, batch    44 | loss: 4.8182144CurrentTrain: epoch  4, batch    45 | loss: 4.4472561CurrentTrain: epoch  4, batch    46 | loss: 4.3491731CurrentTrain: epoch  4, batch    47 | loss: 4.6396637CurrentTrain: epoch  4, batch    48 | loss: 4.3675442CurrentTrain: epoch  4, batch    49 | loss: 4.5942602CurrentTrain: epoch  4, batch    50 | loss: 4.7700887CurrentTrain: epoch  4, batch    51 | loss: 4.4335060CurrentTrain: epoch  4, batch    52 | loss: 4.4661374CurrentTrain: epoch  4, batch    53 | loss: 4.6874180CurrentTrain: epoch  4, batch    54 | loss: 4.2267542CurrentTrain: epoch  4, batch    55 | loss: 4.2201023CurrentTrain: epoch  4, batch    56 | loss: 4.4559407CurrentTrain: epoch  4, batch    57 | loss: 4.5189643CurrentTrain: epoch  4, batch    58 | loss: 4.2846003CurrentTrain: epoch  4, batch    59 | loss: 4.4134064CurrentTrain: epoch  4, batch    60 | loss: 4.3395519CurrentTrain: epoch  4, batch    61 | loss: 4.6074691CurrentTrain: epoch  4, batch    62 | loss: 4.3532324CurrentTrain: epoch  5, batch     0 | loss: 4.4571056CurrentTrain: epoch  5, batch     1 | loss: 4.2424259CurrentTrain: epoch  5, batch     2 | loss: 5.0641127CurrentTrain: epoch  5, batch     3 | loss: 4.4671359CurrentTrain: epoch  5, batch     4 | loss: 4.1341844CurrentTrain: epoch  5, batch     5 | loss: 4.5238309CurrentTrain: epoch  5, batch     6 | loss: 4.2464004CurrentTrain: epoch  5, batch     7 | loss: 4.4511948CurrentTrain: epoch  5, batch     8 | loss: 4.2870526CurrentTrain: epoch  5, batch     9 | loss: 4.3160048CurrentTrain: epoch  5, batch    10 | loss: 4.4059343CurrentTrain: epoch  5, batch    11 | loss: 4.3098621CurrentTrain: epoch  5, batch    12 | loss: 4.4698563CurrentTrain: epoch  5, batch    13 | loss: 4.3944445CurrentTrain: epoch  5, batch    14 | loss: 4.4341235CurrentTrain: epoch  5, batch    15 | loss: 4.2498131CurrentTrain: epoch  5, batch    16 | loss: 4.2577314CurrentTrain: epoch  5, batch    17 | loss: 4.2794199CurrentTrain: epoch  5, batch    18 | loss: 4.4279079CurrentTrain: epoch  5, batch    19 | loss: 4.3466234CurrentTrain: epoch  5, batch    20 | loss: 4.4262304CurrentTrain: epoch  5, batch    21 | loss: 4.2697372CurrentTrain: epoch  5, batch    22 | loss: 4.4035196CurrentTrain: epoch  5, batch    23 | loss: 4.3512387CurrentTrain: epoch  5, batch    24 | loss: 4.3000183CurrentTrain: epoch  5, batch    25 | loss: 4.2528992CurrentTrain: epoch  5, batch    26 | loss: 4.7287602CurrentTrain: epoch  5, batch    27 | loss: 4.2324152CurrentTrain: epoch  5, batch    28 | loss: 4.4668865CurrentTrain: epoch  5, batch    29 | loss: 4.2648983CurrentTrain: epoch  5, batch    30 | loss: 4.2796917CurrentTrain: epoch  5, batch    31 | loss: 4.2825217CurrentTrain: epoch  5, batch    32 | loss: 4.3377972CurrentTrain: epoch  5, batch    33 | loss: 4.2199068CurrentTrain: epoch  5, batch    34 | loss: 4.3386788CurrentTrain: epoch  5, batch    35 | loss: 4.2357016CurrentTrain: epoch  5, batch    36 | loss: 4.3248005CurrentTrain: epoch  5, batch    37 | loss: 4.3772125CurrentTrain: epoch  5, batch    38 | loss: 4.3072829CurrentTrain: epoch  5, batch    39 | loss: 4.3126335CurrentTrain: epoch  5, batch    40 | loss: 4.2809033CurrentTrain: epoch  5, batch    41 | loss: 4.2654438CurrentTrain: epoch  5, batch    42 | loss: 4.3340712CurrentTrain: epoch  5, batch    43 | loss: 4.3404655CurrentTrain: epoch  5, batch    44 | loss: 4.3419447CurrentTrain: epoch  5, batch    45 | loss: 4.2422647CurrentTrain: epoch  5, batch    46 | loss: 4.2836151CurrentTrain: epoch  5, batch    47 | loss: 4.4196963CurrentTrain: epoch  5, batch    48 | loss: 4.3016038CurrentTrain: epoch  5, batch    49 | loss: 4.3089733CurrentTrain: epoch  5, batch    50 | loss: 4.8864679CurrentTrain: epoch  5, batch    51 | loss: 4.4520588CurrentTrain: epoch  5, batch    52 | loss: 4.3034835CurrentTrain: epoch  5, batch    53 | loss: 4.2551064CurrentTrain: epoch  5, batch    54 | loss: 4.2475071CurrentTrain: epoch  5, batch    55 | loss: 4.3480582CurrentTrain: epoch  5, batch    56 | loss: 4.2445345CurrentTrain: epoch  5, batch    57 | loss: 4.2341537CurrentTrain: epoch  5, batch    58 | loss: 4.2499914CurrentTrain: epoch  5, batch    59 | loss: 4.4684362CurrentTrain: epoch  5, batch    60 | loss: 4.3551092CurrentTrain: epoch  5, batch    61 | loss: 4.3036680CurrentTrain: epoch  5, batch    62 | loss: 4.2800217CurrentTrain: epoch  6, batch     0 | loss: 4.2720418CurrentTrain: epoch  6, batch     1 | loss: 4.1991777CurrentTrain: epoch  6, batch     2 | loss: 4.2791128CurrentTrain: epoch  6, batch     3 | loss: 4.2356272CurrentTrain: epoch  6, batch     4 | loss: 4.2406597CurrentTrain: epoch  6, batch     5 | loss: 4.3274841CurrentTrain: epoch  6, batch     6 | loss: 4.2565794CurrentTrain: epoch  6, batch     7 | loss: 4.2139502CurrentTrain: epoch  6, batch     8 | loss: 4.5351439CurrentTrain: epoch  6, batch     9 | loss: 4.3685870CurrentTrain: epoch  6, batch    10 | loss: 4.2017803CurrentTrain: epoch  6, batch    11 | loss: 4.2296538CurrentTrain: epoch  6, batch    12 | loss: 4.2939076CurrentTrain: epoch  6, batch    13 | loss: 4.1924210CurrentTrain: epoch  6, batch    14 | loss: 4.1740112CurrentTrain: epoch  6, batch    15 | loss: 4.1708121CurrentTrain: epoch  6, batch    16 | loss: 4.2226429CurrentTrain: epoch  6, batch    17 | loss: 4.2430639CurrentTrain: epoch  6, batch    18 | loss: 4.3179674CurrentTrain: epoch  6, batch    19 | loss: 4.3374319CurrentTrain: epoch  6, batch    20 | loss: 4.2832808CurrentTrain: epoch  6, batch    21 | loss: 4.1990376CurrentTrain: epoch  6, batch    22 | loss: 4.2664900CurrentTrain: epoch  6, batch    23 | loss: 4.2306232CurrentTrain: epoch  6, batch    24 | loss: 4.3460817CurrentTrain: epoch  6, batch    25 | loss: 4.2330422CurrentTrain: epoch  6, batch    26 | loss: 4.1076288CurrentTrain: epoch  6, batch    27 | loss: 4.2566295CurrentTrain: epoch  6, batch    28 | loss: 4.2589951CurrentTrain: epoch  6, batch    29 | loss: 4.2381825CurrentTrain: epoch  6, batch    30 | loss: 4.1547060CurrentTrain: epoch  6, batch    31 | loss: 4.2725368CurrentTrain: epoch  6, batch    32 | loss: 4.1749444CurrentTrain: epoch  6, batch    33 | loss: 4.1550756CurrentTrain: epoch  6, batch    34 | loss: 4.2562561CurrentTrain: epoch  6, batch    35 | loss: 4.2230196CurrentTrain: epoch  6, batch    36 | loss: 4.2141838CurrentTrain: epoch  6, batch    37 | loss: 4.2293482CurrentTrain: epoch  6, batch    38 | loss: 4.1857958CurrentTrain: epoch  6, batch    39 | loss: 4.3050337CurrentTrain: epoch  6, batch    40 | loss: 4.1787591CurrentTrain: epoch  6, batch    41 | loss: 4.2092738CurrentTrain: epoch  6, batch    42 | loss: 4.1629872CurrentTrain: epoch  6, batch    43 | loss: 4.1549926CurrentTrain: epoch  6, batch    44 | loss: 4.1658783CurrentTrain: epoch  6, batch    45 | loss: 4.2239485CurrentTrain: epoch  6, batch    46 | loss: 4.1741509CurrentTrain: epoch  6, batch    47 | loss: 4.5419521CurrentTrain: epoch  6, batch    48 | loss: 4.2229886CurrentTrain: epoch  6, batch    49 | loss: 4.2552929CurrentTrain: epoch  6, batch    50 | loss: 4.2212114CurrentTrain: epoch  6, batch    51 | loss: 4.2000341CurrentTrain: epoch  6, batch    52 | loss: 4.3160257CurrentTrain: epoch  6, batch    53 | loss: 4.1869125CurrentTrain: epoch  6, batch    54 | loss: 4.0903850CurrentTrain: epoch  6, batch    55 | loss: 4.1684608CurrentTrain: epoch  6, batch    56 | loss: 4.1362185CurrentTrain: epoch  6, batch    57 | loss: 4.1669416CurrentTrain: epoch  6, batch    58 | loss: 4.1749196CurrentTrain: epoch  6, batch    59 | loss: 4.1695409CurrentTrain: epoch  6, batch    60 | loss: 4.2019954CurrentTrain: epoch  6, batch    61 | loss: 4.1016145CurrentTrain: epoch  6, batch    62 | loss: 4.2598152CurrentTrain: epoch  7, batch     0 | loss: 4.1635032CurrentTrain: epoch  7, batch     1 | loss: 4.1022692CurrentTrain: epoch  7, batch     2 | loss: 4.0949106CurrentTrain: epoch  7, batch     3 | loss: 4.1591301CurrentTrain: epoch  7, batch     4 | loss: 4.2628813CurrentTrain: epoch  7, batch     5 | loss: 4.1227722CurrentTrain: epoch  7, batch     6 | loss: 4.1075120CurrentTrain: epoch  7, batch     7 | loss: 4.3539286CurrentTrain: epoch  7, batch     8 | loss: 4.1618190CurrentTrain: epoch  7, batch     9 | loss: 4.1803975CurrentTrain: epoch  7, batch    10 | loss: 4.1595278CurrentTrain: epoch  7, batch    11 | loss: 4.2318192CurrentTrain: epoch  7, batch    12 | loss: 4.2058001CurrentTrain: epoch  7, batch    13 | loss: 4.2321463CurrentTrain: epoch  7, batch    14 | loss: 4.3173389CurrentTrain: epoch  7, batch    15 | loss: 4.0650253CurrentTrain: epoch  7, batch    16 | loss: 4.2113838CurrentTrain: epoch  7, batch    17 | loss: 4.1420870CurrentTrain: epoch  7, batch    18 | loss: 4.1413298CurrentTrain: epoch  7, batch    19 | loss: 4.1005697CurrentTrain: epoch  7, batch    20 | loss: 4.1320229CurrentTrain: epoch  7, batch    21 | loss: 4.2014122CurrentTrain: epoch  7, batch    22 | loss: 4.1355915CurrentTrain: epoch  7, batch    23 | loss: 4.2556591CurrentTrain: epoch  7, batch    24 | loss: 4.1225176CurrentTrain: epoch  7, batch    25 | loss: 4.1785431CurrentTrain: epoch  7, batch    26 | loss: 4.1218309CurrentTrain: epoch  7, batch    27 | loss: 4.1233244CurrentTrain: epoch  7, batch    28 | loss: 4.1197457CurrentTrain: epoch  7, batch    29 | loss: 4.4216843CurrentTrain: epoch  7, batch    30 | loss: 4.1405339CurrentTrain: epoch  7, batch    31 | loss: 4.2406378CurrentTrain: epoch  7, batch    32 | loss: 4.1545868CurrentTrain: epoch  7, batch    33 | loss: 4.1790142CurrentTrain: epoch  7, batch    34 | loss: 4.1205449CurrentTrain: epoch  7, batch    35 | loss: 4.1698160CurrentTrain: epoch  7, batch    36 | loss: 4.1480808CurrentTrain: epoch  7, batch    37 | loss: 4.1406317CurrentTrain: epoch  7, batch    38 | loss: 4.1466398CurrentTrain: epoch  7, batch    39 | loss: 4.1248927CurrentTrain: epoch  7, batch    40 | loss: 4.1265588CurrentTrain: epoch  7, batch    41 | loss: 4.1234055CurrentTrain: epoch  7, batch    42 | loss: 4.1066704CurrentTrain: epoch  7, batch    43 | loss: 4.1812892CurrentTrain: epoch  7, batch    44 | loss: 4.1525049CurrentTrain: epoch  7, batch    45 | loss: 4.1323547CurrentTrain: epoch  7, batch    46 | loss: 4.1547627CurrentTrain: epoch  7, batch    47 | loss: 4.1530461CurrentTrain: epoch  7, batch    48 | loss: 4.1816044CurrentTrain: epoch  7, batch    49 | loss: 4.1300125CurrentTrain: epoch  7, batch    50 | loss: 4.1056299CurrentTrain: epoch  7, batch    51 | loss: 4.1561537CurrentTrain: epoch  7, batch    52 | loss: 4.1471958CurrentTrain: epoch  7, batch    53 | loss: 4.0848346CurrentTrain: epoch  7, batch    54 | loss: 4.1178093CurrentTrain: epoch  7, batch    55 | loss: 4.0469341CurrentTrain: epoch  7, batch    56 | loss: 4.0815830CurrentTrain: epoch  7, batch    57 | loss: 4.1035233CurrentTrain: epoch  7, batch    58 | loss: 4.1167631CurrentTrain: epoch  7, batch    59 | loss: 4.0405841CurrentTrain: epoch  7, batch    60 | loss: 4.1422410CurrentTrain: epoch  7, batch    61 | loss: 4.0397348CurrentTrain: epoch  7, batch    62 | loss: 4.0313530CurrentTrain: epoch  8, batch     0 | loss: 4.1316104CurrentTrain: epoch  8, batch     1 | loss: 4.0825043CurrentTrain: epoch  8, batch     2 | loss: 4.0452061CurrentTrain: epoch  8, batch     3 | loss: 4.0825782CurrentTrain: epoch  8, batch     4 | loss: 4.0750256CurrentTrain: epoch  8, batch     5 | loss: 4.0915112CurrentTrain: epoch  8, batch     6 | loss: 4.0590816CurrentTrain: epoch  8, batch     7 | loss: 4.1182027CurrentTrain: epoch  8, batch     8 | loss: 4.0995126CurrentTrain: epoch  8, batch     9 | loss: 4.0998001CurrentTrain: epoch  8, batch    10 | loss: 4.0584774CurrentTrain: epoch  8, batch    11 | loss: 4.1037664CurrentTrain: epoch  8, batch    12 | loss: 4.1219187CurrentTrain: epoch  8, batch    13 | loss: 4.1360307CurrentTrain: epoch  8, batch    14 | loss: 4.0647407CurrentTrain: epoch  8, batch    15 | loss: 4.1220675CurrentTrain: epoch  8, batch    16 | loss: 4.0821524CurrentTrain: epoch  8, batch    17 | loss: 4.1391001CurrentTrain: epoch  8, batch    18 | loss: 4.0767560CurrentTrain: epoch  8, batch    19 | loss: 4.1028190CurrentTrain: epoch  8, batch    20 | loss: 4.0951581CurrentTrain: epoch  8, batch    21 | loss: 4.0632639CurrentTrain: epoch  8, batch    22 | loss: 4.0907526CurrentTrain: epoch  8, batch    23 | loss: 4.0771475CurrentTrain: epoch  8, batch    24 | loss: 4.0832071CurrentTrain: epoch  8, batch    25 | loss: 4.0108557CurrentTrain: epoch  8, batch    26 | loss: 4.0688777CurrentTrain: epoch  8, batch    27 | loss: 4.0762262CurrentTrain: epoch  8, batch    28 | loss: 4.2459984CurrentTrain: epoch  8, batch    29 | loss: 4.0514226CurrentTrain: epoch  8, batch    30 | loss: 4.0518160CurrentTrain: epoch  8, batch    31 | loss: 4.1129756CurrentTrain: epoch  8, batch    32 | loss: 4.0899734CurrentTrain: epoch  8, batch    33 | loss: 4.1171813CurrentTrain: epoch  8, batch    34 | loss: 4.1034131CurrentTrain: epoch  8, batch    35 | loss: 4.2010183CurrentTrain: epoch  8, batch    36 | loss: 4.1172671CurrentTrain: epoch  8, batch    37 | loss: 4.0896144CurrentTrain: epoch  8, batch    38 | loss: 4.0965948CurrentTrain: epoch  8, batch    39 | loss: 4.0949960CurrentTrain: epoch  8, batch    40 | loss: 4.1202040CurrentTrain: epoch  8, batch    41 | loss: 4.0496664CurrentTrain: epoch  8, batch    42 | loss: 4.0445080CurrentTrain: epoch  8, batch    43 | loss: 4.0748148CurrentTrain: epoch  8, batch    44 | loss: 4.1144266CurrentTrain: epoch  8, batch    45 | loss: 4.0635591CurrentTrain: epoch  8, batch    46 | loss: 4.0533867CurrentTrain: epoch  8, batch    47 | loss: 4.0891542CurrentTrain: epoch  8, batch    48 | loss: 4.0425463CurrentTrain: epoch  8, batch    49 | loss: 4.1114707CurrentTrain: epoch  8, batch    50 | loss: 4.0865216CurrentTrain: epoch  8, batch    51 | loss: 4.0412841CurrentTrain: epoch  8, batch    52 | loss: 4.0493135CurrentTrain: epoch  8, batch    53 | loss: 4.0631680CurrentTrain: epoch  8, batch    54 | loss: 4.0466695CurrentTrain: epoch  8, batch    55 | loss: 4.0796142CurrentTrain: epoch  8, batch    56 | loss: 4.0921183CurrentTrain: epoch  8, batch    57 | loss: 4.0554037CurrentTrain: epoch  8, batch    58 | loss: 4.1016102CurrentTrain: epoch  8, batch    59 | loss: 4.0487967CurrentTrain: epoch  8, batch    60 | loss: 4.0028381CurrentTrain: epoch  8, batch    61 | loss: 4.0567327CurrentTrain: epoch  8, batch    62 | loss: 3.9927249CurrentTrain: epoch  9, batch     0 | loss: 4.0130720CurrentTrain: epoch  9, batch     1 | loss: 4.0530291CurrentTrain: epoch  9, batch     2 | loss: 4.0513344CurrentTrain: epoch  9, batch     3 | loss: 4.0508747CurrentTrain: epoch  9, batch     4 | loss: 4.0397158CurrentTrain: epoch  9, batch     5 | loss: 4.0317926CurrentTrain: epoch  9, batch     6 | loss: 3.9929619CurrentTrain: epoch  9, batch     7 | loss: 4.0285692CurrentTrain: epoch  9, batch     8 | loss: 4.0636759CurrentTrain: epoch  9, batch     9 | loss: 4.0003819CurrentTrain: epoch  9, batch    10 | loss: 3.9724133CurrentTrain: epoch  9, batch    11 | loss: 4.0833149CurrentTrain: epoch  9, batch    12 | loss: 4.0532036CurrentTrain: epoch  9, batch    13 | loss: 4.0530224CurrentTrain: epoch  9, batch    14 | loss: 4.0781240CurrentTrain: epoch  9, batch    15 | loss: 4.0530348CurrentTrain: epoch  9, batch    16 | loss: 4.1908197CurrentTrain: epoch  9, batch    17 | loss: 4.0304136CurrentTrain: epoch  9, batch    18 | loss: 4.0465565CurrentTrain: epoch  9, batch    19 | loss: 4.0358896CurrentTrain: epoch  9, batch    20 | loss: 4.0615888CurrentTrain: epoch  9, batch    21 | loss: 4.0258074CurrentTrain: epoch  9, batch    22 | loss: 4.0654564CurrentTrain: epoch  9, batch    23 | loss: 4.0905175CurrentTrain: epoch  9, batch    24 | loss: 4.0417867CurrentTrain: epoch  9, batch    25 | loss: 4.0176339CurrentTrain: epoch  9, batch    26 | loss: 4.0670738CurrentTrain: epoch  9, batch    27 | loss: 4.0012951CurrentTrain: epoch  9, batch    28 | loss: 4.0989590CurrentTrain: epoch  9, batch    29 | loss: 4.0373125CurrentTrain: epoch  9, batch    30 | loss: 4.0025225CurrentTrain: epoch  9, batch    31 | loss: 4.0424123CurrentTrain: epoch  9, batch    32 | loss: 4.0336723CurrentTrain: epoch  9, batch    33 | loss: 4.0310779CurrentTrain: epoch  9, batch    34 | loss: 4.0668402CurrentTrain: epoch  9, batch    35 | loss: 4.0458035CurrentTrain: epoch  9, batch    36 | loss: 4.0505137CurrentTrain: epoch  9, batch    37 | loss: 4.0513282CurrentTrain: epoch  9, batch    38 | loss: 4.0374503CurrentTrain: epoch  9, batch    39 | loss: 4.0070848CurrentTrain: epoch  9, batch    40 | loss: 4.0510392CurrentTrain: epoch  9, batch    41 | loss: 4.0020933CurrentTrain: epoch  9, batch    42 | loss: 4.0660820CurrentTrain: epoch  9, batch    43 | loss: 4.0377054CurrentTrain: epoch  9, batch    44 | loss: 4.0631790CurrentTrain: epoch  9, batch    45 | loss: 4.0530930CurrentTrain: epoch  9, batch    46 | loss: 4.0937309CurrentTrain: epoch  9, batch    47 | loss: 4.0731440CurrentTrain: epoch  9, batch    48 | loss: 4.0514421CurrentTrain: epoch  9, batch    49 | loss: 4.0024595CurrentTrain: epoch  9, batch    50 | loss: 4.0437694CurrentTrain: epoch  9, batch    51 | loss: 4.0105677CurrentTrain: epoch  9, batch    52 | loss: 3.9971445CurrentTrain: epoch  9, batch    53 | loss: 4.0182171CurrentTrain: epoch  9, batch    54 | loss: 4.0409088CurrentTrain: epoch  9, batch    55 | loss: 4.0097752CurrentTrain: epoch  9, batch    56 | loss: 4.0250268CurrentTrain: epoch  9, batch    57 | loss: 4.0136180CurrentTrain: epoch  9, batch    58 | loss: 4.0049000CurrentTrain: epoch  9, batch    59 | loss: 3.9835577CurrentTrain: epoch  9, batch    60 | loss: 3.9979992CurrentTrain: epoch  9, batch    61 | loss: 4.0581532CurrentTrain: epoch  9, batch    62 | loss: 4.0323496
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the theme of the loss of identity in crowds in this painting can be seen as a precursor to expressionist painting , particularly the work of james ensor .
Head Entity: james ensor
Tail Entity: expressionist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The surrealist movement, characterized by its exploration of the unconscious mind, was significantly influenced by the works of Salvador Dalí.  
Head Entity: Salvador Dalí  
Tail Entity: surrealist  

Relation: movement  
Context: The emergence of the romantic movement in literature was marked by the profound impact of poets like William Wordsworth and Samuel Taylor Coleridge.  
Head Entity: William Wordsworth  
Tail Entity: romantic  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: faldu was born on 1 august 1957 to a family of farmer in kalavad , jamnagar district , bombay state ( present - day gujarat ) .
Head Entity: kalavad
Tail Entity: jamnagar district
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of Springfield is known for its beautiful parks and vibrant community, situated in the heart of Sangamon County, Illinois.  
Head Entity: Springfield  
Tail Entity: Sangamon County  

Relation: located in the administrative territorial entity  
Context: The historic town of Williamsburg is a popular tourist destination, located within the boundaries of James City County, Virginia.  
Head Entity: Williamsburg  
Tail Entity: James City County  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: he also went on a road trip from san antonio to oakland for the magnum project “ postcards from america ” with jim goldberg , alec soth , mikhael subotzky and susan meiselas .
Head Entity: jim goldberg
Tail Entity: magnum
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of dedication and hard work, she was finally recognized as a key member of the prestigious art collective known as the "Creative Minds."  
Head Entity: she  
Tail Entity: Creative Minds  

Relation: member of  
Context: The renowned guitarist joined the band in 2015, contributing to their unique sound and becoming an integral member of the musical group "The Echoes."  
Head Entity: guitarist  
Tail Entity: The Echoes  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County, and is a vital link for traffic crossing over the water.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a crucial transportation route between the UK and mainland Europe.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the koksoak river arises at the confluence of its two main tributaries , the rivière aux mélèzes to the west and the caniapiscau river to the south .
Head Entity: koksoak river
Tail Entity: caniapiscau river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River is fed by several tributaries, including the Yellowstone River, which flows in from the north.  
Head Entity: Missouri River  
Tail Entity: Yellowstone River  

Relation: tributary  
Context: The Amazon River has numerous tributaries, such as the Madeira River, which significantly contribute to its vast water system.  
Head Entity: Amazon River  
Tail Entity: Madeira River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: the previous parliament of new south wales was dissolved on 28 november 1874 by the governor , sir hercules robinson , on the advice of the premier , henry parkes .
Head Entity: sir hercules robinson
Tail Entity: governor
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: after serving as the mayor of new york city for two terms, bill de blasio announced his candidacy for the presidency in 2020.  
Head Entity: bill de blasio  
Tail Entity: mayor of new york city  

Relation: position held  
Context: during her tenure as the secretary of state, hillary clinton played a crucial role in shaping u.s. foreign policy.  
Head Entity: hillary clinton  
Tail Entity: secretary of state  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: " jedi outcast " was developed by raven software and powered by the i d tech 3 game engine .
Head Entity: jedi outcast
Tail Entity: raven software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and has received numerous awards for its storytelling and gameplay.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Minecraft" was developed by Mojang Studios and has become one of the best-selling video games of all time.  
Head Entity: Minecraft  
Tail Entity: Mojang Studios  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: third power was an american psychedelic hard rock band formed in 1969 in detroit , michigan .
Head Entity: third power
Tail Entity: detroit
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the united nations was established in 1945 in san francisco, california, to promote international cooperation.  
Head Entity: united nations  
Tail Entity: san francisco  

Relation: location of formation  
Context: the famous rock band the beatles originated in liverpool, england, in the early 1960s.  
Head Entity: the beatles  
Tail Entity: liverpool  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: " shining time station " won a number of awards and significantly increased the popularity of the " thomas " media franchise in the united states .
Head Entity: shining time station
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish "sushi" is traditionally associated with Japan and has gained immense popularity worldwide.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic brand "Guinness" is known for its rich stout beer, which originated in Ireland and is now enjoyed globally.  
Head Entity: Guinness  
Tail Entity: Ireland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.57%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.53%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.47%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.44%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.74%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.69%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.79%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.54%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.50%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.47%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.55%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.52%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
CurrentTrain: epoch  0, batch     0 | loss: 7.3694386CurrentTrain: epoch  0, batch     1 | loss: 6.7070746CurrentTrain: epoch  0, batch     2 | loss: 7.6032915CurrentTrain: epoch  0, batch     3 | loss: 8.4316893CurrentTrain: epoch  1, batch     0 | loss: 6.7442570CurrentTrain: epoch  1, batch     1 | loss: 5.4952831CurrentTrain: epoch  1, batch     2 | loss: 6.6935387CurrentTrain: epoch  1, batch     3 | loss: 4.8020134CurrentTrain: epoch  2, batch     0 | loss: 5.1830568CurrentTrain: epoch  2, batch     1 | loss: 6.1352649CurrentTrain: epoch  2, batch     2 | loss: 4.9283257CurrentTrain: epoch  2, batch     3 | loss: 5.5066252CurrentTrain: epoch  3, batch     0 | loss: 5.2165546CurrentTrain: epoch  3, batch     1 | loss: 4.4347496CurrentTrain: epoch  3, batch     2 | loss: 4.6802387CurrentTrain: epoch  3, batch     3 | loss: 4.4935536CurrentTrain: epoch  4, batch     0 | loss: 4.4175701CurrentTrain: epoch  4, batch     1 | loss: 5.2495775CurrentTrain: epoch  4, batch     2 | loss: 3.6361058CurrentTrain: epoch  4, batch     3 | loss: 7.7067423CurrentTrain: epoch  5, batch     0 | loss: 5.1366091CurrentTrain: epoch  5, batch     1 | loss: 4.6313686CurrentTrain: epoch  5, batch     2 | loss: 3.2075434CurrentTrain: epoch  5, batch     3 | loss: 3.3141737CurrentTrain: epoch  6, batch     0 | loss: 4.8466721CurrentTrain: epoch  6, batch     1 | loss: 4.0558701CurrentTrain: epoch  6, batch     2 | loss: 3.5185068CurrentTrain: epoch  6, batch     3 | loss: 3.5964632CurrentTrain: epoch  7, batch     0 | loss: 3.7493758CurrentTrain: epoch  7, batch     1 | loss: 4.6836653CurrentTrain: epoch  7, batch     2 | loss: 3.5130060CurrentTrain: epoch  7, batch     3 | loss: 1.8794565CurrentTrain: epoch  8, batch     0 | loss: 3.4294376CurrentTrain: epoch  8, batch     1 | loss: 4.4282846CurrentTrain: epoch  8, batch     2 | loss: 3.9174304CurrentTrain: epoch  8, batch     3 | loss: 2.6134374CurrentTrain: epoch  9, batch     0 | loss: 3.7393100CurrentTrain: epoch  9, batch     1 | loss: 3.6976061CurrentTrain: epoch  9, batch     2 | loss: 3.0724115CurrentTrain: epoch  9, batch     3 | loss: 4.8095636
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: in 2004 the catalan government gave him the george cross .
Head Entity: george cross
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union will affect all member states.  
Head Entity: environmental regulations  
Tail Entity: European Union  

Relation: applies to jurisdiction  
Context: The Supreme Court's ruling has significant implications for the state of California's water rights.  
Head Entity: Supreme Court  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " tenth avenue freeze - out " is the second song on bruce springsteen 's breakthrough album " born to run " , released in 1975 .
Head Entity: tenth avenue freeze - out
Tail Entity: bruce springsteen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a popular song by Ed Sheeran that topped charts worldwide after its release in 2017.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The Beatles are widely regarded as one of the most influential bands in the history of music, with hits like "Hey Jude" and "Let It Be."  
Head Entity: The Beatles  
Tail Entity: Hey Jude  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: norway was represented in the eurovision song contest 2005 by the song " in my dreams " performed by wig wam .
Head Entity: eurovision song contest 2005
Tail Entity: wig wam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: the united states was represented in the 2020 summer olympics by a team of over 600 athletes competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: united states  

Relation: participant  
Context: the annual tech conference featured several startups, including a presentation by innovative solutions, which showcased their latest product.  
Head Entity: annual tech conference  
Tail Entity: innovative solutions  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: This game runs smoothly on multiple operating systems, such as Linux, Windows 10, and the latest version of macOS.  
Head Entity: game  
Tail Entity: Windows 10  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete was classified under the FIS Alpine Skiing World Cup, allowing him to compete at the highest level.  
Head Entity: FIS Alpine Skiing World Cup  
Tail Entity: highest level  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested campaign, the voters chose Sarah Johnson as their representative, marking a significant shift in the local political landscape.  
Head Entity: closely contested campaign  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring change and transparency to the office.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: the deaths of his brothers wenceslaus ii ( 1487 ) , casimir ii ( 1490 ) and władysław ( 1494 ) allowed jan v to reunificated the whole duchy of zator .
Head Entity: casimir ii
Tail Entity: wenceslaus ii
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: both elizabeth and her brother, charles, were known for their contributions to the arts and culture of their time.  
Head Entity: elizabeth  
Tail Entity: charles  

Relation: sibling  
Context: during the family reunion, it was heartwarming to see how much john and his sister, sarah, resembled each other in both looks and personality.  
Head Entity: john  
Tail Entity: sarah  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: sergeant johnson served in the united states marine corps during his military career.  
Head Entity: sergeant johnson  
Tail Entity: united states marine corps  

Relation: military branch  
Context: general smith was a prominent figure in the royal air force, leading several key missions.  
Head Entity: general smith  
Tail Entity: royal air force  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: mukesh is married to nita ambani and has two sons , anant and akash , and a daughter , isha .
Head Entity: nita ambani
Tail Entity: akash
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: elon musk has six children, including a son named x ae a-xii and a daughter named exa dark sidereal.  
Head Entity: elon musk  
Tail Entity: x ae a-xii  

Relation: child  
Context: j.k. rowling is the mother of three children, including a daughter named jessica and a son named david.  
Head Entity: j.k. rowling  
Tail Entity: jessica  
Mixup data size:  198
MixupTrain:  epoch  0, batch     0 | loss: 5.9355314MixupTrain:  epoch  0, batch     1 | loss: 5.5306009MixupTrain:  epoch  0, batch     2 | loss: 5.2679548MixupTrain:  epoch  0, batch     3 | loss: 5.2012347MixupTrain:  epoch  0, batch     4 | loss: 5.5376528MixupTrain:  epoch  0, batch     5 | loss: 5.1179619MixupTrain:  epoch  0, batch     6 | loss: 5.0884082MixupTrain:  epoch  0, batch     7 | loss: 4.8248226MixupTrain:  epoch  0, batch     8 | loss: 4.8956003MixupTrain:  epoch  0, batch     9 | loss: 5.3528902MixupTrain:  epoch  0, batch    10 | loss: 4.6948175MixupTrain:  epoch  0, batch    11 | loss: 4.1760400MixupTrain:  epoch  0, batch    12 | loss: 4.1998275
MemoryTrain:  epoch  0, batch     0 | loss: 3.6001592MemoryTrain:  epoch  0, batch     1 | loss: 4.1271038MemoryTrain:  epoch  0, batch     2 | loss: 3.7234547MemoryTrain:  epoch  0, batch     3 | loss: 3.7733407MemoryTrain:  epoch  1, batch     0 | loss: 4.4875793MemoryTrain:  epoch  1, batch     1 | loss: 2.9912770MemoryTrain:  epoch  1, batch     2 | loss: 2.9550042MemoryTrain:  epoch  1, batch     3 | loss: 2.6347470MemoryTrain:  epoch  2, batch     0 | loss: 3.1602347MemoryTrain:  epoch  2, batch     1 | loss: 2.6456809MemoryTrain:  epoch  2, batch     2 | loss: 3.2501147MemoryTrain:  epoch  2, batch     3 | loss: 2.1908104MemoryTrain:  epoch  3, batch     0 | loss: 2.5346055MemoryTrain:  epoch  3, batch     1 | loss: 2.4845324MemoryTrain:  epoch  3, batch     2 | loss: 2.5851185MemoryTrain:  epoch  3, batch     3 | loss: 2.8162181MemoryTrain:  epoch  4, batch     0 | loss: 2.1990554MemoryTrain:  epoch  4, batch     1 | loss: 2.2381592MemoryTrain:  epoch  4, batch     2 | loss: 2.4070172MemoryTrain:  epoch  4, batch     3 | loss: 2.1646814MemoryTrain:  epoch  5, batch     0 | loss: 2.3214650MemoryTrain:  epoch  5, batch     1 | loss: 2.1457717MemoryTrain:  epoch  5, batch     2 | loss: 2.2228107MemoryTrain:  epoch  5, batch     3 | loss: 1.7179887MemoryTrain:  epoch  6, batch     0 | loss: 1.7667313MemoryTrain:  epoch  6, batch     1 | loss: 2.3611777MemoryTrain:  epoch  6, batch     2 | loss: 2.1653287MemoryTrain:  epoch  6, batch     3 | loss: 1.6128218MemoryTrain:  epoch  7, batch     0 | loss: 1.9220781MemoryTrain:  epoch  7, batch     1 | loss: 1.9064593MemoryTrain:  epoch  7, batch     2 | loss: 1.9074304MemoryTrain:  epoch  7, batch     3 | loss: 1.7467722MemoryTrain:  epoch  8, batch     0 | loss: 1.8454732MemoryTrain:  epoch  8, batch     1 | loss: 1.9156175MemoryTrain:  epoch  8, batch     2 | loss: 1.7806199MemoryTrain:  epoch  8, batch     3 | loss: 1.4432094MemoryTrain:  epoch  9, batch     0 | loss: 1.4070200MemoryTrain:  epoch  9, batch     1 | loss: 1.7459643MemoryTrain:  epoch  9, batch     2 | loss: 2.0037787MemoryTrain:  epoch  9, batch     3 | loss: 1.3878812
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 76.70%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 77.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.39%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.65%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.58%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 83.09%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.87%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.26%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 85.47%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 85.52%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.90%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.08%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 85.87%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 85.24%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 85.29%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 85.08%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 85.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.29%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 85.46%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.61%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 85.53%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 85.68%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 85.60%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 84.65%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 83.73%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 82.84%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 81.88%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 80.84%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 79.54%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 78.57%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 91.12%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.56%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 91.37%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 90.76%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 90.89%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.11%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 91.20%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.38%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.65%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.68%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.88%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.07%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.26%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.43%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.90%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 93.90%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 93.47%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 93.21%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 92.55%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 92.19%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 92.22%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 91.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 91.59%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 91.70%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.74%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 91.89%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 91.81%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 91.95%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.98%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 92.11%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 92.14%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 91.67%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 91.11%   [EVAL] batch:   64 | acc: 62.50%,  total acc: 90.67%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 90.15%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 89.74%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 89.34%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 88.86%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 88.84%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 88.73%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 88.54%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 88.36%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 88.34%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 88.40%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 88.47%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 88.38%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 88.53%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 88.59%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 88.73%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 88.72%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 88.63%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 88.17%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 88.01%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 88.01%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 88.07%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 88.14%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 88.27%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 88.40%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 88.52%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 88.70%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 88.93%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 89.05%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 89.16%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 89.27%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 89.48%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 89.52%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 89.56%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 89.60%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 89.64%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 89.62%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 89.60%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 89.47%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 89.39%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 89.26%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 89.19%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 89.06%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 89.10%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 89.09%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 89.17%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 89.16%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 89.19%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 88.97%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 88.33%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 87.96%   [EVAL] batch:  121 | acc: 31.25%,  total acc: 87.50%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 86.89%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 86.24%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 85.70%   
cur_acc:  ['0.9484', '0.7857']
his_acc:  ['0.9484', '0.8570']
CurrentTrain: epoch  0, batch     0 | loss: 6.6967626CurrentTrain: epoch  0, batch     1 | loss: 5.7315121CurrentTrain: epoch  0, batch     2 | loss: 6.4139090CurrentTrain: epoch  0, batch     3 | loss: 8.9967718CurrentTrain: epoch  1, batch     0 | loss: 5.8049603CurrentTrain: epoch  1, batch     1 | loss: 5.5334959CurrentTrain: epoch  1, batch     2 | loss: 5.1056385CurrentTrain: epoch  1, batch     3 | loss: 6.3891993CurrentTrain: epoch  2, batch     0 | loss: 4.7338800CurrentTrain: epoch  2, batch     1 | loss: 4.6018147CurrentTrain: epoch  2, batch     2 | loss: 4.9165449CurrentTrain: epoch  2, batch     3 | loss: 4.9797297CurrentTrain: epoch  3, batch     0 | loss: 4.5994563CurrentTrain: epoch  3, batch     1 | loss: 4.8824682CurrentTrain: epoch  3, batch     2 | loss: 4.3649044CurrentTrain: epoch  3, batch     3 | loss: 2.7288756CurrentTrain: epoch  4, batch     0 | loss: 4.6377630CurrentTrain: epoch  4, batch     1 | loss: 4.4624176CurrentTrain: epoch  4, batch     2 | loss: 3.8195825CurrentTrain: epoch  4, batch     3 | loss: 3.6691337CurrentTrain: epoch  5, batch     0 | loss: 3.9198565CurrentTrain: epoch  5, batch     1 | loss: 3.9557157CurrentTrain: epoch  5, batch     2 | loss: 4.3897829CurrentTrain: epoch  5, batch     3 | loss: 2.0221233CurrentTrain: epoch  6, batch     0 | loss: 3.9354162CurrentTrain: epoch  6, batch     1 | loss: 3.7128549CurrentTrain: epoch  6, batch     2 | loss: 3.8070328CurrentTrain: epoch  6, batch     3 | loss: 2.5931287CurrentTrain: epoch  7, batch     0 | loss: 3.3333731CurrentTrain: epoch  7, batch     1 | loss: 3.3176374CurrentTrain: epoch  7, batch     2 | loss: 3.7406943CurrentTrain: epoch  7, batch     3 | loss: 4.4253473CurrentTrain: epoch  8, batch     0 | loss: 2.9860005CurrentTrain: epoch  8, batch     1 | loss: 3.8181529CurrentTrain: epoch  8, batch     2 | loss: 3.2896619CurrentTrain: epoch  8, batch     3 | loss: 4.4636889CurrentTrain: epoch  9, batch     0 | loss: 2.9793262CurrentTrain: epoch  9, batch     1 | loss: 3.3795419CurrentTrain: epoch  9, batch     2 | loss: 3.3236711CurrentTrain: epoch  9, batch     3 | loss: 3.9531841
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the book "sapiens: a brief history of humankind" explores the evolution of human societies and cultures.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: human societies  

Relation: main subject  
Context: the documentary "our planet" showcases the beauty of nature and the impact of climate change on wildlife.  
Head Entity: our planet  
Tail Entity: nature  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The Brazilian national team showcased their skills at the 2014 FIFA World Cup, where they faced off against Germany in the semi-finals.  
Head Entity: 2014 FIFA World Cup  
Tail Entity: Germany  

Relation: participating team  
Context: During the 2021 Tokyo Olympics, the United States women's soccer team competed fiercely, ultimately playing against Canada in the semi-finals.  
Head Entity: 2021 Tokyo Olympics  
Tail Entity: Canada  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Judas Iscariot, who is depicted as part of the group gathered around Jesus.  
Head Entity: The Last Supper  
Tail Entity: Judas Iscariot  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with several components, including a powerful camera system that is part of its advanced features.  
Head Entity: Galaxy S21  
Tail Entity: camera system  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: on march 2015 , cube entertainment launched the multi - national girl group clc including original members seunghee , yujin , seungyeon , sorn and yeeun .
Head Entity: clc
Tail Entity: girl group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone brand Apple released its latest model, the iPhone 14, which features advanced technology and improved camera capabilities.  
Head Entity: iPhone 14  
Tail Entity: smartphone  

Relation: instance of  
Context: The famous painting "Starry Night" was created by the artist Vincent van Gogh and is considered one of the most recognized works in Western art.  
Head Entity: Starry Night  
Tail Entity: painting  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: abus valley ( ) is an ice - free valley southeast of turnstile ridge at the north end of britannia range .
Head Entity: turnstile ridge
Tail Entity: britannia range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada ( ) is a mountain range in the western united states, primarily in california, and is known for its stunning landscapes and diverse ecosystems, including the famous yosemite valley, which is part of the sierra nevada.  
Head Entity: sierra nevada  
Tail Entity: yosemite valley  

Relation: mountain range  
Context: the appalachian mountains ( ) extend from the canadian province of newfoundland and labrador down to alabama, encompassing a variety of subranges, including the great smoky mountains, which are renowned for their biodiversity and scenic beauty.  
Head Entity: appalachian mountains  
Tail Entity: great smoky mountains  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: anders matthesen has also released several cds with his radio material , in addition to the animated movie " terkel in trouble " , based on one of these .
Head Entity: terkel in trouble
Tail Entity: anders matthesen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the acclaimed film "inception" features a complex narrative that was intricately crafted by its talented screenwriter, who is known for his unique storytelling style.  
Head Entity: inception  
Tail Entity: christopher nolan  

Relation: screenwriter  
Context: the beloved animated feature "finding nemo" was brought to life through the creative vision of its screenwriter, whose work has captivated audiences of all ages.  
Head Entity: finding nemo  
Tail Entity: andrew stanton  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the earliest written mention of the order is found in " tirant lo blanch " , a chivalric romance written in catalan mainly by valencian joanot martorell .
Head Entity: tirant lo blanch
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The famous novel "One Hundred Years of Solitude" was originally published in Spanish and has been translated into many languages.  
Head Entity: One Hundred Years of Solitude  
Tail Entity: Spanish  

Relation: language of work or name  
Context: The animated series "Naruto" is primarily produced in Japanese and has gained a massive following worldwide, leading to numerous translations.  
Head Entity: Naruto  
Tail Entity: Japanese  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as a central place of worship for the roman catholic community in paris.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic  

Relation: religion  
Context: the dalai lama is a prominent figure in the practice of tibetan buddhism, advocating for peace and compassion around the world.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
Mixup data size:  258
MixupTrain:  epoch  0, batch     0 | loss: 3.8236686MixupTrain:  epoch  0, batch     1 | loss: 3.6801270MixupTrain:  epoch  0, batch     2 | loss: 3.3069651MixupTrain:  epoch  0, batch     3 | loss: 3.0814736MixupTrain:  epoch  0, batch     4 | loss: 3.9113418MixupTrain:  epoch  0, batch     5 | loss: 3.1796914MixupTrain:  epoch  0, batch     6 | loss: 3.2581064MixupTrain:  epoch  0, batch     7 | loss: 3.2152844MixupTrain:  epoch  0, batch     8 | loss: 3.4195589MixupTrain:  epoch  0, batch     9 | loss: 2.8840843MixupTrain:  epoch  0, batch    10 | loss: 3.2340440MixupTrain:  epoch  0, batch    11 | loss: 3.3337132MixupTrain:  epoch  0, batch    12 | loss: 3.5920300MixupTrain:  epoch  0, batch    13 | loss: 3.5406024MixupTrain:  epoch  0, batch    14 | loss: 3.0718346MixupTrain:  epoch  0, batch    15 | loss: 2.8024620MixupTrain:  epoch  0, batch    16 | loss: 3.1927509
MemoryTrain:  epoch  0, batch     0 | loss: 2.9101610MemoryTrain:  epoch  0, batch     1 | loss: 2.2173052MemoryTrain:  epoch  0, batch     2 | loss: 4.0090170MemoryTrain:  epoch  0, batch     3 | loss: 3.0990887MemoryTrain:  epoch  0, batch     4 | loss: 3.9208226MemoryTrain:  epoch  0, batch     5 | loss: 3.9239755MemoryTrain:  epoch  1, batch     0 | loss: 2.7148166MemoryTrain:  epoch  1, batch     1 | loss: 2.4589818MemoryTrain:  epoch  1, batch     2 | loss: 2.5218163MemoryTrain:  epoch  1, batch     3 | loss: 2.9050198MemoryTrain:  epoch  1, batch     4 | loss: 2.8087964MemoryTrain:  epoch  1, batch     5 | loss: 2.6705859MemoryTrain:  epoch  2, batch     0 | loss: 3.0426743MemoryTrain:  epoch  2, batch     1 | loss: 2.3586676MemoryTrain:  epoch  2, batch     2 | loss: 2.2319312MemoryTrain:  epoch  2, batch     3 | loss: 1.6166021MemoryTrain:  epoch  2, batch     4 | loss: 2.6460013MemoryTrain:  epoch  2, batch     5 | loss: 1.7043549MemoryTrain:  epoch  3, batch     0 | loss: 2.6297107MemoryTrain:  epoch  3, batch     1 | loss: 2.0247622MemoryTrain:  epoch  3, batch     2 | loss: 2.4749391MemoryTrain:  epoch  3, batch     3 | loss: 1.9719197MemoryTrain:  epoch  3, batch     4 | loss: 1.7993103MemoryTrain:  epoch  3, batch     5 | loss: 1.8525689MemoryTrain:  epoch  4, batch     0 | loss: 1.9763864MemoryTrain:  epoch  4, batch     1 | loss: 1.6626320MemoryTrain:  epoch  4, batch     2 | loss: 2.0208929MemoryTrain:  epoch  4, batch     3 | loss: 2.0289080MemoryTrain:  epoch  4, batch     4 | loss: 1.8372618MemoryTrain:  epoch  4, batch     5 | loss: 2.2947724MemoryTrain:  epoch  5, batch     0 | loss: 2.0657966MemoryTrain:  epoch  5, batch     1 | loss: 1.8650738MemoryTrain:  epoch  5, batch     2 | loss: 2.2211790MemoryTrain:  epoch  5, batch     3 | loss: 1.7990997MemoryTrain:  epoch  5, batch     4 | loss: 1.5545278MemoryTrain:  epoch  5, batch     5 | loss: 1.8565696MemoryTrain:  epoch  6, batch     0 | loss: 1.6548477MemoryTrain:  epoch  6, batch     1 | loss: 1.7002091MemoryTrain:  epoch  6, batch     2 | loss: 1.8656862MemoryTrain:  epoch  6, batch     3 | loss: 1.7954526MemoryTrain:  epoch  6, batch     4 | loss: 1.6353197MemoryTrain:  epoch  6, batch     5 | loss: 1.8749335MemoryTrain:  epoch  7, batch     0 | loss: 1.6385926MemoryTrain:  epoch  7, batch     1 | loss: 1.4749150MemoryTrain:  epoch  7, batch     2 | loss: 1.6915734MemoryTrain:  epoch  7, batch     3 | loss: 1.8209516MemoryTrain:  epoch  7, batch     4 | loss: 1.8294790MemoryTrain:  epoch  7, batch     5 | loss: 1.3939816MemoryTrain:  epoch  8, batch     0 | loss: 1.4212757MemoryTrain:  epoch  8, batch     1 | loss: 1.5593224MemoryTrain:  epoch  8, batch     2 | loss: 1.7502551MemoryTrain:  epoch  8, batch     3 | loss: 1.5353408MemoryTrain:  epoch  8, batch     4 | loss: 1.6884248MemoryTrain:  epoch  8, batch     5 | loss: 1.3666731MemoryTrain:  epoch  9, batch     0 | loss: 1.5806198MemoryTrain:  epoch  9, batch     1 | loss: 1.8607631MemoryTrain:  epoch  9, batch     2 | loss: 1.5076890MemoryTrain:  epoch  9, batch     3 | loss: 1.3684437MemoryTrain:  epoch  9, batch     4 | loss: 1.3154244MemoryTrain:  epoch  9, batch     5 | loss: 1.5150763
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 31.25%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 64.14%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 31.25%,  total acc: 70.37%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 69.64%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 69.61%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 68.54%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 67.54%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 68.16%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 68.94%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 72.13%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 72.86%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 74.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 74.70%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 75.87%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 76.42%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 76.53%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 76.63%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 76.86%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 76.82%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 76.91%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 77.25%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 77.21%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 77.04%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 77.24%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 77.31%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 77.61%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 77.79%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 77.85%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 78.02%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 78.28%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 78.54%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 78.89%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 79.03%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 78.57%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 84.21%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 84.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.88%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.38%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.64%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.89%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.26%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.60%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.36%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.64%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.16%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 90.24%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.48%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 90.41%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 90.00%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 89.40%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 88.83%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 88.28%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 88.52%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 87.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.99%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 88.10%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 88.43%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 88.30%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 88.60%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 88.58%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 88.56%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 88.65%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 88.83%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 88.81%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 88.59%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 88.18%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 87.88%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 87.88%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 87.78%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 87.68%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 87.59%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 87.24%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 86.98%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 86.99%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 87.08%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 87.17%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 87.26%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 87.18%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 87.26%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 87.42%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 87.27%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 86.75%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 86.16%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 85.66%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 85.10%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 84.84%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 84.87%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 85.14%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 85.57%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 85.72%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 86.02%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 86.44%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 86.51%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 86.58%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 86.65%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 86.72%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 86.85%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 86.91%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 86.92%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 86.75%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 86.70%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 86.70%   [EVAL] batch:  110 | acc: 87.50%,  total acc: 86.71%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 86.56%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 86.51%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 86.63%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 86.58%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 86.59%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 86.60%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 86.45%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 85.83%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 85.43%   [EVAL] batch:  121 | acc: 31.25%,  total acc: 84.99%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 84.40%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 83.77%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 83.20%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 83.13%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 82.97%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 82.91%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 82.85%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 82.79%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 82.78%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 82.86%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 82.89%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 82.88%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 82.92%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 82.95%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 83.03%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 82.93%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 82.46%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 82.10%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 81.65%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 81.29%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 80.99%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 80.69%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 81.04%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 81.17%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 81.29%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 81.42%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 80.92%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 80.72%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 80.64%   [EVAL] batch:  154 | acc: 37.50%,  total acc: 80.36%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 80.09%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 80.14%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 80.22%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 80.35%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 80.55%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 80.67%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 80.79%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 80.91%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 80.98%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 81.10%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 81.21%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 81.32%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 81.43%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 81.43%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 81.43%   [EVAL] batch:  171 | acc: 87.50%,  total acc: 81.47%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 81.43%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 81.43%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 81.50%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 81.46%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 81.39%   [EVAL] batch:  177 | acc: 87.50%,  total acc: 81.43%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 81.42%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 81.49%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 81.53%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 81.52%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 81.62%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 81.69%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 81.82%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 81.65%   
cur_acc:  ['0.9484', '0.7857', '0.7857']
his_acc:  ['0.9484', '0.8570', '0.8165']
CurrentTrain: epoch  0, batch     0 | loss: 5.2664919CurrentTrain: epoch  0, batch     1 | loss: 5.3224192CurrentTrain: epoch  0, batch     2 | loss: 7.1927981CurrentTrain: epoch  0, batch     3 | loss: 9.0629234CurrentTrain: epoch  1, batch     0 | loss: 5.6744862CurrentTrain: epoch  1, batch     1 | loss: 5.2127957CurrentTrain: epoch  1, batch     2 | loss: 4.5991821CurrentTrain: epoch  1, batch     3 | loss: 6.1801429CurrentTrain: epoch  2, batch     0 | loss: 4.7021341CurrentTrain: epoch  2, batch     1 | loss: 4.4416666CurrentTrain: epoch  2, batch     2 | loss: 4.9515443CurrentTrain: epoch  2, batch     3 | loss: 4.7837687CurrentTrain: epoch  3, batch     0 | loss: 5.0408635CurrentTrain: epoch  3, batch     1 | loss: 4.2079754CurrentTrain: epoch  3, batch     2 | loss: 3.8568072CurrentTrain: epoch  3, batch     3 | loss: 2.1037350CurrentTrain: epoch  4, batch     0 | loss: 4.1351786CurrentTrain: epoch  4, batch     1 | loss: 4.1607051CurrentTrain: epoch  4, batch     2 | loss: 3.7024760CurrentTrain: epoch  4, batch     3 | loss: 3.9301691CurrentTrain: epoch  5, batch     0 | loss: 4.0180740CurrentTrain: epoch  5, batch     1 | loss: 3.8222966CurrentTrain: epoch  5, batch     2 | loss: 3.5938706CurrentTrain: epoch  5, batch     3 | loss: 2.5212407CurrentTrain: epoch  6, batch     0 | loss: 3.2081914CurrentTrain: epoch  6, batch     1 | loss: 3.5851812CurrentTrain: epoch  6, batch     2 | loss: 3.5887396CurrentTrain: epoch  6, batch     3 | loss: 2.3361521CurrentTrain: epoch  7, batch     0 | loss: 3.4069324CurrentTrain: epoch  7, batch     1 | loss: 3.0580647CurrentTrain: epoch  7, batch     2 | loss: 2.9326239CurrentTrain: epoch  7, batch     3 | loss: 3.7905083CurrentTrain: epoch  8, batch     0 | loss: 3.4549305CurrentTrain: epoch  8, batch     1 | loss: 2.8651152CurrentTrain: epoch  8, batch     2 | loss: 2.8288031CurrentTrain: epoch  8, batch     3 | loss: 3.4673395CurrentTrain: epoch  9, batch     0 | loss: 2.9767659CurrentTrain: epoch  9, batch     1 | loss: 3.0313144CurrentTrain: epoch  9, batch     2 | loss: 2.9092815CurrentTrain: epoch  9, batch     3 | loss: 1.8360753
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael, the youngest son, had always looked up to his father, robert, for guidance and support.  
Head Entity: michael  
Tail Entity: robert  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: on november 2 , 2016 msnbc 's " rachel maddow show " featured an expose on yahweh ben yahweh and his followers connection to the 2016 donald trump presidential campaign .
Head Entity: rachel maddow show
Tail Entity: msnbc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series "breaking bad" premiered on january 20, 2008, on the amc network, quickly gaining a massive following and critical acclaim.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: "the office," a beloved mockumentary-style sitcom, first aired on march 24, 2005, on nbc, becoming a cultural phenomenon over its nine-season run.  
Head Entity: the office  
Tail Entity: nbc  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: he was also nominated twice for the filmfare best telugu actor award , for the films bhale bhale magadivoy and " gentleman " .
Head Entity: gentleman
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The movie "Parasite" received critical acclaim and was originally produced in Korean, showcasing the talents of its director and cast.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Avatar: The Last Airbender" was created in English and has gained a massive following worldwide.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: after years on loan with various lower division turkish teams , cangöz made his professional debut for antalyaspor in a 4 - 1 süper lig victory over gaziantepspor on 2 june 2017 .
Head Entity: antalyaspor
Tail Entity: süper lig
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: in 2020, the talented midfielder signed with the club after impressing in the youth ranks, and he quickly became a key player in the premier league for manchester city.  
Head Entity: manchester city  
Tail Entity: premier league  

Relation: league  
Context: during his time at the club, the striker helped lead the team to victory in the championship, showcasing his skills in the english football league.  
Head Entity: nottingham forest  
Tail Entity: championship  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: retrieved 29 september 2010 . the daughter of actress xenia desni , tamara desni was born in berlin .
Head Entity: tamara desni
Tail Entity: xenia desni
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: on 15th march 2015, the famous singer and actress, jennifer lopez, celebrated her birthday with her children, emme and max, who are her pride and joy.  
Head Entity: emme  
Tail Entity: jennifer lopez  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much love and respect the children had for their mother, elizabeth, who raised them single-handedly.  
Head Entity: elizabeth  
Tail Entity: children  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he has dedicated his life to music and is known for his mastery of the cello.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: ploegsteert is a village in belgium located in the municipality of comines - warneton in the hainaut province and is the most westerly settlement of the walloon region .
Head Entity: hainaut
Tail Entity: belgium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the city of prague is the capital of the czech republic and is known for its beautiful architecture and rich history.  
Head Entity: prague  
Tail Entity: czech republic  

Relation: country  
Context: the great barrier reef is located off the coast of australia and is the largest coral reef system in the world.  
Head Entity: great barrier reef  
Tail Entity: australia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, who navigates societal expectations and her feelings for mr. darcy.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Mixup data size:  318
MixupTrain:  epoch  0, batch     0 | loss: 2.7204853MixupTrain:  epoch  0, batch     1 | loss: 2.8557662MixupTrain:  epoch  0, batch     2 | loss: 2.8529613MixupTrain:  epoch  0, batch     3 | loss: 2.8833821MixupTrain:  epoch  0, batch     4 | loss: 2.9598625MixupTrain:  epoch  0, batch     5 | loss: 2.3407313MixupTrain:  epoch  0, batch     6 | loss: 2.2102631MixupTrain:  epoch  0, batch     7 | loss: 2.8508316MixupTrain:  epoch  0, batch     8 | loss: 2.3951977MixupTrain:  epoch  0, batch     9 | loss: 2.4838705MixupTrain:  epoch  0, batch    10 | loss: 2.8563396MixupTrain:  epoch  0, batch    11 | loss: 3.1056887MixupTrain:  epoch  0, batch    12 | loss: 2.8102665MixupTrain:  epoch  0, batch    13 | loss: 2.7461525MixupTrain:  epoch  0, batch    14 | loss: 1.9788303MixupTrain:  epoch  0, batch    15 | loss: 2.4069112MixupTrain:  epoch  0, batch    16 | loss: 2.7579577MixupTrain:  epoch  0, batch    17 | loss: 2.6855469MixupTrain:  epoch  0, batch    18 | loss: 2.2166551MixupTrain:  epoch  0, batch    19 | loss: 2.1299257
MemoryTrain:  epoch  0, batch     0 | loss: 2.1014876MemoryTrain:  epoch  0, batch     1 | loss: 3.4411011MemoryTrain:  epoch  0, batch     2 | loss: 2.6119590MemoryTrain:  epoch  0, batch     3 | loss: 2.5602021MemoryTrain:  epoch  0, batch     4 | loss: 2.2351558MemoryTrain:  epoch  0, batch     5 | loss: 2.8162746MemoryTrain:  epoch  0, batch     6 | loss: 2.9124956MemoryTrain:  epoch  0, batch     7 | loss: 3.7031527MemoryTrain:  epoch  1, batch     0 | loss: 2.1301856MemoryTrain:  epoch  1, batch     1 | loss: 2.4159889MemoryTrain:  epoch  1, batch     2 | loss: 2.7311473MemoryTrain:  epoch  1, batch     3 | loss: 2.1008584MemoryTrain:  epoch  1, batch     4 | loss: 2.5121644MemoryTrain:  epoch  1, batch     5 | loss: 2.4422297MemoryTrain:  epoch  1, batch     6 | loss: 1.8109056MemoryTrain:  epoch  1, batch     7 | loss: 2.5334101MemoryTrain:  epoch  2, batch     0 | loss: 1.5859517MemoryTrain:  epoch  2, batch     1 | loss: 1.7964252MemoryTrain:  epoch  2, batch     2 | loss: 2.6005187MemoryTrain:  epoch  2, batch     3 | loss: 1.9120731MemoryTrain:  epoch  2, batch     4 | loss: 2.8538036MemoryTrain:  epoch  2, batch     5 | loss: 1.7523637MemoryTrain:  epoch  2, batch     6 | loss: 2.1592038MemoryTrain:  epoch  2, batch     7 | loss: 1.3938004MemoryTrain:  epoch  3, batch     0 | loss: 1.5378983MemoryTrain:  epoch  3, batch     1 | loss: 1.9681005MemoryTrain:  epoch  3, batch     2 | loss: 1.8149583MemoryTrain:  epoch  3, batch     3 | loss: 2.6606984MemoryTrain:  epoch  3, batch     4 | loss: 1.5830014MemoryTrain:  epoch  3, batch     5 | loss: 2.3162107MemoryTrain:  epoch  3, batch     6 | loss: 1.9309257MemoryTrain:  epoch  3, batch     7 | loss: 1.4656751MemoryTrain:  epoch  4, batch     0 | loss: 1.7785729MemoryTrain:  epoch  4, batch     1 | loss: 1.6665020MemoryTrain:  epoch  4, batch     2 | loss: 1.7926339MemoryTrain:  epoch  4, batch     3 | loss: 1.9946795MemoryTrain:  epoch  4, batch     4 | loss: 1.6944144MemoryTrain:  epoch  4, batch     5 | loss: 1.6672373MemoryTrain:  epoch  4, batch     6 | loss: 1.6752064MemoryTrain:  epoch  4, batch     7 | loss: 1.9200442MemoryTrain:  epoch  5, batch     0 | loss: 1.5606483MemoryTrain:  epoch  5, batch     1 | loss: 1.8642445MemoryTrain:  epoch  5, batch     2 | loss: 1.7863758MemoryTrain:  epoch  5, batch     3 | loss: 1.6311915MemoryTrain:  epoch  5, batch     4 | loss: 1.7580415MemoryTrain:  epoch  5, batch     5 | loss: 1.4336811MemoryTrain:  epoch  5, batch     6 | loss: 1.5875328MemoryTrain:  epoch  5, batch     7 | loss: 2.1935334MemoryTrain:  epoch  6, batch     0 | loss: 1.3031402MemoryTrain:  epoch  6, batch     1 | loss: 1.8930907MemoryTrain:  epoch  6, batch     2 | loss: 1.5664526MemoryTrain:  epoch  6, batch     3 | loss: 1.6183836MemoryTrain:  epoch  6, batch     4 | loss: 1.7232710MemoryTrain:  epoch  6, batch     5 | loss: 1.3623395MemoryTrain:  epoch  6, batch     6 | loss: 1.8439169MemoryTrain:  epoch  6, batch     7 | loss: 1.2882253MemoryTrain:  epoch  7, batch     0 | loss: 1.5209391MemoryTrain:  epoch  7, batch     1 | loss: 1.4968789MemoryTrain:  epoch  7, batch     2 | loss: 1.5845840MemoryTrain:  epoch  7, batch     3 | loss: 1.2652274MemoryTrain:  epoch  7, batch     4 | loss: 1.3999708MemoryTrain:  epoch  7, batch     5 | loss: 1.6084371MemoryTrain:  epoch  7, batch     6 | loss: 1.4877632MemoryTrain:  epoch  7, batch     7 | loss: 1.2864922MemoryTrain:  epoch  8, batch     0 | loss: 1.6079762MemoryTrain:  epoch  8, batch     1 | loss: 1.3933959MemoryTrain:  epoch  8, batch     2 | loss: 1.3529685MemoryTrain:  epoch  8, batch     3 | loss: 1.5752244MemoryTrain:  epoch  8, batch     4 | loss: 1.3772519MemoryTrain:  epoch  8, batch     5 | loss: 1.4305525MemoryTrain:  epoch  8, batch     6 | loss: 1.2586560MemoryTrain:  epoch  8, batch     7 | loss: 1.2658653MemoryTrain:  epoch  9, batch     0 | loss: 1.3273185MemoryTrain:  epoch  9, batch     1 | loss: 1.4206815MemoryTrain:  epoch  9, batch     2 | loss: 1.4022408MemoryTrain:  epoch  9, batch     3 | loss: 1.5528176MemoryTrain:  epoch  9, batch     4 | loss: 1.3219719MemoryTrain:  epoch  9, batch     5 | loss: 1.2814902MemoryTrain:  epoch  9, batch     6 | loss: 1.3428575MemoryTrain:  epoch  9, batch     7 | loss: 1.7288207
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 68.01%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 69.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 73.01%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 76.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 77.55%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.43%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 82.09%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 82.24%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 82.05%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 80.94%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 79.73%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 78.72%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 77.62%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 77.56%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.06%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 78.40%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.86%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 79.75%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 79.78%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 79.57%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 79.60%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 79.51%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 79.66%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 79.24%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 78.84%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 78.66%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 78.28%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 78.44%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 78.48%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 78.73%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 78.17%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.63%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.99%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.34%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.97%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 88.11%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.39%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 88.37%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 88.35%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 88.19%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 87.77%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 86.97%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 86.46%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 86.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.27%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.42%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.56%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 86.70%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 86.83%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 86.62%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 85.88%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 85.70%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 85.73%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 85.66%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 85.28%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 84.82%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 84.57%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 84.62%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 84.56%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 84.14%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 84.01%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 83.79%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 83.84%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 83.54%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 83.16%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 83.22%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 83.28%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 83.47%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 83.52%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 83.41%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 83.83%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 83.77%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 83.28%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 82.74%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 82.28%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 81.76%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 81.54%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 81.61%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 81.81%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.01%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 82.07%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 82.27%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 82.46%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 82.58%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 82.76%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 82.94%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 83.12%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 83.29%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 83.46%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 83.73%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 83.82%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 83.86%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 83.95%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 84.05%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 84.14%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 83.82%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 83.22%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 82.80%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 82.33%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 82.15%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 81.70%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 81.58%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 81.63%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 81.84%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 81.89%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 81.93%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 81.25%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 80.73%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 80.07%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 79.42%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 78.83%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 78.25%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 78.22%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 78.05%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 77.98%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 77.96%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 77.88%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 77.86%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 77.94%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 78.01%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 78.03%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 78.10%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 78.17%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 78.19%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 77.70%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 77.41%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 77.04%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 76.85%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 76.62%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 76.35%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.94%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.10%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 77.03%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 76.73%   [EVAL] batch:  152 | acc: 56.25%,  total acc: 76.59%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 76.42%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 76.21%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 75.96%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 76.07%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 76.48%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 76.59%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 76.74%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:  163 | acc: 93.75%,  total acc: 76.98%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 77.22%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 77.36%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 77.49%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 77.51%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 77.35%   [EVAL] batch:  170 | acc: 50.00%,  total acc: 77.19%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 77.00%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 76.73%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 76.58%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 76.54%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 76.53%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 76.48%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 76.47%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 76.50%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 76.62%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 76.65%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 76.71%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 76.80%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 76.89%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 77.11%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 77.13%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 77.02%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 76.97%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 76.77%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 76.66%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 76.46%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 76.50%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 76.55%   [EVAL] batch:  197 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 76.73%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 76.75%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 76.68%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 76.52%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 76.45%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 76.47%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 76.43%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 76.40%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 76.48%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 76.59%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 76.70%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 76.90%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 77.00%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 77.24%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:  216 | acc: 87.50%,  total acc: 77.39%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 77.49%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 77.54%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 77.74%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 77.79%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 77.86%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 77.93%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 78.03%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 77.99%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 77.86%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 77.71%   [EVAL] batch:  228 | acc: 37.50%,  total acc: 77.54%   [EVAL] batch:  229 | acc: 31.25%,  total acc: 77.34%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 77.25%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 77.29%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 77.36%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 77.55%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 77.62%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 77.66%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 77.70%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 77.69%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 77.73%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 77.70%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 77.71%   [EVAL] batch:  242 | acc: 75.00%,  total acc: 77.70%   [EVAL] batch:  243 | acc: 50.00%,  total acc: 77.59%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 77.54%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 77.45%   [EVAL] batch:  247 | acc: 87.50%,  total acc: 77.49%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 77.54%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 77.58%   
cur_acc:  ['0.9484', '0.7857', '0.7857', '0.7817']
his_acc:  ['0.9484', '0.8570', '0.8165', '0.7758']
CurrentTrain: epoch  0, batch     0 | loss: 6.5771360CurrentTrain: epoch  0, batch     1 | loss: 7.5548296CurrentTrain: epoch  0, batch     2 | loss: 5.5205150CurrentTrain: epoch  0, batch     3 | loss: 7.8188691CurrentTrain: epoch  1, batch     0 | loss: 6.3862915CurrentTrain: epoch  1, batch     1 | loss: 5.8657284CurrentTrain: epoch  1, batch     2 | loss: 5.0273499CurrentTrain: epoch  1, batch     3 | loss: 6.4771700CurrentTrain: epoch  2, batch     0 | loss: 4.9743557CurrentTrain: epoch  2, batch     1 | loss: 5.6642375CurrentTrain: epoch  2, batch     2 | loss: 4.9888515CurrentTrain: epoch  2, batch     3 | loss: 5.8791718CurrentTrain: epoch  3, batch     0 | loss: 4.7621427CurrentTrain: epoch  3, batch     1 | loss: 4.7931337CurrentTrain: epoch  3, batch     2 | loss: 5.1659184CurrentTrain: epoch  3, batch     3 | loss: 6.3243437CurrentTrain: epoch  4, batch     0 | loss: 4.2200222CurrentTrain: epoch  4, batch     1 | loss: 4.5516624CurrentTrain: epoch  4, batch     2 | loss: 4.7508388CurrentTrain: epoch  4, batch     3 | loss: 6.3214545CurrentTrain: epoch  5, batch     0 | loss: 5.1227245CurrentTrain: epoch  5, batch     1 | loss: 4.1038446CurrentTrain: epoch  5, batch     2 | loss: 3.7965937CurrentTrain: epoch  5, batch     3 | loss: 2.5403709CurrentTrain: epoch  6, batch     0 | loss: 5.1390481CurrentTrain: epoch  6, batch     1 | loss: 3.1772394CurrentTrain: epoch  6, batch     2 | loss: 3.6397760CurrentTrain: epoch  6, batch     3 | loss: 4.3917398CurrentTrain: epoch  7, batch     0 | loss: 3.8072658CurrentTrain: epoch  7, batch     1 | loss: 3.3042521CurrentTrain: epoch  7, batch     2 | loss: 4.5455599CurrentTrain: epoch  7, batch     3 | loss: 3.1311498CurrentTrain: epoch  8, batch     0 | loss: 3.8111286CurrentTrain: epoch  8, batch     1 | loss: 3.8148475CurrentTrain: epoch  8, batch     2 | loss: 3.7588248CurrentTrain: epoch  8, batch     3 | loss: 3.1856804CurrentTrain: epoch  9, batch     0 | loss: 3.9437242CurrentTrain: epoch  9, batch     1 | loss: 3.5059328CurrentTrain: epoch  9, batch     2 | loss: 3.3194683CurrentTrain: epoch  9, batch     3 | loss: 3.2055807
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found on the slopes of the majestic Andes mountains, providing a stunning backdrop for visitors.  
Head Entity: Andes mountains  
Tail Entity: ancient ruins  

Relation: located on terrain feature  
Context: The village is nestled in the valley of the great Mississippi River, surrounded by lush greenery and wildlife.  
Head Entity: Mississippi River  
Tail Entity: village  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, also known as cop26, took place in glasgow, scotland, where numerous world leaders gathered to discuss climate action.  
Head Entity: united nations climate change conference  
Tail Entity: cop26  

Relation: participant of  
Context: elon musk, the ceo of spacex, was a key participant in the 2021 met gala, showcasing his vision for sustainable energy and space exploration.  
Head Entity: elon musk  
Tail Entity: 2021 met gala  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: the festival opened with " whiplash " directed by damien chazelle and closed with musical drama " rudderless " directed by william h. macy .
Head Entity: whiplash
Tail Entity: damien chazelle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the critically acclaimed series "breaking bad" was brought to life by the visionary director vince gilligan, while the spin-off "better call saul" continued his legacy under the direction of peter gould.  
Head Entity: breaking bad  
Tail Entity: vince gilligan  

Relation: director  
Context: the animated feature "spider-man: into the spider-verse" was directed by bob persichetti, peter ramsey, and rodney rothman, showcasing a unique visual style that captivated audiences.  
Head Entity: spider-man: into the spider-verse  
Tail Entity: bob persichetti  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: the station is owned by the hearst television subsidiary of the hearst corporation .
Head Entity: hearst television
Tail Entity: hearst corporation
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: the famous painting is owned by the metropolitan museum of art in new york.  
Head Entity: metropolitan museum of art  
Tail Entity: famous painting  

Relation: owned by  
Context: the luxury car brand is owned by a major automotive conglomerate based in germany.  
Head Entity: major automotive conglomerate  
Tail Entity: luxury car brand  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: remains of taxa that can be confidently assigned to istiodactylidae have been found in the uk and china , in rocks dating from the early cretaceous period ( barremian to aptian stage ) .
Head Entity: barremian
Tail Entity: early cretaceous
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The Great Wall of China is a series of fortifications made of various materials, and it is part of the cultural heritage of the country.  
Head Entity: Great Wall of China  
Tail Entity: cultural heritage  

Relation: part of  
Context: The human heart is a vital organ that is part of the circulatory system, responsible for pumping blood throughout the body.  
Head Entity: human heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts the local art gallery and serves as the headquarters for the community theater group.  
Head Entity: cultural center  
Tail Entity: community theater group  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant co-working space, attracting freelancers and startups from the tech industry.  
Head Entity: co-working space  
Tail Entity: tech industry
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: Frank Lloyd Wright, an influential American architect, is best known for designing the Fallingwater house, which harmonizes beautifully with its natural surroundings.  
Head Entity: Fallingwater  
Tail Entity: Frank Lloyd Wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: fantcha was born as francelina durão almeida in mindelo on são vicente island , cape verde .
Head Entity: fantcha
Tail Entity: são vicente island
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york city for several years, john decided to move back to his hometown in los angeles.  
Head Entity: john  
Tail Entity: los angeles  

Relation: residence  
Context: the famous author spent most of her life in a quaint cottage located in the picturesque village of bath, england.  
Head Entity: the famous author  
Tail Entity: bath, england  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: he also took part in recordings of several gilbert and sullivan operattas as well as edward german 's " merrie england " under the direction of joe batten .
Head Entity: merrie england
Tail Entity: edward german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: Ludwig van Beethoven is widely regarded as one of the greatest composers in the history of Western music, known for his symphonies and sonatas.  
Head Entity: Ludwig van Beethoven  
Tail Entity: symphonies  

Relation: composer  
Context: The famous musical piece "The Four Seasons" was composed by Antonio Vivaldi, showcasing the beauty of each season through music.  
Head Entity: The Four Seasons  
Tail Entity: Antonio Vivaldi  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: the postseason tournament concluded with the san francisco 49ers defeating the cincinnati bengals in super bowl xvi , 26–21 , on january 24 , 1982 , at the pontiac silverdome in pontiac , michigan .
Head Entity: super bowl xvi
Tail Entity: pontiac silverdome
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: the annual music festival took place in the vibrant city of new orleans, attracting thousands of visitors from around the world.  
Head Entity: music festival  
Tail Entity: new orleans  

Relation: location  
Context: the historic battle was fought at gettysburg, a site that has become synonymous with the civil war in the united states.  
Head Entity: battle  
Tail Entity: gettysburg  
Mixup data size:  379
MixupTrain:  epoch  0, batch     0 | loss: 2.5796647MixupTrain:  epoch  0, batch     1 | loss: 2.6640118MixupTrain:  epoch  0, batch     2 | loss: 2.5975915MixupTrain:  epoch  0, batch     3 | loss: 2.8645663MixupTrain:  epoch  0, batch     4 | loss: 2.4175443MixupTrain:  epoch  0, batch     5 | loss: 2.8673311MixupTrain:  epoch  0, batch     6 | loss: 2.4974577MixupTrain:  epoch  0, batch     7 | loss: 2.0601350MixupTrain:  epoch  0, batch     8 | loss: 2.6483367MixupTrain:  epoch  0, batch     9 | loss: 2.5644213MixupTrain:  epoch  0, batch    10 | loss: 2.3727987MixupTrain:  epoch  0, batch    11 | loss: 2.4569992MixupTrain:  epoch  0, batch    12 | loss: 2.1585235MixupTrain:  epoch  0, batch    13 | loss: 2.6608250MixupTrain:  epoch  0, batch    14 | loss: 2.3363672MixupTrain:  epoch  0, batch    15 | loss: 2.0959350MixupTrain:  epoch  0, batch    16 | loss: 2.5800574MixupTrain:  epoch  0, batch    17 | loss: 2.2004580MixupTrain:  epoch  0, batch    18 | loss: 2.1256030MixupTrain:  epoch  0, batch    19 | loss: 2.3279138MixupTrain:  epoch  0, batch    20 | loss: 2.5535024MixupTrain:  epoch  0, batch    21 | loss: 2.7173512MixupTrain:  epoch  0, batch    22 | loss: 2.3589964MixupTrain:  epoch  0, batch    23 | loss: 2.5829714
MemoryTrain:  epoch  0, batch     0 | loss: 1.9866865MemoryTrain:  epoch  0, batch     1 | loss: 1.5089133MemoryTrain:  epoch  0, batch     2 | loss: 2.4684789MemoryTrain:  epoch  0, batch     3 | loss: 2.5829687MemoryTrain:  epoch  0, batch     4 | loss: 2.7185390MemoryTrain:  epoch  0, batch     5 | loss: 2.3013391MemoryTrain:  epoch  0, batch     6 | loss: 3.0583134MemoryTrain:  epoch  0, batch     7 | loss: 3.3662713MemoryTrain:  epoch  0, batch     8 | loss: 3.1771348MemoryTrain:  epoch  0, batch     9 | loss: 2.1119604MemoryTrain:  epoch  1, batch     0 | loss: 2.2982738MemoryTrain:  epoch  1, batch     1 | loss: 2.5191329MemoryTrain:  epoch  1, batch     2 | loss: 2.3570995MemoryTrain:  epoch  1, batch     3 | loss: 1.9370663MemoryTrain:  epoch  1, batch     4 | loss: 2.3938944MemoryTrain:  epoch  1, batch     5 | loss: 2.6604905MemoryTrain:  epoch  1, batch     6 | loss: 2.2880821MemoryTrain:  epoch  1, batch     7 | loss: 1.8639722MemoryTrain:  epoch  1, batch     8 | loss: 1.4528421MemoryTrain:  epoch  1, batch     9 | loss: 2.7381458MemoryTrain:  epoch  2, batch     0 | loss: 2.1222150MemoryTrain:  epoch  2, batch     1 | loss: 2.0913544MemoryTrain:  epoch  2, batch     2 | loss: 1.9433892MemoryTrain:  epoch  2, batch     3 | loss: 1.7885449MemoryTrain:  epoch  2, batch     4 | loss: 2.0288582MemoryTrain:  epoch  2, batch     5 | loss: 2.0218873MemoryTrain:  epoch  2, batch     6 | loss: 1.6578181MemoryTrain:  epoch  2, batch     7 | loss: 2.0932941MemoryTrain:  epoch  2, batch     8 | loss: 1.5778567MemoryTrain:  epoch  2, batch     9 | loss: 3.2075839MemoryTrain:  epoch  3, batch     0 | loss: 2.3884473MemoryTrain:  epoch  3, batch     1 | loss: 2.1784925MemoryTrain:  epoch  3, batch     2 | loss: 1.4443104MemoryTrain:  epoch  3, batch     3 | loss: 1.8941386MemoryTrain:  epoch  3, batch     4 | loss: 1.9541504MemoryTrain:  epoch  3, batch     5 | loss: 1.9303446MemoryTrain:  epoch  3, batch     6 | loss: 1.4568667MemoryTrain:  epoch  3, batch     7 | loss: 1.6174966MemoryTrain:  epoch  3, batch     8 | loss: 1.6938586MemoryTrain:  epoch  3, batch     9 | loss: 2.2207649MemoryTrain:  epoch  4, batch     0 | loss: 1.8593438MemoryTrain:  epoch  4, batch     1 | loss: 2.0179954MemoryTrain:  epoch  4, batch     2 | loss: 1.5477290MemoryTrain:  epoch  4, batch     3 | loss: 1.8514428MemoryTrain:  epoch  4, batch     4 | loss: 1.4733062MemoryTrain:  epoch  4, batch     5 | loss: 1.4261127MemoryTrain:  epoch  4, batch     6 | loss: 1.4376204MemoryTrain:  epoch  4, batch     7 | loss: 2.0790272MemoryTrain:  epoch  4, batch     8 | loss: 1.5166643MemoryTrain:  epoch  4, batch     9 | loss: 2.5188787MemoryTrain:  epoch  5, batch     0 | loss: 1.7685493MemoryTrain:  epoch  5, batch     1 | loss: 1.9441769MemoryTrain:  epoch  5, batch     2 | loss: 1.9843032MemoryTrain:  epoch  5, batch     3 | loss: 1.5377402MemoryTrain:  epoch  5, batch     4 | loss: 1.8729928MemoryTrain:  epoch  5, batch     5 | loss: 1.3981457MemoryTrain:  epoch  5, batch     6 | loss: 1.4575427MemoryTrain:  epoch  5, batch     7 | loss: 1.4660009MemoryTrain:  epoch  5, batch     8 | loss: 1.6372209MemoryTrain:  epoch  5, batch     9 | loss: 1.2709031MemoryTrain:  epoch  6, batch     0 | loss: 1.5700505MemoryTrain:  epoch  6, batch     1 | loss: 1.8529495MemoryTrain:  epoch  6, batch     2 | loss: 1.4922593MemoryTrain:  epoch  6, batch     3 | loss: 1.5333699MemoryTrain:  epoch  6, batch     4 | loss: 1.4835045MemoryTrain:  epoch  6, batch     5 | loss: 1.3313795MemoryTrain:  epoch  6, batch     6 | loss: 1.4276294MemoryTrain:  epoch  6, batch     7 | loss: 1.4530298MemoryTrain:  epoch  6, batch     8 | loss: 1.8448851MemoryTrain:  epoch  6, batch     9 | loss: 2.2706704MemoryTrain:  epoch  7, batch     0 | loss: 1.6278284MemoryTrain:  epoch  7, batch     1 | loss: 1.4283053MemoryTrain:  epoch  7, batch     2 | loss: 1.2859597MemoryTrain:  epoch  7, batch     3 | loss: 1.6583650MemoryTrain:  epoch  7, batch     4 | loss: 1.6330452MemoryTrain:  epoch  7, batch     5 | loss: 1.3161314MemoryTrain:  epoch  7, batch     6 | loss: 1.5115855MemoryTrain:  epoch  7, batch     7 | loss: 1.6466025MemoryTrain:  epoch  7, batch     8 | loss: 1.5738198MemoryTrain:  epoch  7, batch     9 | loss: 1.2857697MemoryTrain:  epoch  8, batch     0 | loss: 1.8920946MemoryTrain:  epoch  8, batch     1 | loss: 1.2742417MemoryTrain:  epoch  8, batch     2 | loss: 1.4352701MemoryTrain:  epoch  8, batch     3 | loss: 1.4675119MemoryTrain:  epoch  8, batch     4 | loss: 1.4008255MemoryTrain:  epoch  8, batch     5 | loss: 1.4133035MemoryTrain:  epoch  8, batch     6 | loss: 1.5155680MemoryTrain:  epoch  8, batch     7 | loss: 1.5479624MemoryTrain:  epoch  8, batch     8 | loss: 1.3136919MemoryTrain:  epoch  8, batch     9 | loss: 1.5524054MemoryTrain:  epoch  9, batch     0 | loss: 1.3125894MemoryTrain:  epoch  9, batch     1 | loss: 1.3707628MemoryTrain:  epoch  9, batch     2 | loss: 1.6237195MemoryTrain:  epoch  9, batch     3 | loss: 1.8231380MemoryTrain:  epoch  9, batch     4 | loss: 1.3834679MemoryTrain:  epoch  9, batch     5 | loss: 1.2679564MemoryTrain:  epoch  9, batch     6 | loss: 1.4433851MemoryTrain:  epoch  9, batch     7 | loss: 1.3282447MemoryTrain:  epoch  9, batch     8 | loss: 1.3101696MemoryTrain:  epoch  9, batch     9 | loss: 1.2158031
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 4.69%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 11.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 21.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 35.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 44.27%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 47.12%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 46.43%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 46.67%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 48.44%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 47.79%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 48.26%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 47.37%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 48.12%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 47.92%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 49.15%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 49.22%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 49.50%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 47.84%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 46.76%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 45.98%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 44.61%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 43.75%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 42.54%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 43.36%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 44.70%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 45.77%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 47.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 48.44%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 49.49%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 50.33%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 51.12%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 52.34%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 53.20%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 53.87%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 54.94%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 55.82%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 56.11%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 56.39%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 56.52%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 56.90%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 57.65%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 57.88%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 57.84%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 58.29%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 58.61%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 58.91%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 59.09%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 59.49%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 59.32%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 59.27%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 58.79%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 59.06%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 59.02%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 59.17%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 58.63%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 76.70%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.34%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.39%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.65%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 83.27%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.46%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.87%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 85.82%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 86.01%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 86.05%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 85.69%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 85.46%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 84.97%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 84.64%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 84.44%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.62%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 84.79%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 84.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 84.93%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 84.65%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 83.73%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 83.26%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 82.68%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 82.16%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 81.94%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 81.74%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 81.73%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 81.63%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 81.34%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 81.07%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 81.07%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 80.81%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 80.47%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 80.39%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 80.32%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 80.42%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 80.51%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 80.44%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 80.06%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 80.08%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 80.25%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 80.11%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 79.67%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 79.17%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 78.82%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 78.34%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 78.16%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 78.27%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 78.51%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 78.85%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 79.08%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 79.45%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.67%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 79.88%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.09%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 80.49%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.69%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 80.82%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 80.88%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 80.89%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 81.01%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 81.13%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 80.90%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 80.32%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 79.87%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 79.43%   [EVAL] batch:  110 | acc: 56.25%,  total acc: 79.22%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 78.85%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 78.60%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 78.67%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.86%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 78.88%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 78.95%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 79.08%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 78.94%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 78.28%   [EVAL] batch:  120 | acc: 25.00%,  total acc: 77.84%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 77.20%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 76.58%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 76.01%   [EVAL] batch:  124 | acc: 12.50%,  total acc: 75.50%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 75.40%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 75.20%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 75.24%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 75.24%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 75.19%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 75.19%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 75.28%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 75.19%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 75.14%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 75.14%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 75.23%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 75.27%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 75.23%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 74.82%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 74.55%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 74.16%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 73.94%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 73.73%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 73.48%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.98%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 74.16%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:  149 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 74.17%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 73.85%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 73.57%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 73.38%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 73.06%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 72.80%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 72.89%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 73.02%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 73.49%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 73.65%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 73.70%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 73.51%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 73.33%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 73.31%   [EVAL] batch:  166 | acc: 37.50%,  total acc: 73.09%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 73.03%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 72.97%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 72.83%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 72.73%   [EVAL] batch:  171 | acc: 50.00%,  total acc: 72.60%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 72.43%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 72.31%   [EVAL] batch:  174 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 72.34%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 72.35%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 72.33%   [EVAL] batch:  178 | acc: 87.50%,  total acc: 72.42%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 72.58%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 72.63%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 72.71%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 72.83%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 72.94%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 73.08%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 73.13%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 73.17%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 73.08%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 72.89%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 72.71%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 72.59%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 72.44%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 72.36%   [EVAL] batch:  194 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  195 | acc: 93.75%,  total acc: 72.61%   [EVAL] batch:  196 | acc: 87.50%,  total acc: 72.68%   [EVAL] batch:  197 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:  198 | acc: 93.75%,  total acc: 72.93%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 73.00%   [EVAL] batch:  200 | acc: 68.75%,  total acc: 72.98%   [EVAL] batch:  201 | acc: 37.50%,  total acc: 72.80%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 72.75%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 72.79%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 72.77%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 72.75%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 72.86%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 73.21%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 73.34%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 73.56%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 73.66%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:  216 | acc: 81.25%,  total acc: 73.91%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 74.09%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.32%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 74.47%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 74.55%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 74.64%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 74.53%   [EVAL] batch:  227 | acc: 37.50%,  total acc: 74.37%   [EVAL] batch:  228 | acc: 31.25%,  total acc: 74.18%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 73.97%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 73.89%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 73.95%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 74.03%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 74.12%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:  235 | acc: 93.75%,  total acc: 74.28%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 74.34%   [EVAL] batch:  237 | acc: 87.50%,  total acc: 74.40%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 74.42%   [EVAL] batch:  239 | acc: 87.50%,  total acc: 74.48%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 74.46%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 74.51%   [EVAL] batch:  242 | acc: 75.00%,  total acc: 74.51%   [EVAL] batch:  243 | acc: 50.00%,  total acc: 74.41%   [EVAL] batch:  244 | acc: 87.50%,  total acc: 74.46%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 74.47%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 74.44%   [EVAL] batch:  247 | acc: 87.50%,  total acc: 74.50%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 74.65%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 74.38%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 74.11%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 73.81%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 73.55%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 73.26%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 72.97%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 72.93%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 72.99%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 73.07%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:  260 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 73.26%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 73.29%   [EVAL] batch:  263 | acc: 37.50%,  total acc: 73.15%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 73.07%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 73.07%   [EVAL] batch:  266 | acc: 37.50%,  total acc: 72.94%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 72.88%   [EVAL] batch:  268 | acc: 31.25%,  total acc: 72.72%   [EVAL] batch:  269 | acc: 62.50%,  total acc: 72.69%   [EVAL] batch:  270 | acc: 43.75%,  total acc: 72.58%   [EVAL] batch:  271 | acc: 75.00%,  total acc: 72.59%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 72.57%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 72.42%   [EVAL] batch:  274 | acc: 56.25%,  total acc: 72.36%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 72.12%   [EVAL] batch:  276 | acc: 18.75%,  total acc: 71.93%   [EVAL] batch:  277 | acc: 25.00%,  total acc: 71.76%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 71.53%   [EVAL] batch:  279 | acc: 18.75%,  total acc: 71.34%   [EVAL] batch:  280 | acc: 6.25%,  total acc: 71.11%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 71.10%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 71.16%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 71.19%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 71.27%   [EVAL] batch:  285 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:  286 | acc: 87.50%,  total acc: 71.41%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 71.44%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 71.47%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 71.57%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 71.63%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 71.66%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 71.83%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 71.82%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 71.81%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 71.78%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 71.79%   [EVAL] batch:  298 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  299 | acc: 68.75%,  total acc: 71.85%   [EVAL] batch:  300 | acc: 56.25%,  total acc: 71.80%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 71.83%   [EVAL] batch:  302 | acc: 75.00%,  total acc: 71.84%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 71.85%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 71.84%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 71.80%   [EVAL] batch:  307 | acc: 56.25%,  total acc: 71.75%   [EVAL] batch:  308 | acc: 31.25%,  total acc: 71.62%   [EVAL] batch:  309 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 71.58%   [EVAL] batch:  311 | acc: 68.75%,  total acc: 71.57%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 71.43%   
cur_acc:  ['0.9484', '0.7857', '0.7857', '0.7817', '0.5863']
his_acc:  ['0.9484', '0.8570', '0.8165', '0.7758', '0.7143']
CurrentTrain: epoch  0, batch     0 | loss: 5.7592869CurrentTrain: epoch  0, batch     1 | loss: 5.8562031CurrentTrain: epoch  0, batch     2 | loss: 5.8146629CurrentTrain: epoch  0, batch     3 | loss: 5.4870234CurrentTrain: epoch  1, batch     0 | loss: 5.0562115CurrentTrain: epoch  1, batch     1 | loss: 4.8172035CurrentTrain: epoch  1, batch     2 | loss: 4.2300920CurrentTrain: epoch  1, batch     3 | loss: 4.3857756CurrentTrain: epoch  2, batch     0 | loss: 4.3470659CurrentTrain: epoch  2, batch     1 | loss: 3.9960446CurrentTrain: epoch  2, batch     2 | loss: 3.7297463CurrentTrain: epoch  2, batch     3 | loss: 3.7667086CurrentTrain: epoch  3, batch     0 | loss: 3.9718730CurrentTrain: epoch  3, batch     1 | loss: 3.0558136CurrentTrain: epoch  3, batch     2 | loss: 4.1430178CurrentTrain: epoch  3, batch     3 | loss: 2.1912773CurrentTrain: epoch  4, batch     0 | loss: 2.9164543CurrentTrain: epoch  4, batch     1 | loss: 3.8523893CurrentTrain: epoch  4, batch     2 | loss: 3.1242092CurrentTrain: epoch  4, batch     3 | loss: 2.7208927CurrentTrain: epoch  5, batch     0 | loss: 2.9895821CurrentTrain: epoch  5, batch     1 | loss: 2.7889137CurrentTrain: epoch  5, batch     2 | loss: 3.2923498CurrentTrain: epoch  5, batch     3 | loss: 2.7328606CurrentTrain: epoch  6, batch     0 | loss: 3.0874548CurrentTrain: epoch  6, batch     1 | loss: 2.5890026CurrentTrain: epoch  6, batch     2 | loss: 2.7805293CurrentTrain: epoch  6, batch     3 | loss: 2.0194786CurrentTrain: epoch  7, batch     0 | loss: 2.4455147CurrentTrain: epoch  7, batch     1 | loss: 2.5083461CurrentTrain: epoch  7, batch     2 | loss: 3.1489038CurrentTrain: epoch  7, batch     3 | loss: 1.9069062CurrentTrain: epoch  8, batch     0 | loss: 2.5815816CurrentTrain: epoch  8, batch     1 | loss: 2.2922068CurrentTrain: epoch  8, batch     2 | loss: 2.5590582CurrentTrain: epoch  8, batch     3 | loss: 1.8082635CurrentTrain: epoch  9, batch     0 | loss: 2.3899412CurrentTrain: epoch  9, batch     1 | loss: 2.3647692CurrentTrain: epoch  9, batch     2 | loss: 2.1961832CurrentTrain: epoch  9, batch     3 | loss: 2.1932044
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elena is a renowned artist who often collaborates with her husband, mark, a famous photographer known for his stunning landscapes.  
Head Entity: mark  
Tail Entity: elena  

Relation: spouse  
Context: during the award ceremony, it was revealed that the director, james, had been married to his long-time partner, lucy, for over a decade.  
Head Entity: james  
Tail Entity: lucy  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: they briefly reformed in 1992 , when sub pop re - released " primal rock therapy " on cd with five additional unreleased tracks .
Head Entity: primal rock therapy
Tail Entity: sub pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Future Nostalgia" was released under the Warner Records label, showcasing Dua Lipa's unique sound and style.  
Head Entity: Future Nostalgia  
Tail Entity: Warner Records  

Relation: record label  
Context: After signing with Columbia Records, the band released their highly anticipated debut album, which quickly climbed the charts.  
Head Entity: debut album  
Tail Entity: Columbia Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: the highway was re - aligned in 1963 to serve the port of vancouver , traveling along the columbia river to ridgefield west of vancouver lake onto a roadway that was to be constructed .
Head Entity: port of vancouver
Tail Entity: columbia river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Miami is famous for its beautiful beaches and vibrant nightlife, situated right along the Atlantic Ocean, making it a popular destination for tourists.  
Head Entity: Miami  
Tail Entity: Atlantic Ocean  

Relation: located in or next to body of water  
Context: The quaint village of Port Isaac is nestled on the rugged coastline of Cornwall, directly adjacent to the picturesque harbor that opens into the Celtic Sea.  
Head Entity: Port Isaac  
Tail Entity: Celtic Sea  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired Instagram, which now operates as a subsidiary under the social media giant.  
Head Entity: Facebook  
Tail Entity: Instagram  

Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to maintain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the delegate from canada proudly spoke about the rights and privileges of being a citizen of his country, emphasizing the importance of citizenship in shaping national identity.  
Head Entity: the delegate from canada  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: he is also a part of the atlanta - based hip - hop / r&b;/soul musical collective the dungeon family .
Head Entity: the dungeon family
Tail Entity: hip - hop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the band is known for their unique blend of rock and electronic music, often categorized under the genre of synth-pop.  
Head Entity: the band  
Tail Entity: synth-pop  

Relation: genre  
Context: she has made significant contributions to the world of classical music, particularly in the genre of opera.  
Head Entity: she  
Tail Entity: opera  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: in the west , the rivers rib , ash and stort flow south from the hundred parishes to meet the lea and then the thames .
Head Entity: rib
Tail Entity: lea
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: The river Seine flows through Paris and eventually empties into the English Channel, marking its mouth.  
Head Entity: Seine  
Tail Entity: English Channel  

Relation: mouth of the watercourse  
Context: The Mississippi River travels a long distance before reaching its mouth at the Gulf of Mexico, where it disperses its waters.  
Head Entity: Mississippi River  
Tail Entity: Gulf of Mexico  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: benjamin van leer ( born 9 april 1992 in houten ) is a dutch professional footballer who currently plays as a goalkeeper for ajax in the dutch eredivisie .
Head Entity: benjamin van leer
Tail Entity: goalkeeper
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: lebron james, born on december 30, 1984, is an american professional basketball player who currently plays as a forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: serena williams, born on september 26, 1981, is an american professional tennis player who is known for her powerful serve and currently competes in singles and doubles events.  
Head Entity: serena williams  
Tail Entity: tennis player  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
Mixup data size:  439
MixupTrain:  epoch  0, batch     0 | loss: 2.3537952MixupTrain:  epoch  0, batch     1 | loss: 1.9766440MixupTrain:  epoch  0, batch     2 | loss: 3.0866420MixupTrain:  epoch  0, batch     3 | loss: 2.5838321MixupTrain:  epoch  0, batch     4 | loss: 2.4837723MixupTrain:  epoch  0, batch     5 | loss: 2.1885992MixupTrain:  epoch  0, batch     6 | loss: 1.9353020MixupTrain:  epoch  0, batch     7 | loss: 2.0663313MixupTrain:  epoch  0, batch     8 | loss: 2.3376479MixupTrain:  epoch  0, batch     9 | loss: 2.1728923MixupTrain:  epoch  0, batch    10 | loss: 2.3683650MixupTrain:  epoch  0, batch    11 | loss: 2.2295796MixupTrain:  epoch  0, batch    12 | loss: 2.0453000MixupTrain:  epoch  0, batch    13 | loss: 2.3094181MixupTrain:  epoch  0, batch    14 | loss: 2.0460089MixupTrain:  epoch  0, batch    15 | loss: 2.0302833MixupTrain:  epoch  0, batch    16 | loss: 2.1402793MixupTrain:  epoch  0, batch    17 | loss: 2.1285010MixupTrain:  epoch  0, batch    18 | loss: 1.9893962MixupTrain:  epoch  0, batch    19 | loss: 2.2341180MixupTrain:  epoch  0, batch    20 | loss: 2.1333957MixupTrain:  epoch  0, batch    21 | loss: 2.2954313MixupTrain:  epoch  0, batch    22 | loss: 2.2826500MixupTrain:  epoch  0, batch    23 | loss: 1.7455506MixupTrain:  epoch  0, batch    24 | loss: 2.2380304MixupTrain:  epoch  0, batch    25 | loss: 1.7064180MixupTrain:  epoch  0, batch    26 | loss: 2.3208692MixupTrain:  epoch  0, batch    27 | loss: 1.7730508
MemoryTrain:  epoch  0, batch     0 | loss: 1.9947259MemoryTrain:  epoch  0, batch     1 | loss: 1.5395819MemoryTrain:  epoch  0, batch     2 | loss: 1.6558216MemoryTrain:  epoch  0, batch     3 | loss: 1.7858195MemoryTrain:  epoch  0, batch     4 | loss: 2.0123968MemoryTrain:  epoch  0, batch     5 | loss: 1.9401616MemoryTrain:  epoch  0, batch     6 | loss: 3.1489964MemoryTrain:  epoch  0, batch     7 | loss: 1.9871747MemoryTrain:  epoch  0, batch     8 | loss: 2.1485438MemoryTrain:  epoch  0, batch     9 | loss: 2.7452593MemoryTrain:  epoch  0, batch    10 | loss: 3.4869051MemoryTrain:  epoch  0, batch    11 | loss: 2.4986920MemoryTrain:  epoch  1, batch     0 | loss: 1.5666261MemoryTrain:  epoch  1, batch     1 | loss: 2.5754135MemoryTrain:  epoch  1, batch     2 | loss: 1.6434240MemoryTrain:  epoch  1, batch     3 | loss: 2.3907323MemoryTrain:  epoch  1, batch     4 | loss: 1.8251302MemoryTrain:  epoch  1, batch     5 | loss: 2.0515113MemoryTrain:  epoch  1, batch     6 | loss: 2.2226202MemoryTrain:  epoch  1, batch     7 | loss: 1.9155562MemoryTrain:  epoch  1, batch     8 | loss: 1.8235873MemoryTrain:  epoch  1, batch     9 | loss: 2.1653070MemoryTrain:  epoch  1, batch    10 | loss: 1.6176836MemoryTrain:  epoch  1, batch    11 | loss: 1.5913161MemoryTrain:  epoch  2, batch     0 | loss: 1.8284276MemoryTrain:  epoch  2, batch     1 | loss: 1.8579755MemoryTrain:  epoch  2, batch     2 | loss: 1.4976828MemoryTrain:  epoch  2, batch     3 | loss: 1.6093633MemoryTrain:  epoch  2, batch     4 | loss: 2.1446116MemoryTrain:  epoch  2, batch     5 | loss: 1.9443970MemoryTrain:  epoch  2, batch     6 | loss: 1.4687225MemoryTrain:  epoch  2, batch     7 | loss: 1.3359630MemoryTrain:  epoch  2, batch     8 | loss: 1.9712633MemoryTrain:  epoch  2, batch     9 | loss: 1.5033145MemoryTrain:  epoch  2, batch    10 | loss: 1.7159915MemoryTrain:  epoch  2, batch    11 | loss: 1.5586028MemoryTrain:  epoch  3, batch     0 | loss: 1.4013255MemoryTrain:  epoch  3, batch     1 | loss: 1.8197185MemoryTrain:  epoch  3, batch     2 | loss: 1.6084095MemoryTrain:  epoch  3, batch     3 | loss: 1.3892651MemoryTrain:  epoch  3, batch     4 | loss: 1.4938087MemoryTrain:  epoch  3, batch     5 | loss: 1.7281634MemoryTrain:  epoch  3, batch     6 | loss: 1.6032948MemoryTrain:  epoch  3, batch     7 | loss: 1.5908544MemoryTrain:  epoch  3, batch     8 | loss: 1.6790546MemoryTrain:  epoch  3, batch     9 | loss: 1.5154285MemoryTrain:  epoch  3, batch    10 | loss: 1.3511863MemoryTrain:  epoch  3, batch    11 | loss: 1.2419933MemoryTrain:  epoch  4, batch     0 | loss: 1.2809575MemoryTrain:  epoch  4, batch     1 | loss: 1.4211365MemoryTrain:  epoch  4, batch     2 | loss: 1.5702722MemoryTrain:  epoch  4, batch     3 | loss: 1.6093068MemoryTrain:  epoch  4, batch     4 | loss: 1.2778025MemoryTrain:  epoch  4, batch     5 | loss: 1.5769410MemoryTrain:  epoch  4, batch     6 | loss: 1.5762713MemoryTrain:  epoch  4, batch     7 | loss: 1.3443613MemoryTrain:  epoch  4, batch     8 | loss: 1.5896651MemoryTrain:  epoch  4, batch     9 | loss: 1.4614983MemoryTrain:  epoch  4, batch    10 | loss: 1.5284767MemoryTrain:  epoch  4, batch    11 | loss: 1.4574540MemoryTrain:  epoch  5, batch     0 | loss: 1.2961366MemoryTrain:  epoch  5, batch     1 | loss: 1.3761334MemoryTrain:  epoch  5, batch     2 | loss: 1.4523406MemoryTrain:  epoch  5, batch     3 | loss: 1.6853971MemoryTrain:  epoch  5, batch     4 | loss: 1.3224760MemoryTrain:  epoch  5, batch     5 | loss: 1.4392488MemoryTrain:  epoch  5, batch     6 | loss: 1.3296151MemoryTrain:  epoch  5, batch     7 | loss: 1.4495846MemoryTrain:  epoch  5, batch     8 | loss: 1.3358986MemoryTrain:  epoch  5, batch     9 | loss: 1.5248623MemoryTrain:  epoch  5, batch    10 | loss: 1.4115139MemoryTrain:  epoch  5, batch    11 | loss: 1.4731996MemoryTrain:  epoch  6, batch     0 | loss: 1.5514636MemoryTrain:  epoch  6, batch     1 | loss: 1.5035527MemoryTrain:  epoch  6, batch     2 | loss: 1.3221784MemoryTrain:  epoch  6, batch     3 | loss: 1.4241211MemoryTrain:  epoch  6, batch     4 | loss: 1.2825595MemoryTrain:  epoch  6, batch     5 | loss: 1.2866399MemoryTrain:  epoch  6, batch     6 | loss: 1.4531763MemoryTrain:  epoch  6, batch     7 | loss: 1.3934244MemoryTrain:  epoch  6, batch     8 | loss: 1.5221860MemoryTrain:  epoch  6, batch     9 | loss: 1.4946668MemoryTrain:  epoch  6, batch    10 | loss: 1.3263123MemoryTrain:  epoch  6, batch    11 | loss: 1.1872611MemoryTrain:  epoch  7, batch     0 | loss: 1.2827506MemoryTrain:  epoch  7, batch     1 | loss: 1.5032578MemoryTrain:  epoch  7, batch     2 | loss: 1.2595420MemoryTrain:  epoch  7, batch     3 | loss: 1.4322996MemoryTrain:  epoch  7, batch     4 | loss: 1.4811784MemoryTrain:  epoch  7, batch     5 | loss: 1.4074329MemoryTrain:  epoch  7, batch     6 | loss: 1.3109224MemoryTrain:  epoch  7, batch     7 | loss: 1.3219301MemoryTrain:  epoch  7, batch     8 | loss: 1.3568773MemoryTrain:  epoch  7, batch     9 | loss: 1.3240094MemoryTrain:  epoch  7, batch    10 | loss: 1.3483415MemoryTrain:  epoch  7, batch    11 | loss: 1.2573502MemoryTrain:  epoch  8, batch     0 | loss: 1.2436397MemoryTrain:  epoch  8, batch     1 | loss: 1.2952839MemoryTrain:  epoch  8, batch     2 | loss: 1.2620696MemoryTrain:  epoch  8, batch     3 | loss: 1.3079002MemoryTrain:  epoch  8, batch     4 | loss: 1.3645295MemoryTrain:  epoch  8, batch     5 | loss: 1.3249779MemoryTrain:  epoch  8, batch     6 | loss: 1.3927503MemoryTrain:  epoch  8, batch     7 | loss: 1.2574295MemoryTrain:  epoch  8, batch     8 | loss: 1.2800373MemoryTrain:  epoch  8, batch     9 | loss: 1.3611424MemoryTrain:  epoch  8, batch    10 | loss: 1.4174889MemoryTrain:  epoch  8, batch    11 | loss: 1.4156481MemoryTrain:  epoch  9, batch     0 | loss: 1.3374825MemoryTrain:  epoch  9, batch     1 | loss: 1.3298225MemoryTrain:  epoch  9, batch     2 | loss: 1.3102921MemoryTrain:  epoch  9, batch     3 | loss: 1.4247859MemoryTrain:  epoch  9, batch     4 | loss: 1.2550979MemoryTrain:  epoch  9, batch     5 | loss: 1.3089800MemoryTrain:  epoch  9, batch     6 | loss: 1.3414111MemoryTrain:  epoch  9, batch     7 | loss: 1.2544854MemoryTrain:  epoch  9, batch     8 | loss: 1.2119713MemoryTrain:  epoch  9, batch     9 | loss: 1.3209945MemoryTrain:  epoch  9, batch    10 | loss: 1.3458440MemoryTrain:  epoch  9, batch    11 | loss: 1.2204839
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 79.76%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 79.89%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 80.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 80.53%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 80.83%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 81.05%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 80.47%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 80.11%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 79.41%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 78.93%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 78.82%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 78.55%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 78.45%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 78.04%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 77.97%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 77.74%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 77.68%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 77.47%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 77.41%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 76.53%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 75.95%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 75.13%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 73.98%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 73.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 75.80%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 76.23%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 76.94%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 77.12%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 77.40%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 77.77%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 78.02%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 77.48%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.34%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.37%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.96%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.64%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.20%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 81.07%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 80.91%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 81.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.47%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 82.99%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.10%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 82.92%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 82.88%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 82.45%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 82.16%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 82.40%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 82.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 82.48%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.90%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 83.07%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 83.26%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 83.00%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 82.00%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 81.57%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 81.46%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 80.85%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 80.56%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 80.37%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 80.48%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 80.30%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 79.85%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 79.78%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 79.71%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 79.64%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 79.40%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 78.99%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 78.77%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 78.46%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 78.58%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 78.62%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 78.65%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 78.61%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 78.40%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 78.36%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 78.55%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 78.43%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 78.09%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 77.60%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 77.21%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 76.74%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 76.58%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 76.70%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 77.22%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 77.27%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 77.51%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 77.76%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 77.93%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 78.16%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 78.39%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 78.83%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 79.25%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 79.60%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 79.61%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 79.75%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 79.88%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 79.95%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 79.67%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 79.17%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 78.96%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 78.52%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 78.38%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 78.07%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 77.93%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 78.02%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.21%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 78.23%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 78.31%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 78.39%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 78.31%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 77.66%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 77.12%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 76.49%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 75.86%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 75.30%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 74.70%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 74.60%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 74.41%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 74.41%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 74.37%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 74.42%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 74.48%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 74.62%   [EVAL] batch:  132 | acc: 68.75%,  total acc: 74.58%   [EVAL] batch:  133 | acc: 68.75%,  total acc: 74.53%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 74.58%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 74.63%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 74.68%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 74.64%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 74.24%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 73.97%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 73.58%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 73.33%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 73.12%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 72.87%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.24%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.74%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.88%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 73.55%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 73.19%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 72.96%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 72.81%   [EVAL] batch:  154 | acc: 31.25%,  total acc: 72.54%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 72.32%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 72.41%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 72.55%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 72.68%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 72.94%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  162 | acc: 81.25%,  total acc: 73.16%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 73.09%   [EVAL] batch:  164 | acc: 50.00%,  total acc: 72.95%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 72.93%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 72.75%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 72.69%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 72.60%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 72.43%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 72.26%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 72.09%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 71.89%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 71.77%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 71.75%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 71.73%   [EVAL] batch:  176 | acc: 68.75%,  total acc: 71.72%   [EVAL] batch:  177 | acc: 68.75%,  total acc: 71.70%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 71.75%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 71.96%   [EVAL] batch:  181 | acc: 81.25%,  total acc: 72.01%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 72.06%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 72.18%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 72.23%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 72.38%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 72.43%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 72.44%   [EVAL] batch:  188 | acc: 25.00%,  total acc: 72.19%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 71.88%   [EVAL] batch:  190 | acc: 25.00%,  total acc: 71.63%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 71.29%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 71.08%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 70.81%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 70.80%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 70.70%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 70.75%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 70.85%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 70.88%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 70.70%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 70.57%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 70.59%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 70.55%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 70.48%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 71.33%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 71.44%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 71.54%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  216 | acc: 87.50%,  total acc: 71.75%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 71.97%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 72.23%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 72.30%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 72.39%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 72.49%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:  225 | acc: 31.25%,  total acc: 72.43%   [EVAL] batch:  226 | acc: 25.00%,  total acc: 72.22%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 72.09%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 71.89%   [EVAL] batch:  229 | acc: 6.25%,  total acc: 71.60%   [EVAL] batch:  230 | acc: 37.50%,  total acc: 71.46%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 71.52%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 71.62%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 71.81%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 71.91%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 71.93%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 71.91%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 71.80%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 71.73%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 71.75%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 71.68%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 71.54%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 71.63%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 71.70%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 71.69%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 71.77%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 71.86%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 71.92%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 71.66%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 71.40%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 71.15%   [EVAL] batch:  253 | acc: 0.00%,  total acc: 70.87%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 70.59%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 70.31%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 70.28%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 70.32%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 70.48%   [EVAL] batch:  260 | acc: 93.75%,  total acc: 70.57%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 70.61%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 70.65%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 70.55%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 70.50%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 70.46%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 70.45%   [EVAL] batch:  268 | acc: 18.75%,  total acc: 70.26%   [EVAL] batch:  269 | acc: 0.00%,  total acc: 70.00%   [EVAL] batch:  270 | acc: 0.00%,  total acc: 69.74%   [EVAL] batch:  271 | acc: 0.00%,  total acc: 69.49%   [EVAL] batch:  272 | acc: 6.25%,  total acc: 69.25%   [EVAL] batch:  273 | acc: 0.00%,  total acc: 69.00%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 68.77%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 68.55%   [EVAL] batch:  276 | acc: 18.75%,  total acc: 68.37%   [EVAL] batch:  277 | acc: 12.50%,  total acc: 68.17%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 67.94%   [EVAL] batch:  279 | acc: 18.75%,  total acc: 67.77%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 67.53%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 67.51%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  283 | acc: 93.75%,  total acc: 67.67%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 67.85%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 67.90%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 67.93%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 67.99%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 68.17%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 68.21%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 68.41%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 68.45%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 68.45%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 68.46%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 68.48%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 68.54%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 68.58%   [EVAL] batch:  300 | acc: 62.50%,  total acc: 68.56%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 68.61%   [EVAL] batch:  302 | acc: 75.00%,  total acc: 68.63%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 68.67%   [EVAL] batch:  304 | acc: 68.75%,  total acc: 68.67%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 68.69%   [EVAL] batch:  306 | acc: 50.00%,  total acc: 68.63%   [EVAL] batch:  307 | acc: 56.25%,  total acc: 68.59%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 68.51%   [EVAL] batch:  309 | acc: 68.75%,  total acc: 68.51%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 68.45%   [EVAL] batch:  311 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 68.45%   [EVAL] batch:  313 | acc: 68.75%,  total acc: 68.45%   [EVAL] batch:  314 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:  315 | acc: 56.25%,  total acc: 68.43%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 68.40%   [EVAL] batch:  317 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 68.48%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 68.71%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 68.77%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 68.83%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 68.90%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 68.92%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 68.90%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 68.94%   [EVAL] batch:  328 | acc: 81.25%,  total acc: 68.98%   [EVAL] batch:  329 | acc: 87.50%,  total acc: 69.03%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 69.05%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 69.11%   [EVAL] batch:  332 | acc: 75.00%,  total acc: 69.13%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:  334 | acc: 68.75%,  total acc: 69.20%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:  336 | acc: 81.25%,  total acc: 69.31%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 69.36%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 69.45%   [EVAL] batch:  340 | acc: 87.50%,  total acc: 69.50%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 69.52%   [EVAL] batch:  342 | acc: 81.25%,  total acc: 69.55%   [EVAL] batch:  343 | acc: 62.50%,  total acc: 69.53%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 69.57%   [EVAL] batch:  345 | acc: 50.00%,  total acc: 69.51%   [EVAL] batch:  346 | acc: 56.25%,  total acc: 69.47%   [EVAL] batch:  347 | acc: 75.00%,  total acc: 69.49%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 69.50%   [EVAL] batch:  349 | acc: 81.25%,  total acc: 69.54%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 69.50%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 69.51%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 69.51%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 69.53%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 69.54%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 69.54%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 69.52%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 69.41%   [EVAL] batch:  358 | acc: 50.00%,  total acc: 69.36%   [EVAL] batch:  359 | acc: 56.25%,  total acc: 69.32%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 69.23%   [EVAL] batch:  361 | acc: 43.75%,  total acc: 69.16%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 69.16%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 69.86%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 70.05%   
cur_acc:  ['0.9484', '0.7857', '0.7857', '0.7817', '0.5863', '0.7748']
his_acc:  ['0.9484', '0.8570', '0.8165', '0.7758', '0.7143', '0.7005']
CurrentTrain: epoch  0, batch     0 | loss: 5.2584648CurrentTrain: epoch  0, batch     1 | loss: 6.2537994CurrentTrain: epoch  0, batch     2 | loss: 5.3298521CurrentTrain: epoch  0, batch     3 | loss: 5.7750740CurrentTrain: epoch  1, batch     0 | loss: 4.8208828CurrentTrain: epoch  1, batch     1 | loss: 4.6489029CurrentTrain: epoch  1, batch     2 | loss: 4.1997552CurrentTrain: epoch  1, batch     3 | loss: 4.9582014CurrentTrain: epoch  2, batch     0 | loss: 3.9748435CurrentTrain: epoch  2, batch     1 | loss: 4.9994411CurrentTrain: epoch  2, batch     2 | loss: 3.8201280CurrentTrain: epoch  2, batch     3 | loss: 2.8240399CurrentTrain: epoch  3, batch     0 | loss: 3.3027406CurrentTrain: epoch  3, batch     1 | loss: 3.8705511CurrentTrain: epoch  3, batch     2 | loss: 3.8623323CurrentTrain: epoch  3, batch     3 | loss: 3.6978965CurrentTrain: epoch  4, batch     0 | loss: 4.1253619CurrentTrain: epoch  4, batch     1 | loss: 3.3281307CurrentTrain: epoch  4, batch     2 | loss: 2.5832460CurrentTrain: epoch  4, batch     3 | loss: 2.9371274CurrentTrain: epoch  5, batch     0 | loss: 3.0708408CurrentTrain: epoch  5, batch     1 | loss: 2.8814123CurrentTrain: epoch  5, batch     2 | loss: 3.1827307CurrentTrain: epoch  5, batch     3 | loss: 4.5859947CurrentTrain: epoch  6, batch     0 | loss: 3.3528166CurrentTrain: epoch  6, batch     1 | loss: 2.4392371CurrentTrain: epoch  6, batch     2 | loss: 2.6774826CurrentTrain: epoch  6, batch     3 | loss: 3.3985331CurrentTrain: epoch  7, batch     0 | loss: 2.9552102CurrentTrain: epoch  7, batch     1 | loss: 2.4098885CurrentTrain: epoch  7, batch     2 | loss: 2.4094024CurrentTrain: epoch  7, batch     3 | loss: 2.4118629CurrentTrain: epoch  8, batch     0 | loss: 2.6334887CurrentTrain: epoch  8, batch     1 | loss: 2.5579038CurrentTrain: epoch  8, batch     2 | loss: 2.1347036CurrentTrain: epoch  8, batch     3 | loss: 3.0910945CurrentTrain: epoch  9, batch     0 | loss: 2.1295524CurrentTrain: epoch  9, batch     1 | loss: 2.7232349CurrentTrain: epoch  9, batch     2 | loss: 2.3109865CurrentTrain: epoch  9, batch     3 | loss: 2.3698912
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of the United States are the main political subdivisions, with California being known for its diverse geography and economy.  
Head Entity: United States  
Tail Entity: California  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: heinz , is a german given name , a diminutive of heinrich and cognate of the given name henry .
Head Entity: heinrich
Tail Entity: henry
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as they share a common ancestor and many genetic traits.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In various historical texts, the ancient city of Byzantium is said to be the same as modern-day Istanbul, although the names and cultural significance have evolved over time.  
Head Entity: Byzantium  
Tail Entity: Istanbul  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Picture, highlighting its impactful storytelling.  
Head Entity: Best Picture  
Tail Entity: "Voices of Change"  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the calais region was defended by the 15th army under " generaloberst " ( colonel general ) hans von salmuth , and normandy by the 7th army commanded by " generaloberst " friedrich dollmann .
Head Entity: hans von salmuth
Tail Entity: generaloberst
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: during the battle of stalingrad, the soviet forces were led by general vasily chuikov, who held the rank of major general, while the german troops were commanded by general friedrich paulus.  
Head Entity: vasily chuikov  
Tail Entity: major general  

Relation: military rank  
Context: admiral horatio nelson was a key figure in the british navy during the napoleonic wars, famously holding the rank of vice admiral at the time of the battle of trafalgar.  
Head Entity: horatio nelson  
Tail Entity: vice admiral  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular science magazine was published by National Geographic Society, featuring groundbreaking research and stunning photography.  
Head Entity: National Geographic Society  
Tail Entity: science magazine  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first book in the series, "The Enchanted Forest," was published in 2015, followed by "The Hidden Valley" in 2017.  
Head Entity: The Enchanted Forest  
Tail Entity: The Hidden Valley  

Relation: followed by  
Context: The concert began with a stunning performance of Beethoven's Fifth Symphony, followed by a captivating rendition of Mozart's Requiem.  
Head Entity: Beethoven's Fifth Symphony  
Tail Entity: Mozart's Requiem  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: the headquarters of the company is situated in san francisco, where it has been operating since 2010.  
Head Entity: the company  
Tail Entity: san francisco  

Relation: work location  
Context: during her time at the university, she conducted research in boston, which greatly influenced her career.  
Head Entity: she  
Tail Entity: boston  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: Maria Gonzalez is a renowned chef known for her innovative culinary techniques and delicious recipes.  
Head Entity: Maria Gonzalez  
Tail Entity: chef  

Relation: occupation  
Context: Dr. James Smith has dedicated his life to research in the field of astrophysics, contributing significantly to our understanding of black holes.  
Head Entity: Dr. James Smith  
Tail Entity: astrophysicist  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a UNESCO World Heritage Site, attracting millions of tourists each year.  
Head Entity: Petra  
Tail Entity: UNESCO World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti (october 12, 1935 – september 6, 2007) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: adele (born may 5, 1988) is an english singer-songwriter known for her powerful voice and emotive ballads, which have earned her numerous awards and accolades.  
Head Entity: adele  
Tail Entity: powerful voice  
Mixup data size:  498
MixupTrain:  epoch  0, batch     0 | loss: 2.2816977MixupTrain:  epoch  0, batch     1 | loss: 2.4448867MixupTrain:  epoch  0, batch     2 | loss: 2.3483028MixupTrain:  epoch  0, batch     3 | loss: 2.5171423MixupTrain:  epoch  0, batch     4 | loss: 2.2099522MixupTrain:  epoch  0, batch     5 | loss: 1.8653634MixupTrain:  epoch  0, batch     6 | loss: 2.1990556MixupTrain:  epoch  0, batch     7 | loss: 2.2876265MixupTrain:  epoch  0, batch     8 | loss: 2.3309373MixupTrain:  epoch  0, batch     9 | loss: 2.1336912MixupTrain:  epoch  0, batch    10 | loss: 2.1801678MixupTrain:  epoch  0, batch    11 | loss: 2.1475961MixupTrain:  epoch  0, batch    12 | loss: 1.9229607MixupTrain:  epoch  0, batch    13 | loss: 1.7012091MixupTrain:  epoch  0, batch    14 | loss: 1.7845633MixupTrain:  epoch  0, batch    15 | loss: 1.8814087MixupTrain:  epoch  0, batch    16 | loss: 1.7656348MixupTrain:  epoch  0, batch    17 | loss: 2.0663076MixupTrain:  epoch  0, batch    18 | loss: 2.1919976MixupTrain:  epoch  0, batch    19 | loss: 2.1475714MixupTrain:  epoch  0, batch    20 | loss: 2.1857853MixupTrain:  epoch  0, batch    21 | loss: 1.9567122MixupTrain:  epoch  0, batch    22 | loss: 1.8016962MixupTrain:  epoch  0, batch    23 | loss: 2.1786292MixupTrain:  epoch  0, batch    24 | loss: 2.2194315MixupTrain:  epoch  0, batch    25 | loss: 1.7933447MixupTrain:  epoch  0, batch    26 | loss: 1.9536173MixupTrain:  epoch  0, batch    27 | loss: 1.9462300MixupTrain:  epoch  0, batch    28 | loss: 2.2374987MixupTrain:  epoch  0, batch    29 | loss: 1.7336634MixupTrain:  epoch  0, batch    30 | loss: 1.8123212MixupTrain:  epoch  0, batch    31 | loss: 1.2695280
MemoryTrain:  epoch  0, batch     0 | loss: 2.1568105MemoryTrain:  epoch  0, batch     1 | loss: 1.8655653MemoryTrain:  epoch  0, batch     2 | loss: 2.0103059MemoryTrain:  epoch  0, batch     3 | loss: 2.1938765MemoryTrain:  epoch  0, batch     4 | loss: 1.8874017MemoryTrain:  epoch  0, batch     5 | loss: 2.1115265MemoryTrain:  epoch  0, batch     6 | loss: 2.1959362MemoryTrain:  epoch  0, batch     7 | loss: 1.9363295MemoryTrain:  epoch  0, batch     8 | loss: 2.0046530MemoryTrain:  epoch  0, batch     9 | loss: 2.8108530MemoryTrain:  epoch  0, batch    10 | loss: 1.6008908MemoryTrain:  epoch  0, batch    11 | loss: 2.4621406MemoryTrain:  epoch  0, batch    12 | loss: 1.9774137MemoryTrain:  epoch  0, batch    13 | loss: 3.3639531MemoryTrain:  epoch  1, batch     0 | loss: 1.9950249MemoryTrain:  epoch  1, batch     1 | loss: 2.1416125MemoryTrain:  epoch  1, batch     2 | loss: 1.5317451MemoryTrain:  epoch  1, batch     3 | loss: 1.7759016MemoryTrain:  epoch  1, batch     4 | loss: 1.9082488MemoryTrain:  epoch  1, batch     5 | loss: 1.8853910MemoryTrain:  epoch  1, batch     6 | loss: 1.9434350MemoryTrain:  epoch  1, batch     7 | loss: 1.4434340MemoryTrain:  epoch  1, batch     8 | loss: 2.1025686MemoryTrain:  epoch  1, batch     9 | loss: 1.9954125MemoryTrain:  epoch  1, batch    10 | loss: 1.7160259MemoryTrain:  epoch  1, batch    11 | loss: 2.1332371MemoryTrain:  epoch  1, batch    12 | loss: 1.3197579MemoryTrain:  epoch  1, batch    13 | loss: 1.2153378MemoryTrain:  epoch  2, batch     0 | loss: 1.4384799MemoryTrain:  epoch  2, batch     1 | loss: 1.7258480MemoryTrain:  epoch  2, batch     2 | loss: 1.3538899MemoryTrain:  epoch  2, batch     3 | loss: 1.8609695MemoryTrain:  epoch  2, batch     4 | loss: 1.5166334MemoryTrain:  epoch  2, batch     5 | loss: 1.5935242MemoryTrain:  epoch  2, batch     6 | loss: 1.5719719MemoryTrain:  epoch  2, batch     7 | loss: 1.7149125MemoryTrain:  epoch  2, batch     8 | loss: 1.4455153MemoryTrain:  epoch  2, batch     9 | loss: 1.5362905MemoryTrain:  epoch  2, batch    10 | loss: 1.7398419MemoryTrain:  epoch  2, batch    11 | loss: 1.7399461MemoryTrain:  epoch  2, batch    12 | loss: 1.5046551MemoryTrain:  epoch  2, batch    13 | loss: 1.4301822MemoryTrain:  epoch  3, batch     0 | loss: 1.5429171MemoryTrain:  epoch  3, batch     1 | loss: 1.5795395MemoryTrain:  epoch  3, batch     2 | loss: 1.5035639MemoryTrain:  epoch  3, batch     3 | loss: 1.3306315MemoryTrain:  epoch  3, batch     4 | loss: 1.4787385MemoryTrain:  epoch  3, batch     5 | loss: 1.4726571MemoryTrain:  epoch  3, batch     6 | loss: 1.6109958MemoryTrain:  epoch  3, batch     7 | loss: 1.6140783MemoryTrain:  epoch  3, batch     8 | loss: 1.6370336MemoryTrain:  epoch  3, batch     9 | loss: 1.2765749MemoryTrain:  epoch  3, batch    10 | loss: 1.4672788MemoryTrain:  epoch  3, batch    11 | loss: 1.7620373MemoryTrain:  epoch  3, batch    12 | loss: 1.4660363MemoryTrain:  epoch  3, batch    13 | loss: 1.3938433MemoryTrain:  epoch  4, batch     0 | loss: 1.4703602MemoryTrain:  epoch  4, batch     1 | loss: 1.5500062MemoryTrain:  epoch  4, batch     2 | loss: 1.6027060MemoryTrain:  epoch  4, batch     3 | loss: 1.3856058MemoryTrain:  epoch  4, batch     4 | loss: 1.2777402MemoryTrain:  epoch  4, batch     5 | loss: 1.9408102MemoryTrain:  epoch  4, batch     6 | loss: 1.4542211MemoryTrain:  epoch  4, batch     7 | loss: 1.3791333MemoryTrain:  epoch  4, batch     8 | loss: 1.3166056MemoryTrain:  epoch  4, batch     9 | loss: 1.5086708MemoryTrain:  epoch  4, batch    10 | loss: 1.4210980MemoryTrain:  epoch  4, batch    11 | loss: 1.3819772MemoryTrain:  epoch  4, batch    12 | loss: 1.3836631MemoryTrain:  epoch  4, batch    13 | loss: 1.1551681MemoryTrain:  epoch  5, batch     0 | loss: 1.2896336MemoryTrain:  epoch  5, batch     1 | loss: 1.3622094MemoryTrain:  epoch  5, batch     2 | loss: 1.4073585MemoryTrain:  epoch  5, batch     3 | loss: 1.6587913MemoryTrain:  epoch  5, batch     4 | loss: 1.5349773MemoryTrain:  epoch  5, batch     5 | loss: 1.5363767MemoryTrain:  epoch  5, batch     6 | loss: 1.4614098MemoryTrain:  epoch  5, batch     7 | loss: 1.4158368MemoryTrain:  epoch  5, batch     8 | loss: 1.3334446MemoryTrain:  epoch  5, batch     9 | loss: 1.3615927MemoryTrain:  epoch  5, batch    10 | loss: 1.3959221MemoryTrain:  epoch  5, batch    11 | loss: 1.3283329MemoryTrain:  epoch  5, batch    12 | loss: 1.2704582MemoryTrain:  epoch  5, batch    13 | loss: 1.4029679MemoryTrain:  epoch  6, batch     0 | loss: 1.3058258MemoryTrain:  epoch  6, batch     1 | loss: 1.2657409MemoryTrain:  epoch  6, batch     2 | loss: 1.4776205MemoryTrain:  epoch  6, batch     3 | loss: 1.4149790MemoryTrain:  epoch  6, batch     4 | loss: 1.5994403MemoryTrain:  epoch  6, batch     5 | loss: 1.3204722MemoryTrain:  epoch  6, batch     6 | loss: 1.2810593MemoryTrain:  epoch  6, batch     7 | loss: 1.2057214MemoryTrain:  epoch  6, batch     8 | loss: 1.3604118MemoryTrain:  epoch  6, batch     9 | loss: 1.3538595MemoryTrain:  epoch  6, batch    10 | loss: 1.4083209MemoryTrain:  epoch  6, batch    11 | loss: 1.3106691MemoryTrain:  epoch  6, batch    12 | loss: 1.4487743MemoryTrain:  epoch  6, batch    13 | loss: 1.2956674MemoryTrain:  epoch  7, batch     0 | loss: 1.2449369MemoryTrain:  epoch  7, batch     1 | loss: 1.2605730MemoryTrain:  epoch  7, batch     2 | loss: 1.2117084MemoryTrain:  epoch  7, batch     3 | loss: 1.2350259MemoryTrain:  epoch  7, batch     4 | loss: 1.2415876MemoryTrain:  epoch  7, batch     5 | loss: 1.3386919MemoryTrain:  epoch  7, batch     6 | loss: 1.5938605MemoryTrain:  epoch  7, batch     7 | loss: 1.2529540MemoryTrain:  epoch  7, batch     8 | loss: 1.3349116MemoryTrain:  epoch  7, batch     9 | loss: 1.2722505MemoryTrain:  epoch  7, batch    10 | loss: 1.3327879MemoryTrain:  epoch  7, batch    11 | loss: 1.3906335MemoryTrain:  epoch  7, batch    12 | loss: 1.3518064MemoryTrain:  epoch  7, batch    13 | loss: 1.1931522MemoryTrain:  epoch  8, batch     0 | loss: 1.2313612MemoryTrain:  epoch  8, batch     1 | loss: 1.2419221MemoryTrain:  epoch  8, batch     2 | loss: 1.3959419MemoryTrain:  epoch  8, batch     3 | loss: 1.2749488MemoryTrain:  epoch  8, batch     4 | loss: 1.3105808MemoryTrain:  epoch  8, batch     5 | loss: 1.3765721MemoryTrain:  epoch  8, batch     6 | loss: 1.2972472MemoryTrain:  epoch  8, batch     7 | loss: 1.2216702MemoryTrain:  epoch  8, batch     8 | loss: 1.2805324MemoryTrain:  epoch  8, batch     9 | loss: 1.2375808MemoryTrain:  epoch  8, batch    10 | loss: 1.2633337MemoryTrain:  epoch  8, batch    11 | loss: 1.3118298MemoryTrain:  epoch  8, batch    12 | loss: 1.2723025MemoryTrain:  epoch  8, batch    13 | loss: 1.2183437MemoryTrain:  epoch  9, batch     0 | loss: 1.3449425MemoryTrain:  epoch  9, batch     1 | loss: 1.2538601MemoryTrain:  epoch  9, batch     2 | loss: 1.2079585MemoryTrain:  epoch  9, batch     3 | loss: 1.3026372MemoryTrain:  epoch  9, batch     4 | loss: 1.3557884MemoryTrain:  epoch  9, batch     5 | loss: 1.2951322MemoryTrain:  epoch  9, batch     6 | loss: 1.2454083MemoryTrain:  epoch  9, batch     7 | loss: 1.2896810MemoryTrain:  epoch  9, batch     8 | loss: 1.2543547MemoryTrain:  epoch  9, batch     9 | loss: 1.3264880MemoryTrain:  epoch  9, batch    10 | loss: 1.3307433MemoryTrain:  epoch  9, batch    11 | loss: 1.2541242MemoryTrain:  epoch  9, batch    12 | loss: 1.2367380MemoryTrain:  epoch  9, batch    13 | loss: 1.2016027
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 72.43%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.67%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 78.80%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.50%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 79.81%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 78.24%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 77.90%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 76.52%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 75.55%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 74.64%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 73.26%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 72.80%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 72.70%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 72.76%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 72.81%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 72.71%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 73.11%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 72.73%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 72.96%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 72.74%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 72.66%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 72.83%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 72.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 73.41%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 74.41%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 76.21%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 76.40%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 76.59%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 76.88%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 77.15%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 77.52%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 77.08%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 72.70%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 73.58%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 73.10%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 73.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 75.46%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 78.49%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.39%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 78.47%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 78.78%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 80.18%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 80.36%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 80.52%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 80.54%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 80.28%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 80.03%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 79.39%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 78.91%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 78.83%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 78.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 78.19%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 78.49%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 78.77%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 78.24%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 77.84%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 78.01%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 77.63%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 76.72%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 76.38%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 76.35%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 76.13%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 75.71%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 75.40%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 75.39%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 75.58%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 75.66%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 75.37%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 75.37%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 75.36%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 75.36%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 75.09%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 74.57%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 74.06%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 73.82%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 73.83%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 73.68%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 73.78%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 73.72%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 73.50%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 73.52%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 73.69%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 73.55%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 73.04%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 72.54%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 72.21%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 71.73%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 71.70%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 71.66%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 72.22%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   93 | acc: 81.25%,  total acc: 73.01%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 73.29%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 74.62%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 74.88%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 75.06%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 75.12%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 75.30%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 75.59%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 75.41%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 74.94%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 74.77%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 74.32%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 74.10%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 73.83%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 73.84%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 74.30%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 74.41%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 74.53%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 73.91%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 73.40%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 72.80%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 72.21%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 71.67%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 71.10%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 70.98%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 70.72%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 70.51%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 70.49%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 70.48%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 70.47%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 70.45%   [EVAL] batch:  132 | acc: 25.00%,  total acc: 70.11%   [EVAL] batch:  133 | acc: 56.25%,  total acc: 70.01%   [EVAL] batch:  134 | acc: 50.00%,  total acc: 69.86%   [EVAL] batch:  135 | acc: 43.75%,  total acc: 69.67%   [EVAL] batch:  136 | acc: 62.50%,  total acc: 69.62%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 69.47%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 69.15%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 68.88%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 68.48%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 68.31%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 68.18%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 67.97%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.54%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.12%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 68.83%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 68.50%   [EVAL] batch:  152 | acc: 18.75%,  total acc: 68.18%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 67.98%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 67.66%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 67.35%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 67.44%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 67.60%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:  159 | acc: 100.00%,  total acc: 68.01%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 68.13%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 68.44%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 68.37%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 68.37%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 68.37%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 68.23%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 68.23%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 68.16%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 68.01%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 67.87%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 67.70%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 67.45%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 67.35%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 67.32%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 67.33%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 67.23%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 67.28%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 67.32%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 67.43%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 67.51%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 67.55%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 67.62%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 67.76%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 67.84%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 67.98%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 68.05%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 68.02%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 67.69%   [EVAL] batch:  189 | acc: 18.75%,  total acc: 67.43%   [EVAL] batch:  190 | acc: 6.25%,  total acc: 67.11%   [EVAL] batch:  191 | acc: 0.00%,  total acc: 66.76%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 66.48%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 66.24%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 66.17%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 66.21%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 66.29%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 66.30%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 66.31%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 66.29%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 66.18%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 66.07%   [EVAL] batch:  203 | acc: 87.50%,  total acc: 66.18%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 66.16%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 66.11%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:  207 | acc: 87.50%,  total acc: 66.32%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 66.74%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 67.14%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 67.27%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  216 | acc: 87.50%,  total acc: 67.51%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 67.78%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 68.19%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  225 | acc: 31.25%,  total acc: 68.39%   [EVAL] batch:  226 | acc: 37.50%,  total acc: 68.25%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 68.15%   [EVAL] batch:  228 | acc: 25.00%,  total acc: 67.96%   [EVAL] batch:  229 | acc: 6.25%,  total acc: 67.69%   [EVAL] batch:  230 | acc: 43.75%,  total acc: 67.59%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 67.65%   [EVAL] batch:  232 | acc: 81.25%,  total acc: 67.70%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 67.76%   [EVAL] batch:  234 | acc: 81.25%,  total acc: 67.82%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 67.85%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 67.91%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 67.91%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 67.94%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 67.89%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 67.87%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 67.90%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 67.82%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 67.70%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 67.81%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 67.86%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 67.86%   [EVAL] batch:  247 | acc: 87.50%,  total acc: 67.94%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 68.05%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 68.12%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 67.88%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 67.63%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 67.37%   [EVAL] batch:  253 | acc: 0.00%,  total acc: 67.10%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 66.84%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 66.58%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 66.56%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 66.62%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 66.72%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 66.80%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 66.88%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 66.94%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 66.99%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 66.90%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 66.86%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 66.92%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 66.85%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 66.84%   [EVAL] batch:  268 | acc: 18.75%,  total acc: 66.66%   [EVAL] batch:  269 | acc: 6.25%,  total acc: 66.44%   [EVAL] batch:  270 | acc: 0.00%,  total acc: 66.19%   [EVAL] batch:  271 | acc: 6.25%,  total acc: 65.97%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 65.80%   [EVAL] batch:  273 | acc: 0.00%,  total acc: 65.56%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 65.34%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 65.13%   [EVAL] batch:  276 | acc: 18.75%,  total acc: 64.96%   [EVAL] batch:  277 | acc: 12.50%,  total acc: 64.77%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 64.56%   [EVAL] batch:  279 | acc: 25.00%,  total acc: 64.42%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 64.19%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 64.18%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 64.27%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 64.33%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 64.43%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 64.51%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 64.55%   [EVAL] batch:  287 | acc: 81.25%,  total acc: 64.61%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 64.68%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 64.88%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 64.94%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 65.14%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 65.06%   [EVAL] batch:  295 | acc: 25.00%,  total acc: 64.93%   [EVAL] batch:  296 | acc: 18.75%,  total acc: 64.77%   [EVAL] batch:  297 | acc: 31.25%,  total acc: 64.66%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 64.59%   [EVAL] batch:  299 | acc: 31.25%,  total acc: 64.48%   [EVAL] batch:  300 | acc: 50.00%,  total acc: 64.43%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 64.51%   [EVAL] batch:  302 | acc: 56.25%,  total acc: 64.48%   [EVAL] batch:  303 | acc: 75.00%,  total acc: 64.51%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 64.55%   [EVAL] batch:  305 | acc: 62.50%,  total acc: 64.54%   [EVAL] batch:  306 | acc: 56.25%,  total acc: 64.52%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 64.47%   [EVAL] batch:  308 | acc: 37.50%,  total acc: 64.38%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 64.38%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 64.33%   [EVAL] batch:  311 | acc: 75.00%,  total acc: 64.36%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 64.32%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 64.31%   [EVAL] batch:  314 | acc: 68.75%,  total acc: 64.33%   [EVAL] batch:  315 | acc: 50.00%,  total acc: 64.28%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 64.25%   [EVAL] batch:  317 | acc: 68.75%,  total acc: 64.27%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 64.30%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 64.41%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 64.52%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 64.58%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 64.65%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 64.72%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 64.81%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 64.80%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 64.77%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 64.79%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 64.82%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 64.85%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 64.82%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 64.85%   [EVAL] batch:  332 | acc: 68.75%,  total acc: 64.86%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:  334 | acc: 68.75%,  total acc: 64.96%   [EVAL] batch:  335 | acc: 81.25%,  total acc: 65.01%   [EVAL] batch:  336 | acc: 87.50%,  total acc: 65.08%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 65.14%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 65.17%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 65.20%   [EVAL] batch:  340 | acc: 81.25%,  total acc: 65.25%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 65.28%   [EVAL] batch:  342 | acc: 62.50%,  total acc: 65.27%   [EVAL] batch:  343 | acc: 56.25%,  total acc: 65.24%   [EVAL] batch:  344 | acc: 75.00%,  total acc: 65.27%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 65.25%   [EVAL] batch:  346 | acc: 56.25%,  total acc: 65.22%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 65.21%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 65.24%   [EVAL] batch:  349 | acc: 68.75%,  total acc: 65.25%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 65.26%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 65.29%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 65.32%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 65.34%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 65.39%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 65.40%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 65.39%   [EVAL] batch:  357 | acc: 37.50%,  total acc: 65.31%   [EVAL] batch:  358 | acc: 50.00%,  total acc: 65.27%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 65.23%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 65.15%   [EVAL] batch:  361 | acc: 43.75%,  total acc: 65.09%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 65.08%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 65.27%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 65.37%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 65.82%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 65.89%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 65.97%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 66.12%   [EVAL] batch:  375 | acc: 43.75%,  total acc: 66.06%   [EVAL] batch:  376 | acc: 68.75%,  total acc: 66.06%   [EVAL] batch:  377 | acc: 75.00%,  total acc: 66.09%   [EVAL] batch:  378 | acc: 43.75%,  total acc: 66.03%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 66.02%   [EVAL] batch:  380 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 66.07%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 66.12%   [EVAL] batch:  383 | acc: 81.25%,  total acc: 66.16%   [EVAL] batch:  384 | acc: 68.75%,  total acc: 66.17%   [EVAL] batch:  385 | acc: 56.25%,  total acc: 66.14%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 66.13%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 66.24%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 66.30%   [EVAL] batch:  390 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 66.39%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 66.44%   [EVAL] batch:  393 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 66.60%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 67.00%   [EVAL] batch:  401 | acc: 37.50%,  total acc: 66.93%   [EVAL] batch:  402 | acc: 68.75%,  total acc: 66.94%   [EVAL] batch:  403 | acc: 68.75%,  total acc: 66.94%   [EVAL] batch:  404 | acc: 56.25%,  total acc: 66.91%   [EVAL] batch:  405 | acc: 56.25%,  total acc: 66.89%   [EVAL] batch:  406 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:  407 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:  408 | acc: 43.75%,  total acc: 66.90%   [EVAL] batch:  409 | acc: 43.75%,  total acc: 66.84%   [EVAL] batch:  410 | acc: 25.00%,  total acc: 66.74%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 66.72%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 66.72%   [EVAL] batch:  413 | acc: 75.00%,  total acc: 66.74%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:  415 | acc: 68.75%,  total acc: 66.77%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 66.82%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 66.84%   [EVAL] batch:  418 | acc: 56.25%,  total acc: 66.81%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 66.85%   [EVAL] batch:  420 | acc: 75.00%,  total acc: 66.86%   [EVAL] batch:  421 | acc: 62.50%,  total acc: 66.85%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 66.86%   [EVAL] batch:  423 | acc: 81.25%,  total acc: 66.89%   [EVAL] batch:  424 | acc: 75.00%,  total acc: 66.91%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 67.37%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 67.45%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 67.49%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 67.54%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 67.60%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 67.69%   
cur_acc:  ['0.9484', '0.7857', '0.7857', '0.7817', '0.5863', '0.7748', '0.7708']
his_acc:  ['0.9484', '0.8570', '0.8165', '0.7758', '0.7143', '0.7005', '0.6769']
CurrentTrain: epoch  0, batch     0 | loss: 4.7110653CurrentTrain: epoch  0, batch     1 | loss: 4.6671162CurrentTrain: epoch  0, batch     2 | loss: 4.9502921CurrentTrain: epoch  0, batch     3 | loss: 6.2354326CurrentTrain: epoch  1, batch     0 | loss: 4.3164425CurrentTrain: epoch  1, batch     1 | loss: 3.5889070CurrentTrain: epoch  1, batch     2 | loss: 3.5429063CurrentTrain: epoch  1, batch     3 | loss: 2.4360514CurrentTrain: epoch  2, batch     0 | loss: 3.8889482CurrentTrain: epoch  2, batch     1 | loss: 2.7213335CurrentTrain: epoch  2, batch     2 | loss: 3.0122032CurrentTrain: epoch  2, batch     3 | loss: 2.3049195CurrentTrain: epoch  3, batch     0 | loss: 3.1586313CurrentTrain: epoch  3, batch     1 | loss: 2.7224655CurrentTrain: epoch  3, batch     2 | loss: 2.9145963CurrentTrain: epoch  3, batch     3 | loss: 2.1141186CurrentTrain: epoch  4, batch     0 | loss: 2.7442091CurrentTrain: epoch  4, batch     1 | loss: 2.5774331CurrentTrain: epoch  4, batch     2 | loss: 2.7062349CurrentTrain: epoch  4, batch     3 | loss: 2.9897518CurrentTrain: epoch  5, batch     0 | loss: 3.0326815CurrentTrain: epoch  5, batch     1 | loss: 2.2243452CurrentTrain: epoch  5, batch     2 | loss: 2.4694862CurrentTrain: epoch  5, batch     3 | loss: 2.5559909CurrentTrain: epoch  6, batch     0 | loss: 2.3758948CurrentTrain: epoch  6, batch     1 | loss: 2.3943667CurrentTrain: epoch  6, batch     2 | loss: 2.4074399CurrentTrain: epoch  6, batch     3 | loss: 1.9654801CurrentTrain: epoch  7, batch     0 | loss: 2.5964022CurrentTrain: epoch  7, batch     1 | loss: 2.0809121CurrentTrain: epoch  7, batch     2 | loss: 2.0538816CurrentTrain: epoch  7, batch     3 | loss: 1.8602815CurrentTrain: epoch  8, batch     0 | loss: 1.9567680CurrentTrain: epoch  8, batch     1 | loss: 2.2384651CurrentTrain: epoch  8, batch     2 | loss: 2.1379275CurrentTrain: epoch  8, batch     3 | loss: 1.8462856CurrentTrain: epoch  9, batch     0 | loss: 1.9545124CurrentTrain: epoch  9, batch     1 | loss: 1.8834746CurrentTrain: epoch  9, batch     2 | loss: 2.1518292CurrentTrain: epoch  9, batch     3 | loss: 1.8213794
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: elizabeth warren is a prominent member of the democratic party, advocating for progressive policies and reforms.  
Head Entity: elizabeth warren  
Tail Entity: democratic party  

Relation: member of political party  
Context: during his tenure in the senate, john mccain was a long-time member of the republican party, known for his bipartisan efforts.  
Head Entity: john mccain  
Tail Entity: republican party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the original story of real life escape of betty mahmoody is depicted in the movie " not without my daughter " which itself was based on betty mahmoody 's book of the same name .
Head Entity: not without my daughter
Tail Entity: betty mahmoody
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "the great gatsby" draws heavily from f. scott fitzgerald's classic novel, capturing the essence of the roaring twenties and the complexities of love and ambition.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: the animated feature "the lion king" was inspired by shakespeare's play "hamlet," incorporating themes of betrayal, revenge, and the struggle for power.  
Head Entity: the lion king  
Tail Entity: shakespeare
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: several paintings are also mentioned , including rousseau 's " the dream " and van gogh 's " bedroom in arles " .
Head Entity: rousseau
Tail Entity: the dream
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: Among his many contributions to literature, one of his most acclaimed novels is "The Great Gatsby," which explores themes of wealth and social change in 1920s America.  
Head Entity: F. Scott Fitzgerald  
Tail Entity: The Great Gatsby  

Relation: notable work  
Context: The artist is best known for his groundbreaking sculpture "The Thinker," which has become a symbol of philosophy and contemplation.  
Head Entity: Auguste Rodin  
Tail Entity: The Thinker  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after years of expansion, the non-profit organization moved its headquarters to new york city to better serve its growing membership.  
Head Entity: non-profit organization  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: since march 2006 , wkxp 's programming has been simulcasted on 97.3 wzad wurtsboro , new york .
Head Entity: wzad
Tail Entity: wurtsboro , new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: in 2010, the station wxyz began broadcasting to the community of springfield, illinois.  
Head Entity: wxyz  
Tail Entity: springfield, illinois  

Relation: licensed to broadcast to  
Context: the radio station kqrs has been granted a license to broadcast to the area of minneapolis, minnesota since 1995.  
Head Entity: kqrs  
Tail Entity: minneapolis, minnesota  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion, which is one of the most prominent constellations in the night sky.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major, known for being the brightest star in the night sky.  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to various destinations through the El Prat Airport, which offers flights to numerous European cities.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub in the region.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Mixup data size:  558
MixupTrain:  epoch  0, batch     0 | loss: 1.9251097MixupTrain:  epoch  0, batch     1 | loss: 1.7279934MixupTrain:  epoch  0, batch     2 | loss: 1.4860698MixupTrain:  epoch  0, batch     3 | loss: 1.6575673MixupTrain:  epoch  0, batch     4 | loss: 1.5803685MixupTrain:  epoch  0, batch     5 | loss: 1.5056917MixupTrain:  epoch  0, batch     6 | loss: 1.4578135MixupTrain:  epoch  0, batch     7 | loss: 1.4480709MixupTrain:  epoch  0, batch     8 | loss: 1.6016536MixupTrain:  epoch  0, batch     9 | loss: 1.3814611MixupTrain:  epoch  0, batch    10 | loss: 1.4942103MixupTrain:  epoch  0, batch    11 | loss: 1.5166093MixupTrain:  epoch  0, batch    12 | loss: 1.8284850MixupTrain:  epoch  0, batch    13 | loss: 1.8028587MixupTrain:  epoch  0, batch    14 | loss: 1.5868170MixupTrain:  epoch  0, batch    15 | loss: 1.6004350MixupTrain:  epoch  0, batch    16 | loss: 1.6352898MixupTrain:  epoch  0, batch    17 | loss: 1.4257018MixupTrain:  epoch  0, batch    18 | loss: 1.5105628MixupTrain:  epoch  0, batch    19 | loss: 1.6279726MixupTrain:  epoch  0, batch    20 | loss: 1.5149070MixupTrain:  epoch  0, batch    21 | loss: 1.4241624MixupTrain:  epoch  0, batch    22 | loss: 1.6092700MixupTrain:  epoch  0, batch    23 | loss: 1.4204721MixupTrain:  epoch  0, batch    24 | loss: 1.6845754MixupTrain:  epoch  0, batch    25 | loss: 1.8426770MixupTrain:  epoch  0, batch    26 | loss: 1.3954572MixupTrain:  epoch  0, batch    27 | loss: 1.7761717MixupTrain:  epoch  0, batch    28 | loss: 1.5733028MixupTrain:  epoch  0, batch    29 | loss: 1.8335345MixupTrain:  epoch  0, batch    30 | loss: 1.6755154MixupTrain:  epoch  0, batch    31 | loss: 1.6788078MixupTrain:  epoch  0, batch    32 | loss: 1.5247146MixupTrain:  epoch  0, batch    33 | loss: 1.5567007MixupTrain:  epoch  0, batch    34 | loss: 1.5773760
MemoryTrain:  epoch  0, batch     0 | loss: 1.5172343MemoryTrain:  epoch  0, batch     1 | loss: 1.2193041MemoryTrain:  epoch  0, batch     2 | loss: 1.6329628MemoryTrain:  epoch  0, batch     3 | loss: 1.3504388MemoryTrain:  epoch  0, batch     4 | loss: 1.4940016MemoryTrain:  epoch  0, batch     5 | loss: 2.2884438MemoryTrain:  epoch  0, batch     6 | loss: 1.3692213MemoryTrain:  epoch  0, batch     7 | loss: 1.9549723MemoryTrain:  epoch  0, batch     8 | loss: 1.7870495MemoryTrain:  epoch  0, batch     9 | loss: 1.7359184MemoryTrain:  epoch  0, batch    10 | loss: 2.0345950MemoryTrain:  epoch  0, batch    11 | loss: 1.7951045MemoryTrain:  epoch  0, batch    12 | loss: 1.8094568MemoryTrain:  epoch  0, batch    13 | loss: 1.9440534MemoryTrain:  epoch  0, batch    14 | loss: 1.8080442MemoryTrain:  epoch  1, batch     0 | loss: 1.4006822MemoryTrain:  epoch  1, batch     1 | loss: 1.3202219MemoryTrain:  epoch  1, batch     2 | loss: 1.5388842MemoryTrain:  epoch  1, batch     3 | loss: 1.5530181MemoryTrain:  epoch  1, batch     4 | loss: 1.3164914MemoryTrain:  epoch  1, batch     5 | loss: 1.7482784MemoryTrain:  epoch  1, batch     6 | loss: 1.3587430MemoryTrain:  epoch  1, batch     7 | loss: 1.4690695MemoryTrain:  epoch  1, batch     8 | loss: 2.0467434MemoryTrain:  epoch  1, batch     9 | loss: 1.4804854MemoryTrain:  epoch  1, batch    10 | loss: 1.2790385MemoryTrain:  epoch  1, batch    11 | loss: 1.3636451MemoryTrain:  epoch  1, batch    12 | loss: 1.4618965MemoryTrain:  epoch  1, batch    13 | loss: 1.3175815MemoryTrain:  epoch  1, batch    14 | loss: 1.5961833MemoryTrain:  epoch  2, batch     0 | loss: 1.5163510MemoryTrain:  epoch  2, batch     1 | loss: 1.5952675MemoryTrain:  epoch  2, batch     2 | loss: 1.2820096MemoryTrain:  epoch  2, batch     3 | loss: 1.4029756MemoryTrain:  epoch  2, batch     4 | loss: 1.2242713MemoryTrain:  epoch  2, batch     5 | loss: 1.3257257MemoryTrain:  epoch  2, batch     6 | loss: 1.5821021MemoryTrain:  epoch  2, batch     7 | loss: 1.2149893MemoryTrain:  epoch  2, batch     8 | loss: 1.4745762MemoryTrain:  epoch  2, batch     9 | loss: 1.2760490MemoryTrain:  epoch  2, batch    10 | loss: 1.4061841MemoryTrain:  epoch  2, batch    11 | loss: 1.5398586MemoryTrain:  epoch  2, batch    12 | loss: 1.3389678MemoryTrain:  epoch  2, batch    13 | loss: 1.2932330MemoryTrain:  epoch  2, batch    14 | loss: 1.4790742MemoryTrain:  epoch  3, batch     0 | loss: 1.2726450MemoryTrain:  epoch  3, batch     1 | loss: 1.3557512MemoryTrain:  epoch  3, batch     2 | loss: 1.9427874MemoryTrain:  epoch  3, batch     3 | loss: 1.4283851MemoryTrain:  epoch  3, batch     4 | loss: 1.2438359MemoryTrain:  epoch  3, batch     5 | loss: 1.2355210MemoryTrain:  epoch  3, batch     6 | loss: 1.3923086MemoryTrain:  epoch  3, batch     7 | loss: 1.3090680MemoryTrain:  epoch  3, batch     8 | loss: 1.4988304MemoryTrain:  epoch  3, batch     9 | loss: 1.3837346MemoryTrain:  epoch  3, batch    10 | loss: 1.5565650MemoryTrain:  epoch  3, batch    11 | loss: 1.5038422MemoryTrain:  epoch  3, batch    12 | loss: 1.3860378MemoryTrain:  epoch  3, batch    13 | loss: 1.2373440MemoryTrain:  epoch  3, batch    14 | loss: 1.2501674MemoryTrain:  epoch  4, batch     0 | loss: 1.2920144MemoryTrain:  epoch  4, batch     1 | loss: 1.2216733MemoryTrain:  epoch  4, batch     2 | loss: 1.2268577MemoryTrain:  epoch  4, batch     3 | loss: 1.2887943MemoryTrain:  epoch  4, batch     4 | loss: 1.4288162MemoryTrain:  epoch  4, batch     5 | loss: 1.5969809MemoryTrain:  epoch  4, batch     6 | loss: 1.2859591MemoryTrain:  epoch  4, batch     7 | loss: 1.2985678MemoryTrain:  epoch  4, batch     8 | loss: 1.3114052MemoryTrain:  epoch  4, batch     9 | loss: 1.2665138MemoryTrain:  epoch  4, batch    10 | loss: 1.2497660MemoryTrain:  epoch  4, batch    11 | loss: 1.3361187MemoryTrain:  epoch  4, batch    12 | loss: 1.4131279MemoryTrain:  epoch  4, batch    13 | loss: 1.2058166MemoryTrain:  epoch  4, batch    14 | loss: 1.2193246MemoryTrain:  epoch  5, batch     0 | loss: 1.3235117MemoryTrain:  epoch  5, batch     1 | loss: 1.2992003MemoryTrain:  epoch  5, batch     2 | loss: 1.4376658MemoryTrain:  epoch  5, batch     3 | loss: 1.1917408MemoryTrain:  epoch  5, batch     4 | loss: 1.2125633MemoryTrain:  epoch  5, batch     5 | loss: 1.2576402MemoryTrain:  epoch  5, batch     6 | loss: 1.2425549MemoryTrain:  epoch  5, batch     7 | loss: 1.2285306MemoryTrain:  epoch  5, batch     8 | loss: 1.2380087MemoryTrain:  epoch  5, batch     9 | loss: 1.2301177MemoryTrain:  epoch  5, batch    10 | loss: 1.2500074MemoryTrain:  epoch  5, batch    11 | loss: 1.2716022MemoryTrain:  epoch  5, batch    12 | loss: 1.2675533MemoryTrain:  epoch  5, batch    13 | loss: 1.2109926MemoryTrain:  epoch  5, batch    14 | loss: 1.2383890MemoryTrain:  epoch  6, batch     0 | loss: 1.2963234MemoryTrain:  epoch  6, batch     1 | loss: 1.2312576MemoryTrain:  epoch  6, batch     2 | loss: 1.2625952MemoryTrain:  epoch  6, batch     3 | loss: 1.2716790MemoryTrain:  epoch  6, batch     4 | loss: 1.2522924MemoryTrain:  epoch  6, batch     5 | loss: 1.2462971MemoryTrain:  epoch  6, batch     6 | loss: 1.2045670MemoryTrain:  epoch  6, batch     7 | loss: 1.2053543MemoryTrain:  epoch  6, batch     8 | loss: 1.2914016MemoryTrain:  epoch  6, batch     9 | loss: 1.3259853MemoryTrain:  epoch  6, batch    10 | loss: 1.1882582MemoryTrain:  epoch  6, batch    11 | loss: 1.2885517MemoryTrain:  epoch  6, batch    12 | loss: 1.2508855MemoryTrain:  epoch  6, batch    13 | loss: 1.2666142MemoryTrain:  epoch  6, batch    14 | loss: 1.2381237MemoryTrain:  epoch  7, batch     0 | loss: 1.2087246MemoryTrain:  epoch  7, batch     1 | loss: 1.2293259MemoryTrain:  epoch  7, batch     2 | loss: 1.2946837MemoryTrain:  epoch  7, batch     3 | loss: 1.2481455MemoryTrain:  epoch  7, batch     4 | loss: 1.2036340MemoryTrain:  epoch  7, batch     5 | loss: 1.2749252MemoryTrain:  epoch  7, batch     6 | loss: 1.2285337MemoryTrain:  epoch  7, batch     7 | loss: 1.2137399MemoryTrain:  epoch  7, batch     8 | loss: 1.2099886MemoryTrain:  epoch  7, batch     9 | loss: 1.2370527MemoryTrain:  epoch  7, batch    10 | loss: 1.2331326MemoryTrain:  epoch  7, batch    11 | loss: 1.2511382MemoryTrain:  epoch  7, batch    12 | loss: 1.2195835MemoryTrain:  epoch  7, batch    13 | loss: 1.1866151MemoryTrain:  epoch  7, batch    14 | loss: 1.2081738MemoryTrain:  epoch  8, batch     0 | loss: 1.1866500MemoryTrain:  epoch  8, batch     1 | loss: 1.3126847MemoryTrain:  epoch  8, batch     2 | loss: 1.2382581MemoryTrain:  epoch  8, batch     3 | loss: 1.2146871MemoryTrain:  epoch  8, batch     4 | loss: 1.2408227MemoryTrain:  epoch  8, batch     5 | loss: 1.2027363MemoryTrain:  epoch  8, batch     6 | loss: 1.2450043MemoryTrain:  epoch  8, batch     7 | loss: 1.2528279MemoryTrain:  epoch  8, batch     8 | loss: 1.1697600MemoryTrain:  epoch  8, batch     9 | loss: 1.1845716MemoryTrain:  epoch  8, batch    10 | loss: 1.2013488MemoryTrain:  epoch  8, batch    11 | loss: 1.2008526MemoryTrain:  epoch  8, batch    12 | loss: 1.2083668MemoryTrain:  epoch  8, batch    13 | loss: 1.2511663MemoryTrain:  epoch  8, batch    14 | loss: 1.2015834MemoryTrain:  epoch  9, batch     0 | loss: 1.2603076MemoryTrain:  epoch  9, batch     1 | loss: 1.1845858MemoryTrain:  epoch  9, batch     2 | loss: 1.2627730MemoryTrain:  epoch  9, batch     3 | loss: 1.2059200MemoryTrain:  epoch  9, batch     4 | loss: 1.1915590MemoryTrain:  epoch  9, batch     5 | loss: 1.2238493MemoryTrain:  epoch  9, batch     6 | loss: 1.1944759MemoryTrain:  epoch  9, batch     7 | loss: 1.2016895MemoryTrain:  epoch  9, batch     8 | loss: 1.1953083MemoryTrain:  epoch  9, batch     9 | loss: 1.1510612MemoryTrain:  epoch  9, batch    10 | loss: 1.1958208MemoryTrain:  epoch  9, batch    11 | loss: 1.1937065MemoryTrain:  epoch  9, batch    12 | loss: 1.1981514MemoryTrain:  epoch  9, batch    13 | loss: 1.2356876MemoryTrain:  epoch  9, batch    14 | loss: 1.2363932
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 90.83%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 90.44%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 86.88%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 86.01%   [EVAL] batch:   21 | acc: 56.25%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 62.50%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 84.66%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 83.93%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 83.68%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 82.73%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.99%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 84.23%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 84.59%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.80%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 85.46%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 85.77%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 86.07%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.35%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 86.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.64%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.78%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.91%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 87.15%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 87.16%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.28%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 87.39%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 87.71%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 87.91%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 88.00%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 87.40%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 67.50%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 74.63%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 74.64%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 74.65%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 74.66%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 75.16%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 75.80%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 76.83%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 77.47%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 77.70%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 77.04%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 76.46%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 75.91%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 75.77%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 75.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 75.36%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 75.71%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 75.46%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 75.11%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 75.22%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 74.89%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 74.14%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 73.83%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 73.85%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 73.77%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 73.59%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 73.31%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 73.05%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 73.17%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 72.82%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 72.67%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 72.70%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 72.64%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 72.68%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 72.45%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 72.05%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 71.58%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 71.37%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 71.50%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 71.46%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 71.59%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 71.47%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 71.44%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 71.41%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 71.60%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 71.49%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 71.08%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 70.61%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 70.29%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 69.84%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 69.76%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 69.60%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 69.93%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 69.78%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 70.09%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 70.01%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 71.24%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 72.30%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 72.51%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 72.72%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 73.05%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 72.90%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 72.45%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 72.31%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 71.68%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 71.43%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 71.52%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 71.66%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 71.90%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 72.04%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 72.17%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 72.35%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 72.32%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 71.72%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 71.23%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 70.65%   [EVAL] batch:  122 | acc: 0.00%,  total acc: 70.07%   [EVAL] batch:  123 | acc: 6.25%,  total acc: 69.56%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 69.00%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 68.90%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 68.70%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 68.46%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 68.41%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 68.37%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 68.23%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 68.23%   [EVAL] batch:  132 | acc: 37.50%,  total acc: 68.00%   [EVAL] batch:  133 | acc: 50.00%,  total acc: 67.86%   [EVAL] batch:  134 | acc: 50.00%,  total acc: 67.73%   [EVAL] batch:  135 | acc: 43.75%,  total acc: 67.56%   [EVAL] batch:  136 | acc: 50.00%,  total acc: 67.43%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 67.26%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 66.91%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 66.65%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 66.27%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 66.02%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 65.82%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 65.62%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 65.82%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 66.24%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 66.88%   [EVAL] batch:  150 | acc: 25.00%,  total acc: 66.60%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 66.28%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 66.01%   [EVAL] batch:  153 | acc: 37.50%,  total acc: 65.83%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 65.52%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 65.14%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 65.21%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 65.39%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 65.57%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 65.88%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 66.03%   [EVAL] batch:  163 | acc: 18.75%,  total acc: 65.74%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 65.45%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 65.21%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 64.93%   [EVAL] batch:  167 | acc: 31.25%,  total acc: 64.73%   [EVAL] batch:  168 | acc: 37.50%,  total acc: 64.57%   [EVAL] batch:  169 | acc: 43.75%,  total acc: 64.45%   [EVAL] batch:  170 | acc: 50.00%,  total acc: 64.36%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 64.32%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 64.16%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 64.15%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 64.18%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 64.20%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 64.16%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 64.22%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 64.25%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 64.47%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 64.53%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 64.69%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 64.97%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 65.27%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 65.23%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 64.91%   [EVAL] batch:  189 | acc: 6.25%,  total acc: 64.61%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 64.27%   [EVAL] batch:  191 | acc: 0.00%,  total acc: 63.93%   [EVAL] batch:  192 | acc: 12.50%,  total acc: 63.67%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 63.43%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 63.43%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 63.36%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 63.42%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 63.51%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 63.56%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 63.53%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 63.43%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 63.33%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 63.42%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 63.41%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 63.38%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 63.50%   [EVAL] batch:  207 | acc: 87.50%,  total acc: 63.61%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 63.93%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 64.07%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 64.38%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 64.52%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 64.62%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  216 | acc: 81.25%,  total acc: 64.86%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 65.13%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 65.57%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 65.70%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 65.82%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:  225 | acc: 25.00%,  total acc: 65.79%   [EVAL] batch:  226 | acc: 31.25%,  total acc: 65.64%   [EVAL] batch:  227 | acc: 31.25%,  total acc: 65.49%   [EVAL] batch:  228 | acc: 18.75%,  total acc: 65.28%   [EVAL] batch:  229 | acc: 6.25%,  total acc: 65.03%   [EVAL] batch:  230 | acc: 43.75%,  total acc: 64.94%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 65.03%   [EVAL] batch:  232 | acc: 87.50%,  total acc: 65.13%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 65.20%   [EVAL] batch:  234 | acc: 87.50%,  total acc: 65.29%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 65.33%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 65.40%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 65.41%   [EVAL] batch:  238 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 65.36%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 65.35%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 65.37%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 65.28%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 65.16%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 65.23%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 65.24%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 65.18%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 65.22%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 65.31%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 65.28%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 65.04%   [EVAL] batch:  251 | acc: 6.25%,  total acc: 64.81%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 64.58%   [EVAL] batch:  253 | acc: 0.00%,  total acc: 64.32%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 64.07%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 63.82%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 63.81%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 63.88%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 64.00%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 64.09%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 64.18%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 64.27%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 64.33%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 64.25%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 64.20%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 64.26%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 64.21%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 64.23%   [EVAL] batch:  268 | acc: 18.75%,  total acc: 64.06%   [EVAL] batch:  269 | acc: 0.00%,  total acc: 63.82%   [EVAL] batch:  270 | acc: 0.00%,  total acc: 63.58%   [EVAL] batch:  271 | acc: 6.25%,  total acc: 63.37%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 63.21%   [EVAL] batch:  273 | acc: 6.25%,  total acc: 63.00%   [EVAL] batch:  274 | acc: 0.00%,  total acc: 62.77%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 62.57%   [EVAL] batch:  276 | acc: 18.75%,  total acc: 62.41%   [EVAL] batch:  277 | acc: 6.25%,  total acc: 62.21%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 62.01%   [EVAL] batch:  279 | acc: 12.50%,  total acc: 61.83%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 61.61%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 61.57%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 61.66%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 61.69%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 61.80%   [EVAL] batch:  285 | acc: 81.25%,  total acc: 61.87%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 61.91%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 62.00%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 62.09%   [EVAL] batch:  289 | acc: 93.75%,  total acc: 62.20%   [EVAL] batch:  290 | acc: 87.50%,  total acc: 62.29%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 62.35%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 62.48%   [EVAL] batch:  293 | acc: 87.50%,  total acc: 62.56%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:  295 | acc: 37.50%,  total acc: 62.42%   [EVAL] batch:  296 | acc: 12.50%,  total acc: 62.25%   [EVAL] batch:  297 | acc: 12.50%,  total acc: 62.08%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 62.08%   [EVAL] batch:  299 | acc: 25.00%,  total acc: 61.96%   [EVAL] batch:  300 | acc: 43.75%,  total acc: 61.90%   [EVAL] batch:  301 | acc: 75.00%,  total acc: 61.94%   [EVAL] batch:  302 | acc: 37.50%,  total acc: 61.86%   [EVAL] batch:  303 | acc: 62.50%,  total acc: 61.86%   [EVAL] batch:  304 | acc: 56.25%,  total acc: 61.84%   [EVAL] batch:  305 | acc: 68.75%,  total acc: 61.87%   [EVAL] batch:  306 | acc: 43.75%,  total acc: 61.81%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 61.77%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 61.71%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 61.71%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 61.68%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 61.76%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 61.76%   [EVAL] batch:  313 | acc: 68.75%,  total acc: 61.78%   [EVAL] batch:  314 | acc: 75.00%,  total acc: 61.83%   [EVAL] batch:  315 | acc: 50.00%,  total acc: 61.79%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 61.77%   [EVAL] batch:  317 | acc: 68.75%,  total acc: 61.79%   [EVAL] batch:  318 | acc: 75.00%,  total acc: 61.83%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 61.95%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 62.07%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 62.13%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 62.21%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 62.29%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 62.38%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 62.42%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 62.40%   [EVAL] batch:  327 | acc: 62.50%,  total acc: 62.40%   [EVAL] batch:  328 | acc: 87.50%,  total acc: 62.48%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 62.54%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 62.54%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 62.59%   [EVAL] batch:  332 | acc: 68.75%,  total acc: 62.61%   [EVAL] batch:  333 | acc: 87.50%,  total acc: 62.69%   [EVAL] batch:  334 | acc: 56.25%,  total acc: 62.67%   [EVAL] batch:  335 | acc: 81.25%,  total acc: 62.72%   [EVAL] batch:  336 | acc: 75.00%,  total acc: 62.76%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 62.81%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 62.85%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 62.92%   [EVAL] batch:  340 | acc: 81.25%,  total acc: 62.98%   [EVAL] batch:  341 | acc: 81.25%,  total acc: 63.03%   [EVAL] batch:  342 | acc: 50.00%,  total acc: 62.99%   [EVAL] batch:  343 | acc: 56.25%,  total acc: 62.97%   [EVAL] batch:  344 | acc: 75.00%,  total acc: 63.01%   [EVAL] batch:  345 | acc: 56.25%,  total acc: 62.99%   [EVAL] batch:  346 | acc: 56.25%,  total acc: 62.97%   [EVAL] batch:  347 | acc: 75.00%,  total acc: 63.00%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 63.04%   [EVAL] batch:  349 | acc: 56.25%,  total acc: 63.02%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 63.05%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 63.09%   [EVAL] batch:  352 | acc: 81.25%,  total acc: 63.14%   [EVAL] batch:  353 | acc: 75.00%,  total acc: 63.17%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 63.20%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 63.20%   [EVAL] batch:  356 | acc: 68.75%,  total acc: 63.22%   [EVAL] batch:  357 | acc: 37.50%,  total acc: 63.15%   [EVAL] batch:  358 | acc: 50.00%,  total acc: 63.11%   [EVAL] batch:  359 | acc: 50.00%,  total acc: 63.07%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 63.00%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 62.97%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 63.00%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 63.20%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 63.30%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 63.40%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 63.70%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 63.76%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 63.91%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 64.07%   [EVAL] batch:  375 | acc: 31.25%,  total acc: 63.98%   [EVAL] batch:  376 | acc: 62.50%,  total acc: 63.98%   [EVAL] batch:  377 | acc: 68.75%,  total acc: 63.99%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 63.92%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 63.91%   [EVAL] batch:  380 | acc: 68.75%,  total acc: 63.93%   [EVAL] batch:  381 | acc: 62.50%,  total acc: 63.92%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 63.98%   [EVAL] batch:  383 | acc: 75.00%,  total acc: 64.01%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 64.04%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 64.01%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 64.00%   [EVAL] batch:  387 | acc: 93.75%,  total acc: 64.08%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 64.14%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 64.20%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 64.27%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 64.32%   [EVAL] batch:  392 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:  393 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 64.53%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:  400 | acc: 62.50%,  total acc: 64.96%   [EVAL] batch:  401 | acc: 43.75%,  total acc: 64.91%   [EVAL] batch:  402 | acc: 68.75%,  total acc: 64.92%   [EVAL] batch:  403 | acc: 75.00%,  total acc: 64.94%   [EVAL] batch:  404 | acc: 56.25%,  total acc: 64.92%   [EVAL] batch:  405 | acc: 62.50%,  total acc: 64.92%   [EVAL] batch:  406 | acc: 81.25%,  total acc: 64.96%   [EVAL] batch:  407 | acc: 68.75%,  total acc: 64.97%   [EVAL] batch:  408 | acc: 43.75%,  total acc: 64.91%   [EVAL] batch:  409 | acc: 37.50%,  total acc: 64.85%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 64.74%   [EVAL] batch:  411 | acc: 50.00%,  total acc: 64.70%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 64.72%   [EVAL] batch:  413 | acc: 75.00%,  total acc: 64.75%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 64.80%   [EVAL] batch:  415 | acc: 68.75%,  total acc: 64.81%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 64.87%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 64.88%   [EVAL] batch:  418 | acc: 56.25%,  total acc: 64.86%   [EVAL] batch:  419 | acc: 75.00%,  total acc: 64.88%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 64.92%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 64.96%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 64.98%   [EVAL] batch:  423 | acc: 81.25%,  total acc: 65.02%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 65.06%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 65.66%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 65.71%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 65.83%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  437 | acc: 93.75%,  total acc: 65.97%   [EVAL] batch:  438 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:  439 | acc: 100.00%,  total acc: 66.09%   [EVAL] batch:  440 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  441 | acc: 81.25%,  total acc: 66.20%   [EVAL] batch:  442 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  443 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  444 | acc: 87.50%,  total acc: 66.40%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 66.47%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 66.51%   [EVAL] batch:  447 | acc: 93.75%,  total acc: 66.57%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:  449 | acc: 93.75%,  total acc: 66.68%   [EVAL] batch:  450 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:  451 | acc: 81.25%,  total acc: 66.75%   [EVAL] batch:  452 | acc: 87.50%,  total acc: 66.79%   [EVAL] batch:  453 | acc: 81.25%,  total acc: 66.82%   [EVAL] batch:  454 | acc: 75.00%,  total acc: 66.84%   [EVAL] batch:  455 | acc: 75.00%,  total acc: 66.86%   [EVAL] batch:  456 | acc: 68.75%,  total acc: 66.86%   [EVAL] batch:  457 | acc: 68.75%,  total acc: 66.87%   [EVAL] batch:  458 | acc: 62.50%,  total acc: 66.86%   [EVAL] batch:  459 | acc: 50.00%,  total acc: 66.82%   [EVAL] batch:  460 | acc: 81.25%,  total acc: 66.85%   [EVAL] batch:  461 | acc: 37.50%,  total acc: 66.79%   [EVAL] batch:  462 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 66.98%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 67.24%   [EVAL] batch:  469 | acc: 62.50%,  total acc: 67.23%   [EVAL] batch:  470 | acc: 81.25%,  total acc: 67.26%   [EVAL] batch:  471 | acc: 75.00%,  total acc: 67.28%   [EVAL] batch:  472 | acc: 68.75%,  total acc: 67.28%   [EVAL] batch:  473 | acc: 75.00%,  total acc: 67.30%   [EVAL] batch:  474 | acc: 43.75%,  total acc: 67.25%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 67.32%   [EVAL] batch:  476 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  477 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  478 | acc: 93.75%,  total acc: 67.51%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  481 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  482 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:  483 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  485 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  486 | acc: 93.75%,  total acc: 68.02%   [EVAL] batch:  487 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 68.12%   [EVAL] batch:  489 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  491 | acc: 93.75%,  total acc: 68.29%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 68.34%   [EVAL] batch:  493 | acc: 87.50%,  total acc: 68.38%   [EVAL] batch:  494 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 68.49%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 68.54%   [EVAL] batch:  497 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 68.71%   
cur_acc:  ['0.9484', '0.7857', '0.7857', '0.7817', '0.5863', '0.7748', '0.7708', '0.8740']
his_acc:  ['0.9484', '0.8570', '0.8165', '0.7758', '0.7143', '0.7005', '0.6769', '0.6871']
----------END
his_acc mean:  [0.9502 0.8588 0.8085 0.7739 0.752  0.7209 0.6986 0.6782]
