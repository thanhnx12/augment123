#############params############
cuda:0
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.3724613CurrentTrain: epoch  0, batch     1 | loss: 13.1578608CurrentTrain: epoch  0, batch     2 | loss: 13.0914288CurrentTrain: epoch  0, batch     3 | loss: 12.9665594CurrentTrain: epoch  0, batch     4 | loss: 12.8442793CurrentTrain: epoch  0, batch     5 | loss: 12.5822926CurrentTrain: epoch  0, batch     6 | loss: 12.5752811CurrentTrain: epoch  0, batch     7 | loss: 12.3465538CurrentTrain: epoch  0, batch     8 | loss: 12.2965069CurrentTrain: epoch  0, batch     9 | loss: 12.1475677CurrentTrain: epoch  0, batch    10 | loss: 12.0400219CurrentTrain: epoch  0, batch    11 | loss: 11.9480858CurrentTrain: epoch  0, batch    12 | loss: 12.0638752CurrentTrain: epoch  0, batch    13 | loss: 11.8167038CurrentTrain: epoch  0, batch    14 | loss: 11.6889286CurrentTrain: epoch  0, batch    15 | loss: 11.6254883CurrentTrain: epoch  0, batch    16 | loss: 11.0940533CurrentTrain: epoch  0, batch    17 | loss: 11.2020321CurrentTrain: epoch  0, batch    18 | loss: 11.2489090CurrentTrain: epoch  0, batch    19 | loss: 11.4948883CurrentTrain: epoch  0, batch    20 | loss: 11.0849552CurrentTrain: epoch  0, batch    21 | loss: 11.3767891CurrentTrain: epoch  0, batch    22 | loss: 11.4738388CurrentTrain: epoch  0, batch    23 | loss: 11.0327606CurrentTrain: epoch  0, batch    24 | loss: 11.3491087CurrentTrain: epoch  0, batch    25 | loss: 11.3099518CurrentTrain: epoch  0, batch    26 | loss: 10.8578224CurrentTrain: epoch  0, batch    27 | loss: 10.2841034CurrentTrain: epoch  0, batch    28 | loss: 10.7051201CurrentTrain: epoch  0, batch    29 | loss: 10.7940836CurrentTrain: epoch  0, batch    30 | loss: 10.4195404CurrentTrain: epoch  0, batch    31 | loss: 10.7609634CurrentTrain: epoch  0, batch    32 | loss: 10.3042355CurrentTrain: epoch  0, batch    33 | loss: 10.3129387CurrentTrain: epoch  0, batch    34 | loss: 10.0250797CurrentTrain: epoch  0, batch    35 | loss: 10.2144766CurrentTrain: epoch  0, batch    36 | loss: 10.2306309CurrentTrain: epoch  0, batch    37 | loss: 10.0852337CurrentTrain: epoch  1, batch     0 | loss: 10.2014656CurrentTrain: epoch  1, batch     1 | loss: 10.6530342CurrentTrain: epoch  1, batch     2 | loss: 9.7890358CurrentTrain: epoch  1, batch     3 | loss: 9.7257767CurrentTrain: epoch  1, batch     4 | loss: 9.4564104CurrentTrain: epoch  1, batch     5 | loss: 10.1626291CurrentTrain: epoch  1, batch     6 | loss: 9.8237648CurrentTrain: epoch  1, batch     7 | loss: 9.4988441CurrentTrain: epoch  1, batch     8 | loss: 9.4991245CurrentTrain: epoch  1, batch     9 | loss: 9.6452885CurrentTrain: epoch  1, batch    10 | loss: 9.9743004CurrentTrain: epoch  1, batch    11 | loss: 9.7963219CurrentTrain: epoch  1, batch    12 | loss: 9.6937313CurrentTrain: epoch  1, batch    13 | loss: 9.1966829CurrentTrain: epoch  1, batch    14 | loss: 9.5502548CurrentTrain: epoch  1, batch    15 | loss: 9.2058496CurrentTrain: epoch  1, batch    16 | loss: 9.1888971CurrentTrain: epoch  1, batch    17 | loss: 9.5938950CurrentTrain: epoch  1, batch    18 | loss: 9.4860392CurrentTrain: epoch  1, batch    19 | loss: 9.5833225CurrentTrain: epoch  1, batch    20 | loss: 9.5087862CurrentTrain: epoch  1, batch    21 | loss: 9.1296339CurrentTrain: epoch  1, batch    22 | loss: 9.2063999CurrentTrain: epoch  1, batch    23 | loss: 9.0002327CurrentTrain: epoch  1, batch    24 | loss: 9.2855091CurrentTrain: epoch  1, batch    25 | loss: 8.4403152CurrentTrain: epoch  1, batch    26 | loss: 9.3554955CurrentTrain: epoch  1, batch    27 | loss: 8.4032183CurrentTrain: epoch  1, batch    28 | loss: 8.6823807CurrentTrain: epoch  1, batch    29 | loss: 8.0479479CurrentTrain: epoch  1, batch    30 | loss: 8.6466703CurrentTrain: epoch  1, batch    31 | loss: 9.0862045CurrentTrain: epoch  1, batch    32 | loss: 8.8169594CurrentTrain: epoch  1, batch    33 | loss: 8.2732611CurrentTrain: epoch  1, batch    34 | loss: 8.6010847CurrentTrain: epoch  1, batch    35 | loss: 8.1362267CurrentTrain: epoch  1, batch    36 | loss: 8.8653030CurrentTrain: epoch  1, batch    37 | loss: 8.5752344CurrentTrain: epoch  2, batch     0 | loss: 7.7833271CurrentTrain: epoch  2, batch     1 | loss: 8.4801159CurrentTrain: epoch  2, batch     2 | loss: 8.7112026CurrentTrain: epoch  2, batch     3 | loss: 8.3481588CurrentTrain: epoch  2, batch     4 | loss: 9.3701601CurrentTrain: epoch  2, batch     5 | loss: 9.5443459CurrentTrain: epoch  2, batch     6 | loss: 8.6426182CurrentTrain: epoch  2, batch     7 | loss: 8.3060379CurrentTrain: epoch  2, batch     8 | loss: 8.6485052CurrentTrain: epoch  2, batch     9 | loss: 8.4162703CurrentTrain: epoch  2, batch    10 | loss: 8.0237541CurrentTrain: epoch  2, batch    11 | loss: 8.5527153CurrentTrain: epoch  2, batch    12 | loss: 8.2105560CurrentTrain: epoch  2, batch    13 | loss: 7.9770279CurrentTrain: epoch  2, batch    14 | loss: 7.3642979CurrentTrain: epoch  2, batch    15 | loss: 7.9607420CurrentTrain: epoch  2, batch    16 | loss: 8.3288574CurrentTrain: epoch  2, batch    17 | loss: 8.4686508CurrentTrain: epoch  2, batch    18 | loss: 8.0185461CurrentTrain: epoch  2, batch    19 | loss: 7.8661394CurrentTrain: epoch  2, batch    20 | loss: 7.6271992CurrentTrain: epoch  2, batch    21 | loss: 7.9648695CurrentTrain: epoch  2, batch    22 | loss: 7.4678106CurrentTrain: epoch  2, batch    23 | loss: 7.4899635CurrentTrain: epoch  2, batch    24 | loss: 7.8132114CurrentTrain: epoch  2, batch    25 | loss: 8.1924648CurrentTrain: epoch  2, batch    26 | loss: 7.2895327CurrentTrain: epoch  2, batch    27 | loss: 8.5118685CurrentTrain: epoch  2, batch    28 | loss: 6.8255653CurrentTrain: epoch  2, batch    29 | loss: 7.8991265CurrentTrain: epoch  2, batch    30 | loss: 7.5748453CurrentTrain: epoch  2, batch    31 | loss: 7.1347771CurrentTrain: epoch  2, batch    32 | loss: 7.6812344CurrentTrain: epoch  2, batch    33 | loss: 7.5430918CurrentTrain: epoch  2, batch    34 | loss: 8.4325619CurrentTrain: epoch  2, batch    35 | loss: 7.4143143CurrentTrain: epoch  2, batch    36 | loss: 7.8018699CurrentTrain: epoch  2, batch    37 | loss: 7.7435017CurrentTrain: epoch  3, batch     0 | loss: 7.5232515CurrentTrain: epoch  3, batch     1 | loss: 7.8361187CurrentTrain: epoch  3, batch     2 | loss: 7.8839965CurrentTrain: epoch  3, batch     3 | loss: 7.9287844CurrentTrain: epoch  3, batch     4 | loss: 7.4654551CurrentTrain: epoch  3, batch     5 | loss: 7.8325772CurrentTrain: epoch  3, batch     6 | loss: 8.4771061CurrentTrain: epoch  3, batch     7 | loss: 6.9571447CurrentTrain: epoch  3, batch     8 | loss: 8.0827675CurrentTrain: epoch  3, batch     9 | loss: 7.7508407CurrentTrain: epoch  3, batch    10 | loss: 7.0086536CurrentTrain: epoch  3, batch    11 | loss: 6.6070518CurrentTrain: epoch  3, batch    12 | loss: 7.7624445CurrentTrain: epoch  3, batch    13 | loss: 8.1287346CurrentTrain: epoch  3, batch    14 | loss: 7.2786741CurrentTrain: epoch  3, batch    15 | loss: 7.4686642CurrentTrain: epoch  3, batch    16 | loss: 8.3591795CurrentTrain: epoch  3, batch    17 | loss: 7.4006367CurrentTrain: epoch  3, batch    18 | loss: 7.4785576CurrentTrain: epoch  3, batch    19 | loss: 7.9789510CurrentTrain: epoch  3, batch    20 | loss: 7.5735807CurrentTrain: epoch  3, batch    21 | loss: 7.4664931CurrentTrain: epoch  3, batch    22 | loss: 7.6469474CurrentTrain: epoch  3, batch    23 | loss: 7.7438240CurrentTrain: epoch  3, batch    24 | loss: 6.4601979CurrentTrain: epoch  3, batch    25 | loss: 6.9148345CurrentTrain: epoch  3, batch    26 | loss: 6.8151793CurrentTrain: epoch  3, batch    27 | loss: 7.9257421CurrentTrain: epoch  3, batch    28 | loss: 7.3877330CurrentTrain: epoch  3, batch    29 | loss: 6.2554440CurrentTrain: epoch  3, batch    30 | loss: 7.2944155CurrentTrain: epoch  3, batch    31 | loss: 6.9774604CurrentTrain: epoch  3, batch    32 | loss: 6.2400208CurrentTrain: epoch  3, batch    33 | loss: 6.4645548CurrentTrain: epoch  3, batch    34 | loss: 6.7722120CurrentTrain: epoch  3, batch    35 | loss: 6.4985266CurrentTrain: epoch  3, batch    36 | loss: 6.8035321CurrentTrain: epoch  3, batch    37 | loss: 6.8702993CurrentTrain: epoch  4, batch     0 | loss: 7.0334969CurrentTrain: epoch  4, batch     1 | loss: 7.0049124CurrentTrain: epoch  4, batch     2 | loss: 5.8247557CurrentTrain: epoch  4, batch     3 | loss: 6.8830619CurrentTrain: epoch  4, batch     4 | loss: 6.9699078CurrentTrain: epoch  4, batch     5 | loss: 6.9527235CurrentTrain: epoch  4, batch     6 | loss: 6.2295132CurrentTrain: epoch  4, batch     7 | loss: 6.9355526CurrentTrain: epoch  4, batch     8 | loss: 7.7385564CurrentTrain: epoch  4, batch     9 | loss: 7.0533419CurrentTrain: epoch  4, batch    10 | loss: 7.2315435CurrentTrain: epoch  4, batch    11 | loss: 6.2178564CurrentTrain: epoch  4, batch    12 | loss: 6.9987230CurrentTrain: epoch  4, batch    13 | loss: 6.5305729CurrentTrain: epoch  4, batch    14 | loss: 6.8219061CurrentTrain: epoch  4, batch    15 | loss: 7.0248032CurrentTrain: epoch  4, batch    16 | loss: 6.7223358CurrentTrain: epoch  4, batch    17 | loss: 6.6297302CurrentTrain: epoch  4, batch    18 | loss: 6.2468553CurrentTrain: epoch  4, batch    19 | loss: 6.3774834CurrentTrain: epoch  4, batch    20 | loss: 6.7463589CurrentTrain: epoch  4, batch    21 | loss: 7.4178104CurrentTrain: epoch  4, batch    22 | loss: 6.9252653CurrentTrain: epoch  4, batch    23 | loss: 6.1513681CurrentTrain: epoch  4, batch    24 | loss: 7.1817818CurrentTrain: epoch  4, batch    25 | loss: 7.3936548CurrentTrain: epoch  4, batch    26 | loss: 6.3802161CurrentTrain: epoch  4, batch    27 | loss: 8.6171103CurrentTrain: epoch  4, batch    28 | loss: 6.6908755CurrentTrain: epoch  4, batch    29 | loss: 6.6532860CurrentTrain: epoch  4, batch    30 | loss: 6.7164712CurrentTrain: epoch  4, batch    31 | loss: 6.5105209CurrentTrain: epoch  4, batch    32 | loss: 7.6631536CurrentTrain: epoch  4, batch    33 | loss: 6.2924786CurrentTrain: epoch  4, batch    34 | loss: 7.8369579CurrentTrain: epoch  4, batch    35 | loss: 7.2543831CurrentTrain: epoch  4, batch    36 | loss: 6.5398402CurrentTrain: epoch  4, batch    37 | loss: 7.5059304CurrentTrain: epoch  5, batch     0 | loss: 6.1030679CurrentTrain: epoch  5, batch     1 | loss: 6.8386374CurrentTrain: epoch  5, batch     2 | loss: 7.0808506CurrentTrain: epoch  5, batch     3 | loss: 7.1512556CurrentTrain: epoch  5, batch     4 | loss: 6.9177895CurrentTrain: epoch  5, batch     5 | loss: 6.7688484CurrentTrain: epoch  5, batch     6 | loss: 6.7772846CurrentTrain: epoch  5, batch     7 | loss: 6.8151073CurrentTrain: epoch  5, batch     8 | loss: 6.7718267CurrentTrain: epoch  5, batch     9 | loss: 6.6284990CurrentTrain: epoch  5, batch    10 | loss: 6.2529025CurrentTrain: epoch  5, batch    11 | loss: 6.5486612CurrentTrain: epoch  5, batch    12 | loss: 6.1802988CurrentTrain: epoch  5, batch    13 | loss: 6.8904743CurrentTrain: epoch  5, batch    14 | loss: 6.4440236CurrentTrain: epoch  5, batch    15 | loss: 6.5432043CurrentTrain: epoch  5, batch    16 | loss: 5.7127781CurrentTrain: epoch  5, batch    17 | loss: 5.8208084CurrentTrain: epoch  5, batch    18 | loss: 7.2228193CurrentTrain: epoch  5, batch    19 | loss: 6.8538017CurrentTrain: epoch  5, batch    20 | loss: 6.0343790CurrentTrain: epoch  5, batch    21 | loss: 6.0485401CurrentTrain: epoch  5, batch    22 | loss: 6.1135545CurrentTrain: epoch  5, batch    23 | loss: 5.8876462CurrentTrain: epoch  5, batch    24 | loss: 6.7454958CurrentTrain: epoch  5, batch    25 | loss: 5.8460865CurrentTrain: epoch  5, batch    26 | loss: 5.9473586CurrentTrain: epoch  5, batch    27 | loss: 6.7466745CurrentTrain: epoch  5, batch    28 | loss: 8.3776693CurrentTrain: epoch  5, batch    29 | loss: 6.3754187CurrentTrain: epoch  5, batch    30 | loss: 6.4071417CurrentTrain: epoch  5, batch    31 | loss: 6.0694742CurrentTrain: epoch  5, batch    32 | loss: 5.5056829CurrentTrain: epoch  5, batch    33 | loss: 6.3093948CurrentTrain: epoch  5, batch    34 | loss: 6.6475391CurrentTrain: epoch  5, batch    35 | loss: 5.9693222CurrentTrain: epoch  5, batch    36 | loss: 6.5377998CurrentTrain: epoch  5, batch    37 | loss: 6.0111084CurrentTrain: epoch  6, batch     0 | loss: 6.0047464CurrentTrain: epoch  6, batch     1 | loss: 6.6310277CurrentTrain: epoch  6, batch     2 | loss: 6.6020212CurrentTrain: epoch  6, batch     3 | loss: 6.2475824CurrentTrain: epoch  6, batch     4 | loss: 6.0720997CurrentTrain: epoch  6, batch     5 | loss: 6.0006638CurrentTrain: epoch  6, batch     6 | loss: 6.3973942CurrentTrain: epoch  6, batch     7 | loss: 5.9961662CurrentTrain: epoch  6, batch     8 | loss: 5.7649665CurrentTrain: epoch  6, batch     9 | loss: 6.0096731CurrentTrain: epoch  6, batch    10 | loss: 5.9515557CurrentTrain: epoch  6, batch    11 | loss: 5.9529152CurrentTrain: epoch  6, batch    12 | loss: 5.9199295CurrentTrain: epoch  6, batch    13 | loss: 5.6818867CurrentTrain: epoch  6, batch    14 | loss: 5.8865380CurrentTrain: epoch  6, batch    15 | loss: 5.3489008CurrentTrain: epoch  6, batch    16 | loss: 6.4938850CurrentTrain: epoch  6, batch    17 | loss: 5.9322629CurrentTrain: epoch  6, batch    18 | loss: 6.0616775CurrentTrain: epoch  6, batch    19 | loss: 5.8435102CurrentTrain: epoch  6, batch    20 | loss: 6.5467153CurrentTrain: epoch  6, batch    21 | loss: 6.8287182CurrentTrain: epoch  6, batch    22 | loss: 5.8638916CurrentTrain: epoch  6, batch    23 | loss: 5.8143373CurrentTrain: epoch  6, batch    24 | loss: 5.5679169CurrentTrain: epoch  6, batch    25 | loss: 6.4695663CurrentTrain: epoch  6, batch    26 | loss: 6.4372053CurrentTrain: epoch  6, batch    27 | loss: 5.6673498CurrentTrain: epoch  6, batch    28 | loss: 5.9043579CurrentTrain: epoch  6, batch    29 | loss: 6.1868839CurrentTrain: epoch  6, batch    30 | loss: 6.6498880CurrentTrain: epoch  6, batch    31 | loss: 6.2947693CurrentTrain: epoch  6, batch    32 | loss: 5.6921754CurrentTrain: epoch  6, batch    33 | loss: 6.2850089CurrentTrain: epoch  6, batch    34 | loss: 5.9845924CurrentTrain: epoch  6, batch    35 | loss: 6.4355583CurrentTrain: epoch  6, batch    36 | loss: 6.2763124CurrentTrain: epoch  6, batch    37 | loss: 5.9351416CurrentTrain: epoch  7, batch     0 | loss: 6.6529021CurrentTrain: epoch  7, batch     1 | loss: 5.8108978CurrentTrain: epoch  7, batch     2 | loss: 5.5417938CurrentTrain: epoch  7, batch     3 | loss: 5.5101213CurrentTrain: epoch  7, batch     4 | loss: 6.3973951CurrentTrain: epoch  7, batch     5 | loss: 5.6384888CurrentTrain: epoch  7, batch     6 | loss: 5.6699238CurrentTrain: epoch  7, batch     7 | loss: 5.6597505CurrentTrain: epoch  7, batch     8 | loss: 5.6804357CurrentTrain: epoch  7, batch     9 | loss: 5.5073752CurrentTrain: epoch  7, batch    10 | loss: 5.9107246CurrentTrain: epoch  7, batch    11 | loss: 5.8943958CurrentTrain: epoch  7, batch    12 | loss: 5.7086897CurrentTrain: epoch  7, batch    13 | loss: 5.8933516CurrentTrain: epoch  7, batch    14 | loss: 5.8622904CurrentTrain: epoch  7, batch    15 | loss: 6.0675993CurrentTrain: epoch  7, batch    16 | loss: 5.7433643CurrentTrain: epoch  7, batch    17 | loss: 5.4421778CurrentTrain: epoch  7, batch    18 | loss: 5.6944370CurrentTrain: epoch  7, batch    19 | loss: 5.5842218CurrentTrain: epoch  7, batch    20 | loss: 5.5729561CurrentTrain: epoch  7, batch    21 | loss: 5.3309689CurrentTrain: epoch  7, batch    22 | loss: 6.9911366CurrentTrain: epoch  7, batch    23 | loss: 6.2403383CurrentTrain: epoch  7, batch    24 | loss: 6.1066108CurrentTrain: epoch  7, batch    25 | loss: 5.3430290CurrentTrain: epoch  7, batch    26 | loss: 5.5711193CurrentTrain: epoch  7, batch    27 | loss: 5.4540119CurrentTrain: epoch  7, batch    28 | loss: 5.4332466CurrentTrain: epoch  7, batch    29 | loss: 5.3174152CurrentTrain: epoch  7, batch    30 | loss: 5.4019990CurrentTrain: epoch  7, batch    31 | loss: 5.3367624CurrentTrain: epoch  7, batch    32 | loss: 5.6065183CurrentTrain: epoch  7, batch    33 | loss: 5.8639040CurrentTrain: epoch  7, batch    34 | loss: 5.5942111CurrentTrain: epoch  7, batch    35 | loss: 5.7962923CurrentTrain: epoch  7, batch    36 | loss: 5.5112915CurrentTrain: epoch  7, batch    37 | loss: 4.9952893CurrentTrain: epoch  8, batch     0 | loss: 5.4854412CurrentTrain: epoch  8, batch     1 | loss: 5.6333909CurrentTrain: epoch  8, batch     2 | loss: 5.1421604CurrentTrain: epoch  8, batch     3 | loss: 5.3651385CurrentTrain: epoch  8, batch     4 | loss: 5.3226509CurrentTrain: epoch  8, batch     5 | loss: 5.3203096CurrentTrain: epoch  8, batch     6 | loss: 5.3517303CurrentTrain: epoch  8, batch     7 | loss: 5.4473085CurrentTrain: epoch  8, batch     8 | loss: 5.3135548CurrentTrain: epoch  8, batch     9 | loss: 5.2105174CurrentTrain: epoch  8, batch    10 | loss: 5.6048307CurrentTrain: epoch  8, batch    11 | loss: 5.3615904CurrentTrain: epoch  8, batch    12 | loss: 5.7865286CurrentTrain: epoch  8, batch    13 | loss: 5.3026810CurrentTrain: epoch  8, batch    14 | loss: 5.6099992CurrentTrain: epoch  8, batch    15 | loss: 5.3994226CurrentTrain: epoch  8, batch    16 | loss: 5.7551336CurrentTrain: epoch  8, batch    17 | loss: 5.0666037CurrentTrain: epoch  8, batch    18 | loss: 5.2056007CurrentTrain: epoch  8, batch    19 | loss: 5.7251682CurrentTrain: epoch  8, batch    20 | loss: 5.5557652CurrentTrain: epoch  8, batch    21 | loss: 5.4007759CurrentTrain: epoch  8, batch    22 | loss: 5.9072752CurrentTrain: epoch  8, batch    23 | loss: 5.6366529CurrentTrain: epoch  8, batch    24 | loss: 5.2677555CurrentTrain: epoch  8, batch    25 | loss: 5.3671660CurrentTrain: epoch  8, batch    26 | loss: 6.1590004CurrentTrain: epoch  8, batch    27 | loss: 5.1991730CurrentTrain: epoch  8, batch    28 | loss: 5.2172470CurrentTrain: epoch  8, batch    29 | loss: 5.6280775CurrentTrain: epoch  8, batch    30 | loss: 5.2850456CurrentTrain: epoch  8, batch    31 | loss: 5.3826280CurrentTrain: epoch  8, batch    32 | loss: 4.9245048CurrentTrain: epoch  8, batch    33 | loss: 5.1530337CurrentTrain: epoch  8, batch    34 | loss: 5.0754180CurrentTrain: epoch  8, batch    35 | loss: 5.2098951CurrentTrain: epoch  8, batch    36 | loss: 5.0569038CurrentTrain: epoch  8, batch    37 | loss: 5.1459870CurrentTrain: epoch  9, batch     0 | loss: 5.3714628CurrentTrain: epoch  9, batch     1 | loss: 5.2017689CurrentTrain: epoch  9, batch     2 | loss: 5.2213621CurrentTrain: epoch  9, batch     3 | loss: 5.0896473CurrentTrain: epoch  9, batch     4 | loss: 5.1074209CurrentTrain: epoch  9, batch     5 | loss: 5.1780319CurrentTrain: epoch  9, batch     6 | loss: 5.4686308CurrentTrain: epoch  9, batch     7 | loss: 5.1841688CurrentTrain: epoch  9, batch     8 | loss: 5.1631765CurrentTrain: epoch  9, batch     9 | loss: 5.0416660CurrentTrain: epoch  9, batch    10 | loss: 5.0971518CurrentTrain: epoch  9, batch    11 | loss: 5.0610533CurrentTrain: epoch  9, batch    12 | loss: 5.1421871CurrentTrain: epoch  9, batch    13 | loss: 5.2279015CurrentTrain: epoch  9, batch    14 | loss: 5.3229456CurrentTrain: epoch  9, batch    15 | loss: 5.0906010CurrentTrain: epoch  9, batch    16 | loss: 5.0985069CurrentTrain: epoch  9, batch    17 | loss: 5.8242164CurrentTrain: epoch  9, batch    18 | loss: 5.5273147CurrentTrain: epoch  9, batch    19 | loss: 5.1797643CurrentTrain: epoch  9, batch    20 | loss: 5.3398924CurrentTrain: epoch  9, batch    21 | loss: 4.9716997CurrentTrain: epoch  9, batch    22 | loss: 5.1110754CurrentTrain: epoch  9, batch    23 | loss: 5.2672491CurrentTrain: epoch  9, batch    24 | loss: 5.1435614CurrentTrain: epoch  9, batch    25 | loss: 5.7747397CurrentTrain: epoch  9, batch    26 | loss: 5.6257625CurrentTrain: epoch  9, batch    27 | loss: 5.3424568CurrentTrain: epoch  9, batch    28 | loss: 5.0072770CurrentTrain: epoch  9, batch    29 | loss: 5.5512266CurrentTrain: epoch  9, batch    30 | loss: 4.9825134CurrentTrain: epoch  9, batch    31 | loss: 5.0399342CurrentTrain: epoch  9, batch    32 | loss: 5.1282659CurrentTrain: epoch  9, batch    33 | loss: 5.3177009CurrentTrain: epoch  9, batch    34 | loss: 5.1026111CurrentTrain: epoch  9, batch    35 | loss: 4.9330521CurrentTrain: epoch  9, batch    36 | loss: 4.9632597CurrentTrain: epoch  9, batch    37 | loss: 5.1368017
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: khamenei , 67 , has final say on all state matters in iran as supreme leader , a post he has held since 1989 .
Head Entity: khamenei
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, Maria decided to return to her homeland, spain, where she felt a deep connection to her roots.  
Head Entity: Maria  
Tail Entity: spain  

Relation: person countries of residence  
Context: Following his successful career in technology, Raj moved to canada to enjoy a quieter life surrounded by nature.  
Head Entity: Raj  
Tail Entity: canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: kao charng , vice chairman of the cabinet-level mainland affairs council -lrb- mac -rrb- , explained that the termination clause was added to give both sides an option after the implementation of the tariff-cutting economic cooperation framework agreement -lrb- ecfa -rrb- .
Head Entity: mainland affairs council
Tail Entity: kao charng
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: elon musk, the CEO of spacex, announced the successful launch of the latest satellite into orbit, marking another milestone for the company.  
Head Entity: spacex  
Tail Entity: elon musk  

Relation: organization top members employees  
Context: sheryl sandberg, the chief operating officer of facebook, spoke at the conference about the importance of data privacy and user trust in the digital age.  
Head Entity: facebook  
Tail Entity: sheryl sandberg  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: iranian nuclear negotiator ali larijani said thursday in ankara that talks on settling the iranian nuclear crisis had made some progress towards a `` united view . ''
Head Entity: ali larijani
Tail Entity: iranian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The famous physicist Albert Einstein was born in Germany before he moved to the United States, where he became a citizen.  
Head Entity: Albert Einstein  
Tail Entity: Germany  

Relation: person origin  
Context: The renowned author Chimamanda Ngozi Adichie often speaks about her Nigerian heritage and how it influences her writing.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigeria  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving the company's financial strategy."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the annual conference, Dr. James Lee, the lead researcher at the National Institute of Health, presented his findings on the latest advancements in medical technology."  
Head Entity: Dr. James Lee  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: the credit crisis spread to the largest us bond insurer thursday , sending shares of mbia inc plunging and calling into question the safety of tens of billions of dollars of company and local government debt held by investors .
Head Entity: mbia
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant apple inc announced plans to expand its operations in ireland, taking advantage of the favorable tax environment.  
Head Entity: apple  
Tail Entity: ireland  

Relation: organization country of headquarters  
Context: the multinational corporation samsung electronics has established its main office in south korea, where it continues to innovate in the field of consumer electronics.  
Head Entity: samsung  
Tail Entity: south korea  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
cur_acc:  ['0.8561']
his_acc:  ['0.8561']
CurrentTrain: epoch  0, batch     0 | loss: 6.7105479CurrentTrain: epoch  0, batch     1 | loss: 7.8216114CurrentTrain: epoch  1, batch     0 | loss: 6.5109053CurrentTrain: epoch  1, batch     1 | loss: 5.2046876CurrentTrain: epoch  2, batch     0 | loss: 5.8193226CurrentTrain: epoch  2, batch     1 | loss: 5.3728476CurrentTrain: epoch  3, batch     0 | loss: 5.3471889CurrentTrain: epoch  3, batch     1 | loss: 4.4202156CurrentTrain: epoch  4, batch     0 | loss: 4.8331256CurrentTrain: epoch  4, batch     1 | loss: 4.8644309CurrentTrain: epoch  5, batch     0 | loss: 4.9063978CurrentTrain: epoch  5, batch     1 | loss: 3.9061246CurrentTrain: epoch  6, batch     0 | loss: 4.2706718CurrentTrain: epoch  6, batch     1 | loss: 3.8399904CurrentTrain: epoch  7, batch     0 | loss: 4.1974335CurrentTrain: epoch  7, batch     1 | loss: 4.0149384CurrentTrain: epoch  8, batch     0 | loss: 3.6917348CurrentTrain: epoch  8, batch     1 | loss: 3.3621042CurrentTrain: epoch  9, batch     0 | loss: 3.4238887CurrentTrain: epoch  9, batch     1 | loss: 3.0295978
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire, on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: germany  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, John, to her friends, highlighting how much he has influenced her life choices.  
Head Entity: her father  
Tail Entity: John  

Relation: person parents  
Context: Emily often shares stories about her mother, who has always been her biggest supporter and role model throughout her life.  
Head Entity: her mother  
Tail Entity: Emily
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst in the investment division.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john smith, a renowned author, passed away on march 5 in his residence located in los angeles, california, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, california, surrounded by her family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: california  
Mixup data size:  103
MixupTrain:  epoch  0, batch     0 | loss: 13.5329509MixupTrain:  epoch  0, batch     1 | loss: 12.2621770MixupTrain:  epoch  0, batch     2 | loss: 11.6705172MixupTrain:  epoch  0, batch     3 | loss: 13.5492642MixupTrain:  epoch  0, batch     4 | loss: 11.2570146MixupTrain:  epoch  0, batch     5 | loss: 10.5984418MixupTrain:  epoch  0, batch     6 | loss: 9.4930525
MemoryTrain:  epoch  0, batch     0 | loss: 4.4841199MemoryTrain:  epoch  0, batch     1 | loss: 4.2855787MemoryTrain:  epoch  0, batch     2 | loss: 5.4794412MemoryTrain:  epoch  1, batch     0 | loss: 4.6285825MemoryTrain:  epoch  1, batch     1 | loss: 3.5505323MemoryTrain:  epoch  1, batch     2 | loss: 5.4361367MemoryTrain:  epoch  2, batch     0 | loss: 3.8564169MemoryTrain:  epoch  2, batch     1 | loss: 3.4719014MemoryTrain:  epoch  2, batch     2 | loss: 3.3428097MemoryTrain:  epoch  3, batch     0 | loss: 3.4685268MemoryTrain:  epoch  3, batch     1 | loss: 2.9456034MemoryTrain:  epoch  3, batch     2 | loss: 4.5310140MemoryTrain:  epoch  4, batch     0 | loss: 3.1485329MemoryTrain:  epoch  4, batch     1 | loss: 3.1247137MemoryTrain:  epoch  4, batch     2 | loss: 2.4713542MemoryTrain:  epoch  5, batch     0 | loss: 3.5295601MemoryTrain:  epoch  5, batch     1 | loss: 2.2622652MemoryTrain:  epoch  5, batch     2 | loss: 3.0372076MemoryTrain:  epoch  6, batch     0 | loss: 3.0156069MemoryTrain:  epoch  6, batch     1 | loss: 2.9439936MemoryTrain:  epoch  6, batch     2 | loss: 1.4385204MemoryTrain:  epoch  7, batch     0 | loss: 3.3344679MemoryTrain:  epoch  7, batch     1 | loss: 2.5819464MemoryTrain:  epoch  7, batch     2 | loss: 2.7282810MemoryTrain:  epoch  8, batch     0 | loss: 2.7918992MemoryTrain:  epoch  8, batch     1 | loss: 2.5240045MemoryTrain:  epoch  8, batch     2 | loss: 1.4111525MemoryTrain:  epoch  9, batch     0 | loss: 2.3622169MemoryTrain:  epoch  9, batch     1 | loss: 2.3312836MemoryTrain:  epoch  9, batch     2 | loss: 2.3667080
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 90.18%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 77.56%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.01%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.76%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 83.40%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 83.71%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 83.64%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 84.11%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 84.20%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 84.46%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.87%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.26%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 85.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 85.86%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 85.90%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 85.97%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 86.14%   [EVAL] batch:   46 | acc: 6.25%,  total acc: 84.44%   
cur_acc:  ['0.8561', '0.9018']
his_acc:  ['0.8561', '0.8444']
CurrentTrain: epoch  0, batch     0 | loss: 6.1845760CurrentTrain: epoch  0, batch     1 | loss: 7.3678894CurrentTrain: epoch  1, batch     0 | loss: 5.8003759CurrentTrain: epoch  1, batch     1 | loss: 4.9899979CurrentTrain: epoch  2, batch     0 | loss: 5.3728609CurrentTrain: epoch  2, batch     1 | loss: 4.3252978CurrentTrain: epoch  3, batch     0 | loss: 4.1935554CurrentTrain: epoch  3, batch     1 | loss: 4.0091100CurrentTrain: epoch  4, batch     0 | loss: 3.5903039CurrentTrain: epoch  4, batch     1 | loss: 4.5016251CurrentTrain: epoch  5, batch     0 | loss: 3.6188638CurrentTrain: epoch  5, batch     1 | loss: 3.5742128CurrentTrain: epoch  6, batch     0 | loss: 3.5768309CurrentTrain: epoch  6, batch     1 | loss: 3.1509590CurrentTrain: epoch  7, batch     0 | loss: 3.7032421CurrentTrain: epoch  7, batch     1 | loss: 3.2529759CurrentTrain: epoch  8, batch     0 | loss: 2.9628801CurrentTrain: epoch  8, batch     1 | loss: 3.7400517CurrentTrain: epoch  9, batch     0 | loss: 2.7064054CurrentTrain: epoch  9, batch     1 | loss: 3.7162349
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun kapoor has always been proud of his indian heritage and culture.  
Head Entity: arjun kapoor  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she revealed that despite living in the united states for over a decade, her roots trace back to the beautiful landscapes of italy, where she was born.  
Head Entity: she  
Tail Entity: italy  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.nike.com for the latest sports gear and apparel.  
Head Entity: Nike  
Tail Entity: https://www.nike.com  

Relation: organization website  
Context: For more information about their services, check out https://www.apple.com.  
Head Entity: Apple  
Tail Entity: https://www.apple.com  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has recently acquired a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple inc.  

Relation: organization shareholders  
Context: the investment firm blackrock has increased its holdings in the renewable energy company sunrun.  
Head Entity: sunrun  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its dissolution in the spring of 2018, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring of 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization Hope for Tomorrow was created in 2010 by philanthropist Michael Johnson to provide educational resources to underprivileged children.  
Head Entity: Hope for Tomorrow  
Tail Entity: Michael Johnson  
Mixup data size:  134
MixupTrain:  epoch  0, batch     0 | loss: 8.1827048MixupTrain:  epoch  0, batch     1 | loss: 8.7500888MixupTrain:  epoch  0, batch     2 | loss: 7.8309966MixupTrain:  epoch  0, batch     3 | loss: 9.5574630MixupTrain:  epoch  0, batch     4 | loss: 8.1764308MixupTrain:  epoch  0, batch     5 | loss: 8.9665894MixupTrain:  epoch  0, batch     6 | loss: 9.4369140MixupTrain:  epoch  0, batch     7 | loss: 7.7680052MixupTrain:  epoch  0, batch     8 | loss: 6.0585048
MemoryTrain:  epoch  0, batch     0 | loss: 4.2479029MemoryTrain:  epoch  0, batch     1 | loss: 4.0910664MemoryTrain:  epoch  0, batch     2 | loss: 3.6045218MemoryTrain:  epoch  1, batch     0 | loss: 3.1199098MemoryTrain:  epoch  1, batch     1 | loss: 4.4415312MemoryTrain:  epoch  1, batch     2 | loss: 3.5775332MemoryTrain:  epoch  2, batch     0 | loss: 3.6551762MemoryTrain:  epoch  2, batch     1 | loss: 2.6817298MemoryTrain:  epoch  2, batch     2 | loss: 4.0028038MemoryTrain:  epoch  3, batch     0 | loss: 2.8968492MemoryTrain:  epoch  3, batch     1 | loss: 3.4069428MemoryTrain:  epoch  3, batch     2 | loss: 3.3227415MemoryTrain:  epoch  4, batch     0 | loss: 2.4187608MemoryTrain:  epoch  4, batch     1 | loss: 2.9080176MemoryTrain:  epoch  4, batch     2 | loss: 3.3948464MemoryTrain:  epoch  5, batch     0 | loss: 3.0119407MemoryTrain:  epoch  5, batch     1 | loss: 2.9221871MemoryTrain:  epoch  5, batch     2 | loss: 2.8229642MemoryTrain:  epoch  6, batch     0 | loss: 2.8073840MemoryTrain:  epoch  6, batch     1 | loss: 2.5057065MemoryTrain:  epoch  6, batch     2 | loss: 2.5885825MemoryTrain:  epoch  7, batch     0 | loss: 2.6312761MemoryTrain:  epoch  7, batch     1 | loss: 2.6098788MemoryTrain:  epoch  7, batch     2 | loss: 2.2753015MemoryTrain:  epoch  8, batch     0 | loss: 2.3252165MemoryTrain:  epoch  8, batch     1 | loss: 2.4184012MemoryTrain:  epoch  8, batch     2 | loss: 2.2079277MemoryTrain:  epoch  9, batch     0 | loss: 2.7138467MemoryTrain:  epoch  9, batch     1 | loss: 2.2382562MemoryTrain:  epoch  9, batch     2 | loss: 2.1822455
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 47.92%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 44.64%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 39.84%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 51.39%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 58.52%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 59.90%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 59.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 62.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.99%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 74.17%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.39%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 75.95%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 75.92%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 76.61%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 76.74%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 78.04%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 78.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.81%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 79.55%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 79.89%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 79.52%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.95%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 79.85%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 78.50%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 77.33%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 76.20%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 75.24%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 73.96%   
cur_acc:  ['0.8561', '0.9018', '0.3984']
his_acc:  ['0.8561', '0.8444', '0.7396']
CurrentTrain: epoch  0, batch     0 | loss: 5.0037546CurrentTrain: epoch  0, batch     1 | loss: 5.2331948CurrentTrain: epoch  1, batch     0 | loss: 4.2441988CurrentTrain: epoch  1, batch     1 | loss: 3.7400284CurrentTrain: epoch  2, batch     0 | loss: 3.7379484CurrentTrain: epoch  2, batch     1 | loss: 3.4904068CurrentTrain: epoch  3, batch     0 | loss: 3.5682845CurrentTrain: epoch  3, batch     1 | loss: 2.9925296CurrentTrain: epoch  4, batch     0 | loss: 3.3410931CurrentTrain: epoch  4, batch     1 | loss: 2.9181333CurrentTrain: epoch  5, batch     0 | loss: 2.8849022CurrentTrain: epoch  5, batch     1 | loss: 2.6732538CurrentTrain: epoch  6, batch     0 | loss: 2.6231556CurrentTrain: epoch  6, batch     1 | loss: 2.6762631CurrentTrain: epoch  7, batch     0 | loss: 3.2253547CurrentTrain: epoch  7, batch     1 | loss: 2.8433206CurrentTrain: epoch  8, batch     0 | loss: 2.8803620CurrentTrain: epoch  8, batch     1 | loss: 2.5903084CurrentTrain: epoch  9, batch     0 | loss: 2.1311650CurrentTrain: epoch  9, batch     1 | loss: 2.4117141
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political landscape of the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: new york-based veteran taiwan journalist chang chao-ying and the washington - based formosan association for public affairs -lrb- fapa -rrb- have jointly established a `` freedom of speech prize '' aimed at helping taiwan media improve their professional quality and ethics .
Head Entity: formosan association for public affairs
Tail Entity: washington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: The tech giant Google has its headquarters in Mountain View, California, where it continues to innovate and expand its services globally.  
Head Entity: Google  
Tail Entity: California  

Relation: organization stateorprovince of headquarters  
Context: The multinational corporation Unilever operates its North American headquarters in Englewood Cliffs, New Jersey, focusing on sustainable living and consumer goods.  
Head Entity: Unilever  
Tail Entity: New Jersey  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Emily, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Emily  

Relation: person other family  
Context: Michael often reminisces about the summer vacations spent with his uncle, Robert, who taught him how to fish.  
Head Entity: Michael  
Tail Entity: Robert  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  163
MixupTrain:  epoch  0, batch     0 | loss: 6.8242830MixupTrain:  epoch  0, batch     1 | loss: 7.5794983MixupTrain:  epoch  0, batch     2 | loss: 6.6686583MixupTrain:  epoch  0, batch     3 | loss: 7.3577465MixupTrain:  epoch  0, batch     4 | loss: 6.6362331MixupTrain:  epoch  0, batch     5 | loss: 7.2323879MixupTrain:  epoch  0, batch     6 | loss: 7.8118366MixupTrain:  epoch  0, batch     7 | loss: 6.8493686MixupTrain:  epoch  0, batch     8 | loss: 5.7004163MixupTrain:  epoch  0, batch     9 | loss: 6.2177081MixupTrain:  epoch  0, batch    10 | loss: 4.7672224
MemoryTrain:  epoch  0, batch     0 | loss: 3.4405737MemoryTrain:  epoch  0, batch     1 | loss: 3.2699327MemoryTrain:  epoch  0, batch     2 | loss: 3.1622179MemoryTrain:  epoch  0, batch     3 | loss: 4.2495108MemoryTrain:  epoch  1, batch     0 | loss: 2.8931413MemoryTrain:  epoch  1, batch     1 | loss: 3.7081535MemoryTrain:  epoch  1, batch     2 | loss: 3.1746850MemoryTrain:  epoch  1, batch     3 | loss: 3.0163958MemoryTrain:  epoch  2, batch     0 | loss: 2.6123075MemoryTrain:  epoch  2, batch     1 | loss: 2.6884894MemoryTrain:  epoch  2, batch     2 | loss: 2.6615355MemoryTrain:  epoch  2, batch     3 | loss: 2.8715949MemoryTrain:  epoch  3, batch     0 | loss: 2.7437487MemoryTrain:  epoch  3, batch     1 | loss: 2.3710866MemoryTrain:  epoch  3, batch     2 | loss: 2.3236637MemoryTrain:  epoch  3, batch     3 | loss: 2.0128274MemoryTrain:  epoch  4, batch     0 | loss: 2.6258919MemoryTrain:  epoch  4, batch     1 | loss: 1.9122710MemoryTrain:  epoch  4, batch     2 | loss: 2.4161344MemoryTrain:  epoch  4, batch     3 | loss: 2.1022613MemoryTrain:  epoch  5, batch     0 | loss: 2.1809475MemoryTrain:  epoch  5, batch     1 | loss: 2.1271286MemoryTrain:  epoch  5, batch     2 | loss: 1.8087717MemoryTrain:  epoch  5, batch     3 | loss: 2.6982439MemoryTrain:  epoch  6, batch     0 | loss: 2.2908013MemoryTrain:  epoch  6, batch     1 | loss: 1.7407202MemoryTrain:  epoch  6, batch     2 | loss: 1.7998222MemoryTrain:  epoch  6, batch     3 | loss: 2.0969543MemoryTrain:  epoch  7, batch     0 | loss: 2.0391977MemoryTrain:  epoch  7, batch     1 | loss: 1.8376467MemoryTrain:  epoch  7, batch     2 | loss: 1.7448816MemoryTrain:  epoch  7, batch     3 | loss: 1.5167195MemoryTrain:  epoch  8, batch     0 | loss: 1.7581460MemoryTrain:  epoch  8, batch     1 | loss: 1.8782554MemoryTrain:  epoch  8, batch     2 | loss: 1.4827778MemoryTrain:  epoch  8, batch     3 | loss: 1.7006873MemoryTrain:  epoch  9, batch     0 | loss: 1.4677277MemoryTrain:  epoch  9, batch     1 | loss: 1.5740683MemoryTrain:  epoch  9, batch     2 | loss: 1.6893617MemoryTrain:  epoch  9, batch     3 | loss: 1.7385961
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 66.83%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 62.02%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 63.16%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 63.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.12%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 72.69%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 74.79%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 75.59%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 76.14%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 74.11%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 72.57%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 70.61%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 69.90%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 69.07%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 71.22%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 71.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 71.06%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 71.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 71.81%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 71.00%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 69.98%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 69.59%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 68.99%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 69.21%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 70.50%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 70.04%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 69.60%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 69.17%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 69.06%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 68.45%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 68.35%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 68.46%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 68.65%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 68.66%   
cur_acc:  ['0.8561', '0.9018', '0.3984', '0.6683']
his_acc:  ['0.8561', '0.8444', '0.7396', '0.6866']
CurrentTrain: epoch  0, batch     0 | loss: 7.4322662CurrentTrain: epoch  0, batch     1 | loss: 7.6728873CurrentTrain: epoch  1, batch     0 | loss: 6.6411128CurrentTrain: epoch  1, batch     1 | loss: 7.2601471CurrentTrain: epoch  2, batch     0 | loss: 6.3546038CurrentTrain: epoch  2, batch     1 | loss: 5.7118268CurrentTrain: epoch  3, batch     0 | loss: 6.0976334CurrentTrain: epoch  3, batch     1 | loss: 5.2096930CurrentTrain: epoch  4, batch     0 | loss: 5.4500990CurrentTrain: epoch  4, batch     1 | loss: 5.7762699CurrentTrain: epoch  5, batch     0 | loss: 4.8892889CurrentTrain: epoch  5, batch     1 | loss: 6.2149343CurrentTrain: epoch  6, batch     0 | loss: 5.0807242CurrentTrain: epoch  6, batch     1 | loss: 4.7698851CurrentTrain: epoch  7, batch     0 | loss: 5.0870314CurrentTrain: epoch  7, batch     1 | loss: 4.3992248CurrentTrain: epoch  8, batch     0 | loss: 4.2939897CurrentTrain: epoch  8, batch     1 | loss: 4.7399955CurrentTrain: epoch  9, batch     0 | loss: 4.7616668CurrentTrain: epoch  9, batch     1 | loss: 3.8665538
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has expanded its portfolio by acquiring Fitbit, which specializes in wearable fitness technology, and Nest Labs, known for its smart home products.  
Head Entity: Alphabet Inc.  
Tail Entity: Nest Labs  

Relation: organization subsidiaries  
Context: The automotive manufacturer General Motors has several subsidiaries, including Chevrolet, which produces a range of vehicles, and Cadillac, known for its luxury cars.  
Head Entity: General Motors  
Tail Entity: Cadillac  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant Alphabet Inc. announced that it would acquire the innovative startup DeepMind Technologies, which has been a subsidiary of Alphabet since 2014. This acquisition is expected to enhance Alphabet's capabilities in artificial intelligence.  
Head Entity: DeepMind Technologies  
Tail Entity: Alphabet Inc.  

Relation: organization parents  
Context: The historical records indicate that the renowned publishing house Penguin Random House was formed through the merger of two major companies, Penguin Group and Random House, which were both influential in the literary world.  
Head Entity: Penguin Random House  
Tail Entity: Penguin Group  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: while section 106 of the hyde act openly bans indian testing , and the agreement upholds reinforces that test ban by upholding the applicability of domestic laws , washington has already recommended that the nuclear suppliers group -lrb- nsg -rrb- link its proposed exemption for india to a similar test ban .
Head Entity: nuclear suppliers group
Tail Entity: nsg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability by providing financial assistance and advice to member countries.  
Head Entity: International Monetary Fund  
Tail Entity: IMF  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of global health initiatives, especially during the COVID-19 pandemic.  
Head Entity: World Health Organization  
Tail Entity: WHO  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ in 2015, tech giant apple inc. announced plans to expand its operations in cupertino, california, where it has been headquartered since its founding.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ the multinational corporation samsung electronics is based in suwon, south korea, and has been a leader in the technology sector for decades.  
Head Entity: samsung electronics  
Tail Entity: suwon  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: during the family reunion, john introduced his sister, emily, who had just returned from studying abroad.  
Head Entity: john  
Tail Entity: emily  

Relation: person siblings  
Context: at the award ceremony, sarah proudly accepted her trophy while her brother, michael, cheered from the audience.  
Head Entity: sarah  
Tail Entity: michael  
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 7.9247322MixupTrain:  epoch  0, batch     1 | loss: 6.4509995MixupTrain:  epoch  0, batch     2 | loss: 6.6397075MixupTrain:  epoch  0, batch     3 | loss: 7.6312311MixupTrain:  epoch  0, batch     4 | loss: 5.8675635MixupTrain:  epoch  0, batch     5 | loss: 7.0881086MixupTrain:  epoch  0, batch     6 | loss: 7.0092972MixupTrain:  epoch  0, batch     7 | loss: 7.1257041MixupTrain:  epoch  0, batch     8 | loss: 5.6714565MixupTrain:  epoch  0, batch     9 | loss: 6.4433167MixupTrain:  epoch  0, batch    10 | loss: 6.1428088MixupTrain:  epoch  0, batch    11 | loss: 6.8418133MixupTrain:  epoch  0, batch    12 | loss: 6.0569668
MemoryTrain:  epoch  0, batch     0 | loss: 2.8338904MemoryTrain:  epoch  0, batch     1 | loss: 2.6689534MemoryTrain:  epoch  0, batch     2 | loss: 4.1922469MemoryTrain:  epoch  0, batch     3 | loss: 2.6552444MemoryTrain:  epoch  0, batch     4 | loss: 3.0325630MemoryTrain:  epoch  1, batch     0 | loss: 2.0851259MemoryTrain:  epoch  1, batch     1 | loss: 1.8934745MemoryTrain:  epoch  1, batch     2 | loss: 3.2330101MemoryTrain:  epoch  1, batch     3 | loss: 3.5517678MemoryTrain:  epoch  1, batch     4 | loss: 3.5927997MemoryTrain:  epoch  2, batch     0 | loss: 2.3554392MemoryTrain:  epoch  2, batch     1 | loss: 2.7266364MemoryTrain:  epoch  2, batch     2 | loss: 3.4658961MemoryTrain:  epoch  2, batch     3 | loss: 2.8092813MemoryTrain:  epoch  2, batch     4 | loss: 1.9280773MemoryTrain:  epoch  3, batch     0 | loss: 2.7790184MemoryTrain:  epoch  3, batch     1 | loss: 2.2107348MemoryTrain:  epoch  3, batch     2 | loss: 2.6157632MemoryTrain:  epoch  3, batch     3 | loss: 2.7750492MemoryTrain:  epoch  3, batch     4 | loss: 2.0934572MemoryTrain:  epoch  4, batch     0 | loss: 3.3307188MemoryTrain:  epoch  4, batch     1 | loss: 1.6338952MemoryTrain:  epoch  4, batch     2 | loss: 2.0381794MemoryTrain:  epoch  4, batch     3 | loss: 2.6345978MemoryTrain:  epoch  4, batch     4 | loss: 2.3818541MemoryTrain:  epoch  5, batch     0 | loss: 1.9077880MemoryTrain:  epoch  5, batch     1 | loss: 2.0662208MemoryTrain:  epoch  5, batch     2 | loss: 2.0167208MemoryTrain:  epoch  5, batch     3 | loss: 2.1778562MemoryTrain:  epoch  5, batch     4 | loss: 2.5316327MemoryTrain:  epoch  6, batch     0 | loss: 2.3995249MemoryTrain:  epoch  6, batch     1 | loss: 2.3729839MemoryTrain:  epoch  6, batch     2 | loss: 1.9904046MemoryTrain:  epoch  6, batch     3 | loss: 1.6274371MemoryTrain:  epoch  6, batch     4 | loss: 1.6805265MemoryTrain:  epoch  7, batch     0 | loss: 2.2778053MemoryTrain:  epoch  7, batch     1 | loss: 2.1151066MemoryTrain:  epoch  7, batch     2 | loss: 1.5514568MemoryTrain:  epoch  7, batch     3 | loss: 2.1473458MemoryTrain:  epoch  7, batch     4 | loss: 1.8088183MemoryTrain:  epoch  8, batch     0 | loss: 1.6863719MemoryTrain:  epoch  8, batch     1 | loss: 1.6621451MemoryTrain:  epoch  8, batch     2 | loss: 1.6587646MemoryTrain:  epoch  8, batch     3 | loss: 2.1465745MemoryTrain:  epoch  8, batch     4 | loss: 2.3931701MemoryTrain:  epoch  9, batch     0 | loss: 1.9733425MemoryTrain:  epoch  9, batch     1 | loss: 1.8489108MemoryTrain:  epoch  9, batch     2 | loss: 1.6624815MemoryTrain:  epoch  9, batch     3 | loss: 1.8714367MemoryTrain:  epoch  9, batch     4 | loss: 1.8426830
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 23.75%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 20.54%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 24.22%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 35.23%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 37.50%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 39.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 43.30%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 45.00%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 47.66%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 50.74%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 52.78%   [EVAL] batch:   18 | acc: 18.75%,  total acc: 50.99%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 49.38%   [EVAL] batch:   20 | acc: 12.50%,  total acc: 47.62%   [EVAL] batch:   21 | acc: 6.25%,  total acc: 45.74%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 39.84%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 44.38%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 45.45%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 45.83%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 45.19%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 44.20%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 46.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 48.53%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 49.31%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 50.99%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 52.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.76%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 56.82%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 58.42%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 60.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 61.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.22%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 63.89%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 64.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 66.46%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 66.94%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 67.99%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 67.28%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 66.25%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 65.28%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 63.68%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 62.83%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 62.02%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 62.34%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 63.11%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 63.69%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 64.24%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 64.63%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 64.27%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 64.49%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 64.67%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 64.62%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 64.22%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 64.30%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 64.62%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 64.93%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 65.45%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 66.38%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 65.89%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 65.42%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 65.16%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 64.62%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 64.38%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 64.55%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 64.90%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 65.15%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 64.74%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 64.43%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 63.68%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 62.86%   [EVAL] batch:   70 | acc: 6.25%,  total acc: 62.06%   [EVAL] batch:   71 | acc: 6.25%,  total acc: 61.28%   [EVAL] batch:   72 | acc: 31.25%,  total acc: 60.87%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 60.90%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 60.92%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 60.77%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 60.96%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 60.98%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 61.08%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 61.41%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 61.50%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 61.89%   [EVAL] batch:   82 | acc: 93.75%,  total acc: 62.27%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 62.43%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 61.84%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 61.41%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 60.78%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 60.16%   
cur_acc:  ['0.8561', '0.9018', '0.3984', '0.6683', '0.4574']
his_acc:  ['0.8561', '0.8444', '0.7396', '0.6866', '0.6016']
CurrentTrain: epoch  0, batch     0 | loss: 5.2401934CurrentTrain: epoch  0, batch     1 | loss: 5.4626927CurrentTrain: epoch  1, batch     0 | loss: 4.0288863CurrentTrain: epoch  1, batch     1 | loss: 3.6837964CurrentTrain: epoch  2, batch     0 | loss: 3.6224236CurrentTrain: epoch  2, batch     1 | loss: 2.9970329CurrentTrain: epoch  3, batch     0 | loss: 3.2054276CurrentTrain: epoch  3, batch     1 | loss: 3.0360656CurrentTrain: epoch  4, batch     0 | loss: 3.0358067CurrentTrain: epoch  4, batch     1 | loss: 2.8247347CurrentTrain: epoch  5, batch     0 | loss: 2.8468213CurrentTrain: epoch  5, batch     1 | loss: 2.7727284CurrentTrain: epoch  6, batch     0 | loss: 2.8340919CurrentTrain: epoch  6, batch     1 | loss: 2.3316753CurrentTrain: epoch  7, batch     0 | loss: 2.4915996CurrentTrain: epoch  7, batch     1 | loss: 2.2621837CurrentTrain: epoch  8, batch     0 | loss: 2.3218441CurrentTrain: epoch  8, batch     1 | loss: 2.2522829CurrentTrain: epoch  9, batch     0 | loss: 2.1537747CurrentTrain: epoch  9, batch     1 | loss: 2.1797140
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the jnf was founded in 1901 to buy plots in palestine , then ruled by the ottomans .
Head Entity: jnf
Tail Entity: 1901
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: the united nations was established in 1945 to promote international cooperation and peace.  
Head Entity: united nations  
Tail Entity: 1945  

Relation: organization founded  
Context: the world health organization was created in 1948 to coordinate global health efforts.  
Head Entity: world health organization  
Tail Entity: 1948  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith turned 30 last week.  
Head Entity: john smith  
Tail Entity: 30  

Relation: person age  
Context: the famous actor will celebrate his 45th birthday next month.  
Head Entity: the famous actor  
Tail Entity: 45  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston before relocating to london.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians, including members from the Juilliard School.  
Head Entity: Juilliard School  
Tail Entity: New York Philharmonic  

Relation: organization members  
Context: The National Football League has a number of teams, and the Dallas Cowboys are known for having a strong roster of players, many of whom have come from the University of Alabama.  
Head Entity: University of Alabama  
Tail Entity: Dallas Cowboys  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of study and reflection, Maria decided to embrace Buddhism, finding peace and purpose in its teachings.  
Head Entity: Maria  
Tail Entity: Buddhism  

Relation: person religion  
Context: During the festival, Ahmed proudly wore his traditional attire, celebrating his deep connection to Islam and its rich cultural heritage.  
Head Entity: Ahmed  
Tail Entity: Islam  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 5.5408914MixupTrain:  epoch  0, batch     1 | loss: 6.3820343MixupTrain:  epoch  0, batch     2 | loss: 5.6146951MixupTrain:  epoch  0, batch     3 | loss: 5.5419283MixupTrain:  epoch  0, batch     4 | loss: 5.4657093MixupTrain:  epoch  0, batch     5 | loss: 5.2612590MixupTrain:  epoch  0, batch     6 | loss: 5.7113632MixupTrain:  epoch  0, batch     7 | loss: 5.1597915MixupTrain:  epoch  0, batch     8 | loss: 5.4643854MixupTrain:  epoch  0, batch     9 | loss: 5.5751917MixupTrain:  epoch  0, batch    10 | loss: 5.2922113MixupTrain:  epoch  0, batch    11 | loss: 5.9565447MixupTrain:  epoch  0, batch    12 | loss: 5.7061193MixupTrain:  epoch  0, batch    13 | loss: 4.6388456
MemoryTrain:  epoch  0, batch     0 | loss: 2.3611801MemoryTrain:  epoch  0, batch     1 | loss: 2.3557751MemoryTrain:  epoch  0, batch     2 | loss: 2.9960494MemoryTrain:  epoch  0, batch     3 | loss: 3.0133014MemoryTrain:  epoch  0, batch     4 | loss: 2.8486047MemoryTrain:  epoch  0, batch     5 | loss: 2.7307007MemoryTrain:  epoch  1, batch     0 | loss: 2.2872717MemoryTrain:  epoch  1, batch     1 | loss: 2.6734107MemoryTrain:  epoch  1, batch     2 | loss: 2.8643129MemoryTrain:  epoch  1, batch     3 | loss: 2.2470465MemoryTrain:  epoch  1, batch     4 | loss: 2.1702657MemoryTrain:  epoch  1, batch     5 | loss: 2.5098088MemoryTrain:  epoch  2, batch     0 | loss: 2.2139482MemoryTrain:  epoch  2, batch     1 | loss: 2.5065274MemoryTrain:  epoch  2, batch     2 | loss: 2.0067391MemoryTrain:  epoch  2, batch     3 | loss: 1.7803379MemoryTrain:  epoch  2, batch     4 | loss: 2.0241346MemoryTrain:  epoch  2, batch     5 | loss: 2.1646056MemoryTrain:  epoch  3, batch     0 | loss: 2.2446413MemoryTrain:  epoch  3, batch     1 | loss: 2.0110140MemoryTrain:  epoch  3, batch     2 | loss: 2.2909739MemoryTrain:  epoch  3, batch     3 | loss: 1.6742501MemoryTrain:  epoch  3, batch     4 | loss: 1.7731979MemoryTrain:  epoch  3, batch     5 | loss: 1.7966808MemoryTrain:  epoch  4, batch     0 | loss: 1.7245913MemoryTrain:  epoch  4, batch     1 | loss: 1.7187067MemoryTrain:  epoch  4, batch     2 | loss: 1.8037323MemoryTrain:  epoch  4, batch     3 | loss: 1.7454977MemoryTrain:  epoch  4, batch     4 | loss: 1.7517064MemoryTrain:  epoch  4, batch     5 | loss: 1.8537073MemoryTrain:  epoch  5, batch     0 | loss: 1.5744165MemoryTrain:  epoch  5, batch     1 | loss: 1.9180170MemoryTrain:  epoch  5, batch     2 | loss: 1.9133773MemoryTrain:  epoch  5, batch     3 | loss: 1.7303641MemoryTrain:  epoch  5, batch     4 | loss: 1.4515878MemoryTrain:  epoch  5, batch     5 | loss: 1.7497876MemoryTrain:  epoch  6, batch     0 | loss: 1.4582596MemoryTrain:  epoch  6, batch     1 | loss: 1.5173972MemoryTrain:  epoch  6, batch     2 | loss: 1.4124393MemoryTrain:  epoch  6, batch     3 | loss: 1.6066351MemoryTrain:  epoch  6, batch     4 | loss: 1.9068172MemoryTrain:  epoch  6, batch     5 | loss: 1.6122160MemoryTrain:  epoch  7, batch     0 | loss: 1.8030136MemoryTrain:  epoch  7, batch     1 | loss: 1.5736122MemoryTrain:  epoch  7, batch     2 | loss: 1.6183066MemoryTrain:  epoch  7, batch     3 | loss: 1.4356972MemoryTrain:  epoch  7, batch     4 | loss: 1.5013155MemoryTrain:  epoch  7, batch     5 | loss: 1.6008070MemoryTrain:  epoch  8, batch     0 | loss: 1.5185736MemoryTrain:  epoch  8, batch     1 | loss: 1.5600924MemoryTrain:  epoch  8, batch     2 | loss: 1.6310854MemoryTrain:  epoch  8, batch     3 | loss: 1.4374294MemoryTrain:  epoch  8, batch     4 | loss: 1.4607527MemoryTrain:  epoch  8, batch     5 | loss: 1.5873295MemoryTrain:  epoch  9, batch     0 | loss: 1.3664286MemoryTrain:  epoch  9, batch     1 | loss: 1.4033694MemoryTrain:  epoch  9, batch     2 | loss: 1.5260731MemoryTrain:  epoch  9, batch     3 | loss: 1.4356630MemoryTrain:  epoch  9, batch     4 | loss: 1.3819790MemoryTrain:  epoch  9, batch     5 | loss: 1.6417494
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 83.48%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 45.83%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 53.91%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 58.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 59.13%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 57.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 57.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 58.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 58.68%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 59.54%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 60.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.20%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 63.64%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 69.21%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.12%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 71.46%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 71.77%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.46%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 72.73%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 71.51%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 70.36%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 67.06%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 66.12%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 65.06%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 65.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 67.59%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 67.90%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 68.33%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 67.80%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 67.95%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 67.98%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 67.50%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 66.79%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 66.59%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 66.27%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 66.55%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 67.98%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 68.10%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 68.43%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 68.54%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 68.34%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 67.74%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 67.56%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 67.48%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 67.69%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 67.52%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 66.98%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 66.27%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 65.49%   [EVAL] batch:   69 | acc: 12.50%,  total acc: 64.73%   [EVAL] batch:   70 | acc: 18.75%,  total acc: 64.08%   [EVAL] batch:   71 | acc: 25.00%,  total acc: 63.54%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 63.44%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 63.26%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 63.17%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 62.83%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 62.91%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 62.90%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 62.97%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 63.20%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 63.35%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 63.72%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 63.93%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 64.14%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 63.82%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 63.74%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 63.36%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 63.64%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 63.76%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 64.10%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 64.49%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 66.34%   [EVAL] batch:   96 | acc: 18.75%,  total acc: 65.85%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 65.69%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 65.97%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 66.21%   
cur_acc:  ['0.8561', '0.9018', '0.3984', '0.6683', '0.4574', '0.8348']
his_acc:  ['0.8561', '0.8444', '0.7396', '0.6866', '0.6016', '0.6621']
CurrentTrain: epoch  0, batch     0 | loss: 6.7989798CurrentTrain: epoch  0, batch     1 | loss: 6.3409228CurrentTrain: epoch  1, batch     0 | loss: 5.5850105CurrentTrain: epoch  1, batch     1 | loss: 5.1744003CurrentTrain: epoch  2, batch     0 | loss: 5.1647735CurrentTrain: epoch  2, batch     1 | loss: 4.4205866CurrentTrain: epoch  3, batch     0 | loss: 4.3122501CurrentTrain: epoch  3, batch     1 | loss: 4.7497659CurrentTrain: epoch  4, batch     0 | loss: 4.5757141CurrentTrain: epoch  4, batch     1 | loss: 3.3788843CurrentTrain: epoch  5, batch     0 | loss: 3.7124591CurrentTrain: epoch  5, batch     1 | loss: 4.1909466CurrentTrain: epoch  6, batch     0 | loss: 3.7166853CurrentTrain: epoch  6, batch     1 | loss: 3.5589116CurrentTrain: epoch  7, batch     0 | loss: 3.3863688CurrentTrain: epoch  7, batch     1 | loss: 3.4613326CurrentTrain: epoch  8, batch     0 | loss: 3.2362421CurrentTrain: epoch  8, batch     1 | loss: 3.6803334CurrentTrain: epoch  9, batch     0 | loss: 3.1056807CurrentTrain: epoch  9, batch     1 | loss: 2.7698262
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: san jose , ca , usa speaking of k-fed , him and ex-wife britney spears are in court today , dealing with their custody battle .
Head Entity: britney spears
Tail Entity: ca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York, John decided to settle in the sunny beaches of Miami, Florida, where he found a new job.  
Head Entity: John  
Tail Entity: Florida  

Relation: person stateorprovinces of residence  
Context: During her college years, Emily lived in Austin, Texas, where she developed a passion for music and arts.  
Head Entity: Emily  
Tail Entity: Texas  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away on July 20, 2020, after a long illness.  
Head Entity: The renowned author  
Tail Entity: July 20, 2020  

Relation: person date of death  
Context: She left this world peacefully in her sleep on March 5, 2015.  
Head Entity: She  
Tail Entity: March 5, 2015  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, boasts a workforce of approximately 5,500 skilled professionals across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: GreenEarth Nonprofit has grown significantly over the past few years and currently employs around 1,200 dedicated staff members to support its environmental initiatives.  
Head Entity: GreenEarth Nonprofit  
Tail Entity: 1,200  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to himself as the bard of Avon, '' said the literary critic.  
Head Entity: author  
Tail Entity: bard of Avon  

Relation: person alternate names  
Context: `` during his career, he was often called the king of pop, '' the documentary revealed.  
Head Entity: he  
Tail Entity: king of pop  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: smits stands at the center of this multigenerational saga as alex vega , the adopted son of rum and sugar baron pancho duque -lrb- elizondo -rrb- and his wife , amalia -lrb- moreno -rrb- .
Head Entity: elizondo
Tail Entity: moreno
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in the heart of the bustling city, john doe and his wife, jane doe, celebrated their 10th wedding anniversary with a grand party surrounded by family and friends.  
Head Entity: john doe  
Tail Entity: jane doe  

Relation: person spouse  
Context: during the charity event, the famous actor, robert pattinson, was seen mingling with his partner, kristen stewart, who has always been his biggest supporter.  
Head Entity: robert pattinson  
Tail Entity: kristen stewart  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 4.4543539MixupTrain:  epoch  0, batch     1 | loss: 5.2601931MixupTrain:  epoch  0, batch     2 | loss: 4.3124593MixupTrain:  epoch  0, batch     3 | loss: 4.3390845MixupTrain:  epoch  0, batch     4 | loss: 4.7365610MixupTrain:  epoch  0, batch     5 | loss: 5.1696258MixupTrain:  epoch  0, batch     6 | loss: 4.6879208MixupTrain:  epoch  0, batch     7 | loss: 5.4716375MixupTrain:  epoch  0, batch     8 | loss: 4.3899086MixupTrain:  epoch  0, batch     9 | loss: 4.5636433MixupTrain:  epoch  0, batch    10 | loss: 4.0970956MixupTrain:  epoch  0, batch    11 | loss: 4.9500176MixupTrain:  epoch  0, batch    12 | loss: 4.3230986MixupTrain:  epoch  0, batch    13 | loss: 5.5031640MixupTrain:  epoch  0, batch    14 | loss: 5.0681142MixupTrain:  epoch  0, batch    15 | loss: 3.9864208
MemoryTrain:  epoch  0, batch     0 | loss: 2.3425283MemoryTrain:  epoch  0, batch     1 | loss: 2.4633455MemoryTrain:  epoch  0, batch     2 | loss: 2.4323359MemoryTrain:  epoch  0, batch     3 | loss: 1.7947968MemoryTrain:  epoch  0, batch     4 | loss: 1.9513379MemoryTrain:  epoch  0, batch     5 | loss: 2.3741846MemoryTrain:  epoch  0, batch     6 | loss: 2.5175648MemoryTrain:  epoch  1, batch     0 | loss: 2.6291509MemoryTrain:  epoch  1, batch     1 | loss: 1.7606651MemoryTrain:  epoch  1, batch     2 | loss: 2.0837088MemoryTrain:  epoch  1, batch     3 | loss: 2.2143116MemoryTrain:  epoch  1, batch     4 | loss: 1.5478175MemoryTrain:  epoch  1, batch     5 | loss: 1.4895110MemoryTrain:  epoch  1, batch     6 | loss: 1.9434309MemoryTrain:  epoch  2, batch     0 | loss: 1.7703211MemoryTrain:  epoch  2, batch     1 | loss: 1.5529892MemoryTrain:  epoch  2, batch     2 | loss: 2.0847571MemoryTrain:  epoch  2, batch     3 | loss: 1.6995002MemoryTrain:  epoch  2, batch     4 | loss: 2.0907948MemoryTrain:  epoch  2, batch     5 | loss: 1.9262698MemoryTrain:  epoch  2, batch     6 | loss: 1.7662953MemoryTrain:  epoch  3, batch     0 | loss: 2.0212526MemoryTrain:  epoch  3, batch     1 | loss: 1.5170929MemoryTrain:  epoch  3, batch     2 | loss: 1.5316327MemoryTrain:  epoch  3, batch     3 | loss: 1.7203524MemoryTrain:  epoch  3, batch     4 | loss: 1.5741332MemoryTrain:  epoch  3, batch     5 | loss: 1.7466807MemoryTrain:  epoch  3, batch     6 | loss: 1.3830026MemoryTrain:  epoch  4, batch     0 | loss: 1.7319504MemoryTrain:  epoch  4, batch     1 | loss: 1.6504012MemoryTrain:  epoch  4, batch     2 | loss: 1.4917922MemoryTrain:  epoch  4, batch     3 | loss: 1.4040070MemoryTrain:  epoch  4, batch     4 | loss: 1.5284714MemoryTrain:  epoch  4, batch     5 | loss: 1.9934926MemoryTrain:  epoch  4, batch     6 | loss: 1.4731077MemoryTrain:  epoch  5, batch     0 | loss: 1.5307494MemoryTrain:  epoch  5, batch     1 | loss: 1.5708774MemoryTrain:  epoch  5, batch     2 | loss: 1.5171468MemoryTrain:  epoch  5, batch     3 | loss: 1.6119094MemoryTrain:  epoch  5, batch     4 | loss: 1.7859995MemoryTrain:  epoch  5, batch     5 | loss: 1.4121826MemoryTrain:  epoch  5, batch     6 | loss: 1.3900542MemoryTrain:  epoch  6, batch     0 | loss: 1.7616756MemoryTrain:  epoch  6, batch     1 | loss: 1.4527581MemoryTrain:  epoch  6, batch     2 | loss: 1.5276207MemoryTrain:  epoch  6, batch     3 | loss: 1.4549599MemoryTrain:  epoch  6, batch     4 | loss: 1.4181509MemoryTrain:  epoch  6, batch     5 | loss: 1.2880598MemoryTrain:  epoch  6, batch     6 | loss: 1.3198487MemoryTrain:  epoch  7, batch     0 | loss: 1.5525348MemoryTrain:  epoch  7, batch     1 | loss: 1.4528131MemoryTrain:  epoch  7, batch     2 | loss: 1.4435244MemoryTrain:  epoch  7, batch     3 | loss: 1.5781000MemoryTrain:  epoch  7, batch     4 | loss: 1.3760374MemoryTrain:  epoch  7, batch     5 | loss: 1.3922744MemoryTrain:  epoch  7, batch     6 | loss: 1.2567453MemoryTrain:  epoch  8, batch     0 | loss: 1.3940213MemoryTrain:  epoch  8, batch     1 | loss: 1.5473728MemoryTrain:  epoch  8, batch     2 | loss: 1.3581796MemoryTrain:  epoch  8, batch     3 | loss: 1.3101882MemoryTrain:  epoch  8, batch     4 | loss: 1.4198806MemoryTrain:  epoch  8, batch     5 | loss: 1.3554592MemoryTrain:  epoch  8, batch     6 | loss: 1.6872711MemoryTrain:  epoch  9, batch     0 | loss: 1.3758105MemoryTrain:  epoch  9, batch     1 | loss: 1.4848073MemoryTrain:  epoch  9, batch     2 | loss: 1.3602371MemoryTrain:  epoch  9, batch     3 | loss: 1.4111983MemoryTrain:  epoch  9, batch     4 | loss: 1.2848792MemoryTrain:  epoch  9, batch     5 | loss: 1.3583848MemoryTrain:  epoch  9, batch     6 | loss: 1.2838080
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 77.92%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 57.59%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 58.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 59.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 60.53%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 64.49%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 66.93%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 69.23%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 69.91%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 70.98%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 71.77%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 72.08%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 72.38%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.05%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 73.30%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 71.88%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 70.54%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 68.75%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 66.89%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 65.79%   [EVAL] batch:   38 | acc: 12.50%,  total acc: 64.42%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 64.53%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 65.92%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 66.42%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 66.76%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 67.08%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 66.03%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 66.22%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.93%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 66.07%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 65.38%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 64.71%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 64.54%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 64.39%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 64.70%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 65.90%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 65.84%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 66.31%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 66.09%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 65.22%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 64.58%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 64.36%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 64.62%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 64.39%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 63.90%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 63.14%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 62.59%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 62.14%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 61.71%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 61.28%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 61.13%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 60.98%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 60.83%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 60.53%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 60.55%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 60.58%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 60.68%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 60.86%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 61.11%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 61.43%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 61.75%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 61.90%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 61.47%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 61.12%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 60.70%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 61.01%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 61.17%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 61.53%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 61.95%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 62.36%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 62.77%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 63.16%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 63.55%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 63.87%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 63.47%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 63.27%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 63.57%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 63.75%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 63.99%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 64.15%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 64.32%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 64.42%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 64.70%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 64.98%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 65.19%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 65.99%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 65.79%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 65.82%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 65.90%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 65.98%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 65.73%   
cur_acc:  ['0.8561', '0.9018', '0.3984', '0.6683', '0.4574', '0.8348', '0.7792']
his_acc:  ['0.8561', '0.8444', '0.7396', '0.6866', '0.6016', '0.6621', '0.6573']
CurrentTrain: epoch  0, batch     0 | loss: 4.9700804CurrentTrain: epoch  0, batch     1 | loss: 5.3118405CurrentTrain: epoch  1, batch     0 | loss: 3.6564045CurrentTrain: epoch  1, batch     1 | loss: 3.7254350CurrentTrain: epoch  2, batch     0 | loss: 3.5676539CurrentTrain: epoch  2, batch     1 | loss: 3.0212300CurrentTrain: epoch  3, batch     0 | loss: 2.9634027CurrentTrain: epoch  3, batch     1 | loss: 2.7480364CurrentTrain: epoch  4, batch     0 | loss: 2.8265066CurrentTrain: epoch  4, batch     1 | loss: 2.4703255CurrentTrain: epoch  5, batch     0 | loss: 2.4113841CurrentTrain: epoch  5, batch     1 | loss: 2.3108974CurrentTrain: epoch  6, batch     0 | loss: 2.2216296CurrentTrain: epoch  6, batch     1 | loss: 2.0650690CurrentTrain: epoch  7, batch     0 | loss: 2.0870488CurrentTrain: epoch  7, batch     1 | loss: 2.1201150CurrentTrain: epoch  8, batch     0 | loss: 1.9915166CurrentTrain: epoch  8, batch     1 | loss: 2.0847523CurrentTrain: epoch  9, batch     0 | loss: 2.0032649CurrentTrain: epoch  9, batch     1 | loss: 1.9243913
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones settled in san francisco, where she found a job in tech. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith lived in boston, enjoying the vibrant culture and history of the city. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: born in baltimore in 1922 , parren mitchell was a graduate of morgan state college and earned a master 's degree from the university of maryland , according to biographical information supplied by cummings ' office .
Head Entity: parren mitchell
Tail Entity: university of maryland
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: after completing high school in 1995, jessica went on to study at the university of california, los angeles, where she earned her bachelor's degree in sociology.  
Head Entity: jessica  
Tail Entity: university of california, los angeles  

Relation: person schools attended  
Context: during his early years, steven attended the prestigious harvard university, where he majored in computer science and graduated with honors.  
Head Entity: steven  
Tail Entity: harvard university  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: beirut , lebanon -lrb- ap -rrb- sheik abbas musawi , hezbollah 's secretary-general , his wife and son were killed in february 1992 when israeli helicopters fired rockets at his car in southern lebanon .
Head Entity: abbas musawi
Tail Entity: southern lebanon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: in 1945, the renowned physicist albert einstein passed away in his home in princeton, new jersey, after a long battle with illness.  
Head Entity: albert einstein  
Tail Entity: new jersey  

Relation: person country of death  
Context: the famous author ernest hemingway died by suicide in 1961 at his home in ketchum, idaho, leaving behind a legacy of literary masterpieces.  
Head Entity: ernest hemingway  
Tail Entity: idaho  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the former mayor was facing serious allegations related to corruption and bribery.  
Head Entity: former mayor  
Tail Entity: corruption  

Relation: person charges  
Context: After a lengthy investigation, the authorities confirmed that the celebrity was charged with tax evasion.  
Head Entity: celebrity  
Tail Entity: tax evasion  
Mixup data size:  285
MixupTrain:  epoch  0, batch     0 | loss: 4.6819200MixupTrain:  epoch  0, batch     1 | loss: 4.9864795MixupTrain:  epoch  0, batch     2 | loss: 4.3637112MixupTrain:  epoch  0, batch     3 | loss: 4.2175506MixupTrain:  epoch  0, batch     4 | loss: 4.0181671MixupTrain:  epoch  0, batch     5 | loss: 3.9927031MixupTrain:  epoch  0, batch     6 | loss: 4.4853244MixupTrain:  epoch  0, batch     7 | loss: 4.3254321MixupTrain:  epoch  0, batch     8 | loss: 4.4893502MixupTrain:  epoch  0, batch     9 | loss: 4.4200979MixupTrain:  epoch  0, batch    10 | loss: 4.0002793MixupTrain:  epoch  0, batch    11 | loss: 3.9206360MixupTrain:  epoch  0, batch    12 | loss: 3.8004785MixupTrain:  epoch  0, batch    13 | loss: 3.6320978MixupTrain:  epoch  0, batch    14 | loss: 5.1700669MixupTrain:  epoch  0, batch    15 | loss: 4.2787803MixupTrain:  epoch  0, batch    16 | loss: 4.3568570MixupTrain:  epoch  0, batch    17 | loss: 4.3228287
MemoryTrain:  epoch  0, batch     0 | loss: 2.1761973MemoryTrain:  epoch  0, batch     1 | loss: 1.7700793MemoryTrain:  epoch  0, batch     2 | loss: 2.0467353MemoryTrain:  epoch  0, batch     3 | loss: 2.1172996MemoryTrain:  epoch  0, batch     4 | loss: 1.9372816MemoryTrain:  epoch  0, batch     5 | loss: 2.1534433MemoryTrain:  epoch  0, batch     6 | loss: 2.3489552MemoryTrain:  epoch  0, batch     7 | loss: 2.3536656MemoryTrain:  epoch  1, batch     0 | loss: 2.2440109MemoryTrain:  epoch  1, batch     1 | loss: 1.7687789MemoryTrain:  epoch  1, batch     2 | loss: 2.1723170MemoryTrain:  epoch  1, batch     3 | loss: 1.3694685MemoryTrain:  epoch  1, batch     4 | loss: 2.2712240MemoryTrain:  epoch  1, batch     5 | loss: 1.8687716MemoryTrain:  epoch  1, batch     6 | loss: 1.8621097MemoryTrain:  epoch  1, batch     7 | loss: 1.5330839MemoryTrain:  epoch  2, batch     0 | loss: 2.2251773MemoryTrain:  epoch  2, batch     1 | loss: 1.7027380MemoryTrain:  epoch  2, batch     2 | loss: 1.4854939MemoryTrain:  epoch  2, batch     3 | loss: 1.6022177MemoryTrain:  epoch  2, batch     4 | loss: 1.5111125MemoryTrain:  epoch  2, batch     5 | loss: 1.5491754MemoryTrain:  epoch  2, batch     6 | loss: 1.9446163MemoryTrain:  epoch  2, batch     7 | loss: 1.6265904MemoryTrain:  epoch  3, batch     0 | loss: 1.3165466MemoryTrain:  epoch  3, batch     1 | loss: 1.4462591MemoryTrain:  epoch  3, batch     2 | loss: 1.3571540MemoryTrain:  epoch  3, batch     3 | loss: 1.7493999MemoryTrain:  epoch  3, batch     4 | loss: 1.9411058MemoryTrain:  epoch  3, batch     5 | loss: 1.3871374MemoryTrain:  epoch  3, batch     6 | loss: 1.6357189MemoryTrain:  epoch  3, batch     7 | loss: 1.4252299MemoryTrain:  epoch  4, batch     0 | loss: 1.6219909MemoryTrain:  epoch  4, batch     1 | loss: 1.4517891MemoryTrain:  epoch  4, batch     2 | loss: 1.6993296MemoryTrain:  epoch  4, batch     3 | loss: 1.5131148MemoryTrain:  epoch  4, batch     4 | loss: 1.2561356MemoryTrain:  epoch  4, batch     5 | loss: 1.4891270MemoryTrain:  epoch  4, batch     6 | loss: 1.6700022MemoryTrain:  epoch  4, batch     7 | loss: 1.4179178MemoryTrain:  epoch  5, batch     0 | loss: 1.3816172MemoryTrain:  epoch  5, batch     1 | loss: 1.3818476MemoryTrain:  epoch  5, batch     2 | loss: 1.4580486MemoryTrain:  epoch  5, batch     3 | loss: 1.2459604MemoryTrain:  epoch  5, batch     4 | loss: 1.6547339MemoryTrain:  epoch  5, batch     5 | loss: 1.4344537MemoryTrain:  epoch  5, batch     6 | loss: 1.3612278MemoryTrain:  epoch  5, batch     7 | loss: 1.4509665MemoryTrain:  epoch  6, batch     0 | loss: 1.3475368MemoryTrain:  epoch  6, batch     1 | loss: 1.2561010MemoryTrain:  epoch  6, batch     2 | loss: 1.3926157MemoryTrain:  epoch  6, batch     3 | loss: 1.2914342MemoryTrain:  epoch  6, batch     4 | loss: 1.2590386MemoryTrain:  epoch  6, batch     5 | loss: 1.3800459MemoryTrain:  epoch  6, batch     6 | loss: 1.5187626MemoryTrain:  epoch  6, batch     7 | loss: 1.5056937MemoryTrain:  epoch  7, batch     0 | loss: 1.5004377MemoryTrain:  epoch  7, batch     1 | loss: 1.2905302MemoryTrain:  epoch  7, batch     2 | loss: 1.2666357MemoryTrain:  epoch  7, batch     3 | loss: 1.2854174MemoryTrain:  epoch  7, batch     4 | loss: 1.3961141MemoryTrain:  epoch  7, batch     5 | loss: 1.2702626MemoryTrain:  epoch  7, batch     6 | loss: 1.4502659MemoryTrain:  epoch  7, batch     7 | loss: 1.5161250MemoryTrain:  epoch  8, batch     0 | loss: 1.4117749MemoryTrain:  epoch  8, batch     1 | loss: 1.3335389MemoryTrain:  epoch  8, batch     2 | loss: 1.3347843MemoryTrain:  epoch  8, batch     3 | loss: 1.3542205MemoryTrain:  epoch  8, batch     4 | loss: 1.2407192MemoryTrain:  epoch  8, batch     5 | loss: 1.2932918MemoryTrain:  epoch  8, batch     6 | loss: 1.4177384MemoryTrain:  epoch  8, batch     7 | loss: 1.3760797MemoryTrain:  epoch  9, batch     0 | loss: 1.4616463MemoryTrain:  epoch  9, batch     1 | loss: 1.4416467MemoryTrain:  epoch  9, batch     2 | loss: 1.2510080MemoryTrain:  epoch  9, batch     3 | loss: 1.3156221MemoryTrain:  epoch  9, batch     4 | loss: 1.2675668MemoryTrain:  epoch  9, batch     5 | loss: 1.3025007MemoryTrain:  epoch  9, batch     6 | loss: 1.2740129MemoryTrain:  epoch  9, batch     7 | loss: 1.2466090
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 76.14%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 81.25%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 37.50%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 42.19%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 46.88%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 48.30%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 48.96%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 47.60%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 46.43%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 47.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 48.05%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 49.63%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 50.35%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 51.32%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 52.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.76%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 56.53%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 58.42%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 59.90%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 61.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 62.74%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 63.66%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 66.46%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 66.94%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 67.80%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 66.18%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 64.46%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 62.67%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 60.98%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 59.54%   [EVAL] batch:   38 | acc: 6.25%,  total acc: 58.17%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 58.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 59.30%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 59.67%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 60.32%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 60.37%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 60.83%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 60.05%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 60.11%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 60.33%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 59.88%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 59.44%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 59.62%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 59.67%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 59.03%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 58.52%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 58.26%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 58.62%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 59.22%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 59.58%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 59.43%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 58.67%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 58.04%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 57.71%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 57.88%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 57.67%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 57.28%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 56.71%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 56.25%   [EVAL] batch:   69 | acc: 37.50%,  total acc: 55.98%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 55.90%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 55.73%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 55.65%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 55.49%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 55.42%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 55.18%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 55.28%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 55.29%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 55.46%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 55.55%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 55.94%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 56.33%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 56.63%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 56.77%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 56.10%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 55.67%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 55.03%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 55.40%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 55.69%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 56.11%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 56.59%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 57.07%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 57.53%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 57.98%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 58.42%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 58.66%   [EVAL] batch:   96 | acc: 18.75%,  total acc: 58.25%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 58.10%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 58.46%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 58.81%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 59.03%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 59.07%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 59.04%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 59.01%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 59.29%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 59.32%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 59.64%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 60.01%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 60.32%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 60.62%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 60.64%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 60.49%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 60.56%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 60.58%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 60.71%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 60.83%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 60.95%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 61.12%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 61.24%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 61.35%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 61.36%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 61.53%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 61.84%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 62.05%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 62.10%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 62.10%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 62.25%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 62.55%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 62.84%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 63.12%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 63.67%   
cur_acc:  ['0.8561', '0.9018', '0.3984', '0.6683', '0.4574', '0.8348', '0.7792', '0.8125']
his_acc:  ['0.8561', '0.8444', '0.7396', '0.6866', '0.6016', '0.6621', '0.6573', '0.6367']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.2381439CurrentTrain: epoch  0, batch     1 | loss: 13.2184143CurrentTrain: epoch  0, batch     2 | loss: 12.8331337CurrentTrain: epoch  0, batch     3 | loss: 12.8587294CurrentTrain: epoch  0, batch     4 | loss: 12.5705509CurrentTrain: epoch  0, batch     5 | loss: 12.4302750CurrentTrain: epoch  0, batch     6 | loss: 12.6609669CurrentTrain: epoch  0, batch     7 | loss: 12.1857176CurrentTrain: epoch  0, batch     8 | loss: 12.3870964CurrentTrain: epoch  0, batch     9 | loss: 12.1295414CurrentTrain: epoch  0, batch    10 | loss: 12.0586987CurrentTrain: epoch  0, batch    11 | loss: 11.8807487CurrentTrain: epoch  0, batch    12 | loss: 11.8927736CurrentTrain: epoch  0, batch    13 | loss: 11.3369226CurrentTrain: epoch  0, batch    14 | loss: 11.8249102CurrentTrain: epoch  0, batch    15 | loss: 11.3543606CurrentTrain: epoch  0, batch    16 | loss: 11.2635956CurrentTrain: epoch  0, batch    17 | loss: 11.4939919CurrentTrain: epoch  0, batch    18 | loss: 11.1784992CurrentTrain: epoch  0, batch    19 | loss: 10.9208298CurrentTrain: epoch  0, batch    20 | loss: 10.9880028CurrentTrain: epoch  0, batch    21 | loss: 11.1253948CurrentTrain: epoch  0, batch    22 | loss: 11.3360043CurrentTrain: epoch  0, batch    23 | loss: 10.6067724CurrentTrain: epoch  0, batch    24 | loss: 11.3924446CurrentTrain: epoch  0, batch    25 | loss: 10.5197315CurrentTrain: epoch  0, batch    26 | loss: 11.2010784CurrentTrain: epoch  0, batch    27 | loss: 10.9841614CurrentTrain: epoch  0, batch    28 | loss: 10.9776440CurrentTrain: epoch  0, batch    29 | loss: 10.7958860CurrentTrain: epoch  0, batch    30 | loss: 10.8572998CurrentTrain: epoch  0, batch    31 | loss: 10.6450329CurrentTrain: epoch  0, batch    32 | loss: 10.4231367CurrentTrain: epoch  0, batch    33 | loss: 10.5617065CurrentTrain: epoch  0, batch    34 | loss: 10.3347960CurrentTrain: epoch  0, batch    35 | loss: 10.3540983CurrentTrain: epoch  0, batch    36 | loss: 10.2582417CurrentTrain: epoch  0, batch    37 | loss: 10.4088402CurrentTrain: epoch  1, batch     0 | loss: 10.0330391CurrentTrain: epoch  1, batch     1 | loss: 10.1931343CurrentTrain: epoch  1, batch     2 | loss: 9.2819881CurrentTrain: epoch  1, batch     3 | loss: 10.0088196CurrentTrain: epoch  1, batch     4 | loss: 10.1496391CurrentTrain: epoch  1, batch     5 | loss: 9.1106653CurrentTrain: epoch  1, batch     6 | loss: 9.9037724CurrentTrain: epoch  1, batch     7 | loss: 9.6083870CurrentTrain: epoch  1, batch     8 | loss: 10.5067816CurrentTrain: epoch  1, batch     9 | loss: 9.7668915CurrentTrain: epoch  1, batch    10 | loss: 10.4178810CurrentTrain: epoch  1, batch    11 | loss: 9.8356428CurrentTrain: epoch  1, batch    12 | loss: 9.1732483CurrentTrain: epoch  1, batch    13 | loss: 9.0493488CurrentTrain: epoch  1, batch    14 | loss: 9.5763340CurrentTrain: epoch  1, batch    15 | loss: 8.7041693CurrentTrain: epoch  1, batch    16 | loss: 9.2306461CurrentTrain: epoch  1, batch    17 | loss: 8.6418180CurrentTrain: epoch  1, batch    18 | loss: 9.2334690CurrentTrain: epoch  1, batch    19 | loss: 8.9045420CurrentTrain: epoch  1, batch    20 | loss: 8.9737196CurrentTrain: epoch  1, batch    21 | loss: 8.8186636CurrentTrain: epoch  1, batch    22 | loss: 9.0340643CurrentTrain: epoch  1, batch    23 | loss: 9.2194862CurrentTrain: epoch  1, batch    24 | loss: 8.8590736CurrentTrain: epoch  1, batch    25 | loss: 8.7643328CurrentTrain: epoch  1, batch    26 | loss: 8.4720001CurrentTrain: epoch  1, batch    27 | loss: 8.8666801CurrentTrain: epoch  1, batch    28 | loss: 9.3140163CurrentTrain: epoch  1, batch    29 | loss: 9.3679485CurrentTrain: epoch  1, batch    30 | loss: 9.4417353CurrentTrain: epoch  1, batch    31 | loss: 8.7721281CurrentTrain: epoch  1, batch    32 | loss: 8.2201033CurrentTrain: epoch  1, batch    33 | loss: 8.5807190CurrentTrain: epoch  1, batch    34 | loss: 8.1844616CurrentTrain: epoch  1, batch    35 | loss: 8.2043486CurrentTrain: epoch  1, batch    36 | loss: 8.3934450CurrentTrain: epoch  1, batch    37 | loss: 8.9368868CurrentTrain: epoch  2, batch     0 | loss: 7.9362717CurrentTrain: epoch  2, batch     1 | loss: 8.4822836CurrentTrain: epoch  2, batch     2 | loss: 9.0550222CurrentTrain: epoch  2, batch     3 | loss: 7.8789902CurrentTrain: epoch  2, batch     4 | loss: 7.6144252CurrentTrain: epoch  2, batch     5 | loss: 8.6590290CurrentTrain: epoch  2, batch     6 | loss: 8.3736153CurrentTrain: epoch  2, batch     7 | loss: 8.9810257CurrentTrain: epoch  2, batch     8 | loss: 8.3133450CurrentTrain: epoch  2, batch     9 | loss: 8.2565422CurrentTrain: epoch  2, batch    10 | loss: 8.4541864CurrentTrain: epoch  2, batch    11 | loss: 7.7959385CurrentTrain: epoch  2, batch    12 | loss: 7.5652599CurrentTrain: epoch  2, batch    13 | loss: 7.8029919CurrentTrain: epoch  2, batch    14 | loss: 8.5945301CurrentTrain: epoch  2, batch    15 | loss: 8.0052948CurrentTrain: epoch  2, batch    16 | loss: 8.0766277CurrentTrain: epoch  2, batch    17 | loss: 8.3735199CurrentTrain: epoch  2, batch    18 | loss: 8.1169052CurrentTrain: epoch  2, batch    19 | loss: 8.2601452CurrentTrain: epoch  2, batch    20 | loss: 8.3129797CurrentTrain: epoch  2, batch    21 | loss: 7.6094866CurrentTrain: epoch  2, batch    22 | loss: 8.7177267CurrentTrain: epoch  2, batch    23 | loss: 7.7068052CurrentTrain: epoch  2, batch    24 | loss: 7.6094313CurrentTrain: epoch  2, batch    25 | loss: 7.7425785CurrentTrain: epoch  2, batch    26 | loss: 7.7500143CurrentTrain: epoch  2, batch    27 | loss: 8.0464449CurrentTrain: epoch  2, batch    28 | loss: 7.5250263CurrentTrain: epoch  2, batch    29 | loss: 7.6666179CurrentTrain: epoch  2, batch    30 | loss: 6.9630814CurrentTrain: epoch  2, batch    31 | loss: 7.3585920CurrentTrain: epoch  2, batch    32 | loss: 7.3097768CurrentTrain: epoch  2, batch    33 | loss: 7.4523978CurrentTrain: epoch  2, batch    34 | loss: 8.1020947CurrentTrain: epoch  2, batch    35 | loss: 7.0816793CurrentTrain: epoch  2, batch    36 | loss: 8.4182510CurrentTrain: epoch  2, batch    37 | loss: 7.8897200CurrentTrain: epoch  3, batch     0 | loss: 8.0868273CurrentTrain: epoch  3, batch     1 | loss: 7.0551958CurrentTrain: epoch  3, batch     2 | loss: 7.9745755CurrentTrain: epoch  3, batch     3 | loss: 7.3981023CurrentTrain: epoch  3, batch     4 | loss: 6.9544902CurrentTrain: epoch  3, batch     5 | loss: 6.8552475CurrentTrain: epoch  3, batch     6 | loss: 7.4831867CurrentTrain: epoch  3, batch     7 | loss: 6.4313107CurrentTrain: epoch  3, batch     8 | loss: 7.7462435CurrentTrain: epoch  3, batch     9 | loss: 6.9388480CurrentTrain: epoch  3, batch    10 | loss: 7.5765362CurrentTrain: epoch  3, batch    11 | loss: 6.6239119CurrentTrain: epoch  3, batch    12 | loss: 7.1779757CurrentTrain: epoch  3, batch    13 | loss: 6.6104426CurrentTrain: epoch  3, batch    14 | loss: 7.9223289CurrentTrain: epoch  3, batch    15 | loss: 6.9774685CurrentTrain: epoch  3, batch    16 | loss: 7.4211264CurrentTrain: epoch  3, batch    17 | loss: 6.3152990CurrentTrain: epoch  3, batch    18 | loss: 6.7391672CurrentTrain: epoch  3, batch    19 | loss: 6.8721619CurrentTrain: epoch  3, batch    20 | loss: 6.1065702CurrentTrain: epoch  3, batch    21 | loss: 7.3203764CurrentTrain: epoch  3, batch    22 | loss: 6.6711082CurrentTrain: epoch  3, batch    23 | loss: 6.9030280CurrentTrain: epoch  3, batch    24 | loss: 8.1721611CurrentTrain: epoch  3, batch    25 | loss: 7.1133261CurrentTrain: epoch  3, batch    26 | loss: 7.3062277CurrentTrain: epoch  3, batch    27 | loss: 6.2354603CurrentTrain: epoch  3, batch    28 | loss: 8.7650661CurrentTrain: epoch  3, batch    29 | loss: 8.1592445CurrentTrain: epoch  3, batch    30 | loss: 7.5426087CurrentTrain: epoch  3, batch    31 | loss: 7.1880198CurrentTrain: epoch  3, batch    32 | loss: 7.4604225CurrentTrain: epoch  3, batch    33 | loss: 8.3118029CurrentTrain: epoch  3, batch    34 | loss: 7.5878749CurrentTrain: epoch  3, batch    35 | loss: 6.7640858CurrentTrain: epoch  3, batch    36 | loss: 7.3519115CurrentTrain: epoch  3, batch    37 | loss: 6.1277738CurrentTrain: epoch  4, batch     0 | loss: 6.8327923CurrentTrain: epoch  4, batch     1 | loss: 7.2510738CurrentTrain: epoch  4, batch     2 | loss: 7.1442842CurrentTrain: epoch  4, batch     3 | loss: 6.9252858CurrentTrain: epoch  4, batch     4 | loss: 7.1364861CurrentTrain: epoch  4, batch     5 | loss: 7.3489122CurrentTrain: epoch  4, batch     6 | loss: 6.6799526CurrentTrain: epoch  4, batch     7 | loss: 6.9837351CurrentTrain: epoch  4, batch     8 | loss: 6.9278846CurrentTrain: epoch  4, batch     9 | loss: 7.3746281CurrentTrain: epoch  4, batch    10 | loss: 5.5796499CurrentTrain: epoch  4, batch    11 | loss: 7.3748317CurrentTrain: epoch  4, batch    12 | loss: 6.7892838CurrentTrain: epoch  4, batch    13 | loss: 6.6096406CurrentTrain: epoch  4, batch    14 | loss: 6.8814588CurrentTrain: epoch  4, batch    15 | loss: 7.2459116CurrentTrain: epoch  4, batch    16 | loss: 7.4030008CurrentTrain: epoch  4, batch    17 | loss: 6.6545649CurrentTrain: epoch  4, batch    18 | loss: 7.4066439CurrentTrain: epoch  4, batch    19 | loss: 6.0836535CurrentTrain: epoch  4, batch    20 | loss: 6.2197237CurrentTrain: epoch  4, batch    21 | loss: 6.0504413CurrentTrain: epoch  4, batch    22 | loss: 7.3805971CurrentTrain: epoch  4, batch    23 | loss: 6.0531197CurrentTrain: epoch  4, batch    24 | loss: 6.4565725CurrentTrain: epoch  4, batch    25 | loss: 6.6482630CurrentTrain: epoch  4, batch    26 | loss: 7.2287703CurrentTrain: epoch  4, batch    27 | loss: 5.9069662CurrentTrain: epoch  4, batch    28 | loss: 6.4413776CurrentTrain: epoch  4, batch    29 | loss: 6.1621618CurrentTrain: epoch  4, batch    30 | loss: 6.1184421CurrentTrain: epoch  4, batch    31 | loss: 6.8244205CurrentTrain: epoch  4, batch    32 | loss: 7.3862619CurrentTrain: epoch  4, batch    33 | loss: 7.5837393CurrentTrain: epoch  4, batch    34 | loss: 6.1120453CurrentTrain: epoch  4, batch    35 | loss: 5.8425231CurrentTrain: epoch  4, batch    36 | loss: 6.5208044CurrentTrain: epoch  4, batch    37 | loss: 7.0271206CurrentTrain: epoch  5, batch     0 | loss: 6.0708108CurrentTrain: epoch  5, batch     1 | loss: 6.9682045CurrentTrain: epoch  5, batch     2 | loss: 6.3374891CurrentTrain: epoch  5, batch     3 | loss: 6.6529379CurrentTrain: epoch  5, batch     4 | loss: 6.2683554CurrentTrain: epoch  5, batch     5 | loss: 6.1346531CurrentTrain: epoch  5, batch     6 | loss: 6.0312095CurrentTrain: epoch  5, batch     7 | loss: 5.9233189CurrentTrain: epoch  5, batch     8 | loss: 6.1150565CurrentTrain: epoch  5, batch     9 | loss: 6.3574176CurrentTrain: epoch  5, batch    10 | loss: 6.3124962CurrentTrain: epoch  5, batch    11 | loss: 6.6166029CurrentTrain: epoch  5, batch    12 | loss: 5.7942095CurrentTrain: epoch  5, batch    13 | loss: 6.0483727CurrentTrain: epoch  5, batch    14 | loss: 5.9071369CurrentTrain: epoch  5, batch    15 | loss: 6.4428258CurrentTrain: epoch  5, batch    16 | loss: 6.0244961CurrentTrain: epoch  5, batch    17 | loss: 6.8548093CurrentTrain: epoch  5, batch    18 | loss: 6.3204455CurrentTrain: epoch  5, batch    19 | loss: 6.0723386CurrentTrain: epoch  5, batch    20 | loss: 5.8606977CurrentTrain: epoch  5, batch    21 | loss: 6.5849471CurrentTrain: epoch  5, batch    22 | loss: 7.1307292CurrentTrain: epoch  5, batch    23 | loss: 6.3730206CurrentTrain: epoch  5, batch    24 | loss: 5.6059170CurrentTrain: epoch  5, batch    25 | loss: 6.2854071CurrentTrain: epoch  5, batch    26 | loss: 6.2411275CurrentTrain: epoch  5, batch    27 | loss: 5.9202824CurrentTrain: epoch  5, batch    28 | loss: 6.0834317CurrentTrain: epoch  5, batch    29 | loss: 6.1397719CurrentTrain: epoch  5, batch    30 | loss: 6.9075379CurrentTrain: epoch  5, batch    31 | loss: 6.9230309CurrentTrain: epoch  5, batch    32 | loss: 6.0529404CurrentTrain: epoch  5, batch    33 | loss: 5.9588237CurrentTrain: epoch  5, batch    34 | loss: 5.5524893CurrentTrain: epoch  5, batch    35 | loss: 6.0515676CurrentTrain: epoch  5, batch    36 | loss: 6.6068702CurrentTrain: epoch  5, batch    37 | loss: 6.4925518CurrentTrain: epoch  6, batch     0 | loss: 6.4376502CurrentTrain: epoch  6, batch     1 | loss: 6.1610174CurrentTrain: epoch  6, batch     2 | loss: 6.0785131CurrentTrain: epoch  6, batch     3 | loss: 5.5595112CurrentTrain: epoch  6, batch     4 | loss: 6.1431756CurrentTrain: epoch  6, batch     5 | loss: 6.1291809CurrentTrain: epoch  6, batch     6 | loss: 6.2281094CurrentTrain: epoch  6, batch     7 | loss: 6.0440054CurrentTrain: epoch  6, batch     8 | loss: 5.4999714CurrentTrain: epoch  6, batch     9 | loss: 5.9789085CurrentTrain: epoch  6, batch    10 | loss: 5.5481758CurrentTrain: epoch  6, batch    11 | loss: 5.5085897CurrentTrain: epoch  6, batch    12 | loss: 5.7273660CurrentTrain: epoch  6, batch    13 | loss: 5.8522525CurrentTrain: epoch  6, batch    14 | loss: 5.7660060CurrentTrain: epoch  6, batch    15 | loss: 5.5134630CurrentTrain: epoch  6, batch    16 | loss: 5.9526930CurrentTrain: epoch  6, batch    17 | loss: 5.7295532CurrentTrain: epoch  6, batch    18 | loss: 5.8252630CurrentTrain: epoch  6, batch    19 | loss: 5.3900557CurrentTrain: epoch  6, batch    20 | loss: 5.9820905CurrentTrain: epoch  6, batch    21 | loss: 5.6137114CurrentTrain: epoch  6, batch    22 | loss: 5.7797365CurrentTrain: epoch  6, batch    23 | loss: 5.5285339CurrentTrain: epoch  6, batch    24 | loss: 5.6974010CurrentTrain: epoch  6, batch    25 | loss: 5.5346241CurrentTrain: epoch  6, batch    26 | loss: 6.0115089CurrentTrain: epoch  6, batch    27 | loss: 5.6058340CurrentTrain: epoch  6, batch    28 | loss: 6.1115808CurrentTrain: epoch  6, batch    29 | loss: 6.7152128CurrentTrain: epoch  6, batch    30 | loss: 6.3446088CurrentTrain: epoch  6, batch    31 | loss: 6.2296152CurrentTrain: epoch  6, batch    32 | loss: 6.0410938CurrentTrain: epoch  6, batch    33 | loss: 5.3768835CurrentTrain: epoch  6, batch    34 | loss: 5.5291834CurrentTrain: epoch  6, batch    35 | loss: 5.5529957CurrentTrain: epoch  6, batch    36 | loss: 5.6290550CurrentTrain: epoch  6, batch    37 | loss: 5.3082433CurrentTrain: epoch  7, batch     0 | loss: 5.7618122CurrentTrain: epoch  7, batch     1 | loss: 5.9027395CurrentTrain: epoch  7, batch     2 | loss: 5.7686949CurrentTrain: epoch  7, batch     3 | loss: 5.9019942CurrentTrain: epoch  7, batch     4 | loss: 5.2243547CurrentTrain: epoch  7, batch     5 | loss: 5.5539837CurrentTrain: epoch  7, batch     6 | loss: 5.3439951CurrentTrain: epoch  7, batch     7 | loss: 5.3319402CurrentTrain: epoch  7, batch     8 | loss: 5.2984524CurrentTrain: epoch  7, batch     9 | loss: 5.9302692CurrentTrain: epoch  7, batch    10 | loss: 5.5012932CurrentTrain: epoch  7, batch    11 | loss: 5.3045406CurrentTrain: epoch  7, batch    12 | loss: 6.1255455CurrentTrain: epoch  7, batch    13 | loss: 5.3646526CurrentTrain: epoch  7, batch    14 | loss: 5.4465570CurrentTrain: epoch  7, batch    15 | loss: 5.1587434CurrentTrain: epoch  7, batch    16 | loss: 5.1925941CurrentTrain: epoch  7, batch    17 | loss: 5.4249744CurrentTrain: epoch  7, batch    18 | loss: 6.0222225CurrentTrain: epoch  7, batch    19 | loss: 5.4023657CurrentTrain: epoch  7, batch    20 | loss: 5.3710923CurrentTrain: epoch  7, batch    21 | loss: 5.9872885CurrentTrain: epoch  7, batch    22 | loss: 5.2301841CurrentTrain: epoch  7, batch    23 | loss: 5.2882009CurrentTrain: epoch  7, batch    24 | loss: 5.5335722CurrentTrain: epoch  7, batch    25 | loss: 5.4457932CurrentTrain: epoch  7, batch    26 | loss: 5.1024957CurrentTrain: epoch  7, batch    27 | loss: 5.5883837CurrentTrain: epoch  7, batch    28 | loss: 5.4499717CurrentTrain: epoch  7, batch    29 | loss: 5.5523734CurrentTrain: epoch  7, batch    30 | loss: 5.1548195CurrentTrain: epoch  7, batch    31 | loss: 5.3734531CurrentTrain: epoch  7, batch    32 | loss: 5.2267938CurrentTrain: epoch  7, batch    33 | loss: 5.3681135CurrentTrain: epoch  7, batch    34 | loss: 6.2836914CurrentTrain: epoch  7, batch    35 | loss: 5.2443676CurrentTrain: epoch  7, batch    36 | loss: 5.1986694CurrentTrain: epoch  7, batch    37 | loss: 5.2359047CurrentTrain: epoch  8, batch     0 | loss: 5.6141577CurrentTrain: epoch  8, batch     1 | loss: 5.3101616CurrentTrain: epoch  8, batch     2 | loss: 5.4291835CurrentTrain: epoch  8, batch     3 | loss: 5.1818199CurrentTrain: epoch  8, batch     4 | loss: 5.0688510CurrentTrain: epoch  8, batch     5 | loss: 5.3850622CurrentTrain: epoch  8, batch     6 | loss: 5.1952143CurrentTrain: epoch  8, batch     7 | loss: 5.1206055CurrentTrain: epoch  8, batch     8 | loss: 5.1260138CurrentTrain: epoch  8, batch     9 | loss: 5.1438408CurrentTrain: epoch  8, batch    10 | loss: 5.0956497CurrentTrain: epoch  8, batch    11 | loss: 5.0987148CurrentTrain: epoch  8, batch    12 | loss: 5.0228987CurrentTrain: epoch  8, batch    13 | loss: 5.3498516CurrentTrain: epoch  8, batch    14 | loss: 5.4379835CurrentTrain: epoch  8, batch    15 | loss: 5.2809095CurrentTrain: epoch  8, batch    16 | loss: 5.4279933CurrentTrain: epoch  8, batch    17 | loss: 5.1647134CurrentTrain: epoch  8, batch    18 | loss: 5.1083021CurrentTrain: epoch  8, batch    19 | loss: 5.3500805CurrentTrain: epoch  8, batch    20 | loss: 5.2358546CurrentTrain: epoch  8, batch    21 | loss: 5.1648951CurrentTrain: epoch  8, batch    22 | loss: 5.2363396CurrentTrain: epoch  8, batch    23 | loss: 4.9962068CurrentTrain: epoch  8, batch    24 | loss: 4.8594022CurrentTrain: epoch  8, batch    25 | loss: 5.1695366CurrentTrain: epoch  8, batch    26 | loss: 5.1184225CurrentTrain: epoch  8, batch    27 | loss: 5.1962242CurrentTrain: epoch  8, batch    28 | loss: 5.1321974CurrentTrain: epoch  8, batch    29 | loss: 5.0353441CurrentTrain: epoch  8, batch    30 | loss: 5.1842303CurrentTrain: epoch  8, batch    31 | loss: 5.0084877CurrentTrain: epoch  8, batch    32 | loss: 5.0184259CurrentTrain: epoch  8, batch    33 | loss: 4.9707770CurrentTrain: epoch  8, batch    34 | loss: 5.4150305CurrentTrain: epoch  8, batch    35 | loss: 5.0645723CurrentTrain: epoch  8, batch    36 | loss: 4.9514785CurrentTrain: epoch  8, batch    37 | loss: 5.6055827CurrentTrain: epoch  9, batch     0 | loss: 4.9353161CurrentTrain: epoch  9, batch     1 | loss: 5.0374503CurrentTrain: epoch  9, batch     2 | loss: 5.0131183CurrentTrain: epoch  9, batch     3 | loss: 5.0418520CurrentTrain: epoch  9, batch     4 | loss: 5.3903971CurrentTrain: epoch  9, batch     5 | loss: 4.9899092CurrentTrain: epoch  9, batch     6 | loss: 5.0241909CurrentTrain: epoch  9, batch     7 | loss: 5.2882004CurrentTrain: epoch  9, batch     8 | loss: 5.0499783CurrentTrain: epoch  9, batch     9 | loss: 5.0273571CurrentTrain: epoch  9, batch    10 | loss: 5.0442948CurrentTrain: epoch  9, batch    11 | loss: 5.0897932CurrentTrain: epoch  9, batch    12 | loss: 5.0318599CurrentTrain: epoch  9, batch    13 | loss: 4.9063053CurrentTrain: epoch  9, batch    14 | loss: 5.0258913CurrentTrain: epoch  9, batch    15 | loss: 5.1829519CurrentTrain: epoch  9, batch    16 | loss: 5.0534353CurrentTrain: epoch  9, batch    17 | loss: 5.0950336CurrentTrain: epoch  9, batch    18 | loss: 4.9964304CurrentTrain: epoch  9, batch    19 | loss: 4.9924197CurrentTrain: epoch  9, batch    20 | loss: 4.9649134CurrentTrain: epoch  9, batch    21 | loss: 5.0024824CurrentTrain: epoch  9, batch    22 | loss: 5.0232491CurrentTrain: epoch  9, batch    23 | loss: 5.0325313CurrentTrain: epoch  9, batch    24 | loss: 4.9812922CurrentTrain: epoch  9, batch    25 | loss: 5.0040379CurrentTrain: epoch  9, batch    26 | loss: 4.9266796CurrentTrain: epoch  9, batch    27 | loss: 4.9931221CurrentTrain: epoch  9, batch    28 | loss: 5.1864533CurrentTrain: epoch  9, batch    29 | loss: 5.0266085CurrentTrain: epoch  9, batch    30 | loss: 4.8867617CurrentTrain: epoch  9, batch    31 | loss: 4.9583611CurrentTrain: epoch  9, batch    32 | loss: 4.8135653CurrentTrain: epoch  9, batch    33 | loss: 5.2654867CurrentTrain: epoch  9, batch    34 | loss: 5.2372494CurrentTrain: epoch  9, batch    35 | loss: 5.0998039CurrentTrain: epoch  9, batch    36 | loss: 5.0642552CurrentTrain: epoch  9, batch    37 | loss: 4.8135624
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots and family.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in the tech industry, John Smith moved to Canada to enjoy a quieter lifestyle surrounded by nature and a welcoming community.  
Head Entity: John Smith  
Tail Entity: Canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` this continued delay is an outrage against these service members and the more than 14,000 who have already lost their jobs under this discriminatory law , '' said us army veteran aubrey sarvis , executive director of the servicemembers legal defense network .
Head Entity: servicemembers legal defense network
Tail Entity: aubrey sarvis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: `` as the CEO of the tech giant, she has been instrumental in shaping the company's vision and leading its top talent, including the brilliant engineer who developed the groundbreaking software. ''  
Head Entity: tech giant  
Tail Entity: brilliant engineer  

Relation: organization top members employees  
Context: `` during the annual meeting, the board of directors praised the contributions of their lead designer, who has been with the company since its inception and has played a crucial role in its success. ''  
Head Entity: company  
Tail Entity: lead designer  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union (EU) is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: the former detainee , murat kurnaz , was turned over to german authorities and freed in 2006 after a personal plea from german chancellor angela merkel .
Head Entity: murat kurnaz
Tail Entity: german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist, albert einstein, was born in the kingdom of wurttemberg in the german empire in 1879.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author, gabriel garcía márquez, hailed from the coastal city of aracataca in colombia, where he drew inspiration for many of his works.  
Head Entity: gabriel garcía márquez  
Tail Entity: colombian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Lopez was promoted to Chief Financial Officer at Tech Innovations, where she now oversees all financial operations."  
Head Entity: Maria Lopez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the annual conference, Dr. James Smith, the lead researcher at the National Institute of Health, presented his findings on the latest advancements in medical technology."  
Head Entity: Dr. James Smith  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: taiwan 's largest telecom operator chunghwa telecom began to sell two wp7 models -- the htc hd7 and the htc 7 mozart -- on nov. 11 , while the country 's second - largest telecom carrier , taiwan mobile co. , offered only the hd7 model .
Head Entity: chunghwa telecom
Tail Entity: taiwan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: samsung electronics, a leading global technology company, has its headquarters in suwon, south korea, where it develops innovative products and solutions for consumers worldwide.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization country of headquarters  
Context: the multinational corporation nestlé, known for its food and beverage products, is headquartered in vevey, switzerland, and operates in numerous countries around the globe.  
Head Entity: nestlé  
Tail Entity: switzerland  
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   
cur_acc:  ['0.8617']
his_acc:  ['0.8617']
CurrentTrain: epoch  0, batch     0 | loss: 6.2797942CurrentTrain: epoch  0, batch     1 | loss: 6.4019613CurrentTrain: epoch  1, batch     0 | loss: 5.9193964CurrentTrain: epoch  1, batch     1 | loss: 4.8026938CurrentTrain: epoch  2, batch     0 | loss: 5.0739951CurrentTrain: epoch  2, batch     1 | loss: 5.1765246CurrentTrain: epoch  3, batch     0 | loss: 5.0675259CurrentTrain: epoch  3, batch     1 | loss: 4.0706773CurrentTrain: epoch  4, batch     0 | loss: 4.2800579CurrentTrain: epoch  4, batch     1 | loss: 4.3162050CurrentTrain: epoch  5, batch     0 | loss: 4.0332265CurrentTrain: epoch  5, batch     1 | loss: 3.4309866CurrentTrain: epoch  6, batch     0 | loss: 3.3927922CurrentTrain: epoch  6, batch     1 | loss: 4.0033789CurrentTrain: epoch  7, batch     0 | loss: 3.5397458CurrentTrain: epoch  7, batch     1 | loss: 3.3420193CurrentTrain: epoch  8, batch     0 | loss: 3.4796233CurrentTrain: epoch  8, batch     1 | loss: 2.9944813CurrentTrain: epoch  9, batch     0 | loss: 2.8057799CurrentTrain: epoch  9, batch     1 | loss: 3.2307661
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: the chairman of the senate foreign relations committee , massachusetts democrat john kerry , and the panel 's top republican , richard lugar of indiana , were at the white house meeting , which was led by vice president joe biden , a former chairman of the foreign relations panel .
Head Entity: john kerry
Tail Entity: massachusetts
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York City, the famous author J.K. Rowling settled in Edinburgh, where she found inspiration for her next book.  
Head Entity: J.K. Rowling  
Tail Entity: Edinburgh  

Relation: person stateorprovinces of residence  
Context: During the press conference, the governor of California, Gavin Newsom, announced new policies aimed at improving the state's education system.  
Head Entity: Gavin Newsom  
Tail Entity: California  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away on July 20, 2020, after a long illness.  
Head Entity: The renowned author  
Tail Entity: July 20, 2020  

Relation: person date of death  
Context: She left this world peacefully in her sleep on March 5, 2015.  
Head Entity: She  
Tail Entity: March 5, 2015  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, the newly formed entity, Global Finance Corp, reported an impressive headcount of 15,000 employees, making it one of the largest firms in the industry.  
Head Entity: Global Finance Corp  
Tail Entity: 15,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to as the bard of Avon, is known for his timeless plays and sonnets. ''  
Head Entity: William Shakespeare  
Tail Entity: bard of Avon  

Relation: person alternate names  
Context: `` in the world of music, he is often called the King of Pop, a title that reflects his immense influence and popularity. ''  
Head Entity: Michael Jackson  
Tail Entity: King of Pop  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a lavish ceremony held in new york city, the couple exchanged vows in front of family and friends: john legend and chrissy teigen celebrated their love with a beautiful wedding.  
Head Entity: john legend  
Tail Entity: chrissy teigen  

Relation: person spouse  
Context: during the annual charity gala, it was announced that the famous actor and his long-time partner have tied the knot: ben affleck and jennifer garner are now officially married.  
Head Entity: ben affleck  
Tail Entity: jennifer garner  
Mixup data size:  105
MixupTrain:  epoch  0, batch     0 | loss: 12.7577366MixupTrain:  epoch  0, batch     1 | loss: 12.3116018MixupTrain:  epoch  0, batch     2 | loss: 12.7870472MixupTrain:  epoch  0, batch     3 | loss: 12.0059608MixupTrain:  epoch  0, batch     4 | loss: 10.6301918MixupTrain:  epoch  0, batch     5 | loss: 10.5788872MixupTrain:  epoch  0, batch     6 | loss: 9.1571157
MemoryTrain:  epoch  0, batch     0 | loss: 4.4178343MemoryTrain:  epoch  0, batch     1 | loss: 4.8802161MemoryTrain:  epoch  0, batch     2 | loss: 4.4532781MemoryTrain:  epoch  1, batch     0 | loss: 4.2497387MemoryTrain:  epoch  1, batch     1 | loss: 3.6800089MemoryTrain:  epoch  1, batch     2 | loss: 3.5889132MemoryTrain:  epoch  2, batch     0 | loss: 3.2310410MemoryTrain:  epoch  2, batch     1 | loss: 3.3342428MemoryTrain:  epoch  2, batch     2 | loss: 3.1855068MemoryTrain:  epoch  3, batch     0 | loss: 2.9336572MemoryTrain:  epoch  3, batch     1 | loss: 2.9681756MemoryTrain:  epoch  3, batch     2 | loss: 1.1292304MemoryTrain:  epoch  4, batch     0 | loss: 2.3783154MemoryTrain:  epoch  4, batch     1 | loss: 3.0091796MemoryTrain:  epoch  4, batch     2 | loss: 1.5079519MemoryTrain:  epoch  5, batch     0 | loss: 2.8646472MemoryTrain:  epoch  5, batch     1 | loss: 2.2111621MemoryTrain:  epoch  5, batch     2 | loss: 1.4983435MemoryTrain:  epoch  6, batch     0 | loss: 2.0088832MemoryTrain:  epoch  6, batch     1 | loss: 2.5002317MemoryTrain:  epoch  6, batch     2 | loss: 2.0995603MemoryTrain:  epoch  7, batch     0 | loss: 2.3314705MemoryTrain:  epoch  7, batch     1 | loss: 2.2124085MemoryTrain:  epoch  7, batch     2 | loss: 3.2354629MemoryTrain:  epoch  8, batch     0 | loss: 2.1744094MemoryTrain:  epoch  8, batch     1 | loss: 2.4067950MemoryTrain:  epoch  8, batch     2 | loss: 1.4629694MemoryTrain:  epoch  9, batch     0 | loss: 1.8346634MemoryTrain:  epoch  9, batch     1 | loss: 2.1859698MemoryTrain:  epoch  9, batch     2 | loss: 2.4707582
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 41.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 68.75%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 84.82%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.42%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.91%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 84.93%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 83.39%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 81.94%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 81.08%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 81.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 82.16%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 81.10%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 81.53%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 82.07%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 81.91%   
cur_acc:  ['0.8617', '0.6875']
his_acc:  ['0.8617', '0.8191']
CurrentTrain: epoch  0, batch     0 | loss: 6.7706194CurrentTrain: epoch  0, batch     1 | loss: 7.1350031CurrentTrain: epoch  1, batch     0 | loss: 6.3817825CurrentTrain: epoch  1, batch     1 | loss: 5.0830226CurrentTrain: epoch  2, batch     0 | loss: 5.7402182CurrentTrain: epoch  2, batch     1 | loss: 5.5990076CurrentTrain: epoch  3, batch     0 | loss: 5.0742083CurrentTrain: epoch  3, batch     1 | loss: 4.7599311CurrentTrain: epoch  4, batch     0 | loss: 4.5754642CurrentTrain: epoch  4, batch     1 | loss: 3.9104924CurrentTrain: epoch  5, batch     0 | loss: 3.9459174CurrentTrain: epoch  5, batch     1 | loss: 3.5914438CurrentTrain: epoch  6, batch     0 | loss: 3.9174333CurrentTrain: epoch  6, batch     1 | loss: 3.8694305CurrentTrain: epoch  7, batch     0 | loss: 3.4160099CurrentTrain: epoch  7, batch     1 | loss: 3.2539017CurrentTrain: epoch  8, batch     0 | loss: 3.3369713CurrentTrain: epoch  8, batch     1 | loss: 2.6457651CurrentTrain: epoch  9, batch     0 | loss: 2.8750119CurrentTrain: epoch  9, batch     1 | loss: 3.0477695
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: born in a small village in italy on march 15, 1980, luca grew up surrounded by the beautiful countryside.  
Head Entity: luca  
Tail Entity: march 15, 1980  

Relation: person date of birth  
Context: the famous author was born in new york city on september 5, 1975, where he later found inspiration for his novels.  
Head Entity: the famous author  
Tail Entity: september 5, 1975  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, to parents of polish descent.  
Head Entity: martha stewart  
Tail Entity: new jersey  

Relation: person stateorprovince of birth  
Context: barack obama was born on august 4, 1961, in honolulu, hawaii, where he spent most of his childhood.  
Head Entity: barack obama  
Tail Entity: hawaii  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Sample 1:  
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, John, to her friends, highlighting how much he had influenced her life choices.  
Head Entity: her father  
Tail Entity: John  

Sample 2:  
Relation: person parents  
Context: After the ceremony, Emily shared stories about her mother, who had always been her biggest supporter and role model throughout her life.  
Head Entity: her mother  
Tail Entity: Emily
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst in the investment division.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california last night.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, succumbed to her health issues and died in a hospital in nevada.  
Head Entity: elizabeth taylor  
Tail Entity: nevada  
Mixup data size:  133
MixupTrain:  epoch  0, batch     0 | loss: 8.6799244MixupTrain:  epoch  0, batch     1 | loss: 8.1046913MixupTrain:  epoch  0, batch     2 | loss: 8.1060623MixupTrain:  epoch  0, batch     3 | loss: 6.6378192MixupTrain:  epoch  0, batch     4 | loss: 7.2948552MixupTrain:  epoch  0, batch     5 | loss: 8.0806109MixupTrain:  epoch  0, batch     6 | loss: 7.5987327MixupTrain:  epoch  0, batch     7 | loss: 6.6903701MixupTrain:  epoch  0, batch     8 | loss: 4.9911399
MemoryTrain:  epoch  0, batch     0 | loss: 2.4219053MemoryTrain:  epoch  0, batch     1 | loss: 3.3208694MemoryTrain:  epoch  0, batch     2 | loss: 3.2540445MemoryTrain:  epoch  1, batch     0 | loss: 2.6324599MemoryTrain:  epoch  1, batch     1 | loss: 2.9604006MemoryTrain:  epoch  1, batch     2 | loss: 2.6476438MemoryTrain:  epoch  2, batch     0 | loss: 2.3298802MemoryTrain:  epoch  2, batch     1 | loss: 2.6856196MemoryTrain:  epoch  2, batch     2 | loss: 2.0172706MemoryTrain:  epoch  3, batch     0 | loss: 2.2886505MemoryTrain:  epoch  3, batch     1 | loss: 2.1021261MemoryTrain:  epoch  3, batch     2 | loss: 1.8851861MemoryTrain:  epoch  4, batch     0 | loss: 2.0017374MemoryTrain:  epoch  4, batch     1 | loss: 1.9373384MemoryTrain:  epoch  4, batch     2 | loss: 1.8481892MemoryTrain:  epoch  5, batch     0 | loss: 1.8951008MemoryTrain:  epoch  5, batch     1 | loss: 1.9339666MemoryTrain:  epoch  5, batch     2 | loss: 1.6307570MemoryTrain:  epoch  6, batch     0 | loss: 1.8805304MemoryTrain:  epoch  6, batch     1 | loss: 1.5964715MemoryTrain:  epoch  6, batch     2 | loss: 1.7604113MemoryTrain:  epoch  7, batch     0 | loss: 1.9068179MemoryTrain:  epoch  7, batch     1 | loss: 1.6071433MemoryTrain:  epoch  7, batch     2 | loss: 1.4793044MemoryTrain:  epoch  8, batch     0 | loss: 1.7782869MemoryTrain:  epoch  8, batch     1 | loss: 1.5135169MemoryTrain:  epoch  8, batch     2 | loss: 1.4641197MemoryTrain:  epoch  9, batch     0 | loss: 1.4471588MemoryTrain:  epoch  9, batch     1 | loss: 1.5246481MemoryTrain:  epoch  9, batch     2 | loss: 1.5901914
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 82.14%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 84.01%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 82.86%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 82.12%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 81.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 82.16%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 80.67%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 80.11%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 79.21%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 79.12%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 79.04%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 79.21%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 79.50%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 79.29%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 79.33%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 79.36%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 79.51%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 79.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 80.02%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 80.26%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 80.28%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 80.51%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 80.52%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 79.61%   
cur_acc:  ['0.8617', '0.6875', '0.8214']
his_acc:  ['0.8617', '0.8191', '0.7961']
CurrentTrain: epoch  0, batch     0 | loss: 5.0465679CurrentTrain: epoch  0, batch     1 | loss: 6.5199933CurrentTrain: epoch  1, batch     0 | loss: 4.9897370CurrentTrain: epoch  1, batch     1 | loss: 4.9092278CurrentTrain: epoch  2, batch     0 | loss: 4.6514874CurrentTrain: epoch  2, batch     1 | loss: 3.3894987CurrentTrain: epoch  3, batch     0 | loss: 3.6981328CurrentTrain: epoch  3, batch     1 | loss: 3.8452895CurrentTrain: epoch  4, batch     0 | loss: 3.6270673CurrentTrain: epoch  4, batch     1 | loss: 3.1976161CurrentTrain: epoch  5, batch     0 | loss: 3.4069867CurrentTrain: epoch  5, batch     1 | loss: 3.4331262CurrentTrain: epoch  6, batch     0 | loss: 3.4668970CurrentTrain: epoch  6, batch     1 | loss: 2.6468234CurrentTrain: epoch  7, batch     0 | loss: 3.0438714CurrentTrain: epoch  7, batch     1 | loss: 2.9432471CurrentTrain: epoch  8, batch     0 | loss: 2.9168713CurrentTrain: epoch  8, batch     1 | loss: 2.8150384CurrentTrain: epoch  9, batch     0 | loss: 2.7649724CurrentTrain: epoch  9, batch     1 | loss: 2.7436249
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones settled in san francisco, where she found a job in tech. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith lived in boston, enjoying the vibrant culture and history of the city. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: born in baltimore in 1922 , parren mitchell was a graduate of morgan state college and earned a master 's degree from the university of maryland , according to biographical information supplied by cummings ' office .
Head Entity: parren mitchell
Tail Entity: university of maryland
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: after completing high school in 1995, jessica went on to study at the university of california, los angeles, where she earned her bachelor's degree in sociology.  
Head Entity: jessica  
Tail Entity: university of california, los angeles  

Relation: person schools attended  
Context: during his early years, steven attended the prestigious harvard university, where he majored in computer science and graduated with honors.  
Head Entity: steven  
Tail Entity: harvard university  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: max  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, a crime that could result in significant prison time if convicted.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith has been charged with assault following the altercation that took place last month at the downtown bar.  
Head Entity: Smith  
Tail Entity: assault  
Mixup data size:  164
MixupTrain:  epoch  0, batch     0 | loss: 6.7794323MixupTrain:  epoch  0, batch     1 | loss: 5.6049630MixupTrain:  epoch  0, batch     2 | loss: 6.3264241MixupTrain:  epoch  0, batch     3 | loss: 6.5021380MixupTrain:  epoch  0, batch     4 | loss: 5.5973910MixupTrain:  epoch  0, batch     5 | loss: 6.5956159MixupTrain:  epoch  0, batch     6 | loss: 5.8394969MixupTrain:  epoch  0, batch     7 | loss: 5.6760945MixupTrain:  epoch  0, batch     8 | loss: 5.4247391MixupTrain:  epoch  0, batch     9 | loss: 5.8898772MixupTrain:  epoch  0, batch    10 | loss: 5.0688416
MemoryTrain:  epoch  0, batch     0 | loss: 2.0956049MemoryTrain:  epoch  0, batch     1 | loss: 3.5903885MemoryTrain:  epoch  0, batch     2 | loss: 2.9700513MemoryTrain:  epoch  0, batch     3 | loss: 2.2359483MemoryTrain:  epoch  1, batch     0 | loss: 2.4839954MemoryTrain:  epoch  1, batch     1 | loss: 2.7487166MemoryTrain:  epoch  1, batch     2 | loss: 2.1709013MemoryTrain:  epoch  1, batch     3 | loss: 2.3561513MemoryTrain:  epoch  2, batch     0 | loss: 1.8099163MemoryTrain:  epoch  2, batch     1 | loss: 2.4513164MemoryTrain:  epoch  2, batch     2 | loss: 2.5094814MemoryTrain:  epoch  2, batch     3 | loss: 2.1517406MemoryTrain:  epoch  3, batch     0 | loss: 2.0115683MemoryTrain:  epoch  3, batch     1 | loss: 2.2656145MemoryTrain:  epoch  3, batch     2 | loss: 1.5541135MemoryTrain:  epoch  3, batch     3 | loss: 2.0229745MemoryTrain:  epoch  4, batch     0 | loss: 2.2438412MemoryTrain:  epoch  4, batch     1 | loss: 1.9602026MemoryTrain:  epoch  4, batch     2 | loss: 1.6977916MemoryTrain:  epoch  4, batch     3 | loss: 1.4567958MemoryTrain:  epoch  5, batch     0 | loss: 1.8287390MemoryTrain:  epoch  5, batch     1 | loss: 1.6051967MemoryTrain:  epoch  5, batch     2 | loss: 1.7925109MemoryTrain:  epoch  5, batch     3 | loss: 1.9598682MemoryTrain:  epoch  6, batch     0 | loss: 1.6255910MemoryTrain:  epoch  6, batch     1 | loss: 1.6864653MemoryTrain:  epoch  6, batch     2 | loss: 1.6862049MemoryTrain:  epoch  6, batch     3 | loss: 1.6772211MemoryTrain:  epoch  7, batch     0 | loss: 1.5730689MemoryTrain:  epoch  7, batch     1 | loss: 1.6454873MemoryTrain:  epoch  7, batch     2 | loss: 1.6790452MemoryTrain:  epoch  7, batch     3 | loss: 1.5645678MemoryTrain:  epoch  8, batch     0 | loss: 1.4914186MemoryTrain:  epoch  8, batch     1 | loss: 1.4948497MemoryTrain:  epoch  8, batch     2 | loss: 1.6401460MemoryTrain:  epoch  8, batch     3 | loss: 1.5687782MemoryTrain:  epoch  9, batch     0 | loss: 1.6542017MemoryTrain:  epoch  9, batch     1 | loss: 1.4228294MemoryTrain:  epoch  9, batch     2 | loss: 1.5269568MemoryTrain:  epoch  9, batch     3 | loss: 1.3585352
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 23.96%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 37.50%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 51.56%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 55.29%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 63.54%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 85.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 86.36%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 84.56%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 83.04%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 81.77%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 81.08%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 81.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 82.01%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 81.99%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 80.23%   [EVAL] batch:   43 | acc: 25.00%,  total acc: 78.98%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 77.64%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 76.22%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 75.27%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 75.26%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 74.62%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 73.88%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 72.79%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 72.24%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 71.11%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 71.54%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 71.82%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 72.03%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 72.19%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 71.72%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 70.97%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 70.44%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 69.53%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 68.65%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 68.09%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 67.63%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 67.84%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 67.95%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 68.22%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 68.49%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 70.15%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 70.59%   
cur_acc:  ['0.8617', '0.6875', '0.8214', '0.6354']
his_acc:  ['0.8617', '0.8191', '0.7961', '0.7059']
CurrentTrain: epoch  0, batch     0 | loss: 8.1261044CurrentTrain: epoch  0, batch     1 | loss: 8.8959179CurrentTrain: epoch  1, batch     0 | loss: 8.2257261CurrentTrain: epoch  1, batch     1 | loss: 7.0390940CurrentTrain: epoch  2, batch     0 | loss: 6.8586049CurrentTrain: epoch  2, batch     1 | loss: 7.6200366CurrentTrain: epoch  3, batch     0 | loss: 6.5768518CurrentTrain: epoch  3, batch     1 | loss: 6.1484003CurrentTrain: epoch  4, batch     0 | loss: 6.3614407CurrentTrain: epoch  4, batch     1 | loss: 5.7744241CurrentTrain: epoch  5, batch     0 | loss: 5.9018440CurrentTrain: epoch  5, batch     1 | loss: 6.4080791CurrentTrain: epoch  6, batch     0 | loss: 6.0420732CurrentTrain: epoch  6, batch     1 | loss: 5.9308367CurrentTrain: epoch  7, batch     0 | loss: 5.2818327CurrentTrain: epoch  7, batch     1 | loss: 5.7620535CurrentTrain: epoch  8, batch     0 | loss: 5.1092634CurrentTrain: epoch  8, batch     1 | loss: 4.9652762CurrentTrain: epoch  9, batch     0 | loss: 5.1167254CurrentTrain: epoch  9, batch     1 | loss: 4.0290756
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: `` firstgroup 's acquisition of laidlaw will considerably enhance firstgroup 's existing activities in north america , which themselves have grown strongly since we first invested in the u.s. in 1999 , '' said firstgroup chief executive moir lockhead .
Head Entity: firstgroup
Tail Entity: laidlaw
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: `` in 2015, the tech giant apple inc. announced its acquisition of beats electronics, a move that significantly expanded its presence in the music streaming industry, '' said industry analyst john doe.  
Head Entity: apple inc.  
Tail Entity: beats electronics  

Relation: organization subsidiaries  
Context: `` the merger between disney and pixar in 2006 allowed disney to leverage pixar's innovative animation technology and storytelling prowess, leading to a series of successful films, '' noted entertainment reporter jane smith.  
Head Entity: disney  
Tail Entity: pixar  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant Alphabet Inc. announced that it would acquire the popular video-sharing platform YouTube, which has been a subsidiary of Google since 2006. This acquisition is expected to enhance the company's digital advertising capabilities.  
Head Entity: YouTube  
Tail Entity: Alphabet Inc.  

Relation: organization parents  
Context: The historic partnership between the Ford Motor Company and the Lincoln Motor Company has led to significant advancements in automotive technology, with Lincoln being a luxury vehicle division under Ford's umbrella since 1922.  
Head Entity: Lincoln Motor Company  
Tail Entity: Ford Motor Company  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health crises.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant google has announced plans to expand its headquarters in the heart of san francisco, aiming to create more job opportunities and enhance its presence in the city.  
Head Entity: google  
Tail Entity: san francisco  

Relation: organization city of headquarters  
Context: ------ new york 2019-11-10 14:30:00 utc the financial services firm jp morgan chase has confirmed that its main headquarters will remain in new york city, despite discussions of relocating to other states.  
Head Entity: jp morgan chase  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: During the family reunion, John introduced his sister, Emily, who had just returned from studying abroad.  
Head Entity: John  
Tail Entity: Emily  

Relation: person siblings  
Context: After the ceremony, Sarah shared a heartfelt moment with her brother, Michael, reminiscing about their childhood adventures.  
Head Entity: Sarah  
Tail Entity: Michael  
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 7.2157546MixupTrain:  epoch  0, batch     1 | loss: 5.5603526MixupTrain:  epoch  0, batch     2 | loss: 5.8094361MixupTrain:  epoch  0, batch     3 | loss: 5.6834408MixupTrain:  epoch  0, batch     4 | loss: 6.8927621MixupTrain:  epoch  0, batch     5 | loss: 5.2027956MixupTrain:  epoch  0, batch     6 | loss: 5.3753799MixupTrain:  epoch  0, batch     7 | loss: 5.7424480MixupTrain:  epoch  0, batch     8 | loss: 7.2525962MixupTrain:  epoch  0, batch     9 | loss: 5.9572805MixupTrain:  epoch  0, batch    10 | loss: 5.4850210MixupTrain:  epoch  0, batch    11 | loss: 5.2815901MixupTrain:  epoch  0, batch    12 | loss: 4.3017697
MemoryTrain:  epoch  0, batch     0 | loss: 2.0136209MemoryTrain:  epoch  0, batch     1 | loss: 3.2531741MemoryTrain:  epoch  0, batch     2 | loss: 3.2925360MemoryTrain:  epoch  0, batch     3 | loss: 2.6651502MemoryTrain:  epoch  0, batch     4 | loss: 2.9926493MemoryTrain:  epoch  1, batch     0 | loss: 3.6958792MemoryTrain:  epoch  1, batch     1 | loss: 1.6974967MemoryTrain:  epoch  1, batch     2 | loss: 2.1446934MemoryTrain:  epoch  1, batch     3 | loss: 2.4706721MemoryTrain:  epoch  1, batch     4 | loss: 2.8422847MemoryTrain:  epoch  2, batch     0 | loss: 2.4732554MemoryTrain:  epoch  2, batch     1 | loss: 1.7381302MemoryTrain:  epoch  2, batch     2 | loss: 2.4436736MemoryTrain:  epoch  2, batch     3 | loss: 2.7660556MemoryTrain:  epoch  2, batch     4 | loss: 2.1702294MemoryTrain:  epoch  3, batch     0 | loss: 1.6506267MemoryTrain:  epoch  3, batch     1 | loss: 2.9637902MemoryTrain:  epoch  3, batch     2 | loss: 1.7046051MemoryTrain:  epoch  3, batch     3 | loss: 2.3081865MemoryTrain:  epoch  3, batch     4 | loss: 1.6713803MemoryTrain:  epoch  4, batch     0 | loss: 1.3794898MemoryTrain:  epoch  4, batch     1 | loss: 2.3158867MemoryTrain:  epoch  4, batch     2 | loss: 1.6393927MemoryTrain:  epoch  4, batch     3 | loss: 2.2083180MemoryTrain:  epoch  4, batch     4 | loss: 2.1782336MemoryTrain:  epoch  5, batch     0 | loss: 2.1843441MemoryTrain:  epoch  5, batch     1 | loss: 1.8929681MemoryTrain:  epoch  5, batch     2 | loss: 1.5235628MemoryTrain:  epoch  5, batch     3 | loss: 1.6817049MemoryTrain:  epoch  5, batch     4 | loss: 2.1239433MemoryTrain:  epoch  6, batch     0 | loss: 1.6363904MemoryTrain:  epoch  6, batch     1 | loss: 1.6173675MemoryTrain:  epoch  6, batch     2 | loss: 1.7797533MemoryTrain:  epoch  6, batch     3 | loss: 1.9138496MemoryTrain:  epoch  6, batch     4 | loss: 1.6435866MemoryTrain:  epoch  7, batch     0 | loss: 1.8492111MemoryTrain:  epoch  7, batch     1 | loss: 1.4977615MemoryTrain:  epoch  7, batch     2 | loss: 1.6104281MemoryTrain:  epoch  7, batch     3 | loss: 1.4092746MemoryTrain:  epoch  7, batch     4 | loss: 1.8527904MemoryTrain:  epoch  8, batch     0 | loss: 1.6338844MemoryTrain:  epoch  8, batch     1 | loss: 1.5517042MemoryTrain:  epoch  8, batch     2 | loss: 1.5099070MemoryTrain:  epoch  8, batch     3 | loss: 1.6785440MemoryTrain:  epoch  8, batch     4 | loss: 1.6088418MemoryTrain:  epoch  9, batch     0 | loss: 1.4206307MemoryTrain:  epoch  9, batch     1 | loss: 1.4340034MemoryTrain:  epoch  9, batch     2 | loss: 1.6923988MemoryTrain:  epoch  9, batch     3 | loss: 1.5118470MemoryTrain:  epoch  9, batch     4 | loss: 1.5068967
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 41.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 44.71%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 48.66%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 51.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 56.99%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 58.33%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 56.58%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 53.27%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 51.42%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 78.98%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.89%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 84.07%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 84.47%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 81.79%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 80.57%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 80.76%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.72%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 81.86%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 80.23%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 78.41%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 76.67%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 75.00%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 73.67%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 73.70%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 72.96%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 72.00%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 70.19%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 68.87%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 68.40%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 68.86%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 69.08%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 69.30%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 69.40%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 69.60%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 69.90%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 69.47%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 68.25%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 67.38%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 66.54%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 65.91%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 65.39%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 65.67%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 65.45%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 65.32%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 65.54%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 66.92%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 67.95%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 67.64%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 67.42%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 67.13%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 66.46%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 66.19%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 65.77%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 65.59%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 65.41%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 65.01%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 65.13%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 65.03%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 64.79%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 64.84%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 65.52%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 66.12%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 66.15%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 65.72%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 65.24%   [EVAL] batch:   98 | acc: 25.00%,  total acc: 64.84%   [EVAL] batch:   99 | acc: 6.25%,  total acc: 64.25%   
cur_acc:  ['0.8617', '0.6875', '0.8214', '0.6354', '0.5142']
his_acc:  ['0.8617', '0.8191', '0.7961', '0.7059', '0.6425']
CurrentTrain: epoch  0, batch     0 | loss: 6.0424166CurrentTrain: epoch  0, batch     1 | loss: 6.9378572CurrentTrain: epoch  1, batch     0 | loss: 5.4197803CurrentTrain: epoch  1, batch     1 | loss: 4.1890702CurrentTrain: epoch  2, batch     0 | loss: 4.9615955CurrentTrain: epoch  2, batch     1 | loss: 3.2153945CurrentTrain: epoch  3, batch     0 | loss: 3.8334134CurrentTrain: epoch  3, batch     1 | loss: 4.2148743CurrentTrain: epoch  4, batch     0 | loss: 3.7940915CurrentTrain: epoch  4, batch     1 | loss: 3.7555153CurrentTrain: epoch  5, batch     0 | loss: 4.0952363CurrentTrain: epoch  5, batch     1 | loss: 3.2215474CurrentTrain: epoch  6, batch     0 | loss: 3.1331100CurrentTrain: epoch  6, batch     1 | loss: 4.0187521CurrentTrain: epoch  7, batch     0 | loss: 3.2242651CurrentTrain: epoch  7, batch     1 | loss: 3.2029576CurrentTrain: epoch  8, batch     0 | loss: 3.2006152CurrentTrain: epoch  8, batch     1 | loss: 2.5407584CurrentTrain: epoch  9, batch     0 | loss: 3.2034969CurrentTrain: epoch  9, batch     1 | loss: 2.6377783
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, the renowned artist shared his experiences growing up in the vibrant streets of Barcelona, which he credits as the foundation of his creative journey.  
Head Entity: the renowned artist  
Tail Entity: Barcelona  

Relation: person country of birth  
Context: In her autobiography, the famous scientist details her early life in Tokyo, where she developed a passion for technology and innovation.  
Head Entity: the famous scientist  
Tail Entity: Tokyo  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit our official site at https://www.techinnovators.com for the latest updates on our projects.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For more information about our services, check out our website at http://www.greenearthsolutions.org.  
Head Entity: Green Earth Solutions  
Tail Entity: http://www.greenearthsolutions.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has recently acquired a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple inc.  

Relation: organization shareholders  
Context: the investment firm blackrock has increased its holdings in the renewable energy company sunrun.  
Head Entity: sunrun  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: those records include 46 pounds of phone bills of some 10,000 clients of her business , pamela martin and associates , from 1993 to august 2006 , sibley said .
Head Entity: pamela martin and associates
Tail Entity: august 2006
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The city council announced the dissolution of the local arts organization, Creative Minds, effective immediately, following a series of financial difficulties that began in early 2020.  
Head Entity: Creative Minds  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: After years of struggling to maintain funding, the nonprofit group, Green Future Initiative, officially ceased operations in December 2021, leaving many community projects unfinished.  
Head Entity: Green Future Initiative  
Tail Entity: December 2021  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak started apple inc., which revolutionized the personal computer industry and changed the way people interact with technology.  
Head Entity: apple inc.  
Tail Entity: steve jobs  

Relation: organization founded by  
Context: the famous fashion brand gucci was established in 1921 by guccio gucci in florence, italy, and has since become a symbol of luxury and style.  
Head Entity: gucci  
Tail Entity: guccio gucci  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 5.3440944MixupTrain:  epoch  0, batch     1 | loss: 5.3020311MixupTrain:  epoch  0, batch     2 | loss: 4.7776041MixupTrain:  epoch  0, batch     3 | loss: 5.8076435MixupTrain:  epoch  0, batch     4 | loss: 6.4864661MixupTrain:  epoch  0, batch     5 | loss: 4.4012242MixupTrain:  epoch  0, batch     6 | loss: 7.3380753MixupTrain:  epoch  0, batch     7 | loss: 5.1107998MixupTrain:  epoch  0, batch     8 | loss: 4.9749072MixupTrain:  epoch  0, batch     9 | loss: 5.0737554MixupTrain:  epoch  0, batch    10 | loss: 4.7654141MixupTrain:  epoch  0, batch    11 | loss: 4.6429680MixupTrain:  epoch  0, batch    12 | loss: 5.6800154MixupTrain:  epoch  0, batch    13 | loss: 4.9673519
MemoryTrain:  epoch  0, batch     0 | loss: 2.0612786MemoryTrain:  epoch  0, batch     1 | loss: 3.0523858MemoryTrain:  epoch  0, batch     2 | loss: 2.1808252MemoryTrain:  epoch  0, batch     3 | loss: 2.2444463MemoryTrain:  epoch  0, batch     4 | loss: 2.1703260MemoryTrain:  epoch  0, batch     5 | loss: 2.6547098MemoryTrain:  epoch  1, batch     0 | loss: 1.8171477MemoryTrain:  epoch  1, batch     1 | loss: 2.8505206MemoryTrain:  epoch  1, batch     2 | loss: 1.7902437MemoryTrain:  epoch  1, batch     3 | loss: 1.9868721MemoryTrain:  epoch  1, batch     4 | loss: 1.3879402MemoryTrain:  epoch  1, batch     5 | loss: 2.9739311MemoryTrain:  epoch  2, batch     0 | loss: 1.9088713MemoryTrain:  epoch  2, batch     1 | loss: 2.0818269MemoryTrain:  epoch  2, batch     2 | loss: 2.3066406MemoryTrain:  epoch  2, batch     3 | loss: 1.5290771MemoryTrain:  epoch  2, batch     4 | loss: 2.0473862MemoryTrain:  epoch  2, batch     5 | loss: 2.0297978MemoryTrain:  epoch  3, batch     0 | loss: 1.3250835MemoryTrain:  epoch  3, batch     1 | loss: 2.1509714MemoryTrain:  epoch  3, batch     2 | loss: 2.2765079MemoryTrain:  epoch  3, batch     3 | loss: 2.1041193MemoryTrain:  epoch  3, batch     4 | loss: 1.6667879MemoryTrain:  epoch  3, batch     5 | loss: 1.4807124MemoryTrain:  epoch  4, batch     0 | loss: 1.7916576MemoryTrain:  epoch  4, batch     1 | loss: 1.5779066MemoryTrain:  epoch  4, batch     2 | loss: 1.7642512MemoryTrain:  epoch  4, batch     3 | loss: 1.7661817MemoryTrain:  epoch  4, batch     4 | loss: 1.7194076MemoryTrain:  epoch  4, batch     5 | loss: 1.4942008MemoryTrain:  epoch  5, batch     0 | loss: 1.4966576MemoryTrain:  epoch  5, batch     1 | loss: 1.6433268MemoryTrain:  epoch  5, batch     2 | loss: 1.7584361MemoryTrain:  epoch  5, batch     3 | loss: 1.5245376MemoryTrain:  epoch  5, batch     4 | loss: 1.6415640MemoryTrain:  epoch  5, batch     5 | loss: 1.4751146MemoryTrain:  epoch  6, batch     0 | loss: 1.7670219MemoryTrain:  epoch  6, batch     1 | loss: 1.3541090MemoryTrain:  epoch  6, batch     2 | loss: 1.5947804MemoryTrain:  epoch  6, batch     3 | loss: 1.4330583MemoryTrain:  epoch  6, batch     4 | loss: 1.5824593MemoryTrain:  epoch  6, batch     5 | loss: 1.4206567MemoryTrain:  epoch  7, batch     0 | loss: 1.3498088MemoryTrain:  epoch  7, batch     1 | loss: 1.2795198MemoryTrain:  epoch  7, batch     2 | loss: 1.6688409MemoryTrain:  epoch  7, batch     3 | loss: 1.3540646MemoryTrain:  epoch  7, batch     4 | loss: 1.7316432MemoryTrain:  epoch  7, batch     5 | loss: 1.5407796MemoryTrain:  epoch  8, batch     0 | loss: 1.5551648MemoryTrain:  epoch  8, batch     1 | loss: 1.4451606MemoryTrain:  epoch  8, batch     2 | loss: 1.2903440MemoryTrain:  epoch  8, batch     3 | loss: 1.3839619MemoryTrain:  epoch  8, batch     4 | loss: 1.7456229MemoryTrain:  epoch  8, batch     5 | loss: 1.2567818MemoryTrain:  epoch  9, batch     0 | loss: 1.4640474MemoryTrain:  epoch  9, batch     1 | loss: 1.4754736MemoryTrain:  epoch  9, batch     2 | loss: 1.3896042MemoryTrain:  epoch  9, batch     3 | loss: 1.4320207MemoryTrain:  epoch  9, batch     4 | loss: 1.4523661MemoryTrain:  epoch  9, batch     5 | loss: 1.4176028
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 57.03%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 59.03%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 61.06%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 59.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 60.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 60.53%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 65.06%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 73.99%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 72.43%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 70.36%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 68.07%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 67.93%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 68.43%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 69.36%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 68.46%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 67.05%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 65.56%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 64.40%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 63.30%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 63.41%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 63.01%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 62.12%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 60.91%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 60.34%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 59.32%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 59.03%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 59.66%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 60.27%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 60.75%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 61.10%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 61.44%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 61.56%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 61.07%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 60.38%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 59.92%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 59.28%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 58.46%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 57.86%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 57.37%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 58.00%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 58.15%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 57.95%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 57.92%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 58.16%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 58.73%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 59.29%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 59.83%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 60.36%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 60.88%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 61.14%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 60.68%   [EVAL] batch:   79 | acc: 18.75%,  total acc: 60.16%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 59.57%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 59.30%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 58.89%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 58.41%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 58.09%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 57.92%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 57.54%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 57.39%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 57.16%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 56.88%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 56.87%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 57.34%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 57.80%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 58.24%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 58.55%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 58.72%   [EVAL] batch:   96 | acc: 25.00%,  total acc: 58.38%   [EVAL] batch:   97 | acc: 25.00%,  total acc: 58.04%   [EVAL] batch:   98 | acc: 25.00%,  total acc: 57.70%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 57.75%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 58.17%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 58.15%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 58.25%   [EVAL] batch:  103 | acc: 43.75%,  total acc: 58.11%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 58.15%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 58.14%   [EVAL] batch:  106 | acc: 12.50%,  total acc: 57.71%   
cur_acc:  ['0.8617', '0.6875', '0.8214', '0.6354', '0.5142', '0.5703']
his_acc:  ['0.8617', '0.8191', '0.7961', '0.7059', '0.6425', '0.5771']
CurrentTrain: epoch  0, batch     0 | loss: 5.3230891CurrentTrain: epoch  0, batch     1 | loss: 5.4008470CurrentTrain: epoch  1, batch     0 | loss: 4.0796776CurrentTrain: epoch  1, batch     1 | loss: 3.7928708CurrentTrain: epoch  2, batch     0 | loss: 3.4724076CurrentTrain: epoch  2, batch     1 | loss: 3.0957386CurrentTrain: epoch  3, batch     0 | loss: 2.7914257CurrentTrain: epoch  3, batch     1 | loss: 3.0783572CurrentTrain: epoch  4, batch     0 | loss: 2.7988157CurrentTrain: epoch  4, batch     1 | loss: 2.5563624CurrentTrain: epoch  5, batch     0 | loss: 2.6323152CurrentTrain: epoch  5, batch     1 | loss: 2.4818861CurrentTrain: epoch  6, batch     0 | loss: 2.4215190CurrentTrain: epoch  6, batch     1 | loss: 2.4787197CurrentTrain: epoch  7, batch     0 | loss: 2.4009392CurrentTrain: epoch  7, batch     1 | loss: 2.0607212CurrentTrain: epoch  8, batch     0 | loss: 2.8129268CurrentTrain: epoch  8, batch     1 | loss: 3.3523357CurrentTrain: epoch  9, batch     0 | loss: 3.2750590CurrentTrain: epoch  9, batch     1 | loss: 3.2410290
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily roberts, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily roberts  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The recent conference held by the Islamic Society of North America aimed to strengthen ties between various Muslim organizations and promote interfaith dialogue.  
Head Entity: Islamic Society of North America  
Tail Entity: Islam  

Relation: organization political religious affiliation  
Context: The Catholic Church has been actively involved in various social justice initiatives, reflecting its commitment to the teachings of Christianity.  
Head Entity: Catholic Church  
Tail Entity: Christianity  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: andrew lebow , an oil trader with mf global in new york , said investors have been discouraged by lower-than-expected oil imports in china and the disappointing growth in the u.s. economy .
Head Entity: mf global
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been a hub for innovation and development.  
Head Entity: apple inc.  
Tail Entity: cupertino, california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics has its main office situated in suwon, south korea, which plays a crucial role in its global operations.  
Head Entity: samsung electronics  
Tail Entity: suwon, south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Michael, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person other family  
Context: In her memoir, Lisa shared heartwarming stories about her grandmother, who played a significant role in her upbringing.  
Head Entity: Lisa  
Tail Entity: her grandmother  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 4.3248586MixupTrain:  epoch  0, batch     1 | loss: 4.6181790MixupTrain:  epoch  0, batch     2 | loss: 4.8624260MixupTrain:  epoch  0, batch     3 | loss: 5.0926572MixupTrain:  epoch  0, batch     4 | loss: 3.8784535MixupTrain:  epoch  0, batch     5 | loss: 3.8237059MixupTrain:  epoch  0, batch     6 | loss: 5.3710638MixupTrain:  epoch  0, batch     7 | loss: 4.5222084MixupTrain:  epoch  0, batch     8 | loss: 4.8419484MixupTrain:  epoch  0, batch     9 | loss: 4.2751446MixupTrain:  epoch  0, batch    10 | loss: 3.8185788MixupTrain:  epoch  0, batch    11 | loss: 6.5915855MixupTrain:  epoch  0, batch    12 | loss: 5.4806249MixupTrain:  epoch  0, batch    13 | loss: 4.4999779MixupTrain:  epoch  0, batch    14 | loss: 4.9699944MixupTrain:  epoch  0, batch    15 | loss: 3.8395905
MemoryTrain:  epoch  0, batch     0 | loss: 2.1539929MemoryTrain:  epoch  0, batch     1 | loss: 1.8580308MemoryTrain:  epoch  0, batch     2 | loss: 2.5951126MemoryTrain:  epoch  0, batch     3 | loss: 3.0104666MemoryTrain:  epoch  0, batch     4 | loss: 3.0706117MemoryTrain:  epoch  0, batch     5 | loss: 2.5154839MemoryTrain:  epoch  0, batch     6 | loss: 2.6344066MemoryTrain:  epoch  1, batch     0 | loss: 2.3180275MemoryTrain:  epoch  1, batch     1 | loss: 2.0810516MemoryTrain:  epoch  1, batch     2 | loss: 2.3305225MemoryTrain:  epoch  1, batch     3 | loss: 1.8967401MemoryTrain:  epoch  1, batch     4 | loss: 2.3300827MemoryTrain:  epoch  1, batch     5 | loss: 2.1492836MemoryTrain:  epoch  1, batch     6 | loss: 2.4912763MemoryTrain:  epoch  2, batch     0 | loss: 1.6574569MemoryTrain:  epoch  2, batch     1 | loss: 1.9866935MemoryTrain:  epoch  2, batch     2 | loss: 2.0608110MemoryTrain:  epoch  2, batch     3 | loss: 2.2657032MemoryTrain:  epoch  2, batch     4 | loss: 1.6349986MemoryTrain:  epoch  2, batch     5 | loss: 1.5368369MemoryTrain:  epoch  2, batch     6 | loss: 2.0560179MemoryTrain:  epoch  3, batch     0 | loss: 2.0027657MemoryTrain:  epoch  3, batch     1 | loss: 1.8891126MemoryTrain:  epoch  3, batch     2 | loss: 2.1825361MemoryTrain:  epoch  3, batch     3 | loss: 1.6259222MemoryTrain:  epoch  3, batch     4 | loss: 1.3600726MemoryTrain:  epoch  3, batch     5 | loss: 1.4569930MemoryTrain:  epoch  3, batch     6 | loss: 1.3065331MemoryTrain:  epoch  4, batch     0 | loss: 1.4578607MemoryTrain:  epoch  4, batch     1 | loss: 1.5309184MemoryTrain:  epoch  4, batch     2 | loss: 1.7876587MemoryTrain:  epoch  4, batch     3 | loss: 1.7651589MemoryTrain:  epoch  4, batch     4 | loss: 1.7846901MemoryTrain:  epoch  4, batch     5 | loss: 1.8721037MemoryTrain:  epoch  4, batch     6 | loss: 1.3682635MemoryTrain:  epoch  5, batch     0 | loss: 1.4924860MemoryTrain:  epoch  5, batch     1 | loss: 1.3809636MemoryTrain:  epoch  5, batch     2 | loss: 1.7873836MemoryTrain:  epoch  5, batch     3 | loss: 1.5455970MemoryTrain:  epoch  5, batch     4 | loss: 1.4217091MemoryTrain:  epoch  5, batch     5 | loss: 1.6337665MemoryTrain:  epoch  5, batch     6 | loss: 1.4844873MemoryTrain:  epoch  6, batch     0 | loss: 1.6594403MemoryTrain:  epoch  6, batch     1 | loss: 1.3063558MemoryTrain:  epoch  6, batch     2 | loss: 1.5898569MemoryTrain:  epoch  6, batch     3 | loss: 1.4588397MemoryTrain:  epoch  6, batch     4 | loss: 1.2264874MemoryTrain:  epoch  6, batch     5 | loss: 1.7465653MemoryTrain:  epoch  6, batch     6 | loss: 1.3926286MemoryTrain:  epoch  7, batch     0 | loss: 1.3925867MemoryTrain:  epoch  7, batch     1 | loss: 1.4760203MemoryTrain:  epoch  7, batch     2 | loss: 1.2776245MemoryTrain:  epoch  7, batch     3 | loss: 1.5121880MemoryTrain:  epoch  7, batch     4 | loss: 1.3630126MemoryTrain:  epoch  7, batch     5 | loss: 1.3798132MemoryTrain:  epoch  7, batch     6 | loss: 1.5173526MemoryTrain:  epoch  8, batch     0 | loss: 1.3807482MemoryTrain:  epoch  8, batch     1 | loss: 1.3937048MemoryTrain:  epoch  8, batch     2 | loss: 1.3851552MemoryTrain:  epoch  8, batch     3 | loss: 1.3289415MemoryTrain:  epoch  8, batch     4 | loss: 1.3904386MemoryTrain:  epoch  8, batch     5 | loss: 1.2425764MemoryTrain:  epoch  8, batch     6 | loss: 1.4029245MemoryTrain:  epoch  9, batch     0 | loss: 1.3614522MemoryTrain:  epoch  9, batch     1 | loss: 1.3034148MemoryTrain:  epoch  9, batch     2 | loss: 1.2915468MemoryTrain:  epoch  9, batch     3 | loss: 1.3587650MemoryTrain:  epoch  9, batch     4 | loss: 1.2483906MemoryTrain:  epoch  9, batch     5 | loss: 1.2833185MemoryTrain:  epoch  9, batch     6 | loss: 1.5278652
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 73.08%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 67.79%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 65.46%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.15%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 76.52%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 74.82%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 73.04%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 71.53%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 70.61%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 70.39%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 70.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 71.49%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 70.49%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 68.89%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 67.36%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 65.90%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 64.76%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 64.84%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 63.78%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 62.62%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 61.40%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 60.58%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 59.43%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 59.14%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 59.77%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 60.38%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 60.75%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 61.10%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 61.33%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 61.46%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 60.55%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 59.58%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 58.73%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 57.81%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 56.92%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 56.06%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 55.50%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 56.16%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 56.61%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 55.99%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 56.08%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 56.68%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 57.26%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 57.83%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 58.39%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 59.13%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 58.54%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 57.81%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 57.18%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 56.86%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 56.40%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 55.95%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 55.66%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 55.45%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 55.24%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 55.18%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 54.99%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 54.79%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 54.40%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 53.80%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 53.36%   [EVAL] batch:   93 | acc: 6.25%,  total acc: 52.86%   [EVAL] batch:   94 | acc: 0.00%,  total acc: 52.30%   [EVAL] batch:   95 | acc: 6.25%,  total acc: 51.82%   [EVAL] batch:   96 | acc: 0.00%,  total acc: 51.29%   [EVAL] batch:   97 | acc: 6.25%,  total acc: 50.83%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 50.38%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 50.44%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 50.93%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 50.92%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 51.09%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 51.08%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 51.19%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 51.24%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 51.17%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 51.22%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 51.32%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 51.25%   [EVAL] batch:  110 | acc: 87.50%,  total acc: 51.58%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 52.01%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 52.27%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 52.69%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 52.77%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 52.96%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 53.31%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 53.65%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 53.73%   
cur_acc:  ['0.8617', '0.6875', '0.8214', '0.6354', '0.5142', '0.5703', '0.7308']
his_acc:  ['0.8617', '0.8191', '0.7961', '0.7059', '0.6425', '0.5771', '0.5373']
CurrentTrain: epoch  0, batch     0 | loss: 5.1354666CurrentTrain: epoch  0, batch     1 | loss: 5.4653463CurrentTrain: epoch  1, batch     0 | loss: 4.1675177CurrentTrain: epoch  1, batch     1 | loss: 3.5096989CurrentTrain: epoch  2, batch     0 | loss: 3.4372675CurrentTrain: epoch  2, batch     1 | loss: 3.3857634CurrentTrain: epoch  3, batch     0 | loss: 3.0022297CurrentTrain: epoch  3, batch     1 | loss: 2.7044468CurrentTrain: epoch  4, batch     0 | loss: 2.6878879CurrentTrain: epoch  4, batch     1 | loss: 2.3993301CurrentTrain: epoch  5, batch     0 | loss: 2.4699473CurrentTrain: epoch  5, batch     1 | loss: 2.4996378CurrentTrain: epoch  6, batch     0 | loss: 2.2290361CurrentTrain: epoch  6, batch     1 | loss: 2.2166102CurrentTrain: epoch  7, batch     0 | loss: 2.0562079CurrentTrain: epoch  7, batch     1 | loss: 2.2213881CurrentTrain: epoch  8, batch     0 | loss: 2.0782845CurrentTrain: epoch  8, batch     1 | loss: 2.1893828CurrentTrain: epoch  9, batch     0 | loss: 2.0598631CurrentTrain: epoch  9, batch     1 | loss: 1.9862270
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization Green Earth Initiative was established in 2010 to promote environmental awareness and sustainability practices across communities.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, emily decided to throw a big party with all her friends.  
Head Entity: emily  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in barcelona and spent her childhood there before moving to madrid.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during the summer of 1985, michael was born in new orleans, a city known for its vibrant culture.  
Head Entity: michael  
Tail Entity: new orleans  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: it was berger who made clarke a member of the white house principals committee when it met to discuss terrorist threats , allowing an otherwise middle-ranking nsc bureaucrat to treat tenet and secretary of state madeleine albright as equals -lrb- which the empire-building clarke was pleased to do -rrb- .
Head Entity: nsc
Tail Entity: white house principals committee
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The board of directors of the tech startup includes several prominent figures from the industry, such as the CEO of Innovatech and the founder of Green Solutions, who have both played crucial roles in shaping the company's vision and strategy.  
Head Entity: tech startup  
Tail Entity: board of directors  

Relation: organization members  
Context: During the annual conference, the members of the environmental advocacy group gathered to discuss their initiatives and strategies for the upcoming year, highlighting the contributions of various local chapters across the country.  
Head Entity: environmental advocacy group  
Tail Entity: local chapters  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of study and reflection, Maria decided to embrace Buddhism, finding peace and purpose in its teachings.  
Head Entity: Maria  
Tail Entity: Buddhism  

Relation: person religion  
Context: During the festival, Ahmed proudly wore his traditional attire, celebrating his deep-rooted connection to Islam and its rich cultural heritage.  
Head Entity: Ahmed  
Tail Entity: Islam  
Mixup data size:  284
MixupTrain:  epoch  0, batch     0 | loss: 4.5039062MixupTrain:  epoch  0, batch     1 | loss: 4.9634531MixupTrain:  epoch  0, batch     2 | loss: 4.5821022MixupTrain:  epoch  0, batch     3 | loss: 3.6766644MixupTrain:  epoch  0, batch     4 | loss: 4.8810314MixupTrain:  epoch  0, batch     5 | loss: 4.6475390MixupTrain:  epoch  0, batch     6 | loss: 3.6527705MixupTrain:  epoch  0, batch     7 | loss: 5.0164536MixupTrain:  epoch  0, batch     8 | loss: 4.3928116MixupTrain:  epoch  0, batch     9 | loss: 4.7386373MixupTrain:  epoch  0, batch    10 | loss: 4.1525832MixupTrain:  epoch  0, batch    11 | loss: 3.8649660MixupTrain:  epoch  0, batch    12 | loss: 4.2555482MixupTrain:  epoch  0, batch    13 | loss: 3.7032858MixupTrain:  epoch  0, batch    14 | loss: 4.6646573MixupTrain:  epoch  0, batch    15 | loss: 3.8749208MixupTrain:  epoch  0, batch    16 | loss: 4.9006717MixupTrain:  epoch  0, batch    17 | loss: 3.9324182
MemoryTrain:  epoch  0, batch     0 | loss: 1.7406355MemoryTrain:  epoch  0, batch     1 | loss: 1.9024265MemoryTrain:  epoch  0, batch     2 | loss: 1.9702682MemoryTrain:  epoch  0, batch     3 | loss: 2.4772677MemoryTrain:  epoch  0, batch     4 | loss: 2.8567033MemoryTrain:  epoch  0, batch     5 | loss: 1.9820974MemoryTrain:  epoch  0, batch     6 | loss: 2.4016347MemoryTrain:  epoch  0, batch     7 | loss: 2.6847026MemoryTrain:  epoch  1, batch     0 | loss: 1.6920197MemoryTrain:  epoch  1, batch     1 | loss: 2.3085442MemoryTrain:  epoch  1, batch     2 | loss: 2.1252632MemoryTrain:  epoch  1, batch     3 | loss: 1.6132637MemoryTrain:  epoch  1, batch     4 | loss: 1.7637926MemoryTrain:  epoch  1, batch     5 | loss: 2.0201912MemoryTrain:  epoch  1, batch     6 | loss: 1.9470600MemoryTrain:  epoch  1, batch     7 | loss: 2.1188390MemoryTrain:  epoch  2, batch     0 | loss: 1.7211316MemoryTrain:  epoch  2, batch     1 | loss: 1.8153723MemoryTrain:  epoch  2, batch     2 | loss: 1.5756290MemoryTrain:  epoch  2, batch     3 | loss: 1.6563519MemoryTrain:  epoch  2, batch     4 | loss: 1.8199602MemoryTrain:  epoch  2, batch     5 | loss: 1.4480842MemoryTrain:  epoch  2, batch     6 | loss: 1.5028304MemoryTrain:  epoch  2, batch     7 | loss: 1.6422949MemoryTrain:  epoch  3, batch     0 | loss: 1.8879933MemoryTrain:  epoch  3, batch     1 | loss: 1.3945746MemoryTrain:  epoch  3, batch     2 | loss: 1.4524564MemoryTrain:  epoch  3, batch     3 | loss: 1.5246677MemoryTrain:  epoch  3, batch     4 | loss: 1.4125814MemoryTrain:  epoch  3, batch     5 | loss: 1.2758690MemoryTrain:  epoch  3, batch     6 | loss: 1.8834451MemoryTrain:  epoch  3, batch     7 | loss: 1.5828031MemoryTrain:  epoch  4, batch     0 | loss: 1.4258108MemoryTrain:  epoch  4, batch     1 | loss: 1.3233554MemoryTrain:  epoch  4, batch     2 | loss: 1.7486989MemoryTrain:  epoch  4, batch     3 | loss: 1.5167880MemoryTrain:  epoch  4, batch     4 | loss: 1.6820261MemoryTrain:  epoch  4, batch     5 | loss: 1.3931289MemoryTrain:  epoch  4, batch     6 | loss: 1.3184409MemoryTrain:  epoch  4, batch     7 | loss: 1.4129019MemoryTrain:  epoch  5, batch     0 | loss: 1.3065014MemoryTrain:  epoch  5, batch     1 | loss: 1.4570159MemoryTrain:  epoch  5, batch     2 | loss: 1.4579751MemoryTrain:  epoch  5, batch     3 | loss: 1.4196348MemoryTrain:  epoch  5, batch     4 | loss: 1.4416292MemoryTrain:  epoch  5, batch     5 | loss: 1.3453491MemoryTrain:  epoch  5, batch     6 | loss: 1.4088328MemoryTrain:  epoch  5, batch     7 | loss: 1.3069221MemoryTrain:  epoch  6, batch     0 | loss: 1.2708120MemoryTrain:  epoch  6, batch     1 | loss: 1.3081856MemoryTrain:  epoch  6, batch     2 | loss: 1.3677808MemoryTrain:  epoch  6, batch     3 | loss: 1.4316369MemoryTrain:  epoch  6, batch     4 | loss: 1.4921805MemoryTrain:  epoch  6, batch     5 | loss: 1.3309988MemoryTrain:  epoch  6, batch     6 | loss: 1.3923976MemoryTrain:  epoch  6, batch     7 | loss: 1.3026614MemoryTrain:  epoch  7, batch     0 | loss: 1.4558758MemoryTrain:  epoch  7, batch     1 | loss: 1.2901196MemoryTrain:  epoch  7, batch     2 | loss: 1.3898561MemoryTrain:  epoch  7, batch     3 | loss: 1.2979463MemoryTrain:  epoch  7, batch     4 | loss: 1.2732568MemoryTrain:  epoch  7, batch     5 | loss: 1.4414437MemoryTrain:  epoch  7, batch     6 | loss: 1.4233036MemoryTrain:  epoch  7, batch     7 | loss: 1.4091128MemoryTrain:  epoch  8, batch     0 | loss: 1.5180674MemoryTrain:  epoch  8, batch     1 | loss: 1.4164948MemoryTrain:  epoch  8, batch     2 | loss: 1.4435259MemoryTrain:  epoch  8, batch     3 | loss: 1.2817245MemoryTrain:  epoch  8, batch     4 | loss: 1.2845675MemoryTrain:  epoch  8, batch     5 | loss: 1.3796401MemoryTrain:  epoch  8, batch     6 | loss: 1.3035129MemoryTrain:  epoch  8, batch     7 | loss: 1.3060682MemoryTrain:  epoch  9, batch     0 | loss: 1.3796999MemoryTrain:  epoch  9, batch     1 | loss: 1.3565282MemoryTrain:  epoch  9, batch     2 | loss: 1.3607297MemoryTrain:  epoch  9, batch     3 | loss: 1.3627908MemoryTrain:  epoch  9, batch     4 | loss: 1.2870026MemoryTrain:  epoch  9, batch     5 | loss: 1.3250730MemoryTrain:  epoch  9, batch     6 | loss: 1.2582829MemoryTrain:  epoch  9, batch     7 | loss: 1.2468055
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 98.61%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 94.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 88.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 61.61%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 62.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 61.72%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 63.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 66.48%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 67.39%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 71.53%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 74.19%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 74.24%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 72.43%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 70.36%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 67.57%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 67.43%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 67.79%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 68.60%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 67.59%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 66.05%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 64.58%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 63.18%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 62.10%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 62.11%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 61.10%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 60.00%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 58.82%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 58.05%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 56.96%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 56.71%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 57.39%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 58.15%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 58.66%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 59.16%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 59.43%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 59.58%   [EVAL] batch:   60 | acc: 6.25%,  total acc: 58.71%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 57.76%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 56.85%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 55.96%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 55.10%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 54.26%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 53.73%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 54.41%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 54.71%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 54.55%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 54.40%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 54.60%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 55.22%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 55.83%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 56.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 56.99%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 57.55%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 57.77%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 57.28%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 56.56%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 55.86%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 55.56%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 55.12%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 54.76%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 54.63%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 54.36%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 53.95%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 53.98%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 53.72%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 53.47%   [EVAL] batch:   90 | acc: 12.50%,  total acc: 53.02%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 52.45%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 52.02%   [EVAL] batch:   93 | acc: 6.25%,  total acc: 51.53%   [EVAL] batch:   94 | acc: 6.25%,  total acc: 51.05%   [EVAL] batch:   95 | acc: 6.25%,  total acc: 50.59%   [EVAL] batch:   96 | acc: 0.00%,  total acc: 50.06%   [EVAL] batch:   97 | acc: 6.25%,  total acc: 49.62%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 49.18%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 49.19%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 49.69%   [EVAL] batch:  101 | acc: 37.50%,  total acc: 49.57%   [EVAL] batch:  102 | acc: 81.25%,  total acc: 49.88%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 49.94%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 50.24%   [EVAL] batch:  105 | acc: 75.00%,  total acc: 50.47%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 50.41%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 50.52%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 50.63%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 50.68%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 50.84%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 51.17%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 51.38%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 51.70%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 51.90%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 52.05%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 52.35%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 52.70%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 52.99%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 53.39%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 53.77%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 54.15%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 54.52%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 54.89%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 55.25%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 55.61%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 55.95%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 56.15%   [EVAL] batch:  128 | acc: 56.25%,  total acc: 56.15%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 56.30%   [EVAL] batch:  130 | acc: 87.50%,  total acc: 56.54%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 56.72%   [EVAL] batch:  132 | acc: 50.00%,  total acc: 56.67%   
cur_acc:  ['0.8617', '0.6875', '0.8214', '0.6354', '0.5142', '0.5703', '0.7308', '0.8884']
his_acc:  ['0.8617', '0.8191', '0.7961', '0.7059', '0.6425', '0.5771', '0.5373', '0.5667']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.2009449CurrentTrain: epoch  0, batch     1 | loss: 13.1018791CurrentTrain: epoch  0, batch     2 | loss: 13.0031300CurrentTrain: epoch  0, batch     3 | loss: 12.9572010CurrentTrain: epoch  0, batch     4 | loss: 12.7839060CurrentTrain: epoch  0, batch     5 | loss: 12.6561213CurrentTrain: epoch  0, batch     6 | loss: 12.5891438CurrentTrain: epoch  0, batch     7 | loss: 12.4558907CurrentTrain: epoch  0, batch     8 | loss: 12.1591082CurrentTrain: epoch  0, batch     9 | loss: 12.1470528CurrentTrain: epoch  0, batch    10 | loss: 11.8258801CurrentTrain: epoch  0, batch    11 | loss: 11.9617233CurrentTrain: epoch  0, batch    12 | loss: 11.7804279CurrentTrain: epoch  0, batch    13 | loss: 11.6616516CurrentTrain: epoch  0, batch    14 | loss: 11.8306446CurrentTrain: epoch  0, batch    15 | loss: 11.3319836CurrentTrain: epoch  0, batch    16 | loss: 11.0876541CurrentTrain: epoch  0, batch    17 | loss: 11.0717478CurrentTrain: epoch  0, batch    18 | loss: 11.2931614CurrentTrain: epoch  0, batch    19 | loss: 10.9619732CurrentTrain: epoch  0, batch    20 | loss: 11.1931667CurrentTrain: epoch  0, batch    21 | loss: 10.8363304CurrentTrain: epoch  0, batch    22 | loss: 11.4156895CurrentTrain: epoch  0, batch    23 | loss: 10.8805981CurrentTrain: epoch  0, batch    24 | loss: 11.2158155CurrentTrain: epoch  0, batch    25 | loss: 10.9010878CurrentTrain: epoch  0, batch    26 | loss: 11.0372562CurrentTrain: epoch  0, batch    27 | loss: 11.4196320CurrentTrain: epoch  0, batch    28 | loss: 10.7113342CurrentTrain: epoch  0, batch    29 | loss: 10.6705017CurrentTrain: epoch  0, batch    30 | loss: 9.7869644CurrentTrain: epoch  0, batch    31 | loss: 10.6072035CurrentTrain: epoch  0, batch    32 | loss: 10.8739452CurrentTrain: epoch  0, batch    33 | loss: 10.6398602CurrentTrain: epoch  0, batch    34 | loss: 11.0220833CurrentTrain: epoch  0, batch    35 | loss: 10.3340282CurrentTrain: epoch  0, batch    36 | loss: 10.9742184CurrentTrain: epoch  0, batch    37 | loss: 9.6747875CurrentTrain: epoch  1, batch     0 | loss: 9.6431580CurrentTrain: epoch  1, batch     1 | loss: 9.9342117CurrentTrain: epoch  1, batch     2 | loss: 9.8737354CurrentTrain: epoch  1, batch     3 | loss: 9.8220139CurrentTrain: epoch  1, batch     4 | loss: 9.6623812CurrentTrain: epoch  1, batch     5 | loss: 9.8517399CurrentTrain: epoch  1, batch     6 | loss: 9.9642010CurrentTrain: epoch  1, batch     7 | loss: 9.8011627CurrentTrain: epoch  1, batch     8 | loss: 9.8996172CurrentTrain: epoch  1, batch     9 | loss: 9.1008835CurrentTrain: epoch  1, batch    10 | loss: 9.0774107CurrentTrain: epoch  1, batch    11 | loss: 9.8417492CurrentTrain: epoch  1, batch    12 | loss: 9.4765739CurrentTrain: epoch  1, batch    13 | loss: 9.3529329CurrentTrain: epoch  1, batch    14 | loss: 9.4377184CurrentTrain: epoch  1, batch    15 | loss: 9.6364479CurrentTrain: epoch  1, batch    16 | loss: 10.1211624CurrentTrain: epoch  1, batch    17 | loss: 9.3816547CurrentTrain: epoch  1, batch    18 | loss: 9.6128387CurrentTrain: epoch  1, batch    19 | loss: 9.1696377CurrentTrain: epoch  1, batch    20 | loss: 9.6795578CurrentTrain: epoch  1, batch    21 | loss: 9.6348000CurrentTrain: epoch  1, batch    22 | loss: 9.8776588CurrentTrain: epoch  1, batch    23 | loss: 8.9294119CurrentTrain: epoch  1, batch    24 | loss: 9.1740856CurrentTrain: epoch  1, batch    25 | loss: 9.7264252CurrentTrain: epoch  1, batch    26 | loss: 9.1047153CurrentTrain: epoch  1, batch    27 | loss: 8.8555250CurrentTrain: epoch  1, batch    28 | loss: 8.7606096CurrentTrain: epoch  1, batch    29 | loss: 9.6321039CurrentTrain: epoch  1, batch    30 | loss: 8.1642017CurrentTrain: epoch  1, batch    31 | loss: 8.7404308CurrentTrain: epoch  1, batch    32 | loss: 8.4856300CurrentTrain: epoch  1, batch    33 | loss: 9.8947544CurrentTrain: epoch  1, batch    34 | loss: 9.1643925CurrentTrain: epoch  1, batch    35 | loss: 8.8888569CurrentTrain: epoch  1, batch    36 | loss: 8.9081335CurrentTrain: epoch  1, batch    37 | loss: 7.8563356CurrentTrain: epoch  2, batch     0 | loss: 8.6549664CurrentTrain: epoch  2, batch     1 | loss: 8.4990959CurrentTrain: epoch  2, batch     2 | loss: 8.8096619CurrentTrain: epoch  2, batch     3 | loss: 8.2766600CurrentTrain: epoch  2, batch     4 | loss: 8.1586199CurrentTrain: epoch  2, batch     5 | loss: 8.1030378CurrentTrain: epoch  2, batch     6 | loss: 8.5964069CurrentTrain: epoch  2, batch     7 | loss: 8.2063513CurrentTrain: epoch  2, batch     8 | loss: 8.2887564CurrentTrain: epoch  2, batch     9 | loss: 8.5273209CurrentTrain: epoch  2, batch    10 | loss: 8.1055546CurrentTrain: epoch  2, batch    11 | loss: 8.1063862CurrentTrain: epoch  2, batch    12 | loss: 7.3179331CurrentTrain: epoch  2, batch    13 | loss: 9.3076229CurrentTrain: epoch  2, batch    14 | loss: 7.8501568CurrentTrain: epoch  2, batch    15 | loss: 9.2290258CurrentTrain: epoch  2, batch    16 | loss: 7.7363129CurrentTrain: epoch  2, batch    17 | loss: 8.3757563CurrentTrain: epoch  2, batch    18 | loss: 7.7880716CurrentTrain: epoch  2, batch    19 | loss: 7.5633636CurrentTrain: epoch  2, batch    20 | loss: 8.9719896CurrentTrain: epoch  2, batch    21 | loss: 7.5565467CurrentTrain: epoch  2, batch    22 | loss: 8.1571569CurrentTrain: epoch  2, batch    23 | loss: 7.2662787CurrentTrain: epoch  2, batch    24 | loss: 8.5968466CurrentTrain: epoch  2, batch    25 | loss: 8.0218325CurrentTrain: epoch  2, batch    26 | loss: 8.3063278CurrentTrain: epoch  2, batch    27 | loss: 8.0081291CurrentTrain: epoch  2, batch    28 | loss: 7.5360990CurrentTrain: epoch  2, batch    29 | loss: 7.8075061CurrentTrain: epoch  2, batch    30 | loss: 8.3905087CurrentTrain: epoch  2, batch    31 | loss: 8.3974648CurrentTrain: epoch  2, batch    32 | loss: 7.5952668CurrentTrain: epoch  2, batch    33 | loss: 7.7556295CurrentTrain: epoch  2, batch    34 | loss: 6.9847746CurrentTrain: epoch  2, batch    35 | loss: 7.6777725CurrentTrain: epoch  2, batch    36 | loss: 7.3098650CurrentTrain: epoch  2, batch    37 | loss: 7.5470238CurrentTrain: epoch  3, batch     0 | loss: 7.5009556CurrentTrain: epoch  3, batch     1 | loss: 8.0814915CurrentTrain: epoch  3, batch     2 | loss: 7.0480862CurrentTrain: epoch  3, batch     3 | loss: 6.9093857CurrentTrain: epoch  3, batch     4 | loss: 7.4831338CurrentTrain: epoch  3, batch     5 | loss: 7.3352513CurrentTrain: epoch  3, batch     6 | loss: 7.6371107CurrentTrain: epoch  3, batch     7 | loss: 7.8454976CurrentTrain: epoch  3, batch     8 | loss: 7.3021278CurrentTrain: epoch  3, batch     9 | loss: 7.5981627CurrentTrain: epoch  3, batch    10 | loss: 7.7183108CurrentTrain: epoch  3, batch    11 | loss: 7.8270035CurrentTrain: epoch  3, batch    12 | loss: 7.1020651CurrentTrain: epoch  3, batch    13 | loss: 7.1934156CurrentTrain: epoch  3, batch    14 | loss: 7.6251831CurrentTrain: epoch  3, batch    15 | loss: 7.0935249CurrentTrain: epoch  3, batch    16 | loss: 7.9986973CurrentTrain: epoch  3, batch    17 | loss: 7.9851799CurrentTrain: epoch  3, batch    18 | loss: 7.4358172CurrentTrain: epoch  3, batch    19 | loss: 7.4733377CurrentTrain: epoch  3, batch    20 | loss: 7.1795969CurrentTrain: epoch  3, batch    21 | loss: 6.7753429CurrentTrain: epoch  3, batch    22 | loss: 7.1434202CurrentTrain: epoch  3, batch    23 | loss: 7.2334223CurrentTrain: epoch  3, batch    24 | loss: 7.6450758CurrentTrain: epoch  3, batch    25 | loss: 7.3176622CurrentTrain: epoch  3, batch    26 | loss: 7.4248028CurrentTrain: epoch  3, batch    27 | loss: 7.1188569CurrentTrain: epoch  3, batch    28 | loss: 6.8258939CurrentTrain: epoch  3, batch    29 | loss: 7.1070018CurrentTrain: epoch  3, batch    30 | loss: 7.9101033CurrentTrain: epoch  3, batch    31 | loss: 6.2399521CurrentTrain: epoch  3, batch    32 | loss: 7.1907701CurrentTrain: epoch  3, batch    33 | loss: 7.9642582CurrentTrain: epoch  3, batch    34 | loss: 7.4894457CurrentTrain: epoch  3, batch    35 | loss: 7.6455379CurrentTrain: epoch  3, batch    36 | loss: 6.9690514CurrentTrain: epoch  3, batch    37 | loss: 6.2030535CurrentTrain: epoch  4, batch     0 | loss: 6.3863740CurrentTrain: epoch  4, batch     1 | loss: 7.0669203CurrentTrain: epoch  4, batch     2 | loss: 6.1132112CurrentTrain: epoch  4, batch     3 | loss: 7.1994543CurrentTrain: epoch  4, batch     4 | loss: 7.1177182CurrentTrain: epoch  4, batch     5 | loss: 7.1501808CurrentTrain: epoch  4, batch     6 | loss: 7.2271934CurrentTrain: epoch  4, batch     7 | loss: 6.9186521CurrentTrain: epoch  4, batch     8 | loss: 6.3489428CurrentTrain: epoch  4, batch     9 | loss: 6.1106024CurrentTrain: epoch  4, batch    10 | loss: 7.2861633CurrentTrain: epoch  4, batch    11 | loss: 6.9477806CurrentTrain: epoch  4, batch    12 | loss: 6.7445612CurrentTrain: epoch  4, batch    13 | loss: 6.6490583CurrentTrain: epoch  4, batch    14 | loss: 7.3363905CurrentTrain: epoch  4, batch    15 | loss: 7.4044228CurrentTrain: epoch  4, batch    16 | loss: 6.6919999CurrentTrain: epoch  4, batch    17 | loss: 6.7356462CurrentTrain: epoch  4, batch    18 | loss: 6.3645239CurrentTrain: epoch  4, batch    19 | loss: 7.8263497CurrentTrain: epoch  4, batch    20 | loss: 6.2039490CurrentTrain: epoch  4, batch    21 | loss: 7.6397686CurrentTrain: epoch  4, batch    22 | loss: 7.0771551CurrentTrain: epoch  4, batch    23 | loss: 6.7610521CurrentTrain: epoch  4, batch    24 | loss: 6.8446617CurrentTrain: epoch  4, batch    25 | loss: 7.3940077CurrentTrain: epoch  4, batch    26 | loss: 6.2192154CurrentTrain: epoch  4, batch    27 | loss: 7.5733728CurrentTrain: epoch  4, batch    28 | loss: 6.2507529CurrentTrain: epoch  4, batch    29 | loss: 5.7749429CurrentTrain: epoch  4, batch    30 | loss: 6.9521847CurrentTrain: epoch  4, batch    31 | loss: 6.3150544CurrentTrain: epoch  4, batch    32 | loss: 7.7779012CurrentTrain: epoch  4, batch    33 | loss: 7.4619064CurrentTrain: epoch  4, batch    34 | loss: 6.2997012CurrentTrain: epoch  4, batch    35 | loss: 5.8016706CurrentTrain: epoch  4, batch    36 | loss: 6.0434465CurrentTrain: epoch  4, batch    37 | loss: 6.3745418CurrentTrain: epoch  5, batch     0 | loss: 6.4560223CurrentTrain: epoch  5, batch     1 | loss: 6.6317768CurrentTrain: epoch  5, batch     2 | loss: 6.2414608CurrentTrain: epoch  5, batch     3 | loss: 6.6321406CurrentTrain: epoch  5, batch     4 | loss: 6.3100510CurrentTrain: epoch  5, batch     5 | loss: 5.8990045CurrentTrain: epoch  5, batch     6 | loss: 7.8225141CurrentTrain: epoch  5, batch     7 | loss: 6.8949986CurrentTrain: epoch  5, batch     8 | loss: 7.3005519CurrentTrain: epoch  5, batch     9 | loss: 6.6607599CurrentTrain: epoch  5, batch    10 | loss: 6.2047815CurrentTrain: epoch  5, batch    11 | loss: 6.6718855CurrentTrain: epoch  5, batch    12 | loss: 6.8941698CurrentTrain: epoch  5, batch    13 | loss: 6.6694174CurrentTrain: epoch  5, batch    14 | loss: 6.4162102CurrentTrain: epoch  5, batch    15 | loss: 6.6006565CurrentTrain: epoch  5, batch    16 | loss: 6.1079240CurrentTrain: epoch  5, batch    17 | loss: 7.0024471CurrentTrain: epoch  5, batch    18 | loss: 5.5377502CurrentTrain: epoch  5, batch    19 | loss: 6.3705359CurrentTrain: epoch  5, batch    20 | loss: 6.7141376CurrentTrain: epoch  5, batch    21 | loss: 6.2267203CurrentTrain: epoch  5, batch    22 | loss: 5.7992206CurrentTrain: epoch  5, batch    23 | loss: 6.4798112CurrentTrain: epoch  5, batch    24 | loss: 6.2160959CurrentTrain: epoch  5, batch    25 | loss: 6.2744236CurrentTrain: epoch  5, batch    26 | loss: 5.8150010CurrentTrain: epoch  5, batch    27 | loss: 6.8726778CurrentTrain: epoch  5, batch    28 | loss: 5.7935762CurrentTrain: epoch  5, batch    29 | loss: 5.7580624CurrentTrain: epoch  5, batch    30 | loss: 7.1536021CurrentTrain: epoch  5, batch    31 | loss: 6.1696329CurrentTrain: epoch  5, batch    32 | loss: 5.9703121CurrentTrain: epoch  5, batch    33 | loss: 6.8663149CurrentTrain: epoch  5, batch    34 | loss: 6.1656280CurrentTrain: epoch  5, batch    35 | loss: 6.6366892CurrentTrain: epoch  5, batch    36 | loss: 5.9187245CurrentTrain: epoch  5, batch    37 | loss: 6.8060131CurrentTrain: epoch  6, batch     0 | loss: 6.5833807CurrentTrain: epoch  6, batch     1 | loss: 6.4333715CurrentTrain: epoch  6, batch     2 | loss: 6.2753401CurrentTrain: epoch  6, batch     3 | loss: 6.5682144CurrentTrain: epoch  6, batch     4 | loss: 5.9259882CurrentTrain: epoch  6, batch     5 | loss: 5.8283625CurrentTrain: epoch  6, batch     6 | loss: 6.0998669CurrentTrain: epoch  6, batch     7 | loss: 6.1666551CurrentTrain: epoch  6, batch     8 | loss: 5.7632866CurrentTrain: epoch  6, batch     9 | loss: 6.0483599CurrentTrain: epoch  6, batch    10 | loss: 5.8272552CurrentTrain: epoch  6, batch    11 | loss: 6.7457466CurrentTrain: epoch  6, batch    12 | loss: 6.1361742CurrentTrain: epoch  6, batch    13 | loss: 6.1338706CurrentTrain: epoch  6, batch    14 | loss: 6.2832022CurrentTrain: epoch  6, batch    15 | loss: 5.5793600CurrentTrain: epoch  6, batch    16 | loss: 5.5352154CurrentTrain: epoch  6, batch    17 | loss: 5.6362286CurrentTrain: epoch  6, batch    18 | loss: 5.8777189CurrentTrain: epoch  6, batch    19 | loss: 6.7475629CurrentTrain: epoch  6, batch    20 | loss: 5.4163666CurrentTrain: epoch  6, batch    21 | loss: 5.6286650CurrentTrain: epoch  6, batch    22 | loss: 5.9358320CurrentTrain: epoch  6, batch    23 | loss: 6.4343786CurrentTrain: epoch  6, batch    24 | loss: 5.7557459CurrentTrain: epoch  6, batch    25 | loss: 6.2059851CurrentTrain: epoch  6, batch    26 | loss: 6.2619438CurrentTrain: epoch  6, batch    27 | loss: 5.8247309CurrentTrain: epoch  6, batch    28 | loss: 5.9646721CurrentTrain: epoch  6, batch    29 | loss: 5.7234735CurrentTrain: epoch  6, batch    30 | loss: 6.4067001CurrentTrain: epoch  6, batch    31 | loss: 6.6188869CurrentTrain: epoch  6, batch    32 | loss: 5.5027122CurrentTrain: epoch  6, batch    33 | loss: 5.8659945CurrentTrain: epoch  6, batch    34 | loss: 5.5257459CurrentTrain: epoch  6, batch    35 | loss: 6.1926913CurrentTrain: epoch  6, batch    36 | loss: 5.6276665CurrentTrain: epoch  6, batch    37 | loss: 5.2856698CurrentTrain: epoch  7, batch     0 | loss: 6.2435284CurrentTrain: epoch  7, batch     1 | loss: 5.3802938CurrentTrain: epoch  7, batch     2 | loss: 5.8285413CurrentTrain: epoch  7, batch     3 | loss: 5.7201004CurrentTrain: epoch  7, batch     4 | loss: 6.0358744CurrentTrain: epoch  7, batch     5 | loss: 5.6124301CurrentTrain: epoch  7, batch     6 | loss: 5.5620890CurrentTrain: epoch  7, batch     7 | loss: 6.0148377CurrentTrain: epoch  7, batch     8 | loss: 5.2819414CurrentTrain: epoch  7, batch     9 | loss: 5.7363930CurrentTrain: epoch  7, batch    10 | loss: 5.5529690CurrentTrain: epoch  7, batch    11 | loss: 5.7109299CurrentTrain: epoch  7, batch    12 | loss: 5.2919106CurrentTrain: epoch  7, batch    13 | loss: 5.2962222CurrentTrain: epoch  7, batch    14 | loss: 5.7783241CurrentTrain: epoch  7, batch    15 | loss: 5.9809642CurrentTrain: epoch  7, batch    16 | loss: 5.1965189CurrentTrain: epoch  7, batch    17 | loss: 5.6092024CurrentTrain: epoch  7, batch    18 | loss: 6.0598307CurrentTrain: epoch  7, batch    19 | loss: 6.4596453CurrentTrain: epoch  7, batch    20 | loss: 5.2051573CurrentTrain: epoch  7, batch    21 | loss: 5.5239758CurrentTrain: epoch  7, batch    22 | loss: 5.8118534CurrentTrain: epoch  7, batch    23 | loss: 6.1726885CurrentTrain: epoch  7, batch    24 | loss: 5.0913749CurrentTrain: epoch  7, batch    25 | loss: 5.3433390CurrentTrain: epoch  7, batch    26 | loss: 5.7914705CurrentTrain: epoch  7, batch    27 | loss: 5.2299809CurrentTrain: epoch  7, batch    28 | loss: 5.3771243CurrentTrain: epoch  7, batch    29 | loss: 5.4350204CurrentTrain: epoch  7, batch    30 | loss: 5.5918388CurrentTrain: epoch  7, batch    31 | loss: 5.6652093CurrentTrain: epoch  7, batch    32 | loss: 5.7127810CurrentTrain: epoch  7, batch    33 | loss: 5.9403214CurrentTrain: epoch  7, batch    34 | loss: 5.3915911CurrentTrain: epoch  7, batch    35 | loss: 5.3997297CurrentTrain: epoch  7, batch    36 | loss: 5.5480604CurrentTrain: epoch  7, batch    37 | loss: 6.2568936CurrentTrain: epoch  8, batch     0 | loss: 5.5230379CurrentTrain: epoch  8, batch     1 | loss: 5.2912655CurrentTrain: epoch  8, batch     2 | loss: 5.2938967CurrentTrain: epoch  8, batch     3 | loss: 5.1008348CurrentTrain: epoch  8, batch     4 | loss: 5.2711163CurrentTrain: epoch  8, batch     5 | loss: 5.2072482CurrentTrain: epoch  8, batch     6 | loss: 5.2258306CurrentTrain: epoch  8, batch     7 | loss: 5.5040121CurrentTrain: epoch  8, batch     8 | loss: 5.1989861CurrentTrain: epoch  8, batch     9 | loss: 5.0798912CurrentTrain: epoch  8, batch    10 | loss: 5.0834742CurrentTrain: epoch  8, batch    11 | loss: 5.1397095CurrentTrain: epoch  8, batch    12 | loss: 5.1078482CurrentTrain: epoch  8, batch    13 | loss: 5.2658558CurrentTrain: epoch  8, batch    14 | loss: 5.6545577CurrentTrain: epoch  8, batch    15 | loss: 5.2648497CurrentTrain: epoch  8, batch    16 | loss: 5.2590618CurrentTrain: epoch  8, batch    17 | loss: 5.0609994CurrentTrain: epoch  8, batch    18 | loss: 5.1104851CurrentTrain: epoch  8, batch    19 | loss: 5.1919789CurrentTrain: epoch  8, batch    20 | loss: 5.1076565CurrentTrain: epoch  8, batch    21 | loss: 5.1003981CurrentTrain: epoch  8, batch    22 | loss: 5.2515178CurrentTrain: epoch  8, batch    23 | loss: 6.3898511CurrentTrain: epoch  8, batch    24 | loss: 4.9979773CurrentTrain: epoch  8, batch    25 | loss: 5.0681720CurrentTrain: epoch  8, batch    26 | loss: 5.1445999CurrentTrain: epoch  8, batch    27 | loss: 5.3049870CurrentTrain: epoch  8, batch    28 | loss: 5.1391015CurrentTrain: epoch  8, batch    29 | loss: 5.8242903CurrentTrain: epoch  8, batch    30 | loss: 4.8768892CurrentTrain: epoch  8, batch    31 | loss: 5.5825143CurrentTrain: epoch  8, batch    32 | loss: 5.0793648CurrentTrain: epoch  8, batch    33 | loss: 5.0429254CurrentTrain: epoch  8, batch    34 | loss: 4.8900838CurrentTrain: epoch  8, batch    35 | loss: 5.2026176CurrentTrain: epoch  8, batch    36 | loss: 5.0837126CurrentTrain: epoch  8, batch    37 | loss: 4.9615922CurrentTrain: epoch  9, batch     0 | loss: 5.0006461CurrentTrain: epoch  9, batch     1 | loss: 5.0819583CurrentTrain: epoch  9, batch     2 | loss: 5.3085217CurrentTrain: epoch  9, batch     3 | loss: 5.2632427CurrentTrain: epoch  9, batch     4 | loss: 5.0428743CurrentTrain: epoch  9, batch     5 | loss: 5.1228395CurrentTrain: epoch  9, batch     6 | loss: 5.5440102CurrentTrain: epoch  9, batch     7 | loss: 4.8342276CurrentTrain: epoch  9, batch     8 | loss: 4.9876294CurrentTrain: epoch  9, batch     9 | loss: 5.2767496CurrentTrain: epoch  9, batch    10 | loss: 4.9486065CurrentTrain: epoch  9, batch    11 | loss: 5.3618317CurrentTrain: epoch  9, batch    12 | loss: 4.7828846CurrentTrain: epoch  9, batch    13 | loss: 5.6920605CurrentTrain: epoch  9, batch    14 | loss: 5.5223494CurrentTrain: epoch  9, batch    15 | loss: 5.1150379CurrentTrain: epoch  9, batch    16 | loss: 5.3632112CurrentTrain: epoch  9, batch    17 | loss: 5.5313640CurrentTrain: epoch  9, batch    18 | loss: 5.1483660CurrentTrain: epoch  9, batch    19 | loss: 4.9594631CurrentTrain: epoch  9, batch    20 | loss: 5.0911560CurrentTrain: epoch  9, batch    21 | loss: 4.9325948CurrentTrain: epoch  9, batch    22 | loss: 4.9824114CurrentTrain: epoch  9, batch    23 | loss: 5.4932861CurrentTrain: epoch  9, batch    24 | loss: 5.1805954CurrentTrain: epoch  9, batch    25 | loss: 5.0855799CurrentTrain: epoch  9, batch    26 | loss: 4.9482722CurrentTrain: epoch  9, batch    27 | loss: 6.0146112CurrentTrain: epoch  9, batch    28 | loss: 4.9267254CurrentTrain: epoch  9, batch    29 | loss: 5.0213904CurrentTrain: epoch  9, batch    30 | loss: 5.1053534CurrentTrain: epoch  9, batch    31 | loss: 4.9052439CurrentTrain: epoch  9, batch    32 | loss: 5.0067921CurrentTrain: epoch  9, batch    33 | loss: 5.1787753CurrentTrain: epoch  9, batch    34 | loss: 4.9735007CurrentTrain: epoch  9, batch    35 | loss: 5.3094115CurrentTrain: epoch  9, batch    36 | loss: 4.9861851CurrentTrain: epoch  9, batch    37 | loss: 5.1833839
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: meeting with iran 's majlis speaker gholam-ali haddad-adel in geneva tuesday , he urged all islamic states to strengthen solidarity and to hold consultations .
Head Entity: gholam-ali haddad-adel
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After moving to Canada for his studies, John Smith quickly adapted to the new culture and made many friends in Toronto.  
Head Entity: John Smith  
Tail Entity: Canada  

Relation: person countries of residence  
Context: Maria Lopez has been living in Spain for the past five years, where she works as a graphic designer in Barcelona.  
Head Entity: Maria Lopez  
Tail Entity: Spain  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` customer relationships are the backbone of banking , and banks are constantly striving to meet customer demands , '' said peter garuccio , spokesman for the american bankers association .
Head Entity: american bankers association
Tail Entity: peter garuccio
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: "As the CEO of the tech giant, she has been instrumental in driving innovation and leading her team to success," said a company spokesperson about Jane Doe, who has been with the organization for over a decade.  
Head Entity: tech giant  
Tail Entity: Jane Doe  

Relation: organization top members employees  
Context: "During the annual meeting, the board praised John Smith for his exceptional leadership and dedication to the company," reported the financial analyst covering the firm.  
Head Entity: the board  
Tail Entity: John Smith  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union (EU) is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: iranian atomic chief ali akbar salehi said on wednesday that tehran will address the concerns raised by the vienna group regarding the fuel deal inked by the islamic republic with brazil and turkey .
Head Entity: ali akbar salehi
Tail Entity: iranian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist albert einstein was born in ulm, germany, and later became a citizen of the united states.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author chimamanda ngozi adichie was born in enugu, nigeria, and her works often reflect her nigerian heritage.  
Head Entity: chimamanda ngozi adichie  
Tail Entity: nigerian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: earlier , in jerusalem , he spoke at the state funeral for the city 's fabled former mayor , teddy kollek , who died tuesday at 95 and was buried in the area of the mount herzl cemetery reserved for israel 's leaders .
Head Entity: teddy kollek
Tail Entity: mayor
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: After years of dedicated service, Dr. Emily Carter was honored with a prestigious award at the annual conference, recognizing her as the leading scientist in her field.  
Head Entity: Dr. Emily Carter  
Tail Entity: leading scientist  

Relation: person title  
Context: During the ceremony, the renowned author James Patterson received accolades for his contributions to literature, solidifying his status as a bestselling novelist.  
Head Entity: James Patterson  
Tail Entity: bestselling novelist  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: `` japan 's growth has yet to be sustained by domestic demand alone , '' said yoshimasa maruyama , a senior economist at itochu corp. in tokyo .
Head Entity: itochu corp.
Tail Entity: japan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: "The headquarters of Samsung Electronics is located in Suwon, South Korea, where the company has been a leader in technology innovation."  
Head Entity: Samsung Electronics  
Tail Entity: South Korea  

Relation: organization country of headquarters  
Context: "Nestlé, the world's largest food and beverage company, has its headquarters in Vevey, Switzerland, overseeing operations in numerous countries."  
Head Entity: Nestlé  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 89.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 83.55%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.92%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
cur_acc:  ['0.8655']
his_acc:  ['0.8655']
CurrentTrain: epoch  0, batch     0 | loss: 6.4751997CurrentTrain: epoch  0, batch     1 | loss: 5.6412511CurrentTrain: epoch  1, batch     0 | loss: 5.5539980CurrentTrain: epoch  1, batch     1 | loss: 5.2982950CurrentTrain: epoch  2, batch     0 | loss: 5.1571407CurrentTrain: epoch  2, batch     1 | loss: 4.4418530CurrentTrain: epoch  3, batch     0 | loss: 4.2385359CurrentTrain: epoch  3, batch     1 | loss: 4.6664605CurrentTrain: epoch  4, batch     0 | loss: 3.4656937CurrentTrain: epoch  4, batch     1 | loss: 4.8117652CurrentTrain: epoch  5, batch     0 | loss: 3.7777216CurrentTrain: epoch  5, batch     1 | loss: 4.0036502CurrentTrain: epoch  6, batch     0 | loss: 3.6902957CurrentTrain: epoch  6, batch     1 | loss: 3.6957822CurrentTrain: epoch  7, batch     0 | loss: 3.1333063CurrentTrain: epoch  7, batch     1 | loss: 3.5426950CurrentTrain: epoch  8, batch     0 | loss: 3.0337782CurrentTrain: epoch  8, batch     1 | loss: 3.4060209CurrentTrain: epoch  9, batch     0 | loss: 2.9999597CurrentTrain: epoch  9, batch     1 | loss: 3.0095658
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones settled in san francisco, where she found a job in tech. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith lived in boston, enjoying the vibrant culture and history of the city. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, he enrolled at Stanford University to study computer science.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: max  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: previously , al-khawinay was sentenced to one year in jail for supporting the country 's minority shiite rebels and defaming the president , but was later pardoned by president ali abdullah saleh .
Head Entity: al-khawinay
Tail Entity: defaming the president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: In a shocking turn of events, the local council announced that mayor Thompson was charged with embezzlement after an extensive investigation revealed misuse of public funds.  
Head Entity: mayor Thompson  
Tail Entity: embezzlement  

Relation: person charges  
Context: After a lengthy trial, it was determined that journalist Sarah Lee was charged with libel for publishing false information about a prominent businessman.  
Head Entity: journalist Sarah Lee  
Tail Entity: libel  
Mixup data size:  104
MixupTrain:  epoch  0, batch     0 | loss: 13.2932312MixupTrain:  epoch  0, batch     1 | loss: 12.6078735MixupTrain:  epoch  0, batch     2 | loss: 11.2342868MixupTrain:  epoch  0, batch     3 | loss: 11.0475258MixupTrain:  epoch  0, batch     4 | loss: 9.9235573MixupTrain:  epoch  0, batch     5 | loss: 10.4745319MixupTrain:  epoch  0, batch     6 | loss: 8.2465601
MemoryTrain:  epoch  0, batch     0 | loss: 3.6490173MemoryTrain:  epoch  0, batch     1 | loss: 3.4686146MemoryTrain:  epoch  0, batch     2 | loss: 2.3479052MemoryTrain:  epoch  1, batch     0 | loss: 3.3077497MemoryTrain:  epoch  1, batch     1 | loss: 3.0478339MemoryTrain:  epoch  1, batch     2 | loss: 1.2342397MemoryTrain:  epoch  2, batch     0 | loss: 2.7812691MemoryTrain:  epoch  2, batch     1 | loss: 2.3020287MemoryTrain:  epoch  2, batch     2 | loss: 2.5022933MemoryTrain:  epoch  3, batch     0 | loss: 2.3546648MemoryTrain:  epoch  3, batch     1 | loss: 2.2153935MemoryTrain:  epoch  3, batch     2 | loss: 1.8845814MemoryTrain:  epoch  4, batch     0 | loss: 2.2910542MemoryTrain:  epoch  4, batch     1 | loss: 1.9772177MemoryTrain:  epoch  4, batch     2 | loss: 1.9919878MemoryTrain:  epoch  5, batch     0 | loss: 2.5032024MemoryTrain:  epoch  5, batch     1 | loss: 1.8099403MemoryTrain:  epoch  5, batch     2 | loss: 1.2608646MemoryTrain:  epoch  6, batch     0 | loss: 1.7766246MemoryTrain:  epoch  6, batch     1 | loss: 2.1541479MemoryTrain:  epoch  6, batch     2 | loss: 1.6169305MemoryTrain:  epoch  7, batch     0 | loss: 1.8830051MemoryTrain:  epoch  7, batch     1 | loss: 1.7665553MemoryTrain:  epoch  7, batch     2 | loss: 1.2916588MemoryTrain:  epoch  8, batch     0 | loss: 1.5833284MemoryTrain:  epoch  8, batch     1 | loss: 1.7632682MemoryTrain:  epoch  8, batch     2 | loss: 1.3959002MemoryTrain:  epoch  9, batch     0 | loss: 1.6344334MemoryTrain:  epoch  9, batch     1 | loss: 1.6047862MemoryTrain:  epoch  9, batch     2 | loss: 1.7677498
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 78.47%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 85.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 85.48%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 84.82%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 84.12%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 83.22%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 82.85%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 82.32%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 82.59%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.24%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 83.61%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 83.83%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 84.51%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 84.38%   
cur_acc:  ['0.8655', '0.7847']
his_acc:  ['0.8655', '0.8438']
CurrentTrain: epoch  0, batch     0 | loss: 6.1083479CurrentTrain: epoch  0, batch     1 | loss: 7.1880865CurrentTrain: epoch  1, batch     0 | loss: 6.3908005CurrentTrain: epoch  1, batch     1 | loss: 5.4522696CurrentTrain: epoch  2, batch     0 | loss: 5.4490242CurrentTrain: epoch  2, batch     1 | loss: 4.5600276CurrentTrain: epoch  3, batch     0 | loss: 4.6937284CurrentTrain: epoch  3, batch     1 | loss: 4.7369747CurrentTrain: epoch  4, batch     0 | loss: 4.3337336CurrentTrain: epoch  4, batch     1 | loss: 4.1959929CurrentTrain: epoch  5, batch     0 | loss: 4.5004625CurrentTrain: epoch  5, batch     1 | loss: 3.6124225CurrentTrain: epoch  6, batch     0 | loss: 3.5223973CurrentTrain: epoch  6, batch     1 | loss: 3.3046045CurrentTrain: epoch  7, batch     0 | loss: 3.2517362CurrentTrain: epoch  7, batch     1 | loss: 3.2538261CurrentTrain: epoch  8, batch     0 | loss: 2.9748187CurrentTrain: epoch  8, batch     1 | loss: 2.8773167CurrentTrain: epoch  9, batch     0 | loss: 2.5945010CurrentTrain: epoch  9, batch     1 | loss: 2.7351277
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: david 's father had surrendered the child to an orphanage after his wife of 10 years , marita , died in 2005 shortly after giving birth to david .
Head Entity: david
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: The famous author, Jane Austen, was born on December 16, 1775, in Steventon, Hampshire, England.  
Head Entity: Jane Austen  
Tail Entity: December 16, 1775  

Relation: person date of birth  
Context: Albert Einstein, known for his contributions to physics, was born on March 14, 1879, in Ulm, Germany.  
Head Entity: Albert Einstein  
Tail Entity: March 14, 1879  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, to parents of polish descent.  
Head Entity: martha stewart  
Tail Entity: new jersey  

Relation: person stateorprovince of birth  
Context: barack obama was born on august 4, 1961, in honolulu, hawaii, where he spent most of his childhood.  
Head Entity: barack obama  
Tail Entity: hawaii  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Sample 1:  
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, who had always been a guiding force in her life, to her friends.  
Head Entity: her father  
Tail Entity: Sarah  

Sample 2:  
Relation: person parents  
Context: After the ceremony, Michael expressed his gratitude to his mother for all the sacrifices she made to support him through school.  
Head Entity: his mother  
Tail Entity: Michael  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson has finally landed a position at one of the top tech companies in Silicon Valley, where she will be contributing to innovative projects.  
Head Entity: Sarah Thompson  
Tail Entity: top tech company  

Relation: person employee of  
Context: John Smith, a talented graphic designer, has been working for Creative Solutions for over five years, helping to shape the visual identity of numerous brands.  
Head Entity: John Smith  
Tail Entity: Creative Solutions  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john smith, a renowned author, passed away on march 5 in his residence located in los angeles, california, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: the famous musician, elena rodriguez, died tragically in a car accident on july 12 while traveling through the scenic routes of oregon, where she had spent her childhood.  
Head Entity: elena rodriguez  
Tail Entity: oregon  
Mixup data size:  135
MixupTrain:  epoch  0, batch     0 | loss: 6.7338336MixupTrain:  epoch  0, batch     1 | loss: 5.9410399MixupTrain:  epoch  0, batch     2 | loss: 6.8749034MixupTrain:  epoch  0, batch     3 | loss: 6.8846050MixupTrain:  epoch  0, batch     4 | loss: 6.6624103MixupTrain:  epoch  0, batch     5 | loss: 6.5306872MixupTrain:  epoch  0, batch     6 | loss: 6.5449678MixupTrain:  epoch  0, batch     7 | loss: 5.9987610MixupTrain:  epoch  0, batch     8 | loss: 5.1480430
MemoryTrain:  epoch  0, batch     0 | loss: 2.2464719MemoryTrain:  epoch  0, batch     1 | loss: 2.5046954MemoryTrain:  epoch  0, batch     2 | loss: 3.1363688MemoryTrain:  epoch  1, batch     0 | loss: 2.7148116MemoryTrain:  epoch  1, batch     1 | loss: 2.2368112MemoryTrain:  epoch  1, batch     2 | loss: 2.5270414MemoryTrain:  epoch  2, batch     0 | loss: 2.3714781MemoryTrain:  epoch  2, batch     1 | loss: 2.2554228MemoryTrain:  epoch  2, batch     2 | loss: 1.7261784MemoryTrain:  epoch  3, batch     0 | loss: 1.8508223MemoryTrain:  epoch  3, batch     1 | loss: 1.7636030MemoryTrain:  epoch  3, batch     2 | loss: 2.1429243MemoryTrain:  epoch  4, batch     0 | loss: 1.9157121MemoryTrain:  epoch  4, batch     1 | loss: 2.0044429MemoryTrain:  epoch  4, batch     2 | loss: 1.8197527MemoryTrain:  epoch  5, batch     0 | loss: 1.9872940MemoryTrain:  epoch  5, batch     1 | loss: 1.4475172MemoryTrain:  epoch  5, batch     2 | loss: 1.8820281MemoryTrain:  epoch  6, batch     0 | loss: 1.6728454MemoryTrain:  epoch  6, batch     1 | loss: 1.6325824MemoryTrain:  epoch  6, batch     2 | loss: 1.7594745MemoryTrain:  epoch  7, batch     0 | loss: 1.5909395MemoryTrain:  epoch  7, batch     1 | loss: 1.8499593MemoryTrain:  epoch  7, batch     2 | loss: 1.5153153MemoryTrain:  epoch  8, batch     0 | loss: 1.4954302MemoryTrain:  epoch  8, batch     1 | loss: 1.5868728MemoryTrain:  epoch  8, batch     2 | loss: 1.4795825MemoryTrain:  epoch  9, batch     0 | loss: 1.3887556MemoryTrain:  epoch  9, batch     1 | loss: 1.4743326MemoryTrain:  epoch  9, batch     2 | loss: 1.4334120
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 39.58%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 50.69%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 54.37%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 56.82%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 60.10%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 58.48%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 84.72%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 85.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 84.74%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 83.75%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 82.47%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 81.42%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 80.43%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 79.65%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 78.51%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 77.62%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 77.70%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 78.67%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 79.12%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 79.56%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 79.97%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 80.38%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 79.90%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 79.09%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 78.54%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 77.43%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 76.59%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 75.45%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 75.33%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 75.65%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 75.74%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 75.92%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 76.11%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 75.99%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 75.00%   
cur_acc:  ['0.8655', '0.7847', '0.5848']
his_acc:  ['0.8655', '0.8438', '0.7500']
CurrentTrain: epoch  0, batch     0 | loss: 4.6961565CurrentTrain: epoch  0, batch     1 | loss: 4.9951224CurrentTrain: epoch  1, batch     0 | loss: 3.5935175CurrentTrain: epoch  1, batch     1 | loss: 3.9528491CurrentTrain: epoch  2, batch     0 | loss: 3.1256871CurrentTrain: epoch  2, batch     1 | loss: 3.6891098CurrentTrain: epoch  3, batch     0 | loss: 2.7975810CurrentTrain: epoch  3, batch     1 | loss: 3.0417371CurrentTrain: epoch  4, batch     0 | loss: 2.8058300CurrentTrain: epoch  4, batch     1 | loss: 2.4590447CurrentTrain: epoch  5, batch     0 | loss: 2.6684351CurrentTrain: epoch  5, batch     1 | loss: 2.2278581CurrentTrain: epoch  6, batch     0 | loss: 2.2834702CurrentTrain: epoch  6, batch     1 | loss: 2.5486064CurrentTrain: epoch  7, batch     0 | loss: 2.3417325CurrentTrain: epoch  7, batch     1 | loss: 2.4196637CurrentTrain: epoch  8, batch     0 | loss: 2.3223052CurrentTrain: epoch  8, batch     1 | loss: 2.1597216CurrentTrain: epoch  9, batch     0 | loss: 2.0843377CurrentTrain: epoch  9, batch     1 | loss: 2.0014217
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the company was founded as a hobby in 1979 by the husband and wife team of tim and nina zagat , a pair of lawyers who started recuiting friends to provide their own ratings of the food , decor and service of restaurants they frequented .
Head Entity: zagat
Tail Entity: 1979
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1995, the tech startup was established by a group of engineers who aimed to revolutionize the software industry with innovative solutions.  
Head Entity: tech startup  
Tail Entity: 1995  

Relation: organization founded  
Context: The nonprofit organization was created in 2001 to support local artists and promote cultural events in the community.  
Head Entity: nonprofit organization  
Tail Entity: 2001  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith turned 30 last week.  
Head Entity: john smith  
Tail Entity: 30  

Relation: person age  
Context: the famous actor will celebrate his 45th birthday next month.  
Head Entity: the famous actor  
Tail Entity: 45  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians as members, including the famous conductor Leonard Bernstein.  
Head Entity: New York Philharmonic  
Tail Entity: Leonard Bernstein  

Relation: organization members  
Context: The National Geographic Society has a long history of exploration and education, with many prominent explorers and scientists, such as Jane Goodall, being members of the organization.  
Head Entity: National Geographic Society  
Tail Entity: Jane Goodall  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how every member plays a vital role in upholding their traditions.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The young woman shared her experiences growing up in a Muslim household, highlighting the values of compassion and charity that are central to her beliefs.  
Head Entity: young woman  
Tail Entity: Muslim
Mixup data size:  165
MixupTrain:  epoch  0, batch     0 | loss: 6.8326388MixupTrain:  epoch  0, batch     1 | loss: 7.0792501MixupTrain:  epoch  0, batch     2 | loss: 6.4648933MixupTrain:  epoch  0, batch     3 | loss: 6.3052221MixupTrain:  epoch  0, batch     4 | loss: 7.7334696MixupTrain:  epoch  0, batch     5 | loss: 5.6854750MixupTrain:  epoch  0, batch     6 | loss: 6.5796386MixupTrain:  epoch  0, batch     7 | loss: 5.9384572MixupTrain:  epoch  0, batch     8 | loss: 6.7293959MixupTrain:  epoch  0, batch     9 | loss: 5.1949629MixupTrain:  epoch  0, batch    10 | loss: 6.3503556
MemoryTrain:  epoch  0, batch     0 | loss: 2.4948161MemoryTrain:  epoch  0, batch     1 | loss: 2.9189246MemoryTrain:  epoch  0, batch     2 | loss: 4.0033455MemoryTrain:  epoch  0, batch     3 | loss: 3.5449803MemoryTrain:  epoch  1, batch     0 | loss: 2.5876851MemoryTrain:  epoch  1, batch     1 | loss: 3.6200595MemoryTrain:  epoch  1, batch     2 | loss: 2.6369786MemoryTrain:  epoch  1, batch     3 | loss: 2.4684658MemoryTrain:  epoch  2, batch     0 | loss: 3.2685513MemoryTrain:  epoch  2, batch     1 | loss: 1.6528683MemoryTrain:  epoch  2, batch     2 | loss: 2.7978156MemoryTrain:  epoch  2, batch     3 | loss: 2.0630686MemoryTrain:  epoch  3, batch     0 | loss: 2.3367820MemoryTrain:  epoch  3, batch     1 | loss: 1.8252141MemoryTrain:  epoch  3, batch     2 | loss: 2.4573340MemoryTrain:  epoch  3, batch     3 | loss: 1.9426868MemoryTrain:  epoch  4, batch     0 | loss: 2.2751117MemoryTrain:  epoch  4, batch     1 | loss: 1.8850280MemoryTrain:  epoch  4, batch     2 | loss: 1.7418120MemoryTrain:  epoch  4, batch     3 | loss: 1.7600523MemoryTrain:  epoch  5, batch     0 | loss: 1.5188918MemoryTrain:  epoch  5, batch     1 | loss: 1.7156744MemoryTrain:  epoch  5, batch     2 | loss: 1.8715779MemoryTrain:  epoch  5, batch     3 | loss: 2.1219344MemoryTrain:  epoch  6, batch     0 | loss: 1.9935031MemoryTrain:  epoch  6, batch     1 | loss: 2.1983922MemoryTrain:  epoch  6, batch     2 | loss: 1.5554888MemoryTrain:  epoch  6, batch     3 | loss: 1.3975710MemoryTrain:  epoch  7, batch     0 | loss: 1.4943403MemoryTrain:  epoch  7, batch     1 | loss: 1.6452475MemoryTrain:  epoch  7, batch     2 | loss: 1.9360727MemoryTrain:  epoch  7, batch     3 | loss: 1.5872183MemoryTrain:  epoch  8, batch     0 | loss: 1.4909155MemoryTrain:  epoch  8, batch     1 | loss: 1.7917104MemoryTrain:  epoch  8, batch     2 | loss: 1.4459729MemoryTrain:  epoch  8, batch     3 | loss: 1.7507305MemoryTrain:  epoch  9, batch     0 | loss: 1.4236532MemoryTrain:  epoch  9, batch     1 | loss: 1.8949518MemoryTrain:  epoch  9, batch     2 | loss: 1.4725454MemoryTrain:  epoch  9, batch     3 | loss: 1.3606548
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 78.12%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 82.54%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 80.36%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 78.30%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 76.18%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 74.18%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 73.08%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 73.28%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 72.41%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 72.32%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 72.24%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 72.87%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 74.05%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 76.00%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 75.61%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 74.64%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 73.82%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 72.57%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 71.48%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 70.31%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 70.29%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 70.69%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 70.87%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 71.31%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 71.57%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 71.63%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 72.31%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 73.13%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 73.91%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 74.65%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 74.91%   [EVAL] batch:   72 | acc: 6.25%,  total acc: 73.97%   [EVAL] batch:   73 | acc: 12.50%,  total acc: 73.14%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 72.92%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 72.94%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 72.97%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 72.12%   
cur_acc:  ['0.8655', '0.7847', '0.5848', '0.7812']
his_acc:  ['0.8655', '0.8438', '0.7500', '0.7212']
CurrentTrain: epoch  0, batch     0 | loss: 4.3291464CurrentTrain: epoch  0, batch     1 | loss: 4.6785450CurrentTrain: epoch  1, batch     0 | loss: 3.6800175CurrentTrain: epoch  1, batch     1 | loss: 3.4894519CurrentTrain: epoch  2, batch     0 | loss: 3.4160707CurrentTrain: epoch  2, batch     1 | loss: 2.8617058CurrentTrain: epoch  3, batch     0 | loss: 2.5982792CurrentTrain: epoch  3, batch     1 | loss: 2.6919327CurrentTrain: epoch  4, batch     0 | loss: 2.4807692CurrentTrain: epoch  4, batch     1 | loss: 2.1965272CurrentTrain: epoch  5, batch     0 | loss: 2.1961050CurrentTrain: epoch  5, batch     1 | loss: 2.0891359CurrentTrain: epoch  6, batch     0 | loss: 2.2498620CurrentTrain: epoch  6, batch     1 | loss: 2.0699844CurrentTrain: epoch  7, batch     0 | loss: 2.0428948CurrentTrain: epoch  7, batch     1 | loss: 2.1704149CurrentTrain: epoch  8, batch     0 | loss: 1.9818529CurrentTrain: epoch  8, batch     1 | loss: 1.9162339CurrentTrain: epoch  9, batch     0 | loss: 2.0438337CurrentTrain: epoch  9, batch     1 | loss: 2.0146484
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political landscape of the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: andrew lebow , an oil trader with mf global in new york , said investors have been discouraged by lower-than-expected oil imports in china and the disappointing growth in the u.s. economy .
Head Entity: mf global
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been a hub for innovation and development.  
Head Entity: apple inc.  
Tail Entity: cupertino, california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics has its main office situated in suwon, south korea, which plays a crucial role in its global operations.  
Head Entity: samsung electronics  
Tail Entity: suwon, south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, and their close bond, highlighting how family ties remain strong despite their busy careers.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, uncle bob shared stories about his niece, sarah, and how she has grown into a remarkable young woman, making the entire family proud.  
Head Entity: sarah  
Tail Entity: uncle bob  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: millender-mcdonald , who was 68 , died late saturday at her home in carson , california , said her chief of staff , bandele mcqueen .
Head Entity: millender-mcdonald
Tail Entity: carson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: renowned author and activist, james baldwin, passed away in 1987 in the city of saint-paul, minnesota, where he spent his final years.  
Head Entity: james baldwin  
Tail Entity: saint-paul  

Relation: person city of death  
Context: the famous physicist, stephen hawking, died peacefully at his home in cambridge, england, surrounded by family and friends.  
Head Entity: stephen hawking  
Tail Entity: cambridge  
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 6.3831377MixupTrain:  epoch  0, batch     1 | loss: 5.8429001MixupTrain:  epoch  0, batch     2 | loss: 5.4405988MixupTrain:  epoch  0, batch     3 | loss: 5.2482533MixupTrain:  epoch  0, batch     4 | loss: 4.7127558MixupTrain:  epoch  0, batch     5 | loss: 5.4961214MixupTrain:  epoch  0, batch     6 | loss: 5.2034861MixupTrain:  epoch  0, batch     7 | loss: 5.6710868MixupTrain:  epoch  0, batch     8 | loss: 5.3752414MixupTrain:  epoch  0, batch     9 | loss: 5.0935417MixupTrain:  epoch  0, batch    10 | loss: 5.2930469MixupTrain:  epoch  0, batch    11 | loss: 5.2545996MixupTrain:  epoch  0, batch    12 | loss: 4.8049335
MemoryTrain:  epoch  0, batch     0 | loss: 2.5976145MemoryTrain:  epoch  0, batch     1 | loss: 2.9940741MemoryTrain:  epoch  0, batch     2 | loss: 2.4755485MemoryTrain:  epoch  0, batch     3 | loss: 2.6412399MemoryTrain:  epoch  0, batch     4 | loss: 2.5050263MemoryTrain:  epoch  1, batch     0 | loss: 2.3859262MemoryTrain:  epoch  1, batch     1 | loss: 2.2175250MemoryTrain:  epoch  1, batch     2 | loss: 2.3122249MemoryTrain:  epoch  1, batch     3 | loss: 1.9247921MemoryTrain:  epoch  1, batch     4 | loss: 2.5597703MemoryTrain:  epoch  2, batch     0 | loss: 2.0448222MemoryTrain:  epoch  2, batch     1 | loss: 1.9012058MemoryTrain:  epoch  2, batch     2 | loss: 1.7514844MemoryTrain:  epoch  2, batch     3 | loss: 2.1053252MemoryTrain:  epoch  2, batch     4 | loss: 2.0500267MemoryTrain:  epoch  3, batch     0 | loss: 1.8713437MemoryTrain:  epoch  3, batch     1 | loss: 1.6931050MemoryTrain:  epoch  3, batch     2 | loss: 1.8373610MemoryTrain:  epoch  3, batch     3 | loss: 1.7077701MemoryTrain:  epoch  3, batch     4 | loss: 2.1441989MemoryTrain:  epoch  4, batch     0 | loss: 1.6793957MemoryTrain:  epoch  4, batch     1 | loss: 1.7486145MemoryTrain:  epoch  4, batch     2 | loss: 1.5378847MemoryTrain:  epoch  4, batch     3 | loss: 1.5861055MemoryTrain:  epoch  4, batch     4 | loss: 1.6819017MemoryTrain:  epoch  5, batch     0 | loss: 1.6470050MemoryTrain:  epoch  5, batch     1 | loss: 1.9184216MemoryTrain:  epoch  5, batch     2 | loss: 1.6159389MemoryTrain:  epoch  5, batch     3 | loss: 1.3619134MemoryTrain:  epoch  5, batch     4 | loss: 1.8252826MemoryTrain:  epoch  6, batch     0 | loss: 1.8225743MemoryTrain:  epoch  6, batch     1 | loss: 1.5218545MemoryTrain:  epoch  6, batch     2 | loss: 1.7489747MemoryTrain:  epoch  6, batch     3 | loss: 1.7335298MemoryTrain:  epoch  6, batch     4 | loss: 1.5533216MemoryTrain:  epoch  7, batch     0 | loss: 1.4029771MemoryTrain:  epoch  7, batch     1 | loss: 1.3538668MemoryTrain:  epoch  7, batch     2 | loss: 1.5484283MemoryTrain:  epoch  7, batch     3 | loss: 1.5579786MemoryTrain:  epoch  7, batch     4 | loss: 1.3136972MemoryTrain:  epoch  8, batch     0 | loss: 1.3781476MemoryTrain:  epoch  8, batch     1 | loss: 1.4041884MemoryTrain:  epoch  8, batch     2 | loss: 1.5831335MemoryTrain:  epoch  8, batch     3 | loss: 1.5486439MemoryTrain:  epoch  8, batch     4 | loss: 1.4272710MemoryTrain:  epoch  9, batch     0 | loss: 1.3968239MemoryTrain:  epoch  9, batch     1 | loss: 1.4872789MemoryTrain:  epoch  9, batch     2 | loss: 1.6219897MemoryTrain:  epoch  9, batch     3 | loss: 1.4809208MemoryTrain:  epoch  9, batch     4 | loss: 1.3325814
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 52.68%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 56.25%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 53.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 55.68%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 56.77%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 55.29%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.59%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 85.83%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 82.54%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 80.18%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 77.95%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 75.84%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 73.85%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 72.76%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 72.41%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 71.73%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 71.22%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 71.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 74.36%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 74.75%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 74.39%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 73.08%   [EVAL] batch:   52 | acc: 12.50%,  total acc: 71.93%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 70.60%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 69.43%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 68.19%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 68.20%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 68.64%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 68.86%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 69.27%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 69.26%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 69.46%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 69.92%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 71.27%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 72.89%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 73.18%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 72.35%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 71.79%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 71.42%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 71.13%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 70.94%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 70.59%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 70.65%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 70.55%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 70.12%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 69.65%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 69.35%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 69.49%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 69.04%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 68.97%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 69.03%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 68.96%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 68.68%   
cur_acc:  ['0.8655', '0.7847', '0.5848', '0.7812', '0.5529']
his_acc:  ['0.8655', '0.8438', '0.7500', '0.7212', '0.6868']
CurrentTrain: epoch  0, batch     0 | loss: 5.6795902CurrentTrain: epoch  0, batch     1 | loss: 6.6051812CurrentTrain: epoch  1, batch     0 | loss: 4.9429455CurrentTrain: epoch  1, batch     1 | loss: 5.0928864CurrentTrain: epoch  2, batch     0 | loss: 4.3034325CurrentTrain: epoch  2, batch     1 | loss: 4.3772626CurrentTrain: epoch  3, batch     0 | loss: 3.7899637CurrentTrain: epoch  3, batch     1 | loss: 3.3589664CurrentTrain: epoch  4, batch     0 | loss: 3.3709221CurrentTrain: epoch  4, batch     1 | loss: 3.3570995CurrentTrain: epoch  5, batch     0 | loss: 3.2740159CurrentTrain: epoch  5, batch     1 | loss: 3.0125668CurrentTrain: epoch  6, batch     0 | loss: 2.9906182CurrentTrain: epoch  6, batch     1 | loss: 2.6537411CurrentTrain: epoch  7, batch     0 | loss: 2.6751060CurrentTrain: epoch  7, batch     1 | loss: 2.7576044CurrentTrain: epoch  8, batch     0 | loss: 2.6204376CurrentTrain: epoch  8, batch     1 | loss: 2.3783772CurrentTrain: epoch  9, batch     0 | loss: 2.4167969CurrentTrain: epoch  9, batch     1 | loss: 2.4221523
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: the chairman of the senate foreign relations committee , massachusetts democrat john kerry , and the panel 's top republican , richard lugar of indiana , were at the white house meeting , which was led by vice president joe biden , a former chairman of the foreign relations panel .
Head Entity: john kerry
Tail Entity: massachusetts
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York to California, actress Emma Stone quickly adapted to the vibrant lifestyle of Los Angeles, where she now resides.  
Head Entity: Emma Stone  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: Following his successful career in the tech industry, Mark Zuckerberg decided to settle in Palo Alto, a city known for its innovation and proximity to Silicon Valley.  
Head Entity: Mark Zuckerberg  
Tail Entity: Palo Alto  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: former gov. robert e. mcnair of south carolina , the political moderate who was a finalist to become vice president hubert h. humphrey 's running mate in 1968 but whose promising career was cut short by what became known as the orangeburg massacre , died on nov. 17 in charleston .
Head Entity: robert e. mcnair
Tail Entity: nov. 17
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: renowned physicist stephen hawking, who made groundbreaking contributions to our understanding of black holes and the universe, passed away peacefully at his home in cambridge on march 14, 2018.  
Head Entity: stephen hawking  
Tail Entity: march 14, 2018  

Relation: person date of death  
Context: the beloved author of the harry potter series, j.k. rowling, announced the passing of her dear friend and mentor, who died on january 1, 2020, after a long battle with illness.  
Head Entity: j.k. rowling's dear friend  
Tail Entity: january 1, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, boasts a workforce of approximately 5,500 employees across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: GreenEarth Nonprofit has grown significantly over the past few years and currently employs around 1,200 dedicated staff members to support its environmental initiatives.  
Head Entity: GreenEarth Nonprofit  
Tail Entity: 1,200  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to himself as the bard of Avon, '' said the literary critic during the lecture.  
Head Entity: author  
Tail Entity: bard of Avon  

Relation: person alternate names  
Context: `` in the world of music, she is affectionately known as the queen of pop, '' remarked the radio host during the interview.  
Head Entity: she  
Tail Entity: queen of pop  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: smits stands at the center of this multigenerational saga as alex vega , the adopted son of rum and sugar baron pancho duque -lrb- elizondo -rrb- and his wife , amalia -lrb- moreno -rrb- .
Head Entity: elizondo
Tail Entity: moreno
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a heartfelt ceremony, john and his beloved partner, sarah, exchanged vows surrounded by family and friends, celebrating their love and commitment to each other.  
Head Entity: john  
Tail Entity: sarah  

Relation: person spouse  
Context: after years of companionship, the couple, michael and jessica, finally tied the knot in a beautiful garden wedding, marking the beginning of their new life together.  
Head Entity: michael  
Tail Entity: jessica  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 5.4800156MixupTrain:  epoch  0, batch     1 | loss: 5.1151627MixupTrain:  epoch  0, batch     2 | loss: 4.6123384MixupTrain:  epoch  0, batch     3 | loss: 5.3657229MixupTrain:  epoch  0, batch     4 | loss: 5.4636543MixupTrain:  epoch  0, batch     5 | loss: 4.8444752MixupTrain:  epoch  0, batch     6 | loss: 5.2616308MixupTrain:  epoch  0, batch     7 | loss: 5.0355123MixupTrain:  epoch  0, batch     8 | loss: 5.2058708MixupTrain:  epoch  0, batch     9 | loss: 5.9829622MixupTrain:  epoch  0, batch    10 | loss: 4.2280865MixupTrain:  epoch  0, batch    11 | loss: 5.2084512MixupTrain:  epoch  0, batch    12 | loss: 6.1504659MixupTrain:  epoch  0, batch    13 | loss: 5.1036154
MemoryTrain:  epoch  0, batch     0 | loss: 2.4178929MemoryTrain:  epoch  0, batch     1 | loss: 2.4665341MemoryTrain:  epoch  0, batch     2 | loss: 2.0896280MemoryTrain:  epoch  0, batch     3 | loss: 2.5222607MemoryTrain:  epoch  0, batch     4 | loss: 2.5536001MemoryTrain:  epoch  0, batch     5 | loss: 2.3076603MemoryTrain:  epoch  1, batch     0 | loss: 2.1038027MemoryTrain:  epoch  1, batch     1 | loss: 1.7665839MemoryTrain:  epoch  1, batch     2 | loss: 1.9316399MemoryTrain:  epoch  1, batch     3 | loss: 2.0596004MemoryTrain:  epoch  1, batch     4 | loss: 2.3001523MemoryTrain:  epoch  1, batch     5 | loss: 1.9923069MemoryTrain:  epoch  2, batch     0 | loss: 2.1304610MemoryTrain:  epoch  2, batch     1 | loss: 1.7711785MemoryTrain:  epoch  2, batch     2 | loss: 1.5300469MemoryTrain:  epoch  2, batch     3 | loss: 1.6972622MemoryTrain:  epoch  2, batch     4 | loss: 1.8025920MemoryTrain:  epoch  2, batch     5 | loss: 2.4888048MemoryTrain:  epoch  3, batch     0 | loss: 1.8604696MemoryTrain:  epoch  3, batch     1 | loss: 1.6593637MemoryTrain:  epoch  3, batch     2 | loss: 1.9372189MemoryTrain:  epoch  3, batch     3 | loss: 1.6902813MemoryTrain:  epoch  3, batch     4 | loss: 1.5442511MemoryTrain:  epoch  3, batch     5 | loss: 1.5637046MemoryTrain:  epoch  4, batch     0 | loss: 1.7012497MemoryTrain:  epoch  4, batch     1 | loss: 1.4268962MemoryTrain:  epoch  4, batch     2 | loss: 1.3525810MemoryTrain:  epoch  4, batch     3 | loss: 1.9201573MemoryTrain:  epoch  4, batch     4 | loss: 1.8307700MemoryTrain:  epoch  4, batch     5 | loss: 1.5724721MemoryTrain:  epoch  5, batch     0 | loss: 1.4901903MemoryTrain:  epoch  5, batch     1 | loss: 1.4944128MemoryTrain:  epoch  5, batch     2 | loss: 1.9698985MemoryTrain:  epoch  5, batch     3 | loss: 1.6117876MemoryTrain:  epoch  5, batch     4 | loss: 1.3122795MemoryTrain:  epoch  5, batch     5 | loss: 1.4968076MemoryTrain:  epoch  6, batch     0 | loss: 1.3772880MemoryTrain:  epoch  6, batch     1 | loss: 1.7023582MemoryTrain:  epoch  6, batch     2 | loss: 1.4916036MemoryTrain:  epoch  6, batch     3 | loss: 1.5226486MemoryTrain:  epoch  6, batch     4 | loss: 1.4002221MemoryTrain:  epoch  6, batch     5 | loss: 1.4510109MemoryTrain:  epoch  7, batch     0 | loss: 1.4976745MemoryTrain:  epoch  7, batch     1 | loss: 1.5003399MemoryTrain:  epoch  7, batch     2 | loss: 1.3967437MemoryTrain:  epoch  7, batch     3 | loss: 1.3694094MemoryTrain:  epoch  7, batch     4 | loss: 1.4945600MemoryTrain:  epoch  7, batch     5 | loss: 1.4907544MemoryTrain:  epoch  8, batch     0 | loss: 1.5040464MemoryTrain:  epoch  8, batch     1 | loss: 1.2746633MemoryTrain:  epoch  8, batch     2 | loss: 1.4036452MemoryTrain:  epoch  8, batch     3 | loss: 1.3926905MemoryTrain:  epoch  8, batch     4 | loss: 1.3580073MemoryTrain:  epoch  8, batch     5 | loss: 1.4320753MemoryTrain:  epoch  9, batch     0 | loss: 1.3548594MemoryTrain:  epoch  9, batch     1 | loss: 1.4675388MemoryTrain:  epoch  9, batch     2 | loss: 1.3296251MemoryTrain:  epoch  9, batch     3 | loss: 1.3275974MemoryTrain:  epoch  9, batch     4 | loss: 1.2598770MemoryTrain:  epoch  9, batch     5 | loss: 1.2822974
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 63.75%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 83.41%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 83.80%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.91%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 85.08%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 84.09%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 81.62%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 77.08%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 75.00%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 73.03%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 71.96%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 72.34%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 71.34%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 70.68%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 70.20%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 70.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 73.88%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 73.41%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 72.36%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 71.46%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 70.14%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 68.86%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 67.63%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 67.54%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 68.33%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 68.85%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 69.05%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 69.34%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 71.56%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 71.83%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 71.54%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 71.17%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 70.72%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 70.45%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 70.11%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 70.09%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 69.84%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 69.75%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 69.51%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 69.20%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 69.05%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 69.12%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 68.82%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 68.68%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 68.82%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 68.33%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 68.19%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 68.06%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 68.14%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 68.15%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 68.09%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 68.29%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 68.49%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 69.07%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 68.88%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 68.63%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 68.26%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 68.14%   [EVAL] batch:  103 | acc: 43.75%,  total acc: 67.91%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 67.26%   
cur_acc:  ['0.8655', '0.7847', '0.5848', '0.7812', '0.5529', '0.6375']
his_acc:  ['0.8655', '0.8438', '0.7500', '0.7212', '0.6868', '0.6726']
CurrentTrain: epoch  0, batch     0 | loss: 5.6390748CurrentTrain: epoch  0, batch     1 | loss: 5.0401568CurrentTrain: epoch  1, batch     0 | loss: 4.4505696CurrentTrain: epoch  1, batch     1 | loss: 4.2809415CurrentTrain: epoch  2, batch     0 | loss: 3.3644130CurrentTrain: epoch  2, batch     1 | loss: 4.1455407CurrentTrain: epoch  3, batch     0 | loss: 3.1114032CurrentTrain: epoch  3, batch     1 | loss: 3.9377179CurrentTrain: epoch  4, batch     0 | loss: 3.2747431CurrentTrain: epoch  4, batch     1 | loss: 2.8041987CurrentTrain: epoch  5, batch     0 | loss: 2.9457321CurrentTrain: epoch  5, batch     1 | loss: 3.0717440CurrentTrain: epoch  6, batch     0 | loss: 3.1895692CurrentTrain: epoch  6, batch     1 | loss: 2.3769512CurrentTrain: epoch  7, batch     0 | loss: 3.0599864CurrentTrain: epoch  7, batch     1 | loss: 2.2559621CurrentTrain: epoch  8, batch     0 | loss: 2.8860934CurrentTrain: epoch  8, batch     1 | loss: 2.8153703CurrentTrain: epoch  9, batch     0 | loss: 2.9054449CurrentTrain: epoch  9, batch     1 | loss: 2.3575566
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, the renowned artist shared his experiences growing up in the vibrant streets of Barcelona, which he credits as the foundation of his creative journey.  
Head Entity: the renowned artist  
Tail Entity: Barcelona  

Relation: person country of birth  
Context: In her autobiography, the famous scientist details her early life in Tokyo, where she developed a passion for technology and innovation.  
Head Entity: the famous scientist  
Tail Entity: Tokyo  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit our official site at https://www.techinnovators.com for the latest updates on our projects.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For more information about our services, check out our website at http://www.greenearthsolutions.org.  
Head Entity: Green Earth Solutions  
Tail Entity: http://www.greenearthsolutions.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: ------ liberty media acquired a 41 percent stake in directv in late february by exchanging it for a 16 percent stake in news corp plus $ 625 million -lrb- euro402 5 million -rrb- in cash .
Head Entity: directv
Tail Entity: liberty media
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: In 2020, Amazon announced that it had acquired a significant share in the electric vehicle startup Rivian, marking a strategic investment in the future of transportation.  
Head Entity: Rivian  
Tail Entity: Amazon  

Relation: organization shareholders  
Context: The recent merger between Disney and 21st Century Fox resulted in Disney becoming the majority shareholder of the entertainment giant, expanding its portfolio significantly.  
Head Entity: 21st Century Fox  
Tail Entity: Disney  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in the spring of 2019, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring of 2019  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, John Doe, to support underprivileged children around the world.  
Head Entity: Hope for Tomorrow  
Tail Entity: John Doe  
Mixup data size:  255
MixupTrain:  epoch  0, batch     0 | loss: 5.4704606MixupTrain:  epoch  0, batch     1 | loss: 4.2870122MixupTrain:  epoch  0, batch     2 | loss: 4.7261028MixupTrain:  epoch  0, batch     3 | loss: 5.8470975MixupTrain:  epoch  0, batch     4 | loss: 3.9127918MixupTrain:  epoch  0, batch     5 | loss: 4.1757100MixupTrain:  epoch  0, batch     6 | loss: 4.7598799MixupTrain:  epoch  0, batch     7 | loss: 5.2097045MixupTrain:  epoch  0, batch     8 | loss: 5.7274599MixupTrain:  epoch  0, batch     9 | loss: 5.6813800MixupTrain:  epoch  0, batch    10 | loss: 5.6497204MixupTrain:  epoch  0, batch    11 | loss: 5.3485999MixupTrain:  epoch  0, batch    12 | loss: 3.7963401MixupTrain:  epoch  0, batch    13 | loss: 4.6187563MixupTrain:  epoch  0, batch    14 | loss: 5.9024554MixupTrain:  epoch  0, batch    15 | loss: 4.2577955
MemoryTrain:  epoch  0, batch     0 | loss: 3.3921285MemoryTrain:  epoch  0, batch     1 | loss: 2.1337750MemoryTrain:  epoch  0, batch     2 | loss: 1.8842711MemoryTrain:  epoch  0, batch     3 | loss: 2.1849377MemoryTrain:  epoch  0, batch     4 | loss: 1.8972178MemoryTrain:  epoch  0, batch     5 | loss: 2.2107267MemoryTrain:  epoch  0, batch     6 | loss: 2.6265745MemoryTrain:  epoch  1, batch     0 | loss: 2.2028651MemoryTrain:  epoch  1, batch     1 | loss: 2.5325975MemoryTrain:  epoch  1, batch     2 | loss: 1.8947945MemoryTrain:  epoch  1, batch     3 | loss: 2.3354225MemoryTrain:  epoch  1, batch     4 | loss: 1.4246907MemoryTrain:  epoch  1, batch     5 | loss: 2.3017006MemoryTrain:  epoch  1, batch     6 | loss: 1.7597845MemoryTrain:  epoch  2, batch     0 | loss: 2.0148101MemoryTrain:  epoch  2, batch     1 | loss: 1.6353359MemoryTrain:  epoch  2, batch     2 | loss: 1.8304739MemoryTrain:  epoch  2, batch     3 | loss: 1.8089888MemoryTrain:  epoch  2, batch     4 | loss: 1.4951826MemoryTrain:  epoch  2, batch     5 | loss: 1.6343347MemoryTrain:  epoch  2, batch     6 | loss: 1.8609171MemoryTrain:  epoch  3, batch     0 | loss: 1.9220927MemoryTrain:  epoch  3, batch     1 | loss: 1.5789433MemoryTrain:  epoch  3, batch     2 | loss: 1.7459464MemoryTrain:  epoch  3, batch     3 | loss: 1.6076629MemoryTrain:  epoch  3, batch     4 | loss: 1.5359315MemoryTrain:  epoch  3, batch     5 | loss: 1.6442418MemoryTrain:  epoch  3, batch     6 | loss: 1.4386253MemoryTrain:  epoch  4, batch     0 | loss: 1.6675737MemoryTrain:  epoch  4, batch     1 | loss: 1.5504290MemoryTrain:  epoch  4, batch     2 | loss: 1.3106620MemoryTrain:  epoch  4, batch     3 | loss: 1.7877980MemoryTrain:  epoch  4, batch     4 | loss: 1.4302151MemoryTrain:  epoch  4, batch     5 | loss: 1.4697903MemoryTrain:  epoch  4, batch     6 | loss: 1.7528112MemoryTrain:  epoch  5, batch     0 | loss: 1.3401637MemoryTrain:  epoch  5, batch     1 | loss: 1.4553822MemoryTrain:  epoch  5, batch     2 | loss: 1.4321249MemoryTrain:  epoch  5, batch     3 | loss: 1.6638808MemoryTrain:  epoch  5, batch     4 | loss: 1.6823949MemoryTrain:  epoch  5, batch     5 | loss: 1.3942845MemoryTrain:  epoch  5, batch     6 | loss: 1.3623762MemoryTrain:  epoch  6, batch     0 | loss: 1.5313939MemoryTrain:  epoch  6, batch     1 | loss: 1.4096769MemoryTrain:  epoch  6, batch     2 | loss: 1.5155898MemoryTrain:  epoch  6, batch     3 | loss: 1.3357344MemoryTrain:  epoch  6, batch     4 | loss: 1.5213710MemoryTrain:  epoch  6, batch     5 | loss: 1.3574928MemoryTrain:  epoch  6, batch     6 | loss: 1.4319756MemoryTrain:  epoch  7, batch     0 | loss: 1.4082212MemoryTrain:  epoch  7, batch     1 | loss: 1.4142302MemoryTrain:  epoch  7, batch     2 | loss: 1.3519948MemoryTrain:  epoch  7, batch     3 | loss: 1.4890287MemoryTrain:  epoch  7, batch     4 | loss: 1.2937016MemoryTrain:  epoch  7, batch     5 | loss: 1.3581676MemoryTrain:  epoch  7, batch     6 | loss: 1.4801035MemoryTrain:  epoch  8, batch     0 | loss: 1.2368153MemoryTrain:  epoch  8, batch     1 | loss: 1.3804173MemoryTrain:  epoch  8, batch     2 | loss: 1.3337765MemoryTrain:  epoch  8, batch     3 | loss: 1.3056363MemoryTrain:  epoch  8, batch     4 | loss: 1.3817089MemoryTrain:  epoch  8, batch     5 | loss: 1.3310673MemoryTrain:  epoch  8, batch     6 | loss: 1.5078596MemoryTrain:  epoch  9, batch     0 | loss: 1.2709507MemoryTrain:  epoch  9, batch     1 | loss: 1.3468447MemoryTrain:  epoch  9, batch     2 | loss: 1.3457396MemoryTrain:  epoch  9, batch     3 | loss: 1.2990156MemoryTrain:  epoch  9, batch     4 | loss: 1.3843720MemoryTrain:  epoch  9, batch     5 | loss: 1.2687625MemoryTrain:  epoch  9, batch     6 | loss: 1.2974648
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 67.97%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 45.31%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 41.15%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 39.90%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 39.73%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 41.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 42.58%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 44.49%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 45.49%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 46.38%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 48.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 50.60%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 52.56%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 54.35%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 55.99%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 59.13%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 60.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 63.54%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 64.52%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 65.43%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 64.58%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 62.68%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 60.89%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 59.20%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 57.60%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 56.09%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 55.29%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 56.09%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 55.64%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 55.51%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 55.52%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 56.11%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 57.08%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 58.02%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 58.91%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 59.77%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 60.59%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 61.12%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 61.03%   [EVAL] batch:   51 | acc: 25.00%,  total acc: 60.34%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 59.67%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 58.56%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 57.73%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 56.70%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 56.91%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 57.54%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 58.05%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 58.65%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 58.71%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 58.97%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 59.13%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 59.62%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 60.04%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 60.63%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 61.21%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 61.78%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 62.32%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 62.85%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 63.28%   [EVAL] batch:   72 | acc: 0.00%,  total acc: 62.41%   [EVAL] batch:   73 | acc: 6.25%,  total acc: 61.66%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 61.58%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 61.35%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 61.44%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 61.14%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 61.23%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 61.09%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 61.11%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 60.90%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 60.77%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 60.71%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 60.88%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 60.68%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 60.70%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 60.80%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 60.60%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 60.49%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 60.37%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 60.19%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 60.15%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 60.11%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 60.13%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 60.35%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 60.76%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 61.10%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 61.43%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 61.06%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 60.83%   [EVAL] batch:  101 | acc: 37.50%,  total acc: 60.60%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 60.56%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 60.34%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 60.79%   [EVAL] batch:  106 | acc: 87.50%,  total acc: 61.04%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 61.23%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 61.18%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 61.26%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 60.88%   
cur_acc:  ['0.8655', '0.7847', '0.5848', '0.7812', '0.5529', '0.6375', '0.6797']
his_acc:  ['0.8655', '0.8438', '0.7500', '0.7212', '0.6868', '0.6726', '0.6088']
CurrentTrain: epoch  0, batch     0 | loss: 7.6071882CurrentTrain: epoch  0, batch     1 | loss: 8.3488941CurrentTrain: epoch  1, batch     0 | loss: 7.2666669CurrentTrain: epoch  1, batch     1 | loss: 6.3206277CurrentTrain: epoch  2, batch     0 | loss: 6.0013866CurrentTrain: epoch  2, batch     1 | loss: 6.9461207CurrentTrain: epoch  3, batch     0 | loss: 5.5996723CurrentTrain: epoch  3, batch     1 | loss: 6.4793267CurrentTrain: epoch  4, batch     0 | loss: 5.6733503CurrentTrain: epoch  4, batch     1 | loss: 5.9924722CurrentTrain: epoch  5, batch     0 | loss: 5.2351823CurrentTrain: epoch  5, batch     1 | loss: 5.4256701CurrentTrain: epoch  6, batch     0 | loss: 5.1237602CurrentTrain: epoch  6, batch     1 | loss: 4.7826142CurrentTrain: epoch  7, batch     0 | loss: 4.8393044CurrentTrain: epoch  7, batch     1 | loss: 4.4324265CurrentTrain: epoch  8, batch     0 | loss: 4.6097269CurrentTrain: epoch  8, batch     1 | loss: 3.8492682CurrentTrain: epoch  9, batch     0 | loss: 3.6444521CurrentTrain: epoch  9, batch     1 | loss: 4.7283282
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including Google LLC, which is known for its search engine and various tech services.  
Head Entity: Alphabet Inc.  
Tail Entity: Google LLC  

Relation: organization subsidiaries  
Context: The Walt Disney Company owns multiple subsidiaries, such as Pixar Animation Studios, which is famous for its animated films.  
Head Entity: The Walt Disney Company  
Tail Entity: Pixar Animation Studios  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant SoftTech announced its acquisition of Innovatech, a leading software development firm. This move has raised questions about the future of Innovatech and its role under the SoftTech umbrella. Industry experts believe that this acquisition will strengthen SoftTech's position in the market.  
Head Entity: SoftTech  
Tail Entity: Innovatech  

Relation: organization parents  
Context: The historic partnership between the National Wildlife Federation and the World Wildlife Fund has led to significant advancements in conservation efforts. As these two organizations collaborate, they aim to tackle global environmental challenges more effectively.  
Head Entity: National Wildlife Federation  
Tail Entity: World Wildlife Fund  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the world health organization -lrb- who -rrb- has been instrumental in coordinating global responses to health emergencies, including the recent pandemic.  
Head Entity: world health organization  
Tail Entity: who  

Relation: organization alternate names  
Context: the federal bureau of investigation -lrb- fbi -rrb- is known for its role in investigating federal crimes and maintaining national security.  
Head Entity: federal bureau of investigation  
Tail Entity: fbi  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ in 2015, tech giant apple inc. announced plans to expand its operations in cupertino, california, where it has been headquartered since its founding.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ the multinational corporation samsung electronics is based in suwon, south korea, and has been a leader in the technology sector for decades.  
Head Entity: samsung electronics  
Tail Entity: suwon  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: during the family reunion, john introduced his sister, emily, who had just returned from studying abroad.  
Head Entity: john  
Tail Entity: emily  

Relation: person siblings  
Context: at the award ceremony, sarah proudly accepted her trophy while her brother, michael, cheered from the audience.  
Head Entity: sarah  
Tail Entity: michael  
Mixup data size:  285
MixupTrain:  epoch  0, batch     0 | loss: 5.2700397MixupTrain:  epoch  0, batch     1 | loss: 4.1112915MixupTrain:  epoch  0, batch     2 | loss: 5.4676933MixupTrain:  epoch  0, batch     3 | loss: 5.1984940MixupTrain:  epoch  0, batch     4 | loss: 4.6878123MixupTrain:  epoch  0, batch     5 | loss: 4.9337435MixupTrain:  epoch  0, batch     6 | loss: 4.1504245MixupTrain:  epoch  0, batch     7 | loss: 4.5123356MixupTrain:  epoch  0, batch     8 | loss: 5.1082590MixupTrain:  epoch  0, batch     9 | loss: 5.7102724MixupTrain:  epoch  0, batch    10 | loss: 5.1135990MixupTrain:  epoch  0, batch    11 | loss: 4.9515885MixupTrain:  epoch  0, batch    12 | loss: 4.1116933MixupTrain:  epoch  0, batch    13 | loss: 5.1209858MixupTrain:  epoch  0, batch    14 | loss: 3.6902302MixupTrain:  epoch  0, batch    15 | loss: 4.6510834MixupTrain:  epoch  0, batch    16 | loss: 3.6514869MixupTrain:  epoch  0, batch    17 | loss: 4.1311885
MemoryTrain:  epoch  0, batch     0 | loss: 2.6262271MemoryTrain:  epoch  0, batch     1 | loss: 2.0412452MemoryTrain:  epoch  0, batch     2 | loss: 2.1900702MemoryTrain:  epoch  0, batch     3 | loss: 3.4982896MemoryTrain:  epoch  0, batch     4 | loss: 1.8011266MemoryTrain:  epoch  0, batch     5 | loss: 2.5482907MemoryTrain:  epoch  0, batch     6 | loss: 1.9659864MemoryTrain:  epoch  0, batch     7 | loss: 3.1420889MemoryTrain:  epoch  1, batch     0 | loss: 2.1823952MemoryTrain:  epoch  1, batch     1 | loss: 2.6291838MemoryTrain:  epoch  1, batch     2 | loss: 2.5904670MemoryTrain:  epoch  1, batch     3 | loss: 2.6115866MemoryTrain:  epoch  1, batch     4 | loss: 2.0993443MemoryTrain:  epoch  1, batch     5 | loss: 1.9552722MemoryTrain:  epoch  1, batch     6 | loss: 2.5187614MemoryTrain:  epoch  1, batch     7 | loss: 2.2671549MemoryTrain:  epoch  2, batch     0 | loss: 2.0419264MemoryTrain:  epoch  2, batch     1 | loss: 2.0043380MemoryTrain:  epoch  2, batch     2 | loss: 1.8485643MemoryTrain:  epoch  2, batch     3 | loss: 2.1663857MemoryTrain:  epoch  2, batch     4 | loss: 2.4461038MemoryTrain:  epoch  2, batch     5 | loss: 1.6744328MemoryTrain:  epoch  2, batch     6 | loss: 1.7512362MemoryTrain:  epoch  2, batch     7 | loss: 1.9016155MemoryTrain:  epoch  3, batch     0 | loss: 1.6493678MemoryTrain:  epoch  3, batch     1 | loss: 1.8500268MemoryTrain:  epoch  3, batch     2 | loss: 1.7267181MemoryTrain:  epoch  3, batch     3 | loss: 2.2177980MemoryTrain:  epoch  3, batch     4 | loss: 1.8758401MemoryTrain:  epoch  3, batch     5 | loss: 1.4910492MemoryTrain:  epoch  3, batch     6 | loss: 2.0509791MemoryTrain:  epoch  3, batch     7 | loss: 1.4965110MemoryTrain:  epoch  4, batch     0 | loss: 1.8151054MemoryTrain:  epoch  4, batch     1 | loss: 1.6073017MemoryTrain:  epoch  4, batch     2 | loss: 1.5980005MemoryTrain:  epoch  4, batch     3 | loss: 1.3625516MemoryTrain:  epoch  4, batch     4 | loss: 2.1723013MemoryTrain:  epoch  4, batch     5 | loss: 2.0273845MemoryTrain:  epoch  4, batch     6 | loss: 1.3812718MemoryTrain:  epoch  4, batch     7 | loss: 1.3318892MemoryTrain:  epoch  5, batch     0 | loss: 1.3935881MemoryTrain:  epoch  5, batch     1 | loss: 1.5525041MemoryTrain:  epoch  5, batch     2 | loss: 1.5139706MemoryTrain:  epoch  5, batch     3 | loss: 1.3803695MemoryTrain:  epoch  5, batch     4 | loss: 1.9443495MemoryTrain:  epoch  5, batch     5 | loss: 1.4790115MemoryTrain:  epoch  5, batch     6 | loss: 1.7165838MemoryTrain:  epoch  5, batch     7 | loss: 2.0033216MemoryTrain:  epoch  6, batch     0 | loss: 1.4445161MemoryTrain:  epoch  6, batch     1 | loss: 1.6168051MemoryTrain:  epoch  6, batch     2 | loss: 1.6545252MemoryTrain:  epoch  6, batch     3 | loss: 1.3494098MemoryTrain:  epoch  6, batch     4 | loss: 1.3766985MemoryTrain:  epoch  6, batch     5 | loss: 1.7374227MemoryTrain:  epoch  6, batch     6 | loss: 1.8641244MemoryTrain:  epoch  6, batch     7 | loss: 1.4358473MemoryTrain:  epoch  7, batch     0 | loss: 1.4227070MemoryTrain:  epoch  7, batch     1 | loss: 1.4231064MemoryTrain:  epoch  7, batch     2 | loss: 1.4371802MemoryTrain:  epoch  7, batch     3 | loss: 1.4124827MemoryTrain:  epoch  7, batch     4 | loss: 1.3003434MemoryTrain:  epoch  7, batch     5 | loss: 1.2342740MemoryTrain:  epoch  7, batch     6 | loss: 1.5226978MemoryTrain:  epoch  7, batch     7 | loss: 1.9740233MemoryTrain:  epoch  8, batch     0 | loss: 1.5527792MemoryTrain:  epoch  8, batch     1 | loss: 1.3606904MemoryTrain:  epoch  8, batch     2 | loss: 1.3605433MemoryTrain:  epoch  8, batch     3 | loss: 1.7395678MemoryTrain:  epoch  8, batch     4 | loss: 1.2948834MemoryTrain:  epoch  8, batch     5 | loss: 1.5154088MemoryTrain:  epoch  8, batch     6 | loss: 1.3317624MemoryTrain:  epoch  8, batch     7 | loss: 1.2128855MemoryTrain:  epoch  9, batch     0 | loss: 1.2451673MemoryTrain:  epoch  9, batch     1 | loss: 1.5872850MemoryTrain:  epoch  9, batch     2 | loss: 1.3881400MemoryTrain:  epoch  9, batch     3 | loss: 1.2305021MemoryTrain:  epoch  9, batch     4 | loss: 1.3852116MemoryTrain:  epoch  9, batch     5 | loss: 1.3602560MemoryTrain:  epoch  9, batch     6 | loss: 1.4999135MemoryTrain:  epoch  9, batch     7 | loss: 1.5759501
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 18.75%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 28.47%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 31.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 35.80%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 39.58%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 42.31%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 41.52%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 40.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 41.02%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 41.18%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 41.67%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 41.78%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 41.88%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 40.77%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 40.06%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 59.82%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 50.00%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 48.86%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 46.15%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 45.09%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 46.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 47.27%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 48.90%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 49.65%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 50.33%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 52.19%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 54.17%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 55.97%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 57.61%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 59.11%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 60.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 62.02%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 62.96%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.52%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 67.14%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 67.05%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 65.07%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 63.21%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 61.46%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 59.80%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 58.22%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 57.53%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 58.28%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 57.77%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 57.44%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 57.27%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 57.81%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 58.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 59.65%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 60.51%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 61.33%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 62.12%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 62.62%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 62.25%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 61.30%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 60.26%   [EVAL] batch:   53 | acc: 0.00%,  total acc: 59.14%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 58.18%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 57.14%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 57.13%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 57.76%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 58.37%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 58.96%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 59.02%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 59.27%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 59.72%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 59.96%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 60.19%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 60.61%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 61.19%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 61.76%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 62.32%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 62.86%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 63.38%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 63.80%   [EVAL] batch:   72 | acc: 0.00%,  total acc: 62.93%   [EVAL] batch:   73 | acc: 12.50%,  total acc: 62.25%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 62.17%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 62.17%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 62.26%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 62.02%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 62.10%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 61.95%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 62.04%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 61.59%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 61.30%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 60.94%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 60.59%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 60.25%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 59.77%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 59.45%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 59.27%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 59.17%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 59.07%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 58.97%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 59.01%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 58.98%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 59.14%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 59.79%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 60.08%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 60.06%   [EVAL] batch:  100 | acc: 12.50%,  total acc: 59.59%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 59.01%   [EVAL] batch:  102 | acc: 12.50%,  total acc: 58.56%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 57.99%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 57.98%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 58.37%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 58.35%   [EVAL] batch:  107 | acc: 93.75%,  total acc: 58.68%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 58.72%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 58.81%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 58.84%   [EVAL] batch:  111 | acc: 43.75%,  total acc: 58.71%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 58.52%   [EVAL] batch:  113 | acc: 25.00%,  total acc: 58.22%   [EVAL] batch:  114 | acc: 6.25%,  total acc: 57.77%   [EVAL] batch:  115 | acc: 0.00%,  total acc: 57.27%   [EVAL] batch:  116 | acc: 18.75%,  total acc: 56.94%   [EVAL] batch:  117 | acc: 18.75%,  total acc: 56.62%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 56.62%   [EVAL] batch:  119 | acc: 56.25%,  total acc: 56.61%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 56.66%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 56.81%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 57.06%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 57.21%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 56.90%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 56.70%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 56.64%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 56.54%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 56.49%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 56.44%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 56.39%   [EVAL] batch:  131 | acc: 18.75%,  total acc: 56.11%   [EVAL] batch:  132 | acc: 25.00%,  total acc: 55.87%   
cur_acc:  ['0.8655', '0.7847', '0.5848', '0.7812', '0.5529', '0.6375', '0.6797', '0.4006']
his_acc:  ['0.8655', '0.8438', '0.7500', '0.7212', '0.6868', '0.6726', '0.6088', '0.5587']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.3306980CurrentTrain: epoch  0, batch     1 | loss: 12.9171925CurrentTrain: epoch  0, batch     2 | loss: 13.0603256CurrentTrain: epoch  0, batch     3 | loss: 12.8535118CurrentTrain: epoch  0, batch     4 | loss: 12.7984390CurrentTrain: epoch  0, batch     5 | loss: 12.8592243CurrentTrain: epoch  0, batch     6 | loss: 12.5511436CurrentTrain: epoch  0, batch     7 | loss: 12.3986626CurrentTrain: epoch  0, batch     8 | loss: 12.2888756CurrentTrain: epoch  0, batch     9 | loss: 12.2094164CurrentTrain: epoch  0, batch    10 | loss: 12.2149239CurrentTrain: epoch  0, batch    11 | loss: 12.0873165CurrentTrain: epoch  0, batch    12 | loss: 11.6064587CurrentTrain: epoch  0, batch    13 | loss: 11.3918343CurrentTrain: epoch  0, batch    14 | loss: 11.7270164CurrentTrain: epoch  0, batch    15 | loss: 11.6622562CurrentTrain: epoch  0, batch    16 | loss: 11.5266285CurrentTrain: epoch  0, batch    17 | loss: 11.3130484CurrentTrain: epoch  0, batch    18 | loss: 11.1980724CurrentTrain: epoch  0, batch    19 | loss: 11.0485134CurrentTrain: epoch  0, batch    20 | loss: 10.7129078CurrentTrain: epoch  0, batch    21 | loss: 11.1036720CurrentTrain: epoch  0, batch    22 | loss: 11.1246014CurrentTrain: epoch  0, batch    23 | loss: 11.5370445CurrentTrain: epoch  0, batch    24 | loss: 11.2462368CurrentTrain: epoch  0, batch    25 | loss: 11.3189411CurrentTrain: epoch  0, batch    26 | loss: 11.2438068CurrentTrain: epoch  0, batch    27 | loss: 11.3493509CurrentTrain: epoch  0, batch    28 | loss: 10.7314854CurrentTrain: epoch  0, batch    29 | loss: 10.6626778CurrentTrain: epoch  0, batch    30 | loss: 10.8264828CurrentTrain: epoch  0, batch    31 | loss: 10.9649353CurrentTrain: epoch  0, batch    32 | loss: 10.4672527CurrentTrain: epoch  0, batch    33 | loss: 10.5900078CurrentTrain: epoch  0, batch    34 | loss: 10.2416763CurrentTrain: epoch  0, batch    35 | loss: 10.7742090CurrentTrain: epoch  0, batch    36 | loss: 10.7466631CurrentTrain: epoch  0, batch    37 | loss: 10.8900566CurrentTrain: epoch  1, batch     0 | loss: 9.8694220CurrentTrain: epoch  1, batch     1 | loss: 10.2078505CurrentTrain: epoch  1, batch     2 | loss: 10.6481495CurrentTrain: epoch  1, batch     3 | loss: 10.4952278CurrentTrain: epoch  1, batch     4 | loss: 10.1402473CurrentTrain: epoch  1, batch     5 | loss: 10.1705875CurrentTrain: epoch  1, batch     6 | loss: 9.7978563CurrentTrain: epoch  1, batch     7 | loss: 9.6664314CurrentTrain: epoch  1, batch     8 | loss: 9.7057629CurrentTrain: epoch  1, batch     9 | loss: 9.9080019CurrentTrain: epoch  1, batch    10 | loss: 9.6918802CurrentTrain: epoch  1, batch    11 | loss: 10.1201382CurrentTrain: epoch  1, batch    12 | loss: 9.7918301CurrentTrain: epoch  1, batch    13 | loss: 9.6302290CurrentTrain: epoch  1, batch    14 | loss: 9.7842264CurrentTrain: epoch  1, batch    15 | loss: 9.0679054CurrentTrain: epoch  1, batch    16 | loss: 9.2917309CurrentTrain: epoch  1, batch    17 | loss: 9.0753670CurrentTrain: epoch  1, batch    18 | loss: 9.6770544CurrentTrain: epoch  1, batch    19 | loss: 9.7852249CurrentTrain: epoch  1, batch    20 | loss: 9.7920017CurrentTrain: epoch  1, batch    21 | loss: 9.1483431CurrentTrain: epoch  1, batch    22 | loss: 9.8478355CurrentTrain: epoch  1, batch    23 | loss: 9.2847900CurrentTrain: epoch  1, batch    24 | loss: 9.9179077CurrentTrain: epoch  1, batch    25 | loss: 9.5210342CurrentTrain: epoch  1, batch    26 | loss: 8.9807272CurrentTrain: epoch  1, batch    27 | loss: 9.7327824CurrentTrain: epoch  1, batch    28 | loss: 8.7640629CurrentTrain: epoch  1, batch    29 | loss: 8.5793152CurrentTrain: epoch  1, batch    30 | loss: 8.5866976CurrentTrain: epoch  1, batch    31 | loss: 8.6082020CurrentTrain: epoch  1, batch    32 | loss: 8.3709135CurrentTrain: epoch  1, batch    33 | loss: 8.5839853CurrentTrain: epoch  1, batch    34 | loss: 9.0656605CurrentTrain: epoch  1, batch    35 | loss: 8.1580029CurrentTrain: epoch  1, batch    36 | loss: 8.9546986CurrentTrain: epoch  1, batch    37 | loss: 6.8484335CurrentTrain: epoch  2, batch     0 | loss: 7.4260392CurrentTrain: epoch  2, batch     1 | loss: 8.9324360CurrentTrain: epoch  2, batch     2 | loss: 8.8911762CurrentTrain: epoch  2, batch     3 | loss: 8.0932350CurrentTrain: epoch  2, batch     4 | loss: 8.0686913CurrentTrain: epoch  2, batch     5 | loss: 8.6763496CurrentTrain: epoch  2, batch     6 | loss: 8.8434868CurrentTrain: epoch  2, batch     7 | loss: 9.1854200CurrentTrain: epoch  2, batch     8 | loss: 8.6675930CurrentTrain: epoch  2, batch     9 | loss: 8.3111982CurrentTrain: epoch  2, batch    10 | loss: 8.1448822CurrentTrain: epoch  2, batch    11 | loss: 8.5338364CurrentTrain: epoch  2, batch    12 | loss: 8.5213146CurrentTrain: epoch  2, batch    13 | loss: 8.7536507CurrentTrain: epoch  2, batch    14 | loss: 7.7933855CurrentTrain: epoch  2, batch    15 | loss: 7.7356834CurrentTrain: epoch  2, batch    16 | loss: 8.3964615CurrentTrain: epoch  2, batch    17 | loss: 7.9708824CurrentTrain: epoch  2, batch    18 | loss: 7.8445940CurrentTrain: epoch  2, batch    19 | loss: 7.9893465CurrentTrain: epoch  2, batch    20 | loss: 8.0960236CurrentTrain: epoch  2, batch    21 | loss: 7.8182955CurrentTrain: epoch  2, batch    22 | loss: 7.1916733CurrentTrain: epoch  2, batch    23 | loss: 8.6036024CurrentTrain: epoch  2, batch    24 | loss: 8.2918196CurrentTrain: epoch  2, batch    25 | loss: 7.3195481CurrentTrain: epoch  2, batch    26 | loss: 8.0324974CurrentTrain: epoch  2, batch    27 | loss: 7.5756135CurrentTrain: epoch  2, batch    28 | loss: 7.2080297CurrentTrain: epoch  2, batch    29 | loss: 8.0759726CurrentTrain: epoch  2, batch    30 | loss: 7.8597140CurrentTrain: epoch  2, batch    31 | loss: 8.8081760CurrentTrain: epoch  2, batch    32 | loss: 7.9993386CurrentTrain: epoch  2, batch    33 | loss: 8.0218382CurrentTrain: epoch  2, batch    34 | loss: 8.2765856CurrentTrain: epoch  2, batch    35 | loss: 7.0053267CurrentTrain: epoch  2, batch    36 | loss: 8.1385374CurrentTrain: epoch  2, batch    37 | loss: 8.0938206CurrentTrain: epoch  3, batch     0 | loss: 7.4790201CurrentTrain: epoch  3, batch     1 | loss: 7.2108498CurrentTrain: epoch  3, batch     2 | loss: 7.6671166CurrentTrain: epoch  3, batch     3 | loss: 7.5820198CurrentTrain: epoch  3, batch     4 | loss: 8.2340021CurrentTrain: epoch  3, batch     5 | loss: 7.7397728CurrentTrain: epoch  3, batch     6 | loss: 8.1983986CurrentTrain: epoch  3, batch     7 | loss: 8.0072994CurrentTrain: epoch  3, batch     8 | loss: 6.4154425CurrentTrain: epoch  3, batch     9 | loss: 6.9852095CurrentTrain: epoch  3, batch    10 | loss: 8.6430492CurrentTrain: epoch  3, batch    11 | loss: 6.8536630CurrentTrain: epoch  3, batch    12 | loss: 7.0254326CurrentTrain: epoch  3, batch    13 | loss: 6.5383472CurrentTrain: epoch  3, batch    14 | loss: 7.3602667CurrentTrain: epoch  3, batch    15 | loss: 7.0656672CurrentTrain: epoch  3, batch    16 | loss: 7.6193819CurrentTrain: epoch  3, batch    17 | loss: 7.0022225CurrentTrain: epoch  3, batch    18 | loss: 7.2107863CurrentTrain: epoch  3, batch    19 | loss: 7.6109600CurrentTrain: epoch  3, batch    20 | loss: 6.8491340CurrentTrain: epoch  3, batch    21 | loss: 8.4221745CurrentTrain: epoch  3, batch    22 | loss: 7.2976890CurrentTrain: epoch  3, batch    23 | loss: 8.0302382CurrentTrain: epoch  3, batch    24 | loss: 7.5056448CurrentTrain: epoch  3, batch    25 | loss: 7.9807687CurrentTrain: epoch  3, batch    26 | loss: 6.7120490CurrentTrain: epoch  3, batch    27 | loss: 7.0686216CurrentTrain: epoch  3, batch    28 | loss: 7.5769758CurrentTrain: epoch  3, batch    29 | loss: 6.7545719CurrentTrain: epoch  3, batch    30 | loss: 7.1745100CurrentTrain: epoch  3, batch    31 | loss: 7.4284306CurrentTrain: epoch  3, batch    32 | loss: 6.4889078CurrentTrain: epoch  3, batch    33 | loss: 7.7870426CurrentTrain: epoch  3, batch    34 | loss: 7.3845553CurrentTrain: epoch  3, batch    35 | loss: 7.3553863CurrentTrain: epoch  3, batch    36 | loss: 6.6250801CurrentTrain: epoch  3, batch    37 | loss: 6.8752642CurrentTrain: epoch  4, batch     0 | loss: 6.7049918CurrentTrain: epoch  4, batch     1 | loss: 6.7096629CurrentTrain: epoch  4, batch     2 | loss: 6.2316742CurrentTrain: epoch  4, batch     3 | loss: 6.8788261CurrentTrain: epoch  4, batch     4 | loss: 6.5192709CurrentTrain: epoch  4, batch     5 | loss: 7.2779932CurrentTrain: epoch  4, batch     6 | loss: 6.3018460CurrentTrain: epoch  4, batch     7 | loss: 6.0218954CurrentTrain: epoch  4, batch     8 | loss: 6.3013496CurrentTrain: epoch  4, batch     9 | loss: 7.2872777CurrentTrain: epoch  4, batch    10 | loss: 6.8012052CurrentTrain: epoch  4, batch    11 | loss: 7.1997614CurrentTrain: epoch  4, batch    12 | loss: 7.3787956CurrentTrain: epoch  4, batch    13 | loss: 6.4523973CurrentTrain: epoch  4, batch    14 | loss: 6.8479471CurrentTrain: epoch  4, batch    15 | loss: 6.5111160CurrentTrain: epoch  4, batch    16 | loss: 7.0709171CurrentTrain: epoch  4, batch    17 | loss: 6.7261534CurrentTrain: epoch  4, batch    18 | loss: 7.2888384CurrentTrain: epoch  4, batch    19 | loss: 5.7546749CurrentTrain: epoch  4, batch    20 | loss: 6.8526325CurrentTrain: epoch  4, batch    21 | loss: 6.2062435CurrentTrain: epoch  4, batch    22 | loss: 6.2971554CurrentTrain: epoch  4, batch    23 | loss: 6.3825884CurrentTrain: epoch  4, batch    24 | loss: 6.4261360CurrentTrain: epoch  4, batch    25 | loss: 6.6075063CurrentTrain: epoch  4, batch    26 | loss: 6.6031895CurrentTrain: epoch  4, batch    27 | loss: 8.3527603CurrentTrain: epoch  4, batch    28 | loss: 6.7143145CurrentTrain: epoch  4, batch    29 | loss: 7.0675478CurrentTrain: epoch  4, batch    30 | loss: 7.7401962CurrentTrain: epoch  4, batch    31 | loss: 8.4190941CurrentTrain: epoch  4, batch    32 | loss: 6.2961307CurrentTrain: epoch  4, batch    33 | loss: 6.6738405CurrentTrain: epoch  4, batch    34 | loss: 6.2862415CurrentTrain: epoch  4, batch    35 | loss: 6.4676208CurrentTrain: epoch  4, batch    36 | loss: 7.0003977CurrentTrain: epoch  4, batch    37 | loss: 7.5558147CurrentTrain: epoch  5, batch     0 | loss: 6.8697801CurrentTrain: epoch  5, batch     1 | loss: 6.5756755CurrentTrain: epoch  5, batch     2 | loss: 6.9967818CurrentTrain: epoch  5, batch     3 | loss: 6.4911699CurrentTrain: epoch  5, batch     4 | loss: 6.0503387CurrentTrain: epoch  5, batch     5 | loss: 6.0791726CurrentTrain: epoch  5, batch     6 | loss: 6.0037646CurrentTrain: epoch  5, batch     7 | loss: 6.3744473CurrentTrain: epoch  5, batch     8 | loss: 6.5302916CurrentTrain: epoch  5, batch     9 | loss: 5.9141145CurrentTrain: epoch  5, batch    10 | loss: 6.0965590CurrentTrain: epoch  5, batch    11 | loss: 6.6549897CurrentTrain: epoch  5, batch    12 | loss: 6.4737020CurrentTrain: epoch  5, batch    13 | loss: 6.5070295CurrentTrain: epoch  5, batch    14 | loss: 5.7360353CurrentTrain: epoch  5, batch    15 | loss: 6.3969836CurrentTrain: epoch  5, batch    16 | loss: 6.3311944CurrentTrain: epoch  5, batch    17 | loss: 6.4010391CurrentTrain: epoch  5, batch    18 | loss: 6.4090757CurrentTrain: epoch  5, batch    19 | loss: 5.5298390CurrentTrain: epoch  5, batch    20 | loss: 5.9455323CurrentTrain: epoch  5, batch    21 | loss: 6.4539714CurrentTrain: epoch  5, batch    22 | loss: 7.0914760CurrentTrain: epoch  5, batch    23 | loss: 6.6915960CurrentTrain: epoch  5, batch    24 | loss: 5.9648933CurrentTrain: epoch  5, batch    25 | loss: 6.7548842CurrentTrain: epoch  5, batch    26 | loss: 7.0782681CurrentTrain: epoch  5, batch    27 | loss: 6.7541690CurrentTrain: epoch  5, batch    28 | loss: 6.2099690CurrentTrain: epoch  5, batch    29 | loss: 6.3215084CurrentTrain: epoch  5, batch    30 | loss: 6.0497799CurrentTrain: epoch  5, batch    31 | loss: 5.7578206CurrentTrain: epoch  5, batch    32 | loss: 5.7349949CurrentTrain: epoch  5, batch    33 | loss: 6.3304095CurrentTrain: epoch  5, batch    34 | loss: 6.8883877CurrentTrain: epoch  5, batch    35 | loss: 6.2734814CurrentTrain: epoch  5, batch    36 | loss: 6.0167351CurrentTrain: epoch  5, batch    37 | loss: 8.5839081CurrentTrain: epoch  6, batch     0 | loss: 6.0365896CurrentTrain: epoch  6, batch     1 | loss: 6.5424333CurrentTrain: epoch  6, batch     2 | loss: 6.0474634CurrentTrain: epoch  6, batch     3 | loss: 5.7768641CurrentTrain: epoch  6, batch     4 | loss: 5.9619288CurrentTrain: epoch  6, batch     5 | loss: 6.0361347CurrentTrain: epoch  6, batch     6 | loss: 6.0936379CurrentTrain: epoch  6, batch     7 | loss: 6.7680063CurrentTrain: epoch  6, batch     8 | loss: 5.5974178CurrentTrain: epoch  6, batch     9 | loss: 6.2406182CurrentTrain: epoch  6, batch    10 | loss: 6.1597233CurrentTrain: epoch  6, batch    11 | loss: 6.2074857CurrentTrain: epoch  6, batch    12 | loss: 6.6634998CurrentTrain: epoch  6, batch    13 | loss: 6.0229745CurrentTrain: epoch  6, batch    14 | loss: 5.5321269CurrentTrain: epoch  6, batch    15 | loss: 5.8461561CurrentTrain: epoch  6, batch    16 | loss: 6.3468542CurrentTrain: epoch  6, batch    17 | loss: 6.2976966CurrentTrain: epoch  6, batch    18 | loss: 6.6645942CurrentTrain: epoch  6, batch    19 | loss: 5.5984516CurrentTrain: epoch  6, batch    20 | loss: 5.7601242CurrentTrain: epoch  6, batch    21 | loss: 5.5703926CurrentTrain: epoch  6, batch    22 | loss: 5.6650472CurrentTrain: epoch  6, batch    23 | loss: 5.5658531CurrentTrain: epoch  6, batch    24 | loss: 6.1865578CurrentTrain: epoch  6, batch    25 | loss: 5.4650431CurrentTrain: epoch  6, batch    26 | loss: 5.8584518CurrentTrain: epoch  6, batch    27 | loss: 5.6642122CurrentTrain: epoch  6, batch    28 | loss: 5.5034890CurrentTrain: epoch  6, batch    29 | loss: 5.9545927CurrentTrain: epoch  6, batch    30 | loss: 6.1480012CurrentTrain: epoch  6, batch    31 | loss: 6.3878384CurrentTrain: epoch  6, batch    32 | loss: 5.4814830CurrentTrain: epoch  6, batch    33 | loss: 5.9017315CurrentTrain: epoch  6, batch    34 | loss: 5.9088507CurrentTrain: epoch  6, batch    35 | loss: 5.6708164CurrentTrain: epoch  6, batch    36 | loss: 5.8565130CurrentTrain: epoch  6, batch    37 | loss: 5.5485277CurrentTrain: epoch  7, batch     0 | loss: 5.5804658CurrentTrain: epoch  7, batch     1 | loss: 5.4221210CurrentTrain: epoch  7, batch     2 | loss: 5.9792967CurrentTrain: epoch  7, batch     3 | loss: 5.3908076CurrentTrain: epoch  7, batch     4 | loss: 5.4936619CurrentTrain: epoch  7, batch     5 | loss: 5.9240561CurrentTrain: epoch  7, batch     6 | loss: 5.4205084CurrentTrain: epoch  7, batch     7 | loss: 5.4123793CurrentTrain: epoch  7, batch     8 | loss: 5.4077930CurrentTrain: epoch  7, batch     9 | loss: 6.1311922CurrentTrain: epoch  7, batch    10 | loss: 5.5834017CurrentTrain: epoch  7, batch    11 | loss: 5.7498722CurrentTrain: epoch  7, batch    12 | loss: 5.9791675CurrentTrain: epoch  7, batch    13 | loss: 5.5426068CurrentTrain: epoch  7, batch    14 | loss: 5.5441294CurrentTrain: epoch  7, batch    15 | loss: 5.4176364CurrentTrain: epoch  7, batch    16 | loss: 5.6090350CurrentTrain: epoch  7, batch    17 | loss: 5.6009870CurrentTrain: epoch  7, batch    18 | loss: 5.3407784CurrentTrain: epoch  7, batch    19 | loss: 5.5268717CurrentTrain: epoch  7, batch    20 | loss: 5.1770296CurrentTrain: epoch  7, batch    21 | loss: 5.9698706CurrentTrain: epoch  7, batch    22 | loss: 5.6181574CurrentTrain: epoch  7, batch    23 | loss: 5.4295273CurrentTrain: epoch  7, batch    24 | loss: 5.4687572CurrentTrain: epoch  7, batch    25 | loss: 5.3774886CurrentTrain: epoch  7, batch    26 | loss: 5.6030126CurrentTrain: epoch  7, batch    27 | loss: 5.3958712CurrentTrain: epoch  7, batch    28 | loss: 5.0299678CurrentTrain: epoch  7, batch    29 | loss: 5.2520132CurrentTrain: epoch  7, batch    30 | loss: 5.3250465CurrentTrain: epoch  7, batch    31 | loss: 5.9175482CurrentTrain: epoch  7, batch    32 | loss: 5.0808220CurrentTrain: epoch  7, batch    33 | loss: 5.2479610CurrentTrain: epoch  7, batch    34 | loss: 5.6939373CurrentTrain: epoch  7, batch    35 | loss: 5.2330956CurrentTrain: epoch  7, batch    36 | loss: 5.6380520CurrentTrain: epoch  7, batch    37 | loss: 6.3600016CurrentTrain: epoch  8, batch     0 | loss: 5.6244617CurrentTrain: epoch  8, batch     1 | loss: 5.0475287CurrentTrain: epoch  8, batch     2 | loss: 5.4598341CurrentTrain: epoch  8, batch     3 | loss: 4.9738245CurrentTrain: epoch  8, batch     4 | loss: 5.3170986CurrentTrain: epoch  8, batch     5 | loss: 5.2479625CurrentTrain: epoch  8, batch     6 | loss: 5.3129039CurrentTrain: epoch  8, batch     7 | loss: 5.2339211CurrentTrain: epoch  8, batch     8 | loss: 5.4001369CurrentTrain: epoch  8, batch     9 | loss: 5.7280655CurrentTrain: epoch  8, batch    10 | loss: 5.2007914CurrentTrain: epoch  8, batch    11 | loss: 4.9846783CurrentTrain: epoch  8, batch    12 | loss: 5.5163822CurrentTrain: epoch  8, batch    13 | loss: 5.0911679CurrentTrain: epoch  8, batch    14 | loss: 5.1796570CurrentTrain: epoch  8, batch    15 | loss: 5.2447152CurrentTrain: epoch  8, batch    16 | loss: 5.3742518CurrentTrain: epoch  8, batch    17 | loss: 5.6543841CurrentTrain: epoch  8, batch    18 | loss: 5.4089317CurrentTrain: epoch  8, batch    19 | loss: 5.4008570CurrentTrain: epoch  8, batch    20 | loss: 5.1970468CurrentTrain: epoch  8, batch    21 | loss: 5.3839321CurrentTrain: epoch  8, batch    22 | loss: 5.3724465CurrentTrain: epoch  8, batch    23 | loss: 5.0399628CurrentTrain: epoch  8, batch    24 | loss: 5.0648866CurrentTrain: epoch  8, batch    25 | loss: 5.0484104CurrentTrain: epoch  8, batch    26 | loss: 5.1228151CurrentTrain: epoch  8, batch    27 | loss: 4.8437967CurrentTrain: epoch  8, batch    28 | loss: 4.9819889CurrentTrain: epoch  8, batch    29 | loss: 4.9861851CurrentTrain: epoch  8, batch    30 | loss: 5.0186868CurrentTrain: epoch  8, batch    31 | loss: 5.1669369CurrentTrain: epoch  8, batch    32 | loss: 4.9863219CurrentTrain: epoch  8, batch    33 | loss: 5.0031767CurrentTrain: epoch  8, batch    34 | loss: 5.0392599CurrentTrain: epoch  8, batch    35 | loss: 5.4578290CurrentTrain: epoch  8, batch    36 | loss: 5.0680218CurrentTrain: epoch  8, batch    37 | loss: 4.9787922CurrentTrain: epoch  9, batch     0 | loss: 5.6911354CurrentTrain: epoch  9, batch     1 | loss: 5.2463584CurrentTrain: epoch  9, batch     2 | loss: 5.1276684CurrentTrain: epoch  9, batch     3 | loss: 4.9697800CurrentTrain: epoch  9, batch     4 | loss: 5.0071135CurrentTrain: epoch  9, batch     5 | loss: 5.0913382CurrentTrain: epoch  9, batch     6 | loss: 5.0674639CurrentTrain: epoch  9, batch     7 | loss: 5.2182121CurrentTrain: epoch  9, batch     8 | loss: 4.9882584CurrentTrain: epoch  9, batch     9 | loss: 5.5421848CurrentTrain: epoch  9, batch    10 | loss: 5.0677752CurrentTrain: epoch  9, batch    11 | loss: 5.3117056CurrentTrain: epoch  9, batch    12 | loss: 5.0488615CurrentTrain: epoch  9, batch    13 | loss: 5.0155153CurrentTrain: epoch  9, batch    14 | loss: 4.9400263CurrentTrain: epoch  9, batch    15 | loss: 4.9852557CurrentTrain: epoch  9, batch    16 | loss: 5.0598831CurrentTrain: epoch  9, batch    17 | loss: 5.1293783CurrentTrain: epoch  9, batch    18 | loss: 5.2771187CurrentTrain: epoch  9, batch    19 | loss: 5.1826024CurrentTrain: epoch  9, batch    20 | loss: 4.8732109CurrentTrain: epoch  9, batch    21 | loss: 5.1231995CurrentTrain: epoch  9, batch    22 | loss: 4.8668795CurrentTrain: epoch  9, batch    23 | loss: 5.1977339CurrentTrain: epoch  9, batch    24 | loss: 5.1904097CurrentTrain: epoch  9, batch    25 | loss: 5.1284537CurrentTrain: epoch  9, batch    26 | loss: 5.3275642CurrentTrain: epoch  9, batch    27 | loss: 4.9902925CurrentTrain: epoch  9, batch    28 | loss: 5.0949044CurrentTrain: epoch  9, batch    29 | loss: 5.2333069CurrentTrain: epoch  9, batch    30 | loss: 5.1892390CurrentTrain: epoch  9, batch    31 | loss: 5.1937723CurrentTrain: epoch  9, batch    32 | loss: 5.0479898CurrentTrain: epoch  9, batch    33 | loss: 5.0242972CurrentTrain: epoch  9, batch    34 | loss: 4.8979931CurrentTrain: epoch  9, batch    35 | loss: 5.0008173CurrentTrain: epoch  9, batch    36 | loss: 5.0401039CurrentTrain: epoch  9, batch    37 | loss: 4.7652597
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots and family.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, David Chen moved to Canada to enjoy a quieter lifestyle and to be closer to his family who had settled there years ago.  
Head Entity: David Chen  
Tail Entity: Canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: as for any gay or lesbian people who enlisted with the military this week , `` they may be told they can not join '' if the pentagon decides to enforce the law , said aubrey sarvis , executive director of the servicemembers legal defense network , a group representing service members affected by the law .
Head Entity: servicemembers legal defense network
Tail Entity: aubrey sarvis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: The board of directors at Tech Innovations Inc. recently appointed Sarah Johnson as the new Chief Technology Officer, highlighting her extensive experience in the tech industry and her leadership skills.  
Head Entity: Tech Innovations Inc.  
Tail Entity: Sarah Johnson  

Relation: organization top members employees  
Context: During the annual meeting, the CEO of Green Earth Solutions, Mark Thompson, announced the promotion of Lisa Chen to the position of Chief Financial Officer, recognizing her contributions to the company's growth.  
Head Entity: Green Earth Solutions  
Tail Entity: Lisa Chen  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: since the couple 's divorce , aniston has been romantically linked with actor vince vaughn , british model paul sculfor , actor orlando bloom and singer john mayer .
Head Entity: paul sculfor
Tail Entity: british
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The renowned physicist Albert Einstein was born in Germany and later became a Swiss citizen, reflecting his deep ties to both countries.  
Head Entity: Albert Einstein  
Tail Entity: Germany  

Relation: person origin  
Context: The famous author Chimamanda Ngozi Adichie often speaks about her Nigerian heritage and how it influences her writing.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigeria  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: marcel ospel , the bank 's former chairman who stepped down earlier this year amid heavy criticism of ubs ' staggering losses on its us subprime home loan exposure , accounted for more than two thirds of the sum , pr agency balanx said in a statement tuesday .
Head Entity: marcel ospel
Tail Entity: chairman
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: after years of dedicated service, dr. elena rodriguez was honored as the chief medical officer of the hospital, a role she embraced with great enthusiasm and commitment to patient care.  
Head Entity: dr. elena rodriguez  
Tail Entity: chief medical officer  

Relation: person title  
Context: during the annual awards ceremony, john smith was recognized as the lead software engineer for his innovative contributions to the company's flagship product.  
Head Entity: john smith  
Tail Entity: lead software engineer  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: chunghwa telecom , taiwan 's largest telecommunications company , will invest nt$ 13 billion -lrb- us$ 403.76 million -rrb- this year to set up four major cloud computing centers in what the company hopes will be the largest data hub in asia , chunghwa telecom 's chairman said thursday .
Head Entity: chunghwa telecom
Tail Entity: taiwan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: samsung electronics , a global leader in technology and innovation , is headquartered in south korea and has a significant presence in various markets around the world.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization country of headquarters  
Context: the multinational corporation nestlé , known for its food and beverage products, operates its main offices in switzerland, where it was founded over 150 years ago.  
Head Entity: nestlé  
Tail Entity: switzerland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.34%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.83%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.08%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.81%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.34%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.83%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.08%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
cur_acc:  ['0.8561']
his_acc:  ['0.8561']
CurrentTrain: epoch  0, batch     0 | loss: 6.2468119CurrentTrain: epoch  0, batch     1 | loss: 5.7896433CurrentTrain: epoch  1, batch     0 | loss: 5.4028606CurrentTrain: epoch  1, batch     1 | loss: 5.4053812CurrentTrain: epoch  2, batch     0 | loss: 5.2430296CurrentTrain: epoch  2, batch     1 | loss: 4.0947914CurrentTrain: epoch  3, batch     0 | loss: 4.6794367CurrentTrain: epoch  3, batch     1 | loss: 3.8554599CurrentTrain: epoch  4, batch     0 | loss: 4.0086150CurrentTrain: epoch  4, batch     1 | loss: 3.5112851CurrentTrain: epoch  5, batch     0 | loss: 4.0429306CurrentTrain: epoch  5, batch     1 | loss: 2.7895310CurrentTrain: epoch  6, batch     0 | loss: 3.1977797CurrentTrain: epoch  6, batch     1 | loss: 3.9237065CurrentTrain: epoch  7, batch     0 | loss: 3.4769330CurrentTrain: epoch  7, batch     1 | loss: 2.9692090CurrentTrain: epoch  8, batch     0 | loss: 3.1516812CurrentTrain: epoch  8, batch     1 | loss: 3.2484319CurrentTrain: epoch  9, batch     0 | loss: 2.6915393CurrentTrain: epoch  9, batch     1 | loss: 3.9552786
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun kapoor has always been proud of his indian heritage and culture.  
Head Entity: arjun kapoor  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she revealed that although she now resides in london, her roots trace back to the picturesque landscapes of new zealand where she was born.  
Head Entity: she  
Tail Entity: new zealand  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.nike.com for the latest sports gear and apparel.  
Head Entity: Nike  
Tail Entity: https://www.nike.com  

Relation: organization website  
Context: For more information about their services, check out http://www.tesla.com.  
Head Entity: Tesla  
Tail Entity: http://www.tesla.com  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has recently acquired a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple inc.  

Relation: organization shareholders  
Context: the investment firm blackrock has increased its holdings in the renewable energy company sunrun.  
Head Entity: sunrun  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in the spring of 2019, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring of 2019  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak started apple inc., which revolutionized the personal computer industry and changed the way people interact with technology.  
Head Entity: apple inc.  
Tail Entity: steve jobs  

Relation: organization founded by  
Context: the famous fashion brand gucci was established in 1921 by guccio gucci in florence, italy, and has since become a symbol of luxury and style.  
Head Entity: gucci  
Tail Entity: guccio gucci  
Mixup data size:  103
MixupTrain:  epoch  0, batch     0 | loss: 12.3093659MixupTrain:  epoch  0, batch     1 | loss: 12.0540075MixupTrain:  epoch  0, batch     2 | loss: 12.6704188MixupTrain:  epoch  0, batch     3 | loss: 11.5790858MixupTrain:  epoch  0, batch     4 | loss: 12.3145949MixupTrain:  epoch  0, batch     5 | loss: 11.6476191MixupTrain:  epoch  0, batch     6 | loss: 9.5898910
MemoryTrain:  epoch  0, batch     0 | loss: 5.9875240MemoryTrain:  epoch  0, batch     1 | loss: 6.2081609MemoryTrain:  epoch  0, batch     2 | loss: 6.4431057MemoryTrain:  epoch  1, batch     0 | loss: 6.1101332MemoryTrain:  epoch  1, batch     1 | loss: 5.2845306MemoryTrain:  epoch  1, batch     2 | loss: 3.1772521MemoryTrain:  epoch  2, batch     0 | loss: 4.9315329MemoryTrain:  epoch  2, batch     1 | loss: 5.2893543MemoryTrain:  epoch  2, batch     2 | loss: 1.4186206MemoryTrain:  epoch  3, batch     0 | loss: 4.1558938MemoryTrain:  epoch  3, batch     1 | loss: 5.9028978MemoryTrain:  epoch  3, batch     2 | loss: 1.1794628MemoryTrain:  epoch  4, batch     0 | loss: 4.8617477MemoryTrain:  epoch  4, batch     1 | loss: 4.3834095MemoryTrain:  epoch  4, batch     2 | loss: 3.8516150MemoryTrain:  epoch  5, batch     0 | loss: 4.4302540MemoryTrain:  epoch  5, batch     1 | loss: 4.4060907MemoryTrain:  epoch  5, batch     2 | loss: 1.2131591MemoryTrain:  epoch  6, batch     0 | loss: 4.1320205MemoryTrain:  epoch  6, batch     1 | loss: 4.1249990MemoryTrain:  epoch  6, batch     2 | loss: 1.6034012MemoryTrain:  epoch  7, batch     0 | loss: 4.0585294MemoryTrain:  epoch  7, batch     1 | loss: 3.9471407MemoryTrain:  epoch  7, batch     2 | loss: 6.2549291MemoryTrain:  epoch  8, batch     0 | loss: 4.8548489MemoryTrain:  epoch  8, batch     1 | loss: 3.6104875MemoryTrain:  epoch  8, batch     2 | loss: 1.6335253MemoryTrain:  epoch  9, batch     0 | loss: 3.7020335MemoryTrain:  epoch  9, batch     1 | loss: 4.1444602MemoryTrain:  epoch  9, batch     2 | loss: 5.3257704
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 0.00%,  total acc: 35.94%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 68.01%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 67.76%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 78.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 79.55%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 80.15%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 78.93%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 77.95%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 76.18%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 74.67%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 73.72%   [EVAL] batch:   39 | acc: 0.00%,  total acc: 71.88%   
cur_acc:  ['0.8561', '0.3594']
his_acc:  ['0.8561', '0.7188']
CurrentTrain: epoch  0, batch     0 | loss: 5.2733002CurrentTrain: epoch  0, batch     1 | loss: 5.6686573CurrentTrain: epoch  1, batch     0 | loss: 4.0771236CurrentTrain: epoch  1, batch     1 | loss: 4.4728842CurrentTrain: epoch  2, batch     0 | loss: 4.0504179CurrentTrain: epoch  2, batch     1 | loss: 3.8737988CurrentTrain: epoch  3, batch     0 | loss: 3.8213315CurrentTrain: epoch  3, batch     1 | loss: 3.7189624CurrentTrain: epoch  4, batch     0 | loss: 3.3704922CurrentTrain: epoch  4, batch     1 | loss: 2.8980951CurrentTrain: epoch  5, batch     0 | loss: 2.9821022CurrentTrain: epoch  5, batch     1 | loss: 3.0331054CurrentTrain: epoch  6, batch     0 | loss: 2.9894633CurrentTrain: epoch  6, batch     1 | loss: 2.9368727CurrentTrain: epoch  7, batch     0 | loss: 2.4530277CurrentTrain: epoch  7, batch     1 | loss: 2.8197362CurrentTrain: epoch  8, batch     0 | loss: 2.7527089CurrentTrain: epoch  8, batch     1 | loss: 2.7722745CurrentTrain: epoch  9, batch     0 | loss: 2.6748352CurrentTrain: epoch  9, batch     1 | loss: 2.5219929
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization Green Earth Initiative was launched in 2010 to promote environmental awareness and sustainability practices across communities.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john is celebrating his 30th birthday today.  
Head Entity: john  
Tail Entity: 30  

Relation: person age  
Context: the famous actor turned 45 last week.  
Head Entity: the famous actor  
Tail Entity: 45  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in barcelona and spent her childhood there before moving to madrid.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during the summer of 1985, michael was born in new orleans, a city known for its vibrant culture.  
Head Entity: michael  
Tail Entity: new orleans  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians as members, including the famous conductor Leonard Bernstein.  
Head Entity: New York Philharmonic  
Tail Entity: Leonard Bernstein  

Relation: organization members  
Context: The National Geographic Society has a long history of exploration and education, with many prominent explorers and scientists, such as Jane Goodall, serving as members.  
Head Entity: National Geographic Society  
Tail Entity: Jane Goodall  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of study and reflection, Maria decided to embrace Buddhism, finding peace and purpose in its teachings.  
Head Entity: Maria  
Tail Entity: Buddhism  

Relation: person religion  
Context: During the festival, Ahmed proudly wore his traditional attire, celebrating his deep connection to Islam and its rich cultural heritage.  
Head Entity: Ahmed  
Tail Entity: Islam  
Mixup data size:  134
MixupTrain:  epoch  0, batch     0 | loss: 8.3747983MixupTrain:  epoch  0, batch     1 | loss: 8.9836106MixupTrain:  epoch  0, batch     2 | loss: 8.0295351MixupTrain:  epoch  0, batch     3 | loss: 8.0732889MixupTrain:  epoch  0, batch     4 | loss: 7.5708461MixupTrain:  epoch  0, batch     5 | loss: 8.1030607MixupTrain:  epoch  0, batch     6 | loss: 8.6450047MixupTrain:  epoch  0, batch     7 | loss: 9.0784186MixupTrain:  epoch  0, batch     8 | loss: 8.7516234
MemoryTrain:  epoch  0, batch     0 | loss: 4.2080140MemoryTrain:  epoch  0, batch     1 | loss: 5.8571758MemoryTrain:  epoch  0, batch     2 | loss: 4.0052786MemoryTrain:  epoch  1, batch     0 | loss: 4.9606647MemoryTrain:  epoch  1, batch     1 | loss: 4.3006968MemoryTrain:  epoch  1, batch     2 | loss: 4.6035404MemoryTrain:  epoch  2, batch     0 | loss: 4.7651596MemoryTrain:  epoch  2, batch     1 | loss: 3.3625617MemoryTrain:  epoch  2, batch     2 | loss: 4.6484046MemoryTrain:  epoch  3, batch     0 | loss: 4.3689480MemoryTrain:  epoch  3, batch     1 | loss: 3.4705138MemoryTrain:  epoch  3, batch     2 | loss: 3.6599836MemoryTrain:  epoch  4, batch     0 | loss: 3.3938751MemoryTrain:  epoch  4, batch     1 | loss: 3.5787766MemoryTrain:  epoch  4, batch     2 | loss: 3.2559156MemoryTrain:  epoch  5, batch     0 | loss: 2.9644837MemoryTrain:  epoch  5, batch     1 | loss: 2.8009973MemoryTrain:  epoch  5, batch     2 | loss: 3.7140117MemoryTrain:  epoch  6, batch     0 | loss: 2.8386662MemoryTrain:  epoch  6, batch     1 | loss: 2.8636415MemoryTrain:  epoch  6, batch     2 | loss: 2.7960162MemoryTrain:  epoch  7, batch     0 | loss: 1.9707408MemoryTrain:  epoch  7, batch     1 | loss: 2.4681656MemoryTrain:  epoch  7, batch     2 | loss: 3.9323525MemoryTrain:  epoch  8, batch     0 | loss: 2.5447426MemoryTrain:  epoch  8, batch     1 | loss: 2.5825462MemoryTrain:  epoch  8, batch     2 | loss: 2.7417865MemoryTrain:  epoch  9, batch     0 | loss: 2.4654112MemoryTrain:  epoch  9, batch     1 | loss: 2.2931221MemoryTrain:  epoch  9, batch     2 | loss: 2.4679508
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.14%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 81.70%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 64.80%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 64.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.15%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 75.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 76.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 77.39%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 76.79%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 75.35%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 73.48%   [EVAL] batch:   37 | acc: 18.75%,  total acc: 72.04%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 70.19%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 69.69%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 69.82%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 73.34%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 71.88%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 71.69%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 72.24%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 71.88%   
cur_acc:  ['0.8561', '0.3594', '0.8170']
his_acc:  ['0.8561', '0.7188', '0.7188']
CurrentTrain: epoch  0, batch     0 | loss: 5.7890773CurrentTrain: epoch  0, batch     1 | loss: 6.4446068CurrentTrain: epoch  1, batch     0 | loss: 5.2239256CurrentTrain: epoch  1, batch     1 | loss: 5.0471940CurrentTrain: epoch  2, batch     0 | loss: 4.6040030CurrentTrain: epoch  2, batch     1 | loss: 5.1854496CurrentTrain: epoch  3, batch     0 | loss: 4.3913560CurrentTrain: epoch  3, batch     1 | loss: 4.0520105CurrentTrain: epoch  4, batch     0 | loss: 4.0267015CurrentTrain: epoch  4, batch     1 | loss: 4.2178235CurrentTrain: epoch  5, batch     0 | loss: 3.9569464CurrentTrain: epoch  5, batch     1 | loss: 3.7676697CurrentTrain: epoch  6, batch     0 | loss: 3.7911949CurrentTrain: epoch  6, batch     1 | loss: 3.1440537CurrentTrain: epoch  7, batch     0 | loss: 3.5234742CurrentTrain: epoch  7, batch     1 | loss: 2.8234236CurrentTrain: epoch  8, batch     0 | loss: 3.2356751CurrentTrain: epoch  8, batch     1 | loss: 3.1587813CurrentTrain: epoch  9, batch     0 | loss: 3.0145397CurrentTrain: epoch  9, batch     1 | loss: 2.9075165
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: after years of living in new york city, the famous author decided to return to her hometown of boston, where she feels most at home.  
Head Entity: she  
Tail Entity: boston  

Relation: person cities of residence  
Context: despite being born in san francisco, the tech entrepreneur has spent most of his adult life in silicon valley, where he built his successful startup.  
Head Entity: he  
Tail Entity: silicon valley  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, he enrolled at Stanford University to study computer science.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: after a lengthy investigation, the authorities announced that johnson was charged with embezzlement, a serious offense that could result in significant prison time.  
Head Entity: johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: the district attorney confirmed that smith was charged with assault following the altercation that took place outside the nightclub last weekend.  
Head Entity: smith  
Tail Entity: assault  
Mixup data size:  164
MixupTrain:  epoch  0, batch     0 | loss: 7.7596462MixupTrain:  epoch  0, batch     1 | loss: 6.2172581MixupTrain:  epoch  0, batch     2 | loss: 6.1831836MixupTrain:  epoch  0, batch     3 | loss: 6.1581153MixupTrain:  epoch  0, batch     4 | loss: 7.1117550MixupTrain:  epoch  0, batch     5 | loss: 5.8934688MixupTrain:  epoch  0, batch     6 | loss: 6.1482012MixupTrain:  epoch  0, batch     7 | loss: 6.2143782MixupTrain:  epoch  0, batch     8 | loss: 6.8422676MixupTrain:  epoch  0, batch     9 | loss: 6.1426204MixupTrain:  epoch  0, batch    10 | loss: 7.6438479
MemoryTrain:  epoch  0, batch     0 | loss: 3.0031817MemoryTrain:  epoch  0, batch     1 | loss: 2.8367186MemoryTrain:  epoch  0, batch     2 | loss: 4.2368059MemoryTrain:  epoch  0, batch     3 | loss: 2.8949399MemoryTrain:  epoch  1, batch     0 | loss: 3.1709137MemoryTrain:  epoch  1, batch     1 | loss: 3.3046033MemoryTrain:  epoch  1, batch     2 | loss: 2.6333961MemoryTrain:  epoch  1, batch     3 | loss: 3.2263148MemoryTrain:  epoch  2, batch     0 | loss: 2.4740851MemoryTrain:  epoch  2, batch     1 | loss: 3.0667903MemoryTrain:  epoch  2, batch     2 | loss: 2.7429597MemoryTrain:  epoch  2, batch     3 | loss: 2.7775006MemoryTrain:  epoch  3, batch     0 | loss: 2.7563558MemoryTrain:  epoch  3, batch     1 | loss: 1.8196138MemoryTrain:  epoch  3, batch     2 | loss: 3.1499891MemoryTrain:  epoch  3, batch     3 | loss: 2.7164032MemoryTrain:  epoch  4, batch     0 | loss: 2.6864147MemoryTrain:  epoch  4, batch     1 | loss: 2.8325839MemoryTrain:  epoch  4, batch     2 | loss: 1.9688523MemoryTrain:  epoch  4, batch     3 | loss: 2.1993084MemoryTrain:  epoch  5, batch     0 | loss: 2.1047366MemoryTrain:  epoch  5, batch     1 | loss: 2.3796740MemoryTrain:  epoch  5, batch     2 | loss: 2.2281961MemoryTrain:  epoch  5, batch     3 | loss: 2.1908674MemoryTrain:  epoch  6, batch     0 | loss: 1.8277918MemoryTrain:  epoch  6, batch     1 | loss: 2.3656626MemoryTrain:  epoch  6, batch     2 | loss: 1.9050840MemoryTrain:  epoch  6, batch     3 | loss: 2.1322908MemoryTrain:  epoch  7, batch     0 | loss: 1.8582282MemoryTrain:  epoch  7, batch     1 | loss: 2.0795588MemoryTrain:  epoch  7, batch     2 | loss: 1.7774543MemoryTrain:  epoch  7, batch     3 | loss: 2.0799005MemoryTrain:  epoch  8, batch     0 | loss: 1.8804014MemoryTrain:  epoch  8, batch     1 | loss: 1.7622530MemoryTrain:  epoch  8, batch     2 | loss: 1.9850056MemoryTrain:  epoch  8, batch     3 | loss: 2.0745556MemoryTrain:  epoch  9, batch     0 | loss: 1.8233168MemoryTrain:  epoch  9, batch     1 | loss: 1.5877103MemoryTrain:  epoch  9, batch     2 | loss: 2.0319917MemoryTrain:  epoch  9, batch     3 | loss: 1.9557405
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 84.38%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 51.04%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 67.41%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 55.21%   [EVAL] batch:   18 | acc: 18.75%,  total acc: 53.29%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 51.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 54.17%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 58.15%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 59.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 61.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 62.74%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 63.89%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 66.38%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 67.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 69.32%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 66.89%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 65.46%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 63.78%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 63.44%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 64.18%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 64.88%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 68.24%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 66.88%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 66.79%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 69.08%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 69.19%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 69.40%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 69.49%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 69.76%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 70.41%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 70.77%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 71.64%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 72.06%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 72.86%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 72.54%   
cur_acc:  ['0.8561', '0.3594', '0.8170', '0.8438']
his_acc:  ['0.8561', '0.7188', '0.7188', '0.7254']
CurrentTrain: epoch  0, batch     0 | loss: 4.9407368CurrentTrain: epoch  0, batch     1 | loss: 5.1581016CurrentTrain: epoch  1, batch     0 | loss: 3.7770431CurrentTrain: epoch  1, batch     1 | loss: 3.3828714CurrentTrain: epoch  2, batch     0 | loss: 3.3326473CurrentTrain: epoch  2, batch     1 | loss: 3.3196971CurrentTrain: epoch  3, batch     0 | loss: 2.9469180CurrentTrain: epoch  3, batch     1 | loss: 3.0460961CurrentTrain: epoch  4, batch     0 | loss: 2.7081294CurrentTrain: epoch  4, batch     1 | loss: 2.5824368CurrentTrain: epoch  5, batch     0 | loss: 2.3317690CurrentTrain: epoch  5, batch     1 | loss: 2.6415226CurrentTrain: epoch  6, batch     0 | loss: 2.2583745CurrentTrain: epoch  6, batch     1 | loss: 2.2413714CurrentTrain: epoch  7, batch     0 | loss: 2.1522357CurrentTrain: epoch  7, batch     1 | loss: 2.7266097CurrentTrain: epoch  8, batch     0 | loss: 2.3405716CurrentTrain: epoch  8, batch     1 | loss: 2.2247822CurrentTrain: epoch  9, batch     0 | loss: 2.0112019CurrentTrain: epoch  9, batch     1 | loss: 2.0383673
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: pamela gardner ahearn , who served nine years as chief of protocol at the us house of representatives after earlier experience with the state department 's office of protocol and as elizabeth taylor 's executive assistant , died march 26 of a heart attack at her home in alexandria , va .
Head Entity: pamela gardner ahearn
Tail Entity: heart attack
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: john smith, a renowned author known for his thrilling novels, passed away on january 15 due to complications from pneumonia while receiving treatment in a local hospital.  
Head Entity: john smith  
Tail Entity: pneumonia  

Relation: person cause of death  
Context: the famous musician, elena rodriguez, tragically lost her life in a car accident on july 4, leaving behind a legacy of unforgettable songs.  
Head Entity: elena rodriguez  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political landscape of the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Michael, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person other family  
Context: In her memoir, Angela described the close bond she shared with her aunt, who played a significant role in her upbringing.  
Head Entity: Angela  
Tail Entity: her aunt  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  195
MixupTrain:  epoch  0, batch     0 | loss: 5.4670413MixupTrain:  epoch  0, batch     1 | loss: 5.2840829MixupTrain:  epoch  0, batch     2 | loss: 5.7815887MixupTrain:  epoch  0, batch     3 | loss: 6.7986909MixupTrain:  epoch  0, batch     4 | loss: 6.4410913MixupTrain:  epoch  0, batch     5 | loss: 5.8212907MixupTrain:  epoch  0, batch     6 | loss: 5.0320427MixupTrain:  epoch  0, batch     7 | loss: 5.2161182MixupTrain:  epoch  0, batch     8 | loss: 6.1175245MixupTrain:  epoch  0, batch     9 | loss: 5.8912662MixupTrain:  epoch  0, batch    10 | loss: 5.4026772MixupTrain:  epoch  0, batch    11 | loss: 5.9048324MixupTrain:  epoch  0, batch    12 | loss: 6.3005409
MemoryTrain:  epoch  0, batch     0 | loss: 2.8446131MemoryTrain:  epoch  0, batch     1 | loss: 3.0053294MemoryTrain:  epoch  0, batch     2 | loss: 3.7379823MemoryTrain:  epoch  0, batch     3 | loss: 3.1904128MemoryTrain:  epoch  0, batch     4 | loss: 3.2340238MemoryTrain:  epoch  1, batch     0 | loss: 2.5161102MemoryTrain:  epoch  1, batch     1 | loss: 2.8695712MemoryTrain:  epoch  1, batch     2 | loss: 2.1428664MemoryTrain:  epoch  1, batch     3 | loss: 2.6857030MemoryTrain:  epoch  1, batch     4 | loss: 3.7547426MemoryTrain:  epoch  2, batch     0 | loss: 3.1355324MemoryTrain:  epoch  2, batch     1 | loss: 1.9442053MemoryTrain:  epoch  2, batch     2 | loss: 2.1779988MemoryTrain:  epoch  2, batch     3 | loss: 2.8784068MemoryTrain:  epoch  2, batch     4 | loss: 2.7111261MemoryTrain:  epoch  3, batch     0 | loss: 2.6756783MemoryTrain:  epoch  3, batch     1 | loss: 2.0052845MemoryTrain:  epoch  3, batch     2 | loss: 2.9923263MemoryTrain:  epoch  3, batch     3 | loss: 2.3414307MemoryTrain:  epoch  3, batch     4 | loss: 2.4385729MemoryTrain:  epoch  4, batch     0 | loss: 2.4150691MemoryTrain:  epoch  4, batch     1 | loss: 2.4856153MemoryTrain:  epoch  4, batch     2 | loss: 2.6524205MemoryTrain:  epoch  4, batch     3 | loss: 2.1431994MemoryTrain:  epoch  4, batch     4 | loss: 2.1221466MemoryTrain:  epoch  5, batch     0 | loss: 2.2876096MemoryTrain:  epoch  5, batch     1 | loss: 1.6595192MemoryTrain:  epoch  5, batch     2 | loss: 1.9685742MemoryTrain:  epoch  5, batch     3 | loss: 2.1137662MemoryTrain:  epoch  5, batch     4 | loss: 2.5404432MemoryTrain:  epoch  6, batch     0 | loss: 2.0874915MemoryTrain:  epoch  6, batch     1 | loss: 1.5652518MemoryTrain:  epoch  6, batch     2 | loss: 2.2430775MemoryTrain:  epoch  6, batch     3 | loss: 1.9100550MemoryTrain:  epoch  6, batch     4 | loss: 2.2366607MemoryTrain:  epoch  7, batch     0 | loss: 1.5412879MemoryTrain:  epoch  7, batch     1 | loss: 1.9569889MemoryTrain:  epoch  7, batch     2 | loss: 1.9547154MemoryTrain:  epoch  7, batch     3 | loss: 1.9714155MemoryTrain:  epoch  7, batch     4 | loss: 1.6162225MemoryTrain:  epoch  8, batch     0 | loss: 1.5847083MemoryTrain:  epoch  8, batch     1 | loss: 1.5330675MemoryTrain:  epoch  8, batch     2 | loss: 1.6634684MemoryTrain:  epoch  8, batch     3 | loss: 2.3778696MemoryTrain:  epoch  8, batch     4 | loss: 2.3839045MemoryTrain:  epoch  9, batch     0 | loss: 1.6870915MemoryTrain:  epoch  9, batch     1 | loss: 1.9936520MemoryTrain:  epoch  9, batch     2 | loss: 1.9822958MemoryTrain:  epoch  9, batch     3 | loss: 1.5505741MemoryTrain:  epoch  9, batch     4 | loss: 1.6252141
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 38.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 35.71%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 40.28%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 43.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 46.59%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 49.48%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 49.52%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 75.48%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 67.50%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 61.03%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 57.99%   [EVAL] batch:   18 | acc: 18.75%,  total acc: 55.92%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 54.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.25%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 58.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.05%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.51%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.89%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 69.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 70.71%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 69.44%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 67.57%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 66.12%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 64.42%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 64.06%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 64.79%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 65.48%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.78%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 69.15%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 67.38%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 67.28%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 67.94%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 66.82%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 66.07%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 65.24%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 64.44%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 63.45%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 63.23%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 63.83%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 63.31%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 63.19%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 63.18%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 63.56%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 64.11%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 64.65%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 65.17%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 65.71%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 65.50%   [EVAL] batch:   73 | acc: 31.25%,  total acc: 65.03%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 64.67%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 64.39%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 63.96%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 63.70%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 63.69%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 63.44%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 63.58%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 63.87%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 64.08%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 63.39%   
cur_acc:  ['0.8561', '0.3594', '0.8170', '0.8438', '0.4952']
his_acc:  ['0.8561', '0.7188', '0.7188', '0.7254', '0.6339']
CurrentTrain: epoch  0, batch     0 | loss: 5.7704439CurrentTrain: epoch  0, batch     1 | loss: 5.9032569CurrentTrain: epoch  1, batch     0 | loss: 4.7380199CurrentTrain: epoch  1, batch     1 | loss: 4.3599405CurrentTrain: epoch  2, batch     0 | loss: 4.3899941CurrentTrain: epoch  2, batch     1 | loss: 4.1830034CurrentTrain: epoch  3, batch     0 | loss: 4.0355892CurrentTrain: epoch  3, batch     1 | loss: 3.2674532CurrentTrain: epoch  4, batch     0 | loss: 3.7281075CurrentTrain: epoch  4, batch     1 | loss: 3.0997257CurrentTrain: epoch  5, batch     0 | loss: 2.9714649CurrentTrain: epoch  5, batch     1 | loss: 3.0125263CurrentTrain: epoch  6, batch     0 | loss: 2.8792677CurrentTrain: epoch  6, batch     1 | loss: 2.9417591CurrentTrain: epoch  7, batch     0 | loss: 2.5999866CurrentTrain: epoch  7, batch     1 | loss: 2.7182987CurrentTrain: epoch  8, batch     0 | loss: 2.4518454CurrentTrain: epoch  8, batch     1 | loss: 2.2696810CurrentTrain: epoch  9, batch     0 | loss: 2.5402708CurrentTrain: epoch  9, batch     1 | loss: 2.1816478
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire, on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, to parents of polish descent.  
Head Entity: martha stewart  
Tail Entity: new jersey  

Relation: person stateorprovince of birth  
Context: barack obama was born on august 4, 1961, in honolulu, hawaii, where he spent most of his childhood.  
Head Entity: barack obama  
Tail Entity: hawaii  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: After the ceremony, Emily's father, John Smith, shared heartfelt stories about her childhood, bringing tears to many eyes.  
Head Entity: Emily  
Tail Entity: John Smith  

Relation: person parents  
Context: During the family reunion, Michael's mother, Sarah Johnson, prepared her famous lasagna, which everyone eagerly anticipated.  
Head Entity: Michael  
Tail Entity: Sarah Johnson  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: in 1997 , julian bond , a professor at american university and the university of virginia , said that while derogatory , the n-word is also taken as a term of affection among some black americans .
Head Entity: julian bond
Tail Entity: american university
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After completing her degree, Maria Gonzalez joined the marketing team at Tech Innovations, where she quickly made a name for herself.  
Head Entity: Maria Gonzalez  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: In 2020, David Lee was recognized for his outstanding contributions while working as a software engineer at Global Solutions Inc.  
Head Entity: David Lee  
Tail Entity: Global Solutions Inc.  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california last night.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, succumbed to her health issues and died in a hospital in nevada.  
Head Entity: elizabeth taylor  
Tail Entity: nevada  
Mixup data size:  225
MixupTrain:  epoch  0, batch     0 | loss: 5.8923073MixupTrain:  epoch  0, batch     1 | loss: 5.4783086MixupTrain:  epoch  0, batch     2 | loss: 4.9926822MixupTrain:  epoch  0, batch     3 | loss: 5.8331539MixupTrain:  epoch  0, batch     4 | loss: 6.3383181MixupTrain:  epoch  0, batch     5 | loss: 5.2518673MixupTrain:  epoch  0, batch     6 | loss: 5.8480061MixupTrain:  epoch  0, batch     7 | loss: 6.2766991MixupTrain:  epoch  0, batch     8 | loss: 5.1962541MixupTrain:  epoch  0, batch     9 | loss: 5.4172755MixupTrain:  epoch  0, batch    10 | loss: 4.5456427MixupTrain:  epoch  0, batch    11 | loss: 5.7426768MixupTrain:  epoch  0, batch    12 | loss: 4.8627206MixupTrain:  epoch  0, batch    13 | loss: 4.8488686MixupTrain:  epoch  0, batch    14 | loss: 5.4780636
MemoryTrain:  epoch  0, batch     0 | loss: 2.4985142MemoryTrain:  epoch  0, batch     1 | loss: 2.7093179MemoryTrain:  epoch  0, batch     2 | loss: 2.1714439MemoryTrain:  epoch  0, batch     3 | loss: 3.1084671MemoryTrain:  epoch  0, batch     4 | loss: 3.1349409MemoryTrain:  epoch  0, batch     5 | loss: 2.3616474MemoryTrain:  epoch  1, batch     0 | loss: 2.5935979MemoryTrain:  epoch  1, batch     1 | loss: 2.5016637MemoryTrain:  epoch  1, batch     2 | loss: 1.8562870MemoryTrain:  epoch  1, batch     3 | loss: 2.6169715MemoryTrain:  epoch  1, batch     4 | loss: 2.6846485MemoryTrain:  epoch  1, batch     5 | loss: 2.2241337MemoryTrain:  epoch  2, batch     0 | loss: 2.1199584MemoryTrain:  epoch  2, batch     1 | loss: 1.8074222MemoryTrain:  epoch  2, batch     2 | loss: 2.6689587MemoryTrain:  epoch  2, batch     3 | loss: 2.3745399MemoryTrain:  epoch  2, batch     4 | loss: 2.1032801MemoryTrain:  epoch  2, batch     5 | loss: 1.9472456MemoryTrain:  epoch  3, batch     0 | loss: 1.9999747MemoryTrain:  epoch  3, batch     1 | loss: 1.6247957MemoryTrain:  epoch  3, batch     2 | loss: 2.1820061MemoryTrain:  epoch  3, batch     3 | loss: 2.0695448MemoryTrain:  epoch  3, batch     4 | loss: 2.2426515MemoryTrain:  epoch  3, batch     5 | loss: 2.1619580MemoryTrain:  epoch  4, batch     0 | loss: 1.9503202MemoryTrain:  epoch  4, batch     1 | loss: 1.7767799MemoryTrain:  epoch  4, batch     2 | loss: 2.0743375MemoryTrain:  epoch  4, batch     3 | loss: 2.0777838MemoryTrain:  epoch  4, batch     4 | loss: 1.8430004MemoryTrain:  epoch  4, batch     5 | loss: 1.7781613MemoryTrain:  epoch  5, batch     0 | loss: 1.5339577MemoryTrain:  epoch  5, batch     1 | loss: 1.9904724MemoryTrain:  epoch  5, batch     2 | loss: 1.9231288MemoryTrain:  epoch  5, batch     3 | loss: 1.6925427MemoryTrain:  epoch  5, batch     4 | loss: 1.5541061MemoryTrain:  epoch  5, batch     5 | loss: 1.5260569MemoryTrain:  epoch  6, batch     0 | loss: 1.7212380MemoryTrain:  epoch  6, batch     1 | loss: 1.8965961MemoryTrain:  epoch  6, batch     2 | loss: 1.5028930MemoryTrain:  epoch  6, batch     3 | loss: 1.8146923MemoryTrain:  epoch  6, batch     4 | loss: 1.7053747MemoryTrain:  epoch  6, batch     5 | loss: 1.7121465MemoryTrain:  epoch  7, batch     0 | loss: 1.6890333MemoryTrain:  epoch  7, batch     1 | loss: 1.6212388MemoryTrain:  epoch  7, batch     2 | loss: 1.3804690MemoryTrain:  epoch  7, batch     3 | loss: 1.6929173MemoryTrain:  epoch  7, batch     4 | loss: 1.9976628MemoryTrain:  epoch  7, batch     5 | loss: 1.7818631MemoryTrain:  epoch  8, batch     0 | loss: 1.6646595MemoryTrain:  epoch  8, batch     1 | loss: 1.4936583MemoryTrain:  epoch  8, batch     2 | loss: 1.6633269MemoryTrain:  epoch  8, batch     3 | loss: 1.4792620MemoryTrain:  epoch  8, batch     4 | loss: 1.3887274MemoryTrain:  epoch  8, batch     5 | loss: 1.7525984MemoryTrain:  epoch  9, batch     0 | loss: 1.4715250MemoryTrain:  epoch  9, batch     1 | loss: 1.5306256MemoryTrain:  epoch  9, batch     2 | loss: 1.4678031MemoryTrain:  epoch  9, batch     3 | loss: 1.4908437MemoryTrain:  epoch  9, batch     4 | loss: 1.6967236MemoryTrain:  epoch  9, batch     5 | loss: 1.3623476
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 78.12%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 78.41%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 66.02%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 62.13%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 59.03%   [EVAL] batch:   18 | acc: 18.75%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 58.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.98%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 65.97%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 69.35%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 70.12%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 70.08%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 71.07%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 69.79%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 67.91%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 66.45%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 64.74%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 64.22%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 64.63%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 64.88%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 68.11%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 66.75%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 67.07%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 67.22%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 66.67%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 65.91%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 65.51%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 64.91%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 64.44%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 63.56%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 63.23%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 63.22%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 62.60%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 62.30%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 62.11%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 62.40%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 62.97%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 63.53%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 65.05%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 64.93%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 64.73%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 64.44%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 64.17%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 63.90%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 63.64%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 63.06%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 62.34%   [EVAL] batch:   79 | acc: 18.75%,  total acc: 61.80%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 61.50%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 61.59%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 61.67%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 61.90%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 61.99%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 62.06%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 62.28%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 62.57%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 62.71%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 62.85%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 62.98%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 63.25%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 63.51%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 63.83%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 63.88%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 64.11%   
cur_acc:  ['0.8561', '0.3594', '0.8170', '0.8438', '0.4952', '0.7812']
his_acc:  ['0.8561', '0.7188', '0.7188', '0.7254', '0.6339', '0.6411']
CurrentTrain: epoch  0, batch     0 | loss: 7.4066253CurrentTrain: epoch  0, batch     1 | loss: 8.7176132CurrentTrain: epoch  1, batch     0 | loss: 7.1888003CurrentTrain: epoch  1, batch     1 | loss: 6.7908125CurrentTrain: epoch  2, batch     0 | loss: 6.4592462CurrentTrain: epoch  2, batch     1 | loss: 6.4664941CurrentTrain: epoch  3, batch     0 | loss: 6.6382599CurrentTrain: epoch  3, batch     1 | loss: 5.1225443CurrentTrain: epoch  4, batch     0 | loss: 5.8243794CurrentTrain: epoch  4, batch     1 | loss: 5.4303141CurrentTrain: epoch  5, batch     0 | loss: 5.5495787CurrentTrain: epoch  5, batch     1 | loss: 5.0589132CurrentTrain: epoch  6, batch     0 | loss: 5.3385544CurrentTrain: epoch  6, batch     1 | loss: 5.0624714CurrentTrain: epoch  7, batch     0 | loss: 4.8819590CurrentTrain: epoch  7, batch     1 | loss: 5.7708292CurrentTrain: epoch  8, batch     0 | loss: 4.9016304CurrentTrain: epoch  8, batch     1 | loss: 4.3310819CurrentTrain: epoch  9, batch     0 | loss: 4.5224133CurrentTrain: epoch  9, batch     1 | loss: 3.9671588
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including YouTube, which it acquired in 2006 for $1.65 billion.  
Head Entity: Alphabet Inc.  
Tail Entity: YouTube  

Relation: organization subsidiaries  
Context: The Coca-Cola Company owns numerous subsidiaries, such as Minute Maid, which specializes in fruit juices and drinks.  
Head Entity: The Coca-Cola Company  
Tail Entity: Minute Maid  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: The tech giant Alphabet Inc. has been making headlines with its innovative projects, but many are unaware that it is the parent company of Google, which has been a leader in the search engine market for over two decades.  
Head Entity: Alphabet Inc.  
Tail Entity: Google  

Relation: organization parents  
Context: In the world of finance, JPMorgan Chase & Co. has a significant influence, especially since it is the parent organization of several well-known banks, including Chase Bank, which serves millions of customers across the United States.  
Head Entity: JPMorgan Chase & Co.  
Tail Entity: Chase Bank  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the world health organization -lrb- who -rrb- has been instrumental in coordinating global responses to health emergencies, including the recent pandemic.  
Head Entity: world health organization  
Tail Entity: who  

Relation: organization alternate names  
Context: the federal bureau of investigation -lrb- fbi -rrb- is known for its role in investigating federal crimes and maintaining national security.  
Head Entity: federal bureau of investigation  
Tail Entity: fbi  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: guy-sheftall entered spelman college in atlanta at age 16 and later earned a master 's in english with a thesis titled `` faulkner 's treatment of women in his major novels . ''
Head Entity: spelman college
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been since 1993.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: the united nations has its main office in new york city, which serves as a hub for international diplomacy and cooperation.  
Head Entity: united nations  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time reminiscing about their childhood. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily celebrated with her brother, who had always been her biggest supporter throughout school. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 4.8864709MixupTrain:  epoch  0, batch     1 | loss: 4.9743067MixupTrain:  epoch  0, batch     2 | loss: 3.9909529MixupTrain:  epoch  0, batch     3 | loss: 5.0368997MixupTrain:  epoch  0, batch     4 | loss: 5.1779748MixupTrain:  epoch  0, batch     5 | loss: 4.6511024MixupTrain:  epoch  0, batch     6 | loss: 5.5935498MixupTrain:  epoch  0, batch     7 | loss: 4.7108177MixupTrain:  epoch  0, batch     8 | loss: 4.7114848MixupTrain:  epoch  0, batch     9 | loss: 5.2328619MixupTrain:  epoch  0, batch    10 | loss: 4.6693918MixupTrain:  epoch  0, batch    11 | loss: 4.8329639MixupTrain:  epoch  0, batch    12 | loss: 6.0736574MixupTrain:  epoch  0, batch    13 | loss: 4.5128738MixupTrain:  epoch  0, batch    14 | loss: 4.9850653MixupTrain:  epoch  0, batch    15 | loss: 4.7658289
MemoryTrain:  epoch  0, batch     0 | loss: 1.6099851MemoryTrain:  epoch  0, batch     1 | loss: 2.2849321MemoryTrain:  epoch  0, batch     2 | loss: 2.3039808MemoryTrain:  epoch  0, batch     3 | loss: 2.3122487MemoryTrain:  epoch  0, batch     4 | loss: 2.0976033MemoryTrain:  epoch  0, batch     5 | loss: 2.3883693MemoryTrain:  epoch  0, batch     6 | loss: 2.8207955MemoryTrain:  epoch  1, batch     0 | loss: 1.7333661MemoryTrain:  epoch  1, batch     1 | loss: 3.0175385MemoryTrain:  epoch  1, batch     2 | loss: 2.0809042MemoryTrain:  epoch  1, batch     3 | loss: 1.7996290MemoryTrain:  epoch  1, batch     4 | loss: 2.1153779MemoryTrain:  epoch  1, batch     5 | loss: 1.7527273MemoryTrain:  epoch  1, batch     6 | loss: 2.5002055MemoryTrain:  epoch  2, batch     0 | loss: 2.1984901MemoryTrain:  epoch  2, batch     1 | loss: 2.0503204MemoryTrain:  epoch  2, batch     2 | loss: 2.2717381MemoryTrain:  epoch  2, batch     3 | loss: 1.7638874MemoryTrain:  epoch  2, batch     4 | loss: 1.9610732MemoryTrain:  epoch  2, batch     5 | loss: 1.5529778MemoryTrain:  epoch  2, batch     6 | loss: 1.8947344MemoryTrain:  epoch  3, batch     0 | loss: 1.9218987MemoryTrain:  epoch  3, batch     1 | loss: 1.7228266MemoryTrain:  epoch  3, batch     2 | loss: 1.8367041MemoryTrain:  epoch  3, batch     3 | loss: 1.6309408MemoryTrain:  epoch  3, batch     4 | loss: 1.9875704MemoryTrain:  epoch  3, batch     5 | loss: 1.8482195MemoryTrain:  epoch  3, batch     6 | loss: 1.7976617MemoryTrain:  epoch  4, batch     0 | loss: 1.8353021MemoryTrain:  epoch  4, batch     1 | loss: 1.3857090MemoryTrain:  epoch  4, batch     2 | loss: 1.3998127MemoryTrain:  epoch  4, batch     3 | loss: 2.0162470MemoryTrain:  epoch  4, batch     4 | loss: 1.7286688MemoryTrain:  epoch  4, batch     5 | loss: 1.8280782MemoryTrain:  epoch  4, batch     6 | loss: 1.7940722MemoryTrain:  epoch  5, batch     0 | loss: 1.8799856MemoryTrain:  epoch  5, batch     1 | loss: 1.6922917MemoryTrain:  epoch  5, batch     2 | loss: 1.4112301MemoryTrain:  epoch  5, batch     3 | loss: 1.6835772MemoryTrain:  epoch  5, batch     4 | loss: 1.3393607MemoryTrain:  epoch  5, batch     5 | loss: 1.4537694MemoryTrain:  epoch  5, batch     6 | loss: 1.5158365MemoryTrain:  epoch  6, batch     0 | loss: 1.6011655MemoryTrain:  epoch  6, batch     1 | loss: 1.4703512MemoryTrain:  epoch  6, batch     2 | loss: 1.4201639MemoryTrain:  epoch  6, batch     3 | loss: 1.6616440MemoryTrain:  epoch  6, batch     4 | loss: 1.5223556MemoryTrain:  epoch  6, batch     5 | loss: 1.6186273MemoryTrain:  epoch  6, batch     6 | loss: 1.5942043MemoryTrain:  epoch  7, batch     0 | loss: 1.2996812MemoryTrain:  epoch  7, batch     1 | loss: 1.4699423MemoryTrain:  epoch  7, batch     2 | loss: 1.6007521MemoryTrain:  epoch  7, batch     3 | loss: 1.5755849MemoryTrain:  epoch  7, batch     4 | loss: 1.3941275MemoryTrain:  epoch  7, batch     5 | loss: 1.2740924MemoryTrain:  epoch  7, batch     6 | loss: 1.3913521MemoryTrain:  epoch  8, batch     0 | loss: 1.3253015MemoryTrain:  epoch  8, batch     1 | loss: 1.5822657MemoryTrain:  epoch  8, batch     2 | loss: 1.3364949MemoryTrain:  epoch  8, batch     3 | loss: 1.3084528MemoryTrain:  epoch  8, batch     4 | loss: 1.5016360MemoryTrain:  epoch  8, batch     5 | loss: 1.4357290MemoryTrain:  epoch  8, batch     6 | loss: 1.5184568MemoryTrain:  epoch  9, batch     0 | loss: 1.3870077MemoryTrain:  epoch  9, batch     1 | loss: 1.3475304MemoryTrain:  epoch  9, batch     2 | loss: 1.3310905MemoryTrain:  epoch  9, batch     3 | loss: 1.3459821MemoryTrain:  epoch  9, batch     4 | loss: 1.3740518MemoryTrain:  epoch  9, batch     5 | loss: 1.3885825MemoryTrain:  epoch  9, batch     6 | loss: 1.3770622
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 38.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 42.05%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 45.67%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 49.55%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 52.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 55.47%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 59.72%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 58.88%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 58.44%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 57.44%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 56.25%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 59.38%   [EVAL] batch:   18 | acc: 18.75%,  total acc: 57.24%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 55.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.74%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 59.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 64.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 65.14%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 66.29%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 66.81%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 66.46%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 66.53%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 66.41%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 66.29%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 67.10%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 66.25%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 64.93%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 63.18%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 61.84%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 60.26%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 59.84%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 60.37%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 60.86%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 61.77%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 62.64%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 63.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.03%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 64.67%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 63.38%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 63.36%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 63.94%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 64.27%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 63.77%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 63.18%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 62.61%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 62.06%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 61.53%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 60.59%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 60.10%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 60.14%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 59.58%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 59.03%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 58.89%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 59.23%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 59.85%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 60.45%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 61.03%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 61.59%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 62.14%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 62.15%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 61.98%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 61.82%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 61.57%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 61.33%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 60.94%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 60.63%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 60.10%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 59.57%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 58.91%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 58.41%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 58.61%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 58.89%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 59.23%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 58.97%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 58.94%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 58.76%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 58.52%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 58.22%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 58.13%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 58.45%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 58.76%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 59.07%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 59.44%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 59.61%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 59.90%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 59.92%   [EVAL] batch:   97 | acc: 43.75%,  total acc: 59.76%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 59.53%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 59.13%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 58.91%   [EVAL] batch:  101 | acc: 25.00%,  total acc: 58.58%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 58.25%   [EVAL] batch:  103 | acc: 43.75%,  total acc: 58.11%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 58.21%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 58.14%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 58.12%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 58.22%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 58.31%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 58.47%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 58.84%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 59.15%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 59.51%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 59.81%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 60.00%   [EVAL] batch:  115 | acc: 43.75%,  total acc: 59.86%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 59.72%   [EVAL] batch:  117 | acc: 43.75%,  total acc: 59.59%   [EVAL] batch:  118 | acc: 18.75%,  total acc: 59.24%   
cur_acc:  ['0.8561', '0.3594', '0.8170', '0.8438', '0.4952', '0.7812', '0.5625']
his_acc:  ['0.8561', '0.7188', '0.7188', '0.7254', '0.6339', '0.6411', '0.5924']
CurrentTrain: epoch  0, batch     0 | loss: 5.8782020CurrentTrain: epoch  0, batch     1 | loss: 6.3021989CurrentTrain: epoch  1, batch     0 | loss: 4.4228067CurrentTrain: epoch  1, batch     1 | loss: 5.7096429CurrentTrain: epoch  2, batch     0 | loss: 4.6176786CurrentTrain: epoch  2, batch     1 | loss: 4.3051534CurrentTrain: epoch  3, batch     0 | loss: 4.3036237CurrentTrain: epoch  3, batch     1 | loss: 4.3630576CurrentTrain: epoch  4, batch     0 | loss: 4.3351660CurrentTrain: epoch  4, batch     1 | loss: 3.3616829CurrentTrain: epoch  5, batch     0 | loss: 3.5339131CurrentTrain: epoch  5, batch     1 | loss: 3.8811078CurrentTrain: epoch  6, batch     0 | loss: 3.3795052CurrentTrain: epoch  6, batch     1 | loss: 2.9696686CurrentTrain: epoch  7, batch     0 | loss: 3.0043330CurrentTrain: epoch  7, batch     1 | loss: 3.1952951CurrentTrain: epoch  8, batch     0 | loss: 2.7429605CurrentTrain: epoch  8, batch     1 | loss: 2.9717777CurrentTrain: epoch  9, batch     0 | loss: 2.9386311CurrentTrain: epoch  9, batch     1 | loss: 2.6776776
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: the chairman of the senate foreign relations committee , massachusetts democrat john kerry , and the panel 's top republican , richard lugar of indiana , were at the white house meeting , which was led by vice president joe biden , a former chairman of the foreign relations panel .
Head Entity: john kerry
Tail Entity: massachusetts
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York to California, actress Emma Stone quickly adapted to the vibrant lifestyle of Los Angeles, where she now resides.  
Head Entity: Emma Stone  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: Following his successful career in the tech industry, Mark Zuckerberg decided to settle in Palo Alto, a city known for its innovation and proximity to Silicon Valley.  
Head Entity: Mark Zuckerberg  
Tail Entity: Palo Alto  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away peacefully on January 5, 2020.  
Head Entity: The renowned author  
Tail Entity: January 5, 2020  

Relation: person date of death  
Context: She left this world on March 22, 2015, surrounded by her loved ones.  
Head Entity: She  
Tail Entity: March 22, 2015  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: mcgovern wants to make the red cross , with 30,000 employees and 700 chapters , more efficient .
Head Entity: red cross
Tail Entity: 30,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech giant Google has grown significantly, now boasting a workforce of over 150,000 employees worldwide.  
Head Entity: Google  
Tail Entity: 150,000  

Relation: organization number of employees members  
Context: The local charity organization, Helping Hands, has recently expanded its reach and currently employs around 200 dedicated staff members.  
Head Entity: Helping Hands  
Tail Entity: 200  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Langhorne Clemens, better known by his pen name Mark Twain, is celebrated for his novels like "The Adventures of Tom Sawyer."  
Head Entity: Samuel Langhorne Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician Stefani Joanne Angelina Germanotta, who is widely recognized as Lady Gaga, has made a significant impact on the pop music scene.  
Head Entity: Stefani Joanne Angelina Germanotta  
Tail Entity: Lady Gaga  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: kerry katona feel out with arnold just before she married bryan mcfadden , kerry cancelled his wedding invitation and have n't spoken since .
Head Entity: bryan mcfadden
Tail Entity: kerry katona
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: after years of dating, emma stone finally tied the knot with dave mccary in a beautiful ceremony, surrounded by family and friends.  
Head Entity: dave mccary  
Tail Entity: emma stone  

Relation: person spouse  
Context: during their time together, ben affleck and jennifer garner built a family, but they eventually decided to part ways amicably.  
Head Entity: jennifer garner  
Tail Entity: ben affleck  
Mixup data size:  284
MixupTrain:  epoch  0, batch     0 | loss: 4.4452053MixupTrain:  epoch  0, batch     1 | loss: 4.6342805MixupTrain:  epoch  0, batch     2 | loss: 6.0559112MixupTrain:  epoch  0, batch     3 | loss: 3.9614822MixupTrain:  epoch  0, batch     4 | loss: 4.2613542MixupTrain:  epoch  0, batch     5 | loss: 4.1107959MixupTrain:  epoch  0, batch     6 | loss: 5.8447554MixupTrain:  epoch  0, batch     7 | loss: 4.5435609MixupTrain:  epoch  0, batch     8 | loss: 4.7876933MixupTrain:  epoch  0, batch     9 | loss: 4.5835903MixupTrain:  epoch  0, batch    10 | loss: 4.2812785MixupTrain:  epoch  0, batch    11 | loss: 4.5630523MixupTrain:  epoch  0, batch    12 | loss: 3.7274559MixupTrain:  epoch  0, batch    13 | loss: 4.3873520MixupTrain:  epoch  0, batch    14 | loss: 3.6357699MixupTrain:  epoch  0, batch    15 | loss: 4.5803108MixupTrain:  epoch  0, batch    16 | loss: 3.9896805MixupTrain:  epoch  0, batch    17 | loss: 5.1083281
MemoryTrain:  epoch  0, batch     0 | loss: 1.7861900MemoryTrain:  epoch  0, batch     1 | loss: 1.8306291MemoryTrain:  epoch  0, batch     2 | loss: 1.5804274MemoryTrain:  epoch  0, batch     3 | loss: 2.0057011MemoryTrain:  epoch  0, batch     4 | loss: 2.4728124MemoryTrain:  epoch  0, batch     5 | loss: 2.0209286MemoryTrain:  epoch  0, batch     6 | loss: 2.4689922MemoryTrain:  epoch  0, batch     7 | loss: 2.4226494MemoryTrain:  epoch  1, batch     0 | loss: 1.9505835MemoryTrain:  epoch  1, batch     1 | loss: 1.6713738MemoryTrain:  epoch  1, batch     2 | loss: 1.8551395MemoryTrain:  epoch  1, batch     3 | loss: 1.9378653MemoryTrain:  epoch  1, batch     4 | loss: 1.7500583MemoryTrain:  epoch  1, batch     5 | loss: 1.9098153MemoryTrain:  epoch  1, batch     6 | loss: 1.8006866MemoryTrain:  epoch  1, batch     7 | loss: 1.5103592MemoryTrain:  epoch  2, batch     0 | loss: 1.6623397MemoryTrain:  epoch  2, batch     1 | loss: 1.7224391MemoryTrain:  epoch  2, batch     2 | loss: 1.4585242MemoryTrain:  epoch  2, batch     3 | loss: 1.7378966MemoryTrain:  epoch  2, batch     4 | loss: 1.5406783MemoryTrain:  epoch  2, batch     5 | loss: 1.6519157MemoryTrain:  epoch  2, batch     6 | loss: 1.3626815MemoryTrain:  epoch  2, batch     7 | loss: 1.6409396MemoryTrain:  epoch  3, batch     0 | loss: 1.5195037MemoryTrain:  epoch  3, batch     1 | loss: 1.4996710MemoryTrain:  epoch  3, batch     2 | loss: 1.5641034MemoryTrain:  epoch  3, batch     3 | loss: 1.5464132MemoryTrain:  epoch  3, batch     4 | loss: 1.5609145MemoryTrain:  epoch  3, batch     5 | loss: 1.4035443MemoryTrain:  epoch  3, batch     6 | loss: 1.4269691MemoryTrain:  epoch  3, batch     7 | loss: 1.3271160MemoryTrain:  epoch  4, batch     0 | loss: 1.5203085MemoryTrain:  epoch  4, batch     1 | loss: 1.4558697MemoryTrain:  epoch  4, batch     2 | loss: 1.4336938MemoryTrain:  epoch  4, batch     3 | loss: 1.3665683MemoryTrain:  epoch  4, batch     4 | loss: 1.5746315MemoryTrain:  epoch  4, batch     5 | loss: 1.3423483MemoryTrain:  epoch  4, batch     6 | loss: 1.5235997MemoryTrain:  epoch  4, batch     7 | loss: 1.2565789MemoryTrain:  epoch  5, batch     0 | loss: 1.4505649MemoryTrain:  epoch  5, batch     1 | loss: 1.3526586MemoryTrain:  epoch  5, batch     2 | loss: 1.3071574MemoryTrain:  epoch  5, batch     3 | loss: 1.3904732MemoryTrain:  epoch  5, batch     4 | loss: 1.3396263MemoryTrain:  epoch  5, batch     5 | loss: 1.3603098MemoryTrain:  epoch  5, batch     6 | loss: 1.2545710MemoryTrain:  epoch  5, batch     7 | loss: 1.3937514MemoryTrain:  epoch  6, batch     0 | loss: 1.2629677MemoryTrain:  epoch  6, batch     1 | loss: 1.3080912MemoryTrain:  epoch  6, batch     2 | loss: 1.3411036MemoryTrain:  epoch  6, batch     3 | loss: 1.3536785MemoryTrain:  epoch  6, batch     4 | loss: 1.3458352MemoryTrain:  epoch  6, batch     5 | loss: 1.3456714MemoryTrain:  epoch  6, batch     6 | loss: 1.3789358MemoryTrain:  epoch  6, batch     7 | loss: 1.4473966MemoryTrain:  epoch  7, batch     0 | loss: 1.2521225MemoryTrain:  epoch  7, batch     1 | loss: 1.3194629MemoryTrain:  epoch  7, batch     2 | loss: 1.3527249MemoryTrain:  epoch  7, batch     3 | loss: 1.2635616MemoryTrain:  epoch  7, batch     4 | loss: 1.4401882MemoryTrain:  epoch  7, batch     5 | loss: 1.3869963MemoryTrain:  epoch  7, batch     6 | loss: 1.3527899MemoryTrain:  epoch  7, batch     7 | loss: 1.2490033MemoryTrain:  epoch  8, batch     0 | loss: 1.3039577MemoryTrain:  epoch  8, batch     1 | loss: 1.3108974MemoryTrain:  epoch  8, batch     2 | loss: 1.2535717MemoryTrain:  epoch  8, batch     3 | loss: 1.3152802MemoryTrain:  epoch  8, batch     4 | loss: 1.2982790MemoryTrain:  epoch  8, batch     5 | loss: 1.3714576MemoryTrain:  epoch  8, batch     6 | loss: 1.3007960MemoryTrain:  epoch  8, batch     7 | loss: 1.2563721MemoryTrain:  epoch  9, batch     0 | loss: 1.2582061MemoryTrain:  epoch  9, batch     1 | loss: 1.3583012MemoryTrain:  epoch  9, batch     2 | loss: 1.2555385MemoryTrain:  epoch  9, batch     3 | loss: 1.3591056MemoryTrain:  epoch  9, batch     4 | loss: 1.3104928MemoryTrain:  epoch  9, batch     5 | loss: 1.3180804MemoryTrain:  epoch  9, batch     6 | loss: 1.2695973MemoryTrain:  epoch  9, batch     7 | loss: 1.3335105
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 69.71%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 64.58%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 79.33%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 66.80%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 64.34%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 12.50%,  total acc: 58.55%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 56.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 60.51%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.23%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.54%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 75.00%,  total acc: 66.44%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 67.89%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 67.74%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 67.58%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 67.42%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 68.01%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 67.32%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 65.97%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 64.19%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 62.66%   [EVAL] batch:   38 | acc: 0.00%,  total acc: 61.06%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 60.62%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 61.13%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 61.61%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 64.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 65.31%   [EVAL] batch:   49 | acc: 0.00%,  total acc: 64.00%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 64.09%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 64.54%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 64.74%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 63.89%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 62.95%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 62.05%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 61.29%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 60.45%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 59.53%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 59.17%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 59.22%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 58.77%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 58.43%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 58.40%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 58.75%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 59.98%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 60.57%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 61.14%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 61.70%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 61.62%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 61.28%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 61.13%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 60.81%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 60.75%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 60.61%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 60.23%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 59.94%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 59.41%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 58.67%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 58.02%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 58.08%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 58.36%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 58.56%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 58.46%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 58.36%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 58.17%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 57.94%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 57.99%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 58.31%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 58.70%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 59.01%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 59.31%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 59.41%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 59.64%   [EVAL] batch:   96 | acc: 37.50%,  total acc: 59.41%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 59.18%   [EVAL] batch:   98 | acc: 31.25%,  total acc: 58.90%   [EVAL] batch:   99 | acc: 25.00%,  total acc: 58.56%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 58.48%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 58.21%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 57.95%   [EVAL] batch:  103 | acc: 50.00%,  total acc: 57.87%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 57.92%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 57.84%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 57.83%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 57.87%   [EVAL] batch:  108 | acc: 68.75%,  total acc: 57.97%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 58.13%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 58.50%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 58.87%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 59.24%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 59.54%   [EVAL] batch:  114 | acc: 81.25%,  total acc: 59.73%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 59.54%   [EVAL] batch:  116 | acc: 37.50%,  total acc: 59.35%   [EVAL] batch:  117 | acc: 31.25%,  total acc: 59.11%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 59.24%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 59.27%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 59.30%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 59.53%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 59.71%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 59.88%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 60.10%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 60.48%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 60.69%   [EVAL] batch:  128 | acc: 37.50%,  total acc: 60.51%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 60.34%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 60.26%   [EVAL] batch:  131 | acc: 31.25%,  total acc: 60.04%   [EVAL] batch:  132 | acc: 37.50%,  total acc: 59.87%   
cur_acc:  ['0.8561', '0.3594', '0.8170', '0.8438', '0.4952', '0.7812', '0.5625', '0.6458']
his_acc:  ['0.8561', '0.7188', '0.7188', '0.7254', '0.6339', '0.6411', '0.5924', '0.5987']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.4779005CurrentTrain: epoch  0, batch     1 | loss: 13.0214672CurrentTrain: epoch  0, batch     2 | loss: 13.0299273CurrentTrain: epoch  0, batch     3 | loss: 13.1604996CurrentTrain: epoch  0, batch     4 | loss: 12.9351940CurrentTrain: epoch  0, batch     5 | loss: 12.7379446CurrentTrain: epoch  0, batch     6 | loss: 12.6226921CurrentTrain: epoch  0, batch     7 | loss: 12.4690847CurrentTrain: epoch  0, batch     8 | loss: 12.3863773CurrentTrain: epoch  0, batch     9 | loss: 12.3548994CurrentTrain: epoch  0, batch    10 | loss: 12.0337000CurrentTrain: epoch  0, batch    11 | loss: 12.1963177CurrentTrain: epoch  0, batch    12 | loss: 12.0306358CurrentTrain: epoch  0, batch    13 | loss: 11.9531250CurrentTrain: epoch  0, batch    14 | loss: 11.7238827CurrentTrain: epoch  0, batch    15 | loss: 11.8024206CurrentTrain: epoch  0, batch    16 | loss: 11.6474466CurrentTrain: epoch  0, batch    17 | loss: 11.3490944CurrentTrain: epoch  0, batch    18 | loss: 11.5750761CurrentTrain: epoch  0, batch    19 | loss: 11.4100723CurrentTrain: epoch  0, batch    20 | loss: 11.2864208CurrentTrain: epoch  0, batch    21 | loss: 11.1610279CurrentTrain: epoch  0, batch    22 | loss: 10.9277954CurrentTrain: epoch  0, batch    23 | loss: 11.4107199CurrentTrain: epoch  0, batch    24 | loss: 11.3968792CurrentTrain: epoch  0, batch    25 | loss: 10.7305202CurrentTrain: epoch  0, batch    26 | loss: 10.6349983CurrentTrain: epoch  0, batch    27 | loss: 10.6477585CurrentTrain: epoch  0, batch    28 | loss: 11.0918217CurrentTrain: epoch  0, batch    29 | loss: 10.5511093CurrentTrain: epoch  0, batch    30 | loss: 10.7972307CurrentTrain: epoch  0, batch    31 | loss: 10.8927650CurrentTrain: epoch  0, batch    32 | loss: 10.2790718CurrentTrain: epoch  0, batch    33 | loss: 10.6098328CurrentTrain: epoch  0, batch    34 | loss: 10.3311558CurrentTrain: epoch  0, batch    35 | loss: 10.3390694CurrentTrain: epoch  0, batch    36 | loss: 10.3381691CurrentTrain: epoch  0, batch    37 | loss: 9.8627167CurrentTrain: epoch  1, batch     0 | loss: 10.5253124CurrentTrain: epoch  1, batch     1 | loss: 9.9426994CurrentTrain: epoch  1, batch     2 | loss: 10.1541367CurrentTrain: epoch  1, batch     3 | loss: 9.4669933CurrentTrain: epoch  1, batch     4 | loss: 9.5678463CurrentTrain: epoch  1, batch     5 | loss: 9.8459158CurrentTrain: epoch  1, batch     6 | loss: 10.1500721CurrentTrain: epoch  1, batch     7 | loss: 9.9074945CurrentTrain: epoch  1, batch     8 | loss: 9.0850039CurrentTrain: epoch  1, batch     9 | loss: 10.0816288CurrentTrain: epoch  1, batch    10 | loss: 10.3808098CurrentTrain: epoch  1, batch    11 | loss: 9.9496622CurrentTrain: epoch  1, batch    12 | loss: 9.6355076CurrentTrain: epoch  1, batch    13 | loss: 9.9222870CurrentTrain: epoch  1, batch    14 | loss: 9.0522757CurrentTrain: epoch  1, batch    15 | loss: 9.3146343CurrentTrain: epoch  1, batch    16 | loss: 9.1551723CurrentTrain: epoch  1, batch    17 | loss: 9.0693817CurrentTrain: epoch  1, batch    18 | loss: 9.3971443CurrentTrain: epoch  1, batch    19 | loss: 9.5503387CurrentTrain: epoch  1, batch    20 | loss: 9.5131664CurrentTrain: epoch  1, batch    21 | loss: 9.3528557CurrentTrain: epoch  1, batch    22 | loss: 9.2104797CurrentTrain: epoch  1, batch    23 | loss: 9.2693367CurrentTrain: epoch  1, batch    24 | loss: 9.7628469CurrentTrain: epoch  1, batch    25 | loss: 9.1187649CurrentTrain: epoch  1, batch    26 | loss: 8.3709965CurrentTrain: epoch  1, batch    27 | loss: 9.5259533CurrentTrain: epoch  1, batch    28 | loss: 9.3441830CurrentTrain: epoch  1, batch    29 | loss: 8.0998850CurrentTrain: epoch  1, batch    30 | loss: 9.4375544CurrentTrain: epoch  1, batch    31 | loss: 8.8953123CurrentTrain: epoch  1, batch    32 | loss: 9.5811138CurrentTrain: epoch  1, batch    33 | loss: 9.5205765CurrentTrain: epoch  1, batch    34 | loss: 8.2259607CurrentTrain: epoch  1, batch    35 | loss: 7.9999828CurrentTrain: epoch  1, batch    36 | loss: 9.0095863CurrentTrain: epoch  1, batch    37 | loss: 8.5167589CurrentTrain: epoch  2, batch     0 | loss: 8.0563450CurrentTrain: epoch  2, batch     1 | loss: 8.0289898CurrentTrain: epoch  2, batch     2 | loss: 8.0829239CurrentTrain: epoch  2, batch     3 | loss: 8.2230120CurrentTrain: epoch  2, batch     4 | loss: 8.1157513CurrentTrain: epoch  2, batch     5 | loss: 8.4713640CurrentTrain: epoch  2, batch     6 | loss: 8.9841795CurrentTrain: epoch  2, batch     7 | loss: 8.2561035CurrentTrain: epoch  2, batch     8 | loss: 8.6973648CurrentTrain: epoch  2, batch     9 | loss: 8.0761156CurrentTrain: epoch  2, batch    10 | loss: 8.7735767CurrentTrain: epoch  2, batch    11 | loss: 8.5150795CurrentTrain: epoch  2, batch    12 | loss: 7.6202502CurrentTrain: epoch  2, batch    13 | loss: 7.7805929CurrentTrain: epoch  2, batch    14 | loss: 8.8518562CurrentTrain: epoch  2, batch    15 | loss: 8.2554274CurrentTrain: epoch  2, batch    16 | loss: 8.7118416CurrentTrain: epoch  2, batch    17 | loss: 7.9840555CurrentTrain: epoch  2, batch    18 | loss: 8.3805733CurrentTrain: epoch  2, batch    19 | loss: 8.3547268CurrentTrain: epoch  2, batch    20 | loss: 7.9599452CurrentTrain: epoch  2, batch    21 | loss: 7.4718938CurrentTrain: epoch  2, batch    22 | loss: 7.5885277CurrentTrain: epoch  2, batch    23 | loss: 7.8539829CurrentTrain: epoch  2, batch    24 | loss: 7.5573711CurrentTrain: epoch  2, batch    25 | loss: 8.2834396CurrentTrain: epoch  2, batch    26 | loss: 8.2522125CurrentTrain: epoch  2, batch    27 | loss: 7.7098522CurrentTrain: epoch  2, batch    28 | loss: 7.8019190CurrentTrain: epoch  2, batch    29 | loss: 8.5243073CurrentTrain: epoch  2, batch    30 | loss: 8.3893375CurrentTrain: epoch  2, batch    31 | loss: 7.1042953CurrentTrain: epoch  2, batch    32 | loss: 7.4612169CurrentTrain: epoch  2, batch    33 | loss: 8.5197630CurrentTrain: epoch  2, batch    34 | loss: 8.7650814CurrentTrain: epoch  2, batch    35 | loss: 7.7307324CurrentTrain: epoch  2, batch    36 | loss: 7.9886031CurrentTrain: epoch  2, batch    37 | loss: 7.8878460CurrentTrain: epoch  3, batch     0 | loss: 7.6800232CurrentTrain: epoch  3, batch     1 | loss: 7.8281841CurrentTrain: epoch  3, batch     2 | loss: 8.3886757CurrentTrain: epoch  3, batch     3 | loss: 8.0304050CurrentTrain: epoch  3, batch     4 | loss: 6.7979774CurrentTrain: epoch  3, batch     5 | loss: 8.0053253CurrentTrain: epoch  3, batch     6 | loss: 7.9851274CurrentTrain: epoch  3, batch     7 | loss: 8.1716976CurrentTrain: epoch  3, batch     8 | loss: 6.8393631CurrentTrain: epoch  3, batch     9 | loss: 7.8327608CurrentTrain: epoch  3, batch    10 | loss: 7.8427057CurrentTrain: epoch  3, batch    11 | loss: 7.1692834CurrentTrain: epoch  3, batch    12 | loss: 7.6106920CurrentTrain: epoch  3, batch    13 | loss: 7.7315130CurrentTrain: epoch  3, batch    14 | loss: 7.1245928CurrentTrain: epoch  3, batch    15 | loss: 7.6267543CurrentTrain: epoch  3, batch    16 | loss: 7.5466175CurrentTrain: epoch  3, batch    17 | loss: 6.6674824CurrentTrain: epoch  3, batch    18 | loss: 7.5398889CurrentTrain: epoch  3, batch    19 | loss: 8.1077747CurrentTrain: epoch  3, batch    20 | loss: 7.3863325CurrentTrain: epoch  3, batch    21 | loss: 7.6706433CurrentTrain: epoch  3, batch    22 | loss: 6.3883619CurrentTrain: epoch  3, batch    23 | loss: 8.0054665CurrentTrain: epoch  3, batch    24 | loss: 7.3955283CurrentTrain: epoch  3, batch    25 | loss: 7.6468244CurrentTrain: epoch  3, batch    26 | loss: 6.9923658CurrentTrain: epoch  3, batch    27 | loss: 6.7498827CurrentTrain: epoch  3, batch    28 | loss: 7.4586191CurrentTrain: epoch  3, batch    29 | loss: 7.3130598CurrentTrain: epoch  3, batch    30 | loss: 6.9603281CurrentTrain: epoch  3, batch    31 | loss: 7.2029495CurrentTrain: epoch  3, batch    32 | loss: 7.0731916CurrentTrain: epoch  3, batch    33 | loss: 7.6696453CurrentTrain: epoch  3, batch    34 | loss: 7.0943575CurrentTrain: epoch  3, batch    35 | loss: 6.6377082CurrentTrain: epoch  3, batch    36 | loss: 6.9720860CurrentTrain: epoch  3, batch    37 | loss: 7.2117395CurrentTrain: epoch  4, batch     0 | loss: 7.1374364CurrentTrain: epoch  4, batch     1 | loss: 6.9946556CurrentTrain: epoch  4, batch     2 | loss: 6.4428558CurrentTrain: epoch  4, batch     3 | loss: 6.4395590CurrentTrain: epoch  4, batch     4 | loss: 6.8813539CurrentTrain: epoch  4, batch     5 | loss: 6.9135919CurrentTrain: epoch  4, batch     6 | loss: 8.6053133CurrentTrain: epoch  4, batch     7 | loss: 7.3159475CurrentTrain: epoch  4, batch     8 | loss: 7.5340137CurrentTrain: epoch  4, batch     9 | loss: 7.1324506CurrentTrain: epoch  4, batch    10 | loss: 6.9403496CurrentTrain: epoch  4, batch    11 | loss: 6.3460989CurrentTrain: epoch  4, batch    12 | loss: 7.2684488CurrentTrain: epoch  4, batch    13 | loss: 7.0608959CurrentTrain: epoch  4, batch    14 | loss: 6.2250290CurrentTrain: epoch  4, batch    15 | loss: 7.3804364CurrentTrain: epoch  4, batch    16 | loss: 7.3157539CurrentTrain: epoch  4, batch    17 | loss: 6.2892261CurrentTrain: epoch  4, batch    18 | loss: 6.4259491CurrentTrain: epoch  4, batch    19 | loss: 6.2780981CurrentTrain: epoch  4, batch    20 | loss: 6.4803486CurrentTrain: epoch  4, batch    21 | loss: 6.4328289CurrentTrain: epoch  4, batch    22 | loss: 6.7141972CurrentTrain: epoch  4, batch    23 | loss: 7.1505680CurrentTrain: epoch  4, batch    24 | loss: 6.7790079CurrentTrain: epoch  4, batch    25 | loss: 6.5194364CurrentTrain: epoch  4, batch    26 | loss: 6.8188248CurrentTrain: epoch  4, batch    27 | loss: 7.0476923CurrentTrain: epoch  4, batch    28 | loss: 6.4040575CurrentTrain: epoch  4, batch    29 | loss: 7.3247061CurrentTrain: epoch  4, batch    30 | loss: 6.8810687CurrentTrain: epoch  4, batch    31 | loss: 7.0593176CurrentTrain: epoch  4, batch    32 | loss: 6.2338920CurrentTrain: epoch  4, batch    33 | loss: 7.0822268CurrentTrain: epoch  4, batch    34 | loss: 5.8152866CurrentTrain: epoch  4, batch    35 | loss: 7.3031521CurrentTrain: epoch  4, batch    36 | loss: 5.8512173CurrentTrain: epoch  4, batch    37 | loss: 6.9377718CurrentTrain: epoch  5, batch     0 | loss: 6.3251867CurrentTrain: epoch  5, batch     1 | loss: 6.3336306CurrentTrain: epoch  5, batch     2 | loss: 6.2036691CurrentTrain: epoch  5, batch     3 | loss: 5.8770037CurrentTrain: epoch  5, batch     4 | loss: 6.6338716CurrentTrain: epoch  5, batch     5 | loss: 6.6018524CurrentTrain: epoch  5, batch     6 | loss: 6.6755147CurrentTrain: epoch  5, batch     7 | loss: 6.7231808CurrentTrain: epoch  5, batch     8 | loss: 6.5240145CurrentTrain: epoch  5, batch     9 | loss: 5.9779701CurrentTrain: epoch  5, batch    10 | loss: 6.0359817CurrentTrain: epoch  5, batch    11 | loss: 5.9733634CurrentTrain: epoch  5, batch    12 | loss: 6.2099199CurrentTrain: epoch  5, batch    13 | loss: 6.3854451CurrentTrain: epoch  5, batch    14 | loss: 6.5950389CurrentTrain: epoch  5, batch    15 | loss: 6.6042380CurrentTrain: epoch  5, batch    16 | loss: 6.2754874CurrentTrain: epoch  5, batch    17 | loss: 5.8071251CurrentTrain: epoch  5, batch    18 | loss: 6.5419631CurrentTrain: epoch  5, batch    19 | loss: 6.2742538CurrentTrain: epoch  5, batch    20 | loss: 6.4234438CurrentTrain: epoch  5, batch    21 | loss: 6.7154207CurrentTrain: epoch  5, batch    22 | loss: 5.2714052CurrentTrain: epoch  5, batch    23 | loss: 6.9646516CurrentTrain: epoch  5, batch    24 | loss: 5.8972816CurrentTrain: epoch  5, batch    25 | loss: 6.0114942CurrentTrain: epoch  5, batch    26 | loss: 6.3716817CurrentTrain: epoch  5, batch    27 | loss: 6.9723377CurrentTrain: epoch  5, batch    28 | loss: 7.6066890CurrentTrain: epoch  5, batch    29 | loss: 6.2915506CurrentTrain: epoch  5, batch    30 | loss: 7.2186332CurrentTrain: epoch  5, batch    31 | loss: 6.4361939CurrentTrain: epoch  5, batch    32 | loss: 5.5128298CurrentTrain: epoch  5, batch    33 | loss: 5.7463894CurrentTrain: epoch  5, batch    34 | loss: 6.0145526CurrentTrain: epoch  5, batch    35 | loss: 6.4645882CurrentTrain: epoch  5, batch    36 | loss: 6.7703714CurrentTrain: epoch  5, batch    37 | loss: 6.3922930CurrentTrain: epoch  6, batch     0 | loss: 6.0655661CurrentTrain: epoch  6, batch     1 | loss: 5.5579176CurrentTrain: epoch  6, batch     2 | loss: 6.3698559CurrentTrain: epoch  6, batch     3 | loss: 5.7667074CurrentTrain: epoch  6, batch     4 | loss: 5.9393406CurrentTrain: epoch  6, batch     5 | loss: 5.6704388CurrentTrain: epoch  6, batch     6 | loss: 5.9916263CurrentTrain: epoch  6, batch     7 | loss: 5.9026294CurrentTrain: epoch  6, batch     8 | loss: 5.8897047CurrentTrain: epoch  6, batch     9 | loss: 6.2751079CurrentTrain: epoch  6, batch    10 | loss: 5.6052108CurrentTrain: epoch  6, batch    11 | loss: 5.6123581CurrentTrain: epoch  6, batch    12 | loss: 5.3868375CurrentTrain: epoch  6, batch    13 | loss: 5.9541306CurrentTrain: epoch  6, batch    14 | loss: 5.8034301CurrentTrain: epoch  6, batch    15 | loss: 6.3145823CurrentTrain: epoch  6, batch    16 | loss: 5.9363070CurrentTrain: epoch  6, batch    17 | loss: 6.4128680CurrentTrain: epoch  6, batch    18 | loss: 6.2183642CurrentTrain: epoch  6, batch    19 | loss: 7.3370285CurrentTrain: epoch  6, batch    20 | loss: 6.0876441CurrentTrain: epoch  6, batch    21 | loss: 5.9404926CurrentTrain: epoch  6, batch    22 | loss: 5.7248068CurrentTrain: epoch  6, batch    23 | loss: 5.5174379CurrentTrain: epoch  6, batch    24 | loss: 6.3926830CurrentTrain: epoch  6, batch    25 | loss: 5.9223785CurrentTrain: epoch  6, batch    26 | loss: 5.7721519CurrentTrain: epoch  6, batch    27 | loss: 6.8243761CurrentTrain: epoch  6, batch    28 | loss: 5.8056202CurrentTrain: epoch  6, batch    29 | loss: 6.5822287CurrentTrain: epoch  6, batch    30 | loss: 6.5960956CurrentTrain: epoch  6, batch    31 | loss: 6.1780796CurrentTrain: epoch  6, batch    32 | loss: 6.5711536CurrentTrain: epoch  6, batch    33 | loss: 6.1694889CurrentTrain: epoch  6, batch    34 | loss: 6.3138938CurrentTrain: epoch  6, batch    35 | loss: 6.1280298CurrentTrain: epoch  6, batch    36 | loss: 6.4999018CurrentTrain: epoch  6, batch    37 | loss: 6.2841749CurrentTrain: epoch  7, batch     0 | loss: 6.0532331CurrentTrain: epoch  7, batch     1 | loss: 6.6823506CurrentTrain: epoch  7, batch     2 | loss: 5.6777267CurrentTrain: epoch  7, batch     3 | loss: 5.8840399CurrentTrain: epoch  7, batch     4 | loss: 6.1135793CurrentTrain: epoch  7, batch     5 | loss: 6.4377241CurrentTrain: epoch  7, batch     6 | loss: 5.8106022CurrentTrain: epoch  7, batch     7 | loss: 6.2398186CurrentTrain: epoch  7, batch     8 | loss: 5.9014034CurrentTrain: epoch  7, batch     9 | loss: 5.9684258CurrentTrain: epoch  7, batch    10 | loss: 5.9433365CurrentTrain: epoch  7, batch    11 | loss: 5.3409653CurrentTrain: epoch  7, batch    12 | loss: 6.1093493CurrentTrain: epoch  7, batch    13 | loss: 5.5262771CurrentTrain: epoch  7, batch    14 | loss: 6.2894287CurrentTrain: epoch  7, batch    15 | loss: 5.9683981CurrentTrain: epoch  7, batch    16 | loss: 6.1467175CurrentTrain: epoch  7, batch    17 | loss: 5.9037685CurrentTrain: epoch  7, batch    18 | loss: 5.7638140CurrentTrain: epoch  7, batch    19 | loss: 5.8760138CurrentTrain: epoch  7, batch    20 | loss: 5.7612514CurrentTrain: epoch  7, batch    21 | loss: 6.1951632CurrentTrain: epoch  7, batch    22 | loss: 5.8084326CurrentTrain: epoch  7, batch    23 | loss: 5.5344238CurrentTrain: epoch  7, batch    24 | loss: 5.9586883CurrentTrain: epoch  7, batch    25 | loss: 5.7059259CurrentTrain: epoch  7, batch    26 | loss: 5.4087148CurrentTrain: epoch  7, batch    27 | loss: 5.7042246CurrentTrain: epoch  7, batch    28 | loss: 6.1308074CurrentTrain: epoch  7, batch    29 | loss: 5.3998308CurrentTrain: epoch  7, batch    30 | loss: 5.4864993CurrentTrain: epoch  7, batch    31 | loss: 5.7762756CurrentTrain: epoch  7, batch    32 | loss: 5.4241352CurrentTrain: epoch  7, batch    33 | loss: 5.6537647CurrentTrain: epoch  7, batch    34 | loss: 5.3947701CurrentTrain: epoch  7, batch    35 | loss: 5.5181632CurrentTrain: epoch  7, batch    36 | loss: 5.9016514CurrentTrain: epoch  7, batch    37 | loss: 6.7295675CurrentTrain: epoch  8, batch     0 | loss: 5.4241638CurrentTrain: epoch  8, batch     1 | loss: 5.6356707CurrentTrain: epoch  8, batch     2 | loss: 5.8903089CurrentTrain: epoch  8, batch     3 | loss: 5.0958405CurrentTrain: epoch  8, batch     4 | loss: 5.5194426CurrentTrain: epoch  8, batch     5 | loss: 5.3613968CurrentTrain: epoch  8, batch     6 | loss: 5.6242437CurrentTrain: epoch  8, batch     7 | loss: 5.6247149CurrentTrain: epoch  8, batch     8 | loss: 5.6744037CurrentTrain: epoch  8, batch     9 | loss: 5.5826244CurrentTrain: epoch  8, batch    10 | loss: 5.4366693CurrentTrain: epoch  8, batch    11 | loss: 5.6590729CurrentTrain: epoch  8, batch    12 | loss: 5.1416225CurrentTrain: epoch  8, batch    13 | loss: 5.1598673CurrentTrain: epoch  8, batch    14 | loss: 5.3735933CurrentTrain: epoch  8, batch    15 | loss: 5.3277702CurrentTrain: epoch  8, batch    16 | loss: 5.2737541CurrentTrain: epoch  8, batch    17 | loss: 5.5329790CurrentTrain: epoch  8, batch    18 | loss: 5.2349215CurrentTrain: epoch  8, batch    19 | loss: 5.2035971CurrentTrain: epoch  8, batch    20 | loss: 5.7338805CurrentTrain: epoch  8, batch    21 | loss: 5.3788886CurrentTrain: epoch  8, batch    22 | loss: 6.0865493CurrentTrain: epoch  8, batch    23 | loss: 5.3795838CurrentTrain: epoch  8, batch    24 | loss: 5.9548473CurrentTrain: epoch  8, batch    25 | loss: 5.4904327CurrentTrain: epoch  8, batch    26 | loss: 5.1500540CurrentTrain: epoch  8, batch    27 | loss: 5.5120025CurrentTrain: epoch  8, batch    28 | loss: 5.1916151CurrentTrain: epoch  8, batch    29 | loss: 5.0791302CurrentTrain: epoch  8, batch    30 | loss: 5.9536781CurrentTrain: epoch  8, batch    31 | loss: 5.7528114CurrentTrain: epoch  8, batch    32 | loss: 5.4830942CurrentTrain: epoch  8, batch    33 | loss: 5.5561333CurrentTrain: epoch  8, batch    34 | loss: 5.2637749CurrentTrain: epoch  8, batch    35 | loss: 5.2339044CurrentTrain: epoch  8, batch    36 | loss: 5.6451612CurrentTrain: epoch  8, batch    37 | loss: 5.0515451CurrentTrain: epoch  9, batch     0 | loss: 5.3029490CurrentTrain: epoch  9, batch     1 | loss: 5.2450943CurrentTrain: epoch  9, batch     2 | loss: 5.2911072CurrentTrain: epoch  9, batch     3 | loss: 5.0139804CurrentTrain: epoch  9, batch     4 | loss: 5.1460009CurrentTrain: epoch  9, batch     5 | loss: 5.0396562CurrentTrain: epoch  9, batch     6 | loss: 5.1882930CurrentTrain: epoch  9, batch     7 | loss: 5.0311809CurrentTrain: epoch  9, batch     8 | loss: 6.0053778CurrentTrain: epoch  9, batch     9 | loss: 5.4267502CurrentTrain: epoch  9, batch    10 | loss: 5.1994591CurrentTrain: epoch  9, batch    11 | loss: 5.2627912CurrentTrain: epoch  9, batch    12 | loss: 5.1383095CurrentTrain: epoch  9, batch    13 | loss: 4.9480839CurrentTrain: epoch  9, batch    14 | loss: 5.2772403CurrentTrain: epoch  9, batch    15 | loss: 5.4415331CurrentTrain: epoch  9, batch    16 | loss: 5.0093002CurrentTrain: epoch  9, batch    17 | loss: 5.4738822CurrentTrain: epoch  9, batch    18 | loss: 5.1048889CurrentTrain: epoch  9, batch    19 | loss: 5.1489453CurrentTrain: epoch  9, batch    20 | loss: 5.0310297CurrentTrain: epoch  9, batch    21 | loss: 5.9422283CurrentTrain: epoch  9, batch    22 | loss: 5.0901527CurrentTrain: epoch  9, batch    23 | loss: 5.0930533CurrentTrain: epoch  9, batch    24 | loss: 4.9820065CurrentTrain: epoch  9, batch    25 | loss: 5.0762725CurrentTrain: epoch  9, batch    26 | loss: 5.1643085CurrentTrain: epoch  9, batch    27 | loss: 4.7971578CurrentTrain: epoch  9, batch    28 | loss: 5.0627909CurrentTrain: epoch  9, batch    29 | loss: 4.8696580CurrentTrain: epoch  9, batch    30 | loss: 4.9091406CurrentTrain: epoch  9, batch    31 | loss: 5.0748367CurrentTrain: epoch  9, batch    32 | loss: 4.8871026CurrentTrain: epoch  9, batch    33 | loss: 5.1693220CurrentTrain: epoch  9, batch    34 | loss: 5.0398927CurrentTrain: epoch  9, batch    35 | loss: 5.0356846CurrentTrain: epoch  9, batch    36 | loss: 4.9155340CurrentTrain: epoch  9, batch    37 | loss: 5.6923285
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: meeting with iran 's majlis speaker gholam-ali haddad-adel in geneva tuesday , he urged all islamic states to strengthen solidarity and to hold consultations .
Head Entity: gholam-ali haddad-adel
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After moving to Canada for his studies, John Smith quickly adapted to the new culture and made many friends in Toronto.  
Head Entity: John Smith  
Tail Entity: Canada  

Relation: person countries of residence  
Context: Maria Lopez has been living in Spain for the past five years, enjoying the vibrant culture and beautiful landscapes.  
Head Entity: Maria Lopez  
Tail Entity: Spain  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: as for any gay or lesbian people who enlisted with the military this week , `` they may be told they can not join '' if the pentagon decides to enforce the law , said aubrey sarvis , executive director of the servicemembers legal defense network , a group representing service members affected by the law .
Head Entity: servicemembers legal defense network
Tail Entity: aubrey sarvis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: The board of directors at Tech Innovations Inc. recently appointed Sarah Johnson as the new Chief Technology Officer, recognizing her extensive experience in the tech industry and her leadership skills.  
Head Entity: Tech Innovations Inc.  
Tail Entity: Sarah Johnson  

Relation: organization top members employees  
Context: During the annual meeting, the CEO of Green Earth Solutions, Mark Thompson, announced the promotion of Lisa Chen to the position of Chief Financial Officer, highlighting her contributions to the company's growth.  
Head Entity: Green Earth Solutions  
Tail Entity: Lisa Chen  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union (EU) is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: the former detainee , murat kurnaz , was turned over to german authorities and freed in 2006 after a personal plea from german chancellor angela merkel .
Head Entity: murat kurnaz
Tail Entity: german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist, albert einstein, was born in the kingdom of wurttemberg in the german empire in 1879.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author, j.k. rowling, hails from the united kingdom, where she created the beloved harry potter series.  
Head Entity: j.k. rowling  
Tail Entity: british  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: in 2002 , he sought out mark buse , mccain 's longtime staff director at the senate commerce committee , which the senator chaired .
Head Entity: mark buse
Tail Entity: senator
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: After years of hard work and dedication, Sarah Thompson was finally recognized as the Chief Executive Officer of the company.  
Head Entity: Sarah Thompson  
Tail Entity: Chief Executive Officer  

Relation: person title  
Context: During the award ceremony, John Smith was honored for his contributions and was presented with the title of Best Director for his latest film.  
Head Entity: John Smith  
Tail Entity: Best Director  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: general motors china , china 's faw announce $ 293 million commercial vehicles joint venture .
Head Entity: faw
Tail Entity: china
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: samsung electronics has established its main office in south korea, becoming a leader in the technology sector.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization country of headquarters  
Context: the headquarters of nestle is located in switzerland, where it oversees its global operations.  
Head Entity: nestle  
Tail Entity: switzerland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 84.82%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 84.82%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   
cur_acc:  ['0.8617']
his_acc:  ['0.8617']
CurrentTrain: epoch  0, batch     0 | loss: 4.6685724CurrentTrain: epoch  0, batch     1 | loss: 5.2128744CurrentTrain: epoch  1, batch     0 | loss: 4.3811817CurrentTrain: epoch  1, batch     1 | loss: 4.3360529CurrentTrain: epoch  2, batch     0 | loss: 4.2064004CurrentTrain: epoch  2, batch     1 | loss: 3.9156106CurrentTrain: epoch  3, batch     0 | loss: 3.2033896CurrentTrain: epoch  3, batch     1 | loss: 3.5135400CurrentTrain: epoch  4, batch     0 | loss: 3.3115120CurrentTrain: epoch  4, batch     1 | loss: 2.7737665CurrentTrain: epoch  5, batch     0 | loss: 3.0598953CurrentTrain: epoch  5, batch     1 | loss: 3.0726056CurrentTrain: epoch  6, batch     0 | loss: 2.9171124CurrentTrain: epoch  6, batch     1 | loss: 2.8911459CurrentTrain: epoch  7, batch     0 | loss: 2.7228580CurrentTrain: epoch  7, batch     1 | loss: 2.7594547CurrentTrain: epoch  8, batch     0 | loss: 2.3717852CurrentTrain: epoch  8, batch     1 | loss: 2.8496070CurrentTrain: epoch  9, batch     0 | loss: 2.6058874CurrentTrain: epoch  9, batch     1 | loss: 2.2511213
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the mnlf used to be the largest muslim group fighting for a separate islamic homeland in the southern philippines until it settled for limited autonomy and signed a peace agreement with manila in 1996 .
Head Entity: mnlf
Tail Entity: islamic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: the national council of churches in the philippines has been a significant advocate for social justice and has ties to various religious groups across the country.  
Head Entity: national council of churches in the philippines  
Tail Entity: religious groups  

Relation: organization political religious affiliation  
Context: the interfaith alliance works to promote understanding and cooperation among different faiths, influencing political discourse in the region.  
Head Entity: interfaith alliance  
Tail Entity: faiths  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, highlighting their close bond and shared childhood memories.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, uncle bob introduced his niece, sarah, to the rest of the family, emphasizing how proud he was of her accomplishments.  
Head Entity: uncle bob  
Tail Entity: sarah  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: millender-mcdonald , who was 68 , died late saturday at her home in carson , california , said her chief of staff , bandele mcqueen .
Head Entity: millender-mcdonald
Tail Entity: carson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: renowned author and activist, james baldwin, passed away in 1987 in the city of saint-paul, minnesota, where he spent his final years.  
Head Entity: james baldwin  
Tail Entity: saint-paul  

Relation: person city of death  
Context: the famous physicist, albert einstein, died in 1955 in the city of princeton, new jersey, where he had lived for many years.  
Head Entity: albert einstein  
Tail Entity: princeton  
Mixup data size:  105
MixupTrain:  epoch  0, batch     0 | loss: 13.0480540MixupTrain:  epoch  0, batch     1 | loss: 12.4530518MixupTrain:  epoch  0, batch     2 | loss: 11.5422126MixupTrain:  epoch  0, batch     3 | loss: 11.1502420MixupTrain:  epoch  0, batch     4 | loss: 11.2329553MixupTrain:  epoch  0, batch     5 | loss: 10.3860299MixupTrain:  epoch  0, batch     6 | loss: 10.7618197
MemoryTrain:  epoch  0, batch     0 | loss: 4.9493914MemoryTrain:  epoch  0, batch     1 | loss: 4.9142408MemoryTrain:  epoch  0, batch     2 | loss: 4.8299208MemoryTrain:  epoch  1, batch     0 | loss: 4.3522425MemoryTrain:  epoch  1, batch     1 | loss: 4.2219839MemoryTrain:  epoch  1, batch     2 | loss: 3.3026073MemoryTrain:  epoch  2, batch     0 | loss: 3.4287958MemoryTrain:  epoch  2, batch     1 | loss: 4.0195246MemoryTrain:  epoch  2, batch     2 | loss: 2.9241693MemoryTrain:  epoch  3, batch     0 | loss: 3.1969681MemoryTrain:  epoch  3, batch     1 | loss: 4.0639315MemoryTrain:  epoch  3, batch     2 | loss: 2.2846098MemoryTrain:  epoch  4, batch     0 | loss: 3.1443205MemoryTrain:  epoch  4, batch     1 | loss: 3.7113907MemoryTrain:  epoch  4, batch     2 | loss: 1.5893831MemoryTrain:  epoch  5, batch     0 | loss: 3.4777060MemoryTrain:  epoch  5, batch     1 | loss: 2.5571351MemoryTrain:  epoch  5, batch     2 | loss: 1.1994079MemoryTrain:  epoch  6, batch     0 | loss: 3.5352345MemoryTrain:  epoch  6, batch     1 | loss: 2.5374546MemoryTrain:  epoch  6, batch     2 | loss: 1.1604913MemoryTrain:  epoch  7, batch     0 | loss: 3.1445868MemoryTrain:  epoch  7, batch     1 | loss: 2.9971728MemoryTrain:  epoch  7, batch     2 | loss: 1.2514191MemoryTrain:  epoch  8, batch     0 | loss: 2.7942109MemoryTrain:  epoch  8, batch     1 | loss: 2.6008329MemoryTrain:  epoch  8, batch     2 | loss: 2.6257725MemoryTrain:  epoch  9, batch     0 | loss: 2.8691220MemoryTrain:  epoch  9, batch     1 | loss: 2.4544466MemoryTrain:  epoch  9, batch     2 | loss: 2.0350008
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 70.19%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 87.68%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 87.68%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 87.33%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 85.81%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 84.87%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 83.65%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 83.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.69%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 83.48%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 83.58%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.81%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 83.19%   
cur_acc:  ['0.8617', '0.7019']
his_acc:  ['0.8617', '0.8319']
CurrentTrain: epoch  0, batch     0 | loss: 5.0203352CurrentTrain: epoch  0, batch     1 | loss: 7.4475064CurrentTrain: epoch  1, batch     0 | loss: 6.1425667CurrentTrain: epoch  1, batch     1 | loss: 4.6004429CurrentTrain: epoch  2, batch     0 | loss: 5.0269146CurrentTrain: epoch  2, batch     1 | loss: 5.0067706CurrentTrain: epoch  3, batch     0 | loss: 4.7440319CurrentTrain: epoch  3, batch     1 | loss: 3.9799106CurrentTrain: epoch  4, batch     0 | loss: 4.2128348CurrentTrain: epoch  4, batch     1 | loss: 3.8480458CurrentTrain: epoch  5, batch     0 | loss: 3.9671721CurrentTrain: epoch  5, batch     1 | loss: 3.7966766CurrentTrain: epoch  6, batch     0 | loss: 3.3856487CurrentTrain: epoch  6, batch     1 | loss: 4.0151138CurrentTrain: epoch  7, batch     0 | loss: 3.6411214CurrentTrain: epoch  7, batch     1 | loss: 3.2373180CurrentTrain: epoch  8, batch     0 | loss: 3.1897135CurrentTrain: epoch  8, batch     1 | loss: 3.4193633CurrentTrain: epoch  9, batch     0 | loss: 2.8773208CurrentTrain: epoch  9, batch     1 | loss: 3.5725877
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: the chairman of the senate foreign relations committee , massachusetts democrat john kerry , and the panel 's top republican , richard lugar of indiana , were at the white house meeting , which was led by vice president joe biden , a former chairman of the foreign relations panel .
Head Entity: john kerry
Tail Entity: massachusetts
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York to California, actress Emma Stone quickly adapted to the sunny lifestyle of Los Angeles, where she now resides.  
Head Entity: Emma Stone  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: Following his successful career in the tech industry, Mark Zuckerberg decided to settle down in Palo Alto, a city known for its innovation and proximity to Silicon Valley.  
Head Entity: Mark Zuckerberg  
Tail Entity: Palo Alto  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The famous author passed away on July 10, 2020, after a long illness.  
Head Entity: The famous author  
Tail Entity: July 10, 2020  

Relation: person date of death  
Context: She left this world on March 5, 2018, surrounded by her family.  
Head Entity: She  
Tail Entity: March 5, 2018  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, the newly formed entity, Global Solutions Inc., reported an impressive count of 50,000 employees, making it one of the largest firms in the industry.  
Head Entity: Global Solutions Inc.  
Tail Entity: 50,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to himself as the bard of Avon, '' said the literary critic during the lecture.  
Head Entity: author  
Tail Entity: bard of Avon  

Relation: person alternate names  
Context: `` in the world of music, she is often called the queen of pop, '' remarked the journalist in her article.  
Head Entity: she  
Tail Entity: queen of pop  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: in addition to his wife , meskill is survived by two daughters , eileen gallup of new britain and maureen heneghan of haddon heights , n.j. ; three sons , john , of kensington , conn. ; peter , of east hartford , conn. ; and thomas , of branford , conn. ; two sisters , ruth prior of naples , fla. , and sister laura marie of portland , conn. ; five grandchildren , and two step-grandchildren .
Head Entity: his
Tail Entity: meskill
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: after a long and happy marriage, john and his wife, sarah, decided to celebrate their 50th anniversary with a big family gathering.  
Head Entity: his  
Tail Entity: wife  

Relation: person spouse  
Context: during the interview, she spoke fondly of her husband, who has always been her greatest supporter throughout her career.  
Head Entity: her  
Tail Entity: husband  
Mixup data size:  135
MixupTrain:  epoch  0, batch     0 | loss: 8.5171715MixupTrain:  epoch  0, batch     1 | loss: 7.9060906MixupTrain:  epoch  0, batch     2 | loss: 8.0003373MixupTrain:  epoch  0, batch     3 | loss: 8.6703666MixupTrain:  epoch  0, batch     4 | loss: 7.5913075MixupTrain:  epoch  0, batch     5 | loss: 7.3355795MixupTrain:  epoch  0, batch     6 | loss: 7.8864759MixupTrain:  epoch  0, batch     7 | loss: 7.6349011MixupTrain:  epoch  0, batch     8 | loss: 7.0401980
MemoryTrain:  epoch  0, batch     0 | loss: 4.6489687MemoryTrain:  epoch  0, batch     1 | loss: 3.2724843MemoryTrain:  epoch  0, batch     2 | loss: 4.2616982MemoryTrain:  epoch  1, batch     0 | loss: 2.9858651MemoryTrain:  epoch  1, batch     1 | loss: 4.8583603MemoryTrain:  epoch  1, batch     2 | loss: 3.0862679MemoryTrain:  epoch  2, batch     0 | loss: 2.8415732MemoryTrain:  epoch  2, batch     1 | loss: 3.2116606MemoryTrain:  epoch  2, batch     2 | loss: 3.8566518MemoryTrain:  epoch  3, batch     0 | loss: 2.8021786MemoryTrain:  epoch  3, batch     1 | loss: 2.8480735MemoryTrain:  epoch  3, batch     2 | loss: 3.5621405MemoryTrain:  epoch  4, batch     0 | loss: 3.2237861MemoryTrain:  epoch  4, batch     1 | loss: 2.6648512MemoryTrain:  epoch  4, batch     2 | loss: 2.1783338MemoryTrain:  epoch  5, batch     0 | loss: 2.5382416MemoryTrain:  epoch  5, batch     1 | loss: 3.2382576MemoryTrain:  epoch  5, batch     2 | loss: 2.6210718MemoryTrain:  epoch  6, batch     0 | loss: 2.5988622MemoryTrain:  epoch  6, batch     1 | loss: 2.1735945MemoryTrain:  epoch  6, batch     2 | loss: 2.7557030MemoryTrain:  epoch  7, batch     0 | loss: 2.0934947MemoryTrain:  epoch  7, batch     1 | loss: 2.1943302MemoryTrain:  epoch  7, batch     2 | loss: 2.4521637MemoryTrain:  epoch  8, batch     0 | loss: 2.1786418MemoryTrain:  epoch  8, batch     1 | loss: 1.9721956MemoryTrain:  epoch  8, batch     2 | loss: 2.3308196MemoryTrain:  epoch  9, batch     0 | loss: 2.0937662MemoryTrain:  epoch  9, batch     1 | loss: 2.2114840MemoryTrain:  epoch  9, batch     2 | loss: 2.5368042
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 59.17%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 84.82%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 83.70%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.64%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 86.74%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 86.95%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 86.96%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 86.63%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 84.80%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 83.22%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 81.73%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 80.49%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 79.91%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 79.65%   [EVAL] batch:   43 | acc: 18.75%,  total acc: 78.27%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 76.81%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 76.36%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 76.20%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 76.30%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 76.28%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 76.59%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 77.04%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 77.36%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 77.55%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 77.05%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 76.12%   [EVAL] batch:   56 | acc: 12.50%,  total acc: 75.00%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 74.14%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 73.52%   [EVAL] batch:   59 | acc: 0.00%,  total acc: 72.29%   
cur_acc:  ['0.8617', '0.7019', '0.5917']
his_acc:  ['0.8617', '0.8319', '0.7229']
CurrentTrain: epoch  0, batch     0 | loss: 7.8894057CurrentTrain: epoch  0, batch     1 | loss: 7.7912207CurrentTrain: epoch  1, batch     0 | loss: 7.4721184CurrentTrain: epoch  1, batch     1 | loss: 6.8082299CurrentTrain: epoch  2, batch     0 | loss: 6.8418984CurrentTrain: epoch  2, batch     1 | loss: 5.9911423CurrentTrain: epoch  3, batch     0 | loss: 6.1377344CurrentTrain: epoch  3, batch     1 | loss: 5.6043615CurrentTrain: epoch  4, batch     0 | loss: 5.8977499CurrentTrain: epoch  4, batch     1 | loss: 5.0885534CurrentTrain: epoch  5, batch     0 | loss: 5.4987364CurrentTrain: epoch  5, batch     1 | loss: 5.2321730CurrentTrain: epoch  6, batch     0 | loss: 5.2453423CurrentTrain: epoch  6, batch     1 | loss: 4.9270287CurrentTrain: epoch  7, batch     0 | loss: 5.1503096CurrentTrain: epoch  7, batch     1 | loss: 4.9564853CurrentTrain: epoch  8, batch     0 | loss: 5.1434021CurrentTrain: epoch  8, batch     1 | loss: 4.5360861CurrentTrain: epoch  9, batch     0 | loss: 4.4225464CurrentTrain: epoch  9, batch     1 | loss: 4.7104926
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has become a leading platform for video content, and Waymo, which focuses on self-driving technology.  
Head Entity: Alphabet Inc.  
Tail Entity: Waymo  

Relation: organization subsidiaries  
Context: The multinational corporation Procter & Gamble owns various subsidiaries, such as Gillette, which specializes in personal care products, and Tide, known for its laundry detergents.  
Head Entity: Procter & Gamble  
Tail Entity: Gillette  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: those who try to salvage possessions from the debris of their homes can easily turn into victims too , said dr. irwin redlener , director of the national center for disaster preparedness at columbia university mailman school of medicine .
Head Entity: national center for disaster preparedness
Tail Entity: columbia university mailman school of medicine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: The Massachusetts Institute of Technology, known for its cutting-edge research and innovation, is part of a larger network of educational institutions that includes Harvard University.  
Head Entity: Massachusetts Institute of Technology  
Tail Entity: Harvard University  

Relation: organization parents  
Context: The World Wildlife Fund, a leading organization in conservation efforts, operates under the umbrella of the larger World Wide Fund for Nature, which has a global reach.  
Head Entity: World Wildlife Fund  
Tail Entity: World Wide Fund for Nature  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: while section 106 of the hyde act openly bans indian testing , and the agreement upholds reinforces that test ban by upholding the applicability of domestic laws , washington has already recommended that the nuclear suppliers group -lrb- nsg -rrb- link its proposed exemption for india to a similar test ban .
Head Entity: nuclear suppliers group
Tail Entity: nsg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability by providing financial assistance and advice to member countries.  
Head Entity: International Monetary Fund  
Tail Entity: IMF  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of global health initiatives, especially during the COVID-19 pandemic.  
Head Entity: World Health Organization  
Tail Entity: WHO  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ in 2015, tech giant apple inc. announced plans to expand its operations in cupertino, california, where it has been headquartered since its founding.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ the multinational corporation samsung electronics is based in suwon, south korea, and has been a leader in the technology sector for decades.  
Head Entity: samsung electronics  
Tail Entity: suwon  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time catching up. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily and her brother celebrated their achievements with a big dinner. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  164
MixupTrain:  epoch  0, batch     0 | loss: 8.6929282MixupTrain:  epoch  0, batch     1 | loss: 7.3862654MixupTrain:  epoch  0, batch     2 | loss: 7.6813449MixupTrain:  epoch  0, batch     3 | loss: 7.9314498MixupTrain:  epoch  0, batch     4 | loss: 8.0391816MixupTrain:  epoch  0, batch     5 | loss: 6.9451060MixupTrain:  epoch  0, batch     6 | loss: 7.2534480MixupTrain:  epoch  0, batch     7 | loss: 6.2591600MixupTrain:  epoch  0, batch     8 | loss: 6.6047126MixupTrain:  epoch  0, batch     9 | loss: 6.9360585MixupTrain:  epoch  0, batch    10 | loss: 6.2346792
MemoryTrain:  epoch  0, batch     0 | loss: 3.2581871MemoryTrain:  epoch  0, batch     1 | loss: 3.8189695MemoryTrain:  epoch  0, batch     2 | loss: 3.8967953MemoryTrain:  epoch  0, batch     3 | loss: 3.2788353MemoryTrain:  epoch  1, batch     0 | loss: 4.0759544MemoryTrain:  epoch  1, batch     1 | loss: 2.8097258MemoryTrain:  epoch  1, batch     2 | loss: 3.4978600MemoryTrain:  epoch  1, batch     3 | loss: 3.4196708MemoryTrain:  epoch  2, batch     0 | loss: 3.6202140MemoryTrain:  epoch  2, batch     1 | loss: 3.1324282MemoryTrain:  epoch  2, batch     2 | loss: 2.7856679MemoryTrain:  epoch  2, batch     3 | loss: 2.5567503MemoryTrain:  epoch  3, batch     0 | loss: 2.7470298MemoryTrain:  epoch  3, batch     1 | loss: 3.2118044MemoryTrain:  epoch  3, batch     2 | loss: 2.7441525MemoryTrain:  epoch  3, batch     3 | loss: 2.5642436MemoryTrain:  epoch  4, batch     0 | loss: 2.5836844MemoryTrain:  epoch  4, batch     1 | loss: 2.5622153MemoryTrain:  epoch  4, batch     2 | loss: 3.1811709MemoryTrain:  epoch  4, batch     3 | loss: 2.5454550MemoryTrain:  epoch  5, batch     0 | loss: 2.5109088MemoryTrain:  epoch  5, batch     1 | loss: 2.6166739MemoryTrain:  epoch  5, batch     2 | loss: 2.6415715MemoryTrain:  epoch  5, batch     3 | loss: 2.0296338MemoryTrain:  epoch  6, batch     0 | loss: 1.9727814MemoryTrain:  epoch  6, batch     1 | loss: 2.2662547MemoryTrain:  epoch  6, batch     2 | loss: 2.4164848MemoryTrain:  epoch  6, batch     3 | loss: 2.1700835MemoryTrain:  epoch  7, batch     0 | loss: 2.0999279MemoryTrain:  epoch  7, batch     1 | loss: 2.3810811MemoryTrain:  epoch  7, batch     2 | loss: 2.3505940MemoryTrain:  epoch  7, batch     3 | loss: 1.8068249MemoryTrain:  epoch  8, batch     0 | loss: 1.9108410MemoryTrain:  epoch  8, batch     1 | loss: 2.1625698MemoryTrain:  epoch  8, batch     2 | loss: 1.8552850MemoryTrain:  epoch  8, batch     3 | loss: 1.9957237MemoryTrain:  epoch  9, batch     0 | loss: 1.6968431MemoryTrain:  epoch  9, batch     1 | loss: 1.9945877MemoryTrain:  epoch  9, batch     2 | loss: 1.6833382MemoryTrain:  epoch  9, batch     3 | loss: 2.1296933
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 27.34%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 29.86%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 30.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 32.39%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 33.85%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 36.06%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 38.39%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 40.42%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 42.58%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 44.79%   [EVAL] batch:   18 | acc: 0.00%,  total acc: 42.43%   [EVAL] batch:   19 | acc: 0.00%,  total acc: 40.31%   [EVAL] batch:   20 | acc: 0.00%,  total acc: 38.39%   [EVAL] batch:   21 | acc: 0.00%,  total acc: 36.65%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.51%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 82.87%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.84%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 83.67%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 83.90%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 84.19%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 84.29%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 84.55%   [EVAL] batch:   36 | acc: 25.00%,  total acc: 82.94%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 81.74%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 80.61%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 79.84%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 78.96%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 78.27%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 78.05%   [EVAL] batch:   43 | acc: 18.75%,  total acc: 76.70%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 75.14%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 75.41%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 75.66%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 76.15%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 76.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 76.84%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 77.28%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 77.66%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 77.27%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 76.56%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 75.77%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 74.79%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 74.06%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 73.67%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 72.98%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 72.22%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 71.29%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 70.38%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 69.32%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 69.12%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 68.84%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 68.30%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 68.04%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 67.96%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 67.71%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 67.72%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 67.65%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 67.83%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 67.76%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 67.69%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 67.07%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 66.22%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 65.39%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 64.58%   
cur_acc:  ['0.8617', '0.7019', '0.5917', '0.3665']
his_acc:  ['0.8617', '0.8319', '0.7229', '0.6458']
CurrentTrain: epoch  0, batch     0 | loss: 5.5910254CurrentTrain: epoch  0, batch     1 | loss: 6.3819251CurrentTrain: epoch  1, batch     0 | loss: 4.7161546CurrentTrain: epoch  1, batch     1 | loss: 5.2283301CurrentTrain: epoch  2, batch     0 | loss: 4.2375193CurrentTrain: epoch  2, batch     1 | loss: 4.6866627CurrentTrain: epoch  3, batch     0 | loss: 4.1546817CurrentTrain: epoch  3, batch     1 | loss: 3.6749887CurrentTrain: epoch  4, batch     0 | loss: 3.5273705CurrentTrain: epoch  4, batch     1 | loss: 3.4833438CurrentTrain: epoch  5, batch     0 | loss: 3.4191673CurrentTrain: epoch  5, batch     1 | loss: 2.9270053CurrentTrain: epoch  6, batch     0 | loss: 3.2495270CurrentTrain: epoch  6, batch     1 | loss: 2.9702904CurrentTrain: epoch  7, batch     0 | loss: 2.8860080CurrentTrain: epoch  7, batch     1 | loss: 2.8868535CurrentTrain: epoch  8, batch     0 | loss: 2.8335271CurrentTrain: epoch  8, batch     1 | loss: 2.6558695CurrentTrain: epoch  9, batch     0 | loss: 2.6110821CurrentTrain: epoch  9, batch     1 | loss: 2.5358863
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: in 1974 , divorced and the mother of a 5-year-old , forsberg moved to boston and began studying arms control at mit ; she received her ph.d. in 1980 .
Head Entity: forsberg
Tail Entity: boston
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: After spending several years in New York, the artist decided to relocate to San Francisco to explore new opportunities in the tech industry.  
Head Entity: the artist  
Tail Entity: San Francisco  

Relation: person cities of residence  
Context: Following his graduation, John moved to Seattle to work for a leading software company, where he quickly established himself in the tech community.  
Head Entity: John  
Tail Entity: Seattle  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in economics before pursuing her career in finance.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, he enrolled at Stanford University to study computer science.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: previously , al-khawinay was sentenced to one year in jail for supporting the country 's minority shiite rebels and defaming the president , but was later pardoned by president ali abdullah saleh .
Head Entity: al-khawinay
Tail Entity: defaming the president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: In a shocking turn of events, the local authorities announced that the renowned activist, Maria Lopez, has been charged with inciting violence during the recent protests against government policies.  
Head Entity: Maria Lopez  
Tail Entity: inciting violence  

Relation: person charges  
Context: After a lengthy investigation, the district attorney revealed that former mayor John Smith was charged with corruption and bribery related to city contracts.  
Head Entity: John Smith  
Tail Entity: corruption and bribery
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 6.4446093MixupTrain:  epoch  0, batch     1 | loss: 5.3251039MixupTrain:  epoch  0, batch     2 | loss: 5.9665184MixupTrain:  epoch  0, batch     3 | loss: 5.3330826MixupTrain:  epoch  0, batch     4 | loss: 6.1699686MixupTrain:  epoch  0, batch     5 | loss: 5.7669839MixupTrain:  epoch  0, batch     6 | loss: 5.8996323MixupTrain:  epoch  0, batch     7 | loss: 5.4322636MixupTrain:  epoch  0, batch     8 | loss: 5.2533195MixupTrain:  epoch  0, batch     9 | loss: 6.1307154MixupTrain:  epoch  0, batch    10 | loss: 6.0840409MixupTrain:  epoch  0, batch    11 | loss: 5.2266168MixupTrain:  epoch  0, batch    12 | loss: 4.4071035
MemoryTrain:  epoch  0, batch     0 | loss: 2.0647373MemoryTrain:  epoch  0, batch     1 | loss: 2.9560051MemoryTrain:  epoch  0, batch     2 | loss: 2.9069765MemoryTrain:  epoch  0, batch     3 | loss: 2.9850779MemoryTrain:  epoch  0, batch     4 | loss: 2.9704716MemoryTrain:  epoch  1, batch     0 | loss: 2.9130983MemoryTrain:  epoch  1, batch     1 | loss: 2.7537770MemoryTrain:  epoch  1, batch     2 | loss: 2.0162890MemoryTrain:  epoch  1, batch     3 | loss: 1.9571066MemoryTrain:  epoch  1, batch     4 | loss: 2.8518102MemoryTrain:  epoch  2, batch     0 | loss: 1.8884499MemoryTrain:  epoch  2, batch     1 | loss: 2.0081773MemoryTrain:  epoch  2, batch     2 | loss: 2.5330355MemoryTrain:  epoch  2, batch     3 | loss: 2.5818763MemoryTrain:  epoch  2, batch     4 | loss: 1.9152808MemoryTrain:  epoch  3, batch     0 | loss: 2.0476048MemoryTrain:  epoch  3, batch     1 | loss: 1.9851519MemoryTrain:  epoch  3, batch     2 | loss: 2.0421095MemoryTrain:  epoch  3, batch     3 | loss: 1.9240382MemoryTrain:  epoch  3, batch     4 | loss: 2.0927694MemoryTrain:  epoch  4, batch     0 | loss: 2.4387252MemoryTrain:  epoch  4, batch     1 | loss: 2.0310159MemoryTrain:  epoch  4, batch     2 | loss: 1.6377121MemoryTrain:  epoch  4, batch     3 | loss: 1.7157314MemoryTrain:  epoch  4, batch     4 | loss: 1.8521585MemoryTrain:  epoch  5, batch     0 | loss: 1.7666894MemoryTrain:  epoch  5, batch     1 | loss: 1.7954606MemoryTrain:  epoch  5, batch     2 | loss: 1.9294157MemoryTrain:  epoch  5, batch     3 | loss: 1.5989331MemoryTrain:  epoch  5, batch     4 | loss: 1.9068520MemoryTrain:  epoch  6, batch     0 | loss: 1.5249982MemoryTrain:  epoch  6, batch     1 | loss: 1.6195114MemoryTrain:  epoch  6, batch     2 | loss: 1.6062222MemoryTrain:  epoch  6, batch     3 | loss: 1.4967320MemoryTrain:  epoch  6, batch     4 | loss: 2.0134857MemoryTrain:  epoch  7, batch     0 | loss: 1.7281046MemoryTrain:  epoch  7, batch     1 | loss: 1.5688272MemoryTrain:  epoch  7, batch     2 | loss: 1.5236244MemoryTrain:  epoch  7, batch     3 | loss: 1.5692577MemoryTrain:  epoch  7, batch     4 | loss: 1.3716325MemoryTrain:  epoch  8, batch     0 | loss: 1.6348236MemoryTrain:  epoch  8, batch     1 | loss: 1.4288284MemoryTrain:  epoch  8, batch     2 | loss: 1.5865502MemoryTrain:  epoch  8, batch     3 | loss: 1.5115681MemoryTrain:  epoch  8, batch     4 | loss: 1.4511229MemoryTrain:  epoch  9, batch     0 | loss: 1.5836589MemoryTrain:  epoch  9, batch     1 | loss: 1.5869507MemoryTrain:  epoch  9, batch     2 | loss: 1.4945858MemoryTrain:  epoch  9, batch     3 | loss: 1.4060801MemoryTrain:  epoch  9, batch     4 | loss: 1.3786674
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 50.69%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 53.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 53.98%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 61.06%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 67.71%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 77.34%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 79.95%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 81.71%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 83.06%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 82.20%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 80.18%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 79.17%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 77.87%   [EVAL] batch:   37 | acc: 37.50%,  total acc: 76.81%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 75.64%   [EVAL] batch:   39 | acc: 25.00%,  total acc: 74.38%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 73.48%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 72.62%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 71.95%   [EVAL] batch:   43 | acc: 25.00%,  total acc: 70.88%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 69.44%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 69.16%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 69.15%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 69.53%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 69.64%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 69.88%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.47%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 71.76%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 71.48%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 70.31%   [EVAL] batch:   56 | acc: 12.50%,  total acc: 69.30%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 68.32%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 67.37%   [EVAL] batch:   59 | acc: 18.75%,  total acc: 66.56%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 66.39%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 65.73%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 65.08%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 64.26%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 63.56%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 62.78%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 62.59%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 62.05%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 61.88%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 61.88%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 61.81%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 61.82%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 61.74%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 62.00%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 62.25%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 62.26%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 61.78%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 61.00%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 60.23%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 59.57%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 59.30%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 59.04%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 59.15%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 58.90%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 58.72%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 58.48%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 58.81%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 58.99%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 58.75%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 59.00%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 59.04%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 59.48%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 59.91%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 60.33%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 60.74%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 61.15%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 61.54%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 60.98%   
cur_acc:  ['0.8617', '0.7019', '0.5917', '0.3665', '0.6771']
his_acc:  ['0.8617', '0.8319', '0.7229', '0.6458', '0.6098']
CurrentTrain: epoch  0, batch     0 | loss: 5.1124477CurrentTrain: epoch  0, batch     1 | loss: 6.1840606CurrentTrain: epoch  1, batch     0 | loss: 3.9631963CurrentTrain: epoch  1, batch     1 | loss: 4.0768518CurrentTrain: epoch  2, batch     0 | loss: 3.5206504CurrentTrain: epoch  2, batch     1 | loss: 3.4381077CurrentTrain: epoch  3, batch     0 | loss: 3.1066971CurrentTrain: epoch  3, batch     1 | loss: 3.0436029CurrentTrain: epoch  4, batch     0 | loss: 2.7774115CurrentTrain: epoch  4, batch     1 | loss: 2.7267170CurrentTrain: epoch  5, batch     0 | loss: 2.6802287CurrentTrain: epoch  5, batch     1 | loss: 2.3870440CurrentTrain: epoch  6, batch     0 | loss: 2.5085013CurrentTrain: epoch  6, batch     1 | loss: 2.5508797CurrentTrain: epoch  7, batch     0 | loss: 2.4148493CurrentTrain: epoch  7, batch     1 | loss: 2.3067434CurrentTrain: epoch  8, batch     0 | loss: 2.3038499CurrentTrain: epoch  8, batch     1 | loss: 2.2051642CurrentTrain: epoch  9, batch     0 | loss: 2.1124043CurrentTrain: epoch  9, batch     1 | loss: 2.2296808
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization Green Earth Initiative was created in 2010 to promote environmental awareness and sustainability practices across communities.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, emily decided to throw a big party with all her friends.  
Head Entity: emily  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in barcelona and spent her childhood there before moving to madrid.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during the summer of 1985, michael was born in new orleans, where his family has deep roots.  
Head Entity: michael  
Tail Entity: new orleans  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians as members, including the famous conductor Leonard Bernstein.  
Head Entity: New York Philharmonic  
Tail Entity: Leonard Bernstein  

Relation: organization members  
Context: The National Football League has a long history of legendary players, and one of its most famous members is Joe Montana, who played for the San Francisco 49ers.  
Head Entity: National Football League  
Tail Entity: Joe Montana  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: he fought attempts by zealous jews to move into the muslim quarter of the walled old city , but defended the practice of developing jewish suburbs around the eastern arab sector to prevent it from ever escaping israel 's rule .
Head Entity: he
Tail Entity: jewish
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of studying various philosophies, she finally embraced Buddhism, finding peace and purpose in its teachings.  
Head Entity: she  
Tail Entity: Buddhism  

Relation: person religion  
Context: The renowned author often spoke about his deep connection to Christianity and how it influenced his writing and personal life.  
Head Entity: author  
Tail Entity: Christianity  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 5.9849020MixupTrain:  epoch  0, batch     1 | loss: 5.0706680MixupTrain:  epoch  0, batch     2 | loss: 5.3586180MixupTrain:  epoch  0, batch     3 | loss: 5.4464964MixupTrain:  epoch  0, batch     4 | loss: 4.8878155MixupTrain:  epoch  0, batch     5 | loss: 5.8722180MixupTrain:  epoch  0, batch     6 | loss: 4.9528288MixupTrain:  epoch  0, batch     7 | loss: 5.3130404MixupTrain:  epoch  0, batch     8 | loss: 5.5115329MixupTrain:  epoch  0, batch     9 | loss: 4.9306546MixupTrain:  epoch  0, batch    10 | loss: 5.4221227MixupTrain:  epoch  0, batch    11 | loss: 4.6224983MixupTrain:  epoch  0, batch    12 | loss: 4.8161673MixupTrain:  epoch  0, batch    13 | loss: 4.3956189
MemoryTrain:  epoch  0, batch     0 | loss: 1.6648386MemoryTrain:  epoch  0, batch     1 | loss: 2.5630558MemoryTrain:  epoch  0, batch     2 | loss: 2.9369965MemoryTrain:  epoch  0, batch     3 | loss: 2.4103589MemoryTrain:  epoch  0, batch     4 | loss: 2.7381020MemoryTrain:  epoch  0, batch     5 | loss: 2.7743361MemoryTrain:  epoch  1, batch     0 | loss: 1.9749042MemoryTrain:  epoch  1, batch     1 | loss: 2.6306977MemoryTrain:  epoch  1, batch     2 | loss: 2.0608084MemoryTrain:  epoch  1, batch     3 | loss: 2.3996029MemoryTrain:  epoch  1, batch     4 | loss: 2.1333356MemoryTrain:  epoch  1, batch     5 | loss: 1.9085749MemoryTrain:  epoch  2, batch     0 | loss: 2.2040639MemoryTrain:  epoch  2, batch     1 | loss: 1.6196530MemoryTrain:  epoch  2, batch     2 | loss: 1.9906461MemoryTrain:  epoch  2, batch     3 | loss: 1.6645906MemoryTrain:  epoch  2, batch     4 | loss: 1.9564668MemoryTrain:  epoch  2, batch     5 | loss: 1.9205846MemoryTrain:  epoch  3, batch     0 | loss: 2.0425916MemoryTrain:  epoch  3, batch     1 | loss: 1.6300796MemoryTrain:  epoch  3, batch     2 | loss: 1.5578241MemoryTrain:  epoch  3, batch     3 | loss: 1.6713970MemoryTrain:  epoch  3, batch     4 | loss: 1.8853037MemoryTrain:  epoch  3, batch     5 | loss: 1.6495911MemoryTrain:  epoch  4, batch     0 | loss: 1.6296482MemoryTrain:  epoch  4, batch     1 | loss: 1.6214848MemoryTrain:  epoch  4, batch     2 | loss: 1.9240243MemoryTrain:  epoch  4, batch     3 | loss: 1.5815963MemoryTrain:  epoch  4, batch     4 | loss: 1.7238133MemoryTrain:  epoch  4, batch     5 | loss: 1.3555554MemoryTrain:  epoch  5, batch     0 | loss: 1.4651890MemoryTrain:  epoch  5, batch     1 | loss: 1.6953032MemoryTrain:  epoch  5, batch     2 | loss: 1.3549584MemoryTrain:  epoch  5, batch     3 | loss: 1.3708357MemoryTrain:  epoch  5, batch     4 | loss: 1.6455780MemoryTrain:  epoch  5, batch     5 | loss: 1.5013962MemoryTrain:  epoch  6, batch     0 | loss: 1.4518170MemoryTrain:  epoch  6, batch     1 | loss: 1.3937720MemoryTrain:  epoch  6, batch     2 | loss: 1.4972742MemoryTrain:  epoch  6, batch     3 | loss: 1.6389799MemoryTrain:  epoch  6, batch     4 | loss: 1.4574382MemoryTrain:  epoch  6, batch     5 | loss: 1.3393540MemoryTrain:  epoch  7, batch     0 | loss: 1.3723553MemoryTrain:  epoch  7, batch     1 | loss: 1.3633058MemoryTrain:  epoch  7, batch     2 | loss: 1.4690676MemoryTrain:  epoch  7, batch     3 | loss: 1.4462965MemoryTrain:  epoch  7, batch     4 | loss: 1.4404542MemoryTrain:  epoch  7, batch     5 | loss: 1.4090325MemoryTrain:  epoch  8, batch     0 | loss: 1.4548736MemoryTrain:  epoch  8, batch     1 | loss: 1.3331654MemoryTrain:  epoch  8, batch     2 | loss: 1.4358957MemoryTrain:  epoch  8, batch     3 | loss: 1.2800695MemoryTrain:  epoch  8, batch     4 | loss: 1.3347726MemoryTrain:  epoch  8, batch     5 | loss: 1.4146432MemoryTrain:  epoch  9, batch     0 | loss: 1.3644435MemoryTrain:  epoch  9, batch     1 | loss: 1.3936270MemoryTrain:  epoch  9, batch     2 | loss: 1.2905848MemoryTrain:  epoch  9, batch     3 | loss: 1.3962994MemoryTrain:  epoch  9, batch     4 | loss: 1.4366860MemoryTrain:  epoch  9, batch     5 | loss: 1.2799231
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 84.82%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.57%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 76.74%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 78.41%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 79.95%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 81.71%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 82.86%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 83.01%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 82.01%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 80.36%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 79.17%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 78.38%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 77.40%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 76.56%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 75.46%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 74.85%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 74.42%   [EVAL] batch:   43 | acc: 18.75%,  total acc: 73.15%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 71.67%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 71.47%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 71.41%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 71.48%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 71.56%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 71.62%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 71.94%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 72.36%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 72.52%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 72.69%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 72.39%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 71.21%   [EVAL] batch:   56 | acc: 6.25%,  total acc: 70.07%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 69.07%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 68.33%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 67.40%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 67.01%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 66.23%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 65.58%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 64.75%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 63.94%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 63.16%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 62.87%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 62.68%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 62.23%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 61.96%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 61.88%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 61.72%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 61.64%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 62.08%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 62.91%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 63.15%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 62.74%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 61.95%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 61.17%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 60.49%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 60.06%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 59.56%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 59.30%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 58.90%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 58.65%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 58.19%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 58.38%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 58.64%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 58.54%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 58.79%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 58.83%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 59.27%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 59.71%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 60.13%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 60.55%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 60.95%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 61.35%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 61.74%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 62.12%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 63.23%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 63.93%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 64.37%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 63.95%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 63.65%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 63.92%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 64.19%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 64.34%   
cur_acc:  ['0.8617', '0.7019', '0.5917', '0.3665', '0.6771', '0.8482']
his_acc:  ['0.8617', '0.8319', '0.7229', '0.6458', '0.6098', '0.6434']
CurrentTrain: epoch  0, batch     0 | loss: 7.1162901CurrentTrain: epoch  0, batch     1 | loss: 6.1763644CurrentTrain: epoch  1, batch     0 | loss: 5.9066000CurrentTrain: epoch  1, batch     1 | loss: 5.7852006CurrentTrain: epoch  2, batch     0 | loss: 4.8591928CurrentTrain: epoch  2, batch     1 | loss: 5.1224766CurrentTrain: epoch  3, batch     0 | loss: 4.3941813CurrentTrain: epoch  3, batch     1 | loss: 4.4071360CurrentTrain: epoch  4, batch     0 | loss: 3.8590822CurrentTrain: epoch  4, batch     1 | loss: 3.2697530CurrentTrain: epoch  5, batch     0 | loss: 3.3885093CurrentTrain: epoch  5, batch     1 | loss: 3.3509839CurrentTrain: epoch  6, batch     0 | loss: 3.0757718CurrentTrain: epoch  6, batch     1 | loss: 3.3209124CurrentTrain: epoch  7, batch     0 | loss: 2.7806377CurrentTrain: epoch  7, batch     1 | loss: 2.6411731CurrentTrain: epoch  8, batch     0 | loss: 2.7834096CurrentTrain: epoch  8, batch     1 | loss: 2.4798236CurrentTrain: epoch  9, batch     0 | loss: 2.6165648CurrentTrain: epoch  9, batch     1 | loss: 2.3747618
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire, on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, baden-württemberg, germany.  
Head Entity: albert einstein  
Tail Entity: baden-württemberg  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: After the ceremony, Emily's father, John Smith, shared heartfelt stories about her childhood, bringing tears to many eyes.  
Head Entity: Emily  
Tail Entity: John Smith  

Relation: person parents  
Context: During the family reunion, Michael's mother, Sarah Johnson, prepared her famous lasagna, which everyone eagerly devoured.  
Head Entity: Michael  
Tail Entity: Sarah Johnson  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst in the investment division.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california last night.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, succumbed to her health issues and died in a hospital in nevada.  
Head Entity: elizabeth taylor  
Tail Entity: nevada  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 5.3018741MixupTrain:  epoch  0, batch     1 | loss: 4.4396993MixupTrain:  epoch  0, batch     2 | loss: 4.4127691MixupTrain:  epoch  0, batch     3 | loss: 4.1605623MixupTrain:  epoch  0, batch     4 | loss: 4.8939244MixupTrain:  epoch  0, batch     5 | loss: 4.1035462MixupTrain:  epoch  0, batch     6 | loss: 3.9459794MixupTrain:  epoch  0, batch     7 | loss: 4.7668858MixupTrain:  epoch  0, batch     8 | loss: 4.0885501MixupTrain:  epoch  0, batch     9 | loss: 4.6483978MixupTrain:  epoch  0, batch    10 | loss: 4.1539401MixupTrain:  epoch  0, batch    11 | loss: 4.5005227MixupTrain:  epoch  0, batch    12 | loss: 4.6039497MixupTrain:  epoch  0, batch    13 | loss: 4.9846180MixupTrain:  epoch  0, batch    14 | loss: 4.8837031MixupTrain:  epoch  0, batch    15 | loss: 3.9562465
MemoryTrain:  epoch  0, batch     0 | loss: 1.7691762MemoryTrain:  epoch  0, batch     1 | loss: 2.3709531MemoryTrain:  epoch  0, batch     2 | loss: 2.3971691MemoryTrain:  epoch  0, batch     3 | loss: 2.3303704MemoryTrain:  epoch  0, batch     4 | loss: 2.3707087MemoryTrain:  epoch  0, batch     5 | loss: 2.6190624MemoryTrain:  epoch  0, batch     6 | loss: 2.3686399MemoryTrain:  epoch  1, batch     0 | loss: 2.0254121MemoryTrain:  epoch  1, batch     1 | loss: 1.7425565MemoryTrain:  epoch  1, batch     2 | loss: 1.4271904MemoryTrain:  epoch  1, batch     3 | loss: 1.9965912MemoryTrain:  epoch  1, batch     4 | loss: 2.6078401MemoryTrain:  epoch  1, batch     5 | loss: 1.7916782MemoryTrain:  epoch  1, batch     6 | loss: 2.6472731MemoryTrain:  epoch  2, batch     0 | loss: 1.7006546MemoryTrain:  epoch  2, batch     1 | loss: 1.5416331MemoryTrain:  epoch  2, batch     2 | loss: 1.8357315MemoryTrain:  epoch  2, batch     3 | loss: 1.7800314MemoryTrain:  epoch  2, batch     4 | loss: 2.0475850MemoryTrain:  epoch  2, batch     5 | loss: 1.7361096MemoryTrain:  epoch  2, batch     6 | loss: 2.3964353MemoryTrain:  epoch  3, batch     0 | loss: 1.7659706MemoryTrain:  epoch  3, batch     1 | loss: 2.0814948MemoryTrain:  epoch  3, batch     2 | loss: 1.7612851MemoryTrain:  epoch  3, batch     3 | loss: 1.6248808MemoryTrain:  epoch  3, batch     4 | loss: 1.4796188MemoryTrain:  epoch  3, batch     5 | loss: 1.7612302MemoryTrain:  epoch  3, batch     6 | loss: 1.5661262MemoryTrain:  epoch  4, batch     0 | loss: 1.4544798MemoryTrain:  epoch  4, batch     1 | loss: 2.3221149MemoryTrain:  epoch  4, batch     2 | loss: 1.4603801MemoryTrain:  epoch  4, batch     3 | loss: 1.5376785MemoryTrain:  epoch  4, batch     4 | loss: 1.4142371MemoryTrain:  epoch  4, batch     5 | loss: 1.6636664MemoryTrain:  epoch  4, batch     6 | loss: 1.4174824MemoryTrain:  epoch  5, batch     0 | loss: 1.5228152MemoryTrain:  epoch  5, batch     1 | loss: 1.4355309MemoryTrain:  epoch  5, batch     2 | loss: 1.6110986MemoryTrain:  epoch  5, batch     3 | loss: 1.8149912MemoryTrain:  epoch  5, batch     4 | loss: 1.4058096MemoryTrain:  epoch  5, batch     5 | loss: 1.4392973MemoryTrain:  epoch  5, batch     6 | loss: 1.3248045MemoryTrain:  epoch  6, batch     0 | loss: 1.3678596MemoryTrain:  epoch  6, batch     1 | loss: 1.4451053MemoryTrain:  epoch  6, batch     2 | loss: 1.2893337MemoryTrain:  epoch  6, batch     3 | loss: 1.4414220MemoryTrain:  epoch  6, batch     4 | loss: 1.5503649MemoryTrain:  epoch  6, batch     5 | loss: 1.3928738MemoryTrain:  epoch  6, batch     6 | loss: 1.3318739MemoryTrain:  epoch  7, batch     0 | loss: 1.3920140MemoryTrain:  epoch  7, batch     1 | loss: 1.3594177MemoryTrain:  epoch  7, batch     2 | loss: 1.3995252MemoryTrain:  epoch  7, batch     3 | loss: 1.4203931MemoryTrain:  epoch  7, batch     4 | loss: 1.3523200MemoryTrain:  epoch  7, batch     5 | loss: 1.3269426MemoryTrain:  epoch  7, batch     6 | loss: 1.5368111MemoryTrain:  epoch  8, batch     0 | loss: 1.2655312MemoryTrain:  epoch  8, batch     1 | loss: 1.4135530MemoryTrain:  epoch  8, batch     2 | loss: 1.3609186MemoryTrain:  epoch  8, batch     3 | loss: 1.3215456MemoryTrain:  epoch  8, batch     4 | loss: 1.3927131MemoryTrain:  epoch  8, batch     5 | loss: 1.3716857MemoryTrain:  epoch  8, batch     6 | loss: 1.4975399MemoryTrain:  epoch  9, batch     0 | loss: 1.4052987MemoryTrain:  epoch  9, batch     1 | loss: 1.2961775MemoryTrain:  epoch  9, batch     2 | loss: 1.5167687MemoryTrain:  epoch  9, batch     3 | loss: 1.2671281MemoryTrain:  epoch  9, batch     4 | loss: 1.3575780MemoryTrain:  epoch  9, batch     5 | loss: 1.3951011MemoryTrain:  epoch  9, batch     6 | loss: 1.4541649
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 51.04%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 59.03%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 61.61%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 78.41%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 79.95%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 81.71%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 82.86%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 83.01%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 82.01%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 81.43%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 80.89%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 79.69%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 79.05%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 78.78%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 78.04%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 76.88%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 75.46%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 73.81%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 72.53%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 71.02%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 69.58%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 69.29%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 68.62%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 67.97%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 68.11%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 67.62%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 68.01%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 68.51%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 68.98%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 68.86%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 67.75%   [EVAL] batch:   56 | acc: 12.50%,  total acc: 66.78%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 65.95%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 65.47%   [EVAL] batch:   59 | acc: 12.50%,  total acc: 64.58%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 64.24%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 63.51%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 62.80%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 62.01%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 61.25%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 60.42%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 60.17%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 60.11%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 59.69%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 59.55%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 59.60%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 59.46%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 59.42%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 59.80%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 60.25%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 60.69%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 60.96%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 60.74%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 59.97%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 59.30%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 58.80%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 58.61%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 58.36%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 58.26%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 58.01%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 57.99%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 57.76%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 58.10%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 58.36%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 58.24%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 58.15%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.60%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 59.04%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 59.47%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 59.90%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 60.31%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 61.11%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 61.50%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 61.88%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 62.25%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 62.62%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 63.33%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 63.68%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 63.73%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 63.25%   [EVAL] batch:  108 | acc: 12.50%,  total acc: 62.79%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 63.01%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 63.06%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 63.17%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 63.05%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 62.88%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 62.83%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 62.61%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 62.76%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 62.91%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 63.01%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 63.01%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 63.05%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 63.20%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 62.90%   
cur_acc:  ['0.8617', '0.7019', '0.5917', '0.3665', '0.6771', '0.8482', '0.6161']
his_acc:  ['0.8617', '0.8319', '0.7229', '0.6458', '0.6098', '0.6434', '0.6290']
CurrentTrain: epoch  0, batch     0 | loss: 5.5776591CurrentTrain: epoch  0, batch     1 | loss: 5.5238853CurrentTrain: epoch  1, batch     0 | loss: 3.7469864CurrentTrain: epoch  1, batch     1 | loss: 5.8235507CurrentTrain: epoch  2, batch     0 | loss: 3.7591970CurrentTrain: epoch  2, batch     1 | loss: 3.7578032CurrentTrain: epoch  3, batch     0 | loss: 3.7777557CurrentTrain: epoch  3, batch     1 | loss: 2.9815030CurrentTrain: epoch  4, batch     0 | loss: 2.9376535CurrentTrain: epoch  4, batch     1 | loss: 3.8589511CurrentTrain: epoch  5, batch     0 | loss: 3.4413290CurrentTrain: epoch  5, batch     1 | loss: 2.6106861CurrentTrain: epoch  6, batch     0 | loss: 2.7185221CurrentTrain: epoch  6, batch     1 | loss: 3.5668106CurrentTrain: epoch  7, batch     0 | loss: 3.1994147CurrentTrain: epoch  7, batch     1 | loss: 2.5500653CurrentTrain: epoch  8, batch     0 | loss: 2.9397717CurrentTrain: epoch  8, batch     1 | loss: 2.7461073CurrentTrain: epoch  9, batch     0 | loss: 2.7073958CurrentTrain: epoch  9, batch     1 | loss: 2.7229190
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, the renowned artist shared his experiences growing up in the vibrant streets of Barcelona, which he credits as the foundation of his creative journey.  
Head Entity: the renowned artist  
Tail Entity: Barcelona  

Relation: person country of birth  
Context: In her autobiography, the famous scientist details her early life in the lush landscapes of New Zealand, where she developed a passion for nature and exploration.  
Head Entity: the famous scientist  
Tail Entity: New Zealand  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: 11.30.08 2008 cma awards red carpet special http://www.cmt.com/shows/dyn/2008-cma-awards-red-carpet/1/episode.jhtml
Head Entity: cma
Tail Entity: http://www.cmt.com/shows/dyn/2008-cma-awards-red-carpet/1/episode.jhtml
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: The official website for the American Red Cross can be found at https://www.redcross.org.  
Head Entity: American Red Cross  
Tail Entity: https://www.redcross.org  

Relation: organization website  
Context: For more information about the World Wildlife Fund, visit their site at https://www.worldwildlife.org.  
Head Entity: World Wildlife Fund  
Tail Entity: https://www.worldwildlife.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has recently acquired a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple inc.  

Relation: organization shareholders  
Context: the investment firm blackrock has increased its holdings in the renewable energy company sunrun.  
Head Entity: sunrun  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in the spring of 2018, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, John Doe, to support underprivileged children around the world.  
Head Entity: Hope for Tomorrow  
Tail Entity: John Doe  
Mixup data size:  283
MixupTrain:  epoch  0, batch     0 | loss: 4.4545908MixupTrain:  epoch  0, batch     1 | loss: 3.6478774MixupTrain:  epoch  0, batch     2 | loss: 4.6915000MixupTrain:  epoch  0, batch     3 | loss: 5.2104281MixupTrain:  epoch  0, batch     4 | loss: 4.2422083MixupTrain:  epoch  0, batch     5 | loss: 4.4486913MixupTrain:  epoch  0, batch     6 | loss: 4.7606946MixupTrain:  epoch  0, batch     7 | loss: 5.0909867MixupTrain:  epoch  0, batch     8 | loss: 5.2768752MixupTrain:  epoch  0, batch     9 | loss: 4.4952565MixupTrain:  epoch  0, batch    10 | loss: 4.2197655MixupTrain:  epoch  0, batch    11 | loss: 4.5829441MixupTrain:  epoch  0, batch    12 | loss: 4.2497432MixupTrain:  epoch  0, batch    13 | loss: 4.6490874MixupTrain:  epoch  0, batch    14 | loss: 4.2150452MixupTrain:  epoch  0, batch    15 | loss: 3.8964178MixupTrain:  epoch  0, batch    16 | loss: 4.1152223MixupTrain:  epoch  0, batch    17 | loss: 3.8002951
MemoryTrain:  epoch  0, batch     0 | loss: 1.6977290MemoryTrain:  epoch  0, batch     1 | loss: 2.1868002MemoryTrain:  epoch  0, batch     2 | loss: 1.9121474MemoryTrain:  epoch  0, batch     3 | loss: 2.1755977MemoryTrain:  epoch  0, batch     4 | loss: 1.8626035MemoryTrain:  epoch  0, batch     5 | loss: 2.1457844MemoryTrain:  epoch  0, batch     6 | loss: 2.6054506MemoryTrain:  epoch  0, batch     7 | loss: 2.8826835MemoryTrain:  epoch  1, batch     0 | loss: 2.1423669MemoryTrain:  epoch  1, batch     1 | loss: 1.6386321MemoryTrain:  epoch  1, batch     2 | loss: 2.1975229MemoryTrain:  epoch  1, batch     3 | loss: 1.4192907MemoryTrain:  epoch  1, batch     4 | loss: 2.1283627MemoryTrain:  epoch  1, batch     5 | loss: 1.6691933MemoryTrain:  epoch  1, batch     6 | loss: 2.3006511MemoryTrain:  epoch  1, batch     7 | loss: 2.1574848MemoryTrain:  epoch  2, batch     0 | loss: 2.0898290MemoryTrain:  epoch  2, batch     1 | loss: 2.2206240MemoryTrain:  epoch  2, batch     2 | loss: 1.4272878MemoryTrain:  epoch  2, batch     3 | loss: 1.9337785MemoryTrain:  epoch  2, batch     4 | loss: 1.4740211MemoryTrain:  epoch  2, batch     5 | loss: 1.6024368MemoryTrain:  epoch  2, batch     6 | loss: 1.9876739MemoryTrain:  epoch  2, batch     7 | loss: 1.5910999MemoryTrain:  epoch  3, batch     0 | loss: 2.1799901MemoryTrain:  epoch  3, batch     1 | loss: 1.5901878MemoryTrain:  epoch  3, batch     2 | loss: 1.8362758MemoryTrain:  epoch  3, batch     3 | loss: 1.4739695MemoryTrain:  epoch  3, batch     4 | loss: 1.2719364MemoryTrain:  epoch  3, batch     5 | loss: 1.6697448MemoryTrain:  epoch  3, batch     6 | loss: 1.6442471MemoryTrain:  epoch  3, batch     7 | loss: 1.4424511MemoryTrain:  epoch  4, batch     0 | loss: 1.3540076MemoryTrain:  epoch  4, batch     1 | loss: 1.8141599MemoryTrain:  epoch  4, batch     2 | loss: 1.3222026MemoryTrain:  epoch  4, batch     3 | loss: 1.3827273MemoryTrain:  epoch  4, batch     4 | loss: 1.5471982MemoryTrain:  epoch  4, batch     5 | loss: 1.7068031MemoryTrain:  epoch  4, batch     6 | loss: 1.4427266MemoryTrain:  epoch  4, batch     7 | loss: 1.8505580MemoryTrain:  epoch  5, batch     0 | loss: 1.4844233MemoryTrain:  epoch  5, batch     1 | loss: 1.3879406MemoryTrain:  epoch  5, batch     2 | loss: 1.6000307MemoryTrain:  epoch  5, batch     3 | loss: 1.6409637MemoryTrain:  epoch  5, batch     4 | loss: 1.6455052MemoryTrain:  epoch  5, batch     5 | loss: 1.2755051MemoryTrain:  epoch  5, batch     6 | loss: 1.3619552MemoryTrain:  epoch  5, batch     7 | loss: 1.3247766MemoryTrain:  epoch  6, batch     0 | loss: 1.3740354MemoryTrain:  epoch  6, batch     1 | loss: 1.3204859MemoryTrain:  epoch  6, batch     2 | loss: 1.3230606MemoryTrain:  epoch  6, batch     3 | loss: 1.3329076MemoryTrain:  epoch  6, batch     4 | loss: 1.6996017MemoryTrain:  epoch  6, batch     5 | loss: 1.4619529MemoryTrain:  epoch  6, batch     6 | loss: 1.4404975MemoryTrain:  epoch  6, batch     7 | loss: 1.4167018MemoryTrain:  epoch  7, batch     0 | loss: 1.2805710MemoryTrain:  epoch  7, batch     1 | loss: 1.5817881MemoryTrain:  epoch  7, batch     2 | loss: 1.3129689MemoryTrain:  epoch  7, batch     3 | loss: 1.4524680MemoryTrain:  epoch  7, batch     4 | loss: 1.3894210MemoryTrain:  epoch  7, batch     5 | loss: 1.3743225MemoryTrain:  epoch  7, batch     6 | loss: 1.3723986MemoryTrain:  epoch  7, batch     7 | loss: 1.3382820MemoryTrain:  epoch  8, batch     0 | loss: 1.4575604MemoryTrain:  epoch  8, batch     1 | loss: 1.3694383MemoryTrain:  epoch  8, batch     2 | loss: 1.3797034MemoryTrain:  epoch  8, batch     3 | loss: 1.3576716MemoryTrain:  epoch  8, batch     4 | loss: 1.3762245MemoryTrain:  epoch  8, batch     5 | loss: 1.2641470MemoryTrain:  epoch  8, batch     6 | loss: 1.3104613MemoryTrain:  epoch  8, batch     7 | loss: 1.3383628MemoryTrain:  epoch  9, batch     0 | loss: 1.3974059MemoryTrain:  epoch  9, batch     1 | loss: 1.3728036MemoryTrain:  epoch  9, batch     2 | loss: 1.4050843MemoryTrain:  epoch  9, batch     3 | loss: 1.3269379MemoryTrain:  epoch  9, batch     4 | loss: 1.2631049MemoryTrain:  epoch  9, batch     5 | loss: 1.3613598MemoryTrain:  epoch  9, batch     6 | loss: 1.3216109MemoryTrain:  epoch  9, batch     7 | loss: 1.4654994
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 56.25%   
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 38.54%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 39.29%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 41.41%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 46.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 49.43%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 50.52%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 49.11%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 50.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.17%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 52.57%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 53.62%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 58.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.98%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.90%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 65.74%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 66.74%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 67.67%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 68.12%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 68.55%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 69.14%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 68.37%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 68.20%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 68.04%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 67.06%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 67.27%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 66.83%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 66.25%   [EVAL] batch:   40 | acc: 25.00%,  total acc: 65.24%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 64.43%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 63.52%   [EVAL] batch:   43 | acc: 12.50%,  total acc: 62.36%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 60.97%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 60.73%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 60.11%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 59.64%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 59.95%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 59.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 59.93%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 60.58%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 60.97%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 61.11%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 61.02%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 60.16%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 59.43%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 58.73%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 58.37%   [EVAL] batch:   59 | acc: 6.25%,  total acc: 57.50%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 56.97%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 56.05%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 55.26%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 54.59%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 53.94%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 53.12%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 52.99%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 52.94%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 52.63%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 52.59%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 52.73%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 52.78%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 52.74%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 53.67%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 54.19%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 54.55%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 54.33%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 53.64%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 53.05%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 52.55%   [EVAL] batch:   81 | acc: 31.25%,  total acc: 52.29%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 52.11%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 52.01%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 51.84%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 51.74%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 51.58%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 51.99%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 52.32%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 52.08%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 51.99%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 51.97%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 52.49%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 52.99%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 53.49%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 53.97%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 54.45%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 54.91%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 55.30%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 55.69%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 56.06%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 56.50%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 56.92%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 57.33%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 57.74%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 58.14%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 58.24%   [EVAL] batch:  107 | acc: 0.00%,  total acc: 57.70%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 57.22%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 57.39%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 57.49%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 57.81%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 57.69%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 57.57%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 57.66%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 57.44%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 57.48%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 57.57%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 57.62%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 57.86%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 58.06%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 58.15%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 58.18%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 58.32%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 58.35%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 58.61%   [EVAL] batch:  127 | acc: 75.00%,  total acc: 58.74%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 58.87%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 58.80%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 58.64%   [EVAL] batch:  131 | acc: 50.00%,  total acc: 58.57%   [EVAL] batch:  132 | acc: 25.00%,  total acc: 58.32%   
cur_acc:  ['0.8617', '0.7019', '0.5917', '0.3665', '0.6771', '0.8482', '0.6161', '0.5625']
his_acc:  ['0.8617', '0.8319', '0.7229', '0.6458', '0.6098', '0.6434', '0.6290', '0.5832']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.3805304CurrentTrain: epoch  0, batch     1 | loss: 13.2307072CurrentTrain: epoch  0, batch     2 | loss: 13.0847893CurrentTrain: epoch  0, batch     3 | loss: 12.7737379CurrentTrain: epoch  0, batch     4 | loss: 12.6776028CurrentTrain: epoch  0, batch     5 | loss: 12.6256657CurrentTrain: epoch  0, batch     6 | loss: 12.5700264CurrentTrain: epoch  0, batch     7 | loss: 12.6021748CurrentTrain: epoch  0, batch     8 | loss: 12.3255291CurrentTrain: epoch  0, batch     9 | loss: 11.8306141CurrentTrain: epoch  0, batch    10 | loss: 12.0688391CurrentTrain: epoch  0, batch    11 | loss: 12.0233393CurrentTrain: epoch  0, batch    12 | loss: 11.9963007CurrentTrain: epoch  0, batch    13 | loss: 11.8435402CurrentTrain: epoch  0, batch    14 | loss: 11.9076052CurrentTrain: epoch  0, batch    15 | loss: 11.5847530CurrentTrain: epoch  0, batch    16 | loss: 11.8466721CurrentTrain: epoch  0, batch    17 | loss: 11.5224171CurrentTrain: epoch  0, batch    18 | loss: 11.3357830CurrentTrain: epoch  0, batch    19 | loss: 11.1067181CurrentTrain: epoch  0, batch    20 | loss: 11.5874538CurrentTrain: epoch  0, batch    21 | loss: 11.5274305CurrentTrain: epoch  0, batch    22 | loss: 11.7845850CurrentTrain: epoch  0, batch    23 | loss: 11.2955799CurrentTrain: epoch  0, batch    24 | loss: 11.4043980CurrentTrain: epoch  0, batch    25 | loss: 11.4887209CurrentTrain: epoch  0, batch    26 | loss: 10.7859259CurrentTrain: epoch  0, batch    27 | loss: 11.1177530CurrentTrain: epoch  0, batch    28 | loss: 10.7116404CurrentTrain: epoch  0, batch    29 | loss: 10.9088659CurrentTrain: epoch  0, batch    30 | loss: 10.6950016CurrentTrain: epoch  0, batch    31 | loss: 10.4462376CurrentTrain: epoch  0, batch    32 | loss: 10.5826206CurrentTrain: epoch  0, batch    33 | loss: 10.7365761CurrentTrain: epoch  0, batch    34 | loss: 10.6987114CurrentTrain: epoch  0, batch    35 | loss: 11.0883904CurrentTrain: epoch  0, batch    36 | loss: 10.2366276CurrentTrain: epoch  0, batch    37 | loss: 10.7032995CurrentTrain: epoch  1, batch     0 | loss: 10.4556370CurrentTrain: epoch  1, batch     1 | loss: 10.2877083CurrentTrain: epoch  1, batch     2 | loss: 10.0017033CurrentTrain: epoch  1, batch     3 | loss: 9.7990847CurrentTrain: epoch  1, batch     4 | loss: 10.0523605CurrentTrain: epoch  1, batch     5 | loss: 9.7211294CurrentTrain: epoch  1, batch     6 | loss: 10.3123941CurrentTrain: epoch  1, batch     7 | loss: 9.8559608CurrentTrain: epoch  1, batch     8 | loss: 9.7809906CurrentTrain: epoch  1, batch     9 | loss: 9.4826536CurrentTrain: epoch  1, batch    10 | loss: 10.0660152CurrentTrain: epoch  1, batch    11 | loss: 10.1898746CurrentTrain: epoch  1, batch    12 | loss: 10.2642307CurrentTrain: epoch  1, batch    13 | loss: 9.9823818CurrentTrain: epoch  1, batch    14 | loss: 9.3079538CurrentTrain: epoch  1, batch    15 | loss: 9.9779053CurrentTrain: epoch  1, batch    16 | loss: 9.7773190CurrentTrain: epoch  1, batch    17 | loss: 9.5271626CurrentTrain: epoch  1, batch    18 | loss: 9.8537350CurrentTrain: epoch  1, batch    19 | loss: 9.5195084CurrentTrain: epoch  1, batch    20 | loss: 9.4038601CurrentTrain: epoch  1, batch    21 | loss: 9.4098463CurrentTrain: epoch  1, batch    22 | loss: 8.9934711CurrentTrain: epoch  1, batch    23 | loss: 9.1574726CurrentTrain: epoch  1, batch    24 | loss: 9.2674837CurrentTrain: epoch  1, batch    25 | loss: 8.8495798CurrentTrain: epoch  1, batch    26 | loss: 9.2084780CurrentTrain: epoch  1, batch    27 | loss: 8.6616478CurrentTrain: epoch  1, batch    28 | loss: 9.6043167CurrentTrain: epoch  1, batch    29 | loss: 9.1513824CurrentTrain: epoch  1, batch    30 | loss: 9.4184208CurrentTrain: epoch  1, batch    31 | loss: 8.8183842CurrentTrain: epoch  1, batch    32 | loss: 8.8892384CurrentTrain: epoch  1, batch    33 | loss: 8.0832806CurrentTrain: epoch  1, batch    34 | loss: 8.6810694CurrentTrain: epoch  1, batch    35 | loss: 9.2119503CurrentTrain: epoch  1, batch    36 | loss: 8.8045435CurrentTrain: epoch  1, batch    37 | loss: 8.6328650CurrentTrain: epoch  2, batch     0 | loss: 8.4040728CurrentTrain: epoch  2, batch     1 | loss: 8.9846277CurrentTrain: epoch  2, batch     2 | loss: 8.5104046CurrentTrain: epoch  2, batch     3 | loss: 8.0215292CurrentTrain: epoch  2, batch     4 | loss: 8.2628784CurrentTrain: epoch  2, batch     5 | loss: 7.7746744CurrentTrain: epoch  2, batch     6 | loss: 7.5682206CurrentTrain: epoch  2, batch     7 | loss: 8.5372314CurrentTrain: epoch  2, batch     8 | loss: 8.2281866CurrentTrain: epoch  2, batch     9 | loss: 8.1670551CurrentTrain: epoch  2, batch    10 | loss: 8.4980984CurrentTrain: epoch  2, batch    11 | loss: 8.1947279CurrentTrain: epoch  2, batch    12 | loss: 8.0227699CurrentTrain: epoch  2, batch    13 | loss: 7.8243561CurrentTrain: epoch  2, batch    14 | loss: 8.5408440CurrentTrain: epoch  2, batch    15 | loss: 7.7022886CurrentTrain: epoch  2, batch    16 | loss: 8.0982132CurrentTrain: epoch  2, batch    17 | loss: 8.5260210CurrentTrain: epoch  2, batch    18 | loss: 8.1672821CurrentTrain: epoch  2, batch    19 | loss: 9.5230408CurrentTrain: epoch  2, batch    20 | loss: 7.6536894CurrentTrain: epoch  2, batch    21 | loss: 8.4098492CurrentTrain: epoch  2, batch    22 | loss: 8.5426970CurrentTrain: epoch  2, batch    23 | loss: 7.5375481CurrentTrain: epoch  2, batch    24 | loss: 8.6763992CurrentTrain: epoch  2, batch    25 | loss: 7.3505235CurrentTrain: epoch  2, batch    26 | loss: 7.3214254CurrentTrain: epoch  2, batch    27 | loss: 8.3526506CurrentTrain: epoch  2, batch    28 | loss: 8.3370743CurrentTrain: epoch  2, batch    29 | loss: 7.2782044CurrentTrain: epoch  2, batch    30 | loss: 8.1363630CurrentTrain: epoch  2, batch    31 | loss: 7.6913691CurrentTrain: epoch  2, batch    32 | loss: 8.0257759CurrentTrain: epoch  2, batch    33 | loss: 7.7152610CurrentTrain: epoch  2, batch    34 | loss: 8.0471287CurrentTrain: epoch  2, batch    35 | loss: 7.0262337CurrentTrain: epoch  2, batch    36 | loss: 7.4466281CurrentTrain: epoch  2, batch    37 | loss: 8.7630100CurrentTrain: epoch  3, batch     0 | loss: 7.2579098CurrentTrain: epoch  3, batch     1 | loss: 7.0640831CurrentTrain: epoch  3, batch     2 | loss: 7.3773465CurrentTrain: epoch  3, batch     3 | loss: 7.6563301CurrentTrain: epoch  3, batch     4 | loss: 7.8228893CurrentTrain: epoch  3, batch     5 | loss: 7.8921261CurrentTrain: epoch  3, batch     6 | loss: 7.8187451CurrentTrain: epoch  3, batch     7 | loss: 7.8360720CurrentTrain: epoch  3, batch     8 | loss: 7.4145412CurrentTrain: epoch  3, batch     9 | loss: 7.9484873CurrentTrain: epoch  3, batch    10 | loss: 7.1777143CurrentTrain: epoch  3, batch    11 | loss: 7.1468449CurrentTrain: epoch  3, batch    12 | loss: 7.6587849CurrentTrain: epoch  3, batch    13 | loss: 7.0343933CurrentTrain: epoch  3, batch    14 | loss: 7.7965364CurrentTrain: epoch  3, batch    15 | loss: 6.7987561CurrentTrain: epoch  3, batch    16 | loss: 7.5113497CurrentTrain: epoch  3, batch    17 | loss: 8.1770554CurrentTrain: epoch  3, batch    18 | loss: 7.0775366CurrentTrain: epoch  3, batch    19 | loss: 7.0355735CurrentTrain: epoch  3, batch    20 | loss: 7.5052490CurrentTrain: epoch  3, batch    21 | loss: 7.8017807CurrentTrain: epoch  3, batch    22 | loss: 7.5678306CurrentTrain: epoch  3, batch    23 | loss: 7.4953890CurrentTrain: epoch  3, batch    24 | loss: 6.7109461CurrentTrain: epoch  3, batch    25 | loss: 7.6499014CurrentTrain: epoch  3, batch    26 | loss: 7.0057998CurrentTrain: epoch  3, batch    27 | loss: 7.9391313CurrentTrain: epoch  3, batch    28 | loss: 6.6582127CurrentTrain: epoch  3, batch    29 | loss: 7.2950172CurrentTrain: epoch  3, batch    30 | loss: 5.8554316CurrentTrain: epoch  3, batch    31 | loss: 6.8246608CurrentTrain: epoch  3, batch    32 | loss: 7.2519217CurrentTrain: epoch  3, batch    33 | loss: 6.8571768CurrentTrain: epoch  3, batch    34 | loss: 6.8279881CurrentTrain: epoch  3, batch    35 | loss: 8.0219736CurrentTrain: epoch  3, batch    36 | loss: 6.8137608CurrentTrain: epoch  3, batch    37 | loss: 7.8018484CurrentTrain: epoch  4, batch     0 | loss: 6.1570139CurrentTrain: epoch  4, batch     1 | loss: 8.0692968CurrentTrain: epoch  4, batch     2 | loss: 7.4186492CurrentTrain: epoch  4, batch     3 | loss: 6.6862993CurrentTrain: epoch  4, batch     4 | loss: 6.6383619CurrentTrain: epoch  4, batch     5 | loss: 6.9858074CurrentTrain: epoch  4, batch     6 | loss: 7.0298467CurrentTrain: epoch  4, batch     7 | loss: 7.0465527CurrentTrain: epoch  4, batch     8 | loss: 6.5204229CurrentTrain: epoch  4, batch     9 | loss: 7.2268820CurrentTrain: epoch  4, batch    10 | loss: 6.6949863CurrentTrain: epoch  4, batch    11 | loss: 6.4140363CurrentTrain: epoch  4, batch    12 | loss: 6.6001673CurrentTrain: epoch  4, batch    13 | loss: 6.6300306CurrentTrain: epoch  4, batch    14 | loss: 6.3996725CurrentTrain: epoch  4, batch    15 | loss: 6.2878132CurrentTrain: epoch  4, batch    16 | loss: 6.0952497CurrentTrain: epoch  4, batch    17 | loss: 6.0799365CurrentTrain: epoch  4, batch    18 | loss: 6.7837162CurrentTrain: epoch  4, batch    19 | loss: 7.2581339CurrentTrain: epoch  4, batch    20 | loss: 7.1359644CurrentTrain: epoch  4, batch    21 | loss: 7.0989490CurrentTrain: epoch  4, batch    22 | loss: 6.9939671CurrentTrain: epoch  4, batch    23 | loss: 7.3627977CurrentTrain: epoch  4, batch    24 | loss: 6.8983612CurrentTrain: epoch  4, batch    25 | loss: 6.8243423CurrentTrain: epoch  4, batch    26 | loss: 7.4717140CurrentTrain: epoch  4, batch    27 | loss: 6.9019308CurrentTrain: epoch  4, batch    28 | loss: 7.0379467CurrentTrain: epoch  4, batch    29 | loss: 5.8182020CurrentTrain: epoch  4, batch    30 | loss: 6.6924438CurrentTrain: epoch  4, batch    31 | loss: 6.4389329CurrentTrain: epoch  4, batch    32 | loss: 6.4295907CurrentTrain: epoch  4, batch    33 | loss: 6.8526545CurrentTrain: epoch  4, batch    34 | loss: 6.5612245CurrentTrain: epoch  4, batch    35 | loss: 5.3794446CurrentTrain: epoch  4, batch    36 | loss: 6.7316895CurrentTrain: epoch  4, batch    37 | loss: 6.8579912CurrentTrain: epoch  5, batch     0 | loss: 6.9318895CurrentTrain: epoch  5, batch     1 | loss: 5.8650823CurrentTrain: epoch  5, batch     2 | loss: 6.3773813CurrentTrain: epoch  5, batch     3 | loss: 5.9904213CurrentTrain: epoch  5, batch     4 | loss: 6.4969282CurrentTrain: epoch  5, batch     5 | loss: 6.2617588CurrentTrain: epoch  5, batch     6 | loss: 6.6019926CurrentTrain: epoch  5, batch     7 | loss: 6.8177433CurrentTrain: epoch  5, batch     8 | loss: 6.3887157CurrentTrain: epoch  5, batch     9 | loss: 6.4657707CurrentTrain: epoch  5, batch    10 | loss: 6.9568801CurrentTrain: epoch  5, batch    11 | loss: 6.5672407CurrentTrain: epoch  5, batch    12 | loss: 5.8873596CurrentTrain: epoch  5, batch    13 | loss: 6.0726709CurrentTrain: epoch  5, batch    14 | loss: 6.2599754CurrentTrain: epoch  5, batch    15 | loss: 6.3811188CurrentTrain: epoch  5, batch    16 | loss: 6.0196257CurrentTrain: epoch  5, batch    17 | loss: 5.9904041CurrentTrain: epoch  5, batch    18 | loss: 6.2344046CurrentTrain: epoch  5, batch    19 | loss: 6.7791300CurrentTrain: epoch  5, batch    20 | loss: 5.7334995CurrentTrain: epoch  5, batch    21 | loss: 6.7430949CurrentTrain: epoch  5, batch    22 | loss: 5.9309511CurrentTrain: epoch  5, batch    23 | loss: 6.0853710CurrentTrain: epoch  5, batch    24 | loss: 6.6342292CurrentTrain: epoch  5, batch    25 | loss: 7.1766200CurrentTrain: epoch  5, batch    26 | loss: 6.7426209CurrentTrain: epoch  5, batch    27 | loss: 6.1826811CurrentTrain: epoch  5, batch    28 | loss: 6.0550437CurrentTrain: epoch  5, batch    29 | loss: 6.2456980CurrentTrain: epoch  5, batch    30 | loss: 5.9781046CurrentTrain: epoch  5, batch    31 | loss: 5.6050162CurrentTrain: epoch  5, batch    32 | loss: 6.6081209CurrentTrain: epoch  5, batch    33 | loss: 7.1434917CurrentTrain: epoch  5, batch    34 | loss: 6.2782564CurrentTrain: epoch  5, batch    35 | loss: 6.2240949CurrentTrain: epoch  5, batch    36 | loss: 6.8102574CurrentTrain: epoch  5, batch    37 | loss: 5.6778412CurrentTrain: epoch  6, batch     0 | loss: 5.5559216CurrentTrain: epoch  6, batch     1 | loss: 6.6292958CurrentTrain: epoch  6, batch     2 | loss: 6.4158773CurrentTrain: epoch  6, batch     3 | loss: 5.5434308CurrentTrain: epoch  6, batch     4 | loss: 5.3548756CurrentTrain: epoch  6, batch     5 | loss: 6.1805420CurrentTrain: epoch  6, batch     6 | loss: 5.4158931CurrentTrain: epoch  6, batch     7 | loss: 5.7510500CurrentTrain: epoch  6, batch     8 | loss: 5.5252576CurrentTrain: epoch  6, batch     9 | loss: 5.8873158CurrentTrain: epoch  6, batch    10 | loss: 5.8212271CurrentTrain: epoch  6, batch    11 | loss: 5.7089396CurrentTrain: epoch  6, batch    12 | loss: 5.8340244CurrentTrain: epoch  6, batch    13 | loss: 6.9902887CurrentTrain: epoch  6, batch    14 | loss: 5.9249306CurrentTrain: epoch  6, batch    15 | loss: 6.1920743CurrentTrain: epoch  6, batch    16 | loss: 5.5580311CurrentTrain: epoch  6, batch    17 | loss: 5.8852911CurrentTrain: epoch  6, batch    18 | loss: 5.8089132CurrentTrain: epoch  6, batch    19 | loss: 5.8195105CurrentTrain: epoch  6, batch    20 | loss: 5.8428125CurrentTrain: epoch  6, batch    21 | loss: 6.0608263CurrentTrain: epoch  6, batch    22 | loss: 5.5449262CurrentTrain: epoch  6, batch    23 | loss: 6.1267452CurrentTrain: epoch  6, batch    24 | loss: 5.5072684CurrentTrain: epoch  6, batch    25 | loss: 6.6892376CurrentTrain: epoch  6, batch    26 | loss: 6.9474216CurrentTrain: epoch  6, batch    27 | loss: 5.8687057CurrentTrain: epoch  6, batch    28 | loss: 5.7668276CurrentTrain: epoch  6, batch    29 | loss: 5.6768408CurrentTrain: epoch  6, batch    30 | loss: 5.8729448CurrentTrain: epoch  6, batch    31 | loss: 5.9661670CurrentTrain: epoch  6, batch    32 | loss: 6.5074024CurrentTrain: epoch  6, batch    33 | loss: 5.9308462CurrentTrain: epoch  6, batch    34 | loss: 6.0270214CurrentTrain: epoch  6, batch    35 | loss: 5.9783926CurrentTrain: epoch  6, batch    36 | loss: 5.9523830CurrentTrain: epoch  6, batch    37 | loss: 5.3323669CurrentTrain: epoch  7, batch     0 | loss: 6.2586083CurrentTrain: epoch  7, batch     1 | loss: 5.1244802CurrentTrain: epoch  7, batch     2 | loss: 6.2362165CurrentTrain: epoch  7, batch     3 | loss: 5.7953258CurrentTrain: epoch  7, batch     4 | loss: 5.3923097CurrentTrain: epoch  7, batch     5 | loss: 5.4087563CurrentTrain: epoch  7, batch     6 | loss: 5.2069283CurrentTrain: epoch  7, batch     7 | loss: 6.2747555CurrentTrain: epoch  7, batch     8 | loss: 6.3426352CurrentTrain: epoch  7, batch     9 | loss: 5.6222038CurrentTrain: epoch  7, batch    10 | loss: 5.6547046CurrentTrain: epoch  7, batch    11 | loss: 5.5427794CurrentTrain: epoch  7, batch    12 | loss: 5.5556908CurrentTrain: epoch  7, batch    13 | loss: 5.4778690CurrentTrain: epoch  7, batch    14 | loss: 5.1456423CurrentTrain: epoch  7, batch    15 | loss: 5.8120775CurrentTrain: epoch  7, batch    16 | loss: 5.5379181CurrentTrain: epoch  7, batch    17 | loss: 5.5140276CurrentTrain: epoch  7, batch    18 | loss: 5.4033461CurrentTrain: epoch  7, batch    19 | loss: 5.2250948CurrentTrain: epoch  7, batch    20 | loss: 5.8890314CurrentTrain: epoch  7, batch    21 | loss: 5.9870911CurrentTrain: epoch  7, batch    22 | loss: 5.5855608CurrentTrain: epoch  7, batch    23 | loss: 5.7690496CurrentTrain: epoch  7, batch    24 | loss: 5.7976742CurrentTrain: epoch  7, batch    25 | loss: 5.2997179CurrentTrain: epoch  7, batch    26 | loss: 5.8256941CurrentTrain: epoch  7, batch    27 | loss: 5.6543365CurrentTrain: epoch  7, batch    28 | loss: 5.3607621CurrentTrain: epoch  7, batch    29 | loss: 5.3480892CurrentTrain: epoch  7, batch    30 | loss: 5.2952361CurrentTrain: epoch  7, batch    31 | loss: 5.4426222CurrentTrain: epoch  7, batch    32 | loss: 5.9962931CurrentTrain: epoch  7, batch    33 | loss: 5.3334570CurrentTrain: epoch  7, batch    34 | loss: 6.3236394CurrentTrain: epoch  7, batch    35 | loss: 6.0649810CurrentTrain: epoch  7, batch    36 | loss: 5.2308884CurrentTrain: epoch  7, batch    37 | loss: 6.9498277CurrentTrain: epoch  8, batch     0 | loss: 5.9487033CurrentTrain: epoch  8, batch     1 | loss: 5.4566555CurrentTrain: epoch  8, batch     2 | loss: 5.8391809CurrentTrain: epoch  8, batch     3 | loss: 5.4539304CurrentTrain: epoch  8, batch     4 | loss: 6.0163507CurrentTrain: epoch  8, batch     5 | loss: 5.9758310CurrentTrain: epoch  8, batch     6 | loss: 5.2381845CurrentTrain: epoch  8, batch     7 | loss: 5.2833886CurrentTrain: epoch  8, batch     8 | loss: 5.5556793CurrentTrain: epoch  8, batch     9 | loss: 5.6853948CurrentTrain: epoch  8, batch    10 | loss: 5.4103584CurrentTrain: epoch  8, batch    11 | loss: 5.4282384CurrentTrain: epoch  8, batch    12 | loss: 5.4378281CurrentTrain: epoch  8, batch    13 | loss: 5.1365461CurrentTrain: epoch  8, batch    14 | loss: 5.1980114CurrentTrain: epoch  8, batch    15 | loss: 5.2664280CurrentTrain: epoch  8, batch    16 | loss: 5.2283554CurrentTrain: epoch  8, batch    17 | loss: 5.8390727CurrentTrain: epoch  8, batch    18 | loss: 5.1222115CurrentTrain: epoch  8, batch    19 | loss: 6.2308331CurrentTrain: epoch  8, batch    20 | loss: 5.0947866CurrentTrain: epoch  8, batch    21 | loss: 5.4445920CurrentTrain: epoch  8, batch    22 | loss: 5.3573427CurrentTrain: epoch  8, batch    23 | loss: 5.4640760CurrentTrain: epoch  8, batch    24 | loss: 5.4092708CurrentTrain: epoch  8, batch    25 | loss: 4.9073391CurrentTrain: epoch  8, batch    26 | loss: 5.2537308CurrentTrain: epoch  8, batch    27 | loss: 5.2709742CurrentTrain: epoch  8, batch    28 | loss: 6.1798625CurrentTrain: epoch  8, batch    29 | loss: 5.1652026CurrentTrain: epoch  8, batch    30 | loss: 5.2809358CurrentTrain: epoch  8, batch    31 | loss: 5.6233549CurrentTrain: epoch  8, batch    32 | loss: 5.0885925CurrentTrain: epoch  8, batch    33 | loss: 5.0655527CurrentTrain: epoch  8, batch    34 | loss: 5.3211498CurrentTrain: epoch  8, batch    35 | loss: 5.1902175CurrentTrain: epoch  8, batch    36 | loss: 5.3626771CurrentTrain: epoch  8, batch    37 | loss: 5.4832006CurrentTrain: epoch  9, batch     0 | loss: 4.9238262CurrentTrain: epoch  9, batch     1 | loss: 5.1519661CurrentTrain: epoch  9, batch     2 | loss: 5.1804199CurrentTrain: epoch  9, batch     3 | loss: 6.0380774CurrentTrain: epoch  9, batch     4 | loss: 5.2723417CurrentTrain: epoch  9, batch     5 | loss: 4.9057198CurrentTrain: epoch  9, batch     6 | loss: 5.1805305CurrentTrain: epoch  9, batch     7 | loss: 5.2207403CurrentTrain: epoch  9, batch     8 | loss: 5.0960555CurrentTrain: epoch  9, batch     9 | loss: 4.9499149CurrentTrain: epoch  9, batch    10 | loss: 5.2970409CurrentTrain: epoch  9, batch    11 | loss: 5.1315656CurrentTrain: epoch  9, batch    12 | loss: 5.2012959CurrentTrain: epoch  9, batch    13 | loss: 5.0634785CurrentTrain: epoch  9, batch    14 | loss: 5.3047576CurrentTrain: epoch  9, batch    15 | loss: 5.0938644CurrentTrain: epoch  9, batch    16 | loss: 5.0350256CurrentTrain: epoch  9, batch    17 | loss: 4.9352040CurrentTrain: epoch  9, batch    18 | loss: 4.9675841CurrentTrain: epoch  9, batch    19 | loss: 5.2104807CurrentTrain: epoch  9, batch    20 | loss: 5.1318369CurrentTrain: epoch  9, batch    21 | loss: 5.0702982CurrentTrain: epoch  9, batch    22 | loss: 5.1231737CurrentTrain: epoch  9, batch    23 | loss: 5.0018625CurrentTrain: epoch  9, batch    24 | loss: 5.1204462CurrentTrain: epoch  9, batch    25 | loss: 4.9961720CurrentTrain: epoch  9, batch    26 | loss: 5.6916609CurrentTrain: epoch  9, batch    27 | loss: 4.9254961CurrentTrain: epoch  9, batch    28 | loss: 5.0607157CurrentTrain: epoch  9, batch    29 | loss: 5.1071815CurrentTrain: epoch  9, batch    30 | loss: 4.9985518CurrentTrain: epoch  9, batch    31 | loss: 4.8589873CurrentTrain: epoch  9, batch    32 | loss: 5.1690202CurrentTrain: epoch  9, batch    33 | loss: 4.8685036CurrentTrain: epoch  9, batch    34 | loss: 4.9694386CurrentTrain: epoch  9, batch    35 | loss: 4.9908237CurrentTrain: epoch  9, batch    36 | loss: 4.8247929CurrentTrain: epoch  9, batch    37 | loss: 4.9899201
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots and family.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, David Chen moved to Canada to enjoy a quieter lifestyle and to be closer to his family who had settled there years ago.  
Head Entity: David Chen  
Tail Entity: Canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` customer relationships are the backbone of banking , and banks are constantly striving to meet customer demands , '' said peter garuccio , spokesman for the american bankers association .
Head Entity: american bankers association
Tail Entity: peter garuccio
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: "As the CEO of the tech giant, she has been instrumental in driving innovation and fostering a culture of collaboration," said a company spokesperson about Jane Doe, who leads the organization.  
Head Entity: tech giant  
Tail Entity: Jane Doe  

Relation: organization top members employees  
Context: "During the annual meeting, the board praised John Smith for his leadership and dedication to the company's mission," reported the financial news outlet.  
Head Entity: company  
Tail Entity: John Smith  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union (EU) is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: since the couple 's divorce , aniston has been romantically linked with actor vince vaughn , british model paul sculfor , actor orlando bloom and singer john mayer .
Head Entity: paul sculfor
Tail Entity: british
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The renowned physicist Albert Einstein was born in Germany and later became a Swiss citizen, reflecting his deep ties to both countries.  
Head Entity: Albert Einstein  
Tail Entity: Germany  

Relation: person origin  
Context: The famous author Chimamanda Ngozi Adichie often speaks about her Nigerian heritage and how it influences her writing.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigeria  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving the company's financial strategy."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the annual conference, Dr. James Lee, the lead researcher at the National Institute of Health, presented his findings on the latest advancements in medical technology."  
Head Entity: Dr. James Lee  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: chunghwa telecom , taiwan 's largest telecommunications company , will invest nt$ 13 billion -lrb- us$ 403.76 million -rrb- this year to set up four major cloud computing centers in what the company hopes will be the largest data hub in asia , chunghwa telecom 's chairman said thursday .
Head Entity: chunghwa telecom
Tail Entity: taiwan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: samsung electronics , a global leader in technology and innovation , is headquartered in south korea and has a significant presence in various markets around the world.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization country of headquarters  
Context: the multinational corporation nestlé , known for its food and beverage products, operates its headquarters in switzerland, where it was founded over 150 years ago.  
Head Entity: nestlé  
Tail Entity: switzerland  
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.78%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.17%   
cur_acc:  ['0.8617']
his_acc:  ['0.8617']
CurrentTrain: epoch  0, batch     0 | loss: 6.5259285CurrentTrain: epoch  0, batch     1 | loss: 6.2761250CurrentTrain: epoch  1, batch     0 | loss: 6.4703407CurrentTrain: epoch  1, batch     1 | loss: 5.0463138CurrentTrain: epoch  2, batch     0 | loss: 5.3450971CurrentTrain: epoch  2, batch     1 | loss: 4.9114370CurrentTrain: epoch  3, batch     0 | loss: 5.2720852CurrentTrain: epoch  3, batch     1 | loss: 4.4978962CurrentTrain: epoch  4, batch     0 | loss: 4.5712194CurrentTrain: epoch  4, batch     1 | loss: 4.2745309CurrentTrain: epoch  5, batch     0 | loss: 4.1486287CurrentTrain: epoch  5, batch     1 | loss: 3.9812708CurrentTrain: epoch  6, batch     0 | loss: 4.3267303CurrentTrain: epoch  6, batch     1 | loss: 3.1283443CurrentTrain: epoch  7, batch     0 | loss: 3.6484084CurrentTrain: epoch  7, batch     1 | loss: 3.4957087CurrentTrain: epoch  8, batch     0 | loss: 3.6251023CurrentTrain: epoch  8, batch     1 | loss: 3.2923884CurrentTrain: epoch  9, batch     0 | loss: 3.1728053CurrentTrain: epoch  9, batch     1 | loss: 3.4899161
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones settled in san francisco, where she found a job in tech. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith lived in boston, enjoying the vibrant culture and history of the city. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, he enrolled at Stanford University to study computer science.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by his wife of 63 years , josephine robinson mcnair , of columbia ; a son , robert e. jr. , of columbia ; three daughters , robin lee howell and corinne godshall , of myrtle beach , s.c. , and claudia crawford mcnair , of jamestown , s.c. ; six grandchildren ; and one great-grandchild .
Head Entity: he
Tail Entity: claudia crawford mcnair
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: After the passing of her husband, she dedicated her life to raising their three children, including her youngest, emily, who is now a successful artist in new york.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: The famous actor often shares stories about his childhood and the influence of his parents on his career, especially his daughter, lila, who is following in his footsteps.  
Head Entity: his  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: previously , al-khawinay was sentenced to one year in jail for supporting the country 's minority shiite rebels and defaming the president , but was later pardoned by president ali abdullah saleh .
Head Entity: al-khawinay
Tail Entity: defaming the president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: In a shocking turn of events, the local council announced that mayor Thompson was charged with embezzlement after an extensive investigation revealed misuse of public funds.  
Head Entity: mayor Thompson  
Tail Entity: embezzlement  

Relation: person charges  
Context: Following the protests, activist Jane Doe was charged with inciting violence against the police during the demonstration last month.  
Head Entity: activist Jane Doe  
Tail Entity: inciting violence against the police  
Mixup data size:  105
MixupTrain:  epoch  0, batch     0 | loss: 12.9164028MixupTrain:  epoch  0, batch     1 | loss: 12.6381419MixupTrain:  epoch  0, batch     2 | loss: 10.9518635MixupTrain:  epoch  0, batch     3 | loss: 12.3021025MixupTrain:  epoch  0, batch     4 | loss: 10.5304892MixupTrain:  epoch  0, batch     5 | loss: 9.8335736MixupTrain:  epoch  0, batch     6 | loss: 8.2280435
MemoryTrain:  epoch  0, batch     0 | loss: 3.9199207MemoryTrain:  epoch  0, batch     1 | loss: 3.9392488MemoryTrain:  epoch  0, batch     2 | loss: 4.5387673MemoryTrain:  epoch  1, batch     0 | loss: 3.6882660MemoryTrain:  epoch  1, batch     1 | loss: 3.9125552MemoryTrain:  epoch  1, batch     2 | loss: 4.3665938MemoryTrain:  epoch  2, batch     0 | loss: 3.2471142MemoryTrain:  epoch  2, batch     1 | loss: 3.4902430MemoryTrain:  epoch  2, batch     2 | loss: 4.2491078MemoryTrain:  epoch  3, batch     0 | loss: 3.0700841MemoryTrain:  epoch  3, batch     1 | loss: 2.6410151MemoryTrain:  epoch  3, batch     2 | loss: 1.5361761MemoryTrain:  epoch  4, batch     0 | loss: 2.5325954MemoryTrain:  epoch  4, batch     1 | loss: 3.4611731MemoryTrain:  epoch  4, batch     2 | loss: 1.6920682MemoryTrain:  epoch  5, batch     0 | loss: 3.2989497MemoryTrain:  epoch  5, batch     1 | loss: 2.4870002MemoryTrain:  epoch  5, batch     2 | loss: 4.9683976MemoryTrain:  epoch  6, batch     0 | loss: 2.8005531MemoryTrain:  epoch  6, batch     1 | loss: 2.8048410MemoryTrain:  epoch  6, batch     2 | loss: 1.3009290MemoryTrain:  epoch  7, batch     0 | loss: 2.5542464MemoryTrain:  epoch  7, batch     1 | loss: 2.6351905MemoryTrain:  epoch  7, batch     2 | loss: 1.2783180MemoryTrain:  epoch  8, batch     0 | loss: 2.5644135MemoryTrain:  epoch  8, batch     1 | loss: 2.4630811MemoryTrain:  epoch  8, batch     2 | loss: 1.9038180MemoryTrain:  epoch  9, batch     0 | loss: 2.6467829MemoryTrain:  epoch  9, batch     1 | loss: 2.2193811MemoryTrain:  epoch  9, batch     2 | loss: 1.1769364
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 86.11%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 73.05%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 73.03%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 79.86%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.58%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 81.85%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 82.23%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 82.58%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 82.54%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.32%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 82.43%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 82.24%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 82.53%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.97%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 82.32%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.24%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 83.61%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 83.83%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 84.18%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 84.51%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 84.38%   
cur_acc:  ['0.8617', '0.8611']
his_acc:  ['0.8617', '0.8438']
CurrentTrain: epoch  0, batch     0 | loss: 6.4569979CurrentTrain: epoch  0, batch     1 | loss: 6.6228342CurrentTrain: epoch  1, batch     0 | loss: 5.1552515CurrentTrain: epoch  1, batch     1 | loss: 5.4057178CurrentTrain: epoch  2, batch     0 | loss: 5.3977547CurrentTrain: epoch  2, batch     1 | loss: 3.2950547CurrentTrain: epoch  3, batch     0 | loss: 4.2654066CurrentTrain: epoch  3, batch     1 | loss: 3.8527968CurrentTrain: epoch  4, batch     0 | loss: 4.3247204CurrentTrain: epoch  4, batch     1 | loss: 2.9592824CurrentTrain: epoch  5, batch     0 | loss: 3.1121082CurrentTrain: epoch  5, batch     1 | loss: 4.2527990CurrentTrain: epoch  6, batch     0 | loss: 3.5999596CurrentTrain: epoch  6, batch     1 | loss: 3.6856325CurrentTrain: epoch  7, batch     0 | loss: 3.4151239CurrentTrain: epoch  7, batch     1 | loss: 3.8217707CurrentTrain: epoch  8, batch     0 | loss: 2.9958866CurrentTrain: epoch  8, batch     1 | loss: 3.9060457CurrentTrain: epoch  9, batch     0 | loss: 3.5993869CurrentTrain: epoch  9, batch     1 | loss: 2.7152245
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun kapoor has always been proud of his indian heritage and culture.  
Head Entity: arjun kapoor  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she revealed that she was born in the picturesque town of auckland, new zealand, which she considers her true home.  
Head Entity: she  
Tail Entity: new zealand  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit our official site at https://www.techinnovators.com for the latest updates on our projects.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For more information about our services, check out our website at http://www.greenearthsolutions.org.  
Head Entity: Green Earth Solutions  
Tail Entity: http://www.greenearthsolutions.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple has seen significant investments from billionaire investor warren buffett through his company berkshire hathaway.  
Head Entity: apple  
Tail Entity: warren buffett  

Relation: organization shareholders  
Context: the popular streaming service netflix has attracted funding from various investors, including the well-known venture capital firm sequoia capital.  
Head Entity: netflix  
Tail Entity: sequoia capital  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its dissolution in the spring of 2018, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring of 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, John Doe, to support underprivileged children around the world.  
Head Entity: Hope for Tomorrow  
Tail Entity: John Doe  
Mixup data size:  135
MixupTrain:  epoch  0, batch     0 | loss: 7.7469035MixupTrain:  epoch  0, batch     1 | loss: 7.6050172MixupTrain:  epoch  0, batch     2 | loss: 11.9194113MixupTrain:  epoch  0, batch     3 | loss: 7.1135615MixupTrain:  epoch  0, batch     4 | loss: 8.8652158MixupTrain:  epoch  0, batch     5 | loss: 9.0974899MixupTrain:  epoch  0, batch     6 | loss: 7.2458991MixupTrain:  epoch  0, batch     7 | loss: 7.4793004MixupTrain:  epoch  0, batch     8 | loss: 5.4953144
MemoryTrain:  epoch  0, batch     0 | loss: 3.1297140MemoryTrain:  epoch  0, batch     1 | loss: 4.1391058MemoryTrain:  epoch  0, batch     2 | loss: 4.9059086MemoryTrain:  epoch  1, batch     0 | loss: 4.0782280MemoryTrain:  epoch  1, batch     1 | loss: 3.8119917MemoryTrain:  epoch  1, batch     2 | loss: 3.9341474MemoryTrain:  epoch  2, batch     0 | loss: 3.1273708MemoryTrain:  epoch  2, batch     1 | loss: 4.0509062MemoryTrain:  epoch  2, batch     2 | loss: 3.2639871MemoryTrain:  epoch  3, batch     0 | loss: 3.1233635MemoryTrain:  epoch  3, batch     1 | loss: 3.1157203MemoryTrain:  epoch  3, batch     2 | loss: 3.9897089MemoryTrain:  epoch  4, batch     0 | loss: 3.8008745MemoryTrain:  epoch  4, batch     1 | loss: 2.3639219MemoryTrain:  epoch  4, batch     2 | loss: 3.7493615MemoryTrain:  epoch  5, batch     0 | loss: 2.9049997MemoryTrain:  epoch  5, batch     1 | loss: 3.3807561MemoryTrain:  epoch  5, batch     2 | loss: 3.0624940MemoryTrain:  epoch  6, batch     0 | loss: 3.2694488MemoryTrain:  epoch  6, batch     1 | loss: 2.3617740MemoryTrain:  epoch  6, batch     2 | loss: 3.1036007MemoryTrain:  epoch  7, batch     0 | loss: 2.3436422MemoryTrain:  epoch  7, batch     1 | loss: 3.5035510MemoryTrain:  epoch  7, batch     2 | loss: 2.5313931MemoryTrain:  epoch  8, batch     0 | loss: 2.9128733MemoryTrain:  epoch  8, batch     1 | loss: 2.6860402MemoryTrain:  epoch  8, batch     2 | loss: 1.9395112MemoryTrain:  epoch  9, batch     0 | loss: 2.4448409MemoryTrain:  epoch  9, batch     1 | loss: 2.1044455MemoryTrain:  epoch  9, batch     2 | loss: 2.3486328
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 42.71%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 38.39%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 34.38%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 53.91%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 58.33%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 65.23%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.81%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 66.12%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 67.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 73.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.89%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 77.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 78.02%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.52%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 78.60%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 78.68%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 78.65%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 78.55%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 78.29%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 78.35%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 78.72%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 79.07%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 80.85%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.63%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 81.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 82.11%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 81.73%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 80.78%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 79.51%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 78.30%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 77.34%   [EVAL] batch:   56 | acc: 6.25%,  total acc: 76.10%   
cur_acc:  ['0.8617', '0.8611', '0.3438']
his_acc:  ['0.8617', '0.8438', '0.7610']
CurrentTrain: epoch  0, batch     0 | loss: 5.3784556CurrentTrain: epoch  0, batch     1 | loss: 5.7265697CurrentTrain: epoch  1, batch     0 | loss: 4.1766272CurrentTrain: epoch  1, batch     1 | loss: 4.3438644CurrentTrain: epoch  2, batch     0 | loss: 3.8345551CurrentTrain: epoch  2, batch     1 | loss: 3.6844068CurrentTrain: epoch  3, batch     0 | loss: 3.6075501CurrentTrain: epoch  3, batch     1 | loss: 3.2999432CurrentTrain: epoch  4, batch     0 | loss: 3.0278485CurrentTrain: epoch  4, batch     1 | loss: 3.0322962CurrentTrain: epoch  5, batch     0 | loss: 2.7293963CurrentTrain: epoch  5, batch     1 | loss: 2.7401433CurrentTrain: epoch  6, batch     0 | loss: 2.4768834CurrentTrain: epoch  6, batch     1 | loss: 2.4733813CurrentTrain: epoch  7, batch     0 | loss: 2.4818187CurrentTrain: epoch  7, batch     1 | loss: 2.1352768CurrentTrain: epoch  8, batch     0 | loss: 2.4781187CurrentTrain: epoch  8, batch     1 | loss: 2.2865169CurrentTrain: epoch  9, batch     0 | loss: 2.2567091CurrentTrain: epoch  9, batch     1 | loss: 2.1868024
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization Green Earth Initiative was established in 2010 to promote environmental awareness and sustainability practices across communities.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith turned 30 last week.  
Head Entity: john smith  
Tail Entity: 30  

Relation: person age  
Context: the famous actor will celebrate his 45th birthday next month.  
Head Entity: the famous actor  
Tail Entity: 45  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: it was berger who made clarke a member of the white house principals committee when it met to discuss terrorist threats , allowing an otherwise middle-ranking nsc bureaucrat to treat tenet and secretary of state madeleine albright as equals -lrb- which the empire-building clarke was pleased to do -rrb- .
Head Entity: nsc
Tail Entity: white house principals committee
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The board of directors of the tech startup includes several prominent figures from the industry, such as the CEO of Innovatech, who has been instrumental in guiding the company’s strategic direction.  
Head Entity: tech startup  
Tail Entity: board of directors  

Relation: organization members  
Context: During the annual conference, the president of the environmental group announced the inclusion of several new activists into the organization, highlighting their commitment to sustainability and community engagement.  
Head Entity: environmental group  
Tail Entity: activists  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of study and reflection, Maria decided to embrace Buddhism, finding peace and purpose in its teachings.  
Head Entity: Maria  
Tail Entity: Buddhism  

Relation: person religion  
Context: During the festival, Ahmed proudly wore his traditional attire, celebrating his deep connection to Islam and its rich cultural heritage.  
Head Entity: Ahmed  
Tail Entity: Islam  
Mixup data size:  165
MixupTrain:  epoch  0, batch     0 | loss: 6.5635478MixupTrain:  epoch  0, batch     1 | loss: 7.2114016MixupTrain:  epoch  0, batch     2 | loss: 7.0318152MixupTrain:  epoch  0, batch     3 | loss: 5.9833866MixupTrain:  epoch  0, batch     4 | loss: 5.9593372MixupTrain:  epoch  0, batch     5 | loss: 6.1855882MixupTrain:  epoch  0, batch     6 | loss: 5.5639051MixupTrain:  epoch  0, batch     7 | loss: 7.0810549MixupTrain:  epoch  0, batch     8 | loss: 5.3365960MixupTrain:  epoch  0, batch     9 | loss: 5.8531234MixupTrain:  epoch  0, batch    10 | loss: 6.0399389
MemoryTrain:  epoch  0, batch     0 | loss: 3.4071412MemoryTrain:  epoch  0, batch     1 | loss: 3.9995313MemoryTrain:  epoch  0, batch     2 | loss: 3.5627182MemoryTrain:  epoch  0, batch     3 | loss: 3.6789451MemoryTrain:  epoch  1, batch     0 | loss: 3.1621392MemoryTrain:  epoch  1, batch     1 | loss: 3.3264897MemoryTrain:  epoch  1, batch     2 | loss: 2.7337084MemoryTrain:  epoch  1, batch     3 | loss: 3.4788423MemoryTrain:  epoch  2, batch     0 | loss: 2.5369658MemoryTrain:  epoch  2, batch     1 | loss: 3.0233445MemoryTrain:  epoch  2, batch     2 | loss: 2.9092655MemoryTrain:  epoch  2, batch     3 | loss: 2.3139200MemoryTrain:  epoch  3, batch     0 | loss: 2.9794338MemoryTrain:  epoch  3, batch     1 | loss: 2.0348547MemoryTrain:  epoch  3, batch     2 | loss: 2.1377096MemoryTrain:  epoch  3, batch     3 | loss: 2.3541598MemoryTrain:  epoch  4, batch     0 | loss: 2.7367611MemoryTrain:  epoch  4, batch     1 | loss: 2.0706224MemoryTrain:  epoch  4, batch     2 | loss: 2.1138878MemoryTrain:  epoch  4, batch     3 | loss: 2.3989265MemoryTrain:  epoch  5, batch     0 | loss: 2.3430319MemoryTrain:  epoch  5, batch     1 | loss: 1.7415521MemoryTrain:  epoch  5, batch     2 | loss: 2.1942813MemoryTrain:  epoch  5, batch     3 | loss: 2.2341330MemoryTrain:  epoch  6, batch     0 | loss: 2.1091032MemoryTrain:  epoch  6, batch     1 | loss: 2.0093064MemoryTrain:  epoch  6, batch     2 | loss: 1.8800635MemoryTrain:  epoch  6, batch     3 | loss: 1.8618379MemoryTrain:  epoch  7, batch     0 | loss: 1.8844569MemoryTrain:  epoch  7, batch     1 | loss: 1.6487513MemoryTrain:  epoch  7, batch     2 | loss: 2.1102753MemoryTrain:  epoch  7, batch     3 | loss: 1.7123116MemoryTrain:  epoch  8, batch     0 | loss: 2.1229637MemoryTrain:  epoch  8, batch     1 | loss: 1.4617712MemoryTrain:  epoch  8, batch     2 | loss: 1.9987136MemoryTrain:  epoch  8, batch     3 | loss: 1.8521837MemoryTrain:  epoch  9, batch     0 | loss: 1.7822855MemoryTrain:  epoch  9, batch     1 | loss: 1.6072067MemoryTrain:  epoch  9, batch     2 | loss: 1.7113059MemoryTrain:  epoch  9, batch     3 | loss: 1.7544770
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 0.00%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 83.04%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 47.32%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 51.56%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 63.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 63.82%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 75.83%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 75.76%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 73.53%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 71.96%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 70.31%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 68.58%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 66.94%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 66.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 66.46%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 67.73%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 72.43%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 72.24%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 71.70%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 70.60%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 69.43%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 68.42%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 67.87%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 68.86%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 69.88%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 71.59%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 70.52%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 70.04%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 70.38%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 70.80%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 70.51%   
cur_acc:  ['0.8617', '0.8611', '0.3438', '0.8304']
his_acc:  ['0.8617', '0.8438', '0.7610', '0.7051']
CurrentTrain: epoch  0, batch     0 | loss: 6.1151648CurrentTrain: epoch  0, batch     1 | loss: 6.7207198CurrentTrain: epoch  1, batch     0 | loss: 5.3440022CurrentTrain: epoch  1, batch     1 | loss: 4.8479376CurrentTrain: epoch  2, batch     0 | loss: 5.0084729CurrentTrain: epoch  2, batch     1 | loss: 4.1160817CurrentTrain: epoch  3, batch     0 | loss: 4.5966797CurrentTrain: epoch  3, batch     1 | loss: 3.8716795CurrentTrain: epoch  4, batch     0 | loss: 4.0158572CurrentTrain: epoch  4, batch     1 | loss: 4.7225099CurrentTrain: epoch  5, batch     0 | loss: 3.9188175CurrentTrain: epoch  5, batch     1 | loss: 3.8443849CurrentTrain: epoch  6, batch     0 | loss: 3.6428380CurrentTrain: epoch  6, batch     1 | loss: 3.4473515CurrentTrain: epoch  7, batch     0 | loss: 3.6653290CurrentTrain: epoch  7, batch     1 | loss: 2.5522995CurrentTrain: epoch  8, batch     0 | loss: 3.2058187CurrentTrain: epoch  8, batch     1 | loss: 3.0859301CurrentTrain: epoch  9, batch     0 | loss: 3.1651649CurrentTrain: epoch  9, batch     1 | loss: 2.9162059
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: san jose , ca , usa speaking of k-fed , him and ex-wife britney spears are in court today , dealing with their custody battle .
Head Entity: britney spears
Tail Entity: ca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving to new york city, jessica found a vibrant community and a bustling lifestyle that she had always dreamed of.  
Head Entity: jessica  
Tail Entity: new york city  

Relation: person stateorprovinces of residence  
Context: During his time in toronto, the artist developed a deep appreciation for the local music scene and its diverse culture.  
Head Entity: the artist  
Tail Entity: toronto  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: former gov. robert e. mcnair of south carolina , the political moderate who was a finalist to become vice president hubert h. humphrey 's running mate in 1968 but whose promising career was cut short by what became known as the orangeburg massacre , died on nov. 17 in charleston .
Head Entity: robert e. mcnair
Tail Entity: nov. 17
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: renowned physicist stephen hawking, who made groundbreaking contributions to our understanding of black holes and the universe, passed away peacefully at his home in cambridge on march 14, 2018.  
Head Entity: stephen hawking  
Tail Entity: march 14, 2018  

Relation: person date of death  
Context: the beloved author of the harry potter series, j.k. rowling, announced the passing of her dear friend and mentor, who died on january 1, 2020, after a long battle with illness.  
Head Entity: j.k. rowling's dear friend  
Tail Entity: january 1, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, boasts a workforce of approximately 5,500 employees across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: GreenEarth Nonprofit has grown significantly over the past few years and currently employs around 1,200 dedicated staff members to support its environmental initiatives.  
Head Entity: GreenEarth Nonprofit  
Tail Entity: 1,200  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the famous author often referred to himself as the bard of Avon, '' said the literary critic.  
Head Entity: author  
Tail Entity: bard of Avon  

Relation: person alternate names  
Context: `` during his career, he was often called the king of pop, '' the documentary revealed.  
Head Entity: he  
Tail Entity: king of pop  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: in addition to his wife , meskill is survived by two daughters , eileen gallup of new britain and maureen heneghan of haddon heights , n.j. ; three sons , john , of kensington , conn. ; peter , of east hartford , conn. ; and thomas , of branford , conn. ; two sisters , ruth prior of naples , fla. , and sister laura marie of portland , conn. ; five grandchildren , and two step-grandchildren .
Head Entity: his
Tail Entity: meskill
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: after a long and happy marriage, john and his wife, sarah, decided to celebrate their 50th anniversary with a grand party attended by family and friends.  
Head Entity: his  
Tail Entity: wife  

Relation: person spouse  
Context: during the interview, emily spoke fondly of her husband, who has always been her biggest supporter throughout her career.  
Head Entity: her  
Tail Entity: husband  
Mixup data size:  195
MixupTrain:  epoch  0, batch     0 | loss: 5.3864376MixupTrain:  epoch  0, batch     1 | loss: 6.3508733MixupTrain:  epoch  0, batch     2 | loss: 5.7218688MixupTrain:  epoch  0, batch     3 | loss: 6.4839211MixupTrain:  epoch  0, batch     4 | loss: 5.4762654MixupTrain:  epoch  0, batch     5 | loss: 6.2093118MixupTrain:  epoch  0, batch     6 | loss: 6.2487270MixupTrain:  epoch  0, batch     7 | loss: 5.7630619MixupTrain:  epoch  0, batch     8 | loss: 7.0619133MixupTrain:  epoch  0, batch     9 | loss: 7.6230862MixupTrain:  epoch  0, batch    10 | loss: 5.2472429MixupTrain:  epoch  0, batch    11 | loss: 5.3440211MixupTrain:  epoch  0, batch    12 | loss: 5.7155743
MemoryTrain:  epoch  0, batch     0 | loss: 2.1150012MemoryTrain:  epoch  0, batch     1 | loss: 2.5026989MemoryTrain:  epoch  0, batch     2 | loss: 3.4153705MemoryTrain:  epoch  0, batch     3 | loss: 2.5432529MemoryTrain:  epoch  0, batch     4 | loss: 2.8630977MemoryTrain:  epoch  1, batch     0 | loss: 1.9520963MemoryTrain:  epoch  1, batch     1 | loss: 2.3920743MemoryTrain:  epoch  1, batch     2 | loss: 2.9353113MemoryTrain:  epoch  1, batch     3 | loss: 2.4718120MemoryTrain:  epoch  1, batch     4 | loss: 1.9668436MemoryTrain:  epoch  2, batch     0 | loss: 2.0123768MemoryTrain:  epoch  2, batch     1 | loss: 2.2870126MemoryTrain:  epoch  2, batch     2 | loss: 2.3883083MemoryTrain:  epoch  2, batch     3 | loss: 1.6375942MemoryTrain:  epoch  2, batch     4 | loss: 2.2181263MemoryTrain:  epoch  3, batch     0 | loss: 1.4081078MemoryTrain:  epoch  3, batch     1 | loss: 2.1679184MemoryTrain:  epoch  3, batch     2 | loss: 2.0666289MemoryTrain:  epoch  3, batch     3 | loss: 1.9600537MemoryTrain:  epoch  3, batch     4 | loss: 1.6875085MemoryTrain:  epoch  4, batch     0 | loss: 1.7144434MemoryTrain:  epoch  4, batch     1 | loss: 1.7698935MemoryTrain:  epoch  4, batch     2 | loss: 1.9026945MemoryTrain:  epoch  4, batch     3 | loss: 1.6431928MemoryTrain:  epoch  4, batch     4 | loss: 2.0688219MemoryTrain:  epoch  5, batch     0 | loss: 1.6771256MemoryTrain:  epoch  5, batch     1 | loss: 1.8286445MemoryTrain:  epoch  5, batch     2 | loss: 1.8771394MemoryTrain:  epoch  5, batch     3 | loss: 1.5963373MemoryTrain:  epoch  5, batch     4 | loss: 1.5608027MemoryTrain:  epoch  6, batch     0 | loss: 1.7400661MemoryTrain:  epoch  6, batch     1 | loss: 1.6124450MemoryTrain:  epoch  6, batch     2 | loss: 1.5826077MemoryTrain:  epoch  6, batch     3 | loss: 1.5197718MemoryTrain:  epoch  6, batch     4 | loss: 2.1143548MemoryTrain:  epoch  7, batch     0 | loss: 1.6873827MemoryTrain:  epoch  7, batch     1 | loss: 1.9467121MemoryTrain:  epoch  7, batch     2 | loss: 1.6104684MemoryTrain:  epoch  7, batch     3 | loss: 1.5668778MemoryTrain:  epoch  7, batch     4 | loss: 1.4184010MemoryTrain:  epoch  8, batch     0 | loss: 1.7188313MemoryTrain:  epoch  8, batch     1 | loss: 1.5746138MemoryTrain:  epoch  8, batch     2 | loss: 1.3811182MemoryTrain:  epoch  8, batch     3 | loss: 1.5994370MemoryTrain:  epoch  8, batch     4 | loss: 1.4554604MemoryTrain:  epoch  9, batch     0 | loss: 1.4981854MemoryTrain:  epoch  9, batch     1 | loss: 1.4550575MemoryTrain:  epoch  9, batch     2 | loss: 1.4877119MemoryTrain:  epoch  9, batch     3 | loss: 1.5942460MemoryTrain:  epoch  9, batch     4 | loss: 1.4313654
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 78.65%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 69.58%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 39.58%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 34.38%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 37.50%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 42.19%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 46.53%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 55.29%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 57.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 57.03%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 60.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.20%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 65.49%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.20%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 73.59%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 73.11%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 70.96%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 69.46%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 67.71%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 66.22%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 64.64%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 64.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 64.33%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 63.99%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 64.10%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 64.63%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 66.89%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 68.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 69.00%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 68.99%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 67.94%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 67.16%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 66.41%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 65.90%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 66.49%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 66.95%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.55%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 69.70%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 68.66%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 68.11%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 68.21%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 68.48%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 68.66%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 68.66%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 69.34%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 69.58%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 70.05%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 70.65%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 70.94%   [EVAL] batch:   80 | acc: 37.50%,  total acc: 70.52%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 70.20%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 69.88%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 69.64%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 69.12%   
cur_acc:  ['0.8617', '0.8611', '0.3438', '0.8304', '0.6958']
his_acc:  ['0.8617', '0.8438', '0.7610', '0.7051', '0.6912']
CurrentTrain: epoch  0, batch     0 | loss: 6.5548654CurrentTrain: epoch  0, batch     1 | loss: 7.4019079CurrentTrain: epoch  1, batch     0 | loss: 5.8133078CurrentTrain: epoch  1, batch     1 | loss: 5.1592975CurrentTrain: epoch  2, batch     0 | loss: 4.8119860CurrentTrain: epoch  2, batch     1 | loss: 4.9037704CurrentTrain: epoch  3, batch     0 | loss: 4.5555339CurrentTrain: epoch  3, batch     1 | loss: 4.7464733CurrentTrain: epoch  4, batch     0 | loss: 4.3387184CurrentTrain: epoch  4, batch     1 | loss: 3.8450158CurrentTrain: epoch  5, batch     0 | loss: 3.5561531CurrentTrain: epoch  5, batch     1 | loss: 3.7265277CurrentTrain: epoch  6, batch     0 | loss: 3.3060544CurrentTrain: epoch  6, batch     1 | loss: 3.4883940CurrentTrain: epoch  7, batch     0 | loss: 3.2851045CurrentTrain: epoch  7, batch     1 | loss: 2.8266871CurrentTrain: epoch  8, batch     0 | loss: 2.8804889CurrentTrain: epoch  8, batch     1 | loss: 2.5496428CurrentTrain: epoch  9, batch     0 | loss: 2.5117149CurrentTrain: epoch  9, batch     1 | loss: 2.4180386
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: born in a small village in italy on march 15, 1980, luca grew up surrounded by the beautiful countryside.  
Head Entity: luca  
Tail Entity: march 15, 1980  

Relation: person date of birth  
Context: the famous author was born in new york city on september 5, 1975, where he later found inspiration for his novels.  
Head Entity: the famous author  
Tail Entity: september 5, 1975  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, where she spent her early years before moving to connecticut.  
Head Entity: martha stewart  
Tail Entity: new jersey  

Relation: person stateorprovince of birth  
Context: barack obama was born on august 4, 1961, in honolulu, hawaii, and later became the 44th president of the united states.  
Head Entity: barack obama  
Tail Entity: hawaii  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: After the ceremony, Emily's father, John Smith, shared heartfelt stories about her childhood, bringing tears to many eyes.  
Head Entity: Emily  
Tail Entity: John Smith  

Relation: person parents  
Context: During the family reunion, Michael's mother, Sarah Johnson, prepared her famous lasagna, which everyone eagerly anticipated.  
Head Entity: Michael  
Tail Entity: Sarah Johnson  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst in the investment division.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california last night.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, mary jane, a beloved community leader, succumbed to her condition in a hospital located in new york city.  
Head Entity: mary jane  
Tail Entity: new york city  
Mixup data size:  225
MixupTrain:  epoch  0, batch     0 | loss: 5.3996580MixupTrain:  epoch  0, batch     1 | loss: 4.6879024MixupTrain:  epoch  0, batch     2 | loss: 4.7793999MixupTrain:  epoch  0, batch     3 | loss: 4.7810088MixupTrain:  epoch  0, batch     4 | loss: 4.4787240MixupTrain:  epoch  0, batch     5 | loss: 5.2921528MixupTrain:  epoch  0, batch     6 | loss: 4.8846791MixupTrain:  epoch  0, batch     7 | loss: 5.5637022MixupTrain:  epoch  0, batch     8 | loss: 4.4254104MixupTrain:  epoch  0, batch     9 | loss: 4.3205955MixupTrain:  epoch  0, batch    10 | loss: 4.7059856MixupTrain:  epoch  0, batch    11 | loss: 3.7816634MixupTrain:  epoch  0, batch    12 | loss: 4.1840981MixupTrain:  epoch  0, batch    13 | loss: 4.6672388MixupTrain:  epoch  0, batch    14 | loss: 2.3789620
MemoryTrain:  epoch  0, batch     0 | loss: 1.3966215MemoryTrain:  epoch  0, batch     1 | loss: 1.9816552MemoryTrain:  epoch  0, batch     2 | loss: 2.5927081MemoryTrain:  epoch  0, batch     3 | loss: 2.3575630MemoryTrain:  epoch  0, batch     4 | loss: 3.0747004MemoryTrain:  epoch  0, batch     5 | loss: 2.1360252MemoryTrain:  epoch  1, batch     0 | loss: 1.7881335MemoryTrain:  epoch  1, batch     1 | loss: 2.2858882MemoryTrain:  epoch  1, batch     2 | loss: 1.7612391MemoryTrain:  epoch  1, batch     3 | loss: 2.5951006MemoryTrain:  epoch  1, batch     4 | loss: 2.4484234MemoryTrain:  epoch  1, batch     5 | loss: 1.6187270MemoryTrain:  epoch  2, batch     0 | loss: 1.9162132MemoryTrain:  epoch  2, batch     1 | loss: 1.4432079MemoryTrain:  epoch  2, batch     2 | loss: 1.7743748MemoryTrain:  epoch  2, batch     3 | loss: 2.0179200MemoryTrain:  epoch  2, batch     4 | loss: 2.2223997MemoryTrain:  epoch  2, batch     5 | loss: 2.1053052MemoryTrain:  epoch  3, batch     0 | loss: 1.8204994MemoryTrain:  epoch  3, batch     1 | loss: 1.8471608MemoryTrain:  epoch  3, batch     2 | loss: 1.7102525MemoryTrain:  epoch  3, batch     3 | loss: 1.3784225MemoryTrain:  epoch  3, batch     4 | loss: 1.5246269MemoryTrain:  epoch  3, batch     5 | loss: 1.5886933MemoryTrain:  epoch  4, batch     0 | loss: 1.8006628MemoryTrain:  epoch  4, batch     1 | loss: 1.4721178MemoryTrain:  epoch  4, batch     2 | loss: 1.5320249MemoryTrain:  epoch  4, batch     3 | loss: 1.8003962MemoryTrain:  epoch  4, batch     4 | loss: 1.5313978MemoryTrain:  epoch  4, batch     5 | loss: 1.4761477MemoryTrain:  epoch  5, batch     0 | loss: 1.6020687MemoryTrain:  epoch  5, batch     1 | loss: 1.2979794MemoryTrain:  epoch  5, batch     2 | loss: 1.4513444MemoryTrain:  epoch  5, batch     3 | loss: 1.5473084MemoryTrain:  epoch  5, batch     4 | loss: 1.7315247MemoryTrain:  epoch  5, batch     5 | loss: 1.7018251MemoryTrain:  epoch  6, batch     0 | loss: 1.4339854MemoryTrain:  epoch  6, batch     1 | loss: 1.4263930MemoryTrain:  epoch  6, batch     2 | loss: 1.6843081MemoryTrain:  epoch  6, batch     3 | loss: 1.6158810MemoryTrain:  epoch  6, batch     4 | loss: 1.5473441MemoryTrain:  epoch  6, batch     5 | loss: 1.2796577MemoryTrain:  epoch  7, batch     0 | loss: 1.3628484MemoryTrain:  epoch  7, batch     1 | loss: 1.4339101MemoryTrain:  epoch  7, batch     2 | loss: 1.2911956MemoryTrain:  epoch  7, batch     3 | loss: 1.5532494MemoryTrain:  epoch  7, batch     4 | loss: 1.4642024MemoryTrain:  epoch  7, batch     5 | loss: 1.3390471MemoryTrain:  epoch  8, batch     0 | loss: 1.3953370MemoryTrain:  epoch  8, batch     1 | loss: 1.4346288MemoryTrain:  epoch  8, batch     2 | loss: 1.3398122MemoryTrain:  epoch  8, batch     3 | loss: 1.3472965MemoryTrain:  epoch  8, batch     4 | loss: 1.3967507MemoryTrain:  epoch  8, batch     5 | loss: 1.4168007MemoryTrain:  epoch  9, batch     0 | loss: 1.3604386MemoryTrain:  epoch  9, batch     1 | loss: 1.3083158MemoryTrain:  epoch  9, batch     2 | loss: 1.3802047MemoryTrain:  epoch  9, batch     3 | loss: 1.3708013MemoryTrain:  epoch  9, batch     4 | loss: 1.2764330MemoryTrain:  epoch  9, batch     5 | loss: 1.3650651
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 75.45%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 64.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 63.67%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 64.34%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 64.24%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 64.47%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 65.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.38%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 75.62%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 76.21%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 75.57%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 73.35%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 71.43%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 69.62%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 67.91%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 66.12%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 65.38%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 66.09%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 65.40%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 63.99%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 63.08%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 63.07%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.43%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 67.25%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 67.65%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 67.79%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 67.57%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 66.67%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 65.68%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 64.96%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 64.36%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 64.98%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 65.47%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 66.04%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 67.14%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 68.18%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 67.16%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 66.64%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 66.49%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 66.52%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 66.11%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 65.71%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 65.33%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 65.20%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 64.92%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 64.88%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 65.02%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 65.59%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 65.94%   [EVAL] batch:   80 | acc: 31.25%,  total acc: 65.51%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 64.94%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 64.38%   [EVAL] batch:   83 | acc: 50.00%,  total acc: 64.21%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 63.82%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 64.03%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 64.08%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 64.28%   [EVAL] batch:   88 | acc: 68.75%,  total acc: 64.33%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 64.44%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 64.35%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 64.47%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 64.78%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 65.03%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 65.26%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 65.43%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 65.59%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 65.82%   [EVAL] batch:   98 | acc: 25.00%,  total acc: 65.40%   
cur_acc:  ['0.8617', '0.8611', '0.3438', '0.8304', '0.6958', '0.7545']
his_acc:  ['0.8617', '0.8438', '0.7610', '0.7051', '0.6912', '0.6540']
CurrentTrain: epoch  0, batch     0 | loss: 7.8107605CurrentTrain: epoch  0, batch     1 | loss: 8.4731951CurrentTrain: epoch  1, batch     0 | loss: 7.6754999CurrentTrain: epoch  1, batch     1 | loss: 6.6326056CurrentTrain: epoch  2, batch     0 | loss: 6.5857410CurrentTrain: epoch  2, batch     1 | loss: 6.5710640CurrentTrain: epoch  3, batch     0 | loss: 6.5884943CurrentTrain: epoch  3, batch     1 | loss: 6.2985601CurrentTrain: epoch  4, batch     0 | loss: 6.1075873CurrentTrain: epoch  4, batch     1 | loss: 5.3495312CurrentTrain: epoch  5, batch     0 | loss: 5.1966162CurrentTrain: epoch  5, batch     1 | loss: 5.7007203CurrentTrain: epoch  6, batch     0 | loss: 5.2705250CurrentTrain: epoch  6, batch     1 | loss: 5.6273155CurrentTrain: epoch  7, batch     0 | loss: 4.8667326CurrentTrain: epoch  7, batch     1 | loss: 5.4748487CurrentTrain: epoch  8, batch     0 | loss: 4.5369616CurrentTrain: epoch  8, batch     1 | loss: 5.3712802CurrentTrain: epoch  9, batch     0 | loss: 4.3212032CurrentTrain: epoch  9, batch     1 | loss: 5.3048096
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has become a leading platform for video content, and Waymo, which focuses on self-driving technology.  
Head Entity: Alphabet Inc.  
Tail Entity: Waymo  

Relation: organization subsidiaries  
Context: The multinational corporation Procter & Gamble owns various subsidiaries, such as Gillette, which specializes in personal care products, and Tide, known for its laundry detergents.  
Head Entity: Procter & Gamble  
Tail Entity: Gillette  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant SoftTech announced its acquisition of Innovatech, a leading software development firm. This move has raised questions about the future of Innovatech and its role within the larger SoftTech organization.  
Head Entity: Innovatech  
Tail Entity: SoftTech  

Relation: organization parents  
Context: The historic partnership between the National Wildlife Federation and the World Wildlife Fund has led to significant advancements in conservation efforts. As these organizations collaborate, the National Wildlife Federation continues to operate under the guidance of its parent organization, the World Wildlife Fund.  
Head Entity: National Wildlife Federation  
Tail Entity: World Wildlife Fund  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health emergencies.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: guy-sheftall entered spelman college in atlanta at age 16 and later earned a master 's in english with a thesis titled `` faulkner 's treatment of women in his major novels . ''
Head Entity: spelman college
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been since 1993.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: the united nations has its main office in new york city, serving as a hub for international diplomacy and cooperation.  
Head Entity: united nations  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: during the family reunion, john introduced his sister, emily, who had just returned from studying abroad.  
Head Entity: john  
Tail Entity: emily  

Relation: person siblings  
Context: at the award ceremony, sarah proudly accepted her trophy while her brother, michael, cheered from the audience.  
Head Entity: sarah  
Tail Entity: michael  
Mixup data size:  255
MixupTrain:  epoch  0, batch     0 | loss: 4.6863958MixupTrain:  epoch  0, batch     1 | loss: 4.5478427MixupTrain:  epoch  0, batch     2 | loss: 4.7400076MixupTrain:  epoch  0, batch     3 | loss: 5.4445687MixupTrain:  epoch  0, batch     4 | loss: 5.3267552MixupTrain:  epoch  0, batch     5 | loss: 4.9080114MixupTrain:  epoch  0, batch     6 | loss: 4.9608218MixupTrain:  epoch  0, batch     7 | loss: 5.0697414MixupTrain:  epoch  0, batch     8 | loss: 5.1656309MixupTrain:  epoch  0, batch     9 | loss: 4.6449312MixupTrain:  epoch  0, batch    10 | loss: 4.2862457MixupTrain:  epoch  0, batch    11 | loss: 5.4731274MixupTrain:  epoch  0, batch    12 | loss: 5.1944172MixupTrain:  epoch  0, batch    13 | loss: 4.4307480MixupTrain:  epoch  0, batch    14 | loss: 4.9139013MixupTrain:  epoch  0, batch    15 | loss: 5.5957640
MemoryTrain:  epoch  0, batch     0 | loss: 1.9763577MemoryTrain:  epoch  0, batch     1 | loss: 1.7343271MemoryTrain:  epoch  0, batch     2 | loss: 2.0372672MemoryTrain:  epoch  0, batch     3 | loss: 2.2505705MemoryTrain:  epoch  0, batch     4 | loss: 2.7631414MemoryTrain:  epoch  0, batch     5 | loss: 2.2332406MemoryTrain:  epoch  0, batch     6 | loss: 2.9104548MemoryTrain:  epoch  1, batch     0 | loss: 2.3244467MemoryTrain:  epoch  1, batch     1 | loss: 2.2763467MemoryTrain:  epoch  1, batch     2 | loss: 2.5163388MemoryTrain:  epoch  1, batch     3 | loss: 2.2175126MemoryTrain:  epoch  1, batch     4 | loss: 1.8053705MemoryTrain:  epoch  1, batch     5 | loss: 1.9374315MemoryTrain:  epoch  1, batch     6 | loss: 1.5460743MemoryTrain:  epoch  2, batch     0 | loss: 2.7029533MemoryTrain:  epoch  2, batch     1 | loss: 2.0651648MemoryTrain:  epoch  2, batch     2 | loss: 1.8641871MemoryTrain:  epoch  2, batch     3 | loss: 1.5013406MemoryTrain:  epoch  2, batch     4 | loss: 2.1465218MemoryTrain:  epoch  2, batch     5 | loss: 1.3839573MemoryTrain:  epoch  2, batch     6 | loss: 1.9850817MemoryTrain:  epoch  3, batch     0 | loss: 2.1578410MemoryTrain:  epoch  3, batch     1 | loss: 2.0551131MemoryTrain:  epoch  3, batch     2 | loss: 1.5686299MemoryTrain:  epoch  3, batch     3 | loss: 1.4034513MemoryTrain:  epoch  3, batch     4 | loss: 1.8502994MemoryTrain:  epoch  3, batch     5 | loss: 1.5130781MemoryTrain:  epoch  3, batch     6 | loss: 1.4583154MemoryTrain:  epoch  4, batch     0 | loss: 1.8197342MemoryTrain:  epoch  4, batch     1 | loss: 1.8851335MemoryTrain:  epoch  4, batch     2 | loss: 1.6047698MemoryTrain:  epoch  4, batch     3 | loss: 1.8367263MemoryTrain:  epoch  4, batch     4 | loss: 1.7347865MemoryTrain:  epoch  4, batch     5 | loss: 1.8742023MemoryTrain:  epoch  4, batch     6 | loss: 1.2911562MemoryTrain:  epoch  5, batch     0 | loss: 1.9237713MemoryTrain:  epoch  5, batch     1 | loss: 1.5055996MemoryTrain:  epoch  5, batch     2 | loss: 1.5324371MemoryTrain:  epoch  5, batch     3 | loss: 2.0024638MemoryTrain:  epoch  5, batch     4 | loss: 1.7542124MemoryTrain:  epoch  5, batch     5 | loss: 1.2758362MemoryTrain:  epoch  5, batch     6 | loss: 1.2268302MemoryTrain:  epoch  6, batch     0 | loss: 1.3567344MemoryTrain:  epoch  6, batch     1 | loss: 1.7239084MemoryTrain:  epoch  6, batch     2 | loss: 1.5851130MemoryTrain:  epoch  6, batch     3 | loss: 1.8162093MemoryTrain:  epoch  6, batch     4 | loss: 1.4629810MemoryTrain:  epoch  6, batch     5 | loss: 1.4125409MemoryTrain:  epoch  6, batch     6 | loss: 1.4251270MemoryTrain:  epoch  7, batch     0 | loss: 1.3336469MemoryTrain:  epoch  7, batch     1 | loss: 1.5149282MemoryTrain:  epoch  7, batch     2 | loss: 1.8132033MemoryTrain:  epoch  7, batch     3 | loss: 1.4793866MemoryTrain:  epoch  7, batch     4 | loss: 1.3221225MemoryTrain:  epoch  7, batch     5 | loss: 1.4254408MemoryTrain:  epoch  7, batch     6 | loss: 1.2962284MemoryTrain:  epoch  8, batch     0 | loss: 1.5547453MemoryTrain:  epoch  8, batch     1 | loss: 1.2754283MemoryTrain:  epoch  8, batch     2 | loss: 1.3939626MemoryTrain:  epoch  8, batch     3 | loss: 1.4535384MemoryTrain:  epoch  8, batch     4 | loss: 1.3238049MemoryTrain:  epoch  8, batch     5 | loss: 1.3095431MemoryTrain:  epoch  8, batch     6 | loss: 1.5291581MemoryTrain:  epoch  9, batch     0 | loss: 1.2885778MemoryTrain:  epoch  9, batch     1 | loss: 1.3231542MemoryTrain:  epoch  9, batch     2 | loss: 1.4800086MemoryTrain:  epoch  9, batch     3 | loss: 1.5515563MemoryTrain:  epoch  9, batch     4 | loss: 1.3297161MemoryTrain:  epoch  9, batch     5 | loss: 1.3092146MemoryTrain:  epoch  9, batch     6 | loss: 1.6520133
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 29.46%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 35.94%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 35.42%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 36.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 44.27%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 47.12%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 50.89%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 57.03%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 59.19%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 59.87%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 59.69%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 59.23%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 58.52%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 65.79%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 66.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 70.38%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 73.32%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 74.07%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 76.04%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 76.61%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 77.15%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 75.95%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 73.71%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 71.61%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 69.62%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 67.74%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 65.95%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 65.06%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 65.24%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 63.84%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 63.08%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 63.21%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 64.03%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 67.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 67.67%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 67.22%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 66.20%   [EVAL] batch:   54 | acc: 25.00%,  total acc: 65.45%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 64.62%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 64.04%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 65.15%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 66.29%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 68.37%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 67.99%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 66.98%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 66.45%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 66.58%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 66.61%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 66.46%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 66.23%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 65.84%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 65.79%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 65.75%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 66.04%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 66.23%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 66.85%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 66.08%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 65.29%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 64.96%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 64.41%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 64.61%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 64.08%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 63.71%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 63.27%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 62.99%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 62.36%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 62.09%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 62.43%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 62.77%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 63.03%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 63.22%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 63.40%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 63.65%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 63.51%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 63.50%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 63.55%   [EVAL] batch:  101 | acc: 12.50%,  total acc: 63.05%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 62.50%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 61.90%   [EVAL] batch:  104 | acc: 6.25%,  total acc: 61.37%   [EVAL] batch:  105 | acc: 56.25%,  total acc: 61.32%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 61.21%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 61.17%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 61.18%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:  110 | acc: 87.50%,  total acc: 61.49%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 61.72%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 62.06%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 62.39%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 62.93%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 63.09%   [EVAL] batch:  117 | acc: 31.25%,  total acc: 62.82%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 62.87%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 62.86%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 62.40%   
cur_acc:  ['0.8617', '0.8611', '0.3438', '0.8304', '0.6958', '0.7545', '0.5852']
his_acc:  ['0.8617', '0.8438', '0.7610', '0.7051', '0.6912', '0.6540', '0.6240']
CurrentTrain: epoch  0, batch     0 | loss: 4.0853686CurrentTrain: epoch  0, batch     1 | loss: 5.1887431CurrentTrain: epoch  1, batch     0 | loss: 3.2580967CurrentTrain: epoch  1, batch     1 | loss: 3.2333722CurrentTrain: epoch  2, batch     0 | loss: 2.8440111CurrentTrain: epoch  2, batch     1 | loss: 2.7586529CurrentTrain: epoch  3, batch     0 | loss: 2.6165848CurrentTrain: epoch  3, batch     1 | loss: 2.3970761CurrentTrain: epoch  4, batch     0 | loss: 2.3632166CurrentTrain: epoch  4, batch     1 | loss: 2.1542761CurrentTrain: epoch  5, batch     0 | loss: 2.0682981CurrentTrain: epoch  5, batch     1 | loss: 1.9924594CurrentTrain: epoch  6, batch     0 | loss: 2.1410944CurrentTrain: epoch  6, batch     1 | loss: 2.0524020CurrentTrain: epoch  7, batch     0 | loss: 2.0583639CurrentTrain: epoch  7, batch     1 | loss: 1.9948883CurrentTrain: epoch  8, batch     0 | loss: 1.8935114CurrentTrain: epoch  8, batch     1 | loss: 1.9019877CurrentTrain: epoch  9, batch     0 | loss: 1.9493725CurrentTrain: epoch  9, batch     1 | loss: 1.8197038
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: goodman , who had suffered a series of strokes and seizures in recent weeks , died of natural causes , her son david said .
Head Entity: goodman
Tail Entity: natural causes
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling cancer for several years, the renowned artist passed away peacefully in her sleep, her family announced.  
Head Entity: the renowned artist  
Tail Entity: cancer  

Relation: person cause of death  
Context: following a tragic accident on the highway, the local community mourned the loss of their beloved mayor, who had dedicated his life to public service.  
Head Entity: the beloved mayor  
Tail Entity: tragic accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political landscape of the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: andrew lebow , an oil trader with mf global in new york , said investors have been discouraged by lower-than-expected oil imports in china and the disappointing growth in the u.s. economy .
Head Entity: mf global
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been a hub for innovation and development.  
Head Entity: apple inc.  
Tail Entity: cupertino, california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics has its main office situated in suwon, south korea, which plays a crucial role in its global operations.  
Head Entity: samsung electronics  
Tail Entity: suwon, south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Michael, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person other family  
Context: In her memoir, Lisa shared stories about her aunt, who played a significant role in her upbringing and inspired her to pursue a career in art.  
Head Entity: Lisa  
Tail Entity: her aunt
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  284
MixupTrain:  epoch  0, batch     0 | loss: 5.0384887MixupTrain:  epoch  0, batch     1 | loss: 5.1731915MixupTrain:  epoch  0, batch     2 | loss: 4.8652326MixupTrain:  epoch  0, batch     3 | loss: 4.5117085MixupTrain:  epoch  0, batch     4 | loss: 5.2450126MixupTrain:  epoch  0, batch     5 | loss: 4.0727119MixupTrain:  epoch  0, batch     6 | loss: 4.4749858MixupTrain:  epoch  0, batch     7 | loss: 4.7437388MixupTrain:  epoch  0, batch     8 | loss: 4.0893121MixupTrain:  epoch  0, batch     9 | loss: 4.1448096MixupTrain:  epoch  0, batch    10 | loss: 4.1749499MixupTrain:  epoch  0, batch    11 | loss: 3.8404737MixupTrain:  epoch  0, batch    12 | loss: 4.3371597MixupTrain:  epoch  0, batch    13 | loss: 4.9428381MixupTrain:  epoch  0, batch    14 | loss: 4.5819787MixupTrain:  epoch  0, batch    15 | loss: 3.6981601MixupTrain:  epoch  0, batch    16 | loss: 5.3104345MixupTrain:  epoch  0, batch    17 | loss: 5.5645751
MemoryTrain:  epoch  0, batch     0 | loss: 2.5675068MemoryTrain:  epoch  0, batch     1 | loss: 1.8110863MemoryTrain:  epoch  0, batch     2 | loss: 2.2518151MemoryTrain:  epoch  0, batch     3 | loss: 2.5146880MemoryTrain:  epoch  0, batch     4 | loss: 2.3004215MemoryTrain:  epoch  0, batch     5 | loss: 2.3782210MemoryTrain:  epoch  0, batch     6 | loss: 3.3689332MemoryTrain:  epoch  0, batch     7 | loss: 2.5513728MemoryTrain:  epoch  1, batch     0 | loss: 2.4534411MemoryTrain:  epoch  1, batch     1 | loss: 2.6054294MemoryTrain:  epoch  1, batch     2 | loss: 1.9792581MemoryTrain:  epoch  1, batch     3 | loss: 2.1393437MemoryTrain:  epoch  1, batch     4 | loss: 2.6987228MemoryTrain:  epoch  1, batch     5 | loss: 2.2967188MemoryTrain:  epoch  1, batch     6 | loss: 1.8840286MemoryTrain:  epoch  1, batch     7 | loss: 1.6714976MemoryTrain:  epoch  2, batch     0 | loss: 2.2375183MemoryTrain:  epoch  2, batch     1 | loss: 1.3786805MemoryTrain:  epoch  2, batch     2 | loss: 2.0627129MemoryTrain:  epoch  2, batch     3 | loss: 2.3685057MemoryTrain:  epoch  2, batch     4 | loss: 2.5503273MemoryTrain:  epoch  2, batch     5 | loss: 1.5931337MemoryTrain:  epoch  2, batch     6 | loss: 1.5100046MemoryTrain:  epoch  2, batch     7 | loss: 2.3118515MemoryTrain:  epoch  3, batch     0 | loss: 2.3795755MemoryTrain:  epoch  3, batch     1 | loss: 1.8587661MemoryTrain:  epoch  3, batch     2 | loss: 1.6125610MemoryTrain:  epoch  3, batch     3 | loss: 1.6096092MemoryTrain:  epoch  3, batch     4 | loss: 1.8868190MemoryTrain:  epoch  3, batch     5 | loss: 1.5918891MemoryTrain:  epoch  3, batch     6 | loss: 1.5393537MemoryTrain:  epoch  3, batch     7 | loss: 1.4992607MemoryTrain:  epoch  4, batch     0 | loss: 1.5904758MemoryTrain:  epoch  4, batch     1 | loss: 1.4950721MemoryTrain:  epoch  4, batch     2 | loss: 1.5494797MemoryTrain:  epoch  4, batch     3 | loss: 1.6120939MemoryTrain:  epoch  4, batch     4 | loss: 2.0671685MemoryTrain:  epoch  4, batch     5 | loss: 1.5011694MemoryTrain:  epoch  4, batch     6 | loss: 1.4566014MemoryTrain:  epoch  4, batch     7 | loss: 1.5640961MemoryTrain:  epoch  5, batch     0 | loss: 1.7094121MemoryTrain:  epoch  5, batch     1 | loss: 1.5737152MemoryTrain:  epoch  5, batch     2 | loss: 1.5348178MemoryTrain:  epoch  5, batch     3 | loss: 1.4772551MemoryTrain:  epoch  5, batch     4 | loss: 1.7717612MemoryTrain:  epoch  5, batch     5 | loss: 1.4877564MemoryTrain:  epoch  5, batch     6 | loss: 1.3498162MemoryTrain:  epoch  5, batch     7 | loss: 1.4290549MemoryTrain:  epoch  6, batch     0 | loss: 1.2730975MemoryTrain:  epoch  6, batch     1 | loss: 1.5757074MemoryTrain:  epoch  6, batch     2 | loss: 1.5426257MemoryTrain:  epoch  6, batch     3 | loss: 1.4089267MemoryTrain:  epoch  6, batch     4 | loss: 1.3424687MemoryTrain:  epoch  6, batch     5 | loss: 1.5309744MemoryTrain:  epoch  6, batch     6 | loss: 1.9114850MemoryTrain:  epoch  6, batch     7 | loss: 1.5875793MemoryTrain:  epoch  7, batch     0 | loss: 1.5324528MemoryTrain:  epoch  7, batch     1 | loss: 1.2939901MemoryTrain:  epoch  7, batch     2 | loss: 1.7092628MemoryTrain:  epoch  7, batch     3 | loss: 1.4094272MemoryTrain:  epoch  7, batch     4 | loss: 1.7480943MemoryTrain:  epoch  7, batch     5 | loss: 1.3292892MemoryTrain:  epoch  7, batch     6 | loss: 1.2893193MemoryTrain:  epoch  7, batch     7 | loss: 1.5547538MemoryTrain:  epoch  8, batch     0 | loss: 1.2878929MemoryTrain:  epoch  8, batch     1 | loss: 1.3306398MemoryTrain:  epoch  8, batch     2 | loss: 1.7048478MemoryTrain:  epoch  8, batch     3 | loss: 1.4260128MemoryTrain:  epoch  8, batch     4 | loss: 1.5765418MemoryTrain:  epoch  8, batch     5 | loss: 1.4657993MemoryTrain:  epoch  8, batch     6 | loss: 1.4393594MemoryTrain:  epoch  8, batch     7 | loss: 1.3693649MemoryTrain:  epoch  9, batch     0 | loss: 1.4713962MemoryTrain:  epoch  9, batch     1 | loss: 1.3832959MemoryTrain:  epoch  9, batch     2 | loss: 1.4090514MemoryTrain:  epoch  9, batch     3 | loss: 1.3495533MemoryTrain:  epoch  9, batch     4 | loss: 1.4320588MemoryTrain:  epoch  9, batch     5 | loss: 1.3185902MemoryTrain:  epoch  9, batch     6 | loss: 1.3933316MemoryTrain:  epoch  9, batch     7 | loss: 1.2617695
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 62.98%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 64.73%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 64.80%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 76.01%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 75.38%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 73.16%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 71.07%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 69.10%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 67.23%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 65.46%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 64.58%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 65.31%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 64.79%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 63.39%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 62.35%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 62.36%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 63.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 63.99%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 64.76%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 66.20%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 66.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 67.07%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 66.63%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 65.62%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 65.00%   [EVAL] batch:   55 | acc: 25.00%,  total acc: 64.29%   [EVAL] batch:   56 | acc: 31.25%,  total acc: 63.71%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 64.83%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:   65 | acc: 37.50%,  total acc: 67.61%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 66.70%   [EVAL] batch:   67 | acc: 25.00%,  total acc: 66.08%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 66.03%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 65.98%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 65.49%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 65.19%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 64.90%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 64.86%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 64.56%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 64.69%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 65.19%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 65.55%   [EVAL] batch:   80 | acc: 12.50%,  total acc: 64.89%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 64.18%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 63.40%   [EVAL] batch:   83 | acc: 18.75%,  total acc: 62.87%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 62.35%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 62.65%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 62.07%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 61.65%   [EVAL] batch:   88 | acc: 18.75%,  total acc: 61.17%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 60.83%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 60.16%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 59.92%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 60.15%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 60.57%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 60.86%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 61.07%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 61.28%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 61.54%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 61.55%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 61.31%   [EVAL] batch:  100 | acc: 43.75%,  total acc: 61.14%   [EVAL] batch:  101 | acc: 18.75%,  total acc: 60.72%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 60.68%   [EVAL] batch:  103 | acc: 43.75%,  total acc: 60.52%   [EVAL] batch:  104 | acc: 31.25%,  total acc: 60.24%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 60.14%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 59.87%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 59.72%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 59.58%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 59.49%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 59.52%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 59.54%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 59.68%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 59.81%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 59.95%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 60.02%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 59.88%   [EVAL] batch:  117 | acc: 0.00%,  total acc: 59.38%   [EVAL] batch:  118 | acc: 6.25%,  total acc: 58.93%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 58.44%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 58.11%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 58.09%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 57.98%   [EVAL] batch:  123 | acc: 62.50%,  total acc: 58.01%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 58.20%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 58.48%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 58.76%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 58.84%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 58.96%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 58.89%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 59.02%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 59.04%   [EVAL] batch:  132 | acc: 37.50%,  total acc: 58.88%   
cur_acc:  ['0.8617', '0.8611', '0.3438', '0.8304', '0.6958', '0.7545', '0.5852', '0.6298']
his_acc:  ['0.8617', '0.8438', '0.7610', '0.7051', '0.6912', '0.6540', '0.6240', '0.5888']
----------END
his_acc mean:  [0.8605 0.817  0.7481 0.6983 0.6443 0.6417 0.6081 0.5888]
