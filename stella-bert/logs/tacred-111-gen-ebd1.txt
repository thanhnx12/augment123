#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=5
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 1 0]
Losses:  25.125850677490234 11.09357738494873 0.6415597200393677
CurrentTrain: epoch  0, batch     0 | loss: 25.1258507Losses:  23.431838989257812 9.646484375 0.645329475402832
CurrentTrain: epoch  0, batch     1 | loss: 23.4318390Losses:  22.776517868041992 9.043648719787598 0.6303292512893677
CurrentTrain: epoch  0, batch     2 | loss: 22.7765179Losses:  23.847043991088867 10.20238208770752 0.6439913511276245
CurrentTrain: epoch  0, batch     3 | loss: 23.8470440Losses:  23.505054473876953 9.971214294433594 0.6052734851837158
CurrentTrain: epoch  0, batch     4 | loss: 23.5050545Losses:  20.69438362121582 7.43414306640625 0.6034010648727417
CurrentTrain: epoch  0, batch     5 | loss: 20.6943836Losses:  20.800745010375977 7.519517421722412 0.6097358465194702
CurrentTrain: epoch  0, batch     6 | loss: 20.8007450Losses:  21.975143432617188 9.034991264343262 0.574554443359375
CurrentTrain: epoch  0, batch     7 | loss: 21.9751434Losses:  23.164304733276367 10.19125747680664 0.5602355003356934
CurrentTrain: epoch  0, batch     8 | loss: 23.1643047Losses:  21.56110382080078 8.751775741577148 0.5500503778457642
CurrentTrain: epoch  0, batch     9 | loss: 21.5611038Losses:  20.822589874267578 8.096237182617188 0.5469988584518433
CurrentTrain: epoch  0, batch    10 | loss: 20.8225899Losses:  19.90032196044922 7.294508934020996 0.5579805970191956
CurrentTrain: epoch  0, batch    11 | loss: 19.9003220Losses:  21.06570816040039 8.336751937866211 0.5289536714553833
CurrentTrain: epoch  0, batch    12 | loss: 21.0657082Losses:  20.578697204589844 8.216236114501953 0.4648168683052063
CurrentTrain: epoch  0, batch    13 | loss: 20.5786972Losses:  21.27639389038086 9.091619491577148 0.4568149745464325
CurrentTrain: epoch  0, batch    14 | loss: 21.2763939Losses:  19.128314971923828 7.004145622253418 0.44535893201828003
CurrentTrain: epoch  0, batch    15 | loss: 19.1283150Losses:  21.862041473388672 10.270703315734863 0.4009968042373657
CurrentTrain: epoch  0, batch    16 | loss: 21.8620415Losses:  23.932912826538086 12.342273712158203 0.3586016893386841
CurrentTrain: epoch  0, batch    17 | loss: 23.9329128Losses:  25.5299072265625 13.652383804321289 0.40645521879196167
CurrentTrain: epoch  0, batch    18 | loss: 25.5299072Losses:  20.203922271728516 8.276185035705566 0.412800669670105
CurrentTrain: epoch  0, batch    19 | loss: 20.2039223Losses:  19.889785766601562 8.173059463500977 0.4022510349750519
CurrentTrain: epoch  0, batch    20 | loss: 19.8897858Losses:  22.06855583190918 9.937870025634766 0.4104510247707367
CurrentTrain: epoch  0, batch    21 | loss: 22.0685558Losses:  20.912595748901367 8.983195304870605 0.31304672360420227
CurrentTrain: epoch  0, batch    22 | loss: 20.9125957Losses:  19.39810562133789 8.182428359985352 0.31825998425483704
CurrentTrain: epoch  0, batch    23 | loss: 19.3981056Losses:  19.629039764404297 8.23455810546875 0.32000672817230225
CurrentTrain: epoch  0, batch    24 | loss: 19.6290398Losses:  18.632963180541992 7.124813556671143 0.31021809577941895
CurrentTrain: epoch  0, batch    25 | loss: 18.6329632Losses:  20.40264892578125 9.261884689331055 0.33603164553642273
CurrentTrain: epoch  0, batch    26 | loss: 20.4026489Losses:  18.87028694152832 8.227889060974121 0.26623424887657166
CurrentTrain: epoch  0, batch    27 | loss: 18.8702869Losses:  20.935583114624023 9.788764953613281 0.303941547870636
CurrentTrain: epoch  0, batch    28 | loss: 20.9355831Losses:  17.739852905273438 6.583773612976074 0.2770233750343323
CurrentTrain: epoch  0, batch    29 | loss: 17.7398529Losses:  20.613689422607422 9.936427116394043 0.2853127717971802
CurrentTrain: epoch  0, batch    30 | loss: 20.6136894Losses:  19.210586547851562 8.30224323272705 0.29395580291748047
CurrentTrain: epoch  0, batch    31 | loss: 19.2105865Losses:  19.014310836791992 8.715100288391113 0.2783839702606201
CurrentTrain: epoch  0, batch    32 | loss: 19.0143108Losses:  18.607547760009766 8.045026779174805 0.26448094844818115
CurrentTrain: epoch  0, batch    33 | loss: 18.6075478Losses:  21.60893440246582 11.594880104064941 0.19100400805473328
CurrentTrain: epoch  0, batch    34 | loss: 21.6089344Losses:  20.578155517578125 10.329682350158691 0.2349359691143036
CurrentTrain: epoch  0, batch    35 | loss: 20.5781555Losses:  19.20968246459961 8.578742027282715 0.2425384223461151
CurrentTrain: epoch  0, batch    36 | loss: 19.2096825Losses:  11.845403671264648 1.7446783781051636 0.20155882835388184
CurrentTrain: epoch  0, batch    37 | loss: 11.8454037Losses:  16.681373596191406 6.563851356506348 0.2209884077310562
CurrentTrain: epoch  1, batch     0 | loss: 16.6813736Losses:  17.26825523376465 6.601783752441406 0.21898256242275238
CurrentTrain: epoch  1, batch     1 | loss: 17.2682552Losses:  17.094345092773438 7.565075874328613 0.1790357232093811
CurrentTrain: epoch  1, batch     2 | loss: 17.0943451Losses:  15.67198657989502 5.869254112243652 0.21470186114311218
CurrentTrain: epoch  1, batch     3 | loss: 15.6719866Losses:  16.16132354736328 7.010287284851074 0.22521844506263733
CurrentTrain: epoch  1, batch     4 | loss: 16.1613235Losses:  20.673770904541016 10.340675354003906 0.16211232542991638
CurrentTrain: epoch  1, batch     5 | loss: 20.6737709Losses:  16.988306045532227 7.379750728607178 0.2067183554172516
CurrentTrain: epoch  1, batch     6 | loss: 16.9883060Losses:  17.240692138671875 7.693327903747559 0.22415150701999664
CurrentTrain: epoch  1, batch     7 | loss: 17.2406921Losses:  18.556344985961914 9.10007095336914 0.18723469972610474
CurrentTrain: epoch  1, batch     8 | loss: 18.5563450Losses:  16.15349769592285 6.745006561279297 0.17342770099639893
CurrentTrain: epoch  1, batch     9 | loss: 16.1534977Losses:  18.371896743774414 8.577134132385254 0.19236907362937927
CurrentTrain: epoch  1, batch    10 | loss: 18.3718967Losses:  15.883142471313477 5.937032222747803 0.1407412439584732
CurrentTrain: epoch  1, batch    11 | loss: 15.8831425Losses:  16.916257858276367 6.9367499351501465 0.2323269098997116
CurrentTrain: epoch  1, batch    12 | loss: 16.9162579Losses:  14.569223403930664 5.757464408874512 0.16333098709583282
CurrentTrain: epoch  1, batch    13 | loss: 14.5692234Losses:  17.005233764648438 7.415992259979248 0.15301959216594696
CurrentTrain: epoch  1, batch    14 | loss: 17.0052338Losses:  16.074113845825195 6.854150295257568 0.17481209337711334
CurrentTrain: epoch  1, batch    15 | loss: 16.0741138Losses:  16.287206649780273 7.0727691650390625 0.12997698783874512
CurrentTrain: epoch  1, batch    16 | loss: 16.2872066Losses:  18.224153518676758 8.7212553024292 0.1281939595937729
CurrentTrain: epoch  1, batch    17 | loss: 18.2241535Losses:  15.734225273132324 6.638611316680908 0.18284519016742706
CurrentTrain: epoch  1, batch    18 | loss: 15.7342253Losses:  18.375370025634766 8.739011764526367 0.1421445608139038
CurrentTrain: epoch  1, batch    19 | loss: 18.3753700Losses:  16.762134552001953 7.14921236038208 0.1414203941822052
CurrentTrain: epoch  1, batch    20 | loss: 16.7621346Losses:  19.383209228515625 9.681844711303711 0.09777003526687622
CurrentTrain: epoch  1, batch    21 | loss: 19.3832092Losses:  16.254905700683594 7.2042036056518555 0.11338020116090775
CurrentTrain: epoch  1, batch    22 | loss: 16.2549057Losses:  16.157384872436523 6.794975757598877 0.10579262673854828
CurrentTrain: epoch  1, batch    23 | loss: 16.1573849Losses:  16.89996910095215 7.751102924346924 0.11255672574043274
CurrentTrain: epoch  1, batch    24 | loss: 16.8999691Losses:  16.476974487304688 7.724733829498291 0.13233649730682373
CurrentTrain: epoch  1, batch    25 | loss: 16.4769745Losses:  19.191640853881836 9.345849990844727 0.11362624168395996
CurrentTrain: epoch  1, batch    26 | loss: 19.1916409Losses:  18.047746658325195 9.533241271972656 0.12103438377380371
CurrentTrain: epoch  1, batch    27 | loss: 18.0477467Losses:  20.492820739746094 11.449974060058594 0.09923461079597473
CurrentTrain: epoch  1, batch    28 | loss: 20.4928207Losses:  16.114418029785156 7.8910651206970215 0.13114196062088013
CurrentTrain: epoch  1, batch    29 | loss: 16.1144180Losses:  14.776122093200684 5.8517866134643555 0.0989823043346405
CurrentTrain: epoch  1, batch    30 | loss: 14.7761221Losses:  18.895702362060547 9.267267227172852 0.14180469512939453
CurrentTrain: epoch  1, batch    31 | loss: 18.8957024Losses:  15.365225791931152 6.077577114105225 0.08254300802946091
CurrentTrain: epoch  1, batch    32 | loss: 15.3652258Losses:  15.760005950927734 6.901933670043945 0.12052923440933228
CurrentTrain: epoch  1, batch    33 | loss: 15.7600060Losses:  14.957358360290527 6.342330455780029 0.10733550786972046
CurrentTrain: epoch  1, batch    34 | loss: 14.9573584Losses:  14.954917907714844 6.439545154571533 0.09133508056402206
CurrentTrain: epoch  1, batch    35 | loss: 14.9549179Losses:  15.298835754394531 6.451509475708008 0.12006188929080963
CurrentTrain: epoch  1, batch    36 | loss: 15.2988358Losses:  10.097110748291016 1.1443555355072021 0.11944793164730072
CurrentTrain: epoch  1, batch    37 | loss: 10.0971107Losses:  12.549827575683594 4.8256330490112305 0.1031932383775711
CurrentTrain: epoch  2, batch     0 | loss: 12.5498276Losses:  15.449358940124512 6.590486526489258 0.09543931484222412
CurrentTrain: epoch  2, batch     1 | loss: 15.4493589Losses:  14.691153526306152 5.586191177368164 0.094991534948349
CurrentTrain: epoch  2, batch     2 | loss: 14.6911535Losses:  18.143508911132812 9.88566780090332 0.11501652002334595
CurrentTrain: epoch  2, batch     3 | loss: 18.1435089Losses:  18.791473388671875 9.049774169921875 0.09923407435417175
CurrentTrain: epoch  2, batch     4 | loss: 18.7914734Losses:  22.61136817932129 12.787342071533203 0.0898229256272316
CurrentTrain: epoch  2, batch     5 | loss: 22.6113682Losses:  14.197394371032715 5.251014709472656 0.10088753700256348
CurrentTrain: epoch  2, batch     6 | loss: 14.1973944Losses:  16.013809204101562 7.259494304656982 0.089807890355587
CurrentTrain: epoch  2, batch     7 | loss: 16.0138092Losses:  14.93962574005127 6.630683898925781 0.08773836493492126
CurrentTrain: epoch  2, batch     8 | loss: 14.9396257Losses:  14.806646347045898 6.306816577911377 0.08983567357063293
CurrentTrain: epoch  2, batch     9 | loss: 14.8066463Losses:  14.406153678894043 6.358138084411621 0.0982389971613884
CurrentTrain: epoch  2, batch    10 | loss: 14.4061537Losses:  15.500833511352539 6.720797538757324 0.07263952493667603
CurrentTrain: epoch  2, batch    11 | loss: 15.5008335Losses:  13.994988441467285 5.924368381500244 0.08983922004699707
CurrentTrain: epoch  2, batch    12 | loss: 13.9949884Losses:  14.059629440307617 6.062726974487305 0.09090036898851395
CurrentTrain: epoch  2, batch    13 | loss: 14.0596294Losses:  17.32111167907715 9.98587417602539 0.10364887118339539
CurrentTrain: epoch  2, batch    14 | loss: 17.3211117Losses:  15.692609786987305 7.5553412437438965 0.07996978610754013
CurrentTrain: epoch  2, batch    15 | loss: 15.6926098Losses:  15.2481050491333 6.342137813568115 0.0666210949420929
CurrentTrain: epoch  2, batch    16 | loss: 15.2481050Losses:  15.087841987609863 5.923336982727051 0.06425689905881882
CurrentTrain: epoch  2, batch    17 | loss: 15.0878420Losses:  13.873205184936523 5.742640495300293 0.06718520075082779
CurrentTrain: epoch  2, batch    18 | loss: 13.8732052Losses:  14.92306900024414 6.728753089904785 0.07738826423883438
CurrentTrain: epoch  2, batch    19 | loss: 14.9230690Losses:  14.987604141235352 7.074288845062256 0.06407343596220016
CurrentTrain: epoch  2, batch    20 | loss: 14.9876041Losses:  14.236577987670898 6.43219518661499 0.07198747992515564
CurrentTrain: epoch  2, batch    21 | loss: 14.2365780Losses:  14.122085571289062 6.7977447509765625 0.08374782651662827
CurrentTrain: epoch  2, batch    22 | loss: 14.1220856Losses:  12.870366096496582 5.024600982666016 0.07436314225196838
CurrentTrain: epoch  2, batch    23 | loss: 12.8703661Losses:  13.497639656066895 5.479791641235352 0.06816481798887253
CurrentTrain: epoch  2, batch    24 | loss: 13.4976397Losses:  14.295621871948242 5.595289707183838 0.0591067373752594
CurrentTrain: epoch  2, batch    25 | loss: 14.2956219Losses:  14.005599021911621 6.611112594604492 0.0610036626458168
CurrentTrain: epoch  2, batch    26 | loss: 14.0055990Losses:  15.684307098388672 6.573163032531738 0.06580103933811188
CurrentTrain: epoch  2, batch    27 | loss: 15.6843071Losses:  13.248123168945312 6.526228904724121 0.07637203484773636
CurrentTrain: epoch  2, batch    28 | loss: 13.2481232Losses:  13.996313095092773 5.922971725463867 0.06154247745871544
CurrentTrain: epoch  2, batch    29 | loss: 13.9963131Losses:  16.864810943603516 9.004520416259766 0.08669024705886841
CurrentTrain: epoch  2, batch    30 | loss: 16.8648109Losses:  13.062887191772461 5.757694721221924 0.07367672026157379
CurrentTrain: epoch  2, batch    31 | loss: 13.0628872Losses:  16.38637351989746 8.772555351257324 0.07604347169399261
CurrentTrain: epoch  2, batch    32 | loss: 16.3863735Losses:  13.776101112365723 5.872143745422363 0.06138031929731369
CurrentTrain: epoch  2, batch    33 | loss: 13.7761011Losses:  17.83259391784668 8.469175338745117 0.06351478397846222
CurrentTrain: epoch  2, batch    34 | loss: 17.8325939Losses:  15.796531677246094 8.460408210754395 0.06576135754585266
CurrentTrain: epoch  2, batch    35 | loss: 15.7965317Losses:  16.111248016357422 8.058073043823242 0.06451845914125443
CurrentTrain: epoch  2, batch    36 | loss: 16.1112480Losses:  10.223536491394043 2.583271026611328 0.08442529290914536
CurrentTrain: epoch  2, batch    37 | loss: 10.2235365Losses:  14.379966735839844 6.600128173828125 0.061374545097351074
CurrentTrain: epoch  3, batch     0 | loss: 14.3799667Losses:  21.153696060180664 13.708731651306152 0.1212896853685379
CurrentTrain: epoch  3, batch     1 | loss: 21.1536961Losses:  14.323104858398438 6.2781476974487305 0.06105603650212288
CurrentTrain: epoch  3, batch     2 | loss: 14.3231049Losses:  14.308034896850586 5.8739118576049805 0.06624473631381989
CurrentTrain: epoch  3, batch     3 | loss: 14.3080349Losses:  13.425692558288574 5.632133483886719 0.0658080130815506
CurrentTrain: epoch  3, batch     4 | loss: 13.4256926Losses:  14.522281646728516 6.0928754806518555 0.06116481497883797
CurrentTrain: epoch  3, batch     5 | loss: 14.5222816Losses:  16.46607208251953 7.915857315063477 0.07670910656452179
CurrentTrain: epoch  3, batch     6 | loss: 16.4660721Losses:  11.649679183959961 4.787548065185547 0.06688889116048813
CurrentTrain: epoch  3, batch     7 | loss: 11.6496792Losses:  13.115738868713379 5.499449253082275 0.05960731953382492
CurrentTrain: epoch  3, batch     8 | loss: 13.1157389Losses:  12.928825378417969 5.104748725891113 0.050851788371801376
CurrentTrain: epoch  3, batch     9 | loss: 12.9288254Losses:  13.537358283996582 6.153368949890137 0.06640616059303284
CurrentTrain: epoch  3, batch    10 | loss: 13.5373583Losses:  14.5188627243042 7.6926493644714355 0.09301832318305969
CurrentTrain: epoch  3, batch    11 | loss: 14.5188627Losses:  13.02091121673584 4.932470321655273 0.05710650607943535
CurrentTrain: epoch  3, batch    12 | loss: 13.0209112Losses:  14.861547470092773 6.00023078918457 0.06447184830904007
CurrentTrain: epoch  3, batch    13 | loss: 14.8615475Losses:  13.396810531616211 5.6207451820373535 0.06880957633256912
CurrentTrain: epoch  3, batch    14 | loss: 13.3968105Losses:  14.146342277526855 6.449126243591309 0.06173212081193924
CurrentTrain: epoch  3, batch    15 | loss: 14.1463423Losses:  15.138412475585938 6.4769816398620605 0.05461471900343895
CurrentTrain: epoch  3, batch    16 | loss: 15.1384125Losses:  15.117936134338379 7.096402645111084 0.06994496285915375
CurrentTrain: epoch  3, batch    17 | loss: 15.1179361Losses:  13.932092666625977 6.285314083099365 0.056005872786045074
CurrentTrain: epoch  3, batch    18 | loss: 13.9320927Losses:  15.10561752319336 6.731127738952637 0.06229451298713684
CurrentTrain: epoch  3, batch    19 | loss: 15.1056175Losses:  20.585620880126953 12.79985237121582 0.05159373581409454
CurrentTrain: epoch  3, batch    20 | loss: 20.5856209Losses:  13.853235244750977 6.24882173538208 0.06710053980350494
CurrentTrain: epoch  3, batch    21 | loss: 13.8532352Losses:  15.368743896484375 6.82401180267334 0.05717569217085838
CurrentTrain: epoch  3, batch    22 | loss: 15.3687439Losses:  15.079859733581543 6.923610687255859 0.055112142115831375
CurrentTrain: epoch  3, batch    23 | loss: 15.0798597Losses:  12.519676208496094 5.596071243286133 0.06313960254192352
CurrentTrain: epoch  3, batch    24 | loss: 12.5196762Losses:  16.18533706665039 8.898902893066406 0.06189265474677086
CurrentTrain: epoch  3, batch    25 | loss: 16.1853371Losses:  13.365643501281738 6.417815208435059 0.05357450246810913
CurrentTrain: epoch  3, batch    26 | loss: 13.3656435Losses:  16.212844848632812 7.638328552246094 0.04926644265651703
CurrentTrain: epoch  3, batch    27 | loss: 16.2128448Losses:  12.450340270996094 4.695287704467773 0.04708311706781387
CurrentTrain: epoch  3, batch    28 | loss: 12.4503403Losses:  13.568570137023926 7.3741984367370605 0.06922624260187149
CurrentTrain: epoch  3, batch    29 | loss: 13.5685701Losses:  13.174165725708008 5.231393814086914 0.05603072792291641
CurrentTrain: epoch  3, batch    30 | loss: 13.1741657Losses:  14.848431587219238 6.8541059494018555 0.06014009565114975
CurrentTrain: epoch  3, batch    31 | loss: 14.8484316Losses:  16.57463836669922 9.889715194702148 0.06641754508018494
CurrentTrain: epoch  3, batch    32 | loss: 16.5746384Losses:  13.846667289733887 7.472382068634033 0.06742626428604126
CurrentTrain: epoch  3, batch    33 | loss: 13.8466673Losses:  12.861928939819336 5.6894612312316895 0.05628756806254387
CurrentTrain: epoch  3, batch    34 | loss: 12.8619289Losses:  12.731673240661621 5.483989715576172 0.050729021430015564
CurrentTrain: epoch  3, batch    35 | loss: 12.7316732Losses:  13.030635833740234 5.6950860023498535 0.05083003640174866
CurrentTrain: epoch  3, batch    36 | loss: 13.0306358Losses:  10.094674110412598 3.04046893119812 0.06964078545570374
CurrentTrain: epoch  3, batch    37 | loss: 10.0946741Losses:  13.6388578414917 5.868188858032227 0.07080232352018356
CurrentTrain: epoch  4, batch     0 | loss: 13.6388578Losses:  12.936854362487793 5.765562057495117 0.05692099407315254
CurrentTrain: epoch  4, batch     1 | loss: 12.9368544Losses:  13.09318733215332 6.855022430419922 0.061334576457738876
CurrentTrain: epoch  4, batch     2 | loss: 13.0931873Losses:  14.195549964904785 6.723272323608398 0.06312617659568787
CurrentTrain: epoch  4, batch     3 | loss: 14.1955500Losses:  15.892963409423828 8.040987014770508 0.06718074530363083
CurrentTrain: epoch  4, batch     4 | loss: 15.8929634Losses:  12.4418363571167 5.265856742858887 0.0540357381105423
CurrentTrain: epoch  4, batch     5 | loss: 12.4418364Losses:  14.095233917236328 7.638082981109619 0.0497530922293663
CurrentTrain: epoch  4, batch     6 | loss: 14.0952339Losses:  12.165914535522461 4.599689960479736 0.04598385840654373
CurrentTrain: epoch  4, batch     7 | loss: 12.1659145Losses:  18.64980697631836 9.52994155883789 0.06435281038284302
CurrentTrain: epoch  4, batch     8 | loss: 18.6498070Losses:  12.759356498718262 4.8912506103515625 0.04446534812450409
CurrentTrain: epoch  4, batch     9 | loss: 12.7593565Losses:  15.400309562683105 7.8494110107421875 0.04917656630277634
CurrentTrain: epoch  4, batch    10 | loss: 15.4003096Losses:  11.500475883483887 4.947891712188721 0.043397046625614166
CurrentTrain: epoch  4, batch    11 | loss: 11.5004759Losses:  15.709236145019531 7.826562404632568 0.05721689760684967
CurrentTrain: epoch  4, batch    12 | loss: 15.7092361Losses:  11.480772018432617 4.4059858322143555 0.04765596613287926
CurrentTrain: epoch  4, batch    13 | loss: 11.4807720Losses:  16.500701904296875 8.350202560424805 0.04829658567905426
CurrentTrain: epoch  4, batch    14 | loss: 16.5007019Losses:  15.549789428710938 8.293266296386719 0.06122542545199394
CurrentTrain: epoch  4, batch    15 | loss: 15.5497894Losses:  12.379892349243164 5.059671401977539 0.050199881196022034
CurrentTrain: epoch  4, batch    16 | loss: 12.3798923Losses:  13.264849662780762 6.024660110473633 0.0532698892056942
CurrentTrain: epoch  4, batch    17 | loss: 13.2648497Losses:  11.988496780395508 5.2043890953063965 0.048666927963495255
CurrentTrain: epoch  4, batch    18 | loss: 11.9884968Losses:  11.969176292419434 4.660446643829346 0.04071735218167305
CurrentTrain: epoch  4, batch    19 | loss: 11.9691763Losses:  13.08367919921875 6.23945426940918 0.067869633436203
CurrentTrain: epoch  4, batch    20 | loss: 13.0836792Losses:  13.239025115966797 5.831164360046387 0.05056249350309372
CurrentTrain: epoch  4, batch    21 | loss: 13.2390251Losses:  12.28039264678955 4.991103649139404 0.0473150908946991
CurrentTrain: epoch  4, batch    22 | loss: 12.2803926Losses:  12.257163047790527 5.937103271484375 0.06468697637319565
CurrentTrain: epoch  4, batch    23 | loss: 12.2571630Losses:  13.513687133789062 6.045299530029297 0.05513419583439827
CurrentTrain: epoch  4, batch    24 | loss: 13.5136871Losses:  17.229473114013672 8.89603042602539 0.05766227841377258
CurrentTrain: epoch  4, batch    25 | loss: 17.2294731Losses:  12.412206649780273 5.373465538024902 0.0417918860912323
CurrentTrain: epoch  4, batch    26 | loss: 12.4122066Losses:  15.762731552124023 6.480556488037109 0.05669957026839256
CurrentTrain: epoch  4, batch    27 | loss: 15.7627316Losses:  10.85152816772461 3.7928719520568848 0.04649162292480469
CurrentTrain: epoch  4, batch    28 | loss: 10.8515282Losses:  13.731266975402832 6.437259674072266 0.048926688730716705
CurrentTrain: epoch  4, batch    29 | loss: 13.7312670Losses:  14.023506164550781 6.334373474121094 0.08033095300197601
CurrentTrain: epoch  4, batch    30 | loss: 14.0235062Losses:  16.729108810424805 10.079326629638672 0.05030157044529915
CurrentTrain: epoch  4, batch    31 | loss: 16.7291088Losses:  13.558381080627441 5.180228233337402 0.046167369931936264
CurrentTrain: epoch  4, batch    32 | loss: 13.5583811Losses:  13.009918212890625 6.188647747039795 0.0595470629632473
CurrentTrain: epoch  4, batch    33 | loss: 13.0099182Losses:  15.679183006286621 7.225642681121826 0.044240180402994156
CurrentTrain: epoch  4, batch    34 | loss: 15.6791830Losses:  12.686823844909668 5.187812328338623 0.04814010113477707
CurrentTrain: epoch  4, batch    35 | loss: 12.6868238Losses:  12.452570915222168 5.346888542175293 0.047358639538288116
CurrentTrain: epoch  4, batch    36 | loss: 12.4525709Losses:  8.874615669250488 0.993375301361084 0.04589584842324257
CurrentTrain: epoch  4, batch    37 | loss: 8.8746157Losses:  12.897369384765625 6.504293441772461 0.061634838581085205
CurrentTrain: epoch  5, batch     0 | loss: 12.8973694Losses:  13.269571304321289 5.884230613708496 0.04372168704867363
CurrentTrain: epoch  5, batch     1 | loss: 13.2695713Losses:  14.031704902648926 6.205713748931885 0.05276923254132271
CurrentTrain: epoch  5, batch     2 | loss: 14.0317049Losses:  14.050281524658203 6.481691360473633 0.04576307535171509
CurrentTrain: epoch  5, batch     3 | loss: 14.0502815Losses:  12.812910079956055 5.254762649536133 0.05082686245441437
CurrentTrain: epoch  5, batch     4 | loss: 12.8129101Losses:  13.268198013305664 5.812814712524414 0.05124089494347572
CurrentTrain: epoch  5, batch     5 | loss: 13.2681980Losses:  12.403132438659668 4.689204216003418 0.05374838039278984
CurrentTrain: epoch  5, batch     6 | loss: 12.4031324Losses:  13.205946922302246 5.5268049240112305 0.054958924651145935
CurrentTrain: epoch  5, batch     7 | loss: 13.2059469Losses:  13.947978973388672 6.090076446533203 0.046510398387908936
CurrentTrain: epoch  5, batch     8 | loss: 13.9479790Losses:  12.432111740112305 5.400033473968506 0.04868748039007187
CurrentTrain: epoch  5, batch     9 | loss: 12.4321117Losses:  12.451760292053223 5.149831771850586 0.04031640291213989
CurrentTrain: epoch  5, batch    10 | loss: 12.4517603Losses:  12.452910423278809 4.885457992553711 0.04637440666556358
CurrentTrain: epoch  5, batch    11 | loss: 12.4529104Losses:  11.488569259643555 4.665980815887451 0.04124060645699501
CurrentTrain: epoch  5, batch    12 | loss: 11.4885693Losses:  11.813199996948242 4.479109287261963 0.041280873119831085
CurrentTrain: epoch  5, batch    13 | loss: 11.8132000Losses:  12.355844497680664 4.546241760253906 0.051183417439460754
CurrentTrain: epoch  5, batch    14 | loss: 12.3558445Losses:  15.206583976745605 7.768408298492432 0.04544587805867195
CurrentTrain: epoch  5, batch    15 | loss: 15.2065840Losses:  12.203655242919922 6.250925540924072 0.05679091811180115
CurrentTrain: epoch  5, batch    16 | loss: 12.2036552Losses:  12.592265129089355 6.179316997528076 0.05499345064163208
CurrentTrain: epoch  5, batch    17 | loss: 12.5922651Losses:  14.080955505371094 6.38789176940918 0.04553421586751938
CurrentTrain: epoch  5, batch    18 | loss: 14.0809555Losses:  15.670299530029297 7.2615532875061035 0.04933742433786392
CurrentTrain: epoch  5, batch    19 | loss: 15.6702995Losses:  11.677617073059082 5.343869209289551 0.051817137748003006
CurrentTrain: epoch  5, batch    20 | loss: 11.6776171Losses:  12.203232765197754 5.468866348266602 0.04271730035543442
CurrentTrain: epoch  5, batch    21 | loss: 12.2032328Losses:  10.972736358642578 4.477365970611572 0.04689766839146614
CurrentTrain: epoch  5, batch    22 | loss: 10.9727364Losses:  11.11876392364502 4.273497581481934 0.041221559047698975
CurrentTrain: epoch  5, batch    23 | loss: 11.1187639Losses:  13.959465026855469 6.081573486328125 0.04765153303742409
CurrentTrain: epoch  5, batch    24 | loss: 13.9594650Losses:  10.865632057189941 4.594961643218994 0.04464993625879288
CurrentTrain: epoch  5, batch    25 | loss: 10.8656321Losses:  12.10211181640625 5.160085678100586 0.04596296697854996
CurrentTrain: epoch  5, batch    26 | loss: 12.1021118Losses:  16.705018997192383 10.094310760498047 0.05299367383122444
CurrentTrain: epoch  5, batch    27 | loss: 16.7050190Losses:  18.007923126220703 9.004631042480469 0.04102667048573494
CurrentTrain: epoch  5, batch    28 | loss: 18.0079231Losses:  11.067571640014648 3.7802352905273438 0.03665151447057724
CurrentTrain: epoch  5, batch    29 | loss: 11.0675716Losses:  12.428025245666504 6.179388999938965 0.05196721851825714
CurrentTrain: epoch  5, batch    30 | loss: 12.4280252Losses:  13.166901588439941 6.67750883102417 0.061167508363723755
CurrentTrain: epoch  5, batch    31 | loss: 13.1669016Losses:  15.12236213684082 9.41091537475586 0.055447883903980255
CurrentTrain: epoch  5, batch    32 | loss: 15.1223621Losses:  13.762386322021484 6.757865905761719 0.052961453795433044
CurrentTrain: epoch  5, batch    33 | loss: 13.7623863Losses:  10.640857696533203 3.8588695526123047 0.04082859307527542
CurrentTrain: epoch  5, batch    34 | loss: 10.6408577Losses:  13.52189826965332 7.380572319030762 0.03929300233721733
CurrentTrain: epoch  5, batch    35 | loss: 13.5218983Losses:  12.001466751098633 4.716656684875488 0.03717879578471184
CurrentTrain: epoch  5, batch    36 | loss: 12.0014668Losses:  7.202620983123779 1.316162347793579 0.05385535582900047
CurrentTrain: epoch  5, batch    37 | loss: 7.2026210Losses:  11.571502685546875 5.773098945617676 0.045337654650211334
CurrentTrain: epoch  6, batch     0 | loss: 11.5715027Losses:  13.563324928283691 6.245720863342285 0.052813559770584106
CurrentTrain: epoch  6, batch     1 | loss: 13.5633249Losses:  15.680545806884766 8.244842529296875 0.04495023190975189
CurrentTrain: epoch  6, batch     2 | loss: 15.6805458Losses:  11.316670417785645 4.7025146484375 0.04186351224780083
CurrentTrain: epoch  6, batch     3 | loss: 11.3166704Losses:  12.308402061462402 5.926702499389648 0.0549345500767231
CurrentTrain: epoch  6, batch     4 | loss: 12.3084021Losses:  11.560629844665527 5.394756317138672 0.047447092831134796
CurrentTrain: epoch  6, batch     5 | loss: 11.5606298Losses:  16.718233108520508 9.484319686889648 0.045896075665950775
CurrentTrain: epoch  6, batch     6 | loss: 16.7182331Losses:  11.313870429992676 4.540957450866699 0.03792354837059975
CurrentTrain: epoch  6, batch     7 | loss: 11.3138704Losses:  11.83430004119873 5.792568206787109 0.038633283227682114
CurrentTrain: epoch  6, batch     8 | loss: 11.8343000Losses:  11.114747047424316 4.708328723907471 0.04506624862551689
CurrentTrain: epoch  6, batch     9 | loss: 11.1147470Losses:  10.032743453979492 3.990208387374878 0.03938824310898781
CurrentTrain: epoch  6, batch    10 | loss: 10.0327435Losses:  11.44247817993164 5.021182060241699 0.048608504235744476
CurrentTrain: epoch  6, batch    11 | loss: 11.4424782Losses:  10.023289680480957 4.168726444244385 0.04715220630168915
CurrentTrain: epoch  6, batch    12 | loss: 10.0232897Losses:  11.541033744812012 5.786853790283203 0.06538160145282745
CurrentTrain: epoch  6, batch    13 | loss: 11.5410337Losses:  10.639726638793945 4.29793119430542 0.040928326547145844
CurrentTrain: epoch  6, batch    14 | loss: 10.6397266Losses:  10.024368286132812 4.318648338317871 0.04179548844695091
CurrentTrain: epoch  6, batch    15 | loss: 10.0243683Losses:  13.367337226867676 6.724549293518066 0.05781884863972664
CurrentTrain: epoch  6, batch    16 | loss: 13.3673372Losses:  12.654500007629395 6.787648677825928 0.04597042500972748
CurrentTrain: epoch  6, batch    17 | loss: 12.6545000Losses:  11.739568710327148 5.341253280639648 0.048187077045440674
CurrentTrain: epoch  6, batch    18 | loss: 11.7395687Losses:  12.967484474182129 6.990166664123535 0.061832476407289505
CurrentTrain: epoch  6, batch    19 | loss: 12.9674845Losses:  15.611994743347168 8.644462585449219 0.045949872583150864
CurrentTrain: epoch  6, batch    20 | loss: 15.6119947Losses:  11.8108549118042 4.317327499389648 0.04634413495659828
CurrentTrain: epoch  6, batch    21 | loss: 11.8108549Losses:  11.878473281860352 5.5463056564331055 0.048273298889398575
CurrentTrain: epoch  6, batch    22 | loss: 11.8784733Losses:  10.082202911376953 3.743354320526123 0.03745849430561066
CurrentTrain: epoch  6, batch    23 | loss: 10.0822029Losses:  11.757678031921387 6.249102592468262 0.05720428749918938
CurrentTrain: epoch  6, batch    24 | loss: 11.7576780Losses:  15.804153442382812 8.168617248535156 0.06339222192764282
CurrentTrain: epoch  6, batch    25 | loss: 15.8041534Losses:  14.982641220092773 7.909543037414551 0.05035955458879471
CurrentTrain: epoch  6, batch    26 | loss: 14.9826412Losses:  12.060651779174805 6.083666801452637 0.0615895539522171
CurrentTrain: epoch  6, batch    27 | loss: 12.0606518Losses:  13.030901908874512 6.810239791870117 0.06602491438388824
CurrentTrain: epoch  6, batch    28 | loss: 13.0309019Losses:  12.823343276977539 6.6385393142700195 0.05879750847816467
CurrentTrain: epoch  6, batch    29 | loss: 12.8233433Losses:  12.529163360595703 6.226012229919434 0.04545339569449425
CurrentTrain: epoch  6, batch    30 | loss: 12.5291634Losses:  14.551374435424805 6.669133186340332 0.04649879038333893
CurrentTrain: epoch  6, batch    31 | loss: 14.5513744Losses:  10.836047172546387 5.263298034667969 0.04583749175071716
CurrentTrain: epoch  6, batch    32 | loss: 10.8360472Losses:  11.893218040466309 4.943545341491699 0.043372392654418945
CurrentTrain: epoch  6, batch    33 | loss: 11.8932180Losses:  10.401287078857422 4.87747859954834 0.043781135231256485
CurrentTrain: epoch  6, batch    34 | loss: 10.4012871Losses:  10.556368827819824 4.279298782348633 0.0544709637761116
CurrentTrain: epoch  6, batch    35 | loss: 10.5563688Losses:  10.61439037322998 4.458885192871094 0.04352208971977234
CurrentTrain: epoch  6, batch    36 | loss: 10.6143904Losses:  7.17257022857666 0.5206714868545532 0.043214570730924606
CurrentTrain: epoch  6, batch    37 | loss: 7.1725702Losses:  18.353979110717773 11.294157028198242 0.0628238394856453
CurrentTrain: epoch  7, batch     0 | loss: 18.3539791Losses:  10.172459602355957 4.372285842895508 0.04168950021266937
CurrentTrain: epoch  7, batch     1 | loss: 10.1724596Losses:  13.27603816986084 6.934520721435547 0.0606391504406929
CurrentTrain: epoch  7, batch     2 | loss: 13.2760382Losses:  11.93239974975586 6.900469779968262 0.05258969962596893
CurrentTrain: epoch  7, batch     3 | loss: 11.9323997Losses:  12.361150741577148 5.866969108581543 0.05943451449275017
CurrentTrain: epoch  7, batch     4 | loss: 12.3611507Losses:  11.00195026397705 5.342304229736328 0.047551676630973816
CurrentTrain: epoch  7, batch     5 | loss: 11.0019503Losses:  11.490260124206543 4.826471328735352 0.04395928978919983
CurrentTrain: epoch  7, batch     6 | loss: 11.4902601Losses:  10.392969131469727 4.651520252227783 0.03814411908388138
CurrentTrain: epoch  7, batch     7 | loss: 10.3929691Losses:  12.430315017700195 6.098506450653076 0.0494077131152153
CurrentTrain: epoch  7, batch     8 | loss: 12.4303150Losses:  11.779485702514648 6.056028366088867 0.05239163711667061
CurrentTrain: epoch  7, batch     9 | loss: 11.7794857Losses:  10.845199584960938 4.717237949371338 0.045820239931344986
CurrentTrain: epoch  7, batch    10 | loss: 10.8451996Losses:  12.279756546020508 5.678478717803955 0.05504805967211723
CurrentTrain: epoch  7, batch    11 | loss: 12.2797565Losses:  10.65109634399414 4.488014221191406 0.05062369257211685
CurrentTrain: epoch  7, batch    12 | loss: 10.6510963Losses:  11.6542387008667 5.368305206298828 0.048519134521484375
CurrentTrain: epoch  7, batch    13 | loss: 11.6542387Losses:  13.720139503479004 7.427069187164307 0.045207202434539795
CurrentTrain: epoch  7, batch    14 | loss: 13.7201395Losses:  9.764731407165527 4.033633708953857 0.044901736080646515
CurrentTrain: epoch  7, batch    15 | loss: 9.7647314Losses:  10.176881790161133 4.535053253173828 0.042329609394073486
CurrentTrain: epoch  7, batch    16 | loss: 10.1768818Losses:  11.186882019042969 5.732003211975098 0.04646937549114227
CurrentTrain: epoch  7, batch    17 | loss: 11.1868820Losses:  10.771500587463379 5.4873528480529785 0.047989621758461
CurrentTrain: epoch  7, batch    18 | loss: 10.7715006Losses:  11.535089492797852 6.144762992858887 0.04888707026839256
CurrentTrain: epoch  7, batch    19 | loss: 11.5350895Losses:  12.345934867858887 5.865185737609863 0.04324556514620781
CurrentTrain: epoch  7, batch    20 | loss: 12.3459349Losses:  10.444124221801758 5.130692958831787 0.04612354561686516
CurrentTrain: epoch  7, batch    21 | loss: 10.4441242Losses:  12.464742660522461 5.407098770141602 0.04838109016418457
CurrentTrain: epoch  7, batch    22 | loss: 12.4647427Losses:  15.599995613098145 9.433728218078613 0.09353995323181152
CurrentTrain: epoch  7, batch    23 | loss: 15.5999956Losses:  11.039265632629395 5.531931400299072 0.039429616183042526
CurrentTrain: epoch  7, batch    24 | loss: 11.0392656Losses:  12.138117790222168 6.613720417022705 0.05800566077232361
CurrentTrain: epoch  7, batch    25 | loss: 12.1381178Losses:  14.599740982055664 8.774820327758789 0.054004453122615814
CurrentTrain: epoch  7, batch    26 | loss: 14.5997410Losses:  11.629305839538574 6.116415977478027 0.05159345269203186
CurrentTrain: epoch  7, batch    27 | loss: 11.6293058Losses:  14.392234802246094 7.8302412033081055 0.07198874652385712
CurrentTrain: epoch  7, batch    28 | loss: 14.3922348Losses:  10.316468238830566 4.834024429321289 0.03736467659473419
CurrentTrain: epoch  7, batch    29 | loss: 10.3164682Losses:  10.306130409240723 4.819607257843018 0.04156617447733879
CurrentTrain: epoch  7, batch    30 | loss: 10.3061304Losses:  11.395466804504395 6.000797271728516 0.0459398552775383
CurrentTrain: epoch  7, batch    31 | loss: 11.3954668Losses:  9.764922142028809 4.398059844970703 0.047890763729810715
CurrentTrain: epoch  7, batch    32 | loss: 9.7649221Losses:  10.591438293457031 5.376374244689941 0.04294532537460327
CurrentTrain: epoch  7, batch    33 | loss: 10.5914383Losses:  11.675312042236328 6.614248275756836 0.04575123265385628
CurrentTrain: epoch  7, batch    34 | loss: 11.6753120Losses:  8.659830093383789 3.4907617568969727 0.03711435943841934
CurrentTrain: epoch  7, batch    35 | loss: 8.6598301Losses:  10.500957489013672 5.33123779296875 0.049340829253196716
CurrentTrain: epoch  7, batch    36 | loss: 10.5009575Losses:  6.578190803527832 1.566491723060608 0.06357245147228241
CurrentTrain: epoch  7, batch    37 | loss: 6.5781908Losses:  11.101285934448242 6.057458400726318 0.04485341161489487
CurrentTrain: epoch  8, batch     0 | loss: 11.1012859Losses:  9.245832443237305 4.191166877746582 0.038926851004362106
CurrentTrain: epoch  8, batch     1 | loss: 9.2458324Losses:  14.808436393737793 9.862798690795898 0.04799962043762207
CurrentTrain: epoch  8, batch     2 | loss: 14.8084364Losses:  9.231523513793945 4.231715679168701 0.04068782180547714
CurrentTrain: epoch  8, batch     3 | loss: 9.2315235Losses:  9.179291725158691 4.22799015045166 0.04160008206963539
CurrentTrain: epoch  8, batch     4 | loss: 9.1792917Losses:  9.71870231628418 4.703195095062256 0.04269177466630936
CurrentTrain: epoch  8, batch     5 | loss: 9.7187023Losses:  10.941804885864258 6.037097930908203 0.043649110943078995
CurrentTrain: epoch  8, batch     6 | loss: 10.9418049Losses:  12.018689155578613 7.090282917022705 0.047263242304325104
CurrentTrain: epoch  8, batch     7 | loss: 12.0186892Losses:  10.554306030273438 5.615505695343018 0.04317323863506317
CurrentTrain: epoch  8, batch     8 | loss: 10.5543060Losses:  9.872145652770996 4.155301094055176 0.040110453963279724
CurrentTrain: epoch  8, batch     9 | loss: 9.8721457Losses:  12.579780578613281 6.331524848937988 0.06394043564796448
CurrentTrain: epoch  8, batch    10 | loss: 12.5797806Losses:  11.231524467468262 6.03718376159668 0.04242958873510361
CurrentTrain: epoch  8, batch    11 | loss: 11.2315245Losses:  9.434995651245117 4.2847981452941895 0.03600453957915306
CurrentTrain: epoch  8, batch    12 | loss: 9.4349957Losses:  10.080517768859863 5.050829887390137 0.03749839961528778
CurrentTrain: epoch  8, batch    13 | loss: 10.0805178Losses:  11.09351634979248 4.93921422958374 0.04272684082388878
CurrentTrain: epoch  8, batch    14 | loss: 11.0935163Losses:  10.023612022399902 4.617489337921143 0.04229292646050453
CurrentTrain: epoch  8, batch    15 | loss: 10.0236120Losses:  12.222315788269043 7.164017200469971 0.05530408024787903
CurrentTrain: epoch  8, batch    16 | loss: 12.2223158Losses:  14.22872257232666 8.986705780029297 0.048810042440891266
CurrentTrain: epoch  8, batch    17 | loss: 14.2287226Losses:  10.198689460754395 4.635430335998535 0.04765266925096512
CurrentTrain: epoch  8, batch    18 | loss: 10.1986895Losses:  11.017378807067871 6.0165300369262695 0.050155892968177795
CurrentTrain: epoch  8, batch    19 | loss: 11.0173788Losses:  12.434974670410156 6.9067702293396 0.05333691090345383
CurrentTrain: epoch  8, batch    20 | loss: 12.4349747Losses:  10.278295516967773 4.740538597106934 0.03479752689599991
CurrentTrain: epoch  8, batch    21 | loss: 10.2782955Losses:  12.465810775756836 7.403905391693115 0.04782667011022568
CurrentTrain: epoch  8, batch    22 | loss: 12.4658108Losses:  10.924056053161621 4.976737976074219 0.040232814848423004
CurrentTrain: epoch  8, batch    23 | loss: 10.9240561Losses:  11.219626426696777 5.802973747253418 0.03908655792474747
CurrentTrain: epoch  8, batch    24 | loss: 11.2196264Losses:  10.214580535888672 5.068211555480957 0.04690728336572647
CurrentTrain: epoch  8, batch    25 | loss: 10.2145805Losses:  13.648488998413086 6.944785118103027 0.044301994144916534
CurrentTrain: epoch  8, batch    26 | loss: 13.6484890Losses:  11.406250953674316 6.423225402832031 0.05159429833292961
CurrentTrain: epoch  8, batch    27 | loss: 11.4062510Losses:  10.61247730255127 5.400761127471924 0.03840702772140503
CurrentTrain: epoch  8, batch    28 | loss: 10.6124773Losses:  13.775028228759766 8.305689811706543 0.0595795214176178
CurrentTrain: epoch  8, batch    29 | loss: 13.7750282Losses:  9.967694282531738 5.099582672119141 0.039301712065935135
CurrentTrain: epoch  8, batch    30 | loss: 9.9676943Losses:  14.801877975463867 9.646875381469727 0.06605254113674164
CurrentTrain: epoch  8, batch    31 | loss: 14.8018780Losses:  10.836819648742676 5.986272811889648 0.047599587589502335
CurrentTrain: epoch  8, batch    32 | loss: 10.8368196Losses:  9.68326187133789 4.613973617553711 0.03705204278230667
CurrentTrain: epoch  8, batch    33 | loss: 9.6832619Losses:  8.722150802612305 3.814552068710327 0.03446612134575844
CurrentTrain: epoch  8, batch    34 | loss: 8.7221508Losses:  10.36518669128418 5.652866363525391 0.04911772161722183
CurrentTrain: epoch  8, batch    35 | loss: 10.3651867Losses:  8.347092628479004 3.4172329902648926 0.03302130103111267
CurrentTrain: epoch  8, batch    36 | loss: 8.3470926Losses:  6.678256988525391 1.6033446788787842 0.06782635301351547
CurrentTrain: epoch  8, batch    37 | loss: 6.6782570Losses:  10.945164680480957 5.213252067565918 0.03272045776247978
CurrentTrain: epoch  9, batch     0 | loss: 10.9451647Losses:  15.018755912780762 9.195784568786621 0.03756814822554588
CurrentTrain: epoch  9, batch     1 | loss: 15.0187559Losses:  8.81583023071289 3.8742170333862305 0.030644841492176056
CurrentTrain: epoch  9, batch     2 | loss: 8.8158302Losses:  10.040724754333496 5.189330101013184 0.048178672790527344
CurrentTrain: epoch  9, batch     3 | loss: 10.0407248Losses:  10.042923927307129 5.064560890197754 0.03527866676449776
CurrentTrain: epoch  9, batch     4 | loss: 10.0429239Losses:  9.879067420959473 5.022988319396973 0.03961696848273277
CurrentTrain: epoch  9, batch     5 | loss: 9.8790674Losses:  10.094539642333984 5.230074882507324 0.03418587148189545
CurrentTrain: epoch  9, batch     6 | loss: 10.0945396Losses:  9.563907623291016 4.650809288024902 0.04163472726941109
CurrentTrain: epoch  9, batch     7 | loss: 9.5639076Losses:  8.345351219177246 3.4515857696533203 0.0303091648966074
CurrentTrain: epoch  9, batch     8 | loss: 8.3453512Losses:  14.645771980285645 9.725364685058594 0.036980412900447845
CurrentTrain: epoch  9, batch     9 | loss: 14.6457720Losses:  10.900511741638184 5.955677032470703 0.05304070562124252
CurrentTrain: epoch  9, batch    10 | loss: 10.9005117Losses:  12.437078475952148 7.60675048828125 0.045791544020175934
CurrentTrain: epoch  9, batch    11 | loss: 12.4370785Losses:  9.814504623413086 4.454162120819092 0.03768553584814072
CurrentTrain: epoch  9, batch    12 | loss: 9.8145046Losses:  9.614054679870605 4.712532997131348 0.041395027190446854
CurrentTrain: epoch  9, batch    13 | loss: 9.6140547Losses:  10.666211128234863 5.755980491638184 0.041215211153030396
CurrentTrain: epoch  9, batch    14 | loss: 10.6662111Losses:  10.005367279052734 4.521249771118164 0.03400037810206413
CurrentTrain: epoch  9, batch    15 | loss: 10.0053673Losses:  10.810829162597656 5.953219413757324 0.04834849387407303
CurrentTrain: epoch  9, batch    16 | loss: 10.8108292Losses:  15.149998664855957 8.256232261657715 0.038739483803510666
CurrentTrain: epoch  9, batch    17 | loss: 15.1499987Losses:  11.611742973327637 6.154958248138428 0.03660525754094124
CurrentTrain: epoch  9, batch    18 | loss: 11.6117430Losses:  10.972533226013184 5.825784683227539 0.04282963648438454
CurrentTrain: epoch  9, batch    19 | loss: 10.9725332Losses:  10.773042678833008 5.529105186462402 0.040521446615457535
CurrentTrain: epoch  9, batch    20 | loss: 10.7730427Losses:  10.916666984558105 6.082855701446533 0.03947705775499344
CurrentTrain: epoch  9, batch    21 | loss: 10.9166670Losses:  12.169917106628418 7.093170166015625 0.041748613119125366
CurrentTrain: epoch  9, batch    22 | loss: 12.1699171Losses:  13.381136894226074 7.990601062774658 0.05081881955265999
CurrentTrain: epoch  9, batch    23 | loss: 13.3811369Losses:  11.427901268005371 6.03435754776001 0.0486714132130146
CurrentTrain: epoch  9, batch    24 | loss: 11.4279013Losses:  10.35578441619873 5.1379170417785645 0.04164604842662811
CurrentTrain: epoch  9, batch    25 | loss: 10.3557844Losses:  12.27419376373291 6.072952747344971 0.05121144652366638
CurrentTrain: epoch  9, batch    26 | loss: 12.2741938Losses:  11.230582237243652 6.2340898513793945 0.04148772358894348
CurrentTrain: epoch  9, batch    27 | loss: 11.2305822Losses:  8.945135116577148 4.18138313293457 0.03633779287338257
CurrentTrain: epoch  9, batch    28 | loss: 8.9451351Losses:  11.770059585571289 6.126330852508545 0.04999363422393799
CurrentTrain: epoch  9, batch    29 | loss: 11.7700596Losses:  13.591240882873535 8.874677658081055 0.05013582482933998
CurrentTrain: epoch  9, batch    30 | loss: 13.5912409Losses:  12.153865814208984 6.999767303466797 0.048050519078969955
CurrentTrain: epoch  9, batch    31 | loss: 12.1538658Losses:  10.815380096435547 5.485726833343506 0.040189582854509354
CurrentTrain: epoch  9, batch    32 | loss: 10.8153801Losses:  10.93748664855957 5.34414005279541 0.03970656543970108
CurrentTrain: epoch  9, batch    33 | loss: 10.9374866Losses:  14.523512840270996 7.352354049682617 0.04004248231649399
CurrentTrain: epoch  9, batch    34 | loss: 14.5235128Losses:  10.071390151977539 5.111560821533203 0.038319554179906845
CurrentTrain: epoch  9, batch    35 | loss: 10.0713902Losses:  11.299003601074219 6.126816272735596 0.03655773401260376
CurrentTrain: epoch  9, batch    36 | loss: 11.2990036Losses:  6.130606174468994 0.6937470436096191 0.05373018980026245
CurrentTrain: epoch  9, batch    37 | loss: 6.1306062
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: head of the atomic energy organization of iran -lrb- aeoi -rrb- ali akbar salehi said thursday that iran has proposed russia to establish a joint nuclear consortium to supply fuel for the bushehr plant and plants to be built in iran , the english language satellite press tv reported .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, the renowned artist decided to return to her roots in Italy, where she felt a deep connection to her heritage.  
Head Entity: the renowned artist  
Tail Entity: Italy  

Relation: person countries of residence  
Context: Following his successful career in the tech industry, John moved to Canada to enjoy a quieter life surrounded by nature and a vibrant community.  
Head Entity: John  
Tail Entity: Canada  

Relation: person countries of residence  
Context: The famous author, known for her best-selling novels, has spent the last decade residing in France, where she draws inspiration from the picturesque landscapes.  
Head Entity: the famous author  
Tail Entity: France  

Relation: person countries of residence  
Context: After completing his studies in Australia, Mark decided to settle in New Zealand, where he found a job that perfectly matched his skills and interests.  
Head Entity: Mark  
Tail Entity: New Zealand  

Relation: person countries of residence  
Context: The diplomat, who has represented his country in various international forums, currently lives in Germany, fostering relationships with European leaders.  
Head Entity: the diplomat  
Tail Entity: Germany  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: new york lawyer joseph angland , who heads the american bar association 's antitrust division , said the rules set early in the 20th century `` do n't stand up to scrutiny today . ''
Head Entity: american bar association
Tail Entity: joseph angland
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: the ceo of tech innovations, sarah connor, announced a new initiative to enhance employee engagement at the annual company meeting.  
Head Entity: tech innovations  
Tail Entity: sarah connor  

Relation: organization top members employees  
Context: during the annual conference, the president of green earth initiatives, michael green, emphasized the importance of sustainability in the workplace.  
Head Entity: green earth initiatives  
Tail Entity: michael green  

Relation: organization top members employees  
Context: at the charity gala, the founder of helping hands foundation, emily roberts, shared her vision for expanding the organization's outreach programs.  
Head Entity: helping hands foundation  
Tail Entity: emily roberts  

Relation: organization top members employees  
Context: the board chair of global health alliance, dr. john smith, presented the new health policy recommendations to improve community health services.  
Head Entity: global health alliance  
Tail Entity: dr. john smith  

Relation: organization top members employees  
Context: the director of creative arts collective, lila thompson, unveiled a new mentorship program aimed at supporting young artists in the community.  
Head Entity: creative arts collective  
Tail Entity: lila thompson  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: at the request of local environmentalists , the taipei high administrative court ordered the environmental protection administration -lrb- epa -rrb- , the national science council -lrb- nsc -rrb- and the central taiwan science park administration -lrb- ctspa -rrb- july 30 to suspend the development of both the houli and the erlin branches .
Head Entity: nsc
Tail Entity: national science council
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The United Nations Educational, Scientific and Cultural Organization (UNESCO) has been a member of various international coalitions aimed at promoting education and cultural preservation.  
Head Entity: UNESCO  
Tail Entity: United Nations Educational, Scientific and Cultural Organization  

Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) collaborates with numerous national societies to provide humanitarian aid worldwide.  
Head Entity: IFRC  
Tail Entity: International Federation of Red Cross and Red Crescent Societies  

Relation: organization member of  
Context: The World Health Organization (WHO) is a member of the Global Health Security Agenda, working alongside other health organizations to combat infectious diseases.  
Head Entity: WHO  
Tail Entity: World Health Organization  

Relation: organization member of  
Context: The International Olympic Committee (IOC) oversees the Olympic Games and is a member of various sports federations around the globe.  
Head Entity: IOC  
Tail Entity: International Olympic Committee  

Relation: organization member of  
Context: The European Union (EU) is a member of the World Trade Organization (WTO), participating in discussions about global trade policies.  
Head Entity: EU  
Tail Entity: European Union  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: iranian nuclear negotiator ali larijani said thursday in ankara that talks on settling the iranian nuclear crisis had made some progress towards a `` united view . ''
Head Entity: ali larijani
Tail Entity: iranian
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The famous physicist Albert Einstein was born in Germany before moving to the United States.  
Head Entity: Albert Einstein  
Tail Entity: Germany  

Relation: person origin  
Context: The renowned author Chimamanda Ngozi Adichie often speaks about her Nigerian heritage in her works.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigeria  

Relation: person origin  
Context: The celebrated actor Sofia Vergara has often mentioned her Colombian roots in interviews.  
Head Entity: Sofia Vergara  
Tail Entity: Colombia  

Relation: person origin  
Context: The legendary musician Bob Marley is known for his Jamaican heritage and reggae music.  
Head Entity: Bob Marley  
Tail Entity: Jamaica  

Relation: person origin  
Context: The influential civil rights leader Martin Luther King Jr. was born in the United States and is celebrated for his American legacy.  
Head Entity: Martin Luther King Jr.  
Tail Entity: United States  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving growth."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the conference, Dr. Alan Smith, the lead researcher at BioHealth Labs, presented groundbreaking findings on cancer treatment."  
Head Entity: Dr. Alan Smith  
Tail Entity: lead researcher  

Relation: person title  
Context: "In her new role, Lisa Chen will serve as the Director of Marketing for Green Solutions, focusing on sustainable product development."  
Head Entity: Lisa Chen  
Tail Entity: Director of Marketing  

Relation: person title  
Context: "As the head coach of the national soccer team, John Doe has led the team to several championships over the past decade."  
Head Entity: John Doe  
Tail Entity: head coach  

Relation: person title  
Context: "With over 20 years of experience in the industry, Robert Johnson has taken on the role of Senior Vice President at Global Enterprises."  
Head Entity: Robert Johnson  
Tail Entity: Senior Vice President  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: `` if provocations do not end or military action intensifies , we wo n't just sit there , '' he told russia 's ria-novosti news agency .
Head Entity: ria-novosti
Tail Entity: russia
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: The headquarters of the tech giant is located in Silicon Valley, California, where it has been a major player in the industry.  
Head Entity: tech giant  
Tail Entity: California  

Relation: organization country of headquarters  
Context: The United Nations, based in New York City, plays a crucial role in international diplomacy and peacekeeping.  
Head Entity: United Nations  
Tail Entity: New York  

Relation: organization country of headquarters  
Context: Volkswagen has its main office in Wolfsburg, Germany, where it oversees its global operations.  
Head Entity: Volkswagen  
Tail Entity: Germany  

Relation: organization country of headquarters  
Context: The World Health Organization, headquartered in Geneva, Switzerland, is responsible for global public health initiatives.  
Head Entity: World Health Organization  
Tail Entity: Switzerland  

Relation: organization country of headquarters  
Context: Samsung Electronics operates from its headquarters in Suwon, South Korea, leading the market in technology innovation.  
Head Entity: Samsung Electronics  
Tail Entity: South Korea  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 84.85%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 80.15%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 84.85%   
cur_acc:  ['0.8485']
his_acc:  ['0.8485']
Clustering into  4  clusters
Clusters:  [1 0 0 1 0 2 1 1 3 0 1]
Losses:  15.480242729187012 8.14104175567627 0.3793518543243408
CurrentTrain: epoch  0, batch     0 | loss: 15.4802427Losses:  11.408994674682617 2.8118209838867188 0.33216139674186707
CurrentTrain: epoch  0, batch     1 | loss: 11.4089947Losses:  15.435354232788086 7.985107421875 0.3656344413757324
CurrentTrain: epoch  1, batch     0 | loss: 15.4353542Losses:  7.83984375 2.222308397293091 0.30229175090789795
CurrentTrain: epoch  1, batch     1 | loss: 7.8398438Losses:  13.143253326416016 7.225139141082764 0.22293853759765625
CurrentTrain: epoch  2, batch     0 | loss: 13.1432533Losses:  7.912862777709961 2.5627517700195312 0.28098490834236145
CurrentTrain: epoch  2, batch     1 | loss: 7.9128628Losses:  14.002577781677246 8.453619956970215 0.22652851045131683
CurrentTrain: epoch  3, batch     0 | loss: 14.0025778Losses:  8.01799201965332 3.2056150436401367 0.10445260256528854
CurrentTrain: epoch  3, batch     1 | loss: 8.0179920Losses:  11.96462631225586 6.684503078460693 0.2571052312850952
CurrentTrain: epoch  4, batch     0 | loss: 11.9646263Losses:  6.520382881164551 1.9926806688308716 0.20685988664627075
CurrentTrain: epoch  4, batch     1 | loss: 6.5203829Losses:  10.84306526184082 7.465819835662842 0.18381576240062714
CurrentTrain: epoch  5, batch     0 | loss: 10.8430653Losses:  5.735666751861572 2.4455511569976807 0.26386702060699463
CurrentTrain: epoch  5, batch     1 | loss: 5.7356668Losses:  9.789170265197754 6.409958362579346 0.19081032276153564
CurrentTrain: epoch  6, batch     0 | loss: 9.7891703Losses:  4.824623107910156 2.002527952194214 0.18518424034118652
CurrentTrain: epoch  6, batch     1 | loss: 4.8246231Losses:  9.414222717285156 6.736073970794678 0.17061273753643036
CurrentTrain: epoch  7, batch     0 | loss: 9.4142227Losses:  6.144002437591553 3.218719959259033 0.20754879713058472
CurrentTrain: epoch  7, batch     1 | loss: 6.1440024Losses:  9.789116859436035 7.053234100341797 0.1914445161819458
CurrentTrain: epoch  8, batch     0 | loss: 9.7891169Losses:  5.500617027282715 3.0113792419433594 0.1609555184841156
CurrentTrain: epoch  8, batch     1 | loss: 5.5006170Losses:  7.922000885009766 5.40251350402832 0.16714191436767578
CurrentTrain: epoch  9, batch     0 | loss: 7.9220009Losses:  3.5267109870910645 1.2165766954421997 0.17523585259914398
CurrentTrain: epoch  9, batch     1 | loss: 3.5267110
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  

Relation: person date of birth  
Context: the legendary musician freddie mercury was born farrokh bulsara on september 5, 1946, in zanzibar, tanzania.  
Head Entity: freddie mercury  
Tail Entity: september 5, 1946  

Relation: person date of birth  
Context: the influential civil rights leader martin luther king jr. was born on january 15, 1929, in atlanta, georgia.  
Head Entity: martin luther king jr.  
Tail Entity: january 15, 1929  

Relation: person date of birth  
Context: the famous painter pablo picasso was born on october 25, 1881, in malaga, spain.  
Head Entity: pablo picasso  
Tail Entity: october 25, 1881  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jefferson joseph deblanc sr. was born in lockport , la. , on feb. 15 , 1921 , and grew up in st. martinville .
Head Entity: jefferson joseph deblanc sr.
Tail Entity: la.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: marilyn monroe was born in los angeles, california, on june 1, 1926, and became a famous actress.  
Head Entity: marilyn monroe  
Tail Entity: california  

Relation: person stateorprovince of birth  
Context: barack obama was born in honolulu, hawaii, on august 4, 1961, and later became the 44th president of the united states.  
Head Entity: barack obama  
Tail Entity: hawaii  

Relation: person stateorprovince of birth  
Context: agatha christie was born in torquay, devon, on september 15, 1890, and is known for her detective novels.  
Head Entity: agatha christie  
Tail Entity: devon  

Relation: person stateorprovince of birth  
Context: pablo picasso was born in mlaga, spain, on october 25, 1881, and became one of the most influential artists of the 20th century.  
Head Entity: pablo picasso  
Tail Entity: spain  

Relation: person stateorprovince of birth  
Context: frida kahlo was born in coyoacn, mexico, on july 6, 1907, and is celebrated for her unique painting style.  
Head Entity: frida kahlo  
Tail Entity: mexico  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: After the ceremony, Michael's father, John Smith, shared heartfelt stories about his son's childhood.  
Head Entity: Michael  
Tail Entity: John Smith  

Relation: person parents  
Context: During the family reunion, Emily's mother, Sarah Johnson, prepared her famous apple pie that everyone loves.  
Head Entity: Emily  
Tail Entity: Sarah Johnson  

Relation: person parents  
Context: At the graduation party, Lisa's dad, Robert Brown, proudly announced her acceptance into medical school.  
Head Entity: Lisa  
Tail Entity: Robert Brown  

Relation: person parents  
Context: While reminiscing about their childhood, Tom's mother, Patricia White, recalled the summer vacations they spent at the lake.  
Head Entity: Tom  
Tail Entity: Patricia White  

Relation: person parents  
Context: In the documentary, Anna's father, Michael Green, discussed the challenges of raising a child with special needs.  
Head Entity: Anna  
Tail Entity: Michael Green  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work, Maria finally landed a job at the prestigious tech company, where she collaborates with some of the brightest minds in the industry.  
Head Entity: Maria  
Tail Entity: prestigious tech company  

Relation: person employee of  
Context: John has been with the firm for over a decade, contributing to numerous successful projects and earning the respect of his colleagues.  
Head Entity: John  
Tail Entity: the firm  

Relation: person employee of  
Context: As a lead designer at the fashion house, Sarah showcases her creativity and innovation in every collection she produces.  
Head Entity: Sarah  
Tail Entity: the fashion house  

Relation: person employee of  
Context: After graduating from university, Tom accepted a position at a well-known financial institution, where he quickly climbed the corporate ladder.  
Head Entity: Tom  
Tail Entity: well-known financial institution  

Relation: person employee of  
Context: Emily's dedication to her role at the non-profit organization has made a significant impact on the community, helping countless individuals in need.  
Head Entity: Emily  
Tail Entity: non-profit organization  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his thrilling novels, passed away unexpectedly in his sleep at his residence in los angeles, ca.  
Head Entity: john doe  
Tail Entity: ca.  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, died peacefully in her home located in bel air, ca.  
Head Entity: elizabeth taylor  
Tail Entity: ca.  

Relation: person stateorprovince of death  
Context: the famous physicist, albert einstein, took his last breath in princeton, nj, where he had lived for many years.  
Head Entity: albert einstein  
Tail Entity: nj.  

Relation: person stateorprovince of death  
Context: the beloved musician, prince, was found dead in his home in minneapolis, mn, shocking fans around the world.  
Head Entity: prince  
Tail Entity: mn.  

Relation: person stateorprovince of death  
Context: the legendary civil rights leader, martin luther king jr., was assassinated in memphis, tn, a tragic event that changed the course of history.  
Head Entity: martin luther king jr.  
Tail Entity: tn.  
Losses:  11.705554008483887 5.334738731384277 0.29655587673187256
MemoryTrain:  epoch  0, batch     0 | loss: 11.7055540Losses:  8.025955200195312 2.1450254917144775 0.2992461323738098
MemoryTrain:  epoch  0, batch     1 | loss: 8.0259552Losses:  10.146951675415039 3.718895673751831 0.2018137276172638
MemoryTrain:  epoch  0, batch     2 | loss: 10.1469517Losses:  10.604593276977539 3.3832802772521973 0.3208230137825012
MemoryTrain:  epoch  0, batch     3 | loss: 10.6045933Losses:  3.9859321117401123 -0.0 0.22811976075172424
MemoryTrain:  epoch  0, batch     4 | loss: 3.9859321Losses:  10.142163276672363 2.8423478603363037 0.2947457432746887
MemoryTrain:  epoch  1, batch     0 | loss: 10.1421633Losses:  10.696203231811523 3.989427089691162 0.1558557152748108
MemoryTrain:  epoch  1, batch     1 | loss: 10.6962032Losses:  8.661163330078125 2.8810462951660156 0.2802770435810089
MemoryTrain:  epoch  1, batch     2 | loss: 8.6611633Losses:  8.285100936889648 2.9942946434020996 0.38133537769317627
MemoryTrain:  epoch  1, batch     3 | loss: 8.2851009Losses:  3.4827191829681396 -0.0 0.36086446046829224
MemoryTrain:  epoch  1, batch     4 | loss: 3.4827192Losses:  7.2908172607421875 2.223633050918579 0.18645942211151123
MemoryTrain:  epoch  2, batch     0 | loss: 7.2908173Losses:  8.464341163635254 3.6428050994873047 0.32004401087760925
MemoryTrain:  epoch  2, batch     1 | loss: 8.4643412Losses:  8.213183403015137 3.0126423835754395 0.33485686779022217
MemoryTrain:  epoch  2, batch     2 | loss: 8.2131834Losses:  8.415792465209961 2.6976418495178223 0.29231658577919006
MemoryTrain:  epoch  2, batch     3 | loss: 8.4157925Losses:  2.3000495433807373 -0.0 0.15732033550739288
MemoryTrain:  epoch  2, batch     4 | loss: 2.3000495Losses:  7.319577217102051 4.571494102478027 0.3135615587234497
MemoryTrain:  epoch  3, batch     0 | loss: 7.3195772Losses:  7.781561374664307 3.168844699859619 0.3322717547416687
MemoryTrain:  epoch  3, batch     1 | loss: 7.7815614Losses:  9.211328506469727 3.517979621887207 0.3716779053211212
MemoryTrain:  epoch  3, batch     2 | loss: 9.2113285Losses:  9.976971626281738 6.0839009284973145 0.20499281585216522
MemoryTrain:  epoch  3, batch     3 | loss: 9.9769716Losses:  7.788110256195068 -0.0 -0.0
MemoryTrain:  epoch  3, batch     4 | loss: 7.7881103Losses:  8.688868522644043 4.127544403076172 0.2865857779979706
MemoryTrain:  epoch  4, batch     0 | loss: 8.6888685Losses:  9.325422286987305 4.4957170486450195 0.3569321632385254
MemoryTrain:  epoch  4, batch     1 | loss: 9.3254223Losses:  5.821463108062744 2.416566848754883 0.28104642033576965
MemoryTrain:  epoch  4, batch     2 | loss: 5.8214631Losses:  6.0463151931762695 2.263392448425293 0.20890121161937714
MemoryTrain:  epoch  4, batch     3 | loss: 6.0463152Losses:  7.293379783630371 -0.0 0.07155437022447586
MemoryTrain:  epoch  4, batch     4 | loss: 7.2933798Losses:  7.194293975830078 2.656759738922119 0.1834472417831421
MemoryTrain:  epoch  5, batch     0 | loss: 7.1942940Losses:  5.030355930328369 1.5542263984680176 0.30962127447128296
MemoryTrain:  epoch  5, batch     1 | loss: 5.0303559Losses:  7.957804203033447 3.2751681804656982 0.3367554545402527
MemoryTrain:  epoch  5, batch     2 | loss: 7.9578042Losses:  5.492966175079346 1.9950135946273804 0.3181045651435852
MemoryTrain:  epoch  5, batch     3 | loss: 5.4929662Losses:  1.9468426704406738 -0.0 -0.0
MemoryTrain:  epoch  5, batch     4 | loss: 1.9468427Losses:  5.450529098510742 2.3568854331970215 0.060259804129600525
MemoryTrain:  epoch  6, batch     0 | loss: 5.4505291Losses:  7.839588165283203 4.207571506500244 0.2865114212036133
MemoryTrain:  epoch  6, batch     1 | loss: 7.8395882Losses:  6.270343780517578 3.4141693115234375 0.3407924175262451
MemoryTrain:  epoch  6, batch     2 | loss: 6.2703438Losses:  8.562365531921387 4.247450828552246 0.1884397715330124
MemoryTrain:  epoch  6, batch     3 | loss: 8.5623655Losses:  3.6390786170959473 -0.0 0.10631786286830902
MemoryTrain:  epoch  6, batch     4 | loss: 3.6390786Losses:  5.915499687194824 2.8675971031188965 0.3332046568393707
MemoryTrain:  epoch  7, batch     0 | loss: 5.9154997Losses:  6.934886932373047 4.3135223388671875 0.2865264415740967
MemoryTrain:  epoch  7, batch     1 | loss: 6.9348869Losses:  6.427268981933594 2.7756824493408203 0.25888293981552124
MemoryTrain:  epoch  7, batch     2 | loss: 6.4272690Losses:  4.948677062988281 2.240400791168213 0.19209198653697968
MemoryTrain:  epoch  7, batch     3 | loss: 4.9486771Losses:  3.646493434906006 -0.0 0.0837862491607666
MemoryTrain:  epoch  7, batch     4 | loss: 3.6464934Losses:  5.757976531982422 2.864266872406006 0.2727017402648926
MemoryTrain:  epoch  8, batch     0 | loss: 5.7579765Losses:  4.41184663772583 1.821579933166504 0.32397401332855225
MemoryTrain:  epoch  8, batch     1 | loss: 4.4118466Losses:  5.364590167999268 2.3755905628204346 0.320995956659317
MemoryTrain:  epoch  8, batch     2 | loss: 5.3645902Losses:  4.994659900665283 2.3657307624816895 0.17740873992443085
MemoryTrain:  epoch  8, batch     3 | loss: 4.9946599Losses:  2.588726758956909 -0.0 -0.0
MemoryTrain:  epoch  8, batch     4 | loss: 2.5887268Losses:  4.153841018676758 1.590133786201477 0.20391711592674255
MemoryTrain:  epoch  9, batch     0 | loss: 4.1538410Losses:  4.921629905700684 2.1237056255340576 0.1814355105161667
MemoryTrain:  epoch  9, batch     1 | loss: 4.9216299Losses:  6.167572975158691 3.807363986968994 0.313060998916626
MemoryTrain:  epoch  9, batch     2 | loss: 6.1675730Losses:  4.884148120880127 2.3556041717529297 0.19117659330368042
MemoryTrain:  epoch  9, batch     3 | loss: 4.8841481Losses:  2.7911713123321533 -0.0 0.09875061362981796
MemoryTrain:  epoch  9, batch     4 | loss: 2.7911713
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 92.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 87.95%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 81.70%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 79.61%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 87.31%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 87.13%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.67%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 87.84%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.16%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.46%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 88.28%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.57%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 88.84%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 88.95%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 88.78%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 88.89%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 88.18%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 86.30%   
cur_acc:  ['0.8485', '0.8795']
his_acc:  ['0.8485', '0.8630']
Clustering into  7  clusters
Clusters:  [1 0 2 1 0 4 6 1 5 0 1 1 2 2 3 2]
Losses:  15.226107597351074 8.051651954650879 0.2913562059402466
CurrentTrain: epoch  0, batch     0 | loss: 15.2261076Losses:  12.975377082824707 4.284668922424316 0.2755027115345001
CurrentTrain: epoch  0, batch     1 | loss: 12.9753771Losses:  13.22933578491211 6.608489990234375 0.22824445366859436
CurrentTrain: epoch  1, batch     0 | loss: 13.2293358Losses:  11.342177391052246 3.513484239578247 0.3016819655895233
CurrentTrain: epoch  1, batch     1 | loss: 11.3421774Losses:  14.089207649230957 7.495888710021973 0.257598876953125
CurrentTrain: epoch  2, batch     0 | loss: 14.0892076Losses:  7.1981401443481445 1.8533766269683838 0.2278091162443161
CurrentTrain: epoch  2, batch     1 | loss: 7.1981401Losses:  12.228480339050293 7.099356651306152 0.2467777132987976
CurrentTrain: epoch  3, batch     0 | loss: 12.2284803Losses:  8.480128288269043 2.1221296787261963 0.2652568519115448
CurrentTrain: epoch  3, batch     1 | loss: 8.4801283Losses:  12.025723457336426 6.48841667175293 0.2048632949590683
CurrentTrain: epoch  4, batch     0 | loss: 12.0257235Losses:  6.979622840881348 2.628047466278076 0.254650741815567
CurrentTrain: epoch  4, batch     1 | loss: 6.9796228Losses:  10.7952880859375 6.418986797332764 0.19926071166992188
CurrentTrain: epoch  5, batch     0 | loss: 10.7952881Losses:  7.609441757202148 3.0409839153289795 0.225581094622612
CurrentTrain: epoch  5, batch     1 | loss: 7.6094418Losses:  10.572439193725586 6.444286346435547 0.22269460558891296
CurrentTrain: epoch  6, batch     0 | loss: 10.5724392Losses:  5.845552444458008 1.591520071029663 0.19022393226623535
CurrentTrain: epoch  6, batch     1 | loss: 5.8455524Losses:  12.972487449645996 8.189203262329102 0.20094776153564453
CurrentTrain: epoch  7, batch     0 | loss: 12.9724874Losses:  5.8707594871521 3.0246708393096924 0.19150862097740173
CurrentTrain: epoch  7, batch     1 | loss: 5.8707595Losses:  10.315547943115234 6.634852886199951 0.19179871678352356
CurrentTrain: epoch  8, batch     0 | loss: 10.3155479Losses:  6.85701847076416 2.646467685699463 0.19463074207305908
CurrentTrain: epoch  8, batch     1 | loss: 6.8570185Losses:  9.944604873657227 7.357756614685059 0.20035094022750854
CurrentTrain: epoch  9, batch     0 | loss: 9.9446049Losses:  10.988061904907227 5.696646690368652 0.1485845446586609
CurrentTrain: epoch  9, batch     1 | loss: 10.9880619
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she mentioned that despite living in the united states for over a decade, her roots are firmly planted in the vibrant landscapes of brazil where she was born.  
Head Entity: she  
Tail Entity: brazil  

Relation: person country of birth  
Context: the documentary highlighted the life of the famous artist, who was born in spain but later moved to france to pursue his career.  
Head Entity: the famous artist  
Tail Entity: spain  

Relation: person country of birth  
Context: as a child, emma often spoke about her early years in nigeria, where she was born before her family relocated to canada.  
Head Entity: emma  
Tail Entity: nigeria  

Relation: person country of birth  
Context: the biography revealed that the renowned scientist was born in japan, a fact that influenced his research on environmental issues.  
Head Entity: the renowned scientist  
Tail Entity: japan  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.nike.com for the latest sports gear.  
Head Entity: Nike  
Tail Entity: https://www.nike.com  

Relation: organization website  
Context: For more information, check out the website at https://www.wikipedia.org.  
Head Entity: Wikipedia  
Tail Entity: https://www.wikipedia.org  

Relation: organization website  
Context: The tech giant's homepage can be found at http://www.apple.com.  
Head Entity: Apple  
Tail Entity: http://www.apple.com  

Relation: organization website  
Context: Explore the latest news on their site: https://www.bbc.com.  
Head Entity: BBC  
Tail Entity: https://www.bbc.com  

Relation: organization website  
Context: You can find their services at https://www.microsoft.com.  
Head Entity: Microsoft  
Tail Entity: https://www.microsoft.com  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple has seen significant investments from warren buffett's berkshire hathaway.  
Head Entity: apple  
Tail Entity: berkshire hathaway  

Relation: organization shareholders  
Context: the renowned investment firm blackrock has acquired a substantial stake in the renewable energy company nextera energy.  
Head Entity: nextera energy  
Tail Entity: blackrock  

Relation: organization shareholders  
Context: the popular streaming service netflix has received funding from the venture capital firm sequoia capital.  
Head Entity: netflix  
Tail Entity: sequoia capital  

Relation: organization shareholders  
Context: the pharmaceutical company pfizer has been partially owned by the investment group fidelity investments for several years.  
Head Entity: pfizer  
Tail Entity: fidelity investments  

Relation: organization shareholders  
Context: the automotive manufacturer tesla has attracted investments from the tech company google.  
Head Entity: tesla  
Tail Entity: google  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: After years of financial difficulties, the local charity, Helping Hands, announced its closure in January 2019, leaving many in the community without support.  
Head Entity: Helping Hands  
Tail Entity: January 2019  

Relation: organization dissolved  
Context: The historic publishing house, Classic Reads, was dissolved in July 2021, marking the end of an era in literary history.  
Head Entity: Classic Reads  
Tail Entity: July 2021  

Relation: organization dissolved  
Context: Following a series of scandals, the non-profit organization, Clean Water Initiative, was officially dissolved in February 2022.  
Head Entity: Clean Water Initiative  
Tail Entity: February 2022  

Relation: organization dissolved  
Context: The beloved local theater company, Stage Dreams, announced its dissolution in October 2018 due to declining ticket sales and lack of funding.  
Head Entity: Stage Dreams  
Tail Entity: October 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. John Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. John Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the famous actress and philanthropist, Emily Johnson, to support underprivileged children.  
Head Entity: Hope for Tomorrow  
Tail Entity: Emily Johnson  

Relation: organization founded by  
Context: The tech startup, GreenTech Solutions, was co-founded by environmentalist Mark Thompson and his partner Sarah Lee to develop sustainable energy solutions.  
Head Entity: GreenTech Solutions  
Tail Entity: Mark Thompson  

Relation: organization founded by  
Context: In 1998, the non-profit organization, Clean Oceans Initiative, was founded by marine biologist Dr. Lisa Carter to combat ocean pollution.  
Head Entity: Clean Oceans Initiative  
Tail Entity: Dr. Lisa Carter  

Relation: organization founded by  
Context: The educational institution, Future Leaders Academy, was established in 2015 by former mayor and educator, Robert Garcia, to empower youth through leadership training.  
Head Entity: Future Leaders Academy  
Tail Entity: Robert Garcia  
Losses:  7.690607070922852 1.5999197959899902 0.48821204900741577
MemoryTrain:  epoch  0, batch     0 | loss: 7.6906071Losses:  6.004996299743652 1.0619921684265137 0.4239278733730316
MemoryTrain:  epoch  0, batch     1 | loss: 6.0049963Losses:  5.881938457489014 0.865071713924408 0.6337596774101257
MemoryTrain:  epoch  0, batch     2 | loss: 5.8819385Losses:  7.034947395324707 2.5126571655273438 0.5328406095504761
MemoryTrain:  epoch  0, batch     3 | loss: 7.0349474Losses:  7.150362491607666 1.2354164123535156 0.6121000647544861
MemoryTrain:  epoch  0, batch     4 | loss: 7.1503625Losses:  6.501596450805664 1.2213430404663086 0.6349802017211914
MemoryTrain:  epoch  0, batch     5 | loss: 6.5015965Losses:  6.446406841278076 1.0457179546356201 0.5021805167198181
MemoryTrain:  epoch  1, batch     0 | loss: 6.4464068Losses:  7.917309284210205 2.445101261138916 0.3140467405319214
MemoryTrain:  epoch  1, batch     1 | loss: 7.9173093Losses:  6.210970878601074 1.8470616340637207 0.5108985900878906
MemoryTrain:  epoch  1, batch     2 | loss: 6.2109709Losses:  9.162847518920898 3.29266357421875 0.5641373991966248
MemoryTrain:  epoch  1, batch     3 | loss: 9.1628475Losses:  6.74007511138916 1.2682372331619263 0.3455532193183899
MemoryTrain:  epoch  1, batch     4 | loss: 6.7400751Losses:  5.242829322814941 1.0557034015655518 0.630359411239624
MemoryTrain:  epoch  1, batch     5 | loss: 5.2428293Losses:  6.339935779571533 1.3229172229766846 0.43221911787986755
MemoryTrain:  epoch  2, batch     0 | loss: 6.3399358Losses:  7.023126125335693 1.9373116493225098 0.386451780796051
MemoryTrain:  epoch  2, batch     1 | loss: 7.0231261Losses:  7.665503025054932 1.6756470203399658 0.548839271068573
MemoryTrain:  epoch  2, batch     2 | loss: 7.6655030Losses:  6.373917579650879 1.616647720336914 0.4575713276863098
MemoryTrain:  epoch  2, batch     3 | loss: 6.3739176Losses:  6.963322162628174 1.7769556045532227 0.5575176477432251
MemoryTrain:  epoch  2, batch     4 | loss: 6.9633222Losses:  6.013444423675537 1.935987114906311 0.5285540819168091
MemoryTrain:  epoch  2, batch     5 | loss: 6.0134444Losses:  5.912109375 1.0214886665344238 0.5193614959716797
MemoryTrain:  epoch  3, batch     0 | loss: 5.9121094Losses:  6.604931354522705 1.4347569942474365 0.3896680772304535
MemoryTrain:  epoch  3, batch     1 | loss: 6.6049314Losses:  6.180882453918457 2.0554232597351074 0.6159148216247559
MemoryTrain:  epoch  3, batch     2 | loss: 6.1808825Losses:  5.871657371520996 1.8506121635437012 0.553122341632843
MemoryTrain:  epoch  3, batch     3 | loss: 5.8716574Losses:  6.352721214294434 2.3847854137420654 0.5602216124534607
MemoryTrain:  epoch  3, batch     4 | loss: 6.3527212Losses:  6.539414882659912 2.3428168296813965 0.45710858702659607
MemoryTrain:  epoch  3, batch     5 | loss: 6.5394149Losses:  6.611082077026367 2.281235933303833 0.3617513179779053
MemoryTrain:  epoch  4, batch     0 | loss: 6.6110821Losses:  4.959573745727539 0.7648652791976929 0.5498397946357727
MemoryTrain:  epoch  4, batch     1 | loss: 4.9595737Losses:  4.7256011962890625 1.067152500152588 0.6137813329696655
MemoryTrain:  epoch  4, batch     2 | loss: 4.7256012Losses:  6.31065559387207 2.208263874053955 0.41894465684890747
MemoryTrain:  epoch  4, batch     3 | loss: 6.3106556Losses:  4.638423919677734 1.9862616062164307 0.41111013293266296
MemoryTrain:  epoch  4, batch     4 | loss: 4.6384239Losses:  6.161247730255127 1.7567601203918457 0.5381752848625183
MemoryTrain:  epoch  4, batch     5 | loss: 6.1612477Losses:  5.039784908294678 1.0561579465866089 0.48910459876060486
MemoryTrain:  epoch  5, batch     0 | loss: 5.0397849Losses:  5.13161039352417 1.4785596132278442 0.5105465650558472
MemoryTrain:  epoch  5, batch     1 | loss: 5.1316104Losses:  4.426113128662109 1.0237066745758057 0.5655086040496826
MemoryTrain:  epoch  5, batch     2 | loss: 4.4261131Losses:  6.02870512008667 1.793100118637085 0.4866805076599121
MemoryTrain:  epoch  5, batch     3 | loss: 6.0287051Losses:  5.306599140167236 2.6566519737243652 0.4109761416912079
MemoryTrain:  epoch  5, batch     4 | loss: 5.3065991Losses:  5.200177192687988 1.3530619144439697 0.4398798942565918
MemoryTrain:  epoch  5, batch     5 | loss: 5.2001772Losses:  5.055685043334961 1.2662231922149658 0.5225382447242737
MemoryTrain:  epoch  6, batch     0 | loss: 5.0556850Losses:  5.149840354919434 1.2238006591796875 0.5733742713928223
MemoryTrain:  epoch  6, batch     1 | loss: 5.1498404Losses:  4.138492107391357 1.025362491607666 0.392435759305954
MemoryTrain:  epoch  6, batch     2 | loss: 4.1384921Losses:  4.682090759277344 1.7474523782730103 0.31468236446380615
MemoryTrain:  epoch  6, batch     3 | loss: 4.6820908Losses:  4.187195777893066 0.9825000762939453 0.5368160605430603
MemoryTrain:  epoch  6, batch     4 | loss: 4.1871958Losses:  5.141833782196045 1.5512936115264893 0.5568451285362244
MemoryTrain:  epoch  6, batch     5 | loss: 5.1418338Losses:  4.561307430267334 1.3824546337127686 0.45931100845336914
MemoryTrain:  epoch  7, batch     0 | loss: 4.5613074Losses:  4.182888031005859 1.6601147651672363 0.5487486124038696
MemoryTrain:  epoch  7, batch     1 | loss: 4.1828880Losses:  6.587093353271484 2.5441811084747314 0.18747380375862122
MemoryTrain:  epoch  7, batch     2 | loss: 6.5870934Losses:  4.113813400268555 1.0119438171386719 0.6076311469078064
MemoryTrain:  epoch  7, batch     3 | loss: 4.1138134Losses:  5.29091739654541 2.1096386909484863 0.47527146339416504
MemoryTrain:  epoch  7, batch     4 | loss: 5.2909174Losses:  4.343910217285156 1.4037493467330933 0.3955644369125366
MemoryTrain:  epoch  7, batch     5 | loss: 4.3439102Losses:  5.887613296508789 2.718726396560669 0.4449172914028168
MemoryTrain:  epoch  8, batch     0 | loss: 5.8876133Losses:  4.707970142364502 1.8806989192962646 0.4299912452697754
MemoryTrain:  epoch  8, batch     1 | loss: 4.7079701Losses:  5.161551475524902 2.4651293754577637 0.5003575086593628
MemoryTrain:  epoch  8, batch     2 | loss: 5.1615515Losses:  4.052886009216309 1.2354152202606201 0.43770667910575867
MemoryTrain:  epoch  8, batch     3 | loss: 4.0528860Losses:  4.55148983001709 1.6852171421051025 0.553078830242157
MemoryTrain:  epoch  8, batch     4 | loss: 4.5514898Losses:  3.6229631900787354 1.1328057050704956 0.2759464383125305
MemoryTrain:  epoch  8, batch     5 | loss: 3.6229632Losses:  4.561684608459473 1.93290114402771 0.4899463653564453
MemoryTrain:  epoch  9, batch     0 | loss: 4.5616846Losses:  4.537535667419434 1.577868103981018 0.48872673511505127
MemoryTrain:  epoch  9, batch     1 | loss: 4.5375357Losses:  4.672519207000732 2.4897243976593018 0.28455400466918945
MemoryTrain:  epoch  9, batch     2 | loss: 4.6725192Losses:  6.035031795501709 3.378256320953369 0.4277394711971283
MemoryTrain:  epoch  9, batch     3 | loss: 6.0350318Losses:  5.450138092041016 2.5212526321411133 0.48673367500305176
MemoryTrain:  epoch  9, batch     4 | loss: 5.4501381Losses:  5.099104881286621 2.3021531105041504 0.3278465270996094
MemoryTrain:  epoch  9, batch     5 | loss: 5.0991049
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 41.96%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 37.50%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 71.71%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.13%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.62%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 82.95%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 82.35%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 82.68%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 83.11%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.55%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 83.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.30%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.67%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.88%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 84.66%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 84.24%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 83.64%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 83.80%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 82.25%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 81.00%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 79.81%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 78.66%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 77.31%   
cur_acc:  ['0.8485', '0.8795', '0.3750']
his_acc:  ['0.8485', '0.8630', '0.7731']
Clustering into  9  clusters
Clusters:  [0 3 2 0 8 5 6 0 1 3 0 0 2 2 4 2 7 2 5 1 0]
Losses:  12.10177230834961 7.714545726776123 0.4816344380378723
CurrentTrain: epoch  0, batch     0 | loss: 12.1017723Losses:  9.118268013000488 3.9265670776367188 0.4650546610355377
CurrentTrain: epoch  0, batch     1 | loss: 9.1182680Losses:  10.115608215332031 6.052546501159668 0.4352574348449707
CurrentTrain: epoch  1, batch     0 | loss: 10.1156082Losses:  5.058262348175049 1.283925175666809 0.4500909447669983
CurrentTrain: epoch  1, batch     1 | loss: 5.0582623Losses:  10.799400329589844 7.380656719207764 0.42964550852775574
CurrentTrain: epoch  2, batch     0 | loss: 10.7994003Losses:  5.641404151916504 2.8033268451690674 0.33342403173446655
CurrentTrain: epoch  2, batch     1 | loss: 5.6414042Losses:  9.12759017944336 6.026663780212402 0.40987101197242737
CurrentTrain: epoch  3, batch     0 | loss: 9.1275902Losses:  4.4330267906188965 1.7527775764465332 0.41313251852989197
CurrentTrain: epoch  3, batch     1 | loss: 4.4330268Losses:  8.834975242614746 6.068147659301758 0.4086233675479889
CurrentTrain: epoch  4, batch     0 | loss: 8.8349752Losses:  3.948915719985962 1.565240502357483 0.39246830344200134
CurrentTrain: epoch  4, batch     1 | loss: 3.9489157Losses:  8.869263648986816 6.246793746948242 0.40249964594841003
CurrentTrain: epoch  5, batch     0 | loss: 8.8692636Losses:  4.5201568603515625 2.045971155166626 0.29798197746276855
CurrentTrain: epoch  5, batch     1 | loss: 4.5201569Losses:  11.183117866516113 7.51973295211792 0.41068145632743835
CurrentTrain: epoch  6, batch     0 | loss: 11.1831179Losses:  5.584622859954834 2.831865072250366 0.38579902052879333
CurrentTrain: epoch  6, batch     1 | loss: 5.5846229Losses:  8.996040344238281 5.733262062072754 0.3887481391429901
CurrentTrain: epoch  7, batch     0 | loss: 8.9960403Losses:  3.9032111167907715 1.651197075843811 0.39307498931884766
CurrentTrain: epoch  7, batch     1 | loss: 3.9032111Losses:  7.674133777618408 5.4705657958984375 0.37724754214286804
CurrentTrain: epoch  8, batch     0 | loss: 7.6741338Losses:  3.828336000442505 1.6707899570465088 0.3904455602169037
CurrentTrain: epoch  8, batch     1 | loss: 3.8283360Losses:  7.988203048706055 5.85753059387207 0.37006646394729614
CurrentTrain: epoch  9, batch     0 | loss: 7.9882030Losses:  4.049356460571289 1.9576165676116943 0.3269258141517639
CurrentTrain: epoch  9, batch     1 | loss: 4.0493565
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned artist, elena smith, tragically lost her life due to a car accident while returning from an exhibition.  
Head Entity: elena smith  
Tail Entity: car accident  

Relation: person cause of death  
Context: following a long struggle with heart disease, mr. thompson succumbed to his illness last night at the hospital.  
Head Entity: mr. thompson  
Tail Entity: heart disease  

Relation: person cause of death  
Context: the community mourned the loss of their beloved mayor, who died from a sudden stroke during a city council meeting.  
Head Entity: the mayor  
Tail Entity: stroke  

Relation: person cause of death  
Context: after a courageous fight against pancreatic cancer, sarah jones passed away, leaving behind a legacy of kindness and compassion.  
Head Entity: sarah jones  
Tail Entity: pancreatic cancer  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The recent conference held by the Islamic Society of North America aimed to strengthen ties between various Muslim organizations and promote interfaith dialogue.  
Head Entity: Islamic Society of North America  
Tail Entity: Islam  

Relation: organization political religious affiliation  
Context: The Catholic Church has been actively involved in various social justice initiatives, reflecting its commitment to the teachings of Christianity.  
Head Entity: Catholic Church  
Tail Entity: Christianity  

Relation: organization political religious affiliation  
Context: The annual gathering of the National Council of Churches highlighted the role of religious organizations in advocating for peace and justice in society.  
Head Entity: National Council of Churches  
Tail Entity: Christianity  

Relation: organization political religious affiliation  
Context: The Jewish Federation's outreach programs aim to connect the community with the values and teachings of Judaism.  
Head Entity: Jewish Federation  
Tail Entity: Judaism  

Relation: organization political religious affiliation  
Context: The World Sikh Organization has been working to promote awareness of Sikh values and beliefs in various political discussions.  
Head Entity: World Sikh Organization  
Tail Entity: Sikhism  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: headquartered in seattle, washington, amazon has become a leader in e-commerce and cloud computing.  
Head Entity: amazon  
Tail Entity: washington  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is based in suwon, south korea, and is known for its electronics and technology.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization stateorprovince of headquarters  
Context: based in redmond, washington, microsoft is a major player in software development and technology solutions.  
Head Entity: microsoft  
Tail Entity: washington  

Relation: organization stateorprovince of headquarters  
Context: the famous fast-food chain mcdonald's has its headquarters in chicago, illinois, serving millions of customers worldwide.  
Head Entity: mcdonald's  
Tail Entity: illinois  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, and their close relationship despite the challenges of fame.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, uncle bob shared stories about his adventures with his niece, emily, who is now pursuing a career in photography.  
Head Entity: emily  
Tail Entity: uncle bob  

Relation: person other family  
Context: the documentary highlighted the bond between singer taylor swift and her younger brother, austin swift, showcasing their supportive relationship throughout her career.  
Head Entity: austin swift  
Tail Entity: taylor swift  

Relation: person other family  
Context: at the wedding, the bride's father, mr. johnson, gave a heartfelt speech about his daughter, sarah, and how proud he is of her accomplishments.  
Head Entity: sarah  
Tail Entity: mr. johnson  

Relation: person other family  
Context: in her memoir, actress drew barrymore reflects on her childhood and the influence of her grandmother, who played a significant role in her life.  
Head Entity: drew barrymore  
Tail Entity: her grandmother  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  

Relation: person city of death  
Context: the famous physicist, albert einstein, died on april 18, 1955, in princeton, new jersey, where he had lived for many years while working at the institute for advanced study.  
Head Entity: albert einstein  
Tail Entity: princeton  

Relation: person city of death  
Context: on january 1, 2020, the beloved actor, kobe bryant, tragically lost his life in a helicopter crash in calabasas, california, shocking fans around the world.  
Head Entity: kobe bryant  
Tail Entity: calabasas  

Relation: person city of death  
Context: the legendary musician, freddie mercury, passed away on november 24, 1991, at his home in london, england, leaving a profound impact on the music industry.  
Head Entity: freddie mercury  
Tail Entity: london  
Losses:  6.251361846923828 1.0896532535552979 0.5772364139556885
MemoryTrain:  epoch  0, batch     0 | loss: 6.2513618Losses:  6.9803996086120605 1.8037519454956055 0.44156503677368164
MemoryTrain:  epoch  0, batch     1 | loss: 6.9803996Losses:  6.069705963134766 1.5203723907470703 0.5849593281745911
MemoryTrain:  epoch  0, batch     2 | loss: 6.0697060Losses:  6.270519733428955 1.9462157487869263 0.5027536153793335
MemoryTrain:  epoch  0, batch     3 | loss: 6.2705197Losses:  7.466496467590332 1.6327890157699585 0.5080074071884155
MemoryTrain:  epoch  0, batch     4 | loss: 7.4664965Losses:  6.792648792266846 1.8176054954528809 0.5854872465133667
MemoryTrain:  epoch  0, batch     5 | loss: 6.7926488Losses:  7.556576728820801 1.7670327425003052 0.7160073518753052
MemoryTrain:  epoch  0, batch     6 | loss: 7.5565767Losses:  5.465601921081543 0.5289592742919922 0.46580132842063904
MemoryTrain:  epoch  0, batch     7 | loss: 5.4656019Losses:  6.672515869140625 0.9931509494781494 0.6481132507324219
MemoryTrain:  epoch  1, batch     0 | loss: 6.6725159Losses:  6.629546642303467 1.9724221229553223 0.5511940717697144
MemoryTrain:  epoch  1, batch     1 | loss: 6.6295466Losses:  5.5998735427856445 0.8678296208381653 0.5732561349868774
MemoryTrain:  epoch  1, batch     2 | loss: 5.5998735Losses:  6.390400409698486 1.4391522407531738 0.5318194627761841
MemoryTrain:  epoch  1, batch     3 | loss: 6.3904004Losses:  6.060837268829346 0.7286034822463989 0.5062785148620605
MemoryTrain:  epoch  1, batch     4 | loss: 6.0608373Losses:  6.525659084320068 2.4536356925964355 0.6624998450279236
MemoryTrain:  epoch  1, batch     5 | loss: 6.5256591Losses:  4.522029399871826 0.2506406605243683 0.6654102802276611
MemoryTrain:  epoch  1, batch     6 | loss: 4.5220294Losses:  5.164791584014893 1.064633846282959 0.4702673554420471
MemoryTrain:  epoch  1, batch     7 | loss: 5.1647916Losses:  4.354720592498779 0.5643406510353088 0.48446330428123474
MemoryTrain:  epoch  2, batch     0 | loss: 4.3547206Losses:  5.097451686859131 0.5510249137878418 0.5789241194725037
MemoryTrain:  epoch  2, batch     1 | loss: 5.0974517Losses:  5.711514472961426 1.7037577629089355 0.4187886416912079
MemoryTrain:  epoch  2, batch     2 | loss: 5.7115145Losses:  6.611188888549805 1.7084336280822754 0.5102291703224182
MemoryTrain:  epoch  2, batch     3 | loss: 6.6111889Losses:  4.380070209503174 0.7920124530792236 0.4612637460231781
MemoryTrain:  epoch  2, batch     4 | loss: 4.3800702Losses:  5.069764614105225 1.0235347747802734 0.7093107104301453
MemoryTrain:  epoch  2, batch     5 | loss: 5.0697646Losses:  4.812906265258789 0.7407594919204712 0.6357686519622803
MemoryTrain:  epoch  2, batch     6 | loss: 4.8129063Losses:  4.601711273193359 1.3667924404144287 0.4827480912208557
MemoryTrain:  epoch  2, batch     7 | loss: 4.6017113Losses:  4.3689961433410645 1.0483088493347168 0.4935685098171234
MemoryTrain:  epoch  3, batch     0 | loss: 4.3689961Losses:  4.397280693054199 1.4373316764831543 0.4951488673686981
MemoryTrain:  epoch  3, batch     1 | loss: 4.3972807Losses:  4.769601345062256 1.0336158275604248 0.6075169444084167
MemoryTrain:  epoch  3, batch     2 | loss: 4.7696013Losses:  4.413008213043213 0.9620869755744934 0.6513949632644653
MemoryTrain:  epoch  3, batch     3 | loss: 4.4130082Losses:  4.890720367431641 1.0353047847747803 0.5907337665557861
MemoryTrain:  epoch  3, batch     4 | loss: 4.8907204Losses:  4.595696449279785 0.49598219990730286 0.623089075088501
MemoryTrain:  epoch  3, batch     5 | loss: 4.5956964Losses:  6.250674724578857 1.698401927947998 0.6027424931526184
MemoryTrain:  epoch  3, batch     6 | loss: 6.2506747Losses:  4.9152607917785645 0.8331738114356995 0.490740031003952
MemoryTrain:  epoch  3, batch     7 | loss: 4.9152608Losses:  5.733394145965576 1.2944371700286865 0.7301437258720398
MemoryTrain:  epoch  4, batch     0 | loss: 5.7333941Losses:  4.592862606048584 1.0458393096923828 0.4984551668167114
MemoryTrain:  epoch  4, batch     1 | loss: 4.5928626Losses:  4.378387928009033 0.9619501829147339 0.4915434718132019
MemoryTrain:  epoch  4, batch     2 | loss: 4.3783879Losses:  4.334901809692383 0.7913529872894287 0.5780146718025208
MemoryTrain:  epoch  4, batch     3 | loss: 4.3349018Losses:  3.7104744911193848 0.7706677913665771 0.6165107488632202
MemoryTrain:  epoch  4, batch     4 | loss: 3.7104745Losses:  5.737476348876953 1.9896727800369263 0.6133478283882141
MemoryTrain:  epoch  4, batch     5 | loss: 5.7374763Losses:  4.4993085861206055 0.7308779954910278 0.4967876374721527
MemoryTrain:  epoch  4, batch     6 | loss: 4.4993086Losses:  4.2360029220581055 0.779694139957428 0.6357537508010864
MemoryTrain:  epoch  4, batch     7 | loss: 4.2360029Losses:  3.8258490562438965 1.003000259399414 0.5283196568489075
MemoryTrain:  epoch  5, batch     0 | loss: 3.8258491Losses:  4.2947821617126465 0.7109524011611938 0.4675186574459076
MemoryTrain:  epoch  5, batch     1 | loss: 4.2947822Losses:  3.87593150138855 1.2817184925079346 0.5123671889305115
MemoryTrain:  epoch  5, batch     2 | loss: 3.8759315Losses:  4.077749729156494 0.7495678663253784 0.5030128359794617
MemoryTrain:  epoch  5, batch     3 | loss: 4.0777497Losses:  4.4888834953308105 1.2748911380767822 0.5824596285820007
MemoryTrain:  epoch  5, batch     4 | loss: 4.4888835Losses:  6.452084064483643 1.757326364517212 0.4786257743835449
MemoryTrain:  epoch  5, batch     5 | loss: 6.4520841Losses:  5.8490777015686035 2.347975730895996 0.701389729976654
MemoryTrain:  epoch  5, batch     6 | loss: 5.8490777Losses:  3.805647611618042 0.5484648942947388 0.5694548487663269
MemoryTrain:  epoch  5, batch     7 | loss: 3.8056476Losses:  3.955687999725342 0.7676601409912109 0.5821508169174194
MemoryTrain:  epoch  6, batch     0 | loss: 3.9556880Losses:  5.86564826965332 2.279527187347412 0.48709970712661743
MemoryTrain:  epoch  6, batch     1 | loss: 5.8656483Losses:  3.89194917678833 1.0177488327026367 0.509863018989563
MemoryTrain:  epoch  6, batch     2 | loss: 3.8919492Losses:  5.337099075317383 1.3894035816192627 0.4511888921260834
MemoryTrain:  epoch  6, batch     3 | loss: 5.3370991Losses:  4.728695392608643 1.6840413808822632 0.4536812901496887
MemoryTrain:  epoch  6, batch     4 | loss: 4.7286954Losses:  5.063888072967529 1.8274435997009277 0.651064395904541
MemoryTrain:  epoch  6, batch     5 | loss: 5.0638881Losses:  3.907834529876709 0.998913049697876 0.630107581615448
MemoryTrain:  epoch  6, batch     6 | loss: 3.9078345Losses:  4.019216060638428 0.7722981572151184 0.5613358020782471
MemoryTrain:  epoch  6, batch     7 | loss: 4.0192161Losses:  3.713972568511963 0.544503927230835 0.5848796963691711
MemoryTrain:  epoch  7, batch     0 | loss: 3.7139726Losses:  4.422569274902344 1.2977869510650635 0.6196153163909912
MemoryTrain:  epoch  7, batch     1 | loss: 4.4225693Losses:  3.8223133087158203 0.9489518404006958 0.584109902381897
MemoryTrain:  epoch  7, batch     2 | loss: 3.8223133Losses:  4.198223114013672 1.1193435192108154 0.5838515162467957
MemoryTrain:  epoch  7, batch     3 | loss: 4.1982231Losses:  3.7742953300476074 0.9436135292053223 0.5719311833381653
MemoryTrain:  epoch  7, batch     4 | loss: 3.7742953Losses:  4.095930099487305 1.4790430068969727 0.5058506727218628
MemoryTrain:  epoch  7, batch     5 | loss: 4.0959301Losses:  4.153627872467041 0.7823060154914856 0.5482391119003296
MemoryTrain:  epoch  7, batch     6 | loss: 4.1536279Losses:  4.462973594665527 1.6151463985443115 0.42818379402160645
MemoryTrain:  epoch  7, batch     7 | loss: 4.4629736Losses:  3.9939684867858887 1.1993834972381592 0.3930233418941498
MemoryTrain:  epoch  8, batch     0 | loss: 3.9939685Losses:  4.501128196716309 1.4417216777801514 0.5705609321594238
MemoryTrain:  epoch  8, batch     1 | loss: 4.5011282Losses:  4.292220592498779 1.510514259338379 0.519457995891571
MemoryTrain:  epoch  8, batch     2 | loss: 4.2922206Losses:  3.9121475219726562 1.0805315971374512 0.6884796619415283
MemoryTrain:  epoch  8, batch     3 | loss: 3.9121475Losses:  3.4043092727661133 0.5007091164588928 0.5050119161605835
MemoryTrain:  epoch  8, batch     4 | loss: 3.4043093Losses:  3.3917911052703857 0.949233889579773 0.4757741093635559
MemoryTrain:  epoch  8, batch     5 | loss: 3.3917911Losses:  3.2834696769714355 0.5579032301902771 0.5744665265083313
MemoryTrain:  epoch  8, batch     6 | loss: 3.2834697Losses:  4.205955505371094 1.7793152332305908 0.5439994931221008
MemoryTrain:  epoch  8, batch     7 | loss: 4.2059555Losses:  4.169781684875488 1.370240569114685 0.47341033816337585
MemoryTrain:  epoch  9, batch     0 | loss: 4.1697817Losses:  3.602839708328247 1.0088696479797363 0.48114103078842163
MemoryTrain:  epoch  9, batch     1 | loss: 3.6028397Losses:  4.143586158752441 1.8210053443908691 0.3399483263492584
MemoryTrain:  epoch  9, batch     2 | loss: 4.1435862Losses:  3.8991432189941406 1.2865140438079834 0.5477908849716187
MemoryTrain:  epoch  9, batch     3 | loss: 3.8991432Losses:  5.445301055908203 2.0781211853027344 0.37536898255348206
MemoryTrain:  epoch  9, batch     4 | loss: 5.4453011Losses:  3.805976629257202 0.7866369485855103 0.6398665904998779
MemoryTrain:  epoch  9, batch     5 | loss: 3.8059766Losses:  3.7353365421295166 0.9373372793197632 0.6948702335357666
MemoryTrain:  epoch  9, batch     6 | loss: 3.7353365Losses:  3.7007761001586914 1.0154836177825928 0.5298744440078735
MemoryTrain:  epoch  9, batch     7 | loss: 3.7007761
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 68.18%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 68.27%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 69.85%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 70.07%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 73.86%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 77.40%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.24%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 80.15%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 79.11%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 78.65%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 77.87%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 77.47%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 76.92%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 76.72%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 77.13%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 77.53%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 77.91%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 77.84%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 78.06%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 76.90%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 76.86%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 77.17%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 76.00%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 74.88%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 74.28%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 73.58%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 74.32%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 74.67%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 74.78%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 73.60%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 72.99%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 72.50%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 72.54%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 72.58%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 72.56%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 72.88%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 72.73%   
cur_acc:  ['0.8485', '0.8795', '0.3750', '0.6827']
his_acc:  ['0.8485', '0.8630', '0.7731', '0.7273']
Clustering into  12  clusters
Clusters:  [ 4  1  0  4  8  3  6  2  5  1  2  4 11  0  9  0  7 10  3  5  2  0  0  0
  3  5]
Losses:  18.73726463317871 9.382184982299805 0.28126323223114014
CurrentTrain: epoch  0, batch     0 | loss: 18.7372646Losses:  10.614068031311035 2.63559627532959 0.19501416385173798
CurrentTrain: epoch  0, batch     1 | loss: 10.6140680Losses:  17.64893913269043 9.177844047546387 0.22720399498939514
CurrentTrain: epoch  1, batch     0 | loss: 17.6489391Losses:  11.788480758666992 3.882242441177368 0.2010962963104248
CurrentTrain: epoch  1, batch     1 | loss: 11.7884808Losses:  17.126482009887695 9.219996452331543 0.18149027228355408
CurrentTrain: epoch  2, batch     0 | loss: 17.1264820Losses:  9.004348754882812 2.9816510677337646 0.1831659972667694
CurrentTrain: epoch  2, batch     1 | loss: 9.0043488Losses:  15.02721881866455 7.943259239196777 0.18927906453609467
CurrentTrain: epoch  3, batch     0 | loss: 15.0272188Losses:  8.899301528930664 2.1761906147003174 0.1903529018163681
CurrentTrain: epoch  3, batch     1 | loss: 8.8993015Losses:  12.933361053466797 7.467369079589844 0.18890808522701263
CurrentTrain: epoch  4, batch     0 | loss: 12.9333611Losses:  12.488036155700684 4.6702423095703125 0.0774892121553421
CurrentTrain: epoch  4, batch     1 | loss: 12.4880362Losses:  14.932175636291504 8.378247261047363 0.19396772980690002
CurrentTrain: epoch  5, batch     0 | loss: 14.9321756Losses:  8.691380500793457 3.0418334007263184 0.20501390099525452
CurrentTrain: epoch  5, batch     1 | loss: 8.6913805Losses:  13.404080390930176 7.857137203216553 0.18942700326442719
CurrentTrain: epoch  6, batch     0 | loss: 13.4040804Losses:  10.284402847290039 3.637200355529785 0.0688287764787674
CurrentTrain: epoch  6, batch     1 | loss: 10.2844028Losses:  11.927131652832031 6.88836669921875 0.21224384009838104
CurrentTrain: epoch  7, batch     0 | loss: 11.9271317Losses:  9.503820419311523 2.8720200061798096 0.18927638232707977
CurrentTrain: epoch  7, batch     1 | loss: 9.5038204Losses:  12.837651252746582 7.327031135559082 0.18731705844402313
CurrentTrain: epoch  8, batch     0 | loss: 12.8376513Losses:  7.54429817199707 2.0542099475860596 0.17560380697250366
CurrentTrain: epoch  8, batch     1 | loss: 7.5442982Losses:  11.297778129577637 6.598347187042236 0.18543867766857147
CurrentTrain: epoch  9, batch     0 | loss: 11.2977781Losses:  8.36189079284668 2.8708794116973877 0.1841590404510498
CurrentTrain: epoch  9, batch     1 | loss: 8.3618908
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has transformed the way we consume video content online.  
Head Entity: Alphabet Inc.  
Tail Entity: YouTube  

Relation: organization subsidiaries  
Context: The automotive manufacturer General Motors has a number of subsidiaries, one of which is Chevrolet, known for its popular line of trucks and cars.  
Head Entity: General Motors  
Tail Entity: Chevrolet  

Relation: organization subsidiaries  
Context: The multinational conglomerate Procter & Gamble owns various subsidiaries, including Gillette, which specializes in personal care products.  
Head Entity: Procter & Gamble  
Tail Entity: Gillette  

Relation: organization subsidiaries  
Context: The beverage company Coca-Cola has several subsidiaries, one of which is Minute Maid, famous for its fruit juices and drinks.  
Head Entity: Coca-Cola  
Tail Entity: Minute Maid  

Relation: organization subsidiaries  
Context: The financial services corporation Berkshire Hathaway has numerous subsidiaries, including Geico, which is well-known for its auto insurance services.  
Head Entity: Berkshire Hathaway  
Tail Entity: Geico  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: The tech giant Alphabet Inc. has been making headlines with its innovative projects, but many are unaware that it is the parent company of Google, which has revolutionized the way we access information online.  
Head Entity: Alphabet Inc.  
Tail Entity: Google  

Relation: organization parents  
Context: In the world of finance, JPMorgan Chase & Co. has a significant influence, especially since it is the parent organization of several well-known banks, including Chase Bank, which serves millions of customers across the United States.  
Head Entity: JPMorgan Chase & Co.  
Tail Entity: Chase Bank  

Relation: organization parents  
Context: The Walt Disney Company has expanded its empire over the years, and one of its most notable acquisitions was Pixar Animation Studios, which has produced some of the most beloved animated films in history.  
Head Entity: The Walt Disney Company  
Tail Entity: Pixar Animation Studios  

Relation: organization parents  
Context: In the realm of social media, Facebook, Inc. has grown tremendously, and it is important to note that it is the parent company of Instagram, a platform that has transformed the way people share photos and connect with each other.  
Head Entity: Facebook, Inc.  
Tail Entity: Instagram  

Relation: organization parents  
Context: The pharmaceutical industry is heavily influenced by large corporations, and Pfizer Inc. stands out as a major player, especially since it is the parent organization of several subsidiaries that focus on various aspects of healthcare.  
Head Entity: Pfizer Inc.  
Tail Entity: Pfizer subsidiaries
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the world health organization -lrb- who -rrb- has been instrumental in coordinating global responses to health emergencies.  
Head Entity: world health organization  
Tail Entity: who  

Relation: organization alternate names  
Context: the federal bureau of investigation -lrb- fbi -rrb- is known for its role in investigating federal crimes in the United States.  
Head Entity: federal bureau of investigation  
Tail Entity: fbi  

Relation: organization alternate names  
Context: the international monetary fund -lrb- imf -rrb- provides financial assistance and advice to member countries.  
Head Entity: international monetary fund  
Tail Entity: imf  

Relation: organization alternate names  
Context: the national aeronautics and space administration -lrb- nasa -rrb- is responsible for the nation's civilian space program and for aeronautics and aerospace research.  
Head Entity: national aeronautics and space administration  
Tail Entity: nasa  

Relation: organization alternate names  
Context: the united nations educational, scientific and cultural organization -lrb- unesco -rrb- aims to promote world peace and security through international cooperation in education, the sciences, and culture.  
Head Entity: united nations educational, scientific and cultural organization  
Tail Entity: unesco  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2010-11-15 12:00:00 utc salesforce.com, a cloud computing company, announced its expansion into new offices in the heart of san francisco, aiming to accommodate its growing workforce.  
Head Entity: salesforce.com  
Tail Entity: san francisco  

Relation: organization city of headquarters  
Context: ------ new york 2015-03-10 09:30:00 utc the headquarters of the international business machines corporation, commonly known as ibm, is located in the bustling city of new york, where it has been a key player in technology for decades.  
Head Entity: ibm  
Tail Entity: new york  

Relation: organization city of headquarters  
Context: ------ seattle 2018-07-22 14:45:00 utc amazon.com, the e-commerce giant, continues to thrive with its headquarters situated in seattle, washington, contributing significantly to the local economy.  
Head Entity: amazon.com  
Tail Entity: seattle  

Relation: organization city of headquarters  
Context: ------ boston 2021-01-05 11:15:00 utc the biopharmaceutical company moderna, known for its innovative vaccine technology, has its headquarters in boston, massachusetts, a hub for life sciences.  
Head Entity: moderna  
Tail Entity: boston  

Relation: organization city of headquarters  
Context: ------ austin 2019-09-30 16:00:00 utc the tech startup indeed.com, which specializes in job search engines, has established its headquarters in austin, texas, attracting talent from across the country.  
Head Entity: indeed.com  
Tail Entity: austin  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: more than three decades ago , kerry 's work against the vietnam war set him on course to the senate - and , he often hoped , on to the presidency .
Head Entity: kerry
Tail Entity: he
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: During their childhood, Emma and her brother often played together in the backyard, creating memories that would last a lifetime.  
Head Entity: Emma  
Tail Entity: her brother  

Relation: person siblings  
Context: After the family reunion, Sarah realized how much she missed her sister, who had moved to another state for work.  
Head Entity: Sarah  
Tail Entity: her sister  

Relation: person siblings  
Context: The documentary highlighted the bond between the two brothers, showcasing their adventures growing up in a small town.  
Head Entity: the two brothers  
Tail Entity: their adventures  

Relation: person siblings  
Context: When Michael received the news about his sister's promotion, he couldn't help but feel proud of her achievements.  
Head Entity: Michael  
Tail Entity: his sister  

Relation: person siblings  
Context: At the wedding, Jessica shared a heartfelt speech about the special connection she has with her twin brother.  
Head Entity: Jessica  
Tail Entity: her twin brother  
Losses:  5.97278356552124 0.8028687834739685 0.640291690826416
MemoryTrain:  epoch  0, batch     0 | loss: 5.9727836Losses:  5.038243770599365 0.5096907615661621 0.7333880662918091
MemoryTrain:  epoch  0, batch     1 | loss: 5.0382438Losses:  6.406179428100586 1.6517066955566406 0.5893038511276245
MemoryTrain:  epoch  0, batch     2 | loss: 6.4061794Losses:  7.856832504272461 2.0023465156555176 0.5969493389129639
MemoryTrain:  epoch  0, batch     3 | loss: 7.8568325Losses:  4.61195707321167 0.6920014023780823 0.5551513433456421
MemoryTrain:  epoch  0, batch     4 | loss: 4.6119571Losses:  5.614307880401611 1.3554677963256836 0.7502043843269348
MemoryTrain:  epoch  0, batch     5 | loss: 5.6143079Losses:  7.286420822143555 2.2359776496887207 0.5735117197036743
MemoryTrain:  epoch  0, batch     6 | loss: 7.2864208Losses:  5.142683029174805 0.7895286679267883 0.7160112857818604
MemoryTrain:  epoch  0, batch     7 | loss: 5.1426830Losses:  5.858975410461426 1.17098069190979 0.5226659774780273
MemoryTrain:  epoch  0, batch     8 | loss: 5.8589754Losses:  4.853116989135742 0.6376469135284424 0.7339591979980469
MemoryTrain:  epoch  0, batch     9 | loss: 4.8531170Losses:  5.689091682434082 0.5141700506210327 0.6612927913665771
MemoryTrain:  epoch  1, batch     0 | loss: 5.6890917Losses:  5.935831546783447 1.0200042724609375 0.6421637535095215
MemoryTrain:  epoch  1, batch     1 | loss: 5.9358315Losses:  4.586797714233398 0.7497057914733887 0.7005091905593872
MemoryTrain:  epoch  1, batch     2 | loss: 4.5867977Losses:  4.611212730407715 1.428661823272705 0.715216875076294
MemoryTrain:  epoch  1, batch     3 | loss: 4.6112127Losses:  4.103537559509277 0.7407838106155396 0.772686779499054
MemoryTrain:  epoch  1, batch     4 | loss: 4.1035376Losses:  4.988556861877441 1.0380765199661255 0.6749858260154724
MemoryTrain:  epoch  1, batch     5 | loss: 4.9885569Losses:  5.712064266204834 0.8200997710227966 0.747940182685852
MemoryTrain:  epoch  1, batch     6 | loss: 5.7120643Losses:  5.429947376251221 0.5557318925857544 0.6526122093200684
MemoryTrain:  epoch  1, batch     7 | loss: 5.4299474Losses:  5.123323917388916 0.5509913563728333 0.8007869124412537
MemoryTrain:  epoch  1, batch     8 | loss: 5.1233239Losses:  5.675206661224365 1.3679040670394897 0.5100955963134766
MemoryTrain:  epoch  1, batch     9 | loss: 5.6752067Losses:  5.473517894744873 1.5643072128295898 0.7283471822738647
MemoryTrain:  epoch  2, batch     0 | loss: 5.4735179Losses:  5.764753818511963 2.0282199382781982 0.5169539451599121
MemoryTrain:  epoch  2, batch     1 | loss: 5.7647538Losses:  4.68724250793457 0.733833372592926 0.7964970469474792
MemoryTrain:  epoch  2, batch     2 | loss: 4.6872425Losses:  3.850942611694336 0.7700080871582031 0.4820164442062378
MemoryTrain:  epoch  2, batch     3 | loss: 3.8509426Losses:  4.037509441375732 0.4948163330554962 0.7573626637458801
MemoryTrain:  epoch  2, batch     4 | loss: 4.0375094Losses:  4.057287216186523 0.5402039289474487 0.6593978404998779
MemoryTrain:  epoch  2, batch     5 | loss: 4.0572872Losses:  3.7699451446533203 0.49144288897514343 0.6280437707901001
MemoryTrain:  epoch  2, batch     6 | loss: 3.7699451Losses:  3.7599222660064697 0.4718691110610962 0.7163164615631104
MemoryTrain:  epoch  2, batch     7 | loss: 3.7599223Losses:  5.1043782234191895 0.8704809546470642 0.4129229485988617
MemoryTrain:  epoch  2, batch     8 | loss: 5.1043782Losses:  3.8281919956207275 0.6430026292800903 0.6798751950263977
MemoryTrain:  epoch  2, batch     9 | loss: 3.8281920Losses:  3.982879638671875 0.48982173204421997 0.7671871185302734
MemoryTrain:  epoch  3, batch     0 | loss: 3.9828796Losses:  3.7745611667633057 -0.0 0.7800895571708679
MemoryTrain:  epoch  3, batch     1 | loss: 3.7745612Losses:  3.9508752822875977 0.7553061246871948 0.6813477873802185
MemoryTrain:  epoch  3, batch     2 | loss: 3.9508753Losses:  3.4747769832611084 0.21394428610801697 0.7936643958091736
MemoryTrain:  epoch  3, batch     3 | loss: 3.4747770Losses:  4.4272050857543945 1.5376617908477783 0.6594970226287842
MemoryTrain:  epoch  3, batch     4 | loss: 4.4272051Losses:  3.981071949005127 0.7452647686004639 0.5690472722053528
MemoryTrain:  epoch  3, batch     5 | loss: 3.9810719Losses:  5.207291126251221 1.6953489780426025 0.6660895347595215
MemoryTrain:  epoch  3, batch     6 | loss: 5.2072911Losses:  6.452521800994873 3.0558252334594727 0.6377016305923462
MemoryTrain:  epoch  3, batch     7 | loss: 6.4525218Losses:  4.7927656173706055 1.9357868432998657 0.6710188388824463
MemoryTrain:  epoch  3, batch     8 | loss: 4.7927656Losses:  4.22971773147583 0.6932492256164551 0.413808673620224
MemoryTrain:  epoch  3, batch     9 | loss: 4.2297177Losses:  4.363141059875488 1.1584670543670654 0.5673744678497314
MemoryTrain:  epoch  4, batch     0 | loss: 4.3631411Losses:  4.365549564361572 0.8981881141662598 0.7286698222160339
MemoryTrain:  epoch  4, batch     1 | loss: 4.3655496Losses:  4.264801502227783 0.8598533272743225 0.7050949335098267
MemoryTrain:  epoch  4, batch     2 | loss: 4.2648015Losses:  4.830528259277344 1.85684072971344 0.5689606070518494
MemoryTrain:  epoch  4, batch     3 | loss: 4.8305283Losses:  4.269122123718262 1.5093011856079102 0.7285813093185425
MemoryTrain:  epoch  4, batch     4 | loss: 4.2691221Losses:  3.655519962310791 0.5070210695266724 0.7251981496810913
MemoryTrain:  epoch  4, batch     5 | loss: 3.6555200Losses:  3.926189422607422 1.1798101663589478 0.5399799346923828
MemoryTrain:  epoch  4, batch     6 | loss: 3.9261894Losses:  4.071280479431152 1.1262670755386353 0.6597365140914917
MemoryTrain:  epoch  4, batch     7 | loss: 4.0712805Losses:  4.669102668762207 1.2624776363372803 0.6683225035667419
MemoryTrain:  epoch  4, batch     8 | loss: 4.6691027Losses:  3.354902744293213 0.7995704412460327 0.5435161590576172
MemoryTrain:  epoch  4, batch     9 | loss: 3.3549027Losses:  5.011941909790039 2.034790515899658 0.7263709306716919
MemoryTrain:  epoch  5, batch     0 | loss: 5.0119419Losses:  4.144918918609619 1.0039994716644287 0.7481557130813599
MemoryTrain:  epoch  5, batch     1 | loss: 4.1449189Losses:  3.747321844100952 0.6256030797958374 0.6245386004447937
MemoryTrain:  epoch  5, batch     2 | loss: 3.7473218Losses:  4.330682277679443 1.3935410976409912 0.7243103384971619
MemoryTrain:  epoch  5, batch     3 | loss: 4.3306823Losses:  3.9590814113616943 1.2817249298095703 0.4868158996105194
MemoryTrain:  epoch  5, batch     4 | loss: 3.9590814Losses:  4.270862579345703 0.9487870931625366 0.5944764614105225
MemoryTrain:  epoch  5, batch     5 | loss: 4.2708626Losses:  3.963289260864258 1.1227591037750244 0.6304333209991455
MemoryTrain:  epoch  5, batch     6 | loss: 3.9632893Losses:  3.255394458770752 0.6957228779792786 0.6327129602432251
MemoryTrain:  epoch  5, batch     7 | loss: 3.2553945Losses:  3.4791784286499023 0.799285888671875 0.5586326122283936
MemoryTrain:  epoch  5, batch     8 | loss: 3.4791784Losses:  2.7839837074279785 0.26606449484825134 0.5635931491851807
MemoryTrain:  epoch  5, batch     9 | loss: 2.7839837Losses:  3.8972795009613037 0.9872788190841675 0.6405222415924072
MemoryTrain:  epoch  6, batch     0 | loss: 3.8972795Losses:  3.6322340965270996 0.7365239858627319 0.5070759057998657
MemoryTrain:  epoch  6, batch     1 | loss: 3.6322341Losses:  4.017459392547607 1.3957194089889526 0.5564678311347961
MemoryTrain:  epoch  6, batch     2 | loss: 4.0174594Losses:  5.017187595367432 2.3991384506225586 0.6202782392501831
MemoryTrain:  epoch  6, batch     3 | loss: 5.0171876Losses:  5.402376174926758 2.328742504119873 0.566270112991333
MemoryTrain:  epoch  6, batch     4 | loss: 5.4023762Losses:  3.1647229194641113 0.5187845230102539 0.5354571342468262
MemoryTrain:  epoch  6, batch     5 | loss: 3.1647229Losses:  3.685673236846924 1.0833096504211426 0.5683466196060181
MemoryTrain:  epoch  6, batch     6 | loss: 3.6856732Losses:  3.934821605682373 1.294198751449585 0.6314269304275513
MemoryTrain:  epoch  6, batch     7 | loss: 3.9348216Losses:  4.474922180175781 1.8834404945373535 0.5220125913619995
MemoryTrain:  epoch  6, batch     8 | loss: 4.4749222Losses:  3.607646942138672 0.5427307486534119 0.5408888459205627
MemoryTrain:  epoch  6, batch     9 | loss: 3.6076469Losses:  3.1306731700897217 0.22945961356163025 0.7886013388633728
MemoryTrain:  epoch  7, batch     0 | loss: 3.1306732Losses:  3.50740385055542 0.7500836849212646 0.7299955487251282
MemoryTrain:  epoch  7, batch     1 | loss: 3.5074039Losses:  3.7168655395507812 0.7115736603736877 0.6318032741546631
MemoryTrain:  epoch  7, batch     2 | loss: 3.7168655Losses:  3.336272716522217 0.69100022315979 0.6423553228378296
MemoryTrain:  epoch  7, batch     3 | loss: 3.3362727Losses:  3.418393850326538 0.7075358033180237 0.6831818222999573
MemoryTrain:  epoch  7, batch     4 | loss: 3.4183939Losses:  4.265026569366455 1.534528374671936 0.6674489974975586
MemoryTrain:  epoch  7, batch     5 | loss: 4.2650266Losses:  3.5076169967651367 0.8202993273735046 0.5544459223747253
MemoryTrain:  epoch  7, batch     6 | loss: 3.5076170Losses:  5.366388320922852 2.811962127685547 0.5604552030563354
MemoryTrain:  epoch  7, batch     7 | loss: 5.3663883Losses:  3.8865671157836914 0.7408056855201721 0.7184231877326965
MemoryTrain:  epoch  7, batch     8 | loss: 3.8865671Losses:  3.2851674556732178 0.7445098161697388 0.5821115970611572
MemoryTrain:  epoch  7, batch     9 | loss: 3.2851675Losses:  3.051557779312134 0.4652937650680542 0.6208459734916687
MemoryTrain:  epoch  8, batch     0 | loss: 3.0515578Losses:  4.535499572753906 1.6313502788543701 0.628827691078186
MemoryTrain:  epoch  8, batch     1 | loss: 4.5354996Losses:  3.973815441131592 1.3825699090957642 0.5795774459838867
MemoryTrain:  epoch  8, batch     2 | loss: 3.9738154Losses:  3.1214139461517334 0.47735852003097534 0.6207805275917053
MemoryTrain:  epoch  8, batch     3 | loss: 3.1214139Losses:  3.7980716228485107 0.5406607985496521 0.7631122469902039
MemoryTrain:  epoch  8, batch     4 | loss: 3.7980716Losses:  4.139718055725098 1.2709779739379883 0.693914532661438
MemoryTrain:  epoch  8, batch     5 | loss: 4.1397181Losses:  3.441718816757202 0.9043184518814087 0.6241424083709717
MemoryTrain:  epoch  8, batch     6 | loss: 3.4417188Losses:  3.378392457962036 0.7185624837875366 0.7640144228935242
MemoryTrain:  epoch  8, batch     7 | loss: 3.3783925Losses:  3.955625534057617 1.337834358215332 0.5940070152282715
MemoryTrain:  epoch  8, batch     8 | loss: 3.9556255Losses:  3.0837976932525635 0.5085150003433228 0.5808431506156921
MemoryTrain:  epoch  8, batch     9 | loss: 3.0837977Losses:  4.920775890350342 2.187380075454712 0.7007741332054138
MemoryTrain:  epoch  9, batch     0 | loss: 4.9207759Losses:  3.491685152053833 0.9442083835601807 0.4790270924568176
MemoryTrain:  epoch  9, batch     1 | loss: 3.4916852Losses:  4.925623416900635 2.0198144912719727 0.7158983945846558
MemoryTrain:  epoch  9, batch     2 | loss: 4.9256234Losses:  3.8452084064483643 1.086421251296997 0.7688595652580261
MemoryTrain:  epoch  9, batch     3 | loss: 3.8452084Losses:  3.3266022205352783 0.7108080387115479 0.6394346356391907
MemoryTrain:  epoch  9, batch     4 | loss: 3.3266022Losses:  3.640977621078491 1.0086653232574463 0.7068086266517639
MemoryTrain:  epoch  9, batch     5 | loss: 3.6409776Losses:  3.9001150131225586 1.42626953125 0.5653380751609802
MemoryTrain:  epoch  9, batch     6 | loss: 3.9001150Losses:  3.9853920936584473 1.0896129608154297 0.6381720304489136
MemoryTrain:  epoch  9, batch     7 | loss: 3.9853921Losses:  3.8095505237579346 1.3567795753479004 0.5645436644554138
MemoryTrain:  epoch  9, batch     8 | loss: 3.8095505Losses:  2.773772716522217 0.246852844953537 0.5596803426742554
MemoryTrain:  epoch  9, batch     9 | loss: 2.7737727
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 27.08%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 29.69%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 33.93%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 37.50%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 39.77%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 41.15%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 42.31%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 39.73%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 37.08%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 36.33%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 34.19%   [EVAL] batch:   17 | acc: 6.25%,  total acc: 32.64%   [EVAL] batch:   18 | acc: 12.50%,  total acc: 31.58%   [EVAL] batch:   19 | acc: 6.25%,  total acc: 30.31%   [EVAL] batch:   20 | acc: 6.25%,  total acc: 29.17%   [EVAL] batch:   21 | acc: 0.00%,  total acc: 27.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 62.08%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 62.11%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 62.87%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 64.14%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 73.08%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 73.84%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 75.65%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.15%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 77.65%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 76.29%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 74.31%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 73.48%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 72.20%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 71.63%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 71.56%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 71.95%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 72.82%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 72.87%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 73.06%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 72.01%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 72.07%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 71.81%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 70.88%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 69.98%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 69.59%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 69.10%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 69.56%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 69.89%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 70.31%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 70.50%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 69.61%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 69.17%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 68.65%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:   61 | acc: 37.50%,  total acc: 68.15%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 67.66%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 67.77%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 68.18%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 67.54%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 67.00%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 66.39%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 66.16%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 65.85%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 65.36%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 64.98%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 64.70%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 64.23%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 64.18%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 63.84%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 63.12%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 62.42%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 61.89%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 61.14%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 60.57%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 59.93%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 59.30%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 58.69%   [EVAL] batch:   87 | acc: 0.00%,  total acc: 58.03%   
cur_acc:  ['0.8485', '0.8795', '0.3750', '0.6827', '0.2784']
his_acc:  ['0.8485', '0.8630', '0.7731', '0.7273', '0.5803']
Clustering into  14  clusters
Clusters:  [ 4  3  1  4  0  2  9  4  7  3  5  4 12  1 10 13  6  8  2  7  5  1  1  1
  2  7 10 11  4  1  0]
Losses:  12.599909782409668 7.229437828063965 0.4646375775337219
CurrentTrain: epoch  0, batch     0 | loss: 12.5999098Losses:  8.254851341247559 2.33467435836792 0.42209383845329285
CurrentTrain: epoch  0, batch     1 | loss: 8.2548513Losses:  14.71755313873291 9.86687183380127 0.4290282726287842
CurrentTrain: epoch  1, batch     0 | loss: 14.7175531Losses:  10.056866645812988 6.00651216506958 0.20357565581798553
CurrentTrain: epoch  1, batch     1 | loss: 10.0568666Losses:  11.288165092468262 7.54752254486084 0.3987443447113037
CurrentTrain: epoch  2, batch     0 | loss: 11.2881651Losses:  6.722299098968506 3.5642640590667725 0.3492718040943146
CurrentTrain: epoch  2, batch     1 | loss: 6.7222991Losses:  10.061344146728516 6.986305236816406 0.3823341429233551
CurrentTrain: epoch  3, batch     0 | loss: 10.0613441Losses:  5.531703948974609 2.6428356170654297 0.2841475009918213
CurrentTrain: epoch  3, batch     1 | loss: 5.5317039Losses:  9.171135902404785 6.387124061584473 0.3838810622692108
CurrentTrain: epoch  4, batch     0 | loss: 9.1711359Losses:  4.408958435058594 1.9207327365875244 0.27401915192604065
CurrentTrain: epoch  4, batch     1 | loss: 4.4089584Losses:  8.095113754272461 5.597323417663574 0.3743673861026764
CurrentTrain: epoch  5, batch     0 | loss: 8.0951138Losses:  4.169216632843018 1.7377736568450928 0.3717169463634491
CurrentTrain: epoch  5, batch     1 | loss: 4.1692166Losses:  7.453161716461182 5.154653549194336 0.36069056391716003
CurrentTrain: epoch  6, batch     0 | loss: 7.4531617Losses:  3.236222982406616 0.9917114973068237 0.37007781863212585
CurrentTrain: epoch  6, batch     1 | loss: 3.2362230Losses:  8.205679893493652 5.983545303344727 0.3593069612979889
CurrentTrain: epoch  7, batch     0 | loss: 8.2056799Losses:  4.0690789222717285 1.8959070444107056 0.2912668287754059
CurrentTrain: epoch  7, batch     1 | loss: 4.0690789Losses:  7.731188774108887 5.487556457519531 0.35656142234802246
CurrentTrain: epoch  8, batch     0 | loss: 7.7311888Losses:  3.7337839603424072 1.5474761724472046 0.34932103753089905
CurrentTrain: epoch  8, batch     1 | loss: 3.7337840Losses:  7.600133419036865 5.417283058166504 0.3433775007724762
CurrentTrain: epoch  9, batch     0 | loss: 7.6001334Losses:  3.5767133235931396 1.471261739730835 0.345918744802475
CurrentTrain: epoch  9, batch     1 | loss: 3.5767133
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
1. Relation: organization founded  
   Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, innovative solutions, which has since become a leader in software development.  
   Head Entity: innovative solutions  
   Tail Entity: 1998  

2. Relation: organization founded  
   Context: The non-profit organization, green earth initiative, was established in 2010 to promote environmental awareness and sustainability practices across communities.  
   Head Entity: green earth initiative  
   Tail Entity: 2010  

3. Relation: organization founded  
   Context: After years of research and development, the pharmaceutical company, health first, was founded in 2015 to focus on innovative drug therapies.  
   Head Entity: health first  
   Tail Entity: 2015  

4. Relation: organization founded  
   Context: In 2001, the educational institution, future leaders academy, was founded to provide quality education and leadership training to underprivileged youth.  
   Head Entity: future leaders academy  
   Tail Entity: 2001  

5. Relation: organization founded  
   Context: The fashion brand, chic styles, was founded in 2020 by a group of designers who aimed to revolutionize sustainable fashion.  
   Head Entity: chic styles  
   Tail Entity: 2020  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, emily decided to throw a big party for her friends.  
Head Entity: emily  
Tail Entity: 30  

Relation: person age  
Context: the renowned scientist dr. jane goodall is currently 89 years old.  
Head Entity: dr. jane goodall  
Tail Entity: 89  

Relation: person age  
Context: last year, my grandfather turned 80, and we had a family reunion to celebrate.  
Head Entity: my grandfather  
Tail Entity: 80  

Relation: person age  
Context: the youngest member of the team, 22-year-old sarah, impressed everyone with her skills.  
Head Entity: sarah  
Tail Entity: 22  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in barcelona and spent her childhood there before moving to madrid.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during the summer of 1985, john was born in new orleans, a city known for its vibrant culture.  
Head Entity: john  
Tail Entity: new orleans  

Relation: person city of birth  
Context: after years of research, we discovered that the famous artist was actually born in florence, italy.  
Head Entity: the famous artist  
Tail Entity: florence  

Relation: person city of birth  
Context: in a small town in the midwest, sarah was born in des moines, where she later became a local celebrity.  
Head Entity: sarah  
Tail Entity: des moines  

Relation: person city of birth  
Context: the renowned scientist was born in tokyo, where he developed his groundbreaking theories.  
Head Entity: the renowned scientist  
Tail Entity: tokyo  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: san diego 32 new orleans 37 american football : nfl result result of the nfl match between the san diego chargers of the afc west and the new orleans saints of the nfc south at wembley here sunday :
Head Entity: nfc south
Tail Entity: new orleans saints
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: the tech giant apple inc. has announced its new team of engineers who will be working on the next generation of products, including the new iphone and macbook.  
Head Entity: apple inc.  
Tail Entity: engineering team  

Relation: organization members  
Context: the united nations has appointed a new group of experts to address climate change issues, consisting of scientists and policymakers from various countries.  
Head Entity: united nations  
Tail Entity: group of experts  

Relation: organization members  
Context: the prestigious harvard university has welcomed a new cohort of students into its business school, known for producing some of the world's top business leaders.  
Head Entity: harvard university  
Tail Entity: business school  

Relation: organization members  
Context: the national basketball association (nba) has introduced a new set of players to its all-star team, showcasing talent from across the league.  
Head Entity: national basketball association  
Tail Entity: all-star team  

Relation: organization members  
Context: the world health organization (who) has formed a new advisory panel consisting of leading health experts to tackle global health challenges.  
Head Entity: world health organization  
Tail Entity: advisory panel  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: he fought attempts by zealous jews to move into the muslim quarter of the walled old city , but defended the practice of developing jewish suburbs around the eastern arab sector to prevent it from ever escaping israel 's rule .
Head Entity: he
Tail Entity: jewish
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of studying various philosophies, she finally embraced Buddhism, finding peace and purpose in its teachings.  
Head Entity: she  
Tail Entity: Buddhism  

Relation: person religion  
Context: The community gathered to celebrate the annual festival, where many Christians shared their faith and traditions with others.  
Head Entity: Christians  
Tail Entity: faith  

Relation: person religion  
Context: He often spoke about his deep connection to Hinduism, which shaped his values and way of life.  
Head Entity: He  
Tail Entity: Hinduism  

Relation: person religion  
Context: As a prominent leader in the Islamic community, he dedicated his life to promoting the teachings of the Quran.  
Head Entity: he  
Tail Entity: Quran  

Relation: person religion  
Context: She was raised in a Catholic family, and her upbringing played a significant role in her moral framework.  
Head Entity: She  
Tail Entity: Catholic
Losses:  5.701484203338623 0.7518548965454102 0.7769179344177246
MemoryTrain:  epoch  0, batch     0 | loss: 5.7014842Losses:  5.142165184020996 1.0250669717788696 0.4884573221206665
MemoryTrain:  epoch  0, batch     1 | loss: 5.1421652Losses:  4.427498817443848 0.9397650361061096 0.5580942034721375
MemoryTrain:  epoch  0, batch     2 | loss: 4.4274988Losses:  5.678574085235596 1.0396150350570679 0.6603779196739197
MemoryTrain:  epoch  0, batch     3 | loss: 5.6785741Losses:  5.406181335449219 0.500505805015564 0.6968286633491516
MemoryTrain:  epoch  0, batch     4 | loss: 5.4061813Losses:  5.158024787902832 1.0255131721496582 0.7034038305282593
MemoryTrain:  epoch  0, batch     5 | loss: 5.1580248Losses:  7.046665191650391 1.7177718877792358 0.4806032180786133
MemoryTrain:  epoch  0, batch     6 | loss: 7.0466652Losses:  4.9714202880859375 1.0266193151474 0.6470437049865723
MemoryTrain:  epoch  0, batch     7 | loss: 4.9714203Losses:  5.4061784744262695 1.0053738355636597 0.7729722261428833
MemoryTrain:  epoch  0, batch     8 | loss: 5.4061785Losses:  5.387310028076172 1.055590033531189 0.8462659120559692
MemoryTrain:  epoch  0, batch     9 | loss: 5.3873100Losses:  4.935977458953857 0.47787222266197205 0.7761895060539246
MemoryTrain:  epoch  0, batch    10 | loss: 4.9359775Losses:  5.1616668701171875 0.5506531000137329 0.4160865843296051
MemoryTrain:  epoch  0, batch    11 | loss: 5.1616669Losses:  5.474125385284424 0.8251947164535522 0.7949037551879883
MemoryTrain:  epoch  1, batch     0 | loss: 5.4741254Losses:  4.870800018310547 1.080641746520996 0.7122957706451416
MemoryTrain:  epoch  1, batch     1 | loss: 4.8708000Losses:  4.128874778747559 0.751739501953125 0.7799491882324219
MemoryTrain:  epoch  1, batch     2 | loss: 4.1288748Losses:  4.875033855438232 1.271607518196106 0.6303091049194336
MemoryTrain:  epoch  1, batch     3 | loss: 4.8750339Losses:  6.578881740570068 1.3759492635726929 0.6617690920829773
MemoryTrain:  epoch  1, batch     4 | loss: 6.5788817Losses:  4.343997001647949 1.0314797163009644 0.5563672780990601
MemoryTrain:  epoch  1, batch     5 | loss: 4.3439970Losses:  4.324936866760254 1.1505427360534668 0.7113298773765564
MemoryTrain:  epoch  1, batch     6 | loss: 4.3249369Losses:  6.174830436706543 1.9139933586120605 0.7293728590011597
MemoryTrain:  epoch  1, batch     7 | loss: 6.1748304Losses:  6.041975975036621 2.2929787635803223 0.6784664988517761
MemoryTrain:  epoch  1, batch     8 | loss: 6.0419760Losses:  4.049246311187744 0.950858473777771 0.5707160234451294
MemoryTrain:  epoch  1, batch     9 | loss: 4.0492463Losses:  3.8595659732818604 0.5205965042114258 0.6617420315742493
MemoryTrain:  epoch  1, batch    10 | loss: 3.8595660Losses:  3.0863418579101562 -0.0 0.5630009770393372
MemoryTrain:  epoch  1, batch    11 | loss: 3.0863419Losses:  4.80325984954834 1.248032808303833 0.6687111854553223
MemoryTrain:  epoch  2, batch     0 | loss: 4.8032598Losses:  3.9602856636047363 0.7735286951065063 0.7636404037475586
MemoryTrain:  epoch  2, batch     1 | loss: 3.9602857Losses:  4.2627034187316895 0.5013926029205322 0.8543170094490051
MemoryTrain:  epoch  2, batch     2 | loss: 4.2627034Losses:  4.039462089538574 1.1253777742385864 0.5935528874397278
MemoryTrain:  epoch  2, batch     3 | loss: 4.0394621Losses:  3.8066043853759766 0.5662622451782227 0.7096675634384155
MemoryTrain:  epoch  2, batch     4 | loss: 3.8066044Losses:  3.780930995941162 0.7459607720375061 0.6415438652038574
MemoryTrain:  epoch  2, batch     5 | loss: 3.7809310Losses:  4.309107780456543 1.3436939716339111 0.5677671432495117
MemoryTrain:  epoch  2, batch     6 | loss: 4.3091078Losses:  5.253762245178223 1.9344604015350342 0.568152666091919
MemoryTrain:  epoch  2, batch     7 | loss: 5.2537622Losses:  4.162506580352783 0.8792850971221924 0.7490409016609192
MemoryTrain:  epoch  2, batch     8 | loss: 4.1625066Losses:  3.3303613662719727 0.23106926679611206 0.8332778215408325
MemoryTrain:  epoch  2, batch     9 | loss: 3.3303614Losses:  3.3262510299682617 0.5135719776153564 0.619236946105957
MemoryTrain:  epoch  2, batch    10 | loss: 3.3262510Losses:  3.7612786293029785 0.32837581634521484 0.5180145502090454
MemoryTrain:  epoch  2, batch    11 | loss: 3.7612786Losses:  4.058460235595703 0.9891226291656494 0.7808558344841003
MemoryTrain:  epoch  3, batch     0 | loss: 4.0584602Losses:  3.6593310832977295 0.7101423740386963 0.7791669964790344
MemoryTrain:  epoch  3, batch     1 | loss: 3.6593311Losses:  3.489771842956543 0.7257757186889648 0.7863768935203552
MemoryTrain:  epoch  3, batch     2 | loss: 3.4897718Losses:  3.226982355117798 0.23098599910736084 0.7120144963264465
MemoryTrain:  epoch  3, batch     3 | loss: 3.2269824Losses:  4.709444999694824 1.7853937149047852 0.6526550054550171
MemoryTrain:  epoch  3, batch     4 | loss: 4.7094450Losses:  3.944960117340088 1.272080898284912 0.6432061195373535
MemoryTrain:  epoch  3, batch     5 | loss: 3.9449601Losses:  3.929171085357666 0.9878602027893066 0.6389778852462769
MemoryTrain:  epoch  3, batch     6 | loss: 3.9291711Losses:  3.450697660446167 0.4751097559928894 0.7841567993164062
MemoryTrain:  epoch  3, batch     7 | loss: 3.4506977Losses:  3.4137959480285645 0.48533689975738525 0.631811261177063
MemoryTrain:  epoch  3, batch     8 | loss: 3.4137959Losses:  4.783657073974609 1.9250049591064453 0.5071096420288086
MemoryTrain:  epoch  3, batch     9 | loss: 4.7836571Losses:  5.075230598449707 1.5911500453948975 0.55283522605896
MemoryTrain:  epoch  3, batch    10 | loss: 5.0752306Losses:  2.6900477409362793 -0.0 0.5556191205978394
MemoryTrain:  epoch  3, batch    11 | loss: 2.6900477Losses:  4.03950834274292 1.2584095001220703 0.7060856819152832
MemoryTrain:  epoch  4, batch     0 | loss: 4.0395083Losses:  4.685035705566406 1.3089115619659424 0.6644126176834106
MemoryTrain:  epoch  4, batch     1 | loss: 4.6850357Losses:  3.777994155883789 1.2848670482635498 0.47810491919517517
MemoryTrain:  epoch  4, batch     2 | loss: 3.7779942Losses:  3.3029768466949463 0.7195974588394165 0.6181085705757141
MemoryTrain:  epoch  4, batch     3 | loss: 3.3029768Losses:  3.497218370437622 0.6833301782608032 0.770122230052948
MemoryTrain:  epoch  4, batch     4 | loss: 3.4972184Losses:  4.700024127960205 1.5402982234954834 0.7330062985420227
MemoryTrain:  epoch  4, batch     5 | loss: 4.7000241Losses:  3.441044807434082 0.5070520639419556 0.7877578139305115
MemoryTrain:  epoch  4, batch     6 | loss: 3.4410448Losses:  3.564880609512329 0.761289656162262 0.6928049921989441
MemoryTrain:  epoch  4, batch     7 | loss: 3.5648806Losses:  4.017589092254639 0.8040093779563904 0.7390307188034058
MemoryTrain:  epoch  4, batch     8 | loss: 4.0175891Losses:  3.3192312717437744 0.23089465498924255 0.8395793437957764
MemoryTrain:  epoch  4, batch     9 | loss: 3.3192313Losses:  3.8738489151000977 1.3630717992782593 0.6349763870239258
MemoryTrain:  epoch  4, batch    10 | loss: 3.8738489Losses:  3.096626043319702 0.6041155457496643 0.5737499594688416
MemoryTrain:  epoch  4, batch    11 | loss: 3.0966260Losses:  3.681380271911621 0.7075076103210449 0.6408287286758423
MemoryTrain:  epoch  5, batch     0 | loss: 3.6813803Losses:  2.9632527828216553 0.24403619766235352 0.7007079720497131
MemoryTrain:  epoch  5, batch     1 | loss: 2.9632528Losses:  3.311314344406128 0.515398383140564 0.7028414607048035
MemoryTrain:  epoch  5, batch     2 | loss: 3.3113143Losses:  4.309971332550049 1.3703638315200806 0.6615716218948364
MemoryTrain:  epoch  5, batch     3 | loss: 4.3099713Losses:  3.355943202972412 0.7236822843551636 0.6223500967025757
MemoryTrain:  epoch  5, batch     4 | loss: 3.3559432Losses:  3.281075954437256 0.7030113339424133 0.5648337602615356
MemoryTrain:  epoch  5, batch     5 | loss: 3.2810760Losses:  3.8447134494781494 1.0631048679351807 0.7270086407661438
MemoryTrain:  epoch  5, batch     6 | loss: 3.8447134Losses:  3.484434127807617 0.7220373153686523 0.8499593734741211
MemoryTrain:  epoch  5, batch     7 | loss: 3.4844341Losses:  3.6701738834381104 0.7535036206245422 0.7738971710205078
MemoryTrain:  epoch  5, batch     8 | loss: 3.6701739Losses:  3.942808151245117 0.968518853187561 0.7651981711387634
MemoryTrain:  epoch  5, batch     9 | loss: 3.9428082Losses:  3.6138486862182617 0.9319284558296204 0.6450046300888062
MemoryTrain:  epoch  5, batch    10 | loss: 3.6138487Losses:  2.7197418212890625 0.30981457233428955 0.48940524458885193
MemoryTrain:  epoch  5, batch    11 | loss: 2.7197418Losses:  2.913785696029663 0.2374117076396942 0.7579770684242249
MemoryTrain:  epoch  6, batch     0 | loss: 2.9137857Losses:  3.2649245262145996 0.7441282868385315 0.5598427057266235
MemoryTrain:  epoch  6, batch     1 | loss: 3.2649245Losses:  2.8136234283447266 0.2429736852645874 0.48799657821655273
MemoryTrain:  epoch  6, batch     2 | loss: 2.8136234Losses:  3.4867992401123047 0.773547887802124 0.7351671457290649
MemoryTrain:  epoch  6, batch     3 | loss: 3.4867992Losses:  3.849060535430908 1.2332812547683716 0.6340306997299194
MemoryTrain:  epoch  6, batch     4 | loss: 3.8490605Losses:  3.1491026878356934 0.501664400100708 0.7016567587852478
MemoryTrain:  epoch  6, batch     5 | loss: 3.1491027Losses:  3.1547927856445312 0.23894952237606049 0.8385095000267029
MemoryTrain:  epoch  6, batch     6 | loss: 3.1547928Losses:  3.2066195011138916 0.4861336946487427 0.7243464589118958
MemoryTrain:  epoch  6, batch     7 | loss: 3.2066195Losses:  4.43496561050415 1.889045000076294 0.621834933757782
MemoryTrain:  epoch  6, batch     8 | loss: 4.4349656Losses:  4.466479778289795 1.2643872499465942 0.7300368547439575
MemoryTrain:  epoch  6, batch     9 | loss: 4.4664798Losses:  3.6906542778015137 1.0397835969924927 0.639574408531189
MemoryTrain:  epoch  6, batch    10 | loss: 3.6906543Losses:  3.030027389526367 0.28391653299331665 0.6738699674606323
MemoryTrain:  epoch  6, batch    11 | loss: 3.0300274Losses:  3.2278308868408203 0.5249937772750854 0.6396666765213013
MemoryTrain:  epoch  7, batch     0 | loss: 3.2278309Losses:  3.268328905105591 0.2660715579986572 0.7817971110343933
MemoryTrain:  epoch  7, batch     1 | loss: 3.2683289Losses:  2.866337299346924 0.2405415177345276 0.68226158618927
MemoryTrain:  epoch  7, batch     2 | loss: 2.8663373Losses:  4.192424774169922 1.6351802349090576 0.5934293270111084
MemoryTrain:  epoch  7, batch     3 | loss: 4.1924248Losses:  3.262575626373291 0.7192076444625854 0.6273743510246277
MemoryTrain:  epoch  7, batch     4 | loss: 3.2625756Losses:  3.7836008071899414 1.048050045967102 0.659209132194519
MemoryTrain:  epoch  7, batch     5 | loss: 3.7836008Losses:  3.211766242980957 0.4788210093975067 0.7089811563491821
MemoryTrain:  epoch  7, batch     6 | loss: 3.2117662Losses:  3.3068583011627197 0.49030280113220215 0.7709549069404602
MemoryTrain:  epoch  7, batch     7 | loss: 3.3068583Losses:  4.306982517242432 1.6553223133087158 0.552998423576355
MemoryTrain:  epoch  7, batch     8 | loss: 4.3069825Losses:  3.1647908687591553 0.48877713084220886 0.7568044662475586
MemoryTrain:  epoch  7, batch     9 | loss: 3.1647909Losses:  3.806514024734497 1.075636386871338 0.8003436923027039
MemoryTrain:  epoch  7, batch    10 | loss: 3.8065140Losses:  2.9311330318450928 0.297806978225708 0.71156907081604
MemoryTrain:  epoch  7, batch    11 | loss: 2.9311330Losses:  3.6010420322418213 0.9986457824707031 0.7084496021270752
MemoryTrain:  epoch  8, batch     0 | loss: 3.6010420Losses:  3.3926842212677 0.7279202938079834 0.6119346618652344
MemoryTrain:  epoch  8, batch     1 | loss: 3.3926842Losses:  3.337876558303833 0.6978040933609009 0.7069730758666992
MemoryTrain:  epoch  8, batch     2 | loss: 3.3378766Losses:  4.321620941162109 1.6696391105651855 0.7019132375717163
MemoryTrain:  epoch  8, batch     3 | loss: 4.3216209Losses:  3.2096095085144043 0.47099003195762634 0.7890185713768005
MemoryTrain:  epoch  8, batch     4 | loss: 3.2096095Losses:  3.18919038772583 0.5374047160148621 0.6267693042755127
MemoryTrain:  epoch  8, batch     5 | loss: 3.1891904Losses:  4.442903518676758 1.684132695198059 0.7296677827835083
MemoryTrain:  epoch  8, batch     6 | loss: 4.4429035Losses:  4.0145134925842285 1.3713148832321167 0.6541410684585571
MemoryTrain:  epoch  8, batch     7 | loss: 4.0145135Losses:  3.3029212951660156 0.4653434157371521 0.7943305969238281
MemoryTrain:  epoch  8, batch     8 | loss: 3.3029213Losses:  3.357891798019409 0.6975828409194946 0.7288634181022644
MemoryTrain:  epoch  8, batch     9 | loss: 3.3578918Losses:  3.7060739994049072 1.1276429891586304 0.6469519138336182
MemoryTrain:  epoch  8, batch    10 | loss: 3.7060740Losses:  2.646646738052368 0.24452006816864014 0.491981565952301
MemoryTrain:  epoch  8, batch    11 | loss: 2.6466467Losses:  3.514511823654175 0.7178624868392944 0.7737910747528076
MemoryTrain:  epoch  9, batch     0 | loss: 3.5145118Losses:  2.866529703140259 0.2286326289176941 0.7564367651939392
MemoryTrain:  epoch  9, batch     1 | loss: 2.8665297Losses:  3.5891637802124023 0.9739347696304321 0.7218972444534302
MemoryTrain:  epoch  9, batch     2 | loss: 3.5891638Losses:  2.9470789432525635 0.2405092418193817 0.7154012322425842
MemoryTrain:  epoch  9, batch     3 | loss: 2.9470789Losses:  2.992896318435669 0.23247893154621124 0.8246293663978577
MemoryTrain:  epoch  9, batch     4 | loss: 2.9928963Losses:  3.762216091156006 1.258042812347412 0.5456876158714294
MemoryTrain:  epoch  9, batch     5 | loss: 3.7622161Losses:  3.3951690196990967 0.9350262880325317 0.5811593532562256
MemoryTrain:  epoch  9, batch     6 | loss: 3.3951690Losses:  4.362365245819092 1.8973239660263062 0.5850955843925476
MemoryTrain:  epoch  9, batch     7 | loss: 4.3623652Losses:  3.0840530395507812 0.2433205246925354 0.8451224565505981
MemoryTrain:  epoch  9, batch     8 | loss: 3.0840530Losses:  3.4594955444335938 0.9443132877349854 0.5665407180786133
MemoryTrain:  epoch  9, batch     9 | loss: 3.4594955Losses:  3.5153567790985107 0.8716654777526855 0.6593077182769775
MemoryTrain:  epoch  9, batch    10 | loss: 3.5153568Losses:  2.7965333461761475 0.3100282847881317 0.4618048369884491
MemoryTrain:  epoch  9, batch    11 | loss: 2.7965333
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 79.91%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 57.14%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.76%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 62.15%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 63.16%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 69.02%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 70.05%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 72.12%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 75.60%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 76.70%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 75.37%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 74.11%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 72.64%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 71.31%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 71.65%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 72.17%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 72.67%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 72.87%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 73.33%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 72.42%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 72.47%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 72.32%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 71.50%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 70.47%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 70.19%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 69.58%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 69.91%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 70.23%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 70.54%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 70.61%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 69.72%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 69.39%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 68.85%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 68.15%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 67.66%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 67.77%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 67.99%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 66.98%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 66.18%   [EVAL] batch:   68 | acc: 6.25%,  total acc: 65.31%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 65.09%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 64.61%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 64.15%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 63.96%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 63.85%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 63.75%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 63.40%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 63.47%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 63.46%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 63.13%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 62.42%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 61.73%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 61.20%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 60.47%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 59.82%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 59.19%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 58.72%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 58.26%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 58.52%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 58.99%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 59.44%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 59.89%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 60.33%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 60.75%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 61.17%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 61.58%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 61.85%   [EVAL] batch:   96 | acc: 12.50%,  total acc: 61.34%   [EVAL] batch:   97 | acc: 37.50%,  total acc: 61.10%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 61.11%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 61.31%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 61.26%   
cur_acc:  ['0.8485', '0.8795', '0.3750', '0.6827', '0.2784', '0.7991']
his_acc:  ['0.8485', '0.8630', '0.7731', '0.7273', '0.5803', '0.6126']
Clustering into  17  clusters
Clusters:  [ 1  3  9  1  0  8 13  1  4  3  5  1 14  9  6 15  2 10  8  4  5  9  9 16
  8  4  6  7  1  9 11  1  2 12  0  4]
Losses:  15.920997619628906 9.297090530395508 0.49948984384536743
CurrentTrain: epoch  0, batch     0 | loss: 15.9209976Losses:  13.440625190734863 5.9333930015563965 0.33020129799842834
CurrentTrain: epoch  0, batch     1 | loss: 13.4406252Losses:  13.82408332824707 7.28962516784668 0.48066312074661255
CurrentTrain: epoch  1, batch     0 | loss: 13.8240833Losses:  7.633547782897949 2.2623536586761475 0.4169033169746399
CurrentTrain: epoch  1, batch     1 | loss: 7.6335478Losses:  11.567265510559082 6.226193904876709 0.48076745867729187
CurrentTrain: epoch  2, batch     0 | loss: 11.5672655Losses:  8.557015419006348 2.6182806491851807 0.4157857894897461
CurrentTrain: epoch  2, batch     1 | loss: 8.5570154Losses:  11.410199165344238 6.686439037322998 0.40803262591362
CurrentTrain: epoch  3, batch     0 | loss: 11.4101992Losses:  8.836112022399902 2.560230255126953 0.4646866023540497
CurrentTrain: epoch  3, batch     1 | loss: 8.8361120Losses:  12.080163955688477 6.964568614959717 0.4045984447002411
CurrentTrain: epoch  4, batch     0 | loss: 12.0801640Losses:  6.084707260131836 1.8397399187088013 0.42621979117393494
CurrentTrain: epoch  4, batch     1 | loss: 6.0847073Losses:  12.42617130279541 7.682718276977539 0.44080162048339844
CurrentTrain: epoch  5, batch     0 | loss: 12.4261713Losses:  6.218821048736572 2.5074379444122314 0.3058694005012512
CurrentTrain: epoch  5, batch     1 | loss: 6.2188210Losses:  11.659006118774414 6.815489768981934 0.45348691940307617
CurrentTrain: epoch  6, batch     0 | loss: 11.6590061Losses:  6.845934867858887 2.1664364337921143 0.39139190316200256
CurrentTrain: epoch  6, batch     1 | loss: 6.8459349Losses:  10.80904483795166 6.634775161743164 0.40013620257377625
CurrentTrain: epoch  7, batch     0 | loss: 10.8090448Losses:  6.730958938598633 2.2850341796875 0.3987593650817871
CurrentTrain: epoch  7, batch     1 | loss: 6.7309589Losses:  10.351402282714844 6.59610652923584 0.3723057210445404
CurrentTrain: epoch  8, batch     0 | loss: 10.3514023Losses:  6.1136980056762695 2.5594840049743652 0.3378189504146576
CurrentTrain: epoch  8, batch     1 | loss: 6.1136980Losses:  9.505813598632812 6.004635810852051 0.37036749720573425
CurrentTrain: epoch  9, batch     0 | loss: 9.5058136Losses:  5.557772636413574 2.0875885486602783 0.3730229139328003
CurrentTrain: epoch  9, batch     1 | loss: 5.5577726
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After years of living in the bustling city, actor Tom Hanks has decided to settle down in a quieter place, choosing to make his home in the picturesque state of California.  
Head Entity: Tom Hanks  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: The renowned author J.K. Rowling has spent much of her life in Edinburgh, where she found inspiration for her famous Harry Potter series.  
Head Entity: J.K. Rowling  
Tail Entity: Edinburgh  

Relation: person stateorprovinces of residence  
Context: Following his successful career in the tech industry, entrepreneur Elon Musk has moved to Texas, where he plans to expand his business ventures.  
Head Entity: Elon Musk  
Tail Entity: Texas  

Relation: person stateorprovinces of residence  
Context: After retiring from professional basketball, Michael Jordan has chosen to reside in North Carolina, where he continues to be involved in the local community.  
Head Entity: Michael Jordan  
Tail Entity: North Carolina  

Relation: person stateorprovinces of residence  
Context: The famous singer Taylor Swift has made her home in Nashville, Tennessee, where she began her music career and still finds inspiration for her songs.  
Head Entity: Taylor Swift  
Tail Entity: Nashville
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away on July 10, 2020.  
Head Entity: The renowned author  
Tail Entity: July 10, 2020  

Relation: person date of death  
Context: She left this world on March 5, 2018, after a long illness.  
Head Entity: She  
Tail Entity: March 5, 2018  

Relation: person date of death  
Context: His family announced that he died peacefully in his sleep on December 1, 2019.  
Head Entity: He  
Tail Entity: December 1, 2019  

Relation: person date of death  
Context: The famous scientist's death was reported to have occurred on August 22, 2021.  
Head Entity: The famous scientist  
Tail Entity: August 22, 2021  

Relation: person date of death  
Context: They remembered him fondly as he was laid to rest on January 15, 2022.  
Head Entity: They  
Tail Entity: January 15, 2022  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: mcgovern wants to make the red cross , with 30,000 employees and 700 chapters , more efficient .
Head Entity: red cross
Tail Entity: 30,000
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
1. Relation: organization number of employees members  
   Context: The tech giant Google boasts a workforce of over 156,000 employees worldwide.  
   Head Entity: Google  
   Tail Entity: 156,000  

2. Relation: organization number of employees members  
   Context: With a dedicated team of 50,000 staff, the United Nations works tirelessly to address global issues.  
   Head Entity: United Nations  
   Tail Entity: 50,000  

3. Relation: organization number of employees members  
   Context: Amazon has expanded rapidly, now employing around 1,300,000 individuals across its various services.  
   Head Entity: Amazon  
   Tail Entity: 1,300,000  

4. Relation: organization number of employees members  
   Context: The nonprofit organization Habitat for Humanity has approximately 2,300 employees working towards affordable housing.  
   Head Entity: Habitat for Humanity  
   Tail Entity: 2,300  

5. Relation: organization number of employees members  
   Context: Microsoft reported a total of 181,000 employees as of the end of last year, reflecting its growth in the tech industry.  
   Head Entity: Microsoft  
   Tail Entity: 181,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: the coming of the mahdi will turn the world upside down , and the oppressed shiites will finally see justice .
Head Entity: mahdi
Tail Entity: shiites
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
1. Relation: person alternate names  
   Context: The famous author Mark Twain is known for his wit and humor, but his real name was Samuel Clemens.  
   Head Entity: Mark Twain  
   Tail Entity: Samuel Clemens  

2. Relation: person alternate names  
   Context: The musician known as Lady Gaga was born Stefani Joanne Angelina Germanotta.  
   Head Entity: Lady Gaga  
   Tail Entity: Stefani Joanne Angelina Germanotta  

3. Relation: person alternate names  
   Context: The legendary basketball player Michael Jordan is often referred to simply as "MJ."  
   Head Entity: Michael Jordan  
   Tail Entity: MJ  

4. Relation: person alternate names  
   Context: The artist known as Banksy remains anonymous, but some speculate his real name is Robert Del Naja.  
   Head Entity: Banksy  
   Tail Entity: Robert Del Naja  

5. Relation: person alternate names  
   Context: The famous physicist Albert Einstein is often called the "father of modern physics."  
   Head Entity: Albert Einstein  
   Tail Entity: father of modern physics  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: kerry katona feel out with arnold just before she married bryan mcfadden , kerry cancelled his wedding invitation and have n't spoken since .
Head Entity: bryan mcfadden
Tail Entity: kerry katona
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
1. Relation: person spouse  
   Context: After years of dating, emma stone finally tied the knot with dave mccary in a beautiful ceremony.  
   Head Entity: dave mccary  
   Tail Entity: emma stone  

2. Relation: person spouse  
   Context: During their time together, tom hanks and rita wilson have built a strong partnership both personally and professionally.  
   Head Entity: rita wilson  
   Tail Entity: tom hanks  

3. Relation: person spouse  
   Context: Following their engagement, meghan markle and prince harry announced their wedding date, exciting fans around the world.  
   Head Entity: prince harry  
   Tail Entity: meghan markle  

4. Relation: person spouse  
   Context: After a whirlwind romance, jessica biel and justin timberlake exchanged vows in a private ceremony surrounded by family and friends.  
   Head Entity: justin timberlake  
   Tail Entity: jessica biel  

5. Relation: person spouse  
   Context: The couple celebrated their 10th anniversary, reflecting on their journey together since they first met at a charity event.  
   Head Entity: jennifer aniston  
   Tail Entity: justin theroux  
Losses:  4.2364349365234375 0.4991317391395569 0.8335652947425842
MemoryTrain:  epoch  0, batch     0 | loss: 4.2364349Losses:  4.745320796966553 1.6057242155075073 0.5501574873924255
MemoryTrain:  epoch  0, batch     1 | loss: 4.7453208Losses:  4.420375823974609 0.47096216678619385 0.8610217571258545
MemoryTrain:  epoch  0, batch     2 | loss: 4.4203758Losses:  4.761843681335449 0.5171022415161133 0.794087827205658
MemoryTrain:  epoch  0, batch     3 | loss: 4.7618437Losses:  3.8241922855377197 0.5018685460090637 0.7281217575073242
MemoryTrain:  epoch  0, batch     4 | loss: 3.8241923Losses:  4.009956359863281 0.9774074554443359 0.7090475559234619
MemoryTrain:  epoch  0, batch     5 | loss: 4.0099564Losses:  6.256515979766846 1.4166984558105469 0.8152672052383423
MemoryTrain:  epoch  0, batch     6 | loss: 6.2565160Losses:  4.765568256378174 0.5187682509422302 0.7829238772392273
MemoryTrain:  epoch  0, batch     7 | loss: 4.7655683Losses:  3.8191659450531006 0.7320595383644104 0.8461526036262512
MemoryTrain:  epoch  0, batch     8 | loss: 3.8191659Losses:  5.396018028259277 1.3635395765304565 0.5588295459747314
MemoryTrain:  epoch  0, batch     9 | loss: 5.3960180Losses:  3.8586649894714355 0.49778345227241516 0.6443360447883606
MemoryTrain:  epoch  0, batch    10 | loss: 3.8586650Losses:  4.338093280792236 0.5009431838989258 0.6592007875442505
MemoryTrain:  epoch  0, batch    11 | loss: 4.3380933Losses:  3.8911232948303223 0.48620814085006714 0.9103896617889404
MemoryTrain:  epoch  0, batch    12 | loss: 3.8911233Losses:  5.441725254058838 0.46031662821769714 0.5405639410018921
MemoryTrain:  epoch  0, batch    13 | loss: 5.4417253Losses:  3.7781901359558105 0.4812338948249817 0.9356644153594971
MemoryTrain:  epoch  1, batch     0 | loss: 3.7781901Losses:  4.1854987144470215 0.501736044883728 0.7329404950141907
MemoryTrain:  epoch  1, batch     1 | loss: 4.1854987Losses:  5.984537124633789 2.5786972045898438 0.7392817735671997
MemoryTrain:  epoch  1, batch     2 | loss: 5.9845371Losses:  4.139410972595215 0.2613898515701294 0.7069345116615295
MemoryTrain:  epoch  1, batch     3 | loss: 4.1394110Losses:  4.529815673828125 1.0521050691604614 0.8011605143547058
MemoryTrain:  epoch  1, batch     4 | loss: 4.5298157Losses:  3.9228086471557617 1.166174292564392 0.4798431396484375
MemoryTrain:  epoch  1, batch     5 | loss: 3.9228086Losses:  3.3673906326293945 0.26300060749053955 0.7312153577804565
MemoryTrain:  epoch  1, batch     6 | loss: 3.3673906Losses:  4.510641574859619 1.3240811824798584 0.6289585828781128
MemoryTrain:  epoch  1, batch     7 | loss: 4.5106416Losses:  4.732496738433838 1.5205212831497192 0.7126143574714661
MemoryTrain:  epoch  1, batch     8 | loss: 4.7324967Losses:  4.721466064453125 0.7561694979667664 0.9102522730827332
MemoryTrain:  epoch  1, batch     9 | loss: 4.7214661Losses:  2.805697441101074 -0.0 0.7868037223815918
MemoryTrain:  epoch  1, batch    10 | loss: 2.8056974Losses:  4.243389129638672 0.2360885739326477 0.737464189529419
MemoryTrain:  epoch  1, batch    11 | loss: 4.2433891Losses:  4.573663234710693 1.0713424682617188 0.6396728157997131
MemoryTrain:  epoch  1, batch    12 | loss: 4.5736632Losses:  2.862082004547119 0.30801212787628174 0.5692785978317261
MemoryTrain:  epoch  1, batch    13 | loss: 2.8620820Losses:  4.896421909332275 1.337792158126831 0.7283557057380676
MemoryTrain:  epoch  2, batch     0 | loss: 4.8964219Losses:  3.856914758682251 0.9552448987960815 0.7958909869194031
MemoryTrain:  epoch  2, batch     1 | loss: 3.8569148Losses:  4.921321392059326 1.5796475410461426 0.6157659292221069
MemoryTrain:  epoch  2, batch     2 | loss: 4.9213214Losses:  3.9711012840270996 1.0372538566589355 0.8555741310119629
MemoryTrain:  epoch  2, batch     3 | loss: 3.9711013Losses:  6.100912570953369 2.4135680198669434 0.6635655164718628
MemoryTrain:  epoch  2, batch     4 | loss: 6.1009126Losses:  2.8728280067443848 -0.0 0.8247089385986328
MemoryTrain:  epoch  2, batch     5 | loss: 2.8728280Losses:  3.9699325561523438 0.7907797694206238 0.7031557559967041
MemoryTrain:  epoch  2, batch     6 | loss: 3.9699326Losses:  3.8435235023498535 0.9527652859687805 0.7952049970626831
MemoryTrain:  epoch  2, batch     7 | loss: 3.8435235Losses:  3.9112555980682373 1.1616421937942505 0.698034942150116
MemoryTrain:  epoch  2, batch     8 | loss: 3.9112556Losses:  3.1620705127716064 0.4914137125015259 0.7344158291816711
MemoryTrain:  epoch  2, batch     9 | loss: 3.1620705Losses:  3.746385097503662 0.2167036533355713 0.8295336961746216
MemoryTrain:  epoch  2, batch    10 | loss: 3.7463851Losses:  3.4532854557037354 0.6972821950912476 0.6910063624382019
MemoryTrain:  epoch  2, batch    11 | loss: 3.4532855Losses:  4.282773494720459 1.0782265663146973 0.730786919593811
MemoryTrain:  epoch  2, batch    12 | loss: 4.2827735Losses:  2.80765962600708 -0.0 0.5227277278900146
MemoryTrain:  epoch  2, batch    13 | loss: 2.8076596Losses:  3.225006580352783 0.4850994348526001 0.7992095947265625
MemoryTrain:  epoch  3, batch     0 | loss: 3.2250066Losses:  3.7292563915252686 0.9366183876991272 0.8304426074028015
MemoryTrain:  epoch  3, batch     1 | loss: 3.7292564Losses:  3.789294719696045 0.9499948024749756 0.861132025718689
MemoryTrain:  epoch  3, batch     2 | loss: 3.7892947Losses:  3.696310520172119 0.5120075941085815 0.7180147767066956
MemoryTrain:  epoch  3, batch     3 | loss: 3.6963105Losses:  3.2633235454559326 0.23711849749088287 0.7609294056892395
MemoryTrain:  epoch  3, batch     4 | loss: 3.2633235Losses:  3.3927719593048096 0.46467119455337524 0.8345515131950378
MemoryTrain:  epoch  3, batch     5 | loss: 3.3927720Losses:  4.083713054656982 0.5049739480018616 0.8543274998664856
MemoryTrain:  epoch  3, batch     6 | loss: 4.0837131Losses:  2.993819236755371 0.46717312932014465 0.5362014770507812
MemoryTrain:  epoch  3, batch     7 | loss: 2.9938192Losses:  4.002824783325195 1.214789628982544 0.6395543813705444
MemoryTrain:  epoch  3, batch     8 | loss: 4.0028248Losses:  3.67375111579895 0.5503498315811157 0.7026384472846985
MemoryTrain:  epoch  3, batch     9 | loss: 3.6737511Losses:  4.315308570861816 1.021762490272522 0.849270224571228
MemoryTrain:  epoch  3, batch    10 | loss: 4.3153086Losses:  3.1066718101501465 0.2580246329307556 0.7614072561264038
MemoryTrain:  epoch  3, batch    11 | loss: 3.1066718Losses:  3.6705620288848877 0.5078311562538147 0.7855446338653564
MemoryTrain:  epoch  3, batch    12 | loss: 3.6705620Losses:  3.200517177581787 0.5656225681304932 0.40509578585624695
MemoryTrain:  epoch  3, batch    13 | loss: 3.2005172Losses:  3.67810320854187 0.6979366540908813 0.9018017649650574
MemoryTrain:  epoch  4, batch     0 | loss: 3.6781032Losses:  3.5717012882232666 0.7152893543243408 0.8308711051940918
MemoryTrain:  epoch  4, batch     1 | loss: 3.5717013Losses:  3.267918109893799 0.46568620204925537 0.846217691898346
MemoryTrain:  epoch  4, batch     2 | loss: 3.2679181Losses:  2.9946391582489014 0.25195732712745667 0.7792821526527405
MemoryTrain:  epoch  4, batch     3 | loss: 2.9946392Losses:  3.140058994293213 0.4896884262561798 0.6849686503410339
MemoryTrain:  epoch  4, batch     4 | loss: 3.1400590Losses:  3.3474366664886475 0.5142203569412231 0.7371342182159424
MemoryTrain:  epoch  4, batch     5 | loss: 3.3474367Losses:  3.9479150772094727 1.1556477546691895 0.7280169725418091
MemoryTrain:  epoch  4, batch     6 | loss: 3.9479151Losses:  4.374660968780518 0.6513508558273315 0.8243333697319031
MemoryTrain:  epoch  4, batch     7 | loss: 4.3746610Losses:  3.397531032562256 0.4491063952445984 0.7723902463912964
MemoryTrain:  epoch  4, batch     8 | loss: 3.3975310Losses:  3.6554722785949707 0.9660675525665283 0.7181950211524963
MemoryTrain:  epoch  4, batch     9 | loss: 3.6554723Losses:  3.8116531372070312 1.0000395774841309 0.7803303003311157
MemoryTrain:  epoch  4, batch    10 | loss: 3.8116531Losses:  3.284353256225586 0.5294667482376099 0.7482906579971313
MemoryTrain:  epoch  4, batch    11 | loss: 3.2843533Losses:  3.642658233642578 0.7032226324081421 0.7928470969200134
MemoryTrain:  epoch  4, batch    12 | loss: 3.6426582Losses:  4.233583927154541 0.5835769176483154 0.5419679880142212
MemoryTrain:  epoch  4, batch    13 | loss: 4.2335839Losses:  3.1872220039367676 0.22671082615852356 0.7065272927284241
MemoryTrain:  epoch  5, batch     0 | loss: 3.1872220Losses:  3.6801304817199707 0.9450046420097351 0.7057114839553833
MemoryTrain:  epoch  5, batch     1 | loss: 3.6801305Losses:  3.4993109703063965 0.6989277601242065 0.8060499429702759
MemoryTrain:  epoch  5, batch     2 | loss: 3.4993110Losses:  3.412228584289551 0.7043863534927368 0.7646955251693726
MemoryTrain:  epoch  5, batch     3 | loss: 3.4122286Losses:  3.370912790298462 0.6990140080451965 0.774097740650177
MemoryTrain:  epoch  5, batch     4 | loss: 3.3709128Losses:  3.23483943939209 0.499672532081604 0.7926713228225708
MemoryTrain:  epoch  5, batch     5 | loss: 3.2348394Losses:  2.909998655319214 0.23684971034526825 0.7612064480781555
MemoryTrain:  epoch  5, batch     6 | loss: 2.9099987Losses:  3.340686559677124 0.46896475553512573 0.7245545983314514
MemoryTrain:  epoch  5, batch     7 | loss: 3.3406866Losses:  3.7312638759613037 0.7333018779754639 0.7122089862823486
MemoryTrain:  epoch  5, batch     8 | loss: 3.7312639Losses:  3.8506674766540527 0.8073725700378418 0.706477701663971
MemoryTrain:  epoch  5, batch     9 | loss: 3.8506675Losses:  3.7988200187683105 1.0902284383773804 0.7932671308517456
MemoryTrain:  epoch  5, batch    10 | loss: 3.7988200Losses:  3.740186929702759 0.8324441909790039 0.8000534176826477
MemoryTrain:  epoch  5, batch    11 | loss: 3.7401869Losses:  3.4317026138305664 0.4757143259048462 0.7723398208618164
MemoryTrain:  epoch  5, batch    12 | loss: 3.4317026Losses:  2.6271913051605225 0.2597959041595459 0.40877172350883484
MemoryTrain:  epoch  5, batch    13 | loss: 2.6271913Losses:  3.5670924186706543 0.7190871238708496 0.7292824983596802
MemoryTrain:  epoch  6, batch     0 | loss: 3.5670924Losses:  3.688138723373413 0.9702208638191223 0.7185209393501282
MemoryTrain:  epoch  6, batch     1 | loss: 3.6881387Losses:  3.8484113216400146 0.9409160614013672 0.7812522053718567
MemoryTrain:  epoch  6, batch     2 | loss: 3.8484113Losses:  3.7525274753570557 1.2629897594451904 0.5730745196342468
MemoryTrain:  epoch  6, batch     3 | loss: 3.7525275Losses:  3.3417325019836426 0.7174704670906067 0.7017899751663208
MemoryTrain:  epoch  6, batch     4 | loss: 3.3417325Losses:  3.008897542953491 0.4688029885292053 0.6496546864509583
MemoryTrain:  epoch  6, batch     5 | loss: 3.0088975Losses:  3.548368453979492 1.0795071125030518 0.5612188577651978
MemoryTrain:  epoch  6, batch     6 | loss: 3.5483685Losses:  3.903972864151001 1.0434712171554565 0.6952902674674988
MemoryTrain:  epoch  6, batch     7 | loss: 3.9039729Losses:  3.2835099697113037 0.2682502269744873 0.7761306166648865
MemoryTrain:  epoch  6, batch     8 | loss: 3.2835100Losses:  4.142949104309082 1.2834324836730957 0.8266629576683044
MemoryTrain:  epoch  6, batch     9 | loss: 4.1429491Losses:  2.992702007293701 0.22791001200675964 0.8767036199569702
MemoryTrain:  epoch  6, batch    10 | loss: 2.9927020Losses:  3.8055238723754883 1.1591053009033203 0.7648297548294067
MemoryTrain:  epoch  6, batch    11 | loss: 3.8055239Losses:  3.152625322341919 0.4768621325492859 0.7973747849464417
MemoryTrain:  epoch  6, batch    12 | loss: 3.1526253Losses:  3.2011263370513916 0.5070123672485352 0.4699232578277588
MemoryTrain:  epoch  6, batch    13 | loss: 3.2011263Losses:  3.109750747680664 0.4618818461894989 0.6259143948554993
MemoryTrain:  epoch  7, batch     0 | loss: 3.1097507Losses:  3.330312728881836 0.461317241191864 0.7844668626785278
MemoryTrain:  epoch  7, batch     1 | loss: 3.3303127Losses:  4.249733924865723 1.313685417175293 0.7522336840629578
MemoryTrain:  epoch  7, batch     2 | loss: 4.2497339Losses:  3.6934752464294434 1.0391499996185303 0.7726135849952698
MemoryTrain:  epoch  7, batch     3 | loss: 3.6934752Losses:  2.9802777767181396 0.502859354019165 0.5739734172821045
MemoryTrain:  epoch  7, batch     4 | loss: 2.9802778Losses:  3.512925863265991 0.9708511829376221 0.6411531567573547
MemoryTrain:  epoch  7, batch     5 | loss: 3.5129259Losses:  3.659151792526245 0.7524426579475403 0.8590087890625
MemoryTrain:  epoch  7, batch     6 | loss: 3.6591518Losses:  3.112703800201416 0.4885973334312439 0.7096154093742371
MemoryTrain:  epoch  7, batch     7 | loss: 3.1127038Losses:  2.980250358581543 0.24706938862800598 0.8225764036178589
MemoryTrain:  epoch  7, batch     8 | loss: 2.9802504Losses:  3.343198299407959 0.720726728439331 0.7271857857704163
MemoryTrain:  epoch  7, batch     9 | loss: 3.3431983Losses:  3.6735482215881348 0.7105609774589539 0.8371739983558655
MemoryTrain:  epoch  7, batch    10 | loss: 3.6735482Losses:  3.631232976913452 0.919381320476532 0.7988784909248352
MemoryTrain:  epoch  7, batch    11 | loss: 3.6312330Losses:  2.9270968437194824 0.23781850934028625 0.6904719471931458
MemoryTrain:  epoch  7, batch    12 | loss: 2.9270968Losses:  2.567641019821167 0.2742953598499298 0.40201273560523987
MemoryTrain:  epoch  7, batch    13 | loss: 2.5676410Losses:  2.9659602642059326 0.25649207830429077 0.7998083233833313
MemoryTrain:  epoch  8, batch     0 | loss: 2.9659603Losses:  3.459260940551758 0.8760533928871155 0.6770535707473755
MemoryTrain:  epoch  8, batch     1 | loss: 3.4592609Losses:  3.00982403755188 0.24129816889762878 0.7635439038276672
MemoryTrain:  epoch  8, batch     2 | loss: 3.0098240Losses:  4.126745700836182 1.3141930103302002 0.7067211270332336
MemoryTrain:  epoch  8, batch     3 | loss: 4.1267457Losses:  2.977902412414551 0.259147584438324 0.760818600654602
MemoryTrain:  epoch  8, batch     4 | loss: 2.9779024Losses:  2.9545302391052246 0.24369215965270996 0.7820166945457458
MemoryTrain:  epoch  8, batch     5 | loss: 2.9545302Losses:  3.5040037631988525 0.7120287418365479 0.7638184428215027
MemoryTrain:  epoch  8, batch     6 | loss: 3.5040038Losses:  3.3194851875305176 0.4661284387111664 0.8160157203674316
MemoryTrain:  epoch  8, batch     7 | loss: 3.3194852Losses:  3.321425437927246 0.7276632785797119 0.6826545000076294
MemoryTrain:  epoch  8, batch     8 | loss: 3.3214254Losses:  3.64306640625 0.5904877185821533 0.7977255582809448
MemoryTrain:  epoch  8, batch     9 | loss: 3.6430664Losses:  3.447096586227417 0.7113511562347412 0.7797176241874695
MemoryTrain:  epoch  8, batch    10 | loss: 3.4470966Losses:  3.0804851055145264 0.4624073803424835 0.7068689465522766
MemoryTrain:  epoch  8, batch    11 | loss: 3.0804851Losses:  3.149675130844116 0.4833328127861023 0.7671865820884705
MemoryTrain:  epoch  8, batch    12 | loss: 3.1496751Losses:  2.287080764770508 -0.0 0.43734225630760193
MemoryTrain:  epoch  8, batch    13 | loss: 2.2870808Losses:  3.1935794353485107 0.48260942101478577 0.8249736428260803
MemoryTrain:  epoch  9, batch     0 | loss: 3.1935794Losses:  3.9612045288085938 1.3056092262268066 0.7957600951194763
MemoryTrain:  epoch  9, batch     1 | loss: 3.9612045Losses:  4.870266914367676 2.3934710025787354 0.5055760145187378
MemoryTrain:  epoch  9, batch     2 | loss: 4.8702669Losses:  3.767526865005493 1.1930629014968872 0.7064940333366394
MemoryTrain:  epoch  9, batch     3 | loss: 3.7675269Losses:  3.5705456733703613 0.9721124172210693 0.6598790884017944
MemoryTrain:  epoch  9, batch     4 | loss: 3.5705457Losses:  3.184232473373413 0.46286264061927795 0.8295483589172363
MemoryTrain:  epoch  9, batch     5 | loss: 3.1842325Losses:  4.002364158630371 1.3142898082733154 0.7746244668960571
MemoryTrain:  epoch  9, batch     6 | loss: 4.0023642Losses:  2.7637689113616943 -0.0 0.7769482731819153
MemoryTrain:  epoch  9, batch     7 | loss: 2.7637689Losses:  3.5724685192108154 1.112872838973999 0.5614549517631531
MemoryTrain:  epoch  9, batch     8 | loss: 3.5724685Losses:  3.3228812217712402 0.4784739017486572 0.7750789523124695
MemoryTrain:  epoch  9, batch     9 | loss: 3.3228812Losses:  4.0097761154174805 1.231485366821289 0.6188170909881592
MemoryTrain:  epoch  9, batch    10 | loss: 4.0097761Losses:  3.43060564994812 0.9229047298431396 0.6565496921539307
MemoryTrain:  epoch  9, batch    11 | loss: 3.4306056Losses:  3.1506423950195312 0.22726300358772278 0.8765978217124939
MemoryTrain:  epoch  9, batch    12 | loss: 3.1506424Losses:  2.5887622833251953 -0.0 0.4649229943752289
MemoryTrain:  epoch  9, batch    13 | loss: 2.5887623
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 75.83%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 57.03%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 59.82%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 60.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 61.03%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 61.11%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 61.84%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 62.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 64.29%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 67.12%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 71.30%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.28%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 73.33%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 73.79%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 74.81%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 73.35%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 72.32%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 71.70%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 70.95%   [EVAL] batch:   37 | acc: 43.75%,  total acc: 70.23%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 69.55%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 69.53%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 71.08%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 71.31%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 71.67%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 70.65%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 70.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 70.15%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 69.25%   [EVAL] batch:   50 | acc: 25.00%,  total acc: 68.38%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 68.15%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 67.69%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 67.94%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 68.41%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 68.21%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 67.73%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 66.73%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 65.77%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 65.33%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 65.58%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 65.53%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 64.55%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 63.79%   [EVAL] batch:   68 | acc: 6.25%,  total acc: 62.95%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 62.86%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 62.59%   [EVAL] batch:   71 | acc: 31.25%,  total acc: 62.15%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 61.99%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 61.91%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 61.75%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 61.43%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 61.53%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 61.54%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 61.23%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 60.55%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 59.88%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 59.22%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 58.51%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 57.89%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 57.28%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 56.83%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 56.25%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 56.53%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 56.95%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 57.43%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 57.90%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 58.36%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.80%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 59.24%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 59.67%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 59.90%   [EVAL] batch:   96 | acc: 12.50%,  total acc: 59.41%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 58.99%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 59.28%   [EVAL] batch:   99 | acc: 93.75%,  total acc: 59.62%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 59.84%   [EVAL] batch:  101 | acc: 56.25%,  total acc: 59.80%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 59.95%   [EVAL] batch:  103 | acc: 43.75%,  total acc: 59.80%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 60.00%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 60.26%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 60.57%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 61.24%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 61.48%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 61.49%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 61.55%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 61.73%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 61.95%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 62.07%   [EVAL] batch:  115 | acc: 37.50%,  total acc: 61.85%   
cur_acc:  ['0.8485', '0.8795', '0.3750', '0.6827', '0.2784', '0.7991', '0.7583']
his_acc:  ['0.8485', '0.8630', '0.7731', '0.7273', '0.5803', '0.6126', '0.6185']
Clustering into  18  clusters
Clusters:  [ 2  4  5  2  0 11 13  2  1  4  7  2 14  5  6 16  3 10 11  1  7  5  5 17
 11  1  6  8  2  5 12  2  3 15  0  1  2  9  7  1  0]
Losses:  15.029452323913574 8.764728546142578 0.5905067324638367
CurrentTrain: epoch  0, batch     0 | loss: 15.0294523Losses:  8.941896438598633 3.58508563041687 0.36558958888053894
CurrentTrain: epoch  0, batch     1 | loss: 8.9418964Losses:  12.03625774383545 6.751934051513672 0.5247289538383484
CurrentTrain: epoch  1, batch     0 | loss: 12.0362577Losses:  5.711633205413818 1.5128936767578125 0.4627144932746887
CurrentTrain: epoch  1, batch     1 | loss: 5.7116332Losses:  13.716448783874512 8.896256446838379 0.5000143647193909
CurrentTrain: epoch  2, batch     0 | loss: 13.7164488Losses:  6.486658096313477 3.4858429431915283 0.33859676122665405
CurrentTrain: epoch  2, batch     1 | loss: 6.4866581Losses:  11.465106964111328 7.2874836921691895 0.47120118141174316
CurrentTrain: epoch  3, batch     0 | loss: 11.4651070Losses:  6.036775588989258 2.526977300643921 0.3161618709564209
CurrentTrain: epoch  3, batch     1 | loss: 6.0367756Losses:  10.06974983215332 6.655562877655029 0.4471451938152313
CurrentTrain: epoch  4, batch     0 | loss: 10.0697498Losses:  6.314760208129883 2.5133981704711914 0.31570881605148315
CurrentTrain: epoch  4, batch     1 | loss: 6.3147602Losses:  10.44020938873291 7.249778747558594 0.4397891163825989
CurrentTrain: epoch  5, batch     0 | loss: 10.4402094Losses:  7.331019878387451 3.790858030319214 0.324014276266098
CurrentTrain: epoch  5, batch     1 | loss: 7.3310199Losses:  9.723661422729492 6.250130653381348 0.39014801383018494
CurrentTrain: epoch  6, batch     0 | loss: 9.7236614Losses:  3.9759037494659424 1.6768327951431274 0.3858020305633545
CurrentTrain: epoch  6, batch     1 | loss: 3.9759037Losses:  8.4130277633667 5.746954917907715 0.3789151608943939
CurrentTrain: epoch  7, batch     0 | loss: 8.4130278Losses:  4.90869665145874 1.8506368398666382 0.38205257058143616
CurrentTrain: epoch  7, batch     1 | loss: 4.9086967Losses:  9.0955171585083 6.679828643798828 0.3723936676979065
CurrentTrain: epoch  8, batch     0 | loss: 9.0955172Losses:  5.010092735290527 2.546461820602417 0.29235589504241943
CurrentTrain: epoch  8, batch     1 | loss: 5.0100927Losses:  9.279804229736328 6.673432350158691 0.3695758283138275
CurrentTrain: epoch  9, batch     0 | loss: 9.2798042Losses:  4.737878322601318 2.5154106616973877 0.29014432430267334
CurrentTrain: epoch  9, batch     1 | loss: 4.7378783
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: in 1974 , divorced and the mother of a 5-year-old , forsberg moved to boston and began studying arms control at mit ; she received her ph.d. in 1980 .
Head Entity: forsberg
Tail Entity: boston
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: After years of living in New York, she decided to relocate to San Francisco to pursue her career in tech.  
Head Entity: she  
Tail Entity: San Francisco  

Relation: person cities of residence  
Context: Following his retirement, John moved from Chicago to Miami, where he enjoys the warm weather and vibrant culture.  
Head Entity: John  
Tail Entity: Miami  

Relation: person cities of residence  
Context: The famous author spent most of her life in Paris, drawing inspiration from the city's rich history and artistic atmosphere.  
Head Entity: The famous author  
Tail Entity: Paris  

Relation: person cities of residence  
Context: After completing his studies in London, he returned to his hometown of Seattle to start a family.  
Head Entity: he  
Tail Entity: Seattle  

Relation: person cities of residence  
Context: Maria has lived in various cities, but she currently calls Austin her home, where she loves the music scene.  
Head Entity: Maria  
Tail Entity: Austin  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, John enrolled at Stanford University to study computer science.  
Head Entity: John  
Tail Entity: Stanford University  

Relation: person schools attended  
Context: Maria attended the University of Oxford for her undergraduate studies, where she majored in literature.  
Head Entity: Maria  
Tail Entity: University of Oxford  

Relation: person schools attended  
Context: Following his passion for engineering, David went to the Massachusetts Institute of Technology to further his education.  
Head Entity: David  
Tail Entity: Massachusetts Institute of Technology  

Relation: person schools attended  
Context: Emily spent four years at the University of California, Berkeley, where she earned her degree in environmental science.  
Head Entity: Emily  
Tail Entity: University of California, Berkeley  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author agatha christie died in her home in wallingford, england  
Head Entity: agatha christie  
Tail Entity: england  

Relation: person country of death  
Context: legendary musician freddie mercury succumbed to aids in london, united kingdom  
Head Entity: freddie mercury  
Tail Entity: united kingdom  

Relation: person country of death  
Context: beloved actor robin williams was found dead in his home in california, usa  
Head Entity: robin williams  
Tail Entity: usa  

Relation: person country of death  
Context: influential civil rights leader martin luther king jr. was assassinated in memphis, tennessee, usa  
Head Entity: martin luther king jr.  
Tail Entity: usa  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both very talented.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the divorce, he took custody of his two daughters, lily and rose, who are now thriving in school.  
Head Entity: he  
Tail Entity: rose  

Relation: person children  
Context: the famous actor is a proud father of four, with his youngest being a daughter named sophia.  
Head Entity: the famous actor  
Tail Entity: sophia  

Relation: person children  
Context: they often visit their grandparents, who love spending time with their grandchildren, including their grandson, max.  
Head Entity: they  
Tail Entity: max  

Relation: person children  
Context: she often shares stories about her two sons, alex and ben, who are both passionate about sports.  
Head Entity: she  
Tail Entity: ben  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the suspect was officially charged with theft after a thorough investigation.  
Head Entity: suspect  
Tail Entity: theft  

Relation: person charges  
Context: After the incident, the authorities confirmed that the individual was charged with assault and battery.  
Head Entity: individual  
Tail Entity: assault and battery  

Relation: person charges  
Context: Following the evidence presented, the jury found that the defendant was charged with fraud.  
Head Entity: defendant  
Tail Entity: fraud  

Relation: person charges  
Context: The police reported that the man was charged with driving under the influence after the accident.  
Head Entity: man  
Tail Entity: driving under the influence  

Relation: person charges  
Context: In a surprising turn of events, the celebrity was charged with tax evasion by the federal government.  
Head Entity: celebrity  
Tail Entity: tax evasion  
Losses:  3.8455638885498047 0.5096877813339233 0.5542045831680298
MemoryTrain:  epoch  0, batch     0 | loss: 3.8455639Losses:  5.155953884124756 0.8246543407440186 0.7668395042419434
MemoryTrain:  epoch  0, batch     1 | loss: 5.1559539Losses:  5.89907169342041 1.1169263124465942 0.8001953363418579
MemoryTrain:  epoch  0, batch     2 | loss: 5.8990717Losses:  4.886824131011963 0.8313330411911011 0.626670241355896
MemoryTrain:  epoch  0, batch     3 | loss: 4.8868241Losses:  3.9500439167022705 0.5158627033233643 0.7821912169456482
MemoryTrain:  epoch  0, batch     4 | loss: 3.9500439Losses:  6.303360939025879 1.0835323333740234 0.9001356959342957
MemoryTrain:  epoch  0, batch     5 | loss: 6.3033609Losses:  4.141030788421631 0.46202847361564636 0.8889172077178955
MemoryTrain:  epoch  0, batch     6 | loss: 4.1410308Losses:  4.125980377197266 0.24382852017879486 0.7607617974281311
MemoryTrain:  epoch  0, batch     7 | loss: 4.1259804Losses:  5.263112545013428 0.8743680715560913 0.7720967531204224
MemoryTrain:  epoch  0, batch     8 | loss: 5.2631125Losses:  5.385379314422607 1.3062007427215576 0.8855176568031311
MemoryTrain:  epoch  0, batch     9 | loss: 5.3853793Losses:  4.495347023010254 0.23841840028762817 0.7938212752342224
MemoryTrain:  epoch  0, batch    10 | loss: 4.4953470Losses:  4.565875053405762 0.4897919297218323 0.6887128949165344
MemoryTrain:  epoch  0, batch    11 | loss: 4.5658751Losses:  4.266585826873779 0.5064094662666321 0.7926129102706909
MemoryTrain:  epoch  0, batch    12 | loss: 4.2665858Losses:  3.3070015907287598 0.25500476360321045 0.8450812101364136
MemoryTrain:  epoch  0, batch    13 | loss: 3.3070016Losses:  5.0477519035339355 0.7730169892311096 0.7829009890556335
MemoryTrain:  epoch  0, batch    14 | loss: 5.0477519Losses:  2.9942378997802734 -0.0 0.38109150528907776
MemoryTrain:  epoch  0, batch    15 | loss: 2.9942379Losses:  3.9228515625 0.7662851214408875 0.8120397925376892
MemoryTrain:  epoch  1, batch     0 | loss: 3.9228516Losses:  5.0102105140686035 1.6600756645202637 0.8846182823181152
MemoryTrain:  epoch  1, batch     1 | loss: 5.0102105Losses:  4.1669721603393555 0.2542037069797516 0.815407931804657
MemoryTrain:  epoch  1, batch     2 | loss: 4.1669722Losses:  4.005326271057129 0.709945023059845 0.7015721201896667
MemoryTrain:  epoch  1, batch     3 | loss: 4.0053263Losses:  4.346345901489258 0.5193420648574829 0.7814467549324036
MemoryTrain:  epoch  1, batch     4 | loss: 4.3463459Losses:  4.392549991607666 1.6740268468856812 0.6629329323768616
MemoryTrain:  epoch  1, batch     5 | loss: 4.3925500Losses:  3.80264949798584 0.25112271308898926 0.736893355846405
MemoryTrain:  epoch  1, batch     6 | loss: 3.8026495Losses:  4.202264785766602 1.050105333328247 0.5765930414199829
MemoryTrain:  epoch  1, batch     7 | loss: 4.2022648Losses:  3.477916717529297 0.2425743043422699 0.8087950944900513
MemoryTrain:  epoch  1, batch     8 | loss: 3.4779167Losses:  4.4063920974731445 0.5940778255462646 0.7224079370498657
MemoryTrain:  epoch  1, batch     9 | loss: 4.4063921Losses:  4.138740062713623 1.3381831645965576 0.7768865823745728
MemoryTrain:  epoch  1, batch    10 | loss: 4.1387401Losses:  4.765300750732422 0.9578448534011841 0.6508272290229797
MemoryTrain:  epoch  1, batch    11 | loss: 4.7653008Losses:  3.7092368602752686 0.4686576724052429 0.6904058456420898
MemoryTrain:  epoch  1, batch    12 | loss: 3.7092369Losses:  4.422855377197266 0.7788304090499878 0.6550520658493042
MemoryTrain:  epoch  1, batch    13 | loss: 4.4228554Losses:  3.227442741394043 -0.0 0.8875457644462585
MemoryTrain:  epoch  1, batch    14 | loss: 3.2274427Losses:  2.381049156188965 -0.0 0.4302503764629364
MemoryTrain:  epoch  1, batch    15 | loss: 2.3810492Losses:  3.70389461517334 0.29072004556655884 0.7757441401481628
MemoryTrain:  epoch  2, batch     0 | loss: 3.7038946Losses:  3.1240270137786865 0.2364114671945572 0.7164261341094971
MemoryTrain:  epoch  2, batch     1 | loss: 3.1240270Losses:  3.2369585037231445 0.23583748936653137 0.7497480511665344
MemoryTrain:  epoch  2, batch     2 | loss: 3.2369585Losses:  3.4145309925079346 0.45823588967323303 0.7732510566711426
MemoryTrain:  epoch  2, batch     3 | loss: 3.4145310Losses:  3.5624756813049316 0.7825908660888672 0.6682405471801758
MemoryTrain:  epoch  2, batch     4 | loss: 3.5624757Losses:  3.077270746231079 0.2402799427509308 0.8282696604728699
MemoryTrain:  epoch  2, batch     5 | loss: 3.0772707Losses:  3.491772413253784 0.46442461013793945 0.7274672389030457
MemoryTrain:  epoch  2, batch     6 | loss: 3.4917724Losses:  3.019125461578369 -0.0 0.7241827845573425
MemoryTrain:  epoch  2, batch     7 | loss: 3.0191255Losses:  3.2744522094726562 -0.0 0.9604625105857849
MemoryTrain:  epoch  2, batch     8 | loss: 3.2744522Losses:  3.362497568130493 0.4784713387489319 0.9135205149650574
MemoryTrain:  epoch  2, batch     9 | loss: 3.3624976Losses:  2.991234540939331 0.23871836066246033 0.7835831642150879
MemoryTrain:  epoch  2, batch    10 | loss: 2.9912345Losses:  2.9213881492614746 0.23318904638290405 0.6251324415206909
MemoryTrain:  epoch  2, batch    11 | loss: 2.9213881Losses:  4.634398937225342 1.1642301082611084 0.7814592719078064
MemoryTrain:  epoch  2, batch    12 | loss: 4.6343989Losses:  3.8668887615203857 1.0940043926239014 0.7656999230384827
MemoryTrain:  epoch  2, batch    13 | loss: 3.8668888Losses:  3.2860970497131348 0.4888807237148285 0.7212494611740112
MemoryTrain:  epoch  2, batch    14 | loss: 3.2860970Losses:  2.8457794189453125 -0.0 0.4511357247829437
MemoryTrain:  epoch  2, batch    15 | loss: 2.8457794Losses:  3.948371171951294 1.232795000076294 0.8492417931556702
MemoryTrain:  epoch  3, batch     0 | loss: 3.9483712Losses:  3.4179441928863525 0.7096099853515625 0.7705476880073547
MemoryTrain:  epoch  3, batch     1 | loss: 3.4179442Losses:  3.349693775177002 0.5190340876579285 0.6550523042678833
MemoryTrain:  epoch  3, batch     2 | loss: 3.3496938Losses:  3.0521750450134277 0.25121209025382996 0.7788529396057129
MemoryTrain:  epoch  3, batch     3 | loss: 3.0521750Losses:  2.9548139572143555 0.2500563859939575 0.7224756479263306
MemoryTrain:  epoch  3, batch     4 | loss: 2.9548140Losses:  3.537074089050293 0.6947687864303589 0.7728133201599121
MemoryTrain:  epoch  3, batch     5 | loss: 3.5370741Losses:  3.4826653003692627 0.7348976135253906 0.7071617245674133
MemoryTrain:  epoch  3, batch     6 | loss: 3.4826653Losses:  4.334815502166748 1.3972245454788208 0.6518353223800659
MemoryTrain:  epoch  3, batch     7 | loss: 4.3348155Losses:  4.050739288330078 1.388708233833313 0.558894157409668
MemoryTrain:  epoch  3, batch     8 | loss: 4.0507393Losses:  3.343888282775879 0.5272842645645142 0.7817005515098572
MemoryTrain:  epoch  3, batch     9 | loss: 3.3438883Losses:  3.162942409515381 0.47032827138900757 0.7160423994064331
MemoryTrain:  epoch  3, batch    10 | loss: 3.1629424Losses:  3.443812847137451 0.7447904348373413 0.7779680490493774
MemoryTrain:  epoch  3, batch    11 | loss: 3.4438128Losses:  3.863164186477661 1.2599084377288818 0.7360959649085999
MemoryTrain:  epoch  3, batch    12 | loss: 3.8631642Losses:  3.0891637802124023 0.23827575147151947 0.7123764753341675
MemoryTrain:  epoch  3, batch    13 | loss: 3.0891638Losses:  3.3900680541992188 0.5470390319824219 0.8456629514694214
MemoryTrain:  epoch  3, batch    14 | loss: 3.3900681Losses:  3.0722901821136475 -0.0 0.4231548309326172
MemoryTrain:  epoch  3, batch    15 | loss: 3.0722902Losses:  3.314042091369629 0.7332553863525391 0.6551223397254944
MemoryTrain:  epoch  4, batch     0 | loss: 3.3140421Losses:  3.0296943187713623 0.23481354117393494 0.7480247616767883
MemoryTrain:  epoch  4, batch     1 | loss: 3.0296943Losses:  3.0790040493011475 0.4777355194091797 0.7037237286567688
MemoryTrain:  epoch  4, batch     2 | loss: 3.0790040Losses:  2.7449517250061035 -0.0 0.8195134401321411
MemoryTrain:  epoch  4, batch     3 | loss: 2.7449517Losses:  2.962238311767578 0.22590048611164093 0.7009933590888977
MemoryTrain:  epoch  4, batch     4 | loss: 2.9622383Losses:  3.6796493530273438 0.9691083431243896 0.7388652563095093
MemoryTrain:  epoch  4, batch     5 | loss: 3.6796494Losses:  3.6000072956085205 0.7017304301261902 0.7877965569496155
MemoryTrain:  epoch  4, batch     6 | loss: 3.6000073Losses:  3.009002447128296 0.2557435631752014 0.8413659930229187
MemoryTrain:  epoch  4, batch     7 | loss: 3.0090024Losses:  3.4674060344696045 0.5158041715621948 0.7953274846076965
MemoryTrain:  epoch  4, batch     8 | loss: 3.4674060Losses:  3.1935930252075195 0.49613264203071594 0.7550093531608582
MemoryTrain:  epoch  4, batch     9 | loss: 3.1935930Losses:  3.6445975303649902 0.9544764757156372 0.7269033789634705
MemoryTrain:  epoch  4, batch    10 | loss: 3.6445975Losses:  3.2219934463500977 0.2386847585439682 0.8349223136901855
MemoryTrain:  epoch  4, batch    11 | loss: 3.2219934Losses:  3.8567609786987305 1.126691460609436 0.6567472815513611
MemoryTrain:  epoch  4, batch    12 | loss: 3.8567610Losses:  2.902005195617676 0.23165994882583618 0.7017176151275635
MemoryTrain:  epoch  4, batch    13 | loss: 2.9020052Losses:  3.9519262313842773 1.3416814804077148 0.6342239379882812
MemoryTrain:  epoch  4, batch    14 | loss: 3.9519262Losses:  2.7312333583831787 0.30817756056785583 0.481772243976593
MemoryTrain:  epoch  4, batch    15 | loss: 2.7312334Losses:  3.868645429611206 1.0476075410842896 0.8067075610160828
MemoryTrain:  epoch  5, batch     0 | loss: 3.8686454Losses:  3.5349600315093994 1.0806360244750977 0.5524303317070007
MemoryTrain:  epoch  5, batch     1 | loss: 3.5349600Losses:  3.6165623664855957 1.0187389850616455 0.6529961824417114
MemoryTrain:  epoch  5, batch     2 | loss: 3.6165624Losses:  3.2146244049072266 0.4848235845565796 0.7314826250076294
MemoryTrain:  epoch  5, batch     3 | loss: 3.2146244Losses:  3.356029510498047 0.7129821181297302 0.7052630186080933
MemoryTrain:  epoch  5, batch     4 | loss: 3.3560295Losses:  2.9018311500549316 0.2540037930011749 0.698510468006134
MemoryTrain:  epoch  5, batch     5 | loss: 2.9018312Losses:  3.1779978275299072 0.48463010787963867 0.7691953182220459
MemoryTrain:  epoch  5, batch     6 | loss: 3.1779978Losses:  2.732053279876709 -0.0 0.8327836990356445
MemoryTrain:  epoch  5, batch     7 | loss: 2.7320533Losses:  3.2973573207855225 0.7413806915283203 0.6265357136726379
MemoryTrain:  epoch  5, batch     8 | loss: 3.2973573Losses:  3.239372491836548 0.5062994360923767 0.7140081524848938
MemoryTrain:  epoch  5, batch     9 | loss: 3.2393725Losses:  3.3367106914520264 0.6954348087310791 0.6966044902801514
MemoryTrain:  epoch  5, batch    10 | loss: 3.3367107Losses:  2.966026782989502 0.21511432528495789 0.8249396085739136
MemoryTrain:  epoch  5, batch    11 | loss: 2.9660268Losses:  3.421870231628418 0.5156323313713074 0.9224337339401245
MemoryTrain:  epoch  5, batch    12 | loss: 3.4218702Losses:  3.0492634773254395 0.2357930839061737 0.908977210521698
MemoryTrain:  epoch  5, batch    13 | loss: 3.0492635Losses:  3.0496206283569336 0.4577769935131073 0.7225877046585083
MemoryTrain:  epoch  5, batch    14 | loss: 3.0496206Losses:  2.3856453895568848 -0.0 0.49937599897384644
MemoryTrain:  epoch  5, batch    15 | loss: 2.3856454Losses:  3.0601086616516113 0.47424113750457764 0.6255645751953125
MemoryTrain:  epoch  6, batch     0 | loss: 3.0601087Losses:  3.1801259517669678 0.4777596890926361 0.7854723930358887
MemoryTrain:  epoch  6, batch     1 | loss: 3.1801260Losses:  3.028033494949341 0.22325189411640167 0.8970627784729004
MemoryTrain:  epoch  6, batch     2 | loss: 3.0280335Losses:  3.6691904067993164 1.087690830230713 0.6511054039001465
MemoryTrain:  epoch  6, batch     3 | loss: 3.6691904Losses:  3.096731662750244 0.24481597542762756 0.902664065361023
MemoryTrain:  epoch  6, batch     4 | loss: 3.0967317Losses:  3.305976629257202 0.7019470930099487 0.6865031123161316
MemoryTrain:  epoch  6, batch     5 | loss: 3.3059766Losses:  3.208740234375 0.4902693033218384 0.7713809013366699
MemoryTrain:  epoch  6, batch     6 | loss: 3.2087402Losses:  3.3226044178009033 0.7214094996452332 0.7007277607917786
MemoryTrain:  epoch  6, batch     7 | loss: 3.3226044Losses:  3.3655662536621094 0.5978177785873413 0.7861122488975525
MemoryTrain:  epoch  6, batch     8 | loss: 3.3655663Losses:  3.1398582458496094 0.48160332441329956 0.7729240655899048
MemoryTrain:  epoch  6, batch     9 | loss: 3.1398582Losses:  2.6928558349609375 -0.0 0.7694154977798462
MemoryTrain:  epoch  6, batch    10 | loss: 2.6928558Losses:  3.1494908332824707 0.5348997712135315 0.7127658128738403
MemoryTrain:  epoch  6, batch    11 | loss: 3.1494908Losses:  3.0916972160339355 0.4712978005409241 0.7028895616531372
MemoryTrain:  epoch  6, batch    12 | loss: 3.0916972Losses:  3.1876957416534424 0.47715312242507935 0.8410811424255371
MemoryTrain:  epoch  6, batch    13 | loss: 3.1876957Losses:  3.0972447395324707 0.48620155453681946 0.6453826427459717
MemoryTrain:  epoch  6, batch    14 | loss: 3.0972447Losses:  2.185042381286621 -0.0 0.31699010729789734
MemoryTrain:  epoch  6, batch    15 | loss: 2.1850424Losses:  2.9049015045166016 0.22552210092544556 0.8207940459251404
MemoryTrain:  epoch  7, batch     0 | loss: 2.9049015Losses:  3.2583110332489014 0.9423055052757263 0.37026023864746094
MemoryTrain:  epoch  7, batch     1 | loss: 3.2583110Losses:  4.498036861419678 2.1211965084075928 0.48027580976486206
MemoryTrain:  epoch  7, batch     2 | loss: 4.4980369Losses:  3.923387050628662 1.3056457042694092 0.7189208269119263
MemoryTrain:  epoch  7, batch     3 | loss: 3.9233871Losses:  2.652193307876587 -0.0 0.7490483522415161
MemoryTrain:  epoch  7, batch     4 | loss: 2.6521933Losses:  2.9828052520751953 0.23416659235954285 0.8317866325378418
MemoryTrain:  epoch  7, batch     5 | loss: 2.9828053Losses:  2.942656993865967 0.22976051270961761 0.8458888530731201
MemoryTrain:  epoch  7, batch     6 | loss: 2.9426570Losses:  3.371450901031494 0.6688435673713684 0.767342746257782
MemoryTrain:  epoch  7, batch     7 | loss: 3.3714509Losses:  3.032478094100952 0.4623565673828125 0.6364228129386902
MemoryTrain:  epoch  7, batch     8 | loss: 3.0324781Losses:  3.1797330379486084 0.474258691072464 0.7941893935203552
MemoryTrain:  epoch  7, batch     9 | loss: 3.1797330Losses:  3.579345464706421 0.9429157972335815 0.7314193844795227
MemoryTrain:  epoch  7, batch    10 | loss: 3.5793455Losses:  3.114773750305176 0.5094833374023438 0.6846651434898376
MemoryTrain:  epoch  7, batch    11 | loss: 3.1147738Losses:  2.955028772354126 0.22795137763023376 0.8279446959495544
MemoryTrain:  epoch  7, batch    12 | loss: 2.9550288Losses:  3.278064727783203 0.7012316584587097 0.7046118974685669
MemoryTrain:  epoch  7, batch    13 | loss: 3.2780647Losses:  3.20343279838562 0.5042058229446411 0.7762197852134705
MemoryTrain:  epoch  7, batch    14 | loss: 3.2034328Losses:  2.4021947383880615 -0.0 0.5031642317771912
MemoryTrain:  epoch  7, batch    15 | loss: 2.4021947Losses:  4.116040229797363 1.4330024719238281 0.6330946087837219
MemoryTrain:  epoch  8, batch     0 | loss: 4.1160402Losses:  2.961397409439087 0.2426137626171112 0.8032965064048767
MemoryTrain:  epoch  8, batch     1 | loss: 2.9613974Losses:  2.910165786743164 0.22175690531730652 0.7619941234588623
MemoryTrain:  epoch  8, batch     2 | loss: 2.9101658Losses:  3.8491547107696533 1.2701209783554077 0.7008054852485657
MemoryTrain:  epoch  8, batch     3 | loss: 3.8491547Losses:  3.377445936203003 0.7215670347213745 0.786281168460846
MemoryTrain:  epoch  8, batch     4 | loss: 3.3774459Losses:  3.6622490882873535 1.0817952156066895 0.7086330652236938
MemoryTrain:  epoch  8, batch     5 | loss: 3.6622491Losses:  3.595914363861084 0.9925958514213562 0.6629979610443115
MemoryTrain:  epoch  8, batch     6 | loss: 3.5959144Losses:  3.253556728363037 0.48402857780456543 0.9107427000999451
MemoryTrain:  epoch  8, batch     7 | loss: 3.2535567Losses:  3.136111259460449 0.484732449054718 0.77106773853302
MemoryTrain:  epoch  8, batch     8 | loss: 3.1361113Losses:  3.0959126949310303 0.49124839901924133 0.7112196087837219
MemoryTrain:  epoch  8, batch     9 | loss: 3.0959127Losses:  3.109083414077759 0.4776250422000885 0.717950165271759
MemoryTrain:  epoch  8, batch    10 | loss: 3.1090834Losses:  3.023139476776123 0.4789307117462158 0.6362735033035278
MemoryTrain:  epoch  8, batch    11 | loss: 3.0231395Losses:  3.3490753173828125 0.7318065166473389 0.7436486482620239
MemoryTrain:  epoch  8, batch    12 | loss: 3.3490753Losses:  2.871633529663086 0.22631287574768066 0.7786502838134766
MemoryTrain:  epoch  8, batch    13 | loss: 2.8716335Losses:  3.020160436630249 0.26406294107437134 0.8414891362190247
MemoryTrain:  epoch  8, batch    14 | loss: 3.0201604Losses:  2.157743453979492 -0.0 0.29880812764167786
MemoryTrain:  epoch  8, batch    15 | loss: 2.1577435Losses:  2.857368230819702 0.21392199397087097 0.7505132555961609
MemoryTrain:  epoch  9, batch     0 | loss: 2.8573682Losses:  2.9621222019195557 0.23473839461803436 0.8239526748657227
MemoryTrain:  epoch  9, batch     1 | loss: 2.9621222Losses:  3.60684871673584 1.1007059812545776 0.6442668437957764
MemoryTrain:  epoch  9, batch     2 | loss: 3.6068487Losses:  3.445197820663452 0.7123780846595764 0.8307086825370789
MemoryTrain:  epoch  9, batch     3 | loss: 3.4451978Losses:  3.0225746631622314 0.46364113688468933 0.7042929530143738
MemoryTrain:  epoch  9, batch     4 | loss: 3.0225747Losses:  3.06826114654541 0.46021223068237305 0.7493833303451538
MemoryTrain:  epoch  9, batch     5 | loss: 3.0682611Losses:  3.188807487487793 0.7159546613693237 0.5557622909545898
MemoryTrain:  epoch  9, batch     6 | loss: 3.1888075Losses:  3.141362428665161 0.46728044748306274 0.7711470723152161
MemoryTrain:  epoch  9, batch     7 | loss: 3.1413624Losses:  3.0917775630950928 0.4718077480792999 0.7553786635398865
MemoryTrain:  epoch  9, batch     8 | loss: 3.0917776Losses:  3.5225517749786377 0.9886870980262756 0.6516373753547668
MemoryTrain:  epoch  9, batch     9 | loss: 3.5225518Losses:  3.2008938789367676 0.6889433860778809 0.6402589678764343
MemoryTrain:  epoch  9, batch    10 | loss: 3.2008939Losses:  3.2056937217712402 0.46358686685562134 0.8230836391448975
MemoryTrain:  epoch  9, batch    11 | loss: 3.2056937Losses:  2.859154462814331 0.22902610898017883 0.7698479294776917
MemoryTrain:  epoch  9, batch    12 | loss: 2.8591545Losses:  3.1962056159973145 0.42939117550849915 0.8792047500610352
MemoryTrain:  epoch  9, batch    13 | loss: 3.1962056Losses:  3.2162108421325684 0.7098984122276306 0.6383134126663208
MemoryTrain:  epoch  9, batch    14 | loss: 3.2162108Losses:  2.29774808883667 -0.0 0.3489334285259247
MemoryTrain:  epoch  9, batch    15 | loss: 2.2977481
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 68.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 77.43%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 65.28%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 63.28%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.97%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 63.89%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 63.82%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 64.38%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 70.91%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 71.76%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 72.77%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 75.95%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 74.45%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 73.04%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 72.05%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 70.95%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 69.41%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 68.59%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 68.59%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 69.05%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 69.49%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 69.91%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 69.74%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 70.14%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 69.16%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 69.15%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 68.88%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 68.00%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 67.28%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 66.95%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 66.51%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 66.32%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 66.14%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 66.18%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 66.23%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 65.52%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 65.25%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 64.79%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 64.04%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 63.10%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 62.20%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 61.82%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 61.92%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 61.65%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 61.01%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 60.29%   [EVAL] batch:   68 | acc: 6.25%,  total acc: 59.51%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 59.15%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 58.85%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 58.73%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 58.61%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 58.50%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 58.22%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 58.28%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 58.25%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 57.99%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 57.34%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 56.71%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 56.17%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 55.50%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 54.91%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 54.34%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 53.85%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 53.23%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 53.55%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 54.07%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 54.58%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 55.08%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 55.57%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 56.05%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 56.52%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 56.97%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 57.23%   [EVAL] batch:   96 | acc: 12.50%,  total acc: 56.77%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 56.38%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 56.63%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 56.94%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 57.18%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 57.11%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 57.16%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 57.15%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 57.32%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 57.55%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 57.89%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 58.28%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 58.60%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 58.86%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 58.90%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 58.48%   [EVAL] batch:  112 | acc: 0.00%,  total acc: 57.96%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 57.73%   [EVAL] batch:  114 | acc: 18.75%,  total acc: 57.39%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 57.33%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 57.32%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 57.42%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 57.56%   [EVAL] batch:  119 | acc: 68.75%,  total acc: 57.66%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 57.80%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 57.74%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 57.93%   [EVAL] batch:  123 | acc: 50.00%,  total acc: 57.86%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 58.05%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 58.38%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 58.66%   [EVAL] batch:  127 | acc: 93.75%,  total acc: 58.94%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 59.25%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 59.57%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 59.88%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 60.18%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 60.20%   
cur_acc:  ['0.8485', '0.8795', '0.3750', '0.6827', '0.2784', '0.7991', '0.7583', '0.7743']
his_acc:  ['0.8485', '0.8630', '0.7731', '0.7273', '0.5803', '0.6126', '0.6185', '0.6020']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 1 0]
Losses:  21.983240127563477 8.064126014709473 0.6441260576248169
CurrentTrain: epoch  0, batch     0 | loss: 21.9832401Losses:  23.301156997680664 9.549792289733887 0.6311047077178955
CurrentTrain: epoch  0, batch     1 | loss: 23.3011570Losses:  25.542905807495117 12.034369468688965 0.601656436920166
CurrentTrain: epoch  0, batch     2 | loss: 25.5429058Losses:  24.244396209716797 10.92621898651123 0.601303219795227
CurrentTrain: epoch  0, batch     3 | loss: 24.2443962Losses:  21.78629493713379 8.405982971191406 0.6132410764694214
CurrentTrain: epoch  0, batch     4 | loss: 21.7862949Losses:  25.1481990814209 11.939387321472168 0.5822017192840576
CurrentTrain: epoch  0, batch     5 | loss: 25.1481991Losses:  23.291868209838867 9.94660758972168 0.5888526439666748
CurrentTrain: epoch  0, batch     6 | loss: 23.2918682Losses:  25.46394157409668 12.364317893981934 0.5130481719970703
CurrentTrain: epoch  0, batch     7 | loss: 25.4639416Losses:  21.491609573364258 8.766085624694824 0.5337345600128174
CurrentTrain: epoch  0, batch     8 | loss: 21.4916096Losses:  21.80205535888672 8.952532768249512 0.5448713302612305
CurrentTrain: epoch  0, batch     9 | loss: 21.8020554Losses:  19.37525177001953 6.721527099609375 0.5201565027236938
CurrentTrain: epoch  0, batch    10 | loss: 19.3752518Losses:  20.976552963256836 8.48570442199707 0.5342394113540649
CurrentTrain: epoch  0, batch    11 | loss: 20.9765530Losses:  23.489389419555664 11.011431694030762 0.5267850160598755
CurrentTrain: epoch  0, batch    12 | loss: 23.4893894Losses:  23.145898818969727 10.845605850219727 0.4504944384098053
CurrentTrain: epoch  0, batch    13 | loss: 23.1458988Losses:  20.449079513549805 8.108516693115234 0.4532100558280945
CurrentTrain: epoch  0, batch    14 | loss: 20.4490795Losses:  19.521873474121094 7.481032371520996 0.47045043110847473
CurrentTrain: epoch  0, batch    15 | loss: 19.5218735Losses:  18.947677612304688 6.972293853759766 0.45940858125686646
CurrentTrain: epoch  0, batch    16 | loss: 18.9476776Losses:  20.790437698364258 8.898096084594727 0.43049129843711853
CurrentTrain: epoch  0, batch    17 | loss: 20.7904377Losses:  24.174558639526367 12.297786712646484 0.4548092484474182
CurrentTrain: epoch  0, batch    18 | loss: 24.1745586Losses:  20.601356506347656 9.063933372497559 0.40156859159469604
CurrentTrain: epoch  0, batch    19 | loss: 20.6013565Losses:  21.124008178710938 9.418834686279297 0.39099985361099243
CurrentTrain: epoch  0, batch    20 | loss: 21.1240082Losses:  24.48261070251465 13.081814765930176 0.34361374378204346
CurrentTrain: epoch  0, batch    21 | loss: 24.4826107Losses:  21.47681999206543 9.85167121887207 0.30719658732414246
CurrentTrain: epoch  0, batch    22 | loss: 21.4768200Losses:  21.07490348815918 8.753364562988281 0.35331571102142334
CurrentTrain: epoch  0, batch    23 | loss: 21.0749035Losses:  18.502544403076172 6.953801155090332 0.3326568901538849
CurrentTrain: epoch  0, batch    24 | loss: 18.5025444Losses:  18.08490562438965 6.700458526611328 0.32154595851898193
CurrentTrain: epoch  0, batch    25 | loss: 18.0849056Losses:  26.01982879638672 15.66136360168457 0.236769899725914
CurrentTrain: epoch  0, batch    26 | loss: 26.0198288Losses:  22.721435546875 11.147586822509766 0.3311634659767151
CurrentTrain: epoch  0, batch    27 | loss: 22.7214355Losses:  18.640188217163086 7.640296459197998 0.31465983390808105
CurrentTrain: epoch  0, batch    28 | loss: 18.6401882Losses:  18.08145523071289 6.961367607116699 0.3104587495326996
CurrentTrain: epoch  0, batch    29 | loss: 18.0814552Losses:  22.676332473754883 11.339714050292969 0.2550482451915741
CurrentTrain: epoch  0, batch    30 | loss: 22.6763325Losses:  19.493736267089844 8.920258522033691 0.2967095375061035
CurrentTrain: epoch  0, batch    31 | loss: 19.4937363Losses:  21.89586639404297 10.66482925415039 0.25346118211746216
CurrentTrain: epoch  0, batch    32 | loss: 21.8958664Losses:  18.037527084350586 7.501145839691162 0.23418593406677246
CurrentTrain: epoch  0, batch    33 | loss: 18.0375271Losses:  19.919384002685547 9.480657577514648 0.2052811086177826
CurrentTrain: epoch  0, batch    34 | loss: 19.9193840Losses:  23.31434440612793 12.665576934814453 0.2407238781452179
CurrentTrain: epoch  0, batch    35 | loss: 23.3143444Losses:  19.564617156982422 9.080526351928711 0.3215370774269104
CurrentTrain: epoch  0, batch    36 | loss: 19.5646172Losses:  12.6334867477417 3.4042229652404785 0.2379772663116455
CurrentTrain: epoch  0, batch    37 | loss: 12.6334867Losses:  21.236156463623047 10.281047821044922 0.22818472981452942
CurrentTrain: epoch  1, batch     0 | loss: 21.2361565Losses:  16.249855041503906 6.191751480102539 0.22951820492744446
CurrentTrain: epoch  1, batch     1 | loss: 16.2498550Losses:  17.705333709716797 7.582803249359131 0.2253555804491043
CurrentTrain: epoch  1, batch     2 | loss: 17.7053337Losses:  20.55965805053711 10.437522888183594 0.20307114720344543
CurrentTrain: epoch  1, batch     3 | loss: 20.5596581Losses:  22.742321014404297 12.089523315429688 0.1993224322795868
CurrentTrain: epoch  1, batch     4 | loss: 22.7423210Losses:  19.08481216430664 8.561458587646484 0.22291699051856995
CurrentTrain: epoch  1, batch     5 | loss: 19.0848122Losses:  18.197940826416016 9.209615707397461 0.18067237734794617
CurrentTrain: epoch  1, batch     6 | loss: 18.1979408Losses:  16.628982543945312 7.249279975891113 0.18774014711380005
CurrentTrain: epoch  1, batch     7 | loss: 16.6289825Losses:  19.540910720825195 9.176017761230469 0.19400659203529358
CurrentTrain: epoch  1, batch     8 | loss: 19.5409107Losses:  15.230923652648926 6.113701820373535 0.16311606764793396
CurrentTrain: epoch  1, batch     9 | loss: 15.2309237Losses:  17.211896896362305 6.447182655334473 0.19860276579856873
CurrentTrain: epoch  1, batch    10 | loss: 17.2118969Losses:  19.795278549194336 9.984026908874512 0.16353103518486023
CurrentTrain: epoch  1, batch    11 | loss: 19.7952785Losses:  17.487346649169922 8.215641021728516 0.16765019297599792
CurrentTrain: epoch  1, batch    12 | loss: 17.4873466Losses:  19.634708404541016 9.873046875 0.16542771458625793
CurrentTrain: epoch  1, batch    13 | loss: 19.6347084Losses:  16.886180877685547 6.84886360168457 0.17140670120716095
CurrentTrain: epoch  1, batch    14 | loss: 16.8861809Losses:  15.274496078491211 6.345686912536621 0.16448017954826355
CurrentTrain: epoch  1, batch    15 | loss: 15.2744961Losses:  16.32863998413086 6.681478500366211 0.16936419904232025
CurrentTrain: epoch  1, batch    16 | loss: 16.3286400Losses:  15.674054145812988 6.103376388549805 0.16909581422805786
CurrentTrain: epoch  1, batch    17 | loss: 15.6740541Losses:  16.01823616027832 6.473341941833496 0.15534865856170654
CurrentTrain: epoch  1, batch    18 | loss: 16.0182362Losses:  17.72710609436035 8.675373077392578 0.1712394654750824
CurrentTrain: epoch  1, batch    19 | loss: 17.7271061Losses:  18.694135665893555 8.8675537109375 0.14053094387054443
CurrentTrain: epoch  1, batch    20 | loss: 18.6941357Losses:  16.813016891479492 7.990078926086426 0.12974828481674194
CurrentTrain: epoch  1, batch    21 | loss: 16.8130169Losses:  20.533174514770508 10.525681495666504 0.1416648030281067
CurrentTrain: epoch  1, batch    22 | loss: 20.5331745Losses:  16.612756729125977 7.292789459228516 0.13585412502288818
CurrentTrain: epoch  1, batch    23 | loss: 16.6127567Losses:  18.277517318725586 8.918511390686035 0.1203591376543045
CurrentTrain: epoch  1, batch    24 | loss: 18.2775173Losses:  17.23542594909668 7.5010175704956055 0.14636802673339844
CurrentTrain: epoch  1, batch    25 | loss: 17.2354259Losses:  16.352367401123047 7.622574806213379 0.1572277843952179
CurrentTrain: epoch  1, batch    26 | loss: 16.3523674Losses:  17.009939193725586 6.865938186645508 0.14686426520347595
CurrentTrain: epoch  1, batch    27 | loss: 17.0099392Losses:  15.572760581970215 6.484782695770264 0.10315227508544922
CurrentTrain: epoch  1, batch    28 | loss: 15.5727606Losses:  21.965803146362305 12.077266693115234 0.11024259030818939
CurrentTrain: epoch  1, batch    29 | loss: 21.9658031Losses:  15.584749221801758 6.52692985534668 0.11748528480529785
CurrentTrain: epoch  1, batch    30 | loss: 15.5847492Losses:  20.810998916625977 11.688276290893555 0.12687334418296814
CurrentTrain: epoch  1, batch    31 | loss: 20.8109989Losses:  18.818326950073242 9.482675552368164 0.14630073308944702
CurrentTrain: epoch  1, batch    32 | loss: 18.8183270Losses:  15.347352027893066 6.774790287017822 0.1061505377292633
CurrentTrain: epoch  1, batch    33 | loss: 15.3473520Losses:  17.037466049194336 7.678136825561523 0.10942362248897552
CurrentTrain: epoch  1, batch    34 | loss: 17.0374660Losses:  17.332239151000977 8.370316505432129 0.09503771364688873
CurrentTrain: epoch  1, batch    35 | loss: 17.3322392Losses:  15.261911392211914 6.342768669128418 0.10546800494194031
CurrentTrain: epoch  1, batch    36 | loss: 15.2619114Losses:  13.064769744873047 3.6374425888061523 0.09370505809783936
CurrentTrain: epoch  1, batch    37 | loss: 13.0647697Losses:  13.916990280151367 5.361978530883789 0.08967363089323044
CurrentTrain: epoch  2, batch     0 | loss: 13.9169903Losses:  15.63664722442627 6.992987632751465 0.08182582259178162
CurrentTrain: epoch  2, batch     1 | loss: 15.6366472Losses:  14.343518257141113 5.608401298522949 0.10607688874006271
CurrentTrain: epoch  2, batch     2 | loss: 14.3435183Losses:  14.338422775268555 6.141366004943848 0.07889106869697571
CurrentTrain: epoch  2, batch     3 | loss: 14.3384228Losses:  17.988210678100586 8.622806549072266 0.0917079895734787
CurrentTrain: epoch  2, batch     4 | loss: 17.9882107Losses:  16.88503646850586 7.787230491638184 0.10227540135383606
CurrentTrain: epoch  2, batch     5 | loss: 16.8850365Losses:  15.50273323059082 7.124567985534668 0.08344431966543198
CurrentTrain: epoch  2, batch     6 | loss: 15.5027332Losses:  14.82923698425293 6.384936332702637 0.09856872260570526
CurrentTrain: epoch  2, batch     7 | loss: 14.8292370Losses:  16.263622283935547 7.971634387969971 0.08866719156503677
CurrentTrain: epoch  2, batch     8 | loss: 16.2636223Losses:  13.9367094039917 5.979628562927246 0.08841504156589508
CurrentTrain: epoch  2, batch     9 | loss: 13.9367094Losses:  17.563945770263672 9.030449867248535 0.09952203184366226
CurrentTrain: epoch  2, batch    10 | loss: 17.5639458Losses:  16.388879776000977 8.504374504089355 0.07307520508766174
CurrentTrain: epoch  2, batch    11 | loss: 16.3888798Losses:  15.53650188446045 6.557467937469482 0.07625247538089752
CurrentTrain: epoch  2, batch    12 | loss: 15.5365019Losses:  14.009964942932129 5.624519348144531 0.08861482888460159
CurrentTrain: epoch  2, batch    13 | loss: 14.0099649Losses:  13.67320728302002 5.659711837768555 0.09027204662561417
CurrentTrain: epoch  2, batch    14 | loss: 13.6732073Losses:  14.700348854064941 6.031856536865234 0.07829958200454712
CurrentTrain: epoch  2, batch    15 | loss: 14.7003489Losses:  20.70808219909668 11.50850772857666 0.07397673279047012
CurrentTrain: epoch  2, batch    16 | loss: 20.7080822Losses:  17.077970504760742 7.826116561889648 0.08631612360477448
CurrentTrain: epoch  2, batch    17 | loss: 17.0779705Losses:  16.860780715942383 7.893748760223389 0.06654084473848343
CurrentTrain: epoch  2, batch    18 | loss: 16.8607807Losses:  13.60774040222168 5.1416096687316895 0.06806910037994385
CurrentTrain: epoch  2, batch    19 | loss: 13.6077404Losses:  16.43993377685547 7.207790851593018 0.0667669028043747
CurrentTrain: epoch  2, batch    20 | loss: 16.4399338Losses:  13.596513748168945 5.403927326202393 0.06753513216972351
CurrentTrain: epoch  2, batch    21 | loss: 13.5965137Losses:  20.27021598815918 11.023736000061035 0.06809762120246887
CurrentTrain: epoch  2, batch    22 | loss: 20.2702160Losses:  14.817373275756836 7.109472274780273 0.08852869272232056
CurrentTrain: epoch  2, batch    23 | loss: 14.8173733Losses:  14.753579139709473 6.4607391357421875 0.0763229951262474
CurrentTrain: epoch  2, batch    24 | loss: 14.7535791Losses:  14.98429012298584 5.800985336303711 0.06923352181911469
CurrentTrain: epoch  2, batch    25 | loss: 14.9842901Losses:  17.02952003479004 7.8560614585876465 0.06635172665119171
CurrentTrain: epoch  2, batch    26 | loss: 17.0295200Losses:  15.01464557647705 7.543748378753662 0.0877196416258812
CurrentTrain: epoch  2, batch    27 | loss: 15.0146456Losses:  14.819464683532715 6.5204877853393555 0.0714857280254364
CurrentTrain: epoch  2, batch    28 | loss: 14.8194647Losses:  16.124874114990234 7.467944622039795 0.07497792690992355
CurrentTrain: epoch  2, batch    29 | loss: 16.1248741Losses:  15.449312210083008 7.96240234375 0.08764947950839996
CurrentTrain: epoch  2, batch    30 | loss: 15.4493122Losses:  16.6449031829834 9.281843185424805 0.0804954543709755
CurrentTrain: epoch  2, batch    31 | loss: 16.6449032Losses:  20.81086540222168 12.235990524291992 0.09419166296720505
CurrentTrain: epoch  2, batch    32 | loss: 20.8108654Losses:  13.322301864624023 6.057684898376465 0.09553748369216919
CurrentTrain: epoch  2, batch    33 | loss: 13.3223019Losses:  13.885174751281738 6.755566596984863 0.08727952837944031
CurrentTrain: epoch  2, batch    34 | loss: 13.8851748Losses:  16.578960418701172 8.145490646362305 0.07810009270906448
CurrentTrain: epoch  2, batch    35 | loss: 16.5789604Losses:  15.509818077087402 8.049748420715332 0.07715709507465363
CurrentTrain: epoch  2, batch    36 | loss: 15.5098181Losses:  10.58865737915039 1.9868237972259521 0.08532468974590302
CurrentTrain: epoch  2, batch    37 | loss: 10.5886574Losses:  16.33057975769043 7.060914993286133 0.06423834711313248
CurrentTrain: epoch  3, batch     0 | loss: 16.3305798Losses:  13.524529457092285 5.865793228149414 0.06906521320343018
CurrentTrain: epoch  3, batch     1 | loss: 13.5245295Losses:  13.050897598266602 4.889065742492676 0.06218037009239197
CurrentTrain: epoch  3, batch     2 | loss: 13.0508976Losses:  15.126107215881348 6.3715105056762695 0.08625979721546173
CurrentTrain: epoch  3, batch     3 | loss: 15.1261072Losses:  14.535409927368164 6.444209098815918 0.07352349907159805
CurrentTrain: epoch  3, batch     4 | loss: 14.5354099Losses:  14.21969223022461 6.183701515197754 0.06217549368739128
CurrentTrain: epoch  3, batch     5 | loss: 14.2196922Losses:  16.29659652709961 7.023597717285156 0.06095509976148605
CurrentTrain: epoch  3, batch     6 | loss: 16.2965965Losses:  12.82518482208252 4.646066665649414 0.055776018649339676
CurrentTrain: epoch  3, batch     7 | loss: 12.8251848Losses:  16.605751037597656 10.134132385253906 0.09388816356658936
CurrentTrain: epoch  3, batch     8 | loss: 16.6057510Losses:  15.338430404663086 7.7364821434021 0.0728706419467926
CurrentTrain: epoch  3, batch     9 | loss: 15.3384304Losses:  13.736649513244629 5.862052917480469 0.0570320226252079
CurrentTrain: epoch  3, batch    10 | loss: 13.7366495Losses:  21.819379806518555 12.71790885925293 0.06485949456691742
CurrentTrain: epoch  3, batch    11 | loss: 21.8193798Losses:  14.311017990112305 6.554913520812988 0.07024769484996796
CurrentTrain: epoch  3, batch    12 | loss: 14.3110180Losses:  12.02934455871582 4.546844959259033 0.04603002220392227
CurrentTrain: epoch  3, batch    13 | loss: 12.0293446Losses:  14.289487838745117 5.736931324005127 0.04892731457948685
CurrentTrain: epoch  3, batch    14 | loss: 14.2894878Losses:  13.490750312805176 5.510922431945801 0.05065661296248436
CurrentTrain: epoch  3, batch    15 | loss: 13.4907503Losses:  13.72290325164795 5.768662452697754 0.05069531127810478
CurrentTrain: epoch  3, batch    16 | loss: 13.7229033Losses:  15.883673667907715 7.394350528717041 0.05695601552724838
CurrentTrain: epoch  3, batch    17 | loss: 15.8836737Losses:  12.710846900939941 5.014957427978516 0.049159541726112366
CurrentTrain: epoch  3, batch    18 | loss: 12.7108469Losses:  13.180901527404785 5.42556095123291 0.04991856589913368
CurrentTrain: epoch  3, batch    19 | loss: 13.1809015Losses:  14.084181785583496 6.056612014770508 0.05198656767606735
CurrentTrain: epoch  3, batch    20 | loss: 14.0841818Losses:  15.055191993713379 6.884383201599121 0.0519581064581871
CurrentTrain: epoch  3, batch    21 | loss: 15.0551920Losses:  14.275616645812988 5.985307693481445 0.06796374917030334
CurrentTrain: epoch  3, batch    22 | loss: 14.2756166Losses:  12.383214950561523 4.843874931335449 0.0503910668194294
CurrentTrain: epoch  3, batch    23 | loss: 12.3832150Losses:  12.768198013305664 5.7742156982421875 0.05984353646636009
CurrentTrain: epoch  3, batch    24 | loss: 12.7681980Losses:  14.199857711791992 6.33055305480957 0.047994792461395264
CurrentTrain: epoch  3, batch    25 | loss: 14.1998577Losses:  14.823202133178711 6.769513130187988 0.05276459455490112
CurrentTrain: epoch  3, batch    26 | loss: 14.8232021Losses:  13.728296279907227 6.678784370422363 0.05173909291625023
CurrentTrain: epoch  3, batch    27 | loss: 13.7282963Losses:  15.99505615234375 9.247302055358887 0.04918615520000458
CurrentTrain: epoch  3, batch    28 | loss: 15.9950562Losses:  16.0716495513916 8.364700317382812 0.054228901863098145
CurrentTrain: epoch  3, batch    29 | loss: 16.0716496Losses:  13.508663177490234 5.387790679931641 0.059829726815223694
CurrentTrain: epoch  3, batch    30 | loss: 13.5086632Losses:  14.413162231445312 8.10987377166748 0.08698409050703049
CurrentTrain: epoch  3, batch    31 | loss: 14.4131622Losses:  13.383459091186523 5.956687927246094 0.04561286047101021
CurrentTrain: epoch  3, batch    32 | loss: 13.3834591Losses:  12.118508338928223 5.611649990081787 0.06012892350554466
CurrentTrain: epoch  3, batch    33 | loss: 12.1185083Losses:  11.78028392791748 5.201764106750488 0.05655515938997269
CurrentTrain: epoch  3, batch    34 | loss: 11.7802839Losses:  12.520257949829102 5.759763717651367 0.057469889521598816
CurrentTrain: epoch  3, batch    35 | loss: 12.5202579Losses:  13.81985855102539 5.349756717681885 0.06265993416309357
CurrentTrain: epoch  3, batch    36 | loss: 13.8198586Losses:  9.233973503112793 1.6814584732055664 0.050276972353458405
CurrentTrain: epoch  3, batch    37 | loss: 9.2339735Losses:  11.596176147460938 4.423853874206543 0.04538219794631004
CurrentTrain: epoch  4, batch     0 | loss: 11.5961761Losses:  12.902790069580078 6.063739776611328 0.055516283959150314
CurrentTrain: epoch  4, batch     1 | loss: 12.9027901Losses:  13.892485618591309 6.237800121307373 0.058272622525691986
CurrentTrain: epoch  4, batch     2 | loss: 13.8924856Losses:  18.17629051208496 8.711291313171387 0.04414847865700722
CurrentTrain: epoch  4, batch     3 | loss: 18.1762905Losses:  11.97826862335205 5.392796516418457 0.0499565526843071
CurrentTrain: epoch  4, batch     4 | loss: 11.9782686Losses:  13.733567237854004 7.594699859619141 0.06758037209510803
CurrentTrain: epoch  4, batch     5 | loss: 13.7335672Losses:  15.304730415344238 6.578606605529785 0.049476414918899536
CurrentTrain: epoch  4, batch     6 | loss: 15.3047304Losses:  12.56234359741211 5.316932678222656 0.050474148243665695
CurrentTrain: epoch  4, batch     7 | loss: 12.5623436Losses:  16.58784294128418 8.899269104003906 0.05911095067858696
CurrentTrain: epoch  4, batch     8 | loss: 16.5878429Losses:  14.318465232849121 6.550196647644043 0.046877309679985046
CurrentTrain: epoch  4, batch     9 | loss: 14.3184652Losses:  12.904725074768066 5.015645980834961 0.04541301354765892
CurrentTrain: epoch  4, batch    10 | loss: 12.9047251Losses:  12.644325256347656 5.3343939781188965 0.050806522369384766
CurrentTrain: epoch  4, batch    11 | loss: 12.6443253Losses:  12.41912841796875 5.595113754272461 0.06151137128472328
CurrentTrain: epoch  4, batch    12 | loss: 12.4191284Losses:  11.961772918701172 4.585855960845947 0.04354103282094002
CurrentTrain: epoch  4, batch    13 | loss: 11.9617729Losses:  11.636767387390137 4.697394371032715 0.04459458589553833
CurrentTrain: epoch  4, batch    14 | loss: 11.6367674Losses:  13.229305267333984 6.383785247802734 0.045204125344753265
CurrentTrain: epoch  4, batch    15 | loss: 13.2293053Losses:  11.974309921264648 5.513121604919434 0.04526257514953613
CurrentTrain: epoch  4, batch    16 | loss: 11.9743099Losses:  12.948446273803711 6.413076400756836 0.056039631366729736
CurrentTrain: epoch  4, batch    17 | loss: 12.9484463Losses:  12.379734992980957 4.939534664154053 0.047909967601299286
CurrentTrain: epoch  4, batch    18 | loss: 12.3797350Losses:  13.07385540008545 5.978968620300293 0.056710384786129
CurrentTrain: epoch  4, batch    19 | loss: 13.0738554Losses:  11.835590362548828 5.16880989074707 0.040175843983888626
CurrentTrain: epoch  4, batch    20 | loss: 11.8355904Losses:  16.3044376373291 9.593779563903809 0.05462329834699631
CurrentTrain: epoch  4, batch    21 | loss: 16.3044376Losses:  11.826114654541016 5.078103065490723 0.045854344964027405
CurrentTrain: epoch  4, batch    22 | loss: 11.8261147Losses:  18.17054557800293 10.3472900390625 0.04591212794184685
CurrentTrain: epoch  4, batch    23 | loss: 18.1705456Losses:  12.961959838867188 5.421343803405762 0.045801449567079544
CurrentTrain: epoch  4, batch    24 | loss: 12.9619598Losses:  13.151239395141602 6.433032035827637 0.050203848630189896
CurrentTrain: epoch  4, batch    25 | loss: 13.1512394Losses:  13.256369590759277 5.944034576416016 0.038704391568899155
CurrentTrain: epoch  4, batch    26 | loss: 13.2563696Losses:  13.575867652893066 7.027965545654297 0.04259638860821724
CurrentTrain: epoch  4, batch    27 | loss: 13.5758677Losses:  12.311304092407227 5.803833484649658 0.05631213262677193
CurrentTrain: epoch  4, batch    28 | loss: 12.3113041Losses:  14.119380950927734 6.400477409362793 0.04263448715209961
CurrentTrain: epoch  4, batch    29 | loss: 14.1193810Losses:  13.015063285827637 6.066619873046875 0.04312209039926529
CurrentTrain: epoch  4, batch    30 | loss: 13.0150633Losses:  12.335333824157715 4.755974292755127 0.03940205276012421
CurrentTrain: epoch  4, batch    31 | loss: 12.3353338Losses:  13.513374328613281 6.998073577880859 0.056232184171676636
CurrentTrain: epoch  4, batch    32 | loss: 13.5133743Losses:  12.0051908493042 5.230105400085449 0.040104955434799194
CurrentTrain: epoch  4, batch    33 | loss: 12.0051908Losses:  12.261460304260254 4.198245048522949 0.03997232764959335
CurrentTrain: epoch  4, batch    34 | loss: 12.2614603Losses:  15.992856979370117 7.9239397048950195 0.0671238973736763
CurrentTrain: epoch  4, batch    35 | loss: 15.9928570Losses:  13.453204154968262 5.946908950805664 0.05238484963774681
CurrentTrain: epoch  4, batch    36 | loss: 13.4532042Losses:  8.649031639099121 0.7974684238433838 0.04477088898420334
CurrentTrain: epoch  4, batch    37 | loss: 8.6490316Losses:  11.956232070922852 5.5016703605651855 0.05033841356635094
CurrentTrain: epoch  5, batch     0 | loss: 11.9562321Losses:  13.397192001342773 6.237936973571777 0.05000962316989899
CurrentTrain: epoch  5, batch     1 | loss: 13.3971920Losses:  15.131784439086914 7.214600563049316 0.04773484170436859
CurrentTrain: epoch  5, batch     2 | loss: 15.1317844Losses:  15.313790321350098 7.577685356140137 0.05352393537759781
CurrentTrain: epoch  5, batch     3 | loss: 15.3137903Losses:  13.36482048034668 6.139824867248535 0.04707305505871773
CurrentTrain: epoch  5, batch     4 | loss: 13.3648205Losses:  12.345967292785645 5.408907413482666 0.03898738697171211
CurrentTrain: epoch  5, batch     5 | loss: 12.3459673Losses:  14.519364356994629 6.94149923324585 0.04269697144627571
CurrentTrain: epoch  5, batch     6 | loss: 14.5193644Losses:  12.525574684143066 5.195317268371582 0.045612234622240067
CurrentTrain: epoch  5, batch     7 | loss: 12.5255747Losses:  12.54297924041748 5.711472034454346 0.04835405945777893
CurrentTrain: epoch  5, batch     8 | loss: 12.5429792Losses:  12.668573379516602 5.3372907638549805 0.0406358577311039
CurrentTrain: epoch  5, batch     9 | loss: 12.6685734Losses:  10.746440887451172 4.309744358062744 0.04454590380191803
CurrentTrain: epoch  5, batch    10 | loss: 10.7464409Losses:  11.391641616821289 5.46645450592041 0.049691446125507355
CurrentTrain: epoch  5, batch    11 | loss: 11.3916416Losses:  17.504392623901367 12.079645156860352 0.0542299710214138
CurrentTrain: epoch  5, batch    12 | loss: 17.5043926Losses:  14.104164123535156 7.142925262451172 0.04108650982379913
CurrentTrain: epoch  5, batch    13 | loss: 14.1041641Losses:  11.827558517456055 4.223880290985107 0.038312554359436035
CurrentTrain: epoch  5, batch    14 | loss: 11.8275585Losses:  16.013185501098633 8.328638076782227 0.07015345990657806
CurrentTrain: epoch  5, batch    15 | loss: 16.0131855Losses:  13.824478149414062 7.193943023681641 0.060703493654727936
CurrentTrain: epoch  5, batch    16 | loss: 13.8244781Losses:  13.414892196655273 6.477306842803955 0.047603510320186615
CurrentTrain: epoch  5, batch    17 | loss: 13.4148922Losses:  14.439001083374023 6.538129806518555 0.04634823277592659
CurrentTrain: epoch  5, batch    18 | loss: 14.4390011Losses:  15.94835376739502 8.902777671813965 0.05574242025613785
CurrentTrain: epoch  5, batch    19 | loss: 15.9483538Losses:  11.726824760437012 5.105898857116699 0.04831555485725403
CurrentTrain: epoch  5, batch    20 | loss: 11.7268248Losses:  12.63609504699707 5.645687580108643 0.03694508969783783
CurrentTrain: epoch  5, batch    21 | loss: 12.6360950Losses:  13.889509201049805 6.803145408630371 0.061557721346616745
CurrentTrain: epoch  5, batch    22 | loss: 13.8895092Losses:  14.220881462097168 8.103021621704102 0.05538884177803993
CurrentTrain: epoch  5, batch    23 | loss: 14.2208815Losses:  11.474203109741211 5.285357475280762 0.03787672147154808
CurrentTrain: epoch  5, batch    24 | loss: 11.4742031Losses:  13.506133079528809 6.879876136779785 0.04796706512570381
CurrentTrain: epoch  5, batch    25 | loss: 13.5061331Losses:  13.057321548461914 5.660210132598877 0.034853022545576096
CurrentTrain: epoch  5, batch    26 | loss: 13.0573215Losses:  12.634413719177246 5.445059776306152 0.044339172542095184
CurrentTrain: epoch  5, batch    27 | loss: 12.6344137Losses:  12.188064575195312 5.6048102378845215 0.04267178475856781
CurrentTrain: epoch  5, batch    28 | loss: 12.1880646Losses:  12.520833015441895 5.910430431365967 0.042447641491889954
CurrentTrain: epoch  5, batch    29 | loss: 12.5208330Losses:  15.104043006896973 8.446492195129395 0.0490180104970932
CurrentTrain: epoch  5, batch    30 | loss: 15.1040430Losses:  12.228731155395508 5.849510192871094 0.04244067147374153
CurrentTrain: epoch  5, batch    31 | loss: 12.2287312Losses:  13.381133079528809 7.593604564666748 0.05238094553351402
CurrentTrain: epoch  5, batch    32 | loss: 13.3811331Losses:  14.96776008605957 7.327597141265869 0.057418957352638245
CurrentTrain: epoch  5, batch    33 | loss: 14.9677601Losses:  15.185928344726562 7.7902398109436035 0.060979604721069336
CurrentTrain: epoch  5, batch    34 | loss: 15.1859283Losses:  17.1107120513916 9.966773986816406 0.05562791973352432
CurrentTrain: epoch  5, batch    35 | loss: 17.1107121Losses:  15.209113121032715 8.126506805419922 0.048848360776901245
CurrentTrain: epoch  5, batch    36 | loss: 15.2091131Losses:  7.531753063201904 1.171374797821045 0.052117496728897095
CurrentTrain: epoch  5, batch    37 | loss: 7.5317531Losses:  11.018564224243164 4.989047050476074 0.03973750025033951
CurrentTrain: epoch  6, batch     0 | loss: 11.0185642Losses:  9.919928550720215 4.045901298522949 0.03808146342635155
CurrentTrain: epoch  6, batch     1 | loss: 9.9199286Losses:  12.181488990783691 6.6208696365356445 0.05726080387830734
CurrentTrain: epoch  6, batch     2 | loss: 12.1814890Losses:  14.143441200256348 6.857513427734375 0.05178946256637573
CurrentTrain: epoch  6, batch     3 | loss: 14.1434412Losses:  10.340916633605957 4.758652687072754 0.04034828767180443
CurrentTrain: epoch  6, batch     4 | loss: 10.3409166Losses:  12.253226280212402 5.851032257080078 0.053520943969488144
CurrentTrain: epoch  6, batch     5 | loss: 12.2532263Losses:  12.475544929504395 6.873086929321289 0.054726965725421906
CurrentTrain: epoch  6, batch     6 | loss: 12.4755449Losses:  14.60374641418457 7.394776821136475 0.055658139288425446
CurrentTrain: epoch  6, batch     7 | loss: 14.6037464Losses:  14.019552230834961 6.542784690856934 0.04680667072534561
CurrentTrain: epoch  6, batch     8 | loss: 14.0195522Losses:  12.58458423614502 5.717545032501221 0.04423652961850166
CurrentTrain: epoch  6, batch     9 | loss: 12.5845842Losses:  13.486265182495117 6.278935432434082 0.05630683898925781
CurrentTrain: epoch  6, batch    10 | loss: 13.4862652Losses:  10.811927795410156 4.208259582519531 0.04421759769320488
CurrentTrain: epoch  6, batch    11 | loss: 10.8119278Losses:  17.409263610839844 10.847452163696289 0.05644221603870392
CurrentTrain: epoch  6, batch    12 | loss: 17.4092636Losses:  12.853229522705078 6.320064544677734 0.05819513276219368
CurrentTrain: epoch  6, batch    13 | loss: 12.8532295Losses:  12.476042747497559 6.288189888000488 0.05188504979014397
CurrentTrain: epoch  6, batch    14 | loss: 12.4760427Losses:  11.686442375183105 5.889638423919678 0.039881955832242966
CurrentTrain: epoch  6, batch    15 | loss: 11.6864424Losses:  9.982196807861328 4.295987129211426 0.03964532911777496
CurrentTrain: epoch  6, batch    16 | loss: 9.9821968Losses:  11.457104682922363 5.080228805541992 0.040617313235998154
CurrentTrain: epoch  6, batch    17 | loss: 11.4571047Losses:  10.501688003540039 5.016107559204102 0.044543735682964325
CurrentTrain: epoch  6, batch    18 | loss: 10.5016880Losses:  12.638934135437012 6.325846195220947 0.03887347877025604
CurrentTrain: epoch  6, batch    19 | loss: 12.6389341Losses:  10.467425346374512 4.810054779052734 0.03423206880688667
CurrentTrain: epoch  6, batch    20 | loss: 10.4674253Losses:  11.143681526184082 5.668923377990723 0.04340892285108566
CurrentTrain: epoch  6, batch    21 | loss: 11.1436815Losses:  11.809935569763184 5.392553329467773 0.040132325142621994
CurrentTrain: epoch  6, batch    22 | loss: 11.8099356Losses:  20.313936233520508 12.721842765808105 0.06324385106563568
CurrentTrain: epoch  6, batch    23 | loss: 20.3139362Losses:  12.317424774169922 5.9470930099487305 0.04395638406276703
CurrentTrain: epoch  6, batch    24 | loss: 12.3174248Losses:  16.895341873168945 10.345441818237305 0.06052683666348457
CurrentTrain: epoch  6, batch    25 | loss: 16.8953419Losses:  10.863001823425293 4.485700607299805 0.04167286306619644
CurrentTrain: epoch  6, batch    26 | loss: 10.8630018Losses:  10.494982719421387 4.425207138061523 0.03743863105773926
CurrentTrain: epoch  6, batch    27 | loss: 10.4949827Losses:  16.883033752441406 10.955763816833496 0.061818692833185196
CurrentTrain: epoch  6, batch    28 | loss: 16.8830338Losses:  14.098621368408203 6.745479106903076 0.06144926697015762
CurrentTrain: epoch  6, batch    29 | loss: 14.0986214Losses:  17.269758224487305 8.598068237304688 0.046729423105716705
CurrentTrain: epoch  6, batch    30 | loss: 17.2697582Losses:  9.943792343139648 4.0465264320373535 0.039236150681972504
CurrentTrain: epoch  6, batch    31 | loss: 9.9437923Losses:  11.211416244506836 5.078186988830566 0.04173397645354271
CurrentTrain: epoch  6, batch    32 | loss: 11.2114162Losses:  12.294246673583984 6.056035041809082 0.03727923333644867
CurrentTrain: epoch  6, batch    33 | loss: 12.2942467Losses:  15.539910316467285 9.40680980682373 0.05338296294212341
CurrentTrain: epoch  6, batch    34 | loss: 15.5399103Losses:  13.528463363647461 6.517361164093018 0.04024466499686241
CurrentTrain: epoch  6, batch    35 | loss: 13.5284634Losses:  24.369647979736328 15.983661651611328 0.050900787115097046
CurrentTrain: epoch  6, batch    36 | loss: 24.3696480Losses:  6.926656723022461 1.6366761922836304 0.06845779716968536
CurrentTrain: epoch  6, batch    37 | loss: 6.9266567Losses:  15.579874992370605 9.217180252075195 0.043828144669532776
CurrentTrain: epoch  7, batch     0 | loss: 15.5798750Losses:  11.597250938415527 5.274168968200684 0.03931466490030289
CurrentTrain: epoch  7, batch     1 | loss: 11.5972509Losses:  13.41009521484375 6.542588233947754 0.046566322445869446
CurrentTrain: epoch  7, batch     2 | loss: 13.4100952Losses:  10.655808448791504 4.613375663757324 0.03983917087316513
CurrentTrain: epoch  7, batch     3 | loss: 10.6558084Losses:  13.68134880065918 7.544589996337891 0.03626766800880432
CurrentTrain: epoch  7, batch     4 | loss: 13.6813488Losses:  11.116347312927246 4.744970798492432 0.045142367482185364
CurrentTrain: epoch  7, batch     5 | loss: 11.1163473Losses:  14.214503288269043 8.206326484680176 0.07057096064090729
CurrentTrain: epoch  7, batch     6 | loss: 14.2145033Losses:  12.314918518066406 5.424412727355957 0.045595433562994
CurrentTrain: epoch  7, batch     7 | loss: 12.3149185Losses:  12.030768394470215 6.175514221191406 0.051723018288612366
CurrentTrain: epoch  7, batch     8 | loss: 12.0307684Losses:  10.719722747802734 4.276144504547119 0.03363597393035889
CurrentTrain: epoch  7, batch     9 | loss: 10.7197227Losses:  14.660574913024902 8.015281677246094 0.040273576974868774
CurrentTrain: epoch  7, batch    10 | loss: 14.6605749Losses:  12.89270305633545 7.428877353668213 0.061589330434799194
CurrentTrain: epoch  7, batch    11 | loss: 12.8927031Losses:  12.956735610961914 7.6839494705200195 0.06383910775184631
CurrentTrain: epoch  7, batch    12 | loss: 12.9567356Losses:  11.731571197509766 5.345049858093262 0.04716186970472336
CurrentTrain: epoch  7, batch    13 | loss: 11.7315712Losses:  11.82509708404541 5.433002471923828 0.03741348162293434
CurrentTrain: epoch  7, batch    14 | loss: 11.8250971Losses:  16.59638786315918 10.524482727050781 0.059396713972091675
CurrentTrain: epoch  7, batch    15 | loss: 16.5963879Losses:  12.799566268920898 6.364582061767578 0.037218477576971054
CurrentTrain: epoch  7, batch    16 | loss: 12.7995663Losses:  13.897006034851074 8.11386489868164 0.061707183718681335
CurrentTrain: epoch  7, batch    17 | loss: 13.8970060Losses:  10.936651229858398 5.248468399047852 0.056331850588321686
CurrentTrain: epoch  7, batch    18 | loss: 10.9366512Losses:  9.787428855895996 3.967928647994995 0.035770729184150696
CurrentTrain: epoch  7, batch    19 | loss: 9.7874289Losses:  15.042792320251465 7.851778984069824 0.046781640499830246
CurrentTrain: epoch  7, batch    20 | loss: 15.0427923Losses:  13.112752914428711 7.455328941345215 0.0541103295981884
CurrentTrain: epoch  7, batch    21 | loss: 13.1127529Losses:  17.048377990722656 8.996953964233398 0.06518974900245667
CurrentTrain: epoch  7, batch    22 | loss: 17.0483780Losses:  12.303018569946289 6.451676845550537 0.049051664769649506
CurrentTrain: epoch  7, batch    23 | loss: 12.3030186Losses:  14.604620933532715 8.140928268432617 0.05586947500705719
CurrentTrain: epoch  7, batch    24 | loss: 14.6046209Losses:  16.5488224029541 8.65640640258789 0.045030009001493454
CurrentTrain: epoch  7, batch    25 | loss: 16.5488224Losses:  12.213536262512207 5.806683540344238 0.05367166921496391
CurrentTrain: epoch  7, batch    26 | loss: 12.2135363Losses:  19.087173461914062 10.976667404174805 0.0704791396856308
CurrentTrain: epoch  7, batch    27 | loss: 19.0871735Losses:  13.089615821838379 7.153183460235596 0.04654436558485031
CurrentTrain: epoch  7, batch    28 | loss: 13.0896158Losses:  11.339102745056152 4.620372772216797 0.032496724277734756
CurrentTrain: epoch  7, batch    29 | loss: 11.3391027Losses:  11.084439277648926 4.923212051391602 0.040675289928913116
CurrentTrain: epoch  7, batch    30 | loss: 11.0844393Losses:  15.077152252197266 8.863588333129883 0.0444277748465538
CurrentTrain: epoch  7, batch    31 | loss: 15.0771523Losses:  13.296162605285645 6.1429643630981445 0.04960540309548378
CurrentTrain: epoch  7, batch    32 | loss: 13.2961626Losses:  11.792732238769531 6.284841537475586 0.04717443883419037
CurrentTrain: epoch  7, batch    33 | loss: 11.7927322Losses:  13.242533683776855 7.097685813903809 0.05740269273519516
CurrentTrain: epoch  7, batch    34 | loss: 13.2425337Losses:  11.398058891296387 5.090080261230469 0.04030334949493408
CurrentTrain: epoch  7, batch    35 | loss: 11.3980589Losses:  11.37157154083252 4.780665874481201 0.040579427033662796
CurrentTrain: epoch  7, batch    36 | loss: 11.3715715Losses:  9.025202751159668 3.077863931655884 0.07100054621696472
CurrentTrain: epoch  7, batch    37 | loss: 9.0252028Losses:  12.056782722473145 5.434700965881348 0.03881700336933136
CurrentTrain: epoch  8, batch     0 | loss: 12.0567827Losses:  10.070144653320312 4.144208908081055 0.035584378987550735
CurrentTrain: epoch  8, batch     1 | loss: 10.0701447Losses:  11.870309829711914 5.676922798156738 0.0444922000169754
CurrentTrain: epoch  8, batch     2 | loss: 11.8703098Losses:  10.438400268554688 4.818285942077637 0.04050421714782715
CurrentTrain: epoch  8, batch     3 | loss: 10.4384003Losses:  10.400850296020508 5.11086368560791 0.03645101934671402
CurrentTrain: epoch  8, batch     4 | loss: 10.4008503Losses:  10.851444244384766 5.226339340209961 0.044445451349020004
CurrentTrain: epoch  8, batch     5 | loss: 10.8514442Losses:  12.954231262207031 6.78431510925293 0.04467347636818886
CurrentTrain: epoch  8, batch     6 | loss: 12.9542313Losses:  10.527009963989258 5.195456504821777 0.04032080993056297
CurrentTrain: epoch  8, batch     7 | loss: 10.5270100Losses:  11.077760696411133 5.698886871337891 0.04389215260744095
CurrentTrain: epoch  8, batch     8 | loss: 11.0777607Losses:  10.758406639099121 4.5349836349487305 0.03800073266029358
CurrentTrain: epoch  8, batch     9 | loss: 10.7584066Losses:  10.633305549621582 5.285672664642334 0.04330550134181976
CurrentTrain: epoch  8, batch    10 | loss: 10.6333055Losses:  11.14059829711914 5.854551315307617 0.042946238070726395
CurrentTrain: epoch  8, batch    11 | loss: 11.1405983Losses:  11.16556167602539 4.887151718139648 0.045292310416698456
CurrentTrain: epoch  8, batch    12 | loss: 11.1655617Losses:  9.349966049194336 4.177751541137695 0.036596063524484634
CurrentTrain: epoch  8, batch    13 | loss: 9.3499660Losses:  10.097936630249023 4.753670692443848 0.05221865326166153
CurrentTrain: epoch  8, batch    14 | loss: 10.0979366Losses:  11.844395637512207 6.703976631164551 0.05300011858344078
CurrentTrain: epoch  8, batch    15 | loss: 11.8443956Losses:  10.394208908081055 4.2815985679626465 0.03895024210214615
CurrentTrain: epoch  8, batch    16 | loss: 10.3942089Losses:  18.31890106201172 12.990347862243652 0.055286481976509094
CurrentTrain: epoch  8, batch    17 | loss: 18.3189011Losses:  11.14568042755127 6.052506446838379 0.04642766714096069
CurrentTrain: epoch  8, batch    18 | loss: 11.1456804Losses:  10.352409362792969 4.895591735839844 0.05136744678020477
CurrentTrain: epoch  8, batch    19 | loss: 10.3524094Losses:  10.80453872680664 5.619222640991211 0.05203234404325485
CurrentTrain: epoch  8, batch    20 | loss: 10.8045387Losses:  9.645659446716309 4.415605068206787 0.04209441691637039
CurrentTrain: epoch  8, batch    21 | loss: 9.6456594Losses:  11.301799774169922 6.168684005737305 0.04982426390051842
CurrentTrain: epoch  8, batch    22 | loss: 11.3017998Losses:  10.33526611328125 5.3409881591796875 0.0508384183049202
CurrentTrain: epoch  8, batch    23 | loss: 10.3352661Losses:  17.372203826904297 11.842971801757812 0.061712902039289474
CurrentTrain: epoch  8, batch    24 | loss: 17.3722038Losses:  17.039104461669922 10.733985900878906 0.06436052918434143
CurrentTrain: epoch  8, batch    25 | loss: 17.0391045Losses:  10.858384132385254 5.573248863220215 0.04911760613322258
CurrentTrain: epoch  8, batch    26 | loss: 10.8583841Losses:  11.043978691101074 5.97035026550293 0.04907459020614624
CurrentTrain: epoch  8, batch    27 | loss: 11.0439787Losses:  10.825343132019043 5.547303199768066 0.03500497713685036
CurrentTrain: epoch  8, batch    28 | loss: 10.8253431Losses:  11.498418807983398 5.620362758636475 0.04488658159971237
CurrentTrain: epoch  8, batch    29 | loss: 11.4984188Losses:  10.40456771850586 4.725940704345703 0.044409871101379395
CurrentTrain: epoch  8, batch    30 | loss: 10.4045677Losses:  9.287137985229492 3.894805431365967 0.03688143938779831
CurrentTrain: epoch  8, batch    31 | loss: 9.2871380Losses:  11.21823787689209 5.178305625915527 0.043224845081567764
CurrentTrain: epoch  8, batch    32 | loss: 11.2182379Losses:  10.96590805053711 5.3590617179870605 0.061552342027425766
CurrentTrain: epoch  8, batch    33 | loss: 10.9659081Losses:  10.514739990234375 5.048953056335449 0.04895336553454399
CurrentTrain: epoch  8, batch    34 | loss: 10.5147400Losses:  11.826147079467773 5.468369960784912 0.03913659602403641
CurrentTrain: epoch  8, batch    35 | loss: 11.8261471Losses:  16.416650772094727 11.368841171264648 0.07261840999126434
CurrentTrain: epoch  8, batch    36 | loss: 16.4166508Losses:  8.94122314453125 3.2710623741149902 0.061237141489982605
CurrentTrain: epoch  8, batch    37 | loss: 8.9412231Losses:  9.208683013916016 4.2761030197143555 0.03567963093519211
CurrentTrain: epoch  9, batch     0 | loss: 9.2086830Losses:  11.01763916015625 6.019144058227539 0.04658105969429016
CurrentTrain: epoch  9, batch     1 | loss: 11.0176392Losses:  12.333002090454102 6.615955352783203 0.04941514879465103
CurrentTrain: epoch  9, batch     2 | loss: 12.3330021Losses:  11.506803512573242 5.791584491729736 0.04311152547597885
CurrentTrain: epoch  9, batch     3 | loss: 11.5068035Losses:  11.501718521118164 5.913667678833008 0.038535792380571365
CurrentTrain: epoch  9, batch     4 | loss: 11.5017185Losses:  9.127425193786621 3.8833751678466797 0.03142300993204117
CurrentTrain: epoch  9, batch     5 | loss: 9.1274252Losses:  9.613709449768066 4.579083442687988 0.04087311029434204
CurrentTrain: epoch  9, batch     6 | loss: 9.6137094Losses:  11.470455169677734 6.632842540740967 0.040240827947854996
CurrentTrain: epoch  9, batch     7 | loss: 11.4704552Losses:  9.306828498840332 4.402102947235107 0.03816523030400276
CurrentTrain: epoch  9, batch     8 | loss: 9.3068285Losses:  10.536486625671387 5.551354885101318 0.03500416874885559
CurrentTrain: epoch  9, batch     9 | loss: 10.5364866Losses:  8.63161563873291 3.4864041805267334 0.031144389882683754
CurrentTrain: epoch  9, batch    10 | loss: 8.6316156Losses:  11.37575626373291 6.172204494476318 0.04686450958251953
CurrentTrain: epoch  9, batch    11 | loss: 11.3757563Losses:  9.695357322692871 4.662320137023926 0.03813430666923523
CurrentTrain: epoch  9, batch    12 | loss: 9.6953573Losses:  8.946244239807129 4.2123637199401855 0.03543970361351967
CurrentTrain: epoch  9, batch    13 | loss: 8.9462442Losses:  9.251762390136719 4.237930774688721 0.036808136850595474
CurrentTrain: epoch  9, batch    14 | loss: 9.2517624Losses:  12.425715446472168 5.961255073547363 0.03591012582182884
CurrentTrain: epoch  9, batch    15 | loss: 12.4257154Losses:  8.221210479736328 3.4303932189941406 0.030540669336915016
CurrentTrain: epoch  9, batch    16 | loss: 8.2212105Losses:  9.089463233947754 4.195467472076416 0.032965704798698425
CurrentTrain: epoch  9, batch    17 | loss: 9.0894632Losses:  12.243898391723633 7.366943359375 0.05240226909518242
CurrentTrain: epoch  9, batch    18 | loss: 12.2438984Losses:  12.777957916259766 7.9234209060668945 0.043150290846824646
CurrentTrain: epoch  9, batch    19 | loss: 12.7779579Losses:  11.787395477294922 7.083689212799072 0.04844435304403305
CurrentTrain: epoch  9, batch    20 | loss: 11.7873955Losses:  10.497333526611328 4.8957695960998535 0.036497969180345535
CurrentTrain: epoch  9, batch    21 | loss: 10.4973335Losses:  8.237069129943848 3.4131553173065186 0.030855925753712654
CurrentTrain: epoch  9, batch    22 | loss: 8.2370691Losses:  10.176874160766602 5.520867347717285 0.04178731143474579
CurrentTrain: epoch  9, batch    23 | loss: 10.1768742Losses:  10.5955228805542 5.426030158996582 0.03383490443229675
CurrentTrain: epoch  9, batch    24 | loss: 10.5955229Losses:  9.115686416625977 4.2772536277771 0.033148393034935
CurrentTrain: epoch  9, batch    25 | loss: 9.1156864Losses:  11.800459861755371 6.972301006317139 0.04324813559651375
CurrentTrain: epoch  9, batch    26 | loss: 11.8004599Losses:  9.47012996673584 4.631114482879639 0.0375545397400856
CurrentTrain: epoch  9, batch    27 | loss: 9.4701300Losses:  10.570798873901367 5.576601982116699 0.0380924791097641
CurrentTrain: epoch  9, batch    28 | loss: 10.5707989Losses:  9.94001293182373 4.815739631652832 0.03515667840838432
CurrentTrain: epoch  9, batch    29 | loss: 9.9400129Losses:  11.882216453552246 6.071309566497803 0.0383710041642189
CurrentTrain: epoch  9, batch    30 | loss: 11.8822165Losses:  9.968583106994629 4.829307556152344 0.0325455367565155
CurrentTrain: epoch  9, batch    31 | loss: 9.9685831Losses:  12.22427749633789 7.190266132354736 0.04264248162508011
CurrentTrain: epoch  9, batch    32 | loss: 12.2242775Losses:  12.69871711730957 7.544459342956543 0.04461123049259186
CurrentTrain: epoch  9, batch    33 | loss: 12.6987171Losses:  10.86819839477539 5.676766872406006 0.0412176139652729
CurrentTrain: epoch  9, batch    34 | loss: 10.8681984Losses:  11.046276092529297 4.923770427703857 0.034120090305805206
CurrentTrain: epoch  9, batch    35 | loss: 11.0462761Losses:  9.595193862915039 4.607402801513672 0.0277003925293684
CurrentTrain: epoch  9, batch    36 | loss: 9.5951939Losses:  6.150660514831543 1.486567735671997 0.052611470222473145
CurrentTrain: epoch  9, batch    37 | loss: 6.1506605
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: iran 's top nuclear negotiator , ali larijani , was asked by state tv whether the country had started converting into gas a second batch of uranium , as it had planned to do at its isfahan nuclear facility .
Head Entity: ali larijani
Tail Entity: iran
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, the famous artist decided to return to her homeland, where she felt a deep connection to her roots.  
Head Entity: the famous artist  
Tail Entity: her homeland  

Relation: person countries of residence  
Context: Following his successful career in Europe, the renowned chef opened a restaurant in his native country, showcasing traditional dishes to a new audience.  
Head Entity: the renowned chef  
Tail Entity: his native country  

Relation: person countries of residence  
Context: The scientist, who had spent a decade in Canada, finally moved back to her birthplace to contribute to local research initiatives.  
Head Entity: the scientist  
Tail Entity: her birthplace  

Relation: person countries of residence  
Context: After completing his studies abroad, the young entrepreneur returned to his country to launch a startup that would benefit the local economy.  
Head Entity: the young entrepreneur  
Tail Entity: his country  

Relation: person countries of residence  
Context: The famous author, known for her best-selling novels, decided to settle in her childhood country to find inspiration for her next book.  
Head Entity: the famous author  
Tail Entity: her childhood country  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` since the salinas episode , the bank 's policy on accepting public figures as customers has toughened , '' said tom lahiff , a vice president and lawyer for citibank .
Head Entity: citibank
Tail Entity: tom lahiff
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: `` during the annual meeting, the board of directors praised the contributions of their chief technology officer, who has been instrumental in driving innovation at the company, '' said the spokesperson for tech solutions inc.  
Head Entity: tech solutions inc.  
Tail Entity: chief technology officer  

Relation: organization top members employees  
Context: `` the new marketing strategy was presented by the chief marketing officer, who emphasized the importance of customer engagement in today's digital landscape, '' reported the press release from global marketing group.  
Head Entity: global marketing group  
Tail Entity: chief marketing officer  

Relation: organization top members employees  
Context: `` after the merger, the new chief financial officer outlined the financial goals for the upcoming year, highlighting the need for cost efficiency, '' stated the official announcement from united finance corp.  
Head Entity: united finance corp.  
Tail Entity: chief financial officer  

Relation: organization top members employees  
Context: `` the founder of the startup shared her vision for the future during the tech conference, inspiring many young entrepreneurs in the audience, '' noted the event organizer for innovative tech labs.  
Head Entity: innovative tech labs  
Tail Entity: founder  

Relation: organization top members employees  
Context: `` the head of human resources discussed the new employee benefits program at the company retreat, aiming to improve employee satisfaction and retention, '' said the company spokesperson for wellness solutions.  
Head Entity: wellness solutions  
Tail Entity: head of human resources  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: defending super bowl champions the indianapolis colts dominated every aspect of monday 's showdown against the jacksonville jaguars , easily winning 29-7 and once again taking charge in the american football conference south .
Head Entity: jacksonville jaguars
Tail Entity: american football conference south
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The tech giant Google has been a prominent member of the Internet Association, advocating for policies that benefit the digital economy.  
Head Entity: Google  
Tail Entity: Internet Association  

Relation: organization member of  
Context: As a leading environmental organization, Greenpeace is a key member of the Global Climate Coalition, working towards sustainable solutions.  
Head Entity: Greenpeace  
Tail Entity: Global Climate Coalition  

Relation: organization member of  
Context: The National Football League (NFL) is a member of the International Federation of American Football, promoting the sport worldwide.  
Head Entity: National Football League  
Tail Entity: International Federation of American Football  

Relation: organization member of  
Context: The American Medical Association plays a crucial role as a member of the Coalition for Health Funding, advocating for increased health funding.  
Head Entity: American Medical Association  
Tail Entity: Coalition for Health Funding  

Relation: organization member of  
Context: The World Wildlife Fund is an active member of the Conservation International, collaborating on various global conservation efforts.  
Head Entity: World Wildlife Fund  
Tail Entity: Conservation International  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: cuban president raul castro has met with his namibian counterpart hifkepunye pohamba on the last day of his trip to the country .
Head Entity: raul castro
Tail Entity: cuban
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned scientist albert einstein was born in germany before moving to the united states.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author chimamanda ngozi adichie hails from nigeria, where she draws much of her inspiration.  
Head Entity: chimamanda ngozi adichie  
Tail Entity: nigerian  

Relation: person origin  
Context: the legendary musician bob marley was a prominent figure in jamaican music and culture.  
Head Entity: bob marley  
Tail Entity: jamaican  

Relation: person origin  
Context: the celebrated actress penelope cruz is known for her work in both spanish and american films, reflecting her spanish heritage.  
Head Entity: penelope cruz  
Tail Entity: spanish  

Relation: person origin  
Context: the influential political leader nelson mandela was born in south africa and became a symbol of the fight against apartheid.  
Head Entity: nelson mandela  
Tail Entity: south african  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: but the swiss bank also announced it would need to issue another 15 billion dollars in shares and that chairman marcel ospel had quit .
Head Entity: marcel ospel
Tail Entity: chairman
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: After years of dedicated service, the board of directors appointed Sarah Johnson as the new chief executive officer of the company.  
Head Entity: Sarah Johnson  
Tail Entity: chief executive officer  

Relation: person title  
Context: During the award ceremony, the renowned scientist Dr. Emily Carter was recognized for her groundbreaking research and was awarded the title of lead researcher.  
Head Entity: Dr. Emily Carter  
Tail Entity: lead researcher  

Relation: person title  
Context: In a recent interview, the famous author James Smith discussed his journey and how he became the best-selling novelist of the year.  
Head Entity: James Smith  
Tail Entity: best-selling novelist  

Relation: person title  
Context: The university announced that Professor Alan Brown would be stepping down from his role as department chair after a decade of leadership.  
Head Entity: Professor Alan Brown  
Tail Entity: department chair  

Relation: person title  
Context: At the gala, the community honored Mayor Lisa Green for her outstanding contributions and leadership in local government.  
Head Entity: Mayor Lisa Green  
Tail Entity: local government leader  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: commander viliame naupoto , chairman of the fiji pine limited announced the woodchips exports target here tuesday after signing a woodchip sale agreement with japan 's itochu corporation .
Head Entity: itochu corporation
Tail Entity: japan
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the headquarters of the multinational technology company samsung electronics is located in suwon, south korea, where it conducts extensive research and development.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization country of headquarters  
Context: after expanding its operations in europe, the american fast-food chain mcdonald's has established its main office in amsterdam, netherlands.  
Head Entity: mcdonald's  
Tail Entity: netherlands  

Relation: organization country of headquarters  
Context: the global consulting firm accenture has its headquarters in dublin, ireland, serving clients across various industries worldwide.  
Head Entity: accenture  
Tail Entity: ireland  

Relation: organization country of headquarters  
Context: the famous car manufacturer toyota has its main office in toyota city, japan, where it oversees its production and innovation strategies.  
Head Entity: toyota  
Tail Entity: japan  

Relation: organization country of headquarters  
Context: the international non-profit organization greenpeace is headquartered in amsterdam, netherlands, focusing on environmental issues globally.  
Head Entity: greenpeace  
Tail Entity: netherlands  
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.93%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 87.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 84.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 87.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.58%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.93%   
cur_acc:  ['0.8693']
his_acc:  ['0.8693']
Clustering into  4  clusters
Clusters:  [3 1 1 3 0 2 3 0 1 0 0]
Losses:  14.415072441101074 8.238791465759277 0.20715078711509705
CurrentTrain: epoch  0, batch     0 | loss: 14.4150724Losses:  9.234607696533203 3.3231201171875 0.18515560030937195
CurrentTrain: epoch  0, batch     1 | loss: 9.2346077Losses:  13.266810417175293 7.729004383087158 0.20272120833396912
CurrentTrain: epoch  1, batch     0 | loss: 13.2668104Losses:  7.054591655731201 2.019845724105835 0.17143592238426208
CurrentTrain: epoch  1, batch     1 | loss: 7.0545917Losses:  12.47928524017334 7.345308780670166 0.1923021525144577
CurrentTrain: epoch  2, batch     0 | loss: 12.4792852Losses:  7.596637725830078 2.6662819385528564 0.19460497796535492
CurrentTrain: epoch  2, batch     1 | loss: 7.5966377Losses:  11.625140190124512 7.899788856506348 0.1799582839012146
CurrentTrain: epoch  3, batch     0 | loss: 11.6251402Losses:  11.475458145141602 5.4944305419921875 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 11.4754581Losses:  12.093302726745605 8.034406661987305 0.1808837503194809
CurrentTrain: epoch  4, batch     0 | loss: 12.0933027Losses:  8.076789855957031 3.7248332500457764 0.17859584093093872
CurrentTrain: epoch  4, batch     1 | loss: 8.0767899Losses:  9.411209106445312 5.90028190612793 0.16862058639526367
CurrentTrain: epoch  5, batch     0 | loss: 9.4112091Losses:  5.349453926086426 1.6861499547958374 0.17387168109416962
CurrentTrain: epoch  5, batch     1 | loss: 5.3494539Losses:  10.128975868225098 6.461711883544922 0.15172970294952393
CurrentTrain: epoch  6, batch     0 | loss: 10.1289759Losses:  5.911850929260254 2.1276793479919434 0.1605883538722992
CurrentTrain: epoch  6, batch     1 | loss: 5.9118509Losses:  10.418764114379883 7.067177772521973 0.15414416790008545
CurrentTrain: epoch  7, batch     0 | loss: 10.4187641Losses:  6.5331339836120605 2.936504602432251 0.09988364577293396
CurrentTrain: epoch  7, batch     1 | loss: 6.5331340Losses:  10.816296577453613 7.512131690979004 0.16287541389465332
CurrentTrain: epoch  8, batch     0 | loss: 10.8162966Losses:  4.712640762329102 2.4759762287139893 0.15236692130565643
CurrentTrain: epoch  8, batch     1 | loss: 4.7126408Losses:  8.099685668945312 5.6427836418151855 0.1488431692123413
CurrentTrain: epoch  9, batch     0 | loss: 8.0996857Losses:  3.6302971839904785 1.1397271156311035 0.1418055295944214
CurrentTrain: epoch  9, batch     1 | loss: 3.6302972
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: san jose , ca , usa speaking of k-fed , him and ex-wife britney spears are in court today , dealing with their custody battle .
Head Entity: britney spears
Tail Entity: ca
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York, John settled in Austin, Texas, where he found a job in tech.  
Head Entity: John  
Tail Entity: Texas  

Relation: person stateorprovinces of residence  
Context: The famous actor, who was born in Chicago, now resides in Los Angeles, California, enjoying the sunny weather.  
Head Entity: The famous actor  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: Following her studies in Boston, Emily decided to make Seattle her home, captivated by the vibrant culture.  
Head Entity: Emily  
Tail Entity: Seattle  

Relation: person stateorprovinces of residence  
Context: Mark has lived in Miami, Florida, for over a decade, embracing the lively atmosphere and beach life.  
Head Entity: Mark  
Tail Entity: Florida  

Relation: person stateorprovinces of residence  
Context: After years in the military, Sarah returned to her hometown of Denver, Colorado, where she plans to start a new chapter.  
Head Entity: Sarah  
Tail Entity: Colorado  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: today the nypd upgraded the charges to include murder , in the case of brooklyn gay-bashing/robbery victim michael sandy , who died on friday after being taken off life-support .
Head Entity: michael sandy
Tail Entity: friday
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned physicist Stephen Hawking passed away peacefully at his home in Cambridge on March 14, 2018, at the age of 76.  
Head Entity: Stephen Hawking  
Tail Entity: March 14, 2018  

Relation: person date of death  
Context: The beloved actor Robin Williams was found dead in his home in Paradise Cay, California, on August 11, 2014, which shocked fans around the world.  
Head Entity: Robin Williams  
Tail Entity: August 11, 2014  

Relation: person date of death  
Context: The famous author Gabriel Garca Mrquez, known for his magical realism, died on April 17, 2014, in Mexico City.  
Head Entity: Gabriel Garca Mrquez  
Tail Entity: April 17, 2014  

Relation: person date of death  
Context: The legendary musician David Bowie passed away after a long battle with cancer on January 10, 2016, just two days after his birthday.  
Head Entity: David Bowie  
Tail Entity: January 10, 2016  

Relation: person date of death  
Context: The civil rights leader Martin Luther King Jr. was assassinated on April 4, 1968, in Memphis, Tennessee, leaving a lasting legacy.  
Head Entity: Martin Luther King Jr.  
Tail Entity: April 4, 1968  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company Apple has been expanding its workforce significantly, currently employing over 147,000 people across the globe.  
Head Entity: Apple  
Tail Entity: 147,000  

Relation: organization number of employees members  
Context: In 2022, the multinational corporation Amazon reported a staggering total of 1.5 million employees, making it one of the largest employers in the world.  
Head Entity: Amazon  
Tail Entity: 1.5 million  

Relation: organization number of employees members  
Context: Google, known for its innovative work culture, has a workforce that exceeds 156,000 employees as of the latest reports.  
Head Entity: Google  
Tail Entity: 156,000  

Relation: organization number of employees members  
Context: The automotive giant Toyota has a global workforce of approximately 360,000 employees, contributing to its status as a leading car manufacturer.  
Head Entity: Toyota  
Tail Entity: 360,000  

Relation: organization number of employees members  
Context: With a commitment to sustainability, the renewable energy company NextEra Energy employs around 15,000 individuals dedicated to clean energy solutions.  
Head Entity: NextEra Energy  
Tail Entity: 15,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: but hyperventilating bloviators jumped all over sen. barbara boxer last week for alluding to secretary of state condoleezza rice 's single status -- as though boxer were accusing rice of botching the iraq war because she 's a spinster .
Head Entity: boxer
Tail Entity: barbara boxer
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The artist known as Prince was a musical genius who changed the landscape of pop music forever.  
Head Entity: Prince  
Tail Entity: Prince Rogers Nelson  

Relation: person alternate names  
Context: Many fans remember the actor as "The Rock," a name he used during his wrestling career before transitioning to Hollywood.  
Head Entity: The Rock  
Tail Entity: Dwayne Johnson  

Relation: person alternate names  
Context: The famous author often referred to as J.K. Rowling has captivated millions with her Harry Potter series.  
Head Entity: J.K. Rowling  
Tail Entity: Joanne Rowling  

Relation: person alternate names  
Context: The legendary musician Bob Dylan, originally named Robert Zimmerman, has influenced countless artists over the decades.  
Head Entity: Bob Dylan  
Tail Entity: Robert Zimmerman  

Relation: person alternate names  
Context: The renowned physicist Stephen Hawking, who was often called the "Einstein of our time," made groundbreaking contributions to cosmology.  
Head Entity: Stephen Hawking  
Tail Entity: Einstein of our time  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: kerry katona feel out with arnold just before she married bryan mcfadden , kerry cancelled his wedding invitation and have n't spoken since .
Head Entity: bryan mcfadden
Tail Entity: kerry katona
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
1. Relation: person spouse  
   Context: After years of dating, emma stone finally tied the knot with dave mccary in a beautiful ceremony last summer.  
   Head Entity: dave mccary  
   Tail Entity: emma stone  

2. Relation: person spouse  
   Context: During their time together, ben affleck and jennifer garner built a family, but they eventually decided to part ways amicably.  
   Head Entity: jennifer garner  
   Tail Entity: ben affleck  

3. Relation: person spouse  
   Context: The couple celebrated their 10th anniversary, reminiscing about the day when ryan reynolds and blake lively exchanged vows in a secret ceremony.  
   Head Entity: blake lively  
   Tail Entity: ryan reynolds  

4. Relation: person spouse  
   Context: After a whirlwind romance, prince harry and meghan markle announced their engagement, leading to a royal wedding that captivated the world.  
   Head Entity: meghan markle  
   Tail Entity: prince harry  

5. Relation: person spouse  
   Context: Following their engagement, mila kunis and ashton kutcher welcomed their first child, solidifying their bond as a married couple.  
   Head Entity: ashton kutcher  
   Tail Entity: mila kunis  
Losses:  12.191062927246094 4.438718795776367 0.1928142011165619
MemoryTrain:  epoch  0, batch     0 | loss: 12.1910629Losses:  9.556506156921387 3.460477828979492 0.29541170597076416
MemoryTrain:  epoch  0, batch     1 | loss: 9.5565062Losses:  9.255672454833984 2.535823106765747 0.3003788888454437
MemoryTrain:  epoch  0, batch     2 | loss: 9.2556725Losses:  10.049800872802734 3.2220396995544434 0.26702457666397095
MemoryTrain:  epoch  0, batch     3 | loss: 10.0498009Losses:  7.723141670227051 -0.0 -0.0
MemoryTrain:  epoch  0, batch     4 | loss: 7.7231417Losses:  9.734785079956055 3.1256704330444336 0.2527310252189636
MemoryTrain:  epoch  1, batch     0 | loss: 9.7347851Losses:  8.436739921569824 2.360776424407959 0.3355894386768341
MemoryTrain:  epoch  1, batch     1 | loss: 8.4367399Losses:  8.871803283691406 2.634749174118042 0.16129711270332336
MemoryTrain:  epoch  1, batch     2 | loss: 8.8718033Losses:  7.56774377822876 2.4772565364837646 0.2846466600894928
MemoryTrain:  epoch  1, batch     3 | loss: 7.5677438Losses:  3.6034531593322754 -0.0 0.14498841762542725
MemoryTrain:  epoch  1, batch     4 | loss: 3.6034532Losses:  6.679548263549805 2.283334255218506 0.24935854971408844
MemoryTrain:  epoch  2, batch     0 | loss: 6.6795483Losses:  8.656805038452148 2.818235397338867 0.300234317779541
MemoryTrain:  epoch  2, batch     1 | loss: 8.6568050Losses:  7.166353702545166 1.8787308931350708 0.2733098268508911
MemoryTrain:  epoch  2, batch     2 | loss: 7.1663537Losses:  6.67921781539917 2.0633158683776855 0.27133089303970337
MemoryTrain:  epoch  2, batch     3 | loss: 6.6792178Losses:  2.0666439533233643 -0.0 0.1184927374124527
MemoryTrain:  epoch  2, batch     4 | loss: 2.0666440Losses:  6.1325459480285645 1.440255045890808 0.256207138299942
MemoryTrain:  epoch  3, batch     0 | loss: 6.1325459Losses:  7.3224592208862305 2.4702887535095215 0.21958893537521362
MemoryTrain:  epoch  3, batch     1 | loss: 7.3224592Losses:  5.51375675201416 1.813965082168579 0.26586419343948364
MemoryTrain:  epoch  3, batch     2 | loss: 5.5137568Losses:  5.87744665145874 1.5773067474365234 0.2681831121444702
MemoryTrain:  epoch  3, batch     3 | loss: 5.8774467Losses:  4.006235122680664 -0.0 0.09775868058204651
MemoryTrain:  epoch  3, batch     4 | loss: 4.0062351Losses:  7.629275798797607 3.796013832092285 0.19998393952846527
MemoryTrain:  epoch  4, batch     0 | loss: 7.6292758Losses:  5.089205741882324 1.744504690170288 0.26005154848098755
MemoryTrain:  epoch  4, batch     1 | loss: 5.0892057Losses:  7.5509257316589355 3.0267903804779053 0.30112016201019287
MemoryTrain:  epoch  4, batch     2 | loss: 7.5509257Losses:  8.884474754333496 4.149295330047607 0.21079878509044647
MemoryTrain:  epoch  4, batch     3 | loss: 8.8844748Losses:  1.992920160293579 -0.0 0.09539847075939178
MemoryTrain:  epoch  4, batch     4 | loss: 1.9929202Losses:  6.346028804779053 2.33274245262146 0.2664578855037689
MemoryTrain:  epoch  5, batch     0 | loss: 6.3460288Losses:  6.132112503051758 2.2868027687072754 0.19693486392498016
MemoryTrain:  epoch  5, batch     1 | loss: 6.1321125Losses:  5.04832649230957 1.8971607685089111 0.29444676637649536
MemoryTrain:  epoch  5, batch     2 | loss: 5.0483265Losses:  6.923913955688477 3.1937460899353027 0.29794296622276306
MemoryTrain:  epoch  5, batch     3 | loss: 6.9239140Losses:  3.517118453979492 -0.0 0.10685718804597855
MemoryTrain:  epoch  5, batch     4 | loss: 3.5171185Losses:  5.594400405883789 2.837014675140381 0.273542582988739
MemoryTrain:  epoch  6, batch     0 | loss: 5.5944004Losses:  5.380100727081299 2.2714200019836426 0.30612754821777344
MemoryTrain:  epoch  6, batch     1 | loss: 5.3801007Losses:  7.675568103790283 3.7933945655822754 0.2751181721687317
MemoryTrain:  epoch  6, batch     2 | loss: 7.6755681Losses:  7.180915832519531 3.8271965980529785 0.2959916591644287
MemoryTrain:  epoch  6, batch     3 | loss: 7.1809158Losses:  5.437291145324707 -0.0 0.11110429465770721
MemoryTrain:  epoch  6, batch     4 | loss: 5.4372911Losses:  5.749390602111816 2.211902379989624 0.29520970582962036
MemoryTrain:  epoch  7, batch     0 | loss: 5.7493906Losses:  4.801015377044678 1.822622299194336 0.2946144938468933
MemoryTrain:  epoch  7, batch     1 | loss: 4.8010154Losses:  9.410882949829102 5.858852386474609 0.18137487769126892
MemoryTrain:  epoch  7, batch     2 | loss: 9.4108829Losses:  5.264406204223633 2.3725013732910156 0.3370778560638428
MemoryTrain:  epoch  7, batch     3 | loss: 5.2644062Losses:  2.0021603107452393 -0.0 0.10580763965845108
MemoryTrain:  epoch  7, batch     4 | loss: 2.0021603Losses:  5.836263656616211 3.361841917037964 0.18141573667526245
MemoryTrain:  epoch  8, batch     0 | loss: 5.8362637Losses:  5.499561309814453 1.9614689350128174 0.2662754952907562
MemoryTrain:  epoch  8, batch     1 | loss: 5.4995613Losses:  6.090339660644531 3.3290369510650635 0.297340452671051
MemoryTrain:  epoch  8, batch     2 | loss: 6.0903397Losses:  6.566964626312256 3.444418430328369 0.29043203592300415
MemoryTrain:  epoch  8, batch     3 | loss: 6.5669646Losses:  2.6252245903015137 -0.0 0.09296618402004242
MemoryTrain:  epoch  8, batch     4 | loss: 2.6252246Losses:  5.208691120147705 2.7020466327667236 0.21677520871162415
MemoryTrain:  epoch  9, batch     0 | loss: 5.2086911Losses:  4.852075576782227 2.0583558082580566 0.31737589836120605
MemoryTrain:  epoch  9, batch     1 | loss: 4.8520756Losses:  5.41939115524292 1.9842164516448975 0.25077107548713684
MemoryTrain:  epoch  9, batch     2 | loss: 5.4193912Losses:  4.335493087768555 1.6129757165908813 0.28132036328315735
MemoryTrain:  epoch  9, batch     3 | loss: 4.3354931Losses:  2.0679638385772705 -0.0 0.1737293303012848
MemoryTrain:  epoch  9, batch     4 | loss: 2.0679638
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 77.08%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 34.38%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 35.42%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 33.75%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 40.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 53.47%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 67.65%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 69.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 71.13%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 72.44%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 78.54%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 78.83%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 79.30%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 79.55%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 79.23%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 79.51%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 79.73%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 80.10%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 80.61%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 81.10%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 80.82%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 80.28%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 79.89%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 79.39%   
cur_acc:  ['0.8693', '0.7708']
his_acc:  ['0.8693', '0.7939']
Clustering into  7  clusters
Clusters:  [1 3 3 1 0 5 1 4 6 0 0 4 1 2 3 1]
Losses:  15.82648754119873 8.866256713867188 0.5179307460784912
CurrentTrain: epoch  0, batch     0 | loss: 15.8264875Losses:  11.792881965637207 4.310492515563965 0.3157309889793396
CurrentTrain: epoch  0, batch     1 | loss: 11.7928820Losses:  13.992764472961426 7.734369277954102 0.4872813820838928
CurrentTrain: epoch  1, batch     0 | loss: 13.9927645Losses:  9.595328330993652 2.850282907485962 0.4258887767791748
CurrentTrain: epoch  1, batch     1 | loss: 9.5953283Losses:  11.970377922058105 7.244019031524658 0.4469957947731018
CurrentTrain: epoch  2, batch     0 | loss: 11.9703779Losses:  6.688149452209473 2.3716206550598145 0.43644648790359497
CurrentTrain: epoch  2, batch     1 | loss: 6.6881495Losses:  10.88020133972168 6.932504653930664 0.41721299290657043
CurrentTrain: epoch  3, batch     0 | loss: 10.8802013Losses:  5.2834553718566895 2.1351802349090576 0.36970072984695435
CurrentTrain: epoch  3, batch     1 | loss: 5.2834554Losses:  10.117110252380371 6.690195083618164 0.3899209499359131
CurrentTrain: epoch  4, batch     0 | loss: 10.1171103Losses:  4.854274749755859 2.031961441040039 0.3621320426464081
CurrentTrain: epoch  4, batch     1 | loss: 4.8542747Losses:  9.393828392028809 6.426697731018066 0.3518141806125641
CurrentTrain: epoch  5, batch     0 | loss: 9.3938284Losses:  4.698592662811279 1.9087731838226318 0.342410683631897
CurrentTrain: epoch  5, batch     1 | loss: 4.6985927Losses:  9.784172058105469 7.100765705108643 0.34223857522010803
CurrentTrain: epoch  6, batch     0 | loss: 9.7841721Losses:  5.790092468261719 2.9935035705566406 0.35514700412750244
CurrentTrain: epoch  6, batch     1 | loss: 5.7900925Losses:  9.374530792236328 6.6485090255737305 0.33000433444976807
CurrentTrain: epoch  7, batch     0 | loss: 9.3745308Losses:  4.685461044311523 2.134650707244873 0.33263325691223145
CurrentTrain: epoch  7, batch     1 | loss: 4.6854610Losses:  9.06842041015625 6.701098442077637 0.3107377588748932
CurrentTrain: epoch  8, batch     0 | loss: 9.0684204Losses:  4.80900764465332 2.5485172271728516 0.3121378421783447
CurrentTrain: epoch  8, batch     1 | loss: 4.8090076Losses:  8.054130554199219 5.766868591308594 0.3011013865470886
CurrentTrain: epoch  9, batch     0 | loss: 8.0541306Losses:  4.1590189933776855 1.843181848526001 0.32068783044815063
CurrentTrain: epoch  9, batch     1 | loss: 4.1590190
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  

Relation: person date of birth  
Context: the legendary musician freddie mercury was born farrokh bulsara on september 5, 1946, in zanzibar, tanzania.  
Head Entity: freddie mercury  
Tail Entity: september 5, 1946  

Relation: person date of birth  
Context: the influential civil rights leader martin luther king jr. was born on january 15, 1929, in atlanta, georgia.  
Head Entity: martin luther king jr.  
Tail Entity: january 15, 1929  

Relation: person date of birth  
Context: the famous painter pablo picasso was born on october 25, 1881, in malaga, spain.  
Head Entity: pablo picasso  
Tail Entity: october 25, 1881  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, where she spent her early years.  
Head Entity: martha stewart  
Tail Entity: new jersey  

Relation: person stateorprovince of birth  
Context: the famous actor robert downey jr. was born on april 4, 1965, in manhattan, new york.  
Head Entity: robert downey jr.  
Tail Entity: new york  

Relation: person stateorprovince of birth  
Context: born in 1975, the renowned author j.k. rowling hails from yate, gloucestershire, in the united kingdom.  
Head Entity: j.k. rowling  
Tail Entity: gloucestershire  

Relation: person stateorprovince of birth  
Context: the legendary musician bob marley was born on february 6, 1945, in nine mile, saint ann, jamaica.  
Head Entity: bob marley  
Tail Entity: saint ann  

Relation: person stateorprovince of birth  
Context: the celebrated scientist albert einstein was born on march 14, 1879, in ulm, baden-wrttemberg, germany.  
Head Entity: albert einstein  
Tail Entity: baden-wrttemberg  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
1. Relation: person parents  
   Context: During the family reunion, Sarah shared stories about how her mother always encouraged her to pursue her dreams.  
   Head Entity: her mother  
   Tail Entity: Sarah  

2. Relation: person parents  
   Context: After the ceremony, James reflected on how his father had always been his role model and source of inspiration.  
   Head Entity: his father  
   Tail Entity: James  

3. Relation: person parents  
   Context: Emily often reminisces about the lessons her dad taught her about hard work and perseverance.  
   Head Entity: her dad  
   Tail Entity: Emily  

4. Relation: person parents  
   Context: At the graduation party, Michael thanked his parents for their unwavering support throughout his education.  
   Head Entity: his parents  
   Tail Entity: Michael  

5. Relation: person parents  
   Context: In her memoir, Anna described the sacrifices her mother made to provide for her and her siblings.  
   Head Entity: her mother  
   Tail Entity: Anna  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work, Maria finally landed a job at the prestigious tech company, Innovatech Solutions, where she could showcase her skills.  
Head Entity: Maria  
Tail Entity: Innovatech Solutions  

Relation: person employee of  
Context: John has been with the marketing team at Global Media Corp for over a decade, contributing to numerous successful campaigns.  
Head Entity: John  
Tail Entity: Global Media Corp  

Relation: person employee of  
Context: As a lead designer, Sarah played a crucial role in the success of the new product line at Creative Designs Inc.  
Head Entity: Sarah  
Tail Entity: Creative Designs Inc.  

Relation: person employee of  
Context: After completing his internship, David was offered a full-time position at Green Energy Solutions, where he focuses on renewable resources.  
Head Entity: David  
Tail Entity: Green Energy Solutions  

Relation: person employee of  
Context: Emily's dedication to her work at HealthFirst Hospital has earned her several accolades and recognition from her peers.  
Head Entity: Emily  
Tail Entity: HealthFirst Hospital  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his thrilling novels, passed away peacefully in his sleep at his residence in los angeles, ca.  
Head Entity: john doe  
Tail Entity: ca.  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, died at the age of 79 in her home located in bel air, ca.  
Head Entity: elizabeth taylor  
Tail Entity: ca.  

Relation: person stateorprovince of death  
Context: the famous physicist, albert einstein, took his last breath in princeton, nj, where he had lived for many years.  
Head Entity: albert einstein  
Tail Entity: nj.  

Relation: person stateorprovince of death  
Context: the beloved musician, prince, was found unresponsive in his home in minneapolis, mn, leading to an outpouring of grief from fans worldwide.  
Head Entity: prince  
Tail Entity: mn.  

Relation: person stateorprovince of death  
Context: the legendary civil rights leader, martin luther king jr., was assassinated in memphis, tn, a tragic event that shocked the nation.  
Head Entity: martin luther king jr.  
Tail Entity: tn.  
Losses:  6.726737022399902 1.2723455429077148 0.6715174913406372
MemoryTrain:  epoch  0, batch     0 | loss: 6.7267370Losses:  9.943903923034668 3.235962152481079 0.6021977066993713
MemoryTrain:  epoch  0, batch     1 | loss: 9.9439039Losses:  10.273927688598633 4.0196309089660645 0.4114701747894287
MemoryTrain:  epoch  0, batch     2 | loss: 10.2739277Losses:  6.8016133308410645 1.6049100160598755 0.6219095587730408
MemoryTrain:  epoch  0, batch     3 | loss: 6.8016133Losses:  7.470108985900879 1.3963508605957031 0.6051123142242432
MemoryTrain:  epoch  0, batch     4 | loss: 7.4701090Losses:  6.119480133056641 0.9386541247367859 0.37952885031700134
MemoryTrain:  epoch  0, batch     5 | loss: 6.1194801Losses:  7.040848731994629 1.365419864654541 0.6222578287124634
MemoryTrain:  epoch  1, batch     0 | loss: 7.0408487Losses:  7.016624450683594 1.4547781944274902 0.5470916628837585
MemoryTrain:  epoch  1, batch     1 | loss: 7.0166245Losses:  7.790966033935547 3.300788640975952 0.5660336017608643
MemoryTrain:  epoch  1, batch     2 | loss: 7.7909660Losses:  6.310427188873291 1.7474255561828613 0.4975375533103943
MemoryTrain:  epoch  1, batch     3 | loss: 6.3104272Losses:  6.338622093200684 1.7377527952194214 0.5872547626495361
MemoryTrain:  epoch  1, batch     4 | loss: 6.3386221Losses:  5.851299285888672 1.2772643566131592 0.5251471996307373
MemoryTrain:  epoch  1, batch     5 | loss: 5.8512993Losses:  5.105743408203125 1.4784090518951416 0.5951812267303467
MemoryTrain:  epoch  2, batch     0 | loss: 5.1057434Losses:  4.762382507324219 1.0810573101043701 0.5147799849510193
MemoryTrain:  epoch  2, batch     1 | loss: 4.7623825Losses:  4.803873538970947 1.0810270309448242 0.3786866366863251
MemoryTrain:  epoch  2, batch     2 | loss: 4.8038735Losses:  5.475644588470459 1.4613945484161377 0.554205060005188
MemoryTrain:  epoch  2, batch     3 | loss: 5.4756446Losses:  5.137762069702148 0.7792292237281799 0.5430870652198792
MemoryTrain:  epoch  2, batch     4 | loss: 5.1377621Losses:  5.436925411224365 1.674651861190796 0.6319137811660767
MemoryTrain:  epoch  2, batch     5 | loss: 5.4369254Losses:  5.462647438049316 1.887345314025879 0.5613512992858887
MemoryTrain:  epoch  3, batch     0 | loss: 5.4626474Losses:  6.075389862060547 2.2688562870025635 0.5047875642776489
MemoryTrain:  epoch  3, batch     1 | loss: 6.0753899Losses:  4.920003890991211 1.8513487577438354 0.5154374837875366
MemoryTrain:  epoch  3, batch     2 | loss: 4.9200039Losses:  4.563394546508789 1.2459701299667358 0.6037506461143494
MemoryTrain:  epoch  3, batch     3 | loss: 4.5633945Losses:  5.1121931076049805 1.5770947933197021 0.42430830001831055
MemoryTrain:  epoch  3, batch     4 | loss: 5.1121931Losses:  5.434906482696533 1.970008134841919 0.633783221244812
MemoryTrain:  epoch  3, batch     5 | loss: 5.4349065Losses:  4.473480224609375 1.1967480182647705 0.5099613666534424
MemoryTrain:  epoch  4, batch     0 | loss: 4.4734802Losses:  4.872700214385986 1.6680431365966797 0.5440225601196289
MemoryTrain:  epoch  4, batch     1 | loss: 4.8727002Losses:  3.962688446044922 1.0176935195922852 0.5356025099754333
MemoryTrain:  epoch  4, batch     2 | loss: 3.9626884Losses:  4.365903854370117 1.0566819906234741 0.6138241291046143
MemoryTrain:  epoch  4, batch     3 | loss: 4.3659039Losses:  3.9980249404907227 1.0721027851104736 0.5256156325340271
MemoryTrain:  epoch  4, batch     4 | loss: 3.9980249Losses:  5.343804359436035 1.9584956169128418 0.28651946783065796
MemoryTrain:  epoch  4, batch     5 | loss: 5.3438044Losses:  6.19035530090332 3.5863232612609863 0.4084349572658539
MemoryTrain:  epoch  5, batch     0 | loss: 6.1903553Losses:  4.137480735778809 1.08198082447052 0.5411390066146851
MemoryTrain:  epoch  5, batch     1 | loss: 4.1374807Losses:  6.0081868171691895 2.756437063217163 0.4078372120857239
MemoryTrain:  epoch  5, batch     2 | loss: 6.0081868Losses:  5.572023868560791 2.5966813564300537 0.45449239015579224
MemoryTrain:  epoch  5, batch     3 | loss: 5.5720239Losses:  3.92423677444458 1.2589466571807861 0.5080334544181824
MemoryTrain:  epoch  5, batch     4 | loss: 3.9242368Losses:  4.763491630554199 1.7329139709472656 0.5137990117073059
MemoryTrain:  epoch  5, batch     5 | loss: 4.7634916Losses:  4.401803016662598 1.3514288663864136 0.49930375814437866
MemoryTrain:  epoch  6, batch     0 | loss: 4.4018030Losses:  4.36502742767334 1.6400922536849976 0.43453308939933777
MemoryTrain:  epoch  6, batch     1 | loss: 4.3650274Losses:  5.366663932800293 2.454251289367676 0.5110914707183838
MemoryTrain:  epoch  6, batch     2 | loss: 5.3666639Losses:  4.591507434844971 1.8506367206573486 0.5046000480651855
MemoryTrain:  epoch  6, batch     3 | loss: 4.5915074Losses:  4.621906280517578 1.736990213394165 0.4841787815093994
MemoryTrain:  epoch  6, batch     4 | loss: 4.6219063Losses:  4.01921272277832 1.1121490001678467 0.5874106287956238
MemoryTrain:  epoch  6, batch     5 | loss: 4.0192127Losses:  4.477468013763428 1.8137719631195068 0.584384560585022
MemoryTrain:  epoch  7, batch     0 | loss: 4.4774680Losses:  3.314336061477661 0.49781548976898193 0.5880444645881653
MemoryTrain:  epoch  7, batch     1 | loss: 3.3143361Losses:  4.461515426635742 1.870406150817871 0.5688260197639465
MemoryTrain:  epoch  7, batch     2 | loss: 4.4615154Losses:  4.859479904174805 2.1960678100585938 0.4924311339855194
MemoryTrain:  epoch  7, batch     3 | loss: 4.8594799Losses:  5.984859466552734 3.080451488494873 0.4777061343193054
MemoryTrain:  epoch  7, batch     4 | loss: 5.9848595Losses:  5.9335832595825195 3.0665645599365234 0.4587104320526123
MemoryTrain:  epoch  7, batch     5 | loss: 5.9335833Losses:  4.436212539672852 1.7002577781677246 0.5277671813964844
MemoryTrain:  epoch  8, batch     0 | loss: 4.4362125Losses:  3.4675188064575195 0.9775029420852661 0.4749603271484375
MemoryTrain:  epoch  8, batch     1 | loss: 3.4675188Losses:  4.697097301483154 2.0840296745300293 0.5770664215087891
MemoryTrain:  epoch  8, batch     2 | loss: 4.6970973Losses:  4.245561122894287 1.624494194984436 0.5794497728347778
MemoryTrain:  epoch  8, batch     3 | loss: 4.2455611Losses:  4.161118507385254 1.3831614255905151 0.5267804265022278
MemoryTrain:  epoch  8, batch     4 | loss: 4.1611185Losses:  4.049821376800537 1.5785064697265625 0.37299081683158875
MemoryTrain:  epoch  8, batch     5 | loss: 4.0498214Losses:  4.208012580871582 1.578505277633667 0.4219858646392822
MemoryTrain:  epoch  9, batch     0 | loss: 4.2080126Losses:  4.532280445098877 2.166104793548584 0.35864201188087463
MemoryTrain:  epoch  9, batch     1 | loss: 4.5322804Losses:  4.2963151931762695 1.7302117347717285 0.4945235252380371
MemoryTrain:  epoch  9, batch     2 | loss: 4.2963152Losses:  3.8954970836639404 1.4325876235961914 0.46323299407958984
MemoryTrain:  epoch  9, batch     3 | loss: 3.8954971Losses:  4.251974582672119 1.6216191053390503 0.491744726896286
MemoryTrain:  epoch  9, batch     4 | loss: 4.2519746Losses:  4.21162223815918 1.6418648958206177 0.5676643252372742
MemoryTrain:  epoch  9, batch     5 | loss: 4.2116222
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 77.68%   
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 12.50%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 13.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 13.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 22.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 39.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 45.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 53.65%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 58.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 59.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 59.87%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 61.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 69.95%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 73.79%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 74.41%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 74.81%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 74.08%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 73.21%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 72.57%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 72.64%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 73.19%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 74.53%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 74.85%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 74.27%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 73.44%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 72.78%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 71.60%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 71.01%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 70.96%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 71.50%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 71.20%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 70.91%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 70.99%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 71.18%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 72.10%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 72.48%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 72.63%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 73.23%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 72.34%   
cur_acc:  ['0.8693', '0.7708', '0.7768']
his_acc:  ['0.8693', '0.7939', '0.7234']
Clustering into  9  clusters
Clusters:  [4 1 1 8 0 7 4 2 5 0 3 2 4 3 1 4 4 6 4 3 0]
Losses:  14.476189613342285 8.310333251953125 0.4304829239845276
CurrentTrain: epoch  0, batch     0 | loss: 14.4761896Losses:  8.245575904846191 2.647786855697632 0.4146481156349182
CurrentTrain: epoch  0, batch     1 | loss: 8.2455759Losses:  12.733124732971191 7.4346795082092285 0.38349461555480957
CurrentTrain: epoch  1, batch     0 | loss: 12.7331247Losses:  6.361281871795654 2.037346839904785 0.38481464982032776
CurrentTrain: epoch  1, batch     1 | loss: 6.3612819Losses:  10.874910354614258 6.760884761810303 0.36629366874694824
CurrentTrain: epoch  2, batch     0 | loss: 10.8749104Losses:  6.035064220428467 2.218135356903076 0.3470284342765808
CurrentTrain: epoch  2, batch     1 | loss: 6.0350642Losses:  11.563992500305176 7.789028167724609 0.34029465913772583
CurrentTrain: epoch  3, batch     0 | loss: 11.5639925Losses:  6.009463310241699 2.8883748054504395 0.35582560300827026
CurrentTrain: epoch  3, batch     1 | loss: 6.0094633Losses:  9.867679595947266 6.725832939147949 0.35486072301864624
CurrentTrain: epoch  4, batch     0 | loss: 9.8676796Losses:  6.175841808319092 2.6155288219451904 0.3225272297859192
CurrentTrain: epoch  4, batch     1 | loss: 6.1758418Losses:  9.100678443908691 6.1800761222839355 0.32362547516822815
CurrentTrain: epoch  5, batch     0 | loss: 9.1006784Losses:  5.123887538909912 2.1025407314300537 0.3337024748325348
CurrentTrain: epoch  5, batch     1 | loss: 5.1238875Losses:  8.986632347106934 5.973755836486816 0.31701284646987915
CurrentTrain: epoch  6, batch     0 | loss: 8.9866323Losses:  4.451912879943848 1.8528954982757568 0.3083741068840027
CurrentTrain: epoch  6, batch     1 | loss: 4.4519129Losses:  8.846896171569824 6.427003383636475 0.30514028668403625
CurrentTrain: epoch  7, batch     0 | loss: 8.8468962Losses:  5.114554405212402 2.396135091781616 0.30519017577171326
CurrentTrain: epoch  7, batch     1 | loss: 5.1145544Losses:  8.107074737548828 5.59916877746582 0.31031787395477295
CurrentTrain: epoch  8, batch     0 | loss: 8.1070747Losses:  3.562244415283203 1.160089373588562 0.28842926025390625
CurrentTrain: epoch  8, batch     1 | loss: 3.5622444Losses:  9.131834983825684 6.696775436401367 0.2938055396080017
CurrentTrain: epoch  9, batch     0 | loss: 9.1318350Losses:  5.224715709686279 2.9824931621551514 0.28662726283073425
CurrentTrain: epoch  9, batch     1 | loss: 5.2247157
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones found her new home in san francisco, where she works as a software engineer. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith spent a lot of time in boston, where he attended university and made lifelong friends. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  

Relation: person cities of residence  
Context: -lrb- following her promotion, emily davis relocated to seattle, excited about the opportunities in the tech industry. -rrb-  
Head Entity: emily davis  
Tail Entity: seattle  

Relation: person cities of residence  
Context: -lrb- after retiring, john doe decided to settle down in miami, enjoying the warm weather and vibrant culture. -rrb-  
Head Entity: john doe  
Tail Entity: miami  

Relation: person cities of residence  
Context: -lrb- having grown up in new york, alice brown returned to her roots after years of living in los angeles. -rrb-  
Head Entity: alice brown  
Tail Entity: new york  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, John enrolled at Stanford University to study computer science.  
Head Entity: John  
Tail Entity: Stanford University  

Relation: person schools attended  
Context: Maria attended the University of Oxford for her master's degree in literature, where she developed a passion for writing.  
Head Entity: Maria  
Tail Entity: University of Oxford  

Relation: person schools attended  
Context: Following his time in the military, David went to the Massachusetts Institute of Technology to study engineering.  
Head Entity: David  
Tail Entity: Massachusetts Institute of Technology  

Relation: person schools attended  
Context: Emily completed her undergraduate studies at the University of California, Berkeley, before moving on to law school.  
Head Entity: Emily  
Tail Entity: University of California, Berkeley  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: there were doubts as to whether she would perform in the `` britain 's got talent '' live show in the scottish city after she pulled out of an event in manchester , northwestern england , on sunday night .
Head Entity: she
Tail Entity: scottish
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: The renowned author passed away in a small village in the south of France, where he had spent his final years writing.  
Head Entity: The renowned author  
Tail Entity: France  

Relation: person country of death  
Context: After a long battle with illness, the famous musician died in a hospital located in the heart of Los Angeles, California.  
Head Entity: The famous musician  
Tail Entity: California  

Relation: person country of death  
Context: Following a tragic accident, the beloved actor was pronounced dead at a medical facility in the bustling city of Tokyo, Japan.  
Head Entity: The beloved actor  
Tail Entity: Japan  

Relation: person country of death  
Context: The influential political leader passed away peacefully in his home country, surrounded by family and friends in the capital city of Nairobi, Kenya.  
Head Entity: The influential political leader  
Tail Entity: Kenya  

Relation: person country of death  
Context: The celebrated scientist died unexpectedly while attending a conference in the historic city of Rome, Italy.  
Head Entity: The celebrated scientist  
Tail Entity: Italy  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by his wife of 63 years , josephine robinson mcnair , of columbia ; a son , robert e. jr. , of columbia ; three daughters , robin lee howell and corinne godshall , of myrtle beach , s.c. , and claudia crawford mcnair , of jamestown , s.c. ; six grandchildren ; and one great-grandchild .
Head Entity: he
Tail Entity: claudia crawford mcnair
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: After the passing of her husband, she dedicated her life to raising their three children, who are now successful adults living in different states.  
Head Entity: she  
Tail Entity: their three children  

Relation: person children  
Context: The famous actor often shares stories about his childhood and the lessons he learned from his parents, especially his mother, who raised him and his siblings alone.  
Head Entity: his mother  
Tail Entity: his siblings  

Relation: person children  
Context: During the family reunion, he proudly introduced his children to his cousins, highlighting their achievements in school and sports.  
Head Entity: he  
Tail Entity: his children  

Relation: person children  
Context: The renowned author often mentions her daughter in interviews, expressing how much she admires her creativity and passion for writing.  
Head Entity: she  
Tail Entity: her daughter  

Relation: person children  
Context: As a single father, he has always prioritized his children's education, ensuring they have the best opportunities to succeed in life.  
Head Entity: he  
Tail Entity: his children  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: previously , al-khawinay was sentenced to one year in jail for supporting the country 's minority shiite rebels and defaming the president , but was later pardoned by president ali abdullah saleh .
Head Entity: al-khawinay
Tail Entity: defaming the president
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: In 2020, the former mayor was charged with embezzlement after an investigation revealed he misappropriated city funds for personal use.  
Head Entity: former mayor  
Tail Entity: embezzlement  

Relation: person charges  
Context: The athlete faced serious allegations and was charged with doping violations after a routine test showed banned substances in his system.  
Head Entity: athlete  
Tail Entity: doping violations  

Relation: person charges  
Context: Following the protests, several activists were charged with inciting violence and disturbing the peace, leading to widespread condemnation.  
Head Entity: activists  
Tail Entity: inciting violence  

Relation: person charges  
Context: The CEO was charged with fraud after an internal audit uncovered discrepancies in the company's financial statements.  
Head Entity: CEO  
Tail Entity: fraud  

Relation: person charges  
Context: After a lengthy investigation, the journalist was charged with breaching national security laws for leaking classified information.  
Head Entity: journalist  
Tail Entity: breaching national security laws  
Losses:  6.867663383483887 1.7656553983688354 0.4924046993255615
MemoryTrain:  epoch  0, batch     0 | loss: 6.8676634Losses:  7.098407745361328 3.4178662300109863 0.6962053775787354
MemoryTrain:  epoch  0, batch     1 | loss: 7.0984077Losses:  7.463801383972168 3.0963568687438965 0.5349299907684326
MemoryTrain:  epoch  0, batch     2 | loss: 7.4638014Losses:  6.908938407897949 1.4271111488342285 0.5847184062004089
MemoryTrain:  epoch  0, batch     3 | loss: 6.9089384Losses:  7.682644844055176 2.229060649871826 0.5159879326820374
MemoryTrain:  epoch  0, batch     4 | loss: 7.6826448Losses:  6.576621055603027 1.8291738033294678 0.44192782044410706
MemoryTrain:  epoch  0, batch     5 | loss: 6.5766211Losses:  7.148250579833984 1.9117069244384766 0.5053710341453552
MemoryTrain:  epoch  0, batch     6 | loss: 7.1482506Losses:  6.066826343536377 1.4482697248458862 0.5426009893417358
MemoryTrain:  epoch  0, batch     7 | loss: 6.0668263Losses:  4.700754165649414 0.45457547903060913 0.6627195477485657
MemoryTrain:  epoch  1, batch     0 | loss: 4.7007542Losses:  6.230043411254883 2.9694230556488037 0.5767829418182373
MemoryTrain:  epoch  1, batch     1 | loss: 6.2300434Losses:  5.39587926864624 1.6826930046081543 0.4986998438835144
MemoryTrain:  epoch  1, batch     2 | loss: 5.3958793Losses:  7.616064071655273 3.5163540840148926 0.478728711605072
MemoryTrain:  epoch  1, batch     3 | loss: 7.6160641Losses:  5.706846714019775 1.5294955968856812 0.6285978555679321
MemoryTrain:  epoch  1, batch     4 | loss: 5.7068467Losses:  6.455577850341797 1.9312893152236938 0.5154774188995361
MemoryTrain:  epoch  1, batch     5 | loss: 6.4555779Losses:  4.672772407531738 1.343517541885376 0.557091474533081
MemoryTrain:  epoch  1, batch     6 | loss: 4.6727724Losses:  5.482321262359619 1.6147980690002441 0.429852157831192
MemoryTrain:  epoch  1, batch     7 | loss: 5.4823213Losses:  4.060947418212891 0.5514821410179138 0.6470687389373779
MemoryTrain:  epoch  2, batch     0 | loss: 4.0609474Losses:  3.671055316925049 0.7012421488761902 0.5045888423919678
MemoryTrain:  epoch  2, batch     1 | loss: 3.6710553Losses:  4.189160346984863 1.165008544921875 0.5733184814453125
MemoryTrain:  epoch  2, batch     2 | loss: 4.1891603Losses:  6.229761123657227 2.3676886558532715 0.6701239347457886
MemoryTrain:  epoch  2, batch     3 | loss: 6.2297611Losses:  4.13982629776001 0.7253649830818176 0.5555457472801208
MemoryTrain:  epoch  2, batch     4 | loss: 4.1398263Losses:  4.206051826477051 1.0968533754348755 0.7073473334312439
MemoryTrain:  epoch  2, batch     5 | loss: 4.2060518Losses:  5.535567283630371 2.0894951820373535 0.5948569774627686
MemoryTrain:  epoch  2, batch     6 | loss: 5.5355673Losses:  4.155025005340576 0.7205461859703064 0.4367061257362366
MemoryTrain:  epoch  2, batch     7 | loss: 4.1550250Losses:  4.616756439208984 1.0992718935012817 0.5702983736991882
MemoryTrain:  epoch  3, batch     0 | loss: 4.6167564Losses:  5.625154972076416 2.6294312477111816 0.5318335890769958
MemoryTrain:  epoch  3, batch     1 | loss: 5.6251550Losses:  4.0617218017578125 1.136634111404419 0.5452299118041992
MemoryTrain:  epoch  3, batch     2 | loss: 4.0617218Losses:  4.453296184539795 1.6332225799560547 0.5901850461959839
MemoryTrain:  epoch  3, batch     3 | loss: 4.4532962Losses:  3.7105512619018555 0.7317497730255127 0.6661902666091919
MemoryTrain:  epoch  3, batch     4 | loss: 3.7105513Losses:  4.459924697875977 1.7040420770645142 0.4187791049480438
MemoryTrain:  epoch  3, batch     5 | loss: 4.4599247Losses:  3.800705909729004 1.0205802917480469 0.4692321717739105
MemoryTrain:  epoch  3, batch     6 | loss: 3.8007059Losses:  3.735617160797119 0.6175575256347656 0.5109437704086304
MemoryTrain:  epoch  3, batch     7 | loss: 3.7356172Losses:  3.738736867904663 0.9573536515235901 0.647840678691864
MemoryTrain:  epoch  4, batch     0 | loss: 3.7387369Losses:  4.093514442443848 1.3404827117919922 0.6349453330039978
MemoryTrain:  epoch  4, batch     1 | loss: 4.0935144Losses:  4.015018463134766 1.2868177890777588 0.49014073610305786
MemoryTrain:  epoch  4, batch     2 | loss: 4.0150185Losses:  3.9265799522399902 0.808708667755127 0.46708595752716064
MemoryTrain:  epoch  4, batch     3 | loss: 3.9265800Losses:  4.369687557220459 1.1791906356811523 0.564510703086853
MemoryTrain:  epoch  4, batch     4 | loss: 4.3696876Losses:  3.546426296234131 0.7436121702194214 0.4758819341659546
MemoryTrain:  epoch  4, batch     5 | loss: 3.5464263Losses:  5.342090129852295 2.4151010513305664 0.6701374650001526
MemoryTrain:  epoch  4, batch     6 | loss: 5.3420901Losses:  3.289839267730713 0.7251328229904175 0.5625201463699341
MemoryTrain:  epoch  4, batch     7 | loss: 3.2898393Losses:  7.5582122802734375 4.675173282623291 0.5355292558670044
MemoryTrain:  epoch  5, batch     0 | loss: 7.5582123Losses:  3.2076525688171387 0.4867863655090332 0.6176579594612122
MemoryTrain:  epoch  5, batch     1 | loss: 3.2076526Losses:  4.559535026550293 1.8427562713623047 0.47295546531677246
MemoryTrain:  epoch  5, batch     2 | loss: 4.5595350Losses:  3.899728298187256 1.2451097965240479 0.6489754915237427
MemoryTrain:  epoch  5, batch     3 | loss: 3.8997283Losses:  5.065655708312988 2.119016408920288 0.49373048543930054
MemoryTrain:  epoch  5, batch     4 | loss: 5.0656557Losses:  3.7473220825195312 0.9975298047065735 0.5848499536514282
MemoryTrain:  epoch  5, batch     5 | loss: 3.7473221Losses:  3.2724742889404297 0.4993725121021271 0.47900208830833435
MemoryTrain:  epoch  5, batch     6 | loss: 3.2724743Losses:  3.4968109130859375 0.7653598189353943 0.5711206197738647
MemoryTrain:  epoch  5, batch     7 | loss: 3.4968109Losses:  4.386202812194824 1.5894274711608887 0.5630591511726379
MemoryTrain:  epoch  6, batch     0 | loss: 4.3862028Losses:  3.4667768478393555 0.9884544014930725 0.4752473831176758
MemoryTrain:  epoch  6, batch     1 | loss: 3.4667768Losses:  4.173090934753418 1.4414043426513672 0.6343474388122559
MemoryTrain:  epoch  6, batch     2 | loss: 4.1730909Losses:  4.053011894226074 1.4267282485961914 0.46476444602012634
MemoryTrain:  epoch  6, batch     3 | loss: 4.0530119Losses:  4.052680015563965 1.423539400100708 0.41288185119628906
MemoryTrain:  epoch  6, batch     4 | loss: 4.0526800Losses:  3.972097158432007 1.3864567279815674 0.5532488822937012
MemoryTrain:  epoch  6, batch     5 | loss: 3.9720972Losses:  5.057035446166992 2.603628158569336 0.46880096197128296
MemoryTrain:  epoch  6, batch     6 | loss: 5.0570354Losses:  3.317159414291382 0.7068877816200256 0.5493115782737732
MemoryTrain:  epoch  6, batch     7 | loss: 3.3171594Losses:  3.7527637481689453 0.9974067211151123 0.6308506727218628
MemoryTrain:  epoch  7, batch     0 | loss: 3.7527637Losses:  3.701287031173706 1.242071509361267 0.47300148010253906
MemoryTrain:  epoch  7, batch     1 | loss: 3.7012870Losses:  4.701020240783691 2.0264482498168945 0.49902230501174927
MemoryTrain:  epoch  7, batch     2 | loss: 4.7010202Losses:  3.215304374694824 0.6987956762313843 0.4666023850440979
MemoryTrain:  epoch  7, batch     3 | loss: 3.2153044Losses:  4.051458358764648 1.4122675657272339 0.5602166652679443
MemoryTrain:  epoch  7, batch     4 | loss: 4.0514584Losses:  3.0388989448547363 0.5163688659667969 0.48471367359161377
MemoryTrain:  epoch  7, batch     5 | loss: 3.0388989Losses:  6.415630340576172 3.8896877765655518 0.5479140877723694
MemoryTrain:  epoch  7, batch     6 | loss: 6.4156303Losses:  3.8075122833251953 1.2075681686401367 0.5601018667221069
MemoryTrain:  epoch  7, batch     7 | loss: 3.8075123Losses:  3.3291373252868652 0.5142090916633606 0.6909859776496887
MemoryTrain:  epoch  8, batch     0 | loss: 3.3291373Losses:  3.863644599914551 1.3374744653701782 0.5490765571594238
MemoryTrain:  epoch  8, batch     1 | loss: 3.8636446Losses:  3.4936866760253906 0.9975070953369141 0.46719425916671753
MemoryTrain:  epoch  8, batch     2 | loss: 3.4936867Losses:  3.8881607055664062 1.3385759592056274 0.5476131439208984
MemoryTrain:  epoch  8, batch     3 | loss: 3.8881607Losses:  4.301634311676025 1.8414783477783203 0.47958216071128845
MemoryTrain:  epoch  8, batch     4 | loss: 4.3016343Losses:  3.3191745281219482 0.8934744596481323 0.5521318316459656
MemoryTrain:  epoch  8, batch     5 | loss: 3.3191745Losses:  3.400082588195801 0.8298896551132202 0.5284785628318787
MemoryTrain:  epoch  8, batch     6 | loss: 3.4000826Losses:  4.270226955413818 1.7623283863067627 0.43573710322380066
MemoryTrain:  epoch  8, batch     7 | loss: 4.2702270Losses:  3.510272979736328 0.9757694005966187 0.5580259561538696
MemoryTrain:  epoch  9, batch     0 | loss: 3.5102730Losses:  3.5449366569519043 0.988071084022522 0.5648379921913147
MemoryTrain:  epoch  9, batch     1 | loss: 3.5449367Losses:  3.553445816040039 1.094820261001587 0.46722230315208435
MemoryTrain:  epoch  9, batch     2 | loss: 3.5534458Losses:  4.2962870597839355 1.807917594909668 0.5716556310653687
MemoryTrain:  epoch  9, batch     3 | loss: 4.2962871Losses:  3.4158802032470703 0.9312048554420471 0.5451313853263855
MemoryTrain:  epoch  9, batch     4 | loss: 3.4158802Losses:  4.0128631591796875 1.4523688554763794 0.5504012107849121
MemoryTrain:  epoch  9, batch     5 | loss: 4.0128632Losses:  4.0567626953125 1.5551334619522095 0.5397115349769592
MemoryTrain:  epoch  9, batch     6 | loss: 4.0567627Losses:  3.1777682304382324 0.5219284892082214 0.6340438723564148
MemoryTrain:  epoch  9, batch     7 | loss: 3.1777682
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 59.03%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 73.90%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 71.18%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 13.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 24.22%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 51.44%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 55.90%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 58.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.86%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 68.52%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 69.64%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 72.85%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 71.88%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 70.71%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 69.62%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 69.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 71.49%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 72.02%   [EVAL] batch:   42 | acc: 37.50%,  total acc: 71.22%   [EVAL] batch:   43 | acc: 18.75%,  total acc: 70.03%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 69.44%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 68.34%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 67.69%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 67.58%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 67.47%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 66.62%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 65.69%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 65.02%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 64.39%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 64.24%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 66.35%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 66.09%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 65.83%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:   63 | acc: 31.25%,  total acc: 65.43%   [EVAL] batch:   64 | acc: 43.75%,  total acc: 65.10%   [EVAL] batch:   65 | acc: 31.25%,  total acc: 64.58%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 64.37%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 64.71%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 64.95%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 65.14%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 65.28%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 67.63%   
cur_acc:  ['0.8693', '0.7708', '0.7768', '0.7118']
his_acc:  ['0.8693', '0.7939', '0.7234', '0.6763']
Clustering into  12  clusters
Clusters:  [ 4  3  0 10 11  6  4  2  7  9  1  2  4  1  3  4  4  5  4  1  8  0  0  0
  6  1]
Losses:  17.386032104492188 8.052210807800293 0.20604029297828674
CurrentTrain: epoch  0, batch     0 | loss: 17.3860321Losses:  12.101673126220703 2.8492825031280518 0.28363707661628723
CurrentTrain: epoch  0, batch     1 | loss: 12.1016731Losses:  17.04400062561035 9.436234474182129 0.2515234351158142
CurrentTrain: epoch  1, batch     0 | loss: 17.0440006Losses:  17.603670120239258 7.31128454208374 0.13741427659988403
CurrentTrain: epoch  1, batch     1 | loss: 17.6036701Losses:  16.450241088867188 8.050785064697266 0.19321691989898682
CurrentTrain: epoch  2, batch     0 | loss: 16.4502411Losses:  10.011857986450195 2.9001073837280273 0.22341153025627136
CurrentTrain: epoch  2, batch     1 | loss: 10.0118580Losses:  14.295915603637695 7.136099815368652 0.19154421985149384
CurrentTrain: epoch  3, batch     0 | loss: 14.2959156Losses:  10.295207977294922 2.614712715148926 0.19383814930915833
CurrentTrain: epoch  3, batch     1 | loss: 10.2952080Losses:  14.093023300170898 6.8502702713012695 0.17628389596939087
CurrentTrain: epoch  4, batch     0 | loss: 14.0930233Losses:  7.988385200500488 1.632777452468872 0.17844288051128387
CurrentTrain: epoch  4, batch     1 | loss: 7.9883852Losses:  13.56612777709961 6.828000068664551 0.17369413375854492
CurrentTrain: epoch  5, batch     0 | loss: 13.5661278Losses:  7.373350143432617 1.4095121622085571 0.1708744466304779
CurrentTrain: epoch  5, batch     1 | loss: 7.3733501Losses:  12.414351463317871 6.2601518630981445 0.16559754312038422
CurrentTrain: epoch  6, batch     0 | loss: 12.4143515Losses:  8.334829330444336 1.9438673257827759 0.17377230525016785
CurrentTrain: epoch  6, batch     1 | loss: 8.3348293Losses:  14.989726066589355 8.340877532958984 0.19539408385753632
CurrentTrain: epoch  7, batch     0 | loss: 14.9897261Losses:  7.490720272064209 2.194409132003784 0.17270347476005554
CurrentTrain: epoch  7, batch     1 | loss: 7.4907203Losses:  12.626721382141113 6.680170059204102 0.1868799924850464
CurrentTrain: epoch  8, batch     0 | loss: 12.6267214Losses:  9.55757999420166 2.559196710586548 0.16518235206604004
CurrentTrain: epoch  8, batch     1 | loss: 9.5575800Losses:  12.607866287231445 6.542572975158691 0.15909205377101898
CurrentTrain: epoch  9, batch     0 | loss: 12.6078663Losses:  8.088546752929688 2.175285816192627 0.18360374867916107
CurrentTrain: epoch  9, batch     1 | loss: 8.0885468
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: `` firstgroup 's acquisition of laidlaw will considerably enhance firstgroup 's existing activities in north america , which themselves have grown strongly since we first invested in the u.s. in 1999 , '' said firstgroup chief executive moir lockhead .
Head Entity: firstgroup
Tail Entity: laidlaw
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: `` in 2018, the tech giant google announced its acquisition of nest labs, a company specializing in smart home products, which has allowed google to expand its reach in the home automation market. ''  
Head Entity: google  
Tail Entity: nest labs  

Relation: organization subsidiaries  
Context: `` the merger between disney and pixar in 2006 created a powerhouse in animated films, allowing disney to leverage pixar's innovative technology and storytelling expertise. ''  
Head Entity: disney  
Tail Entity: pixar  

Relation: organization subsidiaries  
Context: `` after acquiring the popular video game company, activision blizzard, microsoft significantly bolstered its gaming portfolio and presence in the industry. ''  
Head Entity: microsoft  
Tail Entity: activision blizzard  

Relation: organization subsidiaries  
Context: `` the recent purchase of whole foods by amazon has transformed the grocery shopping experience, integrating online and offline retail in a unique way. ''  
Head Entity: amazon  
Tail Entity: whole foods  

Relation: organization subsidiaries  
Context: `` in a strategic move to enhance its product offerings, facebook acquired instagram, allowing it to tap into the growing trend of photo-sharing among users. ''  
Head Entity: facebook  
Tail Entity: instagram  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: The tech giant Alphabet Inc. has been making headlines with its innovative projects, but many are unaware that it is the parent company of Google, which has revolutionized the way we access information online.  
Head Entity: Alphabet Inc.  
Tail Entity: Google  

Relation: organization parents  
Context: In the world of finance, JPMorgan Chase & Co. has a significant influence, especially since it is the parent organization of several well-known banks, including Chase Bank, which serves millions of customers across the United States.  
Head Entity: JPMorgan Chase & Co.  
Tail Entity: Chase Bank  

Relation: organization parents  
Context: The Walt Disney Company has expanded its empire over the years, and it is now the parent organization of Pixar Animation Studios, which has produced some of the most beloved animated films in history.  
Head Entity: The Walt Disney Company  
Tail Entity: Pixar Animation Studios  

Relation: organization parents  
Context: In the realm of social media, Facebook, Inc. has grown tremendously and is now the parent company of Instagram, a platform that has transformed the way people share photos and connect with each other.  
Head Entity: Facebook, Inc.  
Tail Entity: Instagram  

Relation: organization parents  
Context: The pharmaceutical industry is heavily influenced by large corporations, and Pfizer Inc. stands out as a major player, being the parent organization of Upjohn, which specializes in off-patent branded and generic medicines.  
Head Entity: Pfizer Inc.  
Tail Entity: Upjohn  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the world health organization -lrb- who -rrb- has been instrumental in coordinating global responses to health emergencies.  
Head Entity: world health organization  
Tail Entity: who  

Relation: organization alternate names  
Context: the federal bureau of investigation -lrb- fbi -rrb- is known for its role in investigating federal crimes in the United States.  
Head Entity: federal bureau of investigation  
Tail Entity: fbi  

Relation: organization alternate names  
Context: the international monetary fund -lrb- imf -rrb- provides financial assistance and advice to member countries.  
Head Entity: international monetary fund  
Tail Entity: imf  

Relation: organization alternate names  
Context: the national aeronautics and space administration -lrb- nasa -rrb- is responsible for the nation's civilian space program and for aeronautics and aerospace research.  
Head Entity: national aeronautics and space administration  
Tail Entity: nasa  

Relation: organization alternate names  
Context: the united nations educational, scientific and cultural organization -lrb- unesco -rrb- aims to promote world peace and security through international cooperation in education, the sciences, and culture.  
Head Entity: united nations educational, scientific and cultural organization  
Tail Entity: unesco  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: the justice and equality movement -lrb- jem -rrb- , a major rebel movement in darfur , is still insisting on its conditions for the resumption of the talks , including the release of dozens of rebel fighters who were arrested after an attack in omdurman near khartoum in may 2008 .
Head Entity: justice and equality movement
Tail Entity: darfur
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the tech giant apple inc. has announced plans to expand its operations in cupertino, where its headquarters is located, aiming to create thousands of new jobs in the area.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: the international red cross, known for its humanitarian efforts, has its main office situated in geneva, a city renowned for its diplomatic significance.  
Head Entity: international red cross  
Tail Entity: geneva  

Relation: organization city of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, a city that has become a hub for technology and innovation in south korea.  
Head Entity: samsung electronics  
Tail Entity: suwon  

Relation: organization city of headquarters  
Context: the united nations, an intergovernmental organization founded in 1945, has its headquarters in new york city, which serves as a central point for international diplomacy.  
Head Entity: united nations  
Tail Entity: new york city  

Relation: organization city of headquarters  
Context: the famous automotive manufacturer toyota motor corporation is headquartered in toyota city, a place that reflects its deep roots in the automotive industry.  
Head Entity: toyota motor corporation  
Tail Entity: toyota city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: After the family reunion, it was clear that John and his brother, Michael, had grown even closer, sharing stories and laughter throughout the weekend.  
Head Entity: John  
Tail Entity: Michael  

Relation: person siblings  
Context: During the interview, Sarah mentioned how her sister, Emily, always supported her through tough times, making their bond stronger than ever.  
Head Entity: Sarah  
Tail Entity: Emily  

Relation: person siblings  
Context: The documentary highlighted the lives of the famous musicians, focusing on how the bond between the sisters, Lisa and Rachel, influenced their music careers.  
Head Entity: Lisa  
Tail Entity: Rachel  

Relation: person siblings  
Context: At the award ceremony, David proudly accepted the trophy, dedicating it to his sister, Anna, who had always been his biggest inspiration.  
Head Entity: David  
Tail Entity: Anna  

Relation: person siblings  
Context: In her memoir, the author reflects on her childhood memories with her brother, Tom, and how their adventures shaped their lives.  
Head Entity: The author  
Tail Entity: Tom  
Losses:  5.344301223754883 0.2509620189666748 0.6877965927124023
MemoryTrain:  epoch  0, batch     0 | loss: 5.3443012Losses:  6.726959228515625 1.8679910898208618 0.7087972164154053
MemoryTrain:  epoch  0, batch     1 | loss: 6.7269592Losses:  6.227289199829102 1.3401464223861694 0.5679538249969482
MemoryTrain:  epoch  0, batch     2 | loss: 6.2272892Losses:  5.267617225646973 0.7795494198799133 0.6717357635498047
MemoryTrain:  epoch  0, batch     3 | loss: 5.2676172Losses:  5.550732612609863 1.9656742811203003 0.5497534275054932
MemoryTrain:  epoch  0, batch     4 | loss: 5.5507326Losses:  6.960629463195801 1.5842467546463013 0.5443114638328552
MemoryTrain:  epoch  0, batch     5 | loss: 6.9606295Losses:  5.88218879699707 0.6438028812408447 0.7294912934303284
MemoryTrain:  epoch  0, batch     6 | loss: 5.8821888Losses:  5.770097255706787 0.5717334747314453 0.7115195989608765
MemoryTrain:  epoch  0, batch     7 | loss: 5.7700973Losses:  8.150822639465332 2.6152164936065674 0.695128858089447
MemoryTrain:  epoch  0, batch     8 | loss: 8.1508226Losses:  4.435311317443848 0.48627567291259766 0.46867164969444275
MemoryTrain:  epoch  0, batch     9 | loss: 4.4353113Losses:  4.963943004608154 0.49041444063186646 0.7164105176925659
MemoryTrain:  epoch  1, batch     0 | loss: 4.9639430Losses:  4.075351715087891 0.25927048921585083 0.793928325176239
MemoryTrain:  epoch  1, batch     1 | loss: 4.0753517Losses:  4.073613166809082 0.464685320854187 0.8414866924285889
MemoryTrain:  epoch  1, batch     2 | loss: 4.0736132Losses:  5.66350793838501 1.2904584407806396 0.7078431844711304
MemoryTrain:  epoch  1, batch     3 | loss: 5.6635079Losses:  5.702688217163086 1.6204789876937866 0.5597498416900635
MemoryTrain:  epoch  1, batch     4 | loss: 5.7026882Losses:  4.800797462463379 0.5372316837310791 0.5722007751464844
MemoryTrain:  epoch  1, batch     5 | loss: 4.8007975Losses:  7.838653087615967 2.497899055480957 0.7373978495597839
MemoryTrain:  epoch  1, batch     6 | loss: 7.8386531Losses:  4.162591457366943 0.24137553572654724 0.6378828287124634
MemoryTrain:  epoch  1, batch     7 | loss: 4.1625915Losses:  5.239620685577393 0.9967458248138428 0.5643707513809204
MemoryTrain:  epoch  1, batch     8 | loss: 5.2396207Losses:  3.982607364654541 0.5537129640579224 0.622014045715332
MemoryTrain:  epoch  1, batch     9 | loss: 3.9826074Losses:  5.126194000244141 1.0669338703155518 0.6983311176300049
MemoryTrain:  epoch  2, batch     0 | loss: 5.1261940Losses:  6.354145050048828 2.588585376739502 0.5981613397598267
MemoryTrain:  epoch  2, batch     1 | loss: 6.3541451Losses:  4.4849467277526855 0.4854762554168701 0.6247695088386536
MemoryTrain:  epoch  2, batch     2 | loss: 4.4849467Losses:  6.588673114776611 1.932000994682312 0.5693745017051697
MemoryTrain:  epoch  2, batch     3 | loss: 6.5886731Losses:  5.216573238372803 2.5967843532562256 0.38371312618255615
MemoryTrain:  epoch  2, batch     4 | loss: 5.2165732Losses:  5.288101673126221 1.725860357284546 0.5708488821983337
MemoryTrain:  epoch  2, batch     5 | loss: 5.2881017Losses:  3.8823013305664062 0.47038960456848145 0.7092234492301941
MemoryTrain:  epoch  2, batch     6 | loss: 3.8823013Losses:  7.297455310821533 3.299628496170044 0.522860586643219
MemoryTrain:  epoch  2, batch     7 | loss: 7.2974553Losses:  3.612055540084839 0.48601341247558594 0.7839760184288025
MemoryTrain:  epoch  2, batch     8 | loss: 3.6120555Losses:  3.287804365158081 0.28279635310173035 0.5686958432197571
MemoryTrain:  epoch  2, batch     9 | loss: 3.2878044Losses:  4.320527076721191 0.7593259811401367 0.8547719120979309
MemoryTrain:  epoch  3, batch     0 | loss: 4.3205271Losses:  4.107706069946289 0.4984484910964966 0.772821843624115
MemoryTrain:  epoch  3, batch     1 | loss: 4.1077061Losses:  4.15955114364624 1.3455748558044434 0.5764498114585876
MemoryTrain:  epoch  3, batch     2 | loss: 4.1595511Losses:  4.447920322418213 0.7384175062179565 0.6472237706184387
MemoryTrain:  epoch  3, batch     3 | loss: 4.4479203Losses:  5.102570533752441 1.2859536409378052 0.5849735140800476
MemoryTrain:  epoch  3, batch     4 | loss: 5.1025705Losses:  3.5539865493774414 0.5176067352294922 0.5823839902877808
MemoryTrain:  epoch  3, batch     5 | loss: 3.5539865Losses:  4.061490535736084 1.1030993461608887 0.6496952772140503
MemoryTrain:  epoch  3, batch     6 | loss: 4.0614905Losses:  3.991739511489868 1.0329225063323975 0.5046328902244568
MemoryTrain:  epoch  3, batch     7 | loss: 3.9917395Losses:  4.452927589416504 1.0685614347457886 0.6436527371406555
MemoryTrain:  epoch  3, batch     8 | loss: 4.4529276Losses:  3.5158393383026123 0.5538307428359985 0.4837539792060852
MemoryTrain:  epoch  3, batch     9 | loss: 3.5158393Losses:  5.921002388000488 2.609732151031494 0.5743017196655273
MemoryTrain:  epoch  4, batch     0 | loss: 5.9210024Losses:  3.5544304847717285 0.5333194732666016 0.5591099262237549
MemoryTrain:  epoch  4, batch     1 | loss: 3.5544305Losses:  5.16916561126709 2.1979193687438965 0.5593105554580688
MemoryTrain:  epoch  4, batch     2 | loss: 5.1691656Losses:  5.645064830780029 2.952528715133667 0.41297706961631775
MemoryTrain:  epoch  4, batch     3 | loss: 5.6450648Losses:  3.0348782539367676 0.23638655245304108 0.6315113306045532
MemoryTrain:  epoch  4, batch     4 | loss: 3.0348783Losses:  3.383143424987793 0.7634367942810059 0.4704812467098236
MemoryTrain:  epoch  4, batch     5 | loss: 3.3831434Losses:  3.781343698501587 1.078711748123169 0.6604371070861816
MemoryTrain:  epoch  4, batch     6 | loss: 3.7813437Losses:  4.676892280578613 1.8913053274154663 0.8023567199707031
MemoryTrain:  epoch  4, batch     7 | loss: 4.6768923Losses:  3.613480567932129 0.9781101942062378 0.5959286093711853
MemoryTrain:  epoch  4, batch     8 | loss: 3.6134806Losses:  3.6901111602783203 1.2338101863861084 0.5698028802871704
MemoryTrain:  epoch  4, batch     9 | loss: 3.6901112Losses:  3.7233378887176514 0.9944151639938354 0.7300984263420105
MemoryTrain:  epoch  5, batch     0 | loss: 3.7233379Losses:  4.048378944396973 1.3603819608688354 0.5674245357513428
MemoryTrain:  epoch  5, batch     1 | loss: 4.0483789Losses:  3.727482557296753 1.0461444854736328 0.5697743892669678
MemoryTrain:  epoch  5, batch     2 | loss: 3.7274826Losses:  3.273512125015259 0.6830335855484009 0.532085657119751
MemoryTrain:  epoch  5, batch     3 | loss: 3.2735121Losses:  3.4200730323791504 0.7628595232963562 0.6428742408752441
MemoryTrain:  epoch  5, batch     4 | loss: 3.4200730Losses:  3.679377555847168 0.9674900770187378 0.7146773934364319
MemoryTrain:  epoch  5, batch     5 | loss: 3.6793776Losses:  3.6316072940826416 1.0293076038360596 0.5901010632514954
MemoryTrain:  epoch  5, batch     6 | loss: 3.6316073Losses:  3.7644007205963135 1.0684146881103516 0.5775572657585144
MemoryTrain:  epoch  5, batch     7 | loss: 3.7644007Losses:  3.7356200218200684 1.2757585048675537 0.39225831627845764
MemoryTrain:  epoch  5, batch     8 | loss: 3.7356200Losses:  5.230451583862305 2.220634698867798 0.4902379512786865
MemoryTrain:  epoch  5, batch     9 | loss: 5.2304516Losses:  2.902805805206299 0.2848236858844757 0.5472370982170105
MemoryTrain:  epoch  6, batch     0 | loss: 2.9028058Losses:  3.7568342685699463 0.9635475873947144 0.7720525860786438
MemoryTrain:  epoch  6, batch     1 | loss: 3.7568343Losses:  3.826742649078369 1.2155020236968994 0.5666402578353882
MemoryTrain:  epoch  6, batch     2 | loss: 3.8267426Losses:  3.393397331237793 0.7580846548080444 0.5533541440963745
MemoryTrain:  epoch  6, batch     3 | loss: 3.3933973Losses:  3.8636131286621094 1.2384843826293945 0.6137837171554565
MemoryTrain:  epoch  6, batch     4 | loss: 3.8636131Losses:  3.594244956970215 1.0101841688156128 0.5050054788589478
MemoryTrain:  epoch  6, batch     5 | loss: 3.5942450Losses:  3.55322527885437 0.7922335863113403 0.6155158877372742
MemoryTrain:  epoch  6, batch     6 | loss: 3.5532253Losses:  3.486133098602295 0.5958280563354492 0.6342326402664185
MemoryTrain:  epoch  6, batch     7 | loss: 3.4861331Losses:  3.951063871383667 1.4705679416656494 0.5694694519042969
MemoryTrain:  epoch  6, batch     8 | loss: 3.9510639Losses:  3.416409969329834 0.7794678211212158 0.7329226732254028
MemoryTrain:  epoch  6, batch     9 | loss: 3.4164100Losses:  3.8597686290740967 1.2319490909576416 0.5945581793785095
MemoryTrain:  epoch  7, batch     0 | loss: 3.8597686Losses:  3.737880229949951 1.092358112335205 0.679118275642395
MemoryTrain:  epoch  7, batch     1 | loss: 3.7378802Losses:  3.130401849746704 0.49439990520477295 0.6319363713264465
MemoryTrain:  epoch  7, batch     2 | loss: 3.1304018Losses:  3.4624850749969482 0.7478430271148682 0.7726075053215027
MemoryTrain:  epoch  7, batch     3 | loss: 3.4624851Losses:  3.8844246864318848 1.2675186395645142 0.6159969568252563
MemoryTrain:  epoch  7, batch     4 | loss: 3.8844247Losses:  3.8562605381011963 1.0047590732574463 0.7331719398498535
MemoryTrain:  epoch  7, batch     5 | loss: 3.8562605Losses:  4.0381927490234375 1.3477848768234253 0.5037949681282043
MemoryTrain:  epoch  7, batch     6 | loss: 4.0381927Losses:  3.4978721141815186 0.9440621137619019 0.5495097637176514
MemoryTrain:  epoch  7, batch     7 | loss: 3.4978721Losses:  4.333733081817627 1.7989373207092285 0.55772465467453
MemoryTrain:  epoch  7, batch     8 | loss: 4.3337331Losses:  3.1908254623413086 0.5270556211471558 0.6507066488265991
MemoryTrain:  epoch  7, batch     9 | loss: 3.1908255Losses:  4.66818904876709 1.839451789855957 0.7922451496124268
MemoryTrain:  epoch  8, batch     0 | loss: 4.6681890Losses:  3.9893531799316406 1.3624608516693115 0.6855085492134094
MemoryTrain:  epoch  8, batch     1 | loss: 3.9893532Losses:  3.4238157272338867 0.775642991065979 0.6665093302726746
MemoryTrain:  epoch  8, batch     2 | loss: 3.4238157Losses:  3.5240397453308105 0.923636257648468 0.7103131413459778
MemoryTrain:  epoch  8, batch     3 | loss: 3.5240397Losses:  3.0390615463256836 0.23113855719566345 0.7649890184402466
MemoryTrain:  epoch  8, batch     4 | loss: 3.0390615Losses:  2.8775877952575684 0.2555813491344452 0.6879202723503113
MemoryTrain:  epoch  8, batch     5 | loss: 2.8775878Losses:  3.4699203968048096 0.7545862197875977 0.755662202835083
MemoryTrain:  epoch  8, batch     6 | loss: 3.4699204Losses:  3.1268808841705322 0.48385143280029297 0.5736019611358643
MemoryTrain:  epoch  8, batch     7 | loss: 3.1268809Losses:  3.2288289070129395 0.7252615690231323 0.5370911359786987
MemoryTrain:  epoch  8, batch     8 | loss: 3.2288289Losses:  3.4938671588897705 1.0042392015457153 0.593313455581665
MemoryTrain:  epoch  8, batch     9 | loss: 3.4938672Losses:  3.318499803543091 0.694801926612854 0.6820622086524963
MemoryTrain:  epoch  9, batch     0 | loss: 3.3184998Losses:  4.855928897857666 2.3346052169799805 0.47113561630249023
MemoryTrain:  epoch  9, batch     1 | loss: 4.8559289Losses:  3.513533115386963 0.9319757223129272 0.7063422799110413
MemoryTrain:  epoch  9, batch     2 | loss: 3.5135331Losses:  3.877345561981201 1.2962993383407593 0.6406545639038086
MemoryTrain:  epoch  9, batch     3 | loss: 3.8773456Losses:  3.859626531600952 1.3579270839691162 0.5663215517997742
MemoryTrain:  epoch  9, batch     4 | loss: 3.8596265Losses:  3.6417863368988037 1.0231852531433105 0.597823441028595
MemoryTrain:  epoch  9, batch     5 | loss: 3.6417863Losses:  3.163639545440674 0.47982364892959595 0.7830347418785095
MemoryTrain:  epoch  9, batch     6 | loss: 3.1636395Losses:  3.4727063179016113 0.9975627064704895 0.5641340613365173
MemoryTrain:  epoch  9, batch     7 | loss: 3.4727063Losses:  2.963425636291504 0.48425957560539246 0.5529487133026123
MemoryTrain:  epoch  9, batch     8 | loss: 2.9634256Losses:  2.514923572540283 -0.0 0.6279655694961548
MemoryTrain:  epoch  9, batch     9 | loss: 2.5149236
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 22.92%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 25.89%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 36.81%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 39.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 43.75%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 47.40%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 49.04%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 51.34%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 52.50%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 53.52%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 54.78%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 55.90%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 56.91%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 58.63%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 57.95%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 4.17%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 3.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 3.12%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 13.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 24.22%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 49.04%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 49.11%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 50.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.17%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 52.57%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 53.47%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 55.26%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 56.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.80%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 66.35%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 67.13%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.30%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 70.36%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.29%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 71.59%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 70.77%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 70.00%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 69.44%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 69.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 70.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 71.49%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 70.93%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 69.32%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 67.78%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 66.44%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 65.29%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 65.36%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 64.67%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 63.62%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 62.62%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 61.54%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 60.38%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 60.19%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 60.80%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 61.50%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 61.95%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 62.28%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 62.61%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 62.71%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 62.30%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 62.10%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 61.90%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 61.52%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 60.87%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 60.13%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 59.98%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 60.39%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 60.69%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 60.45%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 60.12%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 60.24%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 60.79%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 61.32%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 61.83%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 62.34%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 62.82%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 63.06%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 62.58%   [EVAL] batch:   79 | acc: 18.75%,  total acc: 62.03%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 61.50%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 60.98%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 60.54%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 60.12%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 60.15%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 60.39%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 60.42%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 60.58%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 60.81%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 61.11%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 61.26%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 61.48%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 61.56%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 61.64%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 61.71%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 61.85%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 62.05%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 62.05%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 62.31%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 61.88%   
cur_acc:  ['0.8693', '0.7708', '0.7768', '0.7118', '0.5795']
his_acc:  ['0.8693', '0.7939', '0.7234', '0.6763', '0.6188']
Clustering into  14  clusters
Clusters:  [ 1  8  0  1 13  2  1  3  7 11  4  3  1  4  8  1  1 12  1  4  9  0  0  5
  2  4  1 10  0  6  0]
Losses:  14.579072952270508 7.798935890197754 0.4386606812477112
CurrentTrain: epoch  0, batch     0 | loss: 14.5790730Losses:  8.94119930267334 2.341815233230591 0.36299383640289307
CurrentTrain: epoch  0, batch     1 | loss: 8.9411993Losses:  16.63350486755371 9.90188217163086 0.3819218873977661
CurrentTrain: epoch  1, batch     0 | loss: 16.6335049Losses:  8.763450622558594 4.752622127532959 0.2791609764099121
CurrentTrain: epoch  1, batch     1 | loss: 8.7634506Losses:  11.485706329345703 6.1429901123046875 0.3596454858779907
CurrentTrain: epoch  2, batch     0 | loss: 11.4857063Losses:  7.0951361656188965 1.614083170890808 0.30871689319610596
CurrentTrain: epoch  2, batch     1 | loss: 7.0951362Losses:  11.269336700439453 6.314352989196777 0.28926926851272583
CurrentTrain: epoch  3, batch     0 | loss: 11.2693367Losses:  5.726306915283203 1.401741862297058 0.3396958112716675
CurrentTrain: epoch  3, batch     1 | loss: 5.7263069Losses:  10.644539833068848 6.46498441696167 0.31094855070114136
CurrentTrain: epoch  4, batch     0 | loss: 10.6445398Losses:  6.531628608703613 1.9243583679199219 0.2726297378540039
CurrentTrain: epoch  4, batch     1 | loss: 6.5316286Losses:  11.825324058532715 7.557683944702148 0.27939894795417786
CurrentTrain: epoch  5, batch     0 | loss: 11.8253241Losses:  6.266993999481201 3.0307304859161377 0.2824465036392212
CurrentTrain: epoch  5, batch     1 | loss: 6.2669940Losses:  10.032474517822266 6.190755844116211 0.27751725912094116
CurrentTrain: epoch  6, batch     0 | loss: 10.0324745Losses:  4.221885681152344 1.1266765594482422 0.2686012089252472
CurrentTrain: epoch  6, batch     1 | loss: 4.2218857Losses:  11.62772274017334 7.666306495666504 0.2612834870815277
CurrentTrain: epoch  7, batch     0 | loss: 11.6277227Losses:  5.113072872161865 2.417661428451538 0.29176193475723267
CurrentTrain: epoch  7, batch     1 | loss: 5.1130729Losses:  11.082096099853516 7.3368682861328125 0.26474225521087646
CurrentTrain: epoch  8, batch     0 | loss: 11.0820961Losses:  5.783708572387695 2.9495937824249268 0.2712472081184387
CurrentTrain: epoch  8, batch     1 | loss: 5.7837086Losses:  11.583053588867188 8.223173141479492 0.25766897201538086
CurrentTrain: epoch  9, batch     0 | loss: 11.5830536Losses:  7.782814979553223 4.341005325317383 0.22651097178459167
CurrentTrain: epoch  9, batch     1 | loss: 7.7828150
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: The renowned physicist Albert Einstein was born in the Kingdom of Wrttemberg in the German Empire.  
Head Entity: Albert Einstein  
Tail Entity: Germany  

Relation: person country of birth  
Context: During an interview, the famous actress Sofia Vergara shared her experiences growing up in Barranquilla, Colombia.  
Head Entity: Sofia Vergara  
Tail Entity: Colombia  

Relation: person country of birth  
Context: The celebrated author Gabriel Garca Mrquez was born in Aracataca, a small town in Colombia, which greatly influenced his writing.  
Head Entity: Gabriel Garca Mrquez  
Tail Entity: Colombia  

Relation: person country of birth  
Context: The legendary musician Freddie Mercury was born in Zanzibar, which is now part of Tanzania, before moving to England.  
Head Entity: Freddie Mercury  
Tail Entity: Tanzania  

Relation: person country of birth  
Context: The famous scientist Marie Curie was born in Warsaw, Poland, where she began her early education in physics and mathematics.  
Head Entity: Marie Curie  
Tail Entity: Poland  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit our official site at https://www.techinnovators.com for the latest updates.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For more information, check out the website of Green Earth Initiative at http://www.greenearth.org.  
Head Entity: Green Earth Initiative  
Tail Entity: http://www.greenearth.org  

Relation: organization website  
Context: The blog post can be found on the companys website: https://www.creativeworks.com/blog.  
Head Entity: Creative Works  
Tail Entity: https://www.creativeworks.com/blog  

Relation: organization website  
Context: You can learn more about our services by visiting http://www.healthcareplus.com.  
Head Entity: Healthcare Plus  
Tail Entity: http://www.healthcareplus.com  

Relation: organization website  
Context: Explore our projects and initiatives at https://www.artforchange.org.  
Head Entity: Art for Change  
Tail Entity: https://www.artforchange.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant Apple has seen significant investments from Warren Buffett's Berkshire Hathaway.  
Head Entity: Apple  
Tail Entity: Berkshire Hathaway  

Relation: organization shareholders  
Context: The investment firm Vanguard Group has acquired a substantial stake in the renewable energy company NextEra Energy.  
Head Entity: NextEra Energy  
Tail Entity: Vanguard Group  

Relation: organization shareholders  
Context: Tesla's growth has been bolstered by significant shares held by the investment company BlackRock.  
Head Entity: Tesla  
Tail Entity: BlackRock  

Relation: organization shareholders  
Context: The pharmaceutical company Pfizer has received major funding from the investment group Fidelity.  
Head Entity: Pfizer  
Tail Entity: Fidelity  

Relation: organization shareholders  
Context: The popular streaming service Netflix has attracted investments from the private equity firm KKR.  
Head Entity: Netflix  
Tail Entity: KKR  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: After years of financial difficulties, the local arts council announced its dissolution in January 2019, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: January 2019  

Relation: organization dissolved  
Context: The historic publishing house, Classic Reads, was dissolved in July 2021, marking the end of an era in literary history.  
Head Entity: Classic Reads  
Tail Entity: July 2021  

Relation: organization dissolved  
Context: Following a series of scandals, the charity organization, Helping Hands, was officially dissolved in February 2022.  
Head Entity: Helping Hands  
Tail Entity: February 2022  

Relation: organization dissolved  
Context: The environmental group, Green Future, announced its dissolution in October 2023 due to a lack of funding and support.  
Head Entity: Green Future  
Tail Entity: October 2023  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: zagat survey , the guide empire that started as a hobby for tim and nina zagat in 1979 as a two-page typed list of new york restaurants compiled from reviews from friends , has been put up for sale , according to people briefed on the decision .
Head Entity: zagat
Tail Entity: tim
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
1. Relation: organization founded by  
   Context: In 1998, Google was founded by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University.  
   Head Entity: Google  
   Tail Entity: Larry Page  

2. Relation: organization founded by  
   Context: The famous social media platform Facebook was created by Mark Zuckerberg along with his college roommates in 2004.  
   Head Entity: Facebook  
   Tail Entity: Mark Zuckerberg  

3. Relation: organization founded by  
   Context: The innovative electric car company Tesla was established by Martin Eberhard and Marc Tarpenning in 2003.  
   Head Entity: Tesla  
   Tail Entity: Martin Eberhard  

4. Relation: organization founded by  
   Context: The renowned fashion brand Chanel was founded by the iconic designer Coco Chanel in 1910.  
   Head Entity: Chanel  
   Tail Entity: Coco Chanel  

5. Relation: organization founded by  
   Context: The popular video-sharing platform YouTube was co-founded by Steve Chen, Chad Hurley, and Jawed Karim in 2005.  
   Head Entity: YouTube  
   Tail Entity: Steve Chen  
Losses:  4.355144023895264 0.6870954036712646 0.509280264377594
MemoryTrain:  epoch  0, batch     0 | loss: 4.3551440Losses:  4.294599533081055 0.24579845368862152 0.6733317375183105
MemoryTrain:  epoch  0, batch     1 | loss: 4.2945995Losses:  4.237570285797119 0.8173860907554626 0.5590211749076843
MemoryTrain:  epoch  0, batch     2 | loss: 4.2375703Losses:  4.2502055168151855 -0.0 0.7343112230300903
MemoryTrain:  epoch  0, batch     3 | loss: 4.2502055Losses:  4.9066996574401855 0.8113052845001221 0.617164671421051
MemoryTrain:  epoch  0, batch     4 | loss: 4.9066997Losses:  4.003116130828857 0.45802682638168335 0.7864111661911011
MemoryTrain:  epoch  0, batch     5 | loss: 4.0031161Losses:  6.31132173538208 2.285069465637207 0.5747771263122559
MemoryTrain:  epoch  0, batch     6 | loss: 6.3113217Losses:  5.3883466720581055 1.439345359802246 0.4878443479537964
MemoryTrain:  epoch  0, batch     7 | loss: 5.3883467Losses:  5.597969055175781 0.8756009340286255 0.7323057651519775
MemoryTrain:  epoch  0, batch     8 | loss: 5.5979691Losses:  4.550367832183838 1.2222461700439453 0.28011006116867065
MemoryTrain:  epoch  0, batch     9 | loss: 4.5503678Losses:  4.248691082000732 0.7131797671318054 0.7152783870697021
MemoryTrain:  epoch  0, batch    10 | loss: 4.2486911Losses:  4.50480318069458 1.074480414390564 0.40096694231033325
MemoryTrain:  epoch  0, batch    11 | loss: 4.5048032Losses:  4.545305252075195 0.7470875382423401 0.7810354232788086
MemoryTrain:  epoch  1, batch     0 | loss: 4.5453053Losses:  5.261410236358643 1.3612115383148193 0.5541948080062866
MemoryTrain:  epoch  1, batch     1 | loss: 5.2614102Losses:  4.222847938537598 1.0337367057800293 0.6332638263702393
MemoryTrain:  epoch  1, batch     2 | loss: 4.2228479Losses:  4.659099578857422 1.6020647287368774 0.7290657758712769
MemoryTrain:  epoch  1, batch     3 | loss: 4.6590996Losses:  3.802241802215576 0.7133967280387878 0.8292786478996277
MemoryTrain:  epoch  1, batch     4 | loss: 3.8022418Losses:  5.434719085693359 2.0751760005950928 0.4746558666229248
MemoryTrain:  epoch  1, batch     5 | loss: 5.4347191Losses:  4.827651023864746 0.8055235147476196 0.6640489101409912
MemoryTrain:  epoch  1, batch     6 | loss: 4.8276510Losses:  4.1333112716674805 0.9989399909973145 0.6840031743049622
MemoryTrain:  epoch  1, batch     7 | loss: 4.1333113Losses:  4.079685211181641 0.7914330959320068 0.6731932163238525
MemoryTrain:  epoch  1, batch     8 | loss: 4.0796852Losses:  5.363749980926514 1.2919578552246094 0.5703919529914856
MemoryTrain:  epoch  1, batch     9 | loss: 5.3637500Losses:  3.3665332794189453 0.23298686742782593 0.757964551448822
MemoryTrain:  epoch  1, batch    10 | loss: 3.3665333Losses:  3.3587968349456787 -0.0 0.6491798758506775
MemoryTrain:  epoch  1, batch    11 | loss: 3.3587968Losses:  3.447681427001953 0.975477397441864 0.498360276222229
MemoryTrain:  epoch  2, batch     0 | loss: 3.4476814Losses:  5.352105617523193 1.7270060777664185 0.5880402326583862
MemoryTrain:  epoch  2, batch     1 | loss: 5.3521056Losses:  3.183187961578369 0.22353652119636536 0.8999542593955994
MemoryTrain:  epoch  2, batch     2 | loss: 3.1831880Losses:  4.879936695098877 2.2236080169677734 0.48636993765830994
MemoryTrain:  epoch  2, batch     3 | loss: 4.8799367Losses:  3.403353452682495 0.4895547330379486 0.846306324005127
MemoryTrain:  epoch  2, batch     4 | loss: 3.4033535Losses:  4.06048059463501 1.1245157718658447 0.555769145488739
MemoryTrain:  epoch  2, batch     5 | loss: 4.0604806Losses:  3.8140149116516113 0.7286503314971924 0.7207788228988647
MemoryTrain:  epoch  2, batch     6 | loss: 3.8140149Losses:  3.45505952835083 0.6989609599113464 0.7011395692825317
MemoryTrain:  epoch  2, batch     7 | loss: 3.4550595Losses:  3.5298423767089844 0.4723106324672699 0.706278920173645
MemoryTrain:  epoch  2, batch     8 | loss: 3.5298424Losses:  5.123067378997803 1.8673641681671143 0.6452716588973999
MemoryTrain:  epoch  2, batch     9 | loss: 5.1230674Losses:  3.3600118160247803 0.45559605956077576 0.7795203328132629
MemoryTrain:  epoch  2, batch    10 | loss: 3.3600118Losses:  2.7310211658477783 0.2585432529449463 0.42605820298194885
MemoryTrain:  epoch  2, batch    11 | loss: 2.7310212Losses:  3.273123264312744 0.2617616057395935 0.6569595336914062
MemoryTrain:  epoch  3, batch     0 | loss: 3.2731233Losses:  3.415475368499756 0.7192181348800659 0.71188884973526
MemoryTrain:  epoch  3, batch     1 | loss: 3.4154754Losses:  3.2952165603637695 0.7064231634140015 0.4657702147960663
MemoryTrain:  epoch  3, batch     2 | loss: 3.2952166Losses:  3.323472499847412 0.716730535030365 0.5803662538528442
MemoryTrain:  epoch  3, batch     3 | loss: 3.3234725Losses:  3.205885887145996 0.23409518599510193 0.8367581963539124
MemoryTrain:  epoch  3, batch     4 | loss: 3.2058859Losses:  3.8463568687438965 1.3444098234176636 0.561933159828186
MemoryTrain:  epoch  3, batch     5 | loss: 3.8463569Losses:  3.901787519454956 1.1055082082748413 0.5683953166007996
MemoryTrain:  epoch  3, batch     6 | loss: 3.9017875Losses:  3.708761215209961 0.7588755488395691 0.7089150547981262
MemoryTrain:  epoch  3, batch     7 | loss: 3.7087612Losses:  3.6787269115448 0.7719272375106812 0.6879546046257019
MemoryTrain:  epoch  3, batch     8 | loss: 3.6787269Losses:  3.2561521530151367 0.47277191281318665 0.623018741607666
MemoryTrain:  epoch  3, batch     9 | loss: 3.2561522Losses:  3.8811933994293213 1.0787945985794067 0.7149264216423035
MemoryTrain:  epoch  3, batch    10 | loss: 3.8811934Losses:  2.9427244663238525 0.29541417956352234 0.6493940353393555
MemoryTrain:  epoch  3, batch    11 | loss: 2.9427245Losses:  2.9405429363250732 0.24879084527492523 0.752435028553009
MemoryTrain:  epoch  4, batch     0 | loss: 2.9405429Losses:  3.0125818252563477 0.4579485058784485 0.539003312587738
MemoryTrain:  epoch  4, batch     1 | loss: 3.0125818Losses:  3.2527897357940674 0.5357853174209595 0.6424887776374817
MemoryTrain:  epoch  4, batch     2 | loss: 3.2527897Losses:  3.811458110809326 1.2525992393493652 0.6073838472366333
MemoryTrain:  epoch  4, batch     3 | loss: 3.8114581Losses:  3.865790367126465 1.2878515720367432 0.635185182094574
MemoryTrain:  epoch  4, batch     4 | loss: 3.8657904Losses:  4.145233631134033 1.4169762134552002 0.5830041766166687
MemoryTrain:  epoch  4, batch     5 | loss: 4.1452336Losses:  3.35264253616333 0.7046076655387878 0.6878499984741211
MemoryTrain:  epoch  4, batch     6 | loss: 3.3526425Losses:  3.0167746543884277 0.25586456060409546 0.7336748838424683
MemoryTrain:  epoch  4, batch     7 | loss: 3.0167747Losses:  3.3796563148498535 0.7612813711166382 0.7041282653808594
MemoryTrain:  epoch  4, batch     8 | loss: 3.3796563Losses:  4.023696422576904 1.3031280040740967 0.5124635100364685
MemoryTrain:  epoch  4, batch     9 | loss: 4.0236964Losses:  3.567035675048828 0.7319062352180481 0.7815919518470764
MemoryTrain:  epoch  4, batch    10 | loss: 3.5670357Losses:  2.5350868701934814 0.229454904794693 0.36908048391342163
MemoryTrain:  epoch  4, batch    11 | loss: 2.5350869Losses:  2.9008748531341553 0.24627625942230225 0.740583598613739
MemoryTrain:  epoch  5, batch     0 | loss: 2.9008749Losses:  3.5020711421966553 0.9845055341720581 0.48450392484664917
MemoryTrain:  epoch  5, batch     1 | loss: 3.5020711Losses:  2.9551663398742676 0.23408102989196777 0.7743334174156189
MemoryTrain:  epoch  5, batch     2 | loss: 2.9551663Losses:  3.750354290008545 1.266676902770996 0.4993370771408081
MemoryTrain:  epoch  5, batch     3 | loss: 3.7503543Losses:  3.8515899181365967 1.1031031608581543 0.7274613976478577
MemoryTrain:  epoch  5, batch     4 | loss: 3.8515899Losses:  2.74072003364563 0.22733032703399658 0.5537164211273193
MemoryTrain:  epoch  5, batch     5 | loss: 2.7407200Losses:  3.7336978912353516 0.9583814740180969 0.8047725558280945
MemoryTrain:  epoch  5, batch     6 | loss: 3.7336979Losses:  3.479318141937256 0.9445583820343018 0.6381233334541321
MemoryTrain:  epoch  5, batch     7 | loss: 3.4793181Losses:  4.294295310974121 1.4260729551315308 0.70754474401474
MemoryTrain:  epoch  5, batch     8 | loss: 4.2942953Losses:  4.038601398468018 1.2649433612823486 0.6404088735580444
MemoryTrain:  epoch  5, batch     9 | loss: 4.0386014Losses:  3.1440391540527344 0.4925239384174347 0.6690443754196167
MemoryTrain:  epoch  5, batch    10 | loss: 3.1440392Losses:  2.607032537460327 0.30165591835975647 0.41361895203590393
MemoryTrain:  epoch  5, batch    11 | loss: 2.6070325Losses:  3.9340381622314453 1.2915141582489014 0.712334930896759
MemoryTrain:  epoch  6, batch     0 | loss: 3.9340382Losses:  3.687303066253662 1.3586254119873047 0.386639803647995
MemoryTrain:  epoch  6, batch     1 | loss: 3.6873031Losses:  3.9422101974487305 1.112607479095459 0.7655121088027954
MemoryTrain:  epoch  6, batch     2 | loss: 3.9422102Losses:  3.6114230155944824 0.9382821321487427 0.6351732015609741
MemoryTrain:  epoch  6, batch     3 | loss: 3.6114230Losses:  4.109872341156006 1.6617388725280762 0.4912877082824707
MemoryTrain:  epoch  6, batch     4 | loss: 4.1098723Losses:  2.9864234924316406 0.4697147011756897 0.6222816705703735
MemoryTrain:  epoch  6, batch     5 | loss: 2.9864235Losses:  4.053121089935303 1.540946125984192 0.6367596387863159
MemoryTrain:  epoch  6, batch     6 | loss: 4.0531211Losses:  3.83640456199646 1.3914520740509033 0.5032086968421936
MemoryTrain:  epoch  6, batch     7 | loss: 3.8364046Losses:  3.99778413772583 1.3440041542053223 0.7543830871582031
MemoryTrain:  epoch  6, batch     8 | loss: 3.9977841Losses:  2.903120517730713 0.23825600743293762 0.7010942101478577
MemoryTrain:  epoch  6, batch     9 | loss: 2.9031205Losses:  4.016430377960205 1.4930593967437744 0.6305003762245178
MemoryTrain:  epoch  6, batch    10 | loss: 4.0164304Losses:  2.989776849746704 0.6061102747917175 0.403346449136734
MemoryTrain:  epoch  6, batch    11 | loss: 2.9897768Losses:  3.463789463043213 1.0080831050872803 0.5621744990348816
MemoryTrain:  epoch  7, batch     0 | loss: 3.4637895Losses:  3.191652774810791 0.718817949295044 0.5569077730178833
MemoryTrain:  epoch  7, batch     1 | loss: 3.1916528Losses:  3.2474522590637207 0.6980190873146057 0.6235407590866089
MemoryTrain:  epoch  7, batch     2 | loss: 3.2474523Losses:  2.6304640769958496 -0.0 0.6993560791015625
MemoryTrain:  epoch  7, batch     3 | loss: 2.6304641Losses:  3.270820379257202 0.6749483346939087 0.7053998112678528
MemoryTrain:  epoch  7, batch     4 | loss: 3.2708204Losses:  3.7642202377319336 1.1628060340881348 0.6249250173568726
MemoryTrain:  epoch  7, batch     5 | loss: 3.7642202Losses:  3.433856964111328 0.7439337968826294 0.7958508729934692
MemoryTrain:  epoch  7, batch     6 | loss: 3.4338570Losses:  3.534318208694458 1.1348028182983398 0.4621843695640564
MemoryTrain:  epoch  7, batch     7 | loss: 3.5343182Losses:  3.7883460521698 1.130746841430664 0.7327303290367126
MemoryTrain:  epoch  7, batch     8 | loss: 3.7883461Losses:  3.1147122383117676 0.508873701095581 0.7131274938583374
MemoryTrain:  epoch  7, batch     9 | loss: 3.1147122Losses:  3.33088755607605 0.7370327711105347 0.7140995860099792
MemoryTrain:  epoch  7, batch    10 | loss: 3.3308876Losses:  2.791410207748413 0.29869580268859863 0.5582916140556335
MemoryTrain:  epoch  7, batch    11 | loss: 2.7914102Losses:  2.8515517711639404 0.24966347217559814 0.7078470587730408
MemoryTrain:  epoch  8, batch     0 | loss: 2.8515518Losses:  3.112971782684326 0.4780302047729492 0.7459789514541626
MemoryTrain:  epoch  8, batch     1 | loss: 3.1129718Losses:  3.2815091609954834 0.692462146282196 0.6944544911384583
MemoryTrain:  epoch  8, batch     2 | loss: 3.2815092Losses:  2.8397650718688965 0.26004886627197266 0.6344168186187744
MemoryTrain:  epoch  8, batch     3 | loss: 2.8397651Losses:  3.221470355987549 0.6960142850875854 0.6443051695823669
MemoryTrain:  epoch  8, batch     4 | loss: 3.2214704Losses:  3.1118478775024414 0.6790820360183716 0.5591009855270386
MemoryTrain:  epoch  8, batch     5 | loss: 3.1118479Losses:  3.7648797035217285 1.4044275283813477 0.41229209303855896
MemoryTrain:  epoch  8, batch     6 | loss: 3.7648797Losses:  3.227607011795044 0.6892409324645996 0.6253228783607483
MemoryTrain:  epoch  8, batch     7 | loss: 3.2276070Losses:  4.029469966888428 1.3804980516433716 0.7821159362792969
MemoryTrain:  epoch  8, batch     8 | loss: 4.0294700Losses:  3.3591134548187256 0.735378623008728 0.7037680149078369
MemoryTrain:  epoch  8, batch     9 | loss: 3.3591135Losses:  2.7910220623016357 0.22466538846492767 0.6954897046089172
MemoryTrain:  epoch  8, batch    10 | loss: 2.7910221Losses:  2.6141600608825684 0.2791634500026703 0.39440062642097473
MemoryTrain:  epoch  8, batch    11 | loss: 2.6141601Losses:  3.4372096061706543 0.8373358249664307 0.7024460434913635
MemoryTrain:  epoch  9, batch     0 | loss: 3.4372096Losses:  2.895338535308838 0.4467467665672302 0.6010614633560181
MemoryTrain:  epoch  9, batch     1 | loss: 2.8953385Losses:  4.004895210266113 1.3612334728240967 0.7291256785392761
MemoryTrain:  epoch  9, batch     2 | loss: 4.0048952Losses:  3.1115427017211914 0.5221537351608276 0.7225103378295898
MemoryTrain:  epoch  9, batch     3 | loss: 3.1115427Losses:  2.920779228210449 0.2499149590730667 0.7648442387580872
MemoryTrain:  epoch  9, batch     4 | loss: 2.9207792Losses:  3.1416726112365723 0.7121721506118774 0.5493260622024536
MemoryTrain:  epoch  9, batch     5 | loss: 3.1416726Losses:  3.654722213745117 1.083963394165039 0.6866136193275452
MemoryTrain:  epoch  9, batch     6 | loss: 3.6547222Losses:  3.02304744720459 0.4966414272785187 0.6272063255310059
MemoryTrain:  epoch  9, batch     7 | loss: 3.0230474Losses:  4.054662704467773 1.488940715789795 0.7036678791046143
MemoryTrain:  epoch  9, batch     8 | loss: 4.0546627Losses:  2.9334633350372314 0.47968989610671997 0.5403788089752197
MemoryTrain:  epoch  9, batch     9 | loss: 2.9334633Losses:  3.1514391899108887 0.7108603715896606 0.5459818840026855
MemoryTrain:  epoch  9, batch    10 | loss: 3.1514392Losses:  2.6939001083374023 0.3063822388648987 0.48693275451660156
MemoryTrain:  epoch  9, batch    11 | loss: 2.6939001
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 59.82%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 53.12%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 8.75%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 7.29%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 15.18%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 25.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 37.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 41.48%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 45.83%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 46.15%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 45.98%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 47.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 48.44%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 50.69%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 51.32%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.36%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.39%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.24%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 60.68%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 64.35%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 67.29%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 67.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 69.32%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 68.57%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 68.21%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 67.53%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 66.72%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 67.11%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 67.79%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 67.19%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 65.69%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 64.40%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 63.30%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 63.28%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 61.50%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 60.54%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 59.50%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 58.37%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 58.22%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 58.86%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 59.60%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 60.09%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 60.45%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 60.81%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 60.94%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 60.35%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 59.88%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 59.52%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 58.98%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 58.27%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 57.58%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 57.18%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 57.63%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 57.61%   [EVAL] batch:   69 | acc: 18.75%,  total acc: 57.05%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 56.69%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 56.68%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 57.28%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 57.85%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 58.42%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 58.96%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 59.50%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 59.70%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 59.02%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 58.28%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 57.56%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 56.94%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 56.33%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 55.65%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 55.66%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 55.89%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 56.03%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 56.53%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 56.88%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 57.14%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 57.61%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 57.93%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 58.38%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 58.62%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 58.92%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 59.21%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 59.31%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 59.66%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 59.88%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 60.27%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 60.48%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 60.32%   [EVAL] batch:  103 | acc: 25.00%,  total acc: 59.98%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 59.85%   [EVAL] batch:  106 | acc: 6.25%,  total acc: 59.35%   
cur_acc:  ['0.8693', '0.7708', '0.7768', '0.7118', '0.5795', '0.5312']
his_acc:  ['0.8693', '0.7939', '0.7234', '0.6763', '0.6188', '0.5935']
Clustering into  17  clusters
Clusters:  [ 1  3  0  1 16  5  1  2 15 13  4 12  1  4  3 11  1  9 11  4  8  0  0  7
  5  4  1 14  0  6  0  2 10  5  4 11]
Losses:  13.675324440002441 8.320594787597656 0.4209597706794739
CurrentTrain: epoch  0, batch     0 | loss: 13.6753244Losses:  8.578617095947266 2.9927268028259277 0.3349040150642395
CurrentTrain: epoch  0, batch     1 | loss: 8.5786171Losses:  10.903532028198242 6.286872863769531 0.42560824751853943
CurrentTrain: epoch  1, batch     0 | loss: 10.9035320Losses:  5.211599826812744 1.3014297485351562 0.41442710161209106
CurrentTrain: epoch  1, batch     1 | loss: 5.2115998Losses:  9.516297340393066 6.102725982666016 0.414753258228302
CurrentTrain: epoch  2, batch     0 | loss: 9.5162973Losses:  6.297763347625732 2.224616050720215 0.4081512987613678
CurrentTrain: epoch  2, batch     1 | loss: 6.2977633Losses:  9.153342247009277 5.9108734130859375 0.4046914577484131
CurrentTrain: epoch  3, batch     0 | loss: 9.1533422Losses:  5.181800365447998 1.8763529062271118 0.42067527770996094
CurrentTrain: epoch  3, batch     1 | loss: 5.1818004Losses:  9.003902435302734 5.987990379333496 0.39792805910110474
CurrentTrain: epoch  4, batch     0 | loss: 9.0039024Losses:  4.487024307250977 1.8355886936187744 0.404573529958725
CurrentTrain: epoch  4, batch     1 | loss: 4.4870243Losses:  10.703241348266602 7.758301734924316 0.3896578550338745
CurrentTrain: epoch  5, batch     0 | loss: 10.7032413Losses:  6.463987350463867 4.167910099029541 0.30234408378601074
CurrentTrain: epoch  5, batch     1 | loss: 6.4639874Losses:  8.385563850402832 5.766481399536133 0.3832782208919525
CurrentTrain: epoch  6, batch     0 | loss: 8.3855639Losses:  4.041110515594482 1.6574225425720215 0.3817872703075409
CurrentTrain: epoch  6, batch     1 | loss: 4.0411105Losses:  7.908636093139648 5.541024208068848 0.37309154868125916
CurrentTrain: epoch  7, batch     0 | loss: 7.9086361Losses:  4.1142988204956055 1.6965548992156982 0.39104515314102173
CurrentTrain: epoch  7, batch     1 | loss: 4.1142988Losses:  8.471644401550293 6.100897789001465 0.37537068128585815
CurrentTrain: epoch  8, batch     0 | loss: 8.4716444Losses:  4.070036888122559 1.92159903049469 0.2773786187171936
CurrentTrain: epoch  8, batch     1 | loss: 4.0700369Losses:  9.540995597839355 7.299423694610596 0.3688400983810425
CurrentTrain: epoch  9, batch     0 | loss: 9.5409956Losses:  5.831033706665039 3.712315082550049 0.2666599154472351
CurrentTrain: epoch  9, batch     1 | loss: 5.8310337
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  

Relation: person cause of death  
Context: following a long struggle with heart disease, mr. thomas jones succumbed to his illness last night at the hospital.  
Head Entity: mr. thomas jones  
Tail Entity: heart disease  

Relation: person cause of death  
Context: the beloved actor, samuel green, died from a drug overdose, shocking fans and colleagues alike.  
Head Entity: samuel green  
Tail Entity: drug overdose  

Relation: person cause of death  
Context: after a courageous fight against diabetes complications, mrs. linda brown passed away, leaving behind a legacy of kindness.  
Head Entity: mrs. linda brown  
Tail Entity: diabetes complications  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote its agenda.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in political advocacy, representing the interests of the Muslim community in the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  

Relation: organization political religious affiliation  
Context: The National Council of Churches has historically worked to influence legislation that aligns with Christian values and beliefs.  
Head Entity: National Council of Churches  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Jewish Federation of North America plays a crucial role in advocating for policies that support Jewish communities across the continent.  
Head Entity: Jewish Federation of North America  
Tail Entity: Jewish  

Relation: organization political religious affiliation  
Context: The Hindu American Foundation has been instrumental in promoting Hindu values in American politics and fostering community engagement.  
Head Entity: Hindu American Foundation  
Tail Entity: Hindu  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: headquartered in seattle, washington, amazon has transformed the way people shop online.  
Head Entity: amazon  
Tail Entity: washington  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is based in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization stateorprovince of headquarters  
Context: based in redmond, washington, microsoft is known for its software products and cloud services.  
Head Entity: microsoft  
Tail Entity: washington  

Relation: organization stateorprovince of headquarters  
Context: the famous fast-food chain mcdonald's has its headquarters in chicago, illinois, serving millions of customers daily.  
Head Entity: mcdonald's  
Tail Entity: illinois  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: barack obama's half-sister, maya soetoro-ng, is an educator and a prominent figure in her own right.  
Head Entity: barack obama  
Tail Entity: maya soetoro-ng  

Relation: person other family  
Context: the famous actor, tom hanks, has a brother named jim hanks who is also involved in the film industry.  
Head Entity: tom hanks  
Tail Entity: jim hanks  

Relation: person other family  
Context: queen elizabeth ii's cousin, prince michael of kent, often attends royal events and ceremonies.  
Head Entity: queen elizabeth ii  
Tail Entity: prince michael of kent  

Relation: person other family  
Context: the renowned scientist, albert einstein, had a sister named maria einstein who was a talented musician.  
Head Entity: albert einstein  
Tail Entity: maria einstein  

Relation: person other family  
Context: the famous author, j.k. rowling, has a younger sister named dianne who has supported her throughout her career.  
Head Entity: j.k. rowling  
Tail Entity: dianne rowling  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  

Relation: person city of death  
Context: the famous physicist, albert einstein, died on april 18, 1955, in princeton, new jersey, leaving a profound impact on the world of science.  
Head Entity: albert einstein  
Tail Entity: princeton  

Relation: person city of death  
Context: on january 1, 2020, the beloved actor, kobe bryant, tragically lost his life in a helicopter crash in calabasas, california, shocking fans around the globe.  
Head Entity: kobe bryant  
Tail Entity: calabasas  

Relation: person city of death  
Context: the legendary musician, freddie mercury, passed away on november 24, 1991, at his home in london, england, after a courageous battle with aids.  
Head Entity: freddie mercury  
Tail Entity: london  
Losses:  4.443081378936768 0.7362157106399536 0.8610148429870605
MemoryTrain:  epoch  0, batch     0 | loss: 4.4430814Losses:  4.236682415008545 0.47527021169662476 0.899899959564209
MemoryTrain:  epoch  0, batch     1 | loss: 4.2366824Losses:  5.095711708068848 0.48754990100860596 0.6671083569526672
MemoryTrain:  epoch  0, batch     2 | loss: 5.0957117Losses:  4.882330894470215 0.2330387532711029 0.7146484851837158
MemoryTrain:  epoch  0, batch     3 | loss: 4.8823309Losses:  5.074419975280762 0.5243836641311646 0.6341413259506226
MemoryTrain:  epoch  0, batch     4 | loss: 5.0744200Losses:  5.133892059326172 0.5388901233673096 0.6888629198074341
MemoryTrain:  epoch  0, batch     5 | loss: 5.1338921Losses:  4.614009857177734 1.1720261573791504 0.6742044687271118
MemoryTrain:  epoch  0, batch     6 | loss: 4.6140099Losses:  3.8728697299957275 0.4807307720184326 0.605318009853363
MemoryTrain:  epoch  0, batch     7 | loss: 3.8728697Losses:  4.58212947845459 0.22188502550125122 0.8099064826965332
MemoryTrain:  epoch  0, batch     8 | loss: 4.5821295Losses:  5.762187480926514 1.060198187828064 0.7259603142738342
MemoryTrain:  epoch  0, batch     9 | loss: 5.7621875Losses:  4.606882572174072 0.7397790551185608 0.6774272918701172
MemoryTrain:  epoch  0, batch    10 | loss: 4.6068826Losses:  5.004909992218018 1.8268437385559082 0.5776540040969849
MemoryTrain:  epoch  0, batch    11 | loss: 5.0049100Losses:  4.475794315338135 -0.0 0.7718667984008789
MemoryTrain:  epoch  0, batch    12 | loss: 4.4757943Losses:  3.2677054405212402 0.278565376996994 0.6351434588432312
MemoryTrain:  epoch  0, batch    13 | loss: 3.2677054Losses:  4.760452747344971 0.7071040868759155 0.8258859515190125
MemoryTrain:  epoch  1, batch     0 | loss: 4.7604527Losses:  4.883401870727539 1.3021026849746704 0.7838379144668579
MemoryTrain:  epoch  1, batch     1 | loss: 4.8834019Losses:  3.8895041942596436 0.22868700325489044 0.8484048247337341
MemoryTrain:  epoch  1, batch     2 | loss: 3.8895042Losses:  4.707231044769287 0.9786267280578613 0.8992581367492676
MemoryTrain:  epoch  1, batch     3 | loss: 4.7072310Losses:  6.292628288269043 0.8880659937858582 0.6851198673248291
MemoryTrain:  epoch  1, batch     4 | loss: 6.2926283Losses:  4.158733367919922 0.49619096517562866 0.7271519303321838
MemoryTrain:  epoch  1, batch     5 | loss: 4.1587334Losses:  3.212322235107422 0.24475418031215668 0.903450608253479
MemoryTrain:  epoch  1, batch     6 | loss: 3.2123222Losses:  3.806809425354004 0.7079759836196899 0.646885871887207
MemoryTrain:  epoch  1, batch     7 | loss: 3.8068094Losses:  4.21506404876709 0.24291318655014038 0.7151582837104797
MemoryTrain:  epoch  1, batch     8 | loss: 4.2150640Losses:  3.546225070953369 0.5092135667800903 0.9141508340835571
MemoryTrain:  epoch  1, batch     9 | loss: 3.5462251Losses:  3.7420527935028076 0.22073401510715485 0.6323294043540955
MemoryTrain:  epoch  1, batch    10 | loss: 3.7420528Losses:  5.381990432739258 1.3524935245513916 0.7190249562263489
MemoryTrain:  epoch  1, batch    11 | loss: 5.3819904Losses:  2.882563591003418 0.22542332112789154 0.5720140337944031
MemoryTrain:  epoch  1, batch    12 | loss: 2.8825636Losses:  3.38220477104187 0.25805598497390747 0.5887841582298279
MemoryTrain:  epoch  1, batch    13 | loss: 3.3822048Losses:  3.5532965660095215 0.4746885597705841 0.8439208269119263
MemoryTrain:  epoch  2, batch     0 | loss: 3.5532966Losses:  3.711754560470581 0.44584187865257263 0.8397765755653381
MemoryTrain:  epoch  2, batch     1 | loss: 3.7117546Losses:  3.7891762256622314 0.25482797622680664 0.8458938002586365
MemoryTrain:  epoch  2, batch     2 | loss: 3.7891762Losses:  4.203043460845947 1.4358720779418945 0.6349401473999023
MemoryTrain:  epoch  2, batch     3 | loss: 4.2030435Losses:  3.955697774887085 0.7258803844451904 0.8003676533699036
MemoryTrain:  epoch  2, batch     4 | loss: 3.9556978Losses:  4.970900058746338 2.1653738021850586 0.741948127746582
MemoryTrain:  epoch  2, batch     5 | loss: 4.9709001Losses:  4.011439323425293 1.0314165353775024 0.6225031614303589
MemoryTrain:  epoch  2, batch     6 | loss: 4.0114393Losses:  3.262749671936035 0.7234015464782715 0.5397860407829285
MemoryTrain:  epoch  2, batch     7 | loss: 3.2627497Losses:  4.102005958557129 1.1834478378295898 0.7104523181915283
MemoryTrain:  epoch  2, batch     8 | loss: 4.1020060Losses:  4.413342475891113 0.5005102157592773 0.8210801482200623
MemoryTrain:  epoch  2, batch     9 | loss: 4.4133425Losses:  3.6737942695617676 0.7253131866455078 0.6460593938827515
MemoryTrain:  epoch  2, batch    10 | loss: 3.6737943Losses:  3.846928834915161 0.4736449122428894 0.9178705215454102
MemoryTrain:  epoch  2, batch    11 | loss: 3.8469288Losses:  4.519657135009766 0.5756216049194336 0.7113862633705139
MemoryTrain:  epoch  2, batch    12 | loss: 4.5196571Losses:  3.01348876953125 -0.0 0.40381866693496704
MemoryTrain:  epoch  2, batch    13 | loss: 3.0134888Losses:  4.143092155456543 0.7407680749893188 0.8153171539306641
MemoryTrain:  epoch  3, batch     0 | loss: 4.1430922Losses:  3.598789930343628 0.4779263734817505 0.9530631899833679
MemoryTrain:  epoch  3, batch     1 | loss: 3.5987899Losses:  3.124894618988037 0.24355822801589966 0.6905435919761658
MemoryTrain:  epoch  3, batch     2 | loss: 3.1248946Losses:  3.630671977996826 0.7296879291534424 0.7795518040657043
MemoryTrain:  epoch  3, batch     3 | loss: 3.6306720Losses:  3.6407811641693115 0.48955637216567993 0.6916225552558899
MemoryTrain:  epoch  3, batch     4 | loss: 3.6407812Losses:  4.2216620445251465 1.2780940532684326 0.5552769899368286
MemoryTrain:  epoch  3, batch     5 | loss: 4.2216620Losses:  4.349759101867676 1.342491626739502 0.701301634311676
MemoryTrain:  epoch  3, batch     6 | loss: 4.3497591Losses:  3.066068649291992 -0.0 0.824661135673523
MemoryTrain:  epoch  3, batch     7 | loss: 3.0660686Losses:  3.6790473461151123 1.036088228225708 0.6992961764335632
MemoryTrain:  epoch  3, batch     8 | loss: 3.6790473Losses:  3.8919994831085205 1.0889029502868652 0.7967209815979004
MemoryTrain:  epoch  3, batch     9 | loss: 3.8919995Losses:  3.6389923095703125 0.9758968949317932 0.7221821546554565
MemoryTrain:  epoch  3, batch    10 | loss: 3.6389923Losses:  3.623309850692749 0.7561457753181458 0.7836493849754333
MemoryTrain:  epoch  3, batch    11 | loss: 3.6233099Losses:  3.952075958251953 0.49292314052581787 0.6943415403366089
MemoryTrain:  epoch  3, batch    12 | loss: 3.9520760Losses:  2.9319205284118652 0.28123342990875244 0.48434150218963623
MemoryTrain:  epoch  3, batch    13 | loss: 2.9319205Losses:  3.0870046615600586 -0.0 0.8216031193733215
MemoryTrain:  epoch  4, batch     0 | loss: 3.0870047Losses:  3.7409424781799316 0.755282998085022 0.6554926633834839
MemoryTrain:  epoch  4, batch     1 | loss: 3.7409425Losses:  4.087796688079834 1.4099605083465576 0.5648714303970337
MemoryTrain:  epoch  4, batch     2 | loss: 4.0877967Losses:  3.301015853881836 0.45561766624450684 0.6272476315498352
MemoryTrain:  epoch  4, batch     3 | loss: 3.3010159Losses:  4.040651798248291 1.103577733039856 0.7956485748291016
MemoryTrain:  epoch  4, batch     4 | loss: 4.0406518Losses:  3.7089009284973145 0.7173042297363281 0.6507362127304077
MemoryTrain:  epoch  4, batch     5 | loss: 3.7089009Losses:  3.1386053562164307 0.46175792813301086 0.7210772633552551
MemoryTrain:  epoch  4, batch     6 | loss: 3.1386054Losses:  3.33833646774292 0.4746611714363098 0.898506760597229
MemoryTrain:  epoch  4, batch     7 | loss: 3.3383365Losses:  3.87716007232666 1.0100075006484985 0.5688657760620117
MemoryTrain:  epoch  4, batch     8 | loss: 3.8771601Losses:  3.327573537826538 0.5053200721740723 0.8611961007118225
MemoryTrain:  epoch  4, batch     9 | loss: 3.3275735Losses:  3.3057661056518555 0.7022297382354736 0.7059928178787231
MemoryTrain:  epoch  4, batch    10 | loss: 3.3057661Losses:  4.994075298309326 2.3978724479675293 0.6265586614608765
MemoryTrain:  epoch  4, batch    11 | loss: 4.9940753Losses:  3.735598564147949 0.9671961069107056 0.7596544623374939
MemoryTrain:  epoch  4, batch    12 | loss: 3.7355986Losses:  2.5887985229492188 -0.0 0.5992546081542969
MemoryTrain:  epoch  4, batch    13 | loss: 2.5887985Losses:  3.960576057434082 1.3067471981048584 0.755331814289093
MemoryTrain:  epoch  5, batch     0 | loss: 3.9605761Losses:  2.9939804077148438 0.24441054463386536 0.7808325886726379
MemoryTrain:  epoch  5, batch     1 | loss: 2.9939804Losses:  3.403353214263916 0.7394527196884155 0.6562795042991638
MemoryTrain:  epoch  5, batch     2 | loss: 3.4033532Losses:  4.043497562408447 1.3304238319396973 0.7202370166778564
MemoryTrain:  epoch  5, batch     3 | loss: 4.0434976Losses:  4.230429172515869 1.5415287017822266 0.8036344647407532
MemoryTrain:  epoch  5, batch     4 | loss: 4.2304292Losses:  3.819261074066162 1.1340861320495605 0.6618571281433105
MemoryTrain:  epoch  5, batch     5 | loss: 3.8192611Losses:  3.828141927719116 1.08863365650177 0.8607479929924011
MemoryTrain:  epoch  5, batch     6 | loss: 3.8281419Losses:  2.8874244689941406 0.23756839334964752 0.7058802843093872
MemoryTrain:  epoch  5, batch     7 | loss: 2.8874245Losses:  3.031857967376709 0.2390061914920807 0.793477475643158
MemoryTrain:  epoch  5, batch     8 | loss: 3.0318580Losses:  3.7530646324157715 1.0502381324768066 0.7062287926673889
MemoryTrain:  epoch  5, batch     9 | loss: 3.7530646Losses:  3.061108350753784 0.4797852337360382 0.5647342205047607
MemoryTrain:  epoch  5, batch    10 | loss: 3.0611084Losses:  3.2811455726623535 0.7609460949897766 0.5511410236358643
MemoryTrain:  epoch  5, batch    11 | loss: 3.2811456Losses:  3.329437732696533 0.5265246033668518 0.7233372330665588
MemoryTrain:  epoch  5, batch    12 | loss: 3.3294377Losses:  2.5401062965393066 -0.0 0.6446348428726196
MemoryTrain:  epoch  5, batch    13 | loss: 2.5401063Losses:  3.242732048034668 0.7090004682540894 0.5612801313400269
MemoryTrain:  epoch  6, batch     0 | loss: 3.2427320Losses:  3.0423197746276855 0.24094027280807495 0.7676402926445007
MemoryTrain:  epoch  6, batch     1 | loss: 3.0423198Losses:  3.473155975341797 1.0241305828094482 0.5493225455284119
MemoryTrain:  epoch  6, batch     2 | loss: 3.4731560Losses:  3.257131814956665 0.47064173221588135 0.6902138590812683
MemoryTrain:  epoch  6, batch     3 | loss: 3.2571318Losses:  2.903362274169922 0.2565615773200989 0.6976897120475769
MemoryTrain:  epoch  6, batch     4 | loss: 2.9033623Losses:  3.39046049118042 0.7228702902793884 0.6621512174606323
MemoryTrain:  epoch  6, batch     5 | loss: 3.3904605Losses:  3.1946187019348145 0.72821044921875 0.5637959241867065
MemoryTrain:  epoch  6, batch     6 | loss: 3.1946187Losses:  3.3126819133758545 0.6890324950218201 0.7123495936393738
MemoryTrain:  epoch  6, batch     7 | loss: 3.3126819Losses:  3.2648799419403076 0.48570388555526733 0.8501150012016296
MemoryTrain:  epoch  6, batch     8 | loss: 3.2648799Losses:  2.725182056427002 -0.0 0.8439218401908875
MemoryTrain:  epoch  6, batch     9 | loss: 2.7251821Losses:  3.314467191696167 0.45795711874961853 0.8333756923675537
MemoryTrain:  epoch  6, batch    10 | loss: 3.3144672Losses:  2.956378936767578 0.2290884405374527 0.825710654258728
MemoryTrain:  epoch  6, batch    11 | loss: 2.9563789Losses:  2.9515511989593506 0.2559458613395691 0.7694721817970276
MemoryTrain:  epoch  6, batch    12 | loss: 2.9515512Losses:  2.6457605361938477 0.28837820887565613 0.5136553049087524
MemoryTrain:  epoch  6, batch    13 | loss: 2.6457605Losses:  2.8576042652130127 0.24863913655281067 0.7001587748527527
MemoryTrain:  epoch  7, batch     0 | loss: 2.8576043Losses:  3.3277320861816406 0.47354570031166077 0.9003593921661377
MemoryTrain:  epoch  7, batch     1 | loss: 3.3277321Losses:  3.444578170776367 0.7793258428573608 0.7401601076126099
MemoryTrain:  epoch  7, batch     2 | loss: 3.4445782Losses:  2.7622127532958984 0.24169477820396423 0.634070098400116
MemoryTrain:  epoch  7, batch     3 | loss: 2.7622128Losses:  3.275723457336426 0.4964452385902405 0.8853650093078613
MemoryTrain:  epoch  7, batch     4 | loss: 3.2757235Losses:  2.9754157066345215 0.4676312804222107 0.6349285840988159
MemoryTrain:  epoch  7, batch     5 | loss: 2.9754157Losses:  3.201833724975586 0.5000354051589966 0.7855062484741211
MemoryTrain:  epoch  7, batch     6 | loss: 3.2018337Losses:  4.203389644622803 1.3184221982955933 0.8325586318969727
MemoryTrain:  epoch  7, batch     7 | loss: 4.2033896Losses:  2.8879761695861816 0.4573180675506592 0.5588886141777039
MemoryTrain:  epoch  7, batch     8 | loss: 2.8879762Losses:  2.865034818649292 0.25323885679244995 0.7055826187133789
MemoryTrain:  epoch  7, batch     9 | loss: 2.8650348Losses:  3.699423313140869 1.1941454410552979 0.6424492597579956
MemoryTrain:  epoch  7, batch    10 | loss: 3.6994233Losses:  3.0801026821136475 0.26874059438705444 0.7637802958488464
MemoryTrain:  epoch  7, batch    11 | loss: 3.0801027Losses:  3.435267210006714 0.8710260987281799 0.6717009544372559
MemoryTrain:  epoch  7, batch    12 | loss: 3.4352672Losses:  2.7083160877227783 0.5265936255455017 0.3263402283191681
MemoryTrain:  epoch  7, batch    13 | loss: 2.7083161Losses:  2.9051780700683594 0.2338738590478897 0.7531185150146484
MemoryTrain:  epoch  8, batch     0 | loss: 2.9051781Losses:  3.9740264415740967 1.3014860153198242 0.6906906962394714
MemoryTrain:  epoch  8, batch     1 | loss: 3.9740264Losses:  4.2923583984375 1.319305658340454 0.7684159874916077
MemoryTrain:  epoch  8, batch     2 | loss: 4.2923584Losses:  3.6708309650421143 1.0455818176269531 0.7086115479469299
MemoryTrain:  epoch  8, batch     3 | loss: 3.6708310Losses:  4.0078125 1.3482667207717896 0.6956872940063477
MemoryTrain:  epoch  8, batch     4 | loss: 4.0078125Losses:  3.5912415981292725 1.1732956171035767 0.5380694270133972
MemoryTrain:  epoch  8, batch     5 | loss: 3.5912416Losses:  3.1970014572143555 0.4802967309951782 0.8364135026931763
MemoryTrain:  epoch  8, batch     6 | loss: 3.1970015Losses:  3.941275119781494 1.2878444194793701 0.5737704038619995
MemoryTrain:  epoch  8, batch     7 | loss: 3.9412751Losses:  4.662174701690674 1.1591415405273438 0.6279982328414917
MemoryTrain:  epoch  8, batch     8 | loss: 4.6621747Losses:  3.2624623775482178 0.4977429509162903 0.7816255688667297
MemoryTrain:  epoch  8, batch     9 | loss: 3.2624624Losses:  3.372354030609131 0.24480657279491425 0.7168647050857544
MemoryTrain:  epoch  8, batch    10 | loss: 3.3723540Losses:  3.988773822784424 0.6982877850532532 0.695817768573761
MemoryTrain:  epoch  8, batch    11 | loss: 3.9887738Losses:  4.303533554077148 1.5353689193725586 0.5784573554992676
MemoryTrain:  epoch  8, batch    12 | loss: 4.3035336Losses:  2.8549904823303223 0.5220928192138672 0.4238284230232239
MemoryTrain:  epoch  8, batch    13 | loss: 2.8549905Losses:  4.651361465454102 1.3197135925292969 0.7004071474075317
MemoryTrain:  epoch  9, batch     0 | loss: 4.6513615Losses:  3.3848273754119873 0.8194969892501831 0.6981384754180908
MemoryTrain:  epoch  9, batch     1 | loss: 3.3848274Losses:  3.2417843341827393 0.4875839352607727 0.8945195078849792
MemoryTrain:  epoch  9, batch     2 | loss: 3.2417843Losses:  3.2493491172790527 0.4692056179046631 0.8884024620056152
MemoryTrain:  epoch  9, batch     3 | loss: 3.2493491Losses:  5.43895149230957 1.7144051790237427 0.650603175163269
MemoryTrain:  epoch  9, batch     4 | loss: 5.4389515Losses:  3.0244507789611816 0.228229358792305 0.6912180781364441
MemoryTrain:  epoch  9, batch     5 | loss: 3.0244508Losses:  2.8243086338043213 0.2377232164144516 0.6983560919761658
MemoryTrain:  epoch  9, batch     6 | loss: 2.8243086Losses:  2.656644821166992 -0.0 0.7830633521080017
MemoryTrain:  epoch  9, batch     7 | loss: 2.6566448Losses:  3.62986421585083 0.8472378849983215 0.8972221612930298
MemoryTrain:  epoch  9, batch     8 | loss: 3.6298642Losses:  3.5265069007873535 0.9391218423843384 0.7048443555831909
MemoryTrain:  epoch  9, batch     9 | loss: 3.5265069Losses:  3.063685178756714 0.23188287019729614 0.7492231726646423
MemoryTrain:  epoch  9, batch    10 | loss: 3.0636852Losses:  3.723076581954956 1.2827889919281006 0.5531083941459656
MemoryTrain:  epoch  9, batch    11 | loss: 3.7230766Losses:  3.16074275970459 0.44784078001976013 0.7685225605964661
MemoryTrain:  epoch  9, batch    12 | loss: 3.1607428Losses:  2.9189774990081787 -0.0 0.6562407612800598
MemoryTrain:  epoch  9, batch    13 | loss: 2.9189775
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 64.90%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 7.50%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 7.29%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 15.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 25.78%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 37.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 41.48%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 45.83%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 46.15%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 46.43%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 48.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 48.83%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.37%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 51.04%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 52.63%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 54.37%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 56.55%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 58.52%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.33%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.72%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.66%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 64.81%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 65.42%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 65.32%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 66.29%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 65.62%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 65.18%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 64.19%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 65.38%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 67.07%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 66.86%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 65.34%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 63.89%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 62.50%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 61.44%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 61.33%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 60.46%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 59.38%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 58.33%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 57.45%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 56.49%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 56.37%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 57.05%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 57.70%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 58.22%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 58.51%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 58.90%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 58.96%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 57.99%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 57.06%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 56.35%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 55.47%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 54.62%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 53.79%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 53.36%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 53.86%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 53.80%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 53.12%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 52.38%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 52.43%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 53.00%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 53.63%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 54.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 54.85%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 55.44%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 55.69%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 55.06%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 54.37%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 53.70%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 53.12%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 52.64%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 52.16%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 52.13%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 52.25%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 52.37%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 52.56%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 52.81%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 53.19%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 53.30%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 53.19%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 53.23%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 53.19%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 53.42%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 53.58%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 53.67%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 53.76%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 54.04%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 54.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 54.76%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 54.90%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 54.79%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 54.45%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 54.52%   [EVAL] batch:  105 | acc: 43.75%,  total acc: 54.42%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 54.26%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 54.34%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 54.53%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 54.60%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 54.84%   [EVAL] batch:  111 | acc: 87.50%,  total acc: 55.13%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 55.42%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 55.59%   [EVAL] batch:  114 | acc: 31.25%,  total acc: 55.38%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 55.17%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 55.13%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 55.40%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 55.62%   
cur_acc:  ['0.8693', '0.7708', '0.7768', '0.7118', '0.5795', '0.5312', '0.6490']
his_acc:  ['0.8693', '0.7939', '0.7234', '0.6763', '0.6188', '0.5935', '0.5562']
Clustering into  19  clusters
Clusters:  [ 0  4  5  0  2 11  0  3 15 18  1 13  0  1  4  7  0  9  7  1  2  5  5 17
 11  1  0 14  5  6 16  3 10 11  1  7  6  8  0  5 12]
Losses:  13.820655822753906 8.709649085998535 0.4081425666809082
CurrentTrain: epoch  0, batch     0 | loss: 13.8206558Losses:  8.777886390686035 4.235330581665039 0.3022756576538086
CurrentTrain: epoch  0, batch     1 | loss: 8.7778864Losses:  11.198673248291016 7.153951644897461 0.38516518473625183
CurrentTrain: epoch  1, batch     0 | loss: 11.1986732Losses:  6.299328804016113 2.7045512199401855 0.3294961452484131
CurrentTrain: epoch  1, batch     1 | loss: 6.2993288Losses:  9.265409469604492 5.94667911529541 0.3922669291496277
CurrentTrain: epoch  2, batch     0 | loss: 9.2654095Losses:  5.571906566619873 2.022111177444458 0.3751887083053589
CurrentTrain: epoch  2, batch     1 | loss: 5.5719066Losses:  8.644659042358398 5.695886135101318 0.3796444237232208
CurrentTrain: epoch  3, batch     0 | loss: 8.6446590Losses:  4.857449531555176 1.705959677696228 0.37662753462791443
CurrentTrain: epoch  3, batch     1 | loss: 4.8574495Losses:  9.221190452575684 6.423269748687744 0.3637622892856598
CurrentTrain: epoch  4, batch     0 | loss: 9.2211905Losses:  5.6969499588012695 3.1321616172790527 0.37848934531211853
CurrentTrain: epoch  4, batch     1 | loss: 5.6969500Losses:  9.071467399597168 6.458369255065918 0.3580140173435211
CurrentTrain: epoch  5, batch     0 | loss: 9.0714674Losses:  5.404093265533447 3.0003552436828613 0.3742975890636444
CurrentTrain: epoch  5, batch     1 | loss: 5.4040933Losses:  8.73249340057373 6.41066837310791 0.3595435321331024
CurrentTrain: epoch  6, batch     0 | loss: 8.7324934Losses:  4.583942890167236 2.4508540630340576 0.27666229009628296
CurrentTrain: epoch  6, batch     1 | loss: 4.5839429Losses:  8.175209999084473 5.9181413650512695 0.3533766269683838
CurrentTrain: epoch  7, batch     0 | loss: 8.1752100Losses:  4.280063152313232 2.1404881477355957 0.35923275351524353
CurrentTrain: epoch  7, batch     1 | loss: 4.2800632Losses:  7.6026997566223145 5.385039329528809 0.35067781805992126
CurrentTrain: epoch  8, batch     0 | loss: 7.6026998Losses:  3.5912513732910156 1.5406163930892944 0.35072213411331177
CurrentTrain: epoch  8, batch     1 | loss: 3.5912514Losses:  7.087848663330078 4.95914363861084 0.3483419716358185
CurrentTrain: epoch  9, batch     0 | loss: 7.0878487Losses:  3.0423619747161865 0.9741487503051758 0.3471851646900177
CurrentTrain: epoch  9, batch     1 | loss: 3.0423620
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the company was founded as a hobby in 1979 by the husband and wife team of tim and nina zagat , a pair of lawyers who started recuiting friends to provide their own ratings of the food , decor and service of restaurants they frequented .
Head Entity: zagat
Tail Entity: 1979
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, the tech startup was established by a group of engineers who aimed to revolutionize the way people interact with technology.  
Head Entity: tech startup  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization was created in 2005 to provide educational resources to underprivileged children in urban areas.  
Head Entity: non-profit organization  
Tail Entity: 2005  

Relation: organization founded  
Context: After years of planning, the community center officially opened its doors in 2010, serving as a hub for local activities and events.  
Head Entity: community center  
Tail Entity: 2010  

Relation: organization founded  
Context: The famous coffee chain was born in Seattle in 1971, when two friends decided to sell high-quality coffee beans.  
Head Entity: coffee chain  
Tail Entity: 1971  

Relation: organization founded  
Context: The environmental advocacy group was founded in 1985 by a coalition of activists dedicated to preserving natural habitats.  
Head Entity: environmental advocacy group  
Tail Entity: 1985  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john is celebrating his 30th birthday today.  
Head Entity: john  
Tail Entity: 30  

Relation: person age  
Context: the famous actor, robert downey jr., turned 56 last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: my grandmother just turned 75 years old last month.  
Head Entity: my grandmother  
Tail Entity: 75  

Relation: person age  
Context: emma is only 22 years old and already a successful entrepreneur.  
Head Entity: emma  
Tail Entity: 22  

Relation: person age  
Context: the scientist, dr. smith, is currently 45 years old.  
Head Entity: dr. smith  
Tail Entity: 45  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: forsberg was born in 1943 in huntsville , ala. , and grew up on long island in new york .
Head Entity: forsberg
Tail Entity: huntsville
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in 1990 in barcelona, spain, where she spent her childhood before moving to london.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during a press conference, it was revealed that michael was born in 1985 in los angeles, california, and later moved to new york.  
Head Entity: michael  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in 1975 in dublin, ireland, and often draws inspiration from his hometown in his novels.  
Head Entity: the famous author  
Tail Entity: dublin  

Relation: person city of birth  
Context: after years of research, the historian discovered that jane was born in 1962 in sydney, australia, which influenced her later work.  
Head Entity: jane  
Tail Entity: sydney  

Relation: person city of birth  
Context: the athlete proudly shared that he was born in 1992 in toronto, canada, before achieving fame in the sports world.  
Head Entity: the athlete  
Tail Entity: toronto  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: it was berger who made clarke a member of the white house principals committee when it met to discuss terrorist threats , allowing an otherwise middle-ranking nsc bureaucrat to treat tenet and secretary of state madeleine albright as equals -lrb- which the empire-building clarke was pleased to do -rrb- .
Head Entity: nsc
Tail Entity: white house principals committee
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
1. Relation: organization members  
   Context: The board of directors at Tech Innovations Inc. welcomed Sarah as a new member, recognizing her extensive experience in software development and project management.  
   Head Entity: Tech Innovations Inc.  
   Tail Entity: Sarah  

2. Relation: organization members  
   Context: During the annual conference, the National Wildlife Federation announced that John would be joining as a member, contributing his expertise in environmental conservation.  
   Head Entity: National Wildlife Federation  
   Tail Entity: John  

3. Relation: organization members  
   Context: The local chapter of the Red Cross celebrated the induction of Emily, who has volunteered for several years, as an official member of their organization.  
   Head Entity: Red Cross  
   Tail Entity: Emily  

4. Relation: organization members  
   Context: The United Nations appointed Dr. Smith as a member of the Human Rights Council, where he will advocate for global human rights issues.  
   Head Entity: United Nations  
   Tail Entity: Dr. Smith  

5. Relation: organization members  
   Context: The board of the American Medical Association voted unanimously to accept Dr. Lee as a new member, acknowledging her contributions to public health.  
   Head Entity: American Medical Association  
   Tail Entity: Dr. Lee  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how every member plays a vital role.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The imam addressed the congregation, urging them to strengthen their commitment to Islam and to support one another in their spiritual journeys.  
Head Entity: imam  
Tail Entity: Islam  

Relation: person religion  
Context: As a prominent figure in the church, she often shares her experiences and beliefs, inspiring many to embrace Christianity.  
Head Entity: she  
Tail Entity: Christianity  

Relation: person religion  
Context: The monk dedicated his life to Buddhism, teaching others about mindfulness and the path to enlightenment.  
Head Entity: monk  
Tail Entity: Buddhism  

Relation: person religion  
Context: He often participates in community events, proudly representing his faith as a member of the Sikh community.  
Head Entity: he  
Tail Entity: Sikh
Losses:  5.44725227355957 0.49398675560951233 0.9006432294845581
MemoryTrain:  epoch  0, batch     0 | loss: 5.4472523Losses:  5.585270881652832 1.3063324689865112 0.7264837026596069
MemoryTrain:  epoch  0, batch     1 | loss: 5.5852709Losses:  5.230030059814453 1.2245044708251953 0.6089820861816406
MemoryTrain:  epoch  0, batch     2 | loss: 5.2300301Losses:  5.163115501403809 0.7461453676223755 0.7777599692344666
MemoryTrain:  epoch  0, batch     3 | loss: 5.1631155Losses:  3.529449462890625 -0.0 0.7104261517524719
MemoryTrain:  epoch  0, batch     4 | loss: 3.5294495Losses:  5.7916364669799805 1.4908831119537354 0.7798264026641846
MemoryTrain:  epoch  0, batch     5 | loss: 5.7916365Losses:  4.596036434173584 0.6990482211112976 0.7940887808799744
MemoryTrain:  epoch  0, batch     6 | loss: 4.5960364Losses:  3.9477720260620117 0.749990701675415 0.7518018484115601
MemoryTrain:  epoch  0, batch     7 | loss: 3.9477720Losses:  5.028017520904541 0.7660858035087585 0.887863039970398
MemoryTrain:  epoch  0, batch     8 | loss: 5.0280175Losses:  4.179981708526611 1.01102614402771 0.7157241106033325
MemoryTrain:  epoch  0, batch     9 | loss: 4.1799817Losses:  4.313587665557861 0.9153451919555664 0.8328679800033569
MemoryTrain:  epoch  0, batch    10 | loss: 4.3135877Losses:  4.204893112182617 0.4938697814941406 0.7200236916542053
MemoryTrain:  epoch  0, batch    11 | loss: 4.2048931Losses:  3.5699856281280518 0.48614501953125 0.7227983474731445
MemoryTrain:  epoch  0, batch    12 | loss: 3.5699856Losses:  3.789726972579956 0.4829975366592407 0.7124722003936768
MemoryTrain:  epoch  0, batch    13 | loss: 3.7897270Losses:  3.853126049041748 0.23656484484672546 0.945278525352478
MemoryTrain:  epoch  0, batch    14 | loss: 3.8531260Losses:  4.943505764007568 0.3507649600505829 0.4563814103603363
MemoryTrain:  epoch  0, batch    15 | loss: 4.9435058Losses:  3.929569721221924 0.2558031380176544 0.8589520454406738
MemoryTrain:  epoch  1, batch     0 | loss: 3.9295697Losses:  4.276728630065918 0.8019691705703735 0.8180965781211853
MemoryTrain:  epoch  1, batch     1 | loss: 4.2767286Losses:  5.187231540679932 1.9757442474365234 0.45721155405044556
MemoryTrain:  epoch  1, batch     2 | loss: 5.1872315Losses:  3.78375244140625 0.4988822638988495 0.7548027038574219
MemoryTrain:  epoch  1, batch     3 | loss: 3.7837524Losses:  4.861291408538818 1.9527316093444824 0.6452077627182007
MemoryTrain:  epoch  1, batch     4 | loss: 4.8612914Losses:  3.1776702404022217 0.22497037053108215 0.8392655253410339
MemoryTrain:  epoch  1, batch     5 | loss: 3.1776702Losses:  3.8180794715881348 0.4936835467815399 0.7113701105117798
MemoryTrain:  epoch  1, batch     6 | loss: 3.8180795Losses:  4.368949890136719 0.7776427865028381 0.6516454219818115
MemoryTrain:  epoch  1, batch     7 | loss: 4.3689499Losses:  3.970041513442993 0.9667229652404785 0.6539323329925537
MemoryTrain:  epoch  1, batch     8 | loss: 3.9700415Losses:  4.584712028503418 1.1023569107055664 0.791522741317749
MemoryTrain:  epoch  1, batch     9 | loss: 4.5847120Losses:  3.5952749252319336 0.6993957757949829 0.7113460898399353
MemoryTrain:  epoch  1, batch    10 | loss: 3.5952749Losses:  4.359516143798828 1.0199114084243774 0.7325613498687744
MemoryTrain:  epoch  1, batch    11 | loss: 4.3595161Losses:  3.687851905822754 0.7447255849838257 0.760907769203186
MemoryTrain:  epoch  1, batch    12 | loss: 3.6878519Losses:  4.599040985107422 1.2399516105651855 0.7772502303123474
MemoryTrain:  epoch  1, batch    13 | loss: 4.5990410Losses:  3.253448963165283 0.45940300822257996 0.7191678285598755
MemoryTrain:  epoch  1, batch    14 | loss: 3.2534490Losses:  4.315040111541748 -0.0 0.45480820536613464
MemoryTrain:  epoch  1, batch    15 | loss: 4.3150401Losses:  4.242440223693848 1.6176741123199463 0.6382360458374023
MemoryTrain:  epoch  2, batch     0 | loss: 4.2424402Losses:  3.7149243354797363 0.7837474346160889 0.5725709199905396
MemoryTrain:  epoch  2, batch     1 | loss: 3.7149243Losses:  3.3430566787719727 0.48545071482658386 0.8246228098869324
MemoryTrain:  epoch  2, batch     2 | loss: 3.3430567Losses:  4.644911289215088 0.8571777939796448 0.7619684338569641
MemoryTrain:  epoch  2, batch     3 | loss: 4.6449113Losses:  3.653432846069336 0.49970853328704834 0.8111163377761841
MemoryTrain:  epoch  2, batch     4 | loss: 3.6534328Losses:  3.47309947013855 0.7130122780799866 0.7634444832801819
MemoryTrain:  epoch  2, batch     5 | loss: 3.4730995Losses:  3.607226848602295 0.6990645527839661 0.6842837929725647
MemoryTrain:  epoch  2, batch     6 | loss: 3.6072268Losses:  3.3906235694885254 0.49340689182281494 0.7248673439025879
MemoryTrain:  epoch  2, batch     7 | loss: 3.3906236Losses:  3.626854419708252 0.522233247756958 0.6452662348747253
MemoryTrain:  epoch  2, batch     8 | loss: 3.6268544Losses:  3.3159027099609375 0.2263675481081009 0.9120060801506042
MemoryTrain:  epoch  2, batch     9 | loss: 3.3159027Losses:  3.478851079940796 0.6632218360900879 0.6874211430549622
MemoryTrain:  epoch  2, batch    10 | loss: 3.4788511Losses:  3.8317108154296875 1.0556576251983643 0.7118757367134094
MemoryTrain:  epoch  2, batch    11 | loss: 3.8317108Losses:  3.7520973682403564 0.4911702573299408 0.9029316902160645
MemoryTrain:  epoch  2, batch    12 | loss: 3.7520974Losses:  3.077929735183716 0.25985556840896606 0.584155797958374
MemoryTrain:  epoch  2, batch    13 | loss: 3.0779297Losses:  3.438727378845215 0.7451727390289307 0.6696983575820923
MemoryTrain:  epoch  2, batch    14 | loss: 3.4387274Losses:  2.5628867149353027 -0.0 0.50616455078125
MemoryTrain:  epoch  2, batch    15 | loss: 2.5628867Losses:  3.323702812194824 0.46866804361343384 0.8647311925888062
MemoryTrain:  epoch  3, batch     0 | loss: 3.3237028Losses:  3.5463905334472656 0.7508020401000977 0.7448514103889465
MemoryTrain:  epoch  3, batch     1 | loss: 3.5463905Losses:  4.290774345397949 1.3242897987365723 0.8122162818908691
MemoryTrain:  epoch  3, batch     2 | loss: 4.2907743Losses:  3.9002695083618164 0.6563154458999634 0.7210873961448669
MemoryTrain:  epoch  3, batch     3 | loss: 3.9002695Losses:  3.1535696983337402 0.2347366362810135 0.9594075679779053
MemoryTrain:  epoch  3, batch     4 | loss: 3.1535697Losses:  3.295529365539551 0.7218475341796875 0.6359149217605591
MemoryTrain:  epoch  3, batch     5 | loss: 3.2955294Losses:  3.2800464630126953 0.4857959747314453 0.8226814270019531
MemoryTrain:  epoch  3, batch     6 | loss: 3.2800465Losses:  3.303682565689087 0.46343034505844116 0.7827327847480774
MemoryTrain:  epoch  3, batch     7 | loss: 3.3036826Losses:  3.4681615829467773 0.5142976641654968 0.8928834199905396
MemoryTrain:  epoch  3, batch     8 | loss: 3.4681616Losses:  3.0003726482391357 0.2386072278022766 0.7960452437400818
MemoryTrain:  epoch  3, batch     9 | loss: 3.0003726Losses:  3.6100878715515137 0.46563512086868286 0.7181829214096069
MemoryTrain:  epoch  3, batch    10 | loss: 3.6100879Losses:  2.9659550189971924 -0.0 0.8304486870765686
MemoryTrain:  epoch  3, batch    11 | loss: 2.9659550Losses:  3.7278261184692383 1.0412161350250244 0.6824963688850403
MemoryTrain:  epoch  3, batch    12 | loss: 3.7278261Losses:  3.105891704559326 0.2506805956363678 0.7724397778511047
MemoryTrain:  epoch  3, batch    13 | loss: 3.1058917Losses:  3.4598844051361084 0.463442325592041 0.9030525088310242
MemoryTrain:  epoch  3, batch    14 | loss: 3.4598844Losses:  2.553030252456665 0.27985680103302 0.41843196749687195
MemoryTrain:  epoch  3, batch    15 | loss: 2.5530303Losses:  3.3967576026916504 0.4713582992553711 0.8921954035758972
MemoryTrain:  epoch  4, batch     0 | loss: 3.3967576Losses:  3.100233793258667 0.4828674793243408 0.5660889744758606
MemoryTrain:  epoch  4, batch     1 | loss: 3.1002338Losses:  3.26689076423645 0.5201074481010437 0.7083181738853455
MemoryTrain:  epoch  4, batch     2 | loss: 3.2668908Losses:  3.7955617904663086 0.48287534713745117 0.6814011335372925
MemoryTrain:  epoch  4, batch     3 | loss: 3.7955618Losses:  3.7970879077911377 0.5106917023658752 0.9106637835502625
MemoryTrain:  epoch  4, batch     4 | loss: 3.7970879Losses:  3.4782376289367676 0.4876948297023773 0.7592066526412964
MemoryTrain:  epoch  4, batch     5 | loss: 3.4782376Losses:  3.394293785095215 0.7464183568954468 0.7562638521194458
MemoryTrain:  epoch  4, batch     6 | loss: 3.3942938Losses:  3.7122435569763184 1.0640233755111694 0.6363171339035034
MemoryTrain:  epoch  4, batch     7 | loss: 3.7122436Losses:  3.689769983291626 0.7381483316421509 0.6779373288154602
MemoryTrain:  epoch  4, batch     8 | loss: 3.6897700Losses:  3.1476833820343018 0.24321132898330688 0.8405367732048035
MemoryTrain:  epoch  4, batch     9 | loss: 3.1476834Losses:  4.252866268157959 1.5995649099349976 0.7224023938179016
MemoryTrain:  epoch  4, batch    10 | loss: 4.2528663Losses:  3.326824188232422 0.6753168702125549 0.7718526721000671
MemoryTrain:  epoch  4, batch    11 | loss: 3.3268242Losses:  3.9820926189422607 1.285868525505066 0.7848432660102844
MemoryTrain:  epoch  4, batch    12 | loss: 3.9820926Losses:  4.079850196838379 1.1368088722229004 0.7533437013626099
MemoryTrain:  epoch  4, batch    13 | loss: 4.0798502Losses:  3.276684522628784 0.4590969979763031 0.8992974758148193
MemoryTrain:  epoch  4, batch    14 | loss: 3.2766845Losses:  3.42573618888855 -0.0 0.5349476933479309
MemoryTrain:  epoch  4, batch    15 | loss: 3.4257362Losses:  2.888049840927124 0.23611022531986237 0.6993076801300049
MemoryTrain:  epoch  5, batch     0 | loss: 2.8880498Losses:  3.1983118057250977 0.4684983789920807 0.7776533365249634
MemoryTrain:  epoch  5, batch     1 | loss: 3.1983118Losses:  3.3501462936401367 0.511113166809082 0.6665945053100586
MemoryTrain:  epoch  5, batch     2 | loss: 3.3501463Losses:  3.545274257659912 0.46298956871032715 0.9547731280326843
MemoryTrain:  epoch  5, batch     3 | loss: 3.5452743Losses:  3.475787878036499 0.6962882280349731 0.8319047093391418
MemoryTrain:  epoch  5, batch     4 | loss: 3.4757879Losses:  3.1944825649261475 0.5114833116531372 0.7778118252754211
MemoryTrain:  epoch  5, batch     5 | loss: 3.1944826Losses:  5.368245601654053 2.6863880157470703 0.7558053731918335
MemoryTrain:  epoch  5, batch     6 | loss: 5.3682456Losses:  3.040081739425659 0.22488731145858765 0.8453488945960999
MemoryTrain:  epoch  5, batch     7 | loss: 3.0400817Losses:  3.1218056678771973 0.24761247634887695 0.8984608054161072
MemoryTrain:  epoch  5, batch     8 | loss: 3.1218057Losses:  3.5760536193847656 0.9395313262939453 0.7149286866188049
MemoryTrain:  epoch  5, batch     9 | loss: 3.5760536Losses:  3.9575905799865723 1.3216662406921387 0.7293568849563599
MemoryTrain:  epoch  5, batch    10 | loss: 3.9575906Losses:  3.1176376342773438 0.2705901861190796 0.7503213882446289
MemoryTrain:  epoch  5, batch    11 | loss: 3.1176376Losses:  3.4317171573638916 0.47593092918395996 0.9039070010185242
MemoryTrain:  epoch  5, batch    12 | loss: 3.4317172Losses:  3.495075225830078 0.8102184534072876 0.5652355551719666
MemoryTrain:  epoch  5, batch    13 | loss: 3.4950752Losses:  3.450690746307373 0.6992978453636169 0.7320326566696167
MemoryTrain:  epoch  5, batch    14 | loss: 3.4506907Losses:  2.4788615703582764 0.300251305103302 0.3291826546192169
MemoryTrain:  epoch  5, batch    15 | loss: 2.4788616Losses:  4.053190231323242 1.2690529823303223 0.640815794467926
MemoryTrain:  epoch  6, batch     0 | loss: 4.0531902Losses:  3.80167818069458 1.3238399028778076 0.5695011615753174
MemoryTrain:  epoch  6, batch     1 | loss: 3.8016782Losses:  3.657853126525879 0.9283146262168884 0.781494140625
MemoryTrain:  epoch  6, batch     2 | loss: 3.6578531Losses:  3.112902879714966 0.25064989924430847 0.9550673961639404
MemoryTrain:  epoch  6, batch     3 | loss: 3.1129029Losses:  3.610435962677002 0.9530972242355347 0.7401474714279175
MemoryTrain:  epoch  6, batch     4 | loss: 3.6104360Losses:  3.8532238006591797 0.9919483661651611 0.7044543027877808
MemoryTrain:  epoch  6, batch     5 | loss: 3.8532238Losses:  3.912910223007202 1.2532291412353516 0.7696282863616943
MemoryTrain:  epoch  6, batch     6 | loss: 3.9129102Losses:  3.0971572399139404 0.2382279485464096 0.9441624283790588
MemoryTrain:  epoch  6, batch     7 | loss: 3.0971572Losses:  3.6432132720947266 1.0308122634887695 0.7547674775123596
MemoryTrain:  epoch  6, batch     8 | loss: 3.6432133Losses:  3.5515940189361572 0.9436003565788269 0.7267854809761047
MemoryTrain:  epoch  6, batch     9 | loss: 3.5515940Losses:  3.2212536334991455 0.5028076767921448 0.6627941131591797
MemoryTrain:  epoch  6, batch    10 | loss: 3.2212536Losses:  2.9804270267486572 0.2321719527244568 0.8369935154914856
MemoryTrain:  epoch  6, batch    11 | loss: 2.9804270Losses:  3.366724967956543 0.6830260753631592 0.800212025642395
MemoryTrain:  epoch  6, batch    12 | loss: 3.3667250Losses:  2.625833034515381 -0.0 0.7089115381240845
MemoryTrain:  epoch  6, batch    13 | loss: 2.6258330Losses:  3.6484463214874268 1.0931633710861206 0.6510030627250671
MemoryTrain:  epoch  6, batch    14 | loss: 3.6484463Losses:  2.551327705383301 0.32580697536468506 0.3694802522659302
MemoryTrain:  epoch  6, batch    15 | loss: 2.5513277Losses:  3.242699146270752 0.6914070844650269 0.6903846859931946
MemoryTrain:  epoch  7, batch     0 | loss: 3.2426991Losses:  3.4476916790008545 0.7435839176177979 0.785134494304657
MemoryTrain:  epoch  7, batch     1 | loss: 3.4476917Losses:  2.8623123168945312 0.2302088886499405 0.7727269530296326
MemoryTrain:  epoch  7, batch     2 | loss: 2.8623123Losses:  3.46911883354187 0.900901198387146 0.6882340908050537
MemoryTrain:  epoch  7, batch     3 | loss: 3.4691188Losses:  3.558363437652588 0.8552501201629639 0.794468879699707
MemoryTrain:  epoch  7, batch     4 | loss: 3.5583634Losses:  3.2462353706359863 0.5059898495674133 0.8387957811355591
MemoryTrain:  epoch  7, batch     5 | loss: 3.2462354Losses:  3.2110724449157715 0.48729145526885986 0.7925931215286255
MemoryTrain:  epoch  7, batch     6 | loss: 3.2110724Losses:  3.0308568477630615 0.2491065263748169 0.8323461413383484
MemoryTrain:  epoch  7, batch     7 | loss: 3.0308568Losses:  3.1758716106414795 0.4559762179851532 0.6243181228637695
MemoryTrain:  epoch  7, batch     8 | loss: 3.1758716Losses:  3.6044201850891113 0.9699214696884155 0.7096604108810425
MemoryTrain:  epoch  7, batch     9 | loss: 3.6044202Losses:  3.7642879486083984 1.0392332077026367 0.8301690220832825
MemoryTrain:  epoch  7, batch    10 | loss: 3.7642879Losses:  2.9082765579223633 0.26478591561317444 0.6450992822647095
MemoryTrain:  epoch  7, batch    11 | loss: 2.9082766Losses:  3.3096370697021484 0.7164970636367798 0.6333229541778564
MemoryTrain:  epoch  7, batch    12 | loss: 3.3096371Losses:  3.7989206314086914 1.1063306331634521 0.830400824546814
MemoryTrain:  epoch  7, batch    13 | loss: 3.7989206Losses:  3.2401084899902344 0.6720355749130249 0.694349467754364
MemoryTrain:  epoch  7, batch    14 | loss: 3.2401085Losses:  2.668980598449707 0.5216572880744934 0.3008044958114624
MemoryTrain:  epoch  7, batch    15 | loss: 2.6689806Losses:  3.363142490386963 0.7325425744056702 0.6387403011322021
MemoryTrain:  epoch  8, batch     0 | loss: 3.3631425Losses:  3.9061691761016846 1.2656620740890503 0.7794725298881531
MemoryTrain:  epoch  8, batch     1 | loss: 3.9061692Losses:  3.069727897644043 0.47771093249320984 0.7000956535339355
MemoryTrain:  epoch  8, batch     2 | loss: 3.0697279Losses:  2.8622846603393555 0.49776703119277954 0.49114108085632324
MemoryTrain:  epoch  8, batch     3 | loss: 2.8622847Losses:  3.5065479278564453 0.7047833204269409 0.9123090505599976
MemoryTrain:  epoch  8, batch     4 | loss: 3.5065479Losses:  2.8197779655456543 -0.0 0.9405084848403931
MemoryTrain:  epoch  8, batch     5 | loss: 2.8197780Losses:  3.9389398097991943 1.3480525016784668 0.660060703754425
MemoryTrain:  epoch  8, batch     6 | loss: 3.9389398Losses:  3.0035037994384766 0.4637415111064911 0.69915771484375
MemoryTrain:  epoch  8, batch     7 | loss: 3.0035038Losses:  3.095536470413208 0.49975964426994324 0.7047650218009949
MemoryTrain:  epoch  8, batch     8 | loss: 3.0955365Losses:  3.2395920753479004 0.7208430767059326 0.6396626234054565
MemoryTrain:  epoch  8, batch     9 | loss: 3.2395921Losses:  3.2425215244293213 0.49601954221725464 0.8471640944480896
MemoryTrain:  epoch  8, batch    10 | loss: 3.2425215Losses:  3.4509778022766113 0.7373952865600586 0.7817363739013672
MemoryTrain:  epoch  8, batch    11 | loss: 3.4509778Losses:  3.2786898612976074 0.4533731937408447 0.933636486530304
MemoryTrain:  epoch  8, batch    12 | loss: 3.2786899Losses:  2.9944067001342773 0.24743320047855377 0.8807284235954285
MemoryTrain:  epoch  8, batch    13 | loss: 2.9944067Losses:  2.6388211250305176 -0.0 0.7644131779670715
MemoryTrain:  epoch  8, batch    14 | loss: 2.6388211Losses:  2.2741639614105225 -0.0 0.3982718884944916
MemoryTrain:  epoch  8, batch    15 | loss: 2.2741640Losses:  3.035447359085083 0.4831898510456085 0.6557931900024414
MemoryTrain:  epoch  9, batch     0 | loss: 3.0354474Losses:  2.865790605545044 0.46684491634368896 0.5356686115264893
MemoryTrain:  epoch  9, batch     1 | loss: 2.8657906Losses:  2.854898691177368 0.2171373814344406 0.7056760787963867
MemoryTrain:  epoch  9, batch     2 | loss: 2.8548987Losses:  3.2400147914886475 0.7054466605186462 0.6477975249290466
MemoryTrain:  epoch  9, batch     3 | loss: 3.2400148Losses:  2.8390326499938965 0.24402587115764618 0.7119325399398804
MemoryTrain:  epoch  9, batch     4 | loss: 2.8390326Losses:  3.293783664703369 0.45792141556739807 0.941291868686676
MemoryTrain:  epoch  9, batch     5 | loss: 3.2937837Losses:  3.9072160720825195 1.2411155700683594 0.781007707118988
MemoryTrain:  epoch  9, batch     6 | loss: 3.9072161Losses:  2.694845199584961 -0.0 0.8194541931152344
MemoryTrain:  epoch  9, batch     7 | loss: 2.6948452Losses:  3.424438238143921 0.6967043876647949 0.7619895339012146
MemoryTrain:  epoch  9, batch     8 | loss: 3.4244382Losses:  3.3708112239837646 0.7274433970451355 0.7828803062438965
MemoryTrain:  epoch  9, batch     9 | loss: 3.3708112Losses:  3.5033228397369385 0.8778589963912964 0.762350857257843
MemoryTrain:  epoch  9, batch    10 | loss: 3.5033228Losses:  3.2347097396850586 0.7059170603752136 0.6534327268600464
MemoryTrain:  epoch  9, batch    11 | loss: 3.2347097Losses:  3.7262606620788574 1.0535707473754883 0.7878764271736145
MemoryTrain:  epoch  9, batch    12 | loss: 3.7262607Losses:  3.0285680294036865 0.24758371710777283 0.8838221430778503
MemoryTrain:  epoch  9, batch    13 | loss: 3.0285680Losses:  2.8310129642486572 0.2271670550107956 0.7554845809936523
MemoryTrain:  epoch  9, batch    14 | loss: 2.8310130Losses:  2.255981206893921 -0.0 0.3867396414279938
MemoryTrain:  epoch  9, batch    15 | loss: 2.2559812
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 97.22%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 78.12%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 7.29%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 14.29%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 19.53%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 26.39%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 30.00%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 32.95%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 36.98%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 37.02%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 37.95%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 40.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 41.02%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 43.01%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 44.10%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 45.39%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 47.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.70%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 51.99%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 54.08%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 55.73%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.13%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 60.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 62.28%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 63.10%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 63.87%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 64.20%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 63.60%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 63.39%   [EVAL] batch:   35 | acc: 37.50%,  total acc: 62.67%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 62.16%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 62.83%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 63.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 65.12%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 63.64%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 62.22%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 60.87%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 59.84%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 59.77%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 58.93%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 57.88%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 56.86%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 56.01%   [EVAL] batch:   52 | acc: 0.00%,  total acc: 54.95%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 54.86%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 55.57%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 56.91%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 57.22%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 57.73%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 57.92%   [EVAL] batch:   60 | acc: 0.00%,  total acc: 56.97%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 56.05%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 55.16%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 54.30%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 53.46%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 52.65%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 52.15%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 52.67%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 52.72%   [EVAL] batch:   69 | acc: 6.25%,  total acc: 52.05%   [EVAL] batch:   70 | acc: 0.00%,  total acc: 51.32%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 51.39%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 52.05%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 52.70%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 53.33%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 53.95%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 54.55%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 54.81%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 54.19%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 53.52%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 52.85%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 52.29%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 51.66%   [EVAL] batch:   83 | acc: 0.00%,  total acc: 51.04%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 51.03%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 51.24%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 51.44%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 51.63%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 51.97%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 52.36%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 52.40%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 52.45%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 52.55%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 52.66%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 52.89%   [EVAL] batch:   95 | acc: 75.00%,  total acc: 53.12%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 53.35%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 53.57%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 53.85%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 54.12%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 54.58%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 54.78%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 54.73%   [EVAL] batch:  103 | acc: 12.50%,  total acc: 54.33%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 54.40%   [EVAL] batch:  105 | acc: 37.50%,  total acc: 54.25%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 54.15%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 54.22%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 54.42%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 54.60%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 54.79%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 54.85%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 55.03%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 54.99%   [EVAL] batch:  114 | acc: 12.50%,  total acc: 54.62%   [EVAL] batch:  115 | acc: 25.00%,  total acc: 54.36%   [EVAL] batch:  116 | acc: 12.50%,  total acc: 54.01%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 54.18%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 54.36%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 54.74%   [EVAL] batch:  120 | acc: 100.00%,  total acc: 55.11%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 55.48%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 55.84%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 56.20%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 56.55%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 56.89%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 57.23%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 57.28%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 56.88%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 56.73%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 56.77%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 56.87%   [EVAL] batch:  132 | acc: 43.75%,  total acc: 56.77%   
cur_acc:  ['0.8693', '0.7708', '0.7768', '0.7118', '0.5795', '0.5312', '0.6490', '0.7812']
his_acc:  ['0.8693', '0.7939', '0.7234', '0.6763', '0.6188', '0.5935', '0.5562', '0.5677']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 1 0]
Losses:  21.52346420288086 7.577602386474609 0.6503322720527649
CurrentTrain: epoch  0, batch     0 | loss: 21.5234642Losses:  27.36985206604004 13.675230026245117 0.6334609985351562
CurrentTrain: epoch  0, batch     1 | loss: 27.3698521Losses:  23.33268165588379 9.576217651367188 0.6254751086235046
CurrentTrain: epoch  0, batch     2 | loss: 23.3326817Losses:  25.501583099365234 12.324003219604492 0.5926978588104248
CurrentTrain: epoch  0, batch     3 | loss: 25.5015831Losses:  22.28077507019043 8.972999572753906 0.5951621532440186
CurrentTrain: epoch  0, batch     4 | loss: 22.2807751Losses:  23.960769653320312 10.889913558959961 0.5959562659263611
CurrentTrain: epoch  0, batch     5 | loss: 23.9607697Losses:  21.64835548400879 8.747546195983887 0.5753891468048096
CurrentTrain: epoch  0, batch     6 | loss: 21.6483555Losses:  20.054319381713867 7.18316650390625 0.5612980723381042
CurrentTrain: epoch  0, batch     7 | loss: 20.0543194Losses:  23.108932495117188 10.470720291137695 0.507012128829956
CurrentTrain: epoch  0, batch     8 | loss: 23.1089325Losses:  21.464630126953125 8.848984718322754 0.564643383026123
CurrentTrain: epoch  0, batch     9 | loss: 21.4646301Losses:  23.388879776000977 11.081949234008789 0.515228807926178
CurrentTrain: epoch  0, batch    10 | loss: 23.3888798Losses:  23.503602981567383 11.319602966308594 0.4924139678478241
CurrentTrain: epoch  0, batch    11 | loss: 23.5036030Losses:  23.05845832824707 11.335543632507324 0.4615795910358429
CurrentTrain: epoch  0, batch    12 | loss: 23.0584583Losses:  26.875823974609375 14.50745964050293 0.47685861587524414
CurrentTrain: epoch  0, batch    13 | loss: 26.8758240Losses:  22.734012603759766 10.470396041870117 0.46795889735221863
CurrentTrain: epoch  0, batch    14 | loss: 22.7340126Losses:  20.69978141784668 8.633015632629395 0.4439457058906555
CurrentTrain: epoch  0, batch    15 | loss: 20.6997814Losses:  19.6554012298584 7.888984680175781 0.4326724410057068
CurrentTrain: epoch  0, batch    16 | loss: 19.6554012Losses:  20.08089828491211 8.688749313354492 0.39742356538772583
CurrentTrain: epoch  0, batch    17 | loss: 20.0808983Losses:  20.082984924316406 7.756923675537109 0.43743085861206055
CurrentTrain: epoch  0, batch    18 | loss: 20.0829849Losses:  18.95432472229004 7.359433650970459 0.3984353542327881
CurrentTrain: epoch  0, batch    19 | loss: 18.9543247Losses:  22.4089412689209 10.412300109863281 0.3566366732120514
CurrentTrain: epoch  0, batch    20 | loss: 22.4089413Losses:  21.914907455444336 10.200039863586426 0.395955890417099
CurrentTrain: epoch  0, batch    21 | loss: 21.9149075Losses:  20.25486946105957 8.49108600616455 0.3666554093360901
CurrentTrain: epoch  0, batch    22 | loss: 20.2548695Losses:  18.975263595581055 7.926403999328613 0.29584985971450806
CurrentTrain: epoch  0, batch    23 | loss: 18.9752636Losses:  18.068073272705078 7.481583595275879 0.2859043478965759
CurrentTrain: epoch  0, batch    24 | loss: 18.0680733Losses:  19.670753479003906 9.002952575683594 0.2526371479034424
CurrentTrain: epoch  0, batch    25 | loss: 19.6707535Losses:  21.299598693847656 9.381254196166992 0.3690744936466217
CurrentTrain: epoch  0, batch    26 | loss: 21.2995987Losses:  23.444229125976562 12.200185775756836 0.29386013746261597
CurrentTrain: epoch  0, batch    27 | loss: 23.4442291Losses:  17.526723861694336 6.813063144683838 0.2843720614910126
CurrentTrain: epoch  0, batch    28 | loss: 17.5267239Losses:  17.828472137451172 6.995780944824219 0.28754210472106934
CurrentTrain: epoch  0, batch    29 | loss: 17.8284721Losses:  17.494367599487305 6.754099369049072 0.33005422353744507
CurrentTrain: epoch  0, batch    30 | loss: 17.4943676Losses:  19.615846633911133 9.02491569519043 0.33629482984542847
CurrentTrain: epoch  0, batch    31 | loss: 19.6158466Losses:  19.5922908782959 8.248319625854492 0.3250185251235962
CurrentTrain: epoch  0, batch    32 | loss: 19.5922909Losses:  22.7019100189209 11.097768783569336 0.24784792959690094
CurrentTrain: epoch  0, batch    33 | loss: 22.7019100Losses:  17.860801696777344 7.325587272644043 0.33266377449035645
CurrentTrain: epoch  0, batch    34 | loss: 17.8608017Losses:  19.833005905151367 8.331769943237305 0.26689666509628296
CurrentTrain: epoch  0, batch    35 | loss: 19.8330059Losses:  17.281015396118164 7.060671329498291 0.3147957921028137
CurrentTrain: epoch  0, batch    36 | loss: 17.2810154Losses:  13.626019477844238 2.838961601257324 0.20838531851768494
CurrentTrain: epoch  0, batch    37 | loss: 13.6260195Losses:  22.07362937927246 11.833183288574219 0.1740805208683014
CurrentTrain: epoch  1, batch     0 | loss: 22.0736294Losses:  18.707347869873047 8.332083702087402 0.264320969581604
CurrentTrain: epoch  1, batch     1 | loss: 18.7073479Losses:  17.810565948486328 7.744918346405029 0.23703351616859436
CurrentTrain: epoch  1, batch     2 | loss: 17.8105659Losses:  17.520891189575195 7.460202217102051 0.2045799344778061
CurrentTrain: epoch  1, batch     3 | loss: 17.5208912Losses:  18.503379821777344 8.171747207641602 0.22695061564445496
CurrentTrain: epoch  1, batch     4 | loss: 18.5033798Losses:  20.079566955566406 9.766931533813477 0.20720213651657104
CurrentTrain: epoch  1, batch     5 | loss: 20.0795670Losses:  22.27716827392578 12.007308006286621 0.1696699857711792
CurrentTrain: epoch  1, batch     6 | loss: 22.2771683Losses:  17.214088439941406 7.484330654144287 0.21786953508853912
CurrentTrain: epoch  1, batch     7 | loss: 17.2140884Losses:  17.837890625 8.196027755737305 0.19530095160007477
CurrentTrain: epoch  1, batch     8 | loss: 17.8378906Losses:  19.646780014038086 9.655167579650879 0.2348307967185974
CurrentTrain: epoch  1, batch     9 | loss: 19.6467800Losses:  15.94297981262207 6.476161956787109 0.19326245784759521
CurrentTrain: epoch  1, batch    10 | loss: 15.9429798Losses:  15.969527244567871 6.3000993728637695 0.17276230454444885
CurrentTrain: epoch  1, batch    11 | loss: 15.9695272Losses:  16.490554809570312 7.440930366516113 0.18036749958992004
CurrentTrain: epoch  1, batch    12 | loss: 16.4905548Losses:  18.73805809020996 8.781493186950684 0.16818071901798248
CurrentTrain: epoch  1, batch    13 | loss: 18.7380581Losses:  16.02245330810547 6.533490180969238 0.1498340666294098
CurrentTrain: epoch  1, batch    14 | loss: 16.0224533Losses:  15.03691291809082 5.370882034301758 0.18062818050384521
CurrentTrain: epoch  1, batch    15 | loss: 15.0369129Losses:  21.909053802490234 11.478707313537598 0.16333681344985962
CurrentTrain: epoch  1, batch    16 | loss: 21.9090538Losses:  19.316511154174805 9.736800193786621 0.18929101526737213
CurrentTrain: epoch  1, batch    17 | loss: 19.3165112Losses:  16.905607223510742 7.225757122039795 0.1643979549407959
CurrentTrain: epoch  1, batch    18 | loss: 16.9056072Losses:  18.1546573638916 8.85141372680664 0.17295370995998383
CurrentTrain: epoch  1, batch    19 | loss: 18.1546574Losses:  16.279829025268555 6.862334728240967 0.1803184151649475
CurrentTrain: epoch  1, batch    20 | loss: 16.2798290Losses:  18.10845184326172 8.012174606323242 0.1294066309928894
CurrentTrain: epoch  1, batch    21 | loss: 18.1084518Losses:  18.738876342773438 10.045644760131836 0.14314009249210358
CurrentTrain: epoch  1, batch    22 | loss: 18.7388763Losses:  16.361305236816406 7.265666484832764 0.14535629749298096
CurrentTrain: epoch  1, batch    23 | loss: 16.3613052Losses:  19.1196346282959 9.695505142211914 0.15706050395965576
CurrentTrain: epoch  1, batch    24 | loss: 19.1196346Losses:  15.613077163696289 5.960324287414551 0.1369379609823227
CurrentTrain: epoch  1, batch    25 | loss: 15.6130772Losses:  16.560169219970703 6.90478515625 0.13150162994861603
CurrentTrain: epoch  1, batch    26 | loss: 16.5601692Losses:  16.61936378479004 7.44473123550415 0.13465847074985504
CurrentTrain: epoch  1, batch    27 | loss: 16.6193638Losses:  17.901405334472656 9.803693771362305 0.12260403484106064
CurrentTrain: epoch  1, batch    28 | loss: 17.9014053Losses:  15.72885799407959 6.0745158195495605 0.16525523364543915
CurrentTrain: epoch  1, batch    29 | loss: 15.7288580Losses:  16.05870246887207 6.721548080444336 0.11457566916942596
CurrentTrain: epoch  1, batch    30 | loss: 16.0587025Losses:  18.165504455566406 8.122657775878906 0.15607360005378723
CurrentTrain: epoch  1, batch    31 | loss: 18.1655045Losses:  14.418088912963867 6.061409950256348 0.12081065773963928
CurrentTrain: epoch  1, batch    32 | loss: 14.4180889Losses:  17.674604415893555 8.27159309387207 0.10606177151203156
CurrentTrain: epoch  1, batch    33 | loss: 17.6746044Losses:  18.753864288330078 9.797558784484863 0.10045232623815536
CurrentTrain: epoch  1, batch    34 | loss: 18.7538643Losses:  16.085800170898438 7.54443359375 0.10793437063694
CurrentTrain: epoch  1, batch    35 | loss: 16.0858002Losses:  15.3909912109375 6.598541259765625 0.08601601421833038
CurrentTrain: epoch  1, batch    36 | loss: 15.3909912Losses:  11.417880058288574 3.853020668029785 0.12934860587120056
CurrentTrain: epoch  1, batch    37 | loss: 11.4178801Losses:  13.667080879211426 6.120675563812256 0.11330899596214294
CurrentTrain: epoch  2, batch     0 | loss: 13.6670809Losses:  17.075681686401367 8.257014274597168 0.1099373996257782
CurrentTrain: epoch  2, batch     1 | loss: 17.0756817Losses:  14.307079315185547 5.683180809020996 0.08348928391933441
CurrentTrain: epoch  2, batch     2 | loss: 14.3070793Losses:  14.76303768157959 6.714482307434082 0.0855734646320343
CurrentTrain: epoch  2, batch     3 | loss: 14.7630377Losses:  15.691995620727539 6.8129072189331055 0.09901706874370575
CurrentTrain: epoch  2, batch     4 | loss: 15.6919956Losses:  15.856884002685547 7.888076305389404 0.08398477733135223
CurrentTrain: epoch  2, batch     5 | loss: 15.8568840Losses:  14.124963760375977 5.951905250549316 0.09741663187742233
CurrentTrain: epoch  2, batch     6 | loss: 14.1249638Losses:  16.035480499267578 8.356029510498047 0.09771592915058136
CurrentTrain: epoch  2, batch     7 | loss: 16.0354805Losses:  14.857792854309082 6.582699298858643 0.10541624575853348
CurrentTrain: epoch  2, batch     8 | loss: 14.8577929Losses:  13.611075401306152 6.185683250427246 0.08854745328426361
CurrentTrain: epoch  2, batch     9 | loss: 13.6110754Losses:  19.015344619750977 10.118383407592773 0.08248142898082733
CurrentTrain: epoch  2, batch    10 | loss: 19.0153446Losses:  18.910810470581055 9.210209846496582 0.10821689665317535
CurrentTrain: epoch  2, batch    11 | loss: 18.9108105Losses:  16.619325637817383 7.011247634887695 0.08594781160354614
CurrentTrain: epoch  2, batch    12 | loss: 16.6193256Losses:  17.860820770263672 8.280311584472656 0.09864795207977295
CurrentTrain: epoch  2, batch    13 | loss: 17.8608208Losses:  12.746027946472168 4.57365608215332 0.07853412628173828
CurrentTrain: epoch  2, batch    14 | loss: 12.7460279Losses:  18.173128128051758 9.063122749328613 0.06841862201690674
CurrentTrain: epoch  2, batch    15 | loss: 18.1731281Losses:  14.136127471923828 5.883998870849609 0.08116303384304047
CurrentTrain: epoch  2, batch    16 | loss: 14.1361275Losses:  14.711804389953613 6.3434157371521 0.06128580495715141
CurrentTrain: epoch  2, batch    17 | loss: 14.7118044Losses:  14.092129707336426 6.022609710693359 0.07887262105941772
CurrentTrain: epoch  2, batch    18 | loss: 14.0921297Losses:  15.816803932189941 7.077948570251465 0.07327702641487122
CurrentTrain: epoch  2, batch    19 | loss: 15.8168039Losses:  13.635204315185547 5.921525478363037 0.11027376353740692
CurrentTrain: epoch  2, batch    20 | loss: 13.6352043Losses:  15.8506441116333 7.009957313537598 0.08786961436271667
CurrentTrain: epoch  2, batch    21 | loss: 15.8506441Losses:  15.733851432800293 8.250100135803223 0.0902186781167984
CurrentTrain: epoch  2, batch    22 | loss: 15.7338514Losses:  16.70340347290039 7.622086048126221 0.07851902395486832
CurrentTrain: epoch  2, batch    23 | loss: 16.7034035Losses:  14.16154670715332 5.778413772583008 0.06581287086009979
CurrentTrain: epoch  2, batch    24 | loss: 14.1615467Losses:  15.052789688110352 7.595029830932617 0.07950560748577118
CurrentTrain: epoch  2, batch    25 | loss: 15.0527897Losses:  15.615585327148438 6.545661449432373 0.07081076502799988
CurrentTrain: epoch  2, batch    26 | loss: 15.6155853Losses:  13.892431259155273 5.415699481964111 0.07461780309677124
CurrentTrain: epoch  2, batch    27 | loss: 13.8924313Losses:  14.96986198425293 6.529763221740723 0.08493921160697937
CurrentTrain: epoch  2, batch    28 | loss: 14.9698620Losses:  14.877790451049805 6.351672172546387 0.060945115983486176
CurrentTrain: epoch  2, batch    29 | loss: 14.8777905Losses:  14.131054878234863 5.9856109619140625 0.059542570263147354
CurrentTrain: epoch  2, batch    30 | loss: 14.1310549Losses:  14.959837913513184 7.527420997619629 0.0781429260969162
CurrentTrain: epoch  2, batch    31 | loss: 14.9598379Losses:  13.705554962158203 5.570037841796875 0.06788088381290436
CurrentTrain: epoch  2, batch    32 | loss: 13.7055550Losses:  13.959986686706543 6.662193298339844 0.0927954614162445
CurrentTrain: epoch  2, batch    33 | loss: 13.9599867Losses:  16.747665405273438 7.9092631340026855 0.06817261129617691
CurrentTrain: epoch  2, batch    34 | loss: 16.7476654Losses:  12.274806022644043 4.513235569000244 0.06456472724676132
CurrentTrain: epoch  2, batch    35 | loss: 12.2748060Losses:  19.759336471557617 11.76305866241455 0.07530811429023743
CurrentTrain: epoch  2, batch    36 | loss: 19.7593365Losses:  9.904518127441406 1.688366174697876 0.07390443235635757
CurrentTrain: epoch  2, batch    37 | loss: 9.9045181Losses:  15.83669662475586 8.191496849060059 0.06136709079146385
CurrentTrain: epoch  3, batch     0 | loss: 15.8366966Losses:  18.34747314453125 9.403722763061523 0.08210182189941406
CurrentTrain: epoch  3, batch     1 | loss: 18.3474731Losses:  15.930619239807129 8.266263008117676 0.06820034235715866
CurrentTrain: epoch  3, batch     2 | loss: 15.9306192Losses:  13.694012641906738 5.28151273727417 0.06437360495328903
CurrentTrain: epoch  3, batch     3 | loss: 13.6940126Losses:  13.655152320861816 5.443358421325684 0.057548388838768005
CurrentTrain: epoch  3, batch     4 | loss: 13.6551523Losses:  14.98116683959961 7.500202178955078 0.055231381207704544
CurrentTrain: epoch  3, batch     5 | loss: 14.9811668Losses:  14.978544235229492 6.635850429534912 0.0589047335088253
CurrentTrain: epoch  3, batch     6 | loss: 14.9785442Losses:  13.18307876586914 5.445744514465332 0.05213781073689461
CurrentTrain: epoch  3, batch     7 | loss: 13.1830788Losses:  19.319522857666016 13.027812957763672 0.08757079392671585
CurrentTrain: epoch  3, batch     8 | loss: 19.3195229Losses:  13.411089897155762 5.652247428894043 0.06326964497566223
CurrentTrain: epoch  3, batch     9 | loss: 13.4110899Losses:  12.105267524719238 4.907885551452637 0.05465095117688179
CurrentTrain: epoch  3, batch    10 | loss: 12.1052675Losses:  14.517515182495117 5.892634391784668 0.0610588975250721
CurrentTrain: epoch  3, batch    11 | loss: 14.5175152Losses:  12.530282020568848 4.46994686126709 0.052900850772857666
CurrentTrain: epoch  3, batch    12 | loss: 12.5302820Losses:  13.495194435119629 5.716776371002197 0.05749954655766487
CurrentTrain: epoch  3, batch    13 | loss: 13.4951944Losses:  14.88464641571045 7.842316627502441 0.0791766345500946
CurrentTrain: epoch  3, batch    14 | loss: 14.8846464Losses:  12.215959548950195 4.888457775115967 0.05385151505470276
CurrentTrain: epoch  3, batch    15 | loss: 12.2159595Losses:  13.055832862854004 5.339852333068848 0.05323479324579239
CurrentTrain: epoch  3, batch    16 | loss: 13.0558329Losses:  13.208495140075684 6.258070945739746 0.06839808076620102
CurrentTrain: epoch  3, batch    17 | loss: 13.2084951Losses:  12.677977561950684 4.787286758422852 0.067104771733284
CurrentTrain: epoch  3, batch    18 | loss: 12.6779776Losses:  17.36248779296875 8.690811157226562 0.06613484025001526
CurrentTrain: epoch  3, batch    19 | loss: 17.3624878Losses:  14.180220603942871 5.762134075164795 0.054789092391729355
CurrentTrain: epoch  3, batch    20 | loss: 14.1802206Losses:  17.145627975463867 8.592058181762695 0.05851432681083679
CurrentTrain: epoch  3, batch    21 | loss: 17.1456280Losses:  16.682573318481445 8.386951446533203 0.0631624385714531
CurrentTrain: epoch  3, batch    22 | loss: 16.6825733Losses:  12.397929191589355 5.970620155334473 0.07263897359371185
CurrentTrain: epoch  3, batch    23 | loss: 12.3979292Losses:  14.809700965881348 7.429304122924805 0.061159659177064896
CurrentTrain: epoch  3, batch    24 | loss: 14.8097010Losses:  15.141438484191895 8.15555191040039 0.06951607763767242
CurrentTrain: epoch  3, batch    25 | loss: 15.1414385Losses:  14.708518981933594 7.133934497833252 0.06226354092359543
CurrentTrain: epoch  3, batch    26 | loss: 14.7085190Losses:  16.974550247192383 8.883984565734863 0.05549045652151108
CurrentTrain: epoch  3, batch    27 | loss: 16.9745502Losses:  18.87700653076172 10.518315315246582 0.05796848237514496
CurrentTrain: epoch  3, batch    28 | loss: 18.8770065Losses:  18.57990074157715 9.828264236450195 0.05042082071304321
CurrentTrain: epoch  3, batch    29 | loss: 18.5799007Losses:  13.27955150604248 6.324853420257568 0.05885601043701172
CurrentTrain: epoch  3, batch    30 | loss: 13.2795515Losses:  12.744129180908203 6.035121917724609 0.06249554827809334
CurrentTrain: epoch  3, batch    31 | loss: 12.7441292Losses:  19.039306640625 11.796420097351074 0.05127713456749916
CurrentTrain: epoch  3, batch    32 | loss: 19.0393066Losses:  18.894569396972656 10.535212516784668 0.042899418622255325
CurrentTrain: epoch  3, batch    33 | loss: 18.8945694