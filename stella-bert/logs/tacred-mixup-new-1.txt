#############params############
cuda:0
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.3724613
CurrentTrain: epoch  0, batch     1 | loss: 13.1578608
CurrentTrain: epoch  0, batch     2 | loss: 13.0914288
CurrentTrain: epoch  0, batch     3 | loss: 12.9665594
CurrentTrain: epoch  0, batch     4 | loss: 12.8442793
CurrentTrain: epoch  0, batch     5 | loss: 12.5822926
CurrentTrain: epoch  0, batch     6 | loss: 12.5752811
CurrentTrain: epoch  0, batch     7 | loss: 12.3465538
CurrentTrain: epoch  0, batch     8 | loss: 12.2965069
CurrentTrain: epoch  0, batch     9 | loss: 12.1475677
CurrentTrain: epoch  0, batch    10 | loss: 12.0400219
CurrentTrain: epoch  0, batch    11 | loss: 11.9480858
CurrentTrain: epoch  0, batch    12 | loss: 12.0638752
CurrentTrain: epoch  0, batch    13 | loss: 11.8167038
CurrentTrain: epoch  0, batch    14 | loss: 11.6889286
CurrentTrain: epoch  0, batch    15 | loss: 11.6254883
CurrentTrain: epoch  0, batch    16 | loss: 11.0940533
CurrentTrain: epoch  0, batch    17 | loss: 11.2020321
CurrentTrain: epoch  0, batch    18 | loss: 11.2489090
CurrentTrain: epoch  0, batch    19 | loss: 11.4948883
CurrentTrain: epoch  0, batch    20 | loss: 11.0849552
CurrentTrain: epoch  0, batch    21 | loss: 11.3767891
CurrentTrain: epoch  0, batch    22 | loss: 11.4738388
CurrentTrain: epoch  0, batch    23 | loss: 11.0327606
CurrentTrain: epoch  0, batch    24 | loss: 11.3491087
CurrentTrain: epoch  0, batch    25 | loss: 11.3099518
CurrentTrain: epoch  0, batch    26 | loss: 10.8578224
CurrentTrain: epoch  0, batch    27 | loss: 10.2841034
CurrentTrain: epoch  0, batch    28 | loss: 10.7051201
CurrentTrain: epoch  0, batch    29 | loss: 10.7940836
CurrentTrain: epoch  0, batch    30 | loss: 10.4195404
CurrentTrain: epoch  0, batch    31 | loss: 10.7609634
CurrentTrain: epoch  0, batch    32 | loss: 10.3042355
CurrentTrain: epoch  0, batch    33 | loss: 10.3129387
CurrentTrain: epoch  0, batch    34 | loss: 10.0250797
CurrentTrain: epoch  0, batch    35 | loss: 10.2144766
CurrentTrain: epoch  0, batch    36 | loss: 10.2306309
CurrentTrain: epoch  0, batch    37 | loss: 10.0852337
CurrentTrain: epoch  1, batch     0 | loss: 10.2014656
CurrentTrain: epoch  1, batch     1 | loss: 10.6530342
CurrentTrain: epoch  1, batch     2 | loss: 9.7890358
CurrentTrain: epoch  1, batch     3 | loss: 9.7257767
CurrentTrain: epoch  1, batch     4 | loss: 9.4564104
CurrentTrain: epoch  1, batch     5 | loss: 10.1626291
CurrentTrain: epoch  1, batch     6 | loss: 9.8237648
CurrentTrain: epoch  1, batch     7 | loss: 9.4988441
CurrentTrain: epoch  1, batch     8 | loss: 9.4991245
CurrentTrain: epoch  1, batch     9 | loss: 9.6452885
CurrentTrain: epoch  1, batch    10 | loss: 9.9743004
CurrentTrain: epoch  1, batch    11 | loss: 9.7963219
CurrentTrain: epoch  1, batch    12 | loss: 9.6937313
CurrentTrain: epoch  1, batch    13 | loss: 9.1966829
CurrentTrain: epoch  1, batch    14 | loss: 9.5502548
CurrentTrain: epoch  1, batch    15 | loss: 9.2058496
CurrentTrain: epoch  1, batch    16 | loss: 9.1888971
CurrentTrain: epoch  1, batch    17 | loss: 9.5938950
CurrentTrain: epoch  1, batch    18 | loss: 9.4860392
CurrentTrain: epoch  1, batch    19 | loss: 9.5833225
CurrentTrain: epoch  1, batch    20 | loss: 9.5087862
CurrentTrain: epoch  1, batch    21 | loss: 9.1296339
CurrentTrain: epoch  1, batch    22 | loss: 9.2063999
CurrentTrain: epoch  1, batch    23 | loss: 9.0002327
CurrentTrain: epoch  1, batch    24 | loss: 9.2855091
CurrentTrain: epoch  1, batch    25 | loss: 8.4403152
CurrentTrain: epoch  1, batch    26 | loss: 9.3554955
CurrentTrain: epoch  1, batch    27 | loss: 8.4032183
CurrentTrain: epoch  1, batch    28 | loss: 8.6823807
CurrentTrain: epoch  1, batch    29 | loss: 8.0479479
CurrentTrain: epoch  1, batch    30 | loss: 8.6466703
CurrentTrain: epoch  1, batch    31 | loss: 9.0862045
CurrentTrain: epoch  1, batch    32 | loss: 8.8169594
CurrentTrain: epoch  1, batch    33 | loss: 8.2732611
CurrentTrain: epoch  1, batch    34 | loss: 8.6010847
CurrentTrain: epoch  1, batch    35 | loss: 8.1362267
CurrentTrain: epoch  1, batch    36 | loss: 8.8653030
CurrentTrain: epoch  1, batch    37 | loss: 8.5752344
CurrentTrain: epoch  2, batch     0 | loss: 7.7833271
CurrentTrain: epoch  2, batch     1 | loss: 8.4801159
CurrentTrain: epoch  2, batch     2 | loss: 8.7112026
CurrentTrain: epoch  2, batch     3 | loss: 8.3481588
CurrentTrain: epoch  2, batch     4 | loss: 9.3701601
CurrentTrain: epoch  2, batch     5 | loss: 9.5443459
CurrentTrain: epoch  2, batch     6 | loss: 8.6426182
CurrentTrain: epoch  2, batch     7 | loss: 8.3060379
CurrentTrain: epoch  2, batch     8 | loss: 8.6485052
CurrentTrain: epoch  2, batch     9 | loss: 8.4162703
CurrentTrain: epoch  2, batch    10 | loss: 8.0237541
CurrentTrain: epoch  2, batch    11 | loss: 8.5527153
CurrentTrain: epoch  2, batch    12 | loss: 8.2105560
CurrentTrain: epoch  2, batch    13 | loss: 7.9770279
CurrentTrain: epoch  2, batch    14 | loss: 7.3642979
CurrentTrain: epoch  2, batch    15 | loss: 7.9607420
CurrentTrain: epoch  2, batch    16 | loss: 8.3288574
CurrentTrain: epoch  2, batch    17 | loss: 8.4686508
CurrentTrain: epoch  2, batch    18 | loss: 8.0185461
CurrentTrain: epoch  2, batch    19 | loss: 7.8661394
CurrentTrain: epoch  2, batch    20 | loss: 7.6271992
CurrentTrain: epoch  2, batch    21 | loss: 7.9648695
CurrentTrain: epoch  2, batch    22 | loss: 7.4678106
CurrentTrain: epoch  2, batch    23 | loss: 7.4899635
CurrentTrain: epoch  2, batch    24 | loss: 7.8132114
CurrentTrain: epoch  2, batch    25 | loss: 8.1924648
CurrentTrain: epoch  2, batch    26 | loss: 7.2895327
CurrentTrain: epoch  2, batch    27 | loss: 8.5118685
CurrentTrain: epoch  2, batch    28 | loss: 6.8255653
CurrentTrain: epoch  2, batch    29 | loss: 7.8991265
CurrentTrain: epoch  2, batch    30 | loss: 7.5748453
CurrentTrain: epoch  2, batch    31 | loss: 7.1347771
CurrentTrain: epoch  2, batch    32 | loss: 7.6812344
CurrentTrain: epoch  2, batch    33 | loss: 7.5430918
CurrentTrain: epoch  2, batch    34 | loss: 8.4325619
CurrentTrain: epoch  2, batch    35 | loss: 7.4143143
CurrentTrain: epoch  2, batch    36 | loss: 7.8018699
CurrentTrain: epoch  2, batch    37 | loss: 7.7435017
CurrentTrain: epoch  3, batch     0 | loss: 7.5232515
CurrentTrain: epoch  3, batch     1 | loss: 7.8361187
CurrentTrain: epoch  3, batch     2 | loss: 7.8839965
CurrentTrain: epoch  3, batch     3 | loss: 7.9287844
CurrentTrain: epoch  3, batch     4 | loss: 7.4654551
CurrentTrain: epoch  3, batch     5 | loss: 7.8325772
CurrentTrain: epoch  3, batch     6 | loss: 8.4771061
CurrentTrain: epoch  3, batch     7 | loss: 6.9571447
CurrentTrain: epoch  3, batch     8 | loss: 8.0827675
CurrentTrain: epoch  3, batch     9 | loss: 7.7508407
CurrentTrain: epoch  3, batch    10 | loss: 7.0086536
CurrentTrain: epoch  3, batch    11 | loss: 6.6070518
CurrentTrain: epoch  3, batch    12 | loss: 7.7624445
CurrentTrain: epoch  3, batch    13 | loss: 8.1287346
CurrentTrain: epoch  3, batch    14 | loss: 7.2786741
CurrentTrain: epoch  3, batch    15 | loss: 7.4686642
CurrentTrain: epoch  3, batch    16 | loss: 8.3591795
CurrentTrain: epoch  3, batch    17 | loss: 7.4006367
CurrentTrain: epoch  3, batch    18 | loss: 7.4785576
CurrentTrain: epoch  3, batch    19 | loss: 7.9789510
CurrentTrain: epoch  3, batch    20 | loss: 7.5735807
CurrentTrain: epoch  3, batch    21 | loss: 7.4664931
CurrentTrain: epoch  3, batch    22 | loss: 7.6469474
CurrentTrain: epoch  3, batch    23 | loss: 7.7438240
CurrentTrain: epoch  3, batch    24 | loss: 6.4601979
CurrentTrain: epoch  3, batch    25 | loss: 6.9148345
CurrentTrain: epoch  3, batch    26 | loss: 6.8151793
CurrentTrain: epoch  3, batch    27 | loss: 7.9257421
CurrentTrain: epoch  3, batch    28 | loss: 7.3877330
CurrentTrain: epoch  3, batch    29 | loss: 6.2554440
CurrentTrain: epoch  3, batch    30 | loss: 7.2944155
CurrentTrain: epoch  3, batch    31 | loss: 6.9774604
CurrentTrain: epoch  3, batch    32 | loss: 6.2400208
CurrentTrain: epoch  3, batch    33 | loss: 6.4645548
CurrentTrain: epoch  3, batch    34 | loss: 6.7722120
CurrentTrain: epoch  3, batch    35 | loss: 6.4985266
CurrentTrain: epoch  3, batch    36 | loss: 6.8035321
CurrentTrain: epoch  3, batch    37 | loss: 6.8702993
CurrentTrain: epoch  4, batch     0 | loss: 7.0334969
CurrentTrain: epoch  4, batch     1 | loss: 7.0049124
CurrentTrain: epoch  4, batch     2 | loss: 5.8247557
CurrentTrain: epoch  4, batch     3 | loss: 6.8830619
CurrentTrain: epoch  4, batch     4 | loss: 6.9699078
CurrentTrain: epoch  4, batch     5 | loss: 6.9527235
CurrentTrain: epoch  4, batch     6 | loss: 6.2295132
CurrentTrain: epoch  4, batch     7 | loss: 6.9355526
CurrentTrain: epoch  4, batch     8 | loss: 7.7385564
CurrentTrain: epoch  4, batch     9 | loss: 7.0533419
CurrentTrain: epoch  4, batch    10 | loss: 7.2315435
CurrentTrain: epoch  4, batch    11 | loss: 6.2178564
CurrentTrain: epoch  4, batch    12 | loss: 6.9987230
CurrentTrain: epoch  4, batch    13 | loss: 6.5305729
CurrentTrain: epoch  4, batch    14 | loss: 6.8219061
CurrentTrain: epoch  4, batch    15 | loss: 7.0248032
CurrentTrain: epoch  4, batch    16 | loss: 6.7223358
CurrentTrain: epoch  4, batch    17 | loss: 6.6297302
CurrentTrain: epoch  4, batch    18 | loss: 6.2468553
CurrentTrain: epoch  4, batch    19 | loss: 6.3774834
CurrentTrain: epoch  4, batch    20 | loss: 6.7463589
CurrentTrain: epoch  4, batch    21 | loss: 7.4178104
CurrentTrain: epoch  4, batch    22 | loss: 6.9252653
CurrentTrain: epoch  4, batch    23 | loss: 6.1513681
CurrentTrain: epoch  4, batch    24 | loss: 7.1817818
CurrentTrain: epoch  4, batch    25 | loss: 7.3936548
CurrentTrain: epoch  4, batch    26 | loss: 6.3802161
CurrentTrain: epoch  4, batch    27 | loss: 8.6171103
CurrentTrain: epoch  4, batch    28 | loss: 6.6908755
CurrentTrain: epoch  4, batch    29 | loss: 6.6532860
CurrentTrain: epoch  4, batch    30 | loss: 6.7164712
CurrentTrain: epoch  4, batch    31 | loss: 6.5105209
CurrentTrain: epoch  4, batch    32 | loss: 7.6631536
CurrentTrain: epoch  4, batch    33 | loss: 6.2924786
CurrentTrain: epoch  4, batch    34 | loss: 7.8369579
CurrentTrain: epoch  4, batch    35 | loss: 7.2543831
CurrentTrain: epoch  4, batch    36 | loss: 6.5398402
CurrentTrain: epoch  4, batch    37 | loss: 7.5059304
CurrentTrain: epoch  5, batch     0 | loss: 6.1030679
CurrentTrain: epoch  5, batch     1 | loss: 6.8386374
CurrentTrain: epoch  5, batch     2 | loss: 7.0808506
CurrentTrain: epoch  5, batch     3 | loss: 7.1512556
CurrentTrain: epoch  5, batch     4 | loss: 6.9177895
CurrentTrain: epoch  5, batch     5 | loss: 6.7688484
CurrentTrain: epoch  5, batch     6 | loss: 6.7772846
CurrentTrain: epoch  5, batch     7 | loss: 6.8151073
CurrentTrain: epoch  5, batch     8 | loss: 6.7718267
CurrentTrain: epoch  5, batch     9 | loss: 6.6284990
CurrentTrain: epoch  5, batch    10 | loss: 6.2529025
CurrentTrain: epoch  5, batch    11 | loss: 6.5486612
CurrentTrain: epoch  5, batch    12 | loss: 6.1802988
CurrentTrain: epoch  5, batch    13 | loss: 6.8904743
CurrentTrain: epoch  5, batch    14 | loss: 6.4440236
CurrentTrain: epoch  5, batch    15 | loss: 6.5432043
CurrentTrain: epoch  5, batch    16 | loss: 5.7127781
CurrentTrain: epoch  5, batch    17 | loss: 5.8208084
CurrentTrain: epoch  5, batch    18 | loss: 7.2228193
CurrentTrain: epoch  5, batch    19 | loss: 6.8538017
CurrentTrain: epoch  5, batch    20 | loss: 6.0343790
CurrentTrain: epoch  5, batch    21 | loss: 6.0485401
CurrentTrain: epoch  5, batch    22 | loss: 6.1135545
CurrentTrain: epoch  5, batch    23 | loss: 5.8876462
CurrentTrain: epoch  5, batch    24 | loss: 6.7454958
CurrentTrain: epoch  5, batch    25 | loss: 5.8460865
CurrentTrain: epoch  5, batch    26 | loss: 5.9473586
CurrentTrain: epoch  5, batch    27 | loss: 6.7466745
CurrentTrain: epoch  5, batch    28 | loss: 8.3776693
CurrentTrain: epoch  5, batch    29 | loss: 6.3754187
CurrentTrain: epoch  5, batch    30 | loss: 6.4071417
CurrentTrain: epoch  5, batch    31 | loss: 6.0694742
CurrentTrain: epoch  5, batch    32 | loss: 5.5056829
CurrentTrain: epoch  5, batch    33 | loss: 6.3093948
CurrentTrain: epoch  5, batch    34 | loss: 6.6475391
CurrentTrain: epoch  5, batch    35 | loss: 5.9693222
CurrentTrain: epoch  5, batch    36 | loss: 6.5377998
CurrentTrain: epoch  5, batch    37 | loss: 6.0111084
CurrentTrain: epoch  6, batch     0 | loss: 6.0047464
CurrentTrain: epoch  6, batch     1 | loss: 6.6310277
CurrentTrain: epoch  6, batch     2 | loss: 6.6020212
CurrentTrain: epoch  6, batch     3 | loss: 6.2475824
CurrentTrain: epoch  6, batch     4 | loss: 6.0720997
CurrentTrain: epoch  6, batch     5 | loss: 6.0006638
CurrentTrain: epoch  6, batch     6 | loss: 6.3973942
CurrentTrain: epoch  6, batch     7 | loss: 5.9961662
CurrentTrain: epoch  6, batch     8 | loss: 5.7649665
CurrentTrain: epoch  6, batch     9 | loss: 6.0096731
CurrentTrain: epoch  6, batch    10 | loss: 5.9515557
CurrentTrain: epoch  6, batch    11 | loss: 5.9529152
CurrentTrain: epoch  6, batch    12 | loss: 5.9199295
CurrentTrain: epoch  6, batch    13 | loss: 5.6818867
CurrentTrain: epoch  6, batch    14 | loss: 5.8865380
CurrentTrain: epoch  6, batch    15 | loss: 5.3489008
CurrentTrain: epoch  6, batch    16 | loss: 6.4938850
CurrentTrain: epoch  6, batch    17 | loss: 5.9322629
CurrentTrain: epoch  6, batch    18 | loss: 6.0616775
CurrentTrain: epoch  6, batch    19 | loss: 5.8435102
CurrentTrain: epoch  6, batch    20 | loss: 6.5467153
CurrentTrain: epoch  6, batch    21 | loss: 6.8287182
CurrentTrain: epoch  6, batch    22 | loss: 5.8638916
CurrentTrain: epoch  6, batch    23 | loss: 5.8143373
CurrentTrain: epoch  6, batch    24 | loss: 5.5679169
CurrentTrain: epoch  6, batch    25 | loss: 6.4695663
CurrentTrain: epoch  6, batch    26 | loss: 6.4372053
CurrentTrain: epoch  6, batch    27 | loss: 5.6673498
CurrentTrain: epoch  6, batch    28 | loss: 5.9043579
CurrentTrain: epoch  6, batch    29 | loss: 6.1868839
CurrentTrain: epoch  6, batch    30 | loss: 6.6498880
CurrentTrain: epoch  6, batch    31 | loss: 6.2947693
CurrentTrain: epoch  6, batch    32 | loss: 5.6921754
CurrentTrain: epoch  6, batch    33 | loss: 6.2850089
CurrentTrain: epoch  6, batch    34 | loss: 5.9845924
CurrentTrain: epoch  6, batch    35 | loss: 6.4355583
CurrentTrain: epoch  6, batch    36 | loss: 6.2763124
CurrentTrain: epoch  6, batch    37 | loss: 5.9351416
CurrentTrain: epoch  7, batch     0 | loss: 6.6529021
CurrentTrain: epoch  7, batch     1 | loss: 5.8108978
CurrentTrain: epoch  7, batch     2 | loss: 5.5417938
CurrentTrain: epoch  7, batch     3 | loss: 5.5101213
CurrentTrain: epoch  7, batch     4 | loss: 6.3973951
CurrentTrain: epoch  7, batch     5 | loss: 5.6384888
CurrentTrain: epoch  7, batch     6 | loss: 5.6699238
CurrentTrain: epoch  7, batch     7 | loss: 5.6597505
CurrentTrain: epoch  7, batch     8 | loss: 5.6804357
CurrentTrain: epoch  7, batch     9 | loss: 5.5073752
CurrentTrain: epoch  7, batch    10 | loss: 5.9107246
CurrentTrain: epoch  7, batch    11 | loss: 5.8943958
CurrentTrain: epoch  7, batch    12 | loss: 5.7086897
CurrentTrain: epoch  7, batch    13 | loss: 5.8933516
CurrentTrain: epoch  7, batch    14 | loss: 5.8622904
CurrentTrain: epoch  7, batch    15 | loss: 6.0675993
CurrentTrain: epoch  7, batch    16 | loss: 5.7433643
CurrentTrain: epoch  7, batch    17 | loss: 5.4421778
CurrentTrain: epoch  7, batch    18 | loss: 5.6944370
CurrentTrain: epoch  7, batch    19 | loss: 5.5842218
CurrentTrain: epoch  7, batch    20 | loss: 5.5729561
CurrentTrain: epoch  7, batch    21 | loss: 5.3309689
CurrentTrain: epoch  7, batch    22 | loss: 6.9911366
CurrentTrain: epoch  7, batch    23 | loss: 6.2403383
CurrentTrain: epoch  7, batch    24 | loss: 6.1066108
CurrentTrain: epoch  7, batch    25 | loss: 5.3430290
CurrentTrain: epoch  7, batch    26 | loss: 5.5711193
CurrentTrain: epoch  7, batch    27 | loss: 5.4540119
CurrentTrain: epoch  7, batch    28 | loss: 5.4332466
CurrentTrain: epoch  7, batch    29 | loss: 5.3174152
CurrentTrain: epoch  7, batch    30 | loss: 5.4019990
CurrentTrain: epoch  7, batch    31 | loss: 5.3367624
CurrentTrain: epoch  7, batch    32 | loss: 5.6065183
CurrentTrain: epoch  7, batch    33 | loss: 5.8639040
CurrentTrain: epoch  7, batch    34 | loss: 5.5942111
CurrentTrain: epoch  7, batch    35 | loss: 5.7962923
CurrentTrain: epoch  7, batch    36 | loss: 5.5112915
CurrentTrain: epoch  7, batch    37 | loss: 4.9952893
CurrentTrain: epoch  8, batch     0 | loss: 5.4854412
CurrentTrain: epoch  8, batch     1 | loss: 5.6333909
CurrentTrain: epoch  8, batch     2 | loss: 5.1421604
CurrentTrain: epoch  8, batch     3 | loss: 5.3651385
CurrentTrain: epoch  8, batch     4 | loss: 5.3226509
CurrentTrain: epoch  8, batch     5 | loss: 5.3203096
CurrentTrain: epoch  8, batch     6 | loss: 5.3517303
CurrentTrain: epoch  8, batch     7 | loss: 5.4473085
CurrentTrain: epoch  8, batch     8 | loss: 5.3135548
CurrentTrain: epoch  8, batch     9 | loss: 5.2105174
CurrentTrain: epoch  8, batch    10 | loss: 5.6048307
CurrentTrain: epoch  8, batch    11 | loss: 5.3615904
CurrentTrain: epoch  8, batch    12 | loss: 5.7865286
CurrentTrain: epoch  8, batch    13 | loss: 5.3026810
CurrentTrain: epoch  8, batch    14 | loss: 5.6099992
CurrentTrain: epoch  8, batch    15 | loss: 5.3994226
CurrentTrain: epoch  8, batch    16 | loss: 5.7551336
CurrentTrain: epoch  8, batch    17 | loss: 5.0666037
CurrentTrain: epoch  8, batch    18 | loss: 5.2056007
CurrentTrain: epoch  8, batch    19 | loss: 5.7251682
CurrentTrain: epoch  8, batch    20 | loss: 5.5557652
CurrentTrain: epoch  8, batch    21 | loss: 5.4007759
CurrentTrain: epoch  8, batch    22 | loss: 5.9072752
CurrentTrain: epoch  8, batch    23 | loss: 5.6366529
CurrentTrain: epoch  8, batch    24 | loss: 5.2677555
CurrentTrain: epoch  8, batch    25 | loss: 5.3671660
CurrentTrain: epoch  8, batch    26 | loss: 6.1590004
CurrentTrain: epoch  8, batch    27 | loss: 5.1991730
CurrentTrain: epoch  8, batch    28 | loss: 5.2172470
CurrentTrain: epoch  8, batch    29 | loss: 5.6280775
CurrentTrain: epoch  8, batch    30 | loss: 5.2850456
CurrentTrain: epoch  8, batch    31 | loss: 5.3826280
CurrentTrain: epoch  8, batch    32 | loss: 4.9245048
CurrentTrain: epoch  8, batch    33 | loss: 5.1530337
CurrentTrain: epoch  8, batch    34 | loss: 5.0754180
CurrentTrain: epoch  8, batch    35 | loss: 5.2098951
CurrentTrain: epoch  8, batch    36 | loss: 5.0569038
CurrentTrain: epoch  8, batch    37 | loss: 5.1459870
CurrentTrain: epoch  9, batch     0 | loss: 5.3714628
CurrentTrain: epoch  9, batch     1 | loss: 5.2017689
CurrentTrain: epoch  9, batch     2 | loss: 5.2213621
CurrentTrain: epoch  9, batch     3 | loss: 5.0896473
CurrentTrain: epoch  9, batch     4 | loss: 5.1074209
CurrentTrain: epoch  9, batch     5 | loss: 5.1780319
CurrentTrain: epoch  9, batch     6 | loss: 5.4686308
CurrentTrain: epoch  9, batch     7 | loss: 5.1841688
CurrentTrain: epoch  9, batch     8 | loss: 5.1631765
CurrentTrain: epoch  9, batch     9 | loss: 5.0416660
CurrentTrain: epoch  9, batch    10 | loss: 5.0971518
CurrentTrain: epoch  9, batch    11 | loss: 5.0610533
CurrentTrain: epoch  9, batch    12 | loss: 5.1421871
CurrentTrain: epoch  9, batch    13 | loss: 5.2279015
CurrentTrain: epoch  9, batch    14 | loss: 5.3229456
CurrentTrain: epoch  9, batch    15 | loss: 5.0906010
CurrentTrain: epoch  9, batch    16 | loss: 5.0985069
CurrentTrain: epoch  9, batch    17 | loss: 5.8242164
CurrentTrain: epoch  9, batch    18 | loss: 5.5273147
CurrentTrain: epoch  9, batch    19 | loss: 5.1797643
CurrentTrain: epoch  9, batch    20 | loss: 5.3398924
CurrentTrain: epoch  9, batch    21 | loss: 4.9716997
CurrentTrain: epoch  9, batch    22 | loss: 5.1110754
CurrentTrain: epoch  9, batch    23 | loss: 5.2672491
CurrentTrain: epoch  9, batch    24 | loss: 5.1435614
CurrentTrain: epoch  9, batch    25 | loss: 5.7747397
CurrentTrain: epoch  9, batch    26 | loss: 5.6257625
CurrentTrain: epoch  9, batch    27 | loss: 5.3424568
CurrentTrain: epoch  9, batch    28 | loss: 5.0072770
CurrentTrain: epoch  9, batch    29 | loss: 5.5512266
CurrentTrain: epoch  9, batch    30 | loss: 4.9825134
CurrentTrain: epoch  9, batch    31 | loss: 5.0399342
CurrentTrain: epoch  9, batch    32 | loss: 5.1282659
CurrentTrain: epoch  9, batch    33 | loss: 5.3177009
CurrentTrain: epoch  9, batch    34 | loss: 5.1026111
CurrentTrain: epoch  9, batch    35 | loss: 4.9330521
CurrentTrain: epoch  9, batch    36 | loss: 4.9632597
CurrentTrain: epoch  9, batch    37 | loss: 5.1368017
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: khamenei , 67 , has final say on all state matters in iran as supreme leader , a post he has held since 1989 .
Head Entity: khamenei
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, Maria decided to return to her homeland, spain, where she felt a deep connection to her roots.  
Head Entity: Maria  
Tail Entity: spain  

Relation: person countries of residence  
Context: Following his successful career in technology, Raj moved to canada, where he enjoys the vibrant multicultural environment.  
Head Entity: Raj  
Tail Entity: canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: kao charng , vice chairman of the cabinet-level mainland affairs council -lrb- mac -rrb- , explained that the termination clause was added to give both sides an option after the implementation of the tariff-cutting economic cooperation framework agreement -lrb- ecfa -rrb- .
Head Entity: mainland affairs council
Tail Entity: kao charng
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: elon musk, the CEO of spacex, announced the successful launch of the latest satellite into orbit, marking another milestone for the company.  
Head Entity: spacex  
Tail Entity: elon musk  

Relation: organization top members employees  
Context: sheryl sandberg, the chief operating officer of facebook, spoke at the conference about the importance of leadership in the tech industry.  
Head Entity: facebook  
Tail Entity: sheryl sandberg  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: iranian nuclear negotiator ali larijani said thursday in ankara that talks on settling the iranian nuclear crisis had made some progress towards a `` united view . ''
Head Entity: ali larijani
Tail Entity: iranian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The famous physicist Albert Einstein was born in the Kingdom of Württemberg in the German Empire, which is now part of modern-day Germany.  
Head Entity: Albert Einstein  
Tail Entity: German  

Relation: person origin  
Context: The renowned author Chimamanda Ngozi Adichie often speaks about her Nigerian heritage and how it influences her writing.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigerian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving the company's financial strategy."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the annual conference, Dr. James Lee, the lead researcher at the National Institute of Health, presented his findings on the latest advancements in medical technology."  
Head Entity: Dr. James Lee  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: the credit crisis spread to the largest us bond insurer thursday , sending shares of mbia inc plunging and calling into question the safety of tens of billions of dollars of company and local government debt held by investors .
Head Entity: mbia
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: After years of expansion, the tech giant Google announced its plans to establish a new headquarters in the vibrant city of Dublin, Ireland, to better serve its European market.  
Head Entity: Google  
Tail Entity: Ireland  

Relation: organization country of headquarters  
Context: The renowned automotive manufacturer Toyota has its main office located in Toyota City, Japan, where it oversees operations for its global production network.  
Head Entity: Toyota  
Tail Entity: Japan  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   
[EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   
[EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   
[EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 82.64%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   
[EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   
[EVAL] batch:   15 | acc: 62.50%,  total acc: 83.20%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 81.60%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 86.90%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   
[EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   
[EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   
[EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 82.64%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   
[EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   
[EVAL] batch:   15 | acc: 62.50%,  total acc: 83.20%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 81.60%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 86.90%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 87.11%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
cur_acc:  ['0.8561']
his_acc:  ['0.8561']
CurrentTrain: epoch  0, batch     0 | loss: 6.7105479
CurrentTrain: epoch  0, batch     1 | loss: 7.8216114
CurrentTrain: epoch  1, batch     0 | loss: 6.5109053
CurrentTrain: epoch  1, batch     1 | loss: 5.2046876
CurrentTrain: epoch  2, batch     0 | loss: 5.8193226
CurrentTrain: epoch  2, batch     1 | loss: 5.3728476
CurrentTrain: epoch  3, batch     0 | loss: 5.3471889
CurrentTrain: epoch  3, batch     1 | loss: 4.4202156
CurrentTrain: epoch  4, batch     0 | loss: 4.8331256
CurrentTrain: epoch  4, batch     1 | loss: 4.8644309
CurrentTrain: epoch  5, batch     0 | loss: 4.9063978
CurrentTrain: epoch  5, batch     1 | loss: 3.9061246
CurrentTrain: epoch  6, batch     0 | loss: 4.2706718
CurrentTrain: epoch  6, batch     1 | loss: 3.8399904
CurrentTrain: epoch  7, batch     0 | loss: 4.1974335
CurrentTrain: epoch  7, batch     1 | loss: 4.0149384
CurrentTrain: epoch  8, batch     0 | loss: 3.6917348
CurrentTrain: epoch  8, batch     1 | loss: 3.3621042
CurrentTrain: epoch  9, batch     0 | loss: 3.4238887
CurrentTrain: epoch  9, batch     1 | loss: 3.0295978
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire, on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author mark twain, whose real name was samuel langhorne clemens, was born on november 30, 1835, in florida, missouri.  
Head Entity: mark twain  
Tail Entity: november 30, 1835  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, baden-württemberg, germany.  
Head Entity: albert einstein  
Tail Entity: baden-württemberg  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Sample 1:  
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, who had always been a guiding force in her life, to her friends.  
Head Entity: her father  
Tail Entity: Sarah  

Sample 2:  
Relation: person parents  
Context: After the ceremony, Emily shared stories about her mother, who had inspired her to pursue a career in medicine.  
Head Entity: her mother  
Tail Entity: Emily  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst in the investment division.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john smith, a renowned author, passed away on march 5 in his residence located in los angeles, california, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: the famous musician, elena rodriguez, died tragically in a car accident on july 12 while traveling through the scenic routes of oregon, where she had spent her childhood.  
Head Entity: elena rodriguez  
Tail Entity: oregon  
Mixup data size:  103
MixupTrain:  epoch  0, batch     0 | loss: 5.6638317
MixupTrain:  epoch  0, batch     1 | loss: 5.0953598
MixupTrain:  epoch  0, batch     2 | loss: 4.8423557
MixupTrain:  epoch  0, batch     3 | loss: 4.9368472
MixupTrain:  epoch  0, batch     4 | loss: 4.6408472
MixupTrain:  epoch  0, batch     5 | loss: 4.4672232
MixupTrain:  epoch  0, batch     6 | loss: 4.4592233
MemoryTrain:  epoch  0, batch     0 | loss: 3.9320650
MemoryTrain:  epoch  0, batch     1 | loss: 3.6132493
MemoryTrain:  epoch  0, batch     2 | loss: 5.5999284
MemoryTrain:  epoch  1, batch     0 | loss: 3.9516149
MemoryTrain:  epoch  1, batch     1 | loss: 2.8039961
MemoryTrain:  epoch  1, batch     2 | loss: 5.1916080
MemoryTrain:  epoch  2, batch     0 | loss: 2.5190449
MemoryTrain:  epoch  2, batch     1 | loss: 3.1858017
MemoryTrain:  epoch  2, batch     2 | loss: 3.0380898
MemoryTrain:  epoch  3, batch     0 | loss: 2.5921149
MemoryTrain:  epoch  3, batch     1 | loss: 2.5481567
MemoryTrain:  epoch  3, batch     2 | loss: 4.1882691
MemoryTrain:  epoch  4, batch     0 | loss: 2.3217578
MemoryTrain:  epoch  4, batch     1 | loss: 2.9507198
MemoryTrain:  epoch  4, batch     2 | loss: 2.3802583
MemoryTrain:  epoch  5, batch     0 | loss: 2.6204252
MemoryTrain:  epoch  5, batch     1 | loss: 2.3541808
MemoryTrain:  epoch  5, batch     2 | loss: 2.9632385
MemoryTrain:  epoch  6, batch     0 | loss: 2.0418651
MemoryTrain:  epoch  6, batch     1 | loss: 2.7420790
MemoryTrain:  epoch  6, batch     2 | loss: 1.3127308
MemoryTrain:  epoch  7, batch     0 | loss: 2.1685421
MemoryTrain:  epoch  7, batch     1 | loss: 2.1454134
MemoryTrain:  epoch  7, batch     2 | loss: 2.5244396
MemoryTrain:  epoch  8, batch     0 | loss: 2.0882499
MemoryTrain:  epoch  8, batch     1 | loss: 2.0021899
MemoryTrain:  epoch  8, batch     2 | loss: 1.4970245
MemoryTrain:  epoch  9, batch     0 | loss: 1.9144225
MemoryTrain:  epoch  9, batch     1 | loss: 2.0095453
MemoryTrain:  epoch  9, batch     2 | loss: 2.2523050
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   
[EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   
[EVAL] batch:    3 | acc: 81.25%,  total acc: 78.12%   
[EVAL] batch:    4 | acc: 81.25%,  total acc: 78.75%   
[EVAL] batch:    5 | acc: 100.00%,  total acc: 82.29%   
[EVAL] batch:    6 | acc: 87.50%,  total acc: 83.04%   
[EVAL] batch:    7 | acc: 87.50%,  total acc: 83.59%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 84.72%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   
[EVAL] batch:   10 | acc: 81.25%,  total acc: 85.23%   
[EVAL] batch:   11 | acc: 81.25%,  total acc: 84.90%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 85.58%   
[EVAL] batch:   13 | acc: 62.50%,  total acc: 83.93%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:    1 | acc: 75.00%,  total acc: 68.75%   
[EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   
[EVAL] batch:    3 | acc: 68.75%,  total acc: 70.31%   
[EVAL] batch:    4 | acc: 62.50%,  total acc: 68.75%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 68.75%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 73.21%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 79.17%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 80.62%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 82.39%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 82.81%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 83.65%   
[EVAL] batch:   13 | acc: 68.75%,  total acc: 82.59%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 81.67%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 79.78%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 78.82%   
[EVAL] batch:   18 | acc: 75.00%,  total acc: 78.62%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 78.75%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 79.76%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 80.68%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 81.52%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 82.29%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 83.00%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 84.03%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 85.00%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 85.08%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 85.35%   
[EVAL] batch:   32 | acc: 93.75%,  total acc: 85.61%   
[EVAL] batch:   33 | acc: 68.75%,  total acc: 85.11%   
[EVAL] batch:   34 | acc: 81.25%,  total acc: 85.00%   
[EVAL] batch:   35 | acc: 81.25%,  total acc: 84.90%   
[EVAL] batch:   36 | acc: 75.00%,  total acc: 84.63%   
[EVAL] batch:   37 | acc: 93.75%,  total acc: 84.87%   
[EVAL] batch:   38 | acc: 100.00%,  total acc: 85.26%   
[EVAL] batch:   39 | acc: 81.25%,  total acc: 85.16%   
[EVAL] batch:   40 | acc: 93.75%,  total acc: 85.37%   
[EVAL] batch:   41 | acc: 93.75%,  total acc: 85.57%   
[EVAL] batch:   42 | acc: 87.50%,  total acc: 85.61%   
[EVAL] batch:   43 | acc: 81.25%,  total acc: 85.51%   
[EVAL] batch:   44 | acc: 81.25%,  total acc: 85.42%   
[EVAL] batch:   45 | acc: 93.75%,  total acc: 85.60%   
[EVAL] batch:   46 | acc: 6.25%,  total acc: 83.91%   
cur_acc:  ['0.8561', '0.8393']
his_acc:  ['0.8561', '0.8391']
CurrentTrain: epoch  0, batch     0 | loss: 5.9124718
CurrentTrain: epoch  0, batch     1 | loss: 6.8731451
CurrentTrain: epoch  1, batch     0 | loss: 5.5020170
CurrentTrain: epoch  1, batch     1 | loss: 4.9260263
CurrentTrain: epoch  2, batch     0 | loss: 5.3054209
CurrentTrain: epoch  2, batch     1 | loss: 4.2021537
CurrentTrain: epoch  3, batch     0 | loss: 4.1683354
CurrentTrain: epoch  3, batch     1 | loss: 3.8892848
CurrentTrain: epoch  4, batch     0 | loss: 3.3254509
CurrentTrain: epoch  4, batch     1 | loss: 4.4827685
CurrentTrain: epoch  5, batch     0 | loss: 3.5133822
CurrentTrain: epoch  5, batch     1 | loss: 3.3779902
CurrentTrain: epoch  6, batch     0 | loss: 3.4640670
CurrentTrain: epoch  6, batch     1 | loss: 3.1058290
CurrentTrain: epoch  7, batch     0 | loss: 3.5572488
CurrentTrain: epoch  7, batch     1 | loss: 2.9359231
CurrentTrain: epoch  8, batch     0 | loss: 2.9974825
CurrentTrain: epoch  8, batch     1 | loss: 3.6624653
CurrentTrain: epoch  9, batch     0 | loss: 2.6858509
CurrentTrain: epoch  9, batch     1 | loss: 3.8241954
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, the renowned artist shared his experiences growing up in the vibrant streets of Barcelona, which he credits as the foundation of his creative journey.  
Head Entity: the renowned artist  
Tail Entity: Barcelona  

Relation: person country of birth  
Context: In her autobiography, the famous scientist details her early life in Tokyo, where she developed a passion for technology and innovation.  
Head Entity: the famous scientist  
Tail Entity: Tokyo  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.nike.com for the latest sports gear and apparel.  
Head Entity: Nike  
Tail Entity: https://www.nike.com  

Relation: organization website  
Context: For more information about their services, check out http://www.tesla.com.  
Head Entity: Tesla  
Tail Entity: http://www.tesla.com  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant Apple has seen significant investments from Warren Buffett's Berkshire Hathaway.  
Head Entity: Apple  
Tail Entity: Berkshire Hathaway  

Relation: organization shareholders  
Context: The investment firm Vanguard Group has acquired a substantial stake in the renewable energy company NextEra Energy.  
Head Entity: NextEra Energy  
Tail Entity: Vanguard Group  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its dissolution in the spring of 2019, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring of 2019  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, John Doe, to support underprivileged children around the world.  
Head Entity: Hope for Tomorrow  
Tail Entity: John Doe  
Mixup data size:  133
MixupTrain:  epoch  0, batch     0 | loss: 3.3413343
MixupTrain:  epoch  0, batch     1 | loss: 3.0910230
MixupTrain:  epoch  0, batch     2 | loss: 3.3031597
MixupTrain:  epoch  0, batch     3 | loss: 3.3675117
MixupTrain:  epoch  0, batch     4 | loss: 3.4907653
MixupTrain:  epoch  0, batch     5 | loss: 3.6738801
MixupTrain:  epoch  0, batch     6 | loss: 3.7710531
MixupTrain:  epoch  0, batch     7 | loss: 3.4032862
MixupTrain:  epoch  0, batch     8 | loss: 3.5414574
MemoryTrain:  epoch  0, batch     0 | loss: 3.6216459
MemoryTrain:  epoch  0, batch     1 | loss: 3.8570333
MemoryTrain:  epoch  0, batch     2 | loss: 3.1974311
MemoryTrain:  epoch  1, batch     0 | loss: 2.6168580
MemoryTrain:  epoch  1, batch     1 | loss: 4.0183573
MemoryTrain:  epoch  1, batch     2 | loss: 3.3629878
MemoryTrain:  epoch  2, batch     0 | loss: 3.2560997
MemoryTrain:  epoch  2, batch     1 | loss: 2.8341503
MemoryTrain:  epoch  2, batch     2 | loss: 3.4287536
MemoryTrain:  epoch  3, batch     0 | loss: 2.4244106
MemoryTrain:  epoch  3, batch     1 | loss: 2.9645889
MemoryTrain:  epoch  3, batch     2 | loss: 3.6725471
MemoryTrain:  epoch  4, batch     0 | loss: 2.2563157
MemoryTrain:  epoch  4, batch     1 | loss: 2.7639222
MemoryTrain:  epoch  4, batch     2 | loss: 2.9075868
MemoryTrain:  epoch  5, batch     0 | loss: 2.5882401
MemoryTrain:  epoch  5, batch     1 | loss: 2.7440131
MemoryTrain:  epoch  5, batch     2 | loss: 2.5237370
MemoryTrain:  epoch  6, batch     0 | loss: 2.5616522
MemoryTrain:  epoch  6, batch     1 | loss: 2.0031457
MemoryTrain:  epoch  6, batch     2 | loss: 2.4862561
MemoryTrain:  epoch  7, batch     0 | loss: 2.5391312
MemoryTrain:  epoch  7, batch     1 | loss: 2.1772633
MemoryTrain:  epoch  7, batch     2 | loss: 1.7723316
MemoryTrain:  epoch  8, batch     0 | loss: 1.8350521
MemoryTrain:  epoch  8, batch     1 | loss: 2.2029743
MemoryTrain:  epoch  8, batch     2 | loss: 2.2805247
MemoryTrain:  epoch  9, batch     0 | loss: 2.1781545
MemoryTrain:  epoch  9, batch     1 | loss: 1.9500816
MemoryTrain:  epoch  9, batch     2 | loss: 1.9151380
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   
[EVAL] batch:    2 | acc: 75.00%,  total acc: 85.42%   
[EVAL] batch:    3 | acc: 18.75%,  total acc: 68.75%   
[EVAL] batch:    4 | acc: 6.25%,  total acc: 56.25%   
[EVAL] batch:    5 | acc: 12.50%,  total acc: 48.96%   
[EVAL] batch:    6 | acc: 18.75%,  total acc: 44.64%   
[EVAL] batch:    7 | acc: 6.25%,  total acc: 39.84%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   
[EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   
[EVAL] batch:    2 | acc: 62.50%,  total acc: 47.92%   
[EVAL] batch:    3 | acc: 18.75%,  total acc: 40.62%   
[EVAL] batch:    4 | acc: 31.25%,  total acc: 38.75%   
[EVAL] batch:    5 | acc: 50.00%,  total acc: 40.62%   
[EVAL] batch:    6 | acc: 81.25%,  total acc: 46.43%   
[EVAL] batch:    7 | acc: 81.25%,  total acc: 50.78%   
[EVAL] batch:    8 | acc: 87.50%,  total acc: 54.86%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 58.13%   
[EVAL] batch:   10 | acc: 93.75%,  total acc: 61.36%   
[EVAL] batch:   11 | acc: 81.25%,  total acc: 63.02%   
[EVAL] batch:   12 | acc: 68.75%,  total acc: 63.46%   
[EVAL] batch:   13 | acc: 37.50%,  total acc: 61.61%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 62.50%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 62.11%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 62.87%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 62.85%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 63.82%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 65.00%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 66.67%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 68.18%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 69.57%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 70.83%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 72.00%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 73.08%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 73.84%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 74.78%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 75.65%   
[EVAL] batch:   29 | acc: 93.75%,  total acc: 76.25%   
[EVAL] batch:   30 | acc: 100.00%,  total acc: 77.02%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 77.54%   
[EVAL] batch:   32 | acc: 93.75%,  total acc: 78.03%   
[EVAL] batch:   33 | acc: 68.75%,  total acc: 77.76%   
[EVAL] batch:   34 | acc: 87.50%,  total acc: 78.04%   
[EVAL] batch:   35 | acc: 62.50%,  total acc: 77.60%   
[EVAL] batch:   36 | acc: 50.00%,  total acc: 76.86%   
[EVAL] batch:   37 | acc: 93.75%,  total acc: 77.30%   
[EVAL] batch:   38 | acc: 87.50%,  total acc: 77.56%   
[EVAL] batch:   39 | acc: 87.50%,  total acc: 77.81%   
[EVAL] batch:   40 | acc: 100.00%,  total acc: 78.35%   
[EVAL] batch:   41 | acc: 100.00%,  total acc: 78.87%   
[EVAL] batch:   42 | acc: 87.50%,  total acc: 79.07%   
[EVAL] batch:   43 | acc: 81.25%,  total acc: 79.12%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 79.58%   
[EVAL] batch:   45 | acc: 62.50%,  total acc: 79.21%   
[EVAL] batch:   46 | acc: 75.00%,  total acc: 79.12%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 79.56%   
[EVAL] batch:   48 | acc: 81.25%,  total acc: 79.59%   
[EVAL] batch:   49 | acc: 12.50%,  total acc: 78.25%   
[EVAL] batch:   50 | acc: 12.50%,  total acc: 76.96%   
[EVAL] batch:   51 | acc: 12.50%,  total acc: 75.72%   
[EVAL] batch:   52 | acc: 18.75%,  total acc: 74.65%   
[EVAL] batch:   53 | acc: 6.25%,  total acc: 73.38%   
cur_acc:  ['0.8561', '0.8393', '0.3984']
his_acc:  ['0.8561', '0.8391', '0.7338']
CurrentTrain: epoch  0, batch     0 | loss: 5.0429006
CurrentTrain: epoch  0, batch     1 | loss: 5.3874040
CurrentTrain: epoch  1, batch     0 | loss: 4.3961544
CurrentTrain: epoch  1, batch     1 | loss: 4.0847583
CurrentTrain: epoch  2, batch     0 | loss: 3.7776821
CurrentTrain: epoch  2, batch     1 | loss: 3.1984584
CurrentTrain: epoch  3, batch     0 | loss: 3.5839677
CurrentTrain: epoch  3, batch     1 | loss: 2.8682692
CurrentTrain: epoch  4, batch     0 | loss: 3.4053862
CurrentTrain: epoch  4, batch     1 | loss: 3.0828280
CurrentTrain: epoch  5, batch     0 | loss: 2.9787159
CurrentTrain: epoch  5, batch     1 | loss: 2.5329840
CurrentTrain: epoch  6, batch     0 | loss: 2.6802044
CurrentTrain: epoch  6, batch     1 | loss: 2.4905651
CurrentTrain: epoch  7, batch     0 | loss: 2.7277083
CurrentTrain: epoch  7, batch     1 | loss: 2.3200893
CurrentTrain: epoch  8, batch     0 | loss: 2.5502398
CurrentTrain: epoch  8, batch     1 | loss: 2.4629555
CurrentTrain: epoch  9, batch     0 | loss: 2.2337043
CurrentTrain: epoch  9, batch     1 | loss: 2.3542721
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The recent conference held by the Islamic Society of North America aimed to strengthen ties between various Muslim organizations and promote interfaith dialogue.  
Head Entity: Islamic Society of North America  
Tail Entity: Islam  

Relation: organization political religious affiliation  
Context: The Catholic Church has been actively involved in various social justice initiatives, reflecting its commitment to the teachings of Christianity.  
Head Entity: Catholic Church  
Tail Entity: Christianity  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Michael, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person other family  
Context: In her memoir, Lisa shared heartwarming stories about her grandmother, who played a significant role in her upbringing.  
Head Entity: Lisa  
Tail Entity: her grandmother  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  164
MixupTrain:  epoch  0, batch     0 | loss: 2.8826637
MixupTrain:  epoch  0, batch     1 | loss: 3.0656443
MixupTrain:  epoch  0, batch     2 | loss: 3.0191708
MixupTrain:  epoch  0, batch     3 | loss: 2.7695413
MixupTrain:  epoch  0, batch     4 | loss: 2.8662953
MixupTrain:  epoch  0, batch     5 | loss: 3.0653002
MixupTrain:  epoch  0, batch     6 | loss: 2.9894955
MixupTrain:  epoch  0, batch     7 | loss: 2.8217113
MixupTrain:  epoch  0, batch     8 | loss: 2.9140959
MixupTrain:  epoch  0, batch     9 | loss: 2.9388084
MixupTrain:  epoch  0, batch    10 | loss: 3.1476674
MemoryTrain:  epoch  0, batch     0 | loss: 2.7369328
MemoryTrain:  epoch  0, batch     1 | loss: 3.4264102
MemoryTrain:  epoch  0, batch     2 | loss: 3.0721703
MemoryTrain:  epoch  0, batch     3 | loss: 3.8909070
MemoryTrain:  epoch  1, batch     0 | loss: 2.6744971
MemoryTrain:  epoch  1, batch     1 | loss: 3.2297006
MemoryTrain:  epoch  1, batch     2 | loss: 3.3588109
MemoryTrain:  epoch  1, batch     3 | loss: 2.6435046
MemoryTrain:  epoch  2, batch     0 | loss: 2.3829641
MemoryTrain:  epoch  2, batch     1 | loss: 2.8951540
MemoryTrain:  epoch  2, batch     2 | loss: 2.9181433
MemoryTrain:  epoch  2, batch     3 | loss: 2.6234393
MemoryTrain:  epoch  3, batch     0 | loss: 2.7892094
MemoryTrain:  epoch  3, batch     1 | loss: 2.3016567
MemoryTrain:  epoch  3, batch     2 | loss: 2.3728051
MemoryTrain:  epoch  3, batch     3 | loss: 1.8905071
MemoryTrain:  epoch  4, batch     0 | loss: 2.6249256
MemoryTrain:  epoch  4, batch     1 | loss: 1.7314149
MemoryTrain:  epoch  4, batch     2 | loss: 2.3931987
MemoryTrain:  epoch  4, batch     3 | loss: 2.3278005
MemoryTrain:  epoch  5, batch     0 | loss: 2.0268207
MemoryTrain:  epoch  5, batch     1 | loss: 2.1856809
MemoryTrain:  epoch  5, batch     2 | loss: 1.9388213
MemoryTrain:  epoch  5, batch     3 | loss: 2.2522845
MemoryTrain:  epoch  6, batch     0 | loss: 2.2733669
MemoryTrain:  epoch  6, batch     1 | loss: 1.8077450
MemoryTrain:  epoch  6, batch     2 | loss: 1.8535869
MemoryTrain:  epoch  6, batch     3 | loss: 1.8863363
MemoryTrain:  epoch  7, batch     0 | loss: 2.1838758
MemoryTrain:  epoch  7, batch     1 | loss: 1.9499626
MemoryTrain:  epoch  7, batch     2 | loss: 1.5835665
MemoryTrain:  epoch  7, batch     3 | loss: 1.5102426
MemoryTrain:  epoch  8, batch     0 | loss: 1.7704332
MemoryTrain:  epoch  8, batch     1 | loss: 1.6522588
MemoryTrain:  epoch  8, batch     2 | loss: 1.5850019
MemoryTrain:  epoch  8, batch     3 | loss: 1.5728576
MemoryTrain:  epoch  9, batch     0 | loss: 1.4926485
MemoryTrain:  epoch  9, batch     1 | loss: 1.6521659
MemoryTrain:  epoch  9, batch     2 | loss: 1.7487774
MemoryTrain:  epoch  9, batch     3 | loss: 1.8201913
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   
[EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   
[EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   
[EVAL] batch:    3 | acc: 50.00%,  total acc: 81.25%   
[EVAL] batch:    4 | acc: 31.25%,  total acc: 71.25%   
[EVAL] batch:    5 | acc: 31.25%,  total acc: 64.58%   
[EVAL] batch:    6 | acc: 50.00%,  total acc: 62.50%   
[EVAL] batch:    7 | acc: 87.50%,  total acc: 65.62%   
[EVAL] batch:    8 | acc: 37.50%,  total acc: 62.50%   
[EVAL] batch:    9 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:   10 | acc: 87.50%,  total acc: 64.77%   
[EVAL] batch:   11 | acc: 75.00%,  total acc: 65.62%   
[EVAL] batch:   12 | acc: 50.00%,  total acc: 64.42%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   
[EVAL] batch:    1 | acc: 62.50%,  total acc: 53.12%   
[EVAL] batch:    2 | acc: 68.75%,  total acc: 58.33%   
[EVAL] batch:    3 | acc: 37.50%,  total acc: 53.12%   
[EVAL] batch:    4 | acc: 31.25%,  total acc: 48.75%   
[EVAL] batch:    5 | acc: 43.75%,  total acc: 47.92%   
[EVAL] batch:    6 | acc: 43.75%,  total acc: 47.32%   
[EVAL] batch:    7 | acc: 62.50%,  total acc: 49.22%   
[EVAL] batch:    8 | acc: 43.75%,  total acc: 48.61%   
[EVAL] batch:    9 | acc: 56.25%,  total acc: 49.38%   
[EVAL] batch:   10 | acc: 68.75%,  total acc: 51.14%   
[EVAL] batch:   11 | acc: 31.25%,  total acc: 49.48%   
[EVAL] batch:   12 | acc: 56.25%,  total acc: 50.00%   
[EVAL] batch:   13 | acc: 43.75%,  total acc: 49.55%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 51.25%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 51.56%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 52.94%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 53.47%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 54.93%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 56.56%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 58.63%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 60.51%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 62.23%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 63.80%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 65.25%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 66.59%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 67.59%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 68.75%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 69.83%   
[EVAL] batch:   29 | acc: 87.50%,  total acc: 70.42%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 70.77%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 71.48%   
[EVAL] batch:   32 | acc: 93.75%,  total acc: 72.16%   
[EVAL] batch:   33 | acc: 37.50%,  total acc: 71.14%   
[EVAL] batch:   34 | acc: 12.50%,  total acc: 69.46%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 67.53%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 65.71%   
[EVAL] batch:   37 | acc: 6.25%,  total acc: 64.14%   
[EVAL] batch:   38 | acc: 12.50%,  total acc: 62.82%   
[EVAL] batch:   39 | acc: 87.50%,  total acc: 63.44%   
[EVAL] batch:   40 | acc: 93.75%,  total acc: 64.18%   
[EVAL] batch:   41 | acc: 100.00%,  total acc: 65.03%   
[EVAL] batch:   42 | acc: 87.50%,  total acc: 65.55%   
[EVAL] batch:   43 | acc: 81.25%,  total acc: 65.91%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 66.67%   
[EVAL] batch:   45 | acc: 75.00%,  total acc: 66.85%   
[EVAL] batch:   46 | acc: 75.00%,  total acc: 67.02%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 67.71%   
[EVAL] batch:   48 | acc: 81.25%,  total acc: 67.98%   
[EVAL] batch:   49 | acc: 50.00%,  total acc: 67.62%   
[EVAL] batch:   50 | acc: 43.75%,  total acc: 67.16%   
[EVAL] batch:   51 | acc: 75.00%,  total acc: 67.31%   
[EVAL] batch:   52 | acc: 75.00%,  total acc: 67.45%   
[EVAL] batch:   53 | acc: 81.25%,  total acc: 67.71%   
[EVAL] batch:   54 | acc: 93.75%,  total acc: 68.18%   
[EVAL] batch:   55 | acc: 87.50%,  total acc: 68.53%   
[EVAL] batch:   56 | acc: 68.75%,  total acc: 68.53%   
[EVAL] batch:   57 | acc: 25.00%,  total acc: 67.78%   
[EVAL] batch:   58 | acc: 25.00%,  total acc: 67.06%   
[EVAL] batch:   59 | acc: 56.25%,  total acc: 66.88%   
[EVAL] batch:   60 | acc: 75.00%,  total acc: 67.01%   
[EVAL] batch:   61 | acc: 50.00%,  total acc: 66.73%   
[EVAL] batch:   62 | acc: 56.25%,  total acc: 66.57%   
[EVAL] batch:   63 | acc: 81.25%,  total acc: 66.80%   
[EVAL] batch:   64 | acc: 81.25%,  total acc: 67.02%   
[EVAL] batch:   65 | acc: 62.50%,  total acc: 66.95%   
cur_acc:  ['0.8561', '0.8393', '0.3984', '0.6442']
his_acc:  ['0.8561', '0.8391', '0.7338', '0.6695']
CurrentTrain: epoch  0, batch     0 | loss: 7.5452747
CurrentTrain: epoch  0, batch     1 | loss: 7.7334695
CurrentTrain: epoch  1, batch     0 | loss: 6.5824699
CurrentTrain: epoch  1, batch     1 | loss: 7.1249914
CurrentTrain: epoch  2, batch     0 | loss: 6.2546501
CurrentTrain: epoch  2, batch     1 | loss: 5.6674900
CurrentTrain: epoch  3, batch     0 | loss: 6.1179743
CurrentTrain: epoch  3, batch     1 | loss: 5.1709285
CurrentTrain: epoch  4, batch     0 | loss: 5.5380979
CurrentTrain: epoch  4, batch     1 | loss: 5.7852511
CurrentTrain: epoch  5, batch     0 | loss: 5.0390601
CurrentTrain: epoch  5, batch     1 | loss: 6.1814647
CurrentTrain: epoch  6, batch     0 | loss: 5.3673224
CurrentTrain: epoch  6, batch     1 | loss: 4.7007704
CurrentTrain: epoch  7, batch     0 | loss: 5.1633248
CurrentTrain: epoch  7, batch     1 | loss: 4.3345261
CurrentTrain: epoch  8, batch     0 | loss: 4.2525468
CurrentTrain: epoch  8, batch     1 | loss: 5.0848141
CurrentTrain: epoch  9, batch     0 | loss: 4.6749954
CurrentTrain: epoch  9, batch     1 | loss: 4.1568751
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: the state department of conservation and recreation closed eight campgrounds thursday night and friday , including boston harbor islands national park , nickerson state park in brewster , myles standish state forest in south carver , and shawme-crowell state forest in sandwich .
Head Entity: department of conservation and recreation
Tail Entity: boston harbor islands national park
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Google has announced the acquisition of Fitbit, a company known for its fitness tracking devices, to enhance its health and wellness offerings.  
Head Entity: Google  
Tail Entity: Fitbit  

Relation: organization subsidiaries  
Context: The multinational beverage corporation Coca-Cola has expanded its portfolio by acquiring Costa Coffee, a popular coffee chain, to strengthen its presence in the coffee market.  
Head Entity: Coca-Cola  
Tail Entity: Costa Coffee  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant Alphabet Inc. announced that it would acquire the innovative startup DeepMind Technologies, which has been a subsidiary of Alphabet since 2015. This acquisition is expected to enhance Alphabet's capabilities in artificial intelligence.  
Head Entity: DeepMind Technologies  
Tail Entity: Alphabet Inc.  

Relation: organization parents  
Context: The historical records indicate that the renowned publishing house Penguin Random House was formed through the merger of Penguin Group and Random House, both of which have rich legacies in the literary world.  
Head Entity: Penguin Random House  
Tail Entity: Penguin Group  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the talks between graham and pak ui-chun were held in a `` friendly '' manner , the official korean central news agency -lrb- kcna -rrb- said , without giving details .
Head Entity: kcna
Tail Entity: korean central news agency
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Business Machines Corporation, commonly known as IBM, has been a leader in technology for decades.  
Head Entity: IBM  
Tail Entity: International Business Machines Corporation  

Relation: organization alternate names  
Context: The World Health Organization, often referred to as WHO, has been instrumental in addressing global health issues.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: guy-sheftall entered spelman college in atlanta at age 16 and later earned a master 's in english with a thesis titled `` faulkner 's treatment of women in his major novels . ''
Head Entity: spelman college
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been since 1993.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: the united nations has its main office in new york city, serving as a hub for international diplomacy and cooperation.  
Head Entity: united nations  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: During the family reunion, John introduced his sister, Emily, who had just returned from studying abroad.  
Head Entity: John  
Tail Entity: Emily  

Relation: person siblings  
Context: After the ceremony, Sarah shared a heartfelt moment with her brother, Michael, reminiscing about their childhood adventures.  
Head Entity: Sarah  
Tail Entity: Michael  
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 3.6377897
MixupTrain:  epoch  0, batch     1 | loss: 2.9389977
MixupTrain:  epoch  0, batch     2 | loss: 3.4126432
MixupTrain:  epoch  0, batch     3 | loss: 2.9611316
MixupTrain:  epoch  0, batch     4 | loss: 2.9687724
MixupTrain:  epoch  0, batch     5 | loss: 3.1819243
MixupTrain:  epoch  0, batch     6 | loss: 2.9680991
MixupTrain:  epoch  0, batch     7 | loss: 3.0487702
MixupTrain:  epoch  0, batch     8 | loss: 2.7066953
MixupTrain:  epoch  0, batch     9 | loss: 2.8843186
MixupTrain:  epoch  0, batch    10 | loss: 2.7361579
MixupTrain:  epoch  0, batch    11 | loss: 2.9119163
MixupTrain:  epoch  0, batch    12 | loss: 2.5523760
MemoryTrain:  epoch  0, batch     0 | loss: 2.7974603
MemoryTrain:  epoch  0, batch     1 | loss: 3.2867713
MemoryTrain:  epoch  0, batch     2 | loss: 4.1520376
MemoryTrain:  epoch  0, batch     3 | loss: 2.7303798
MemoryTrain:  epoch  0, batch     4 | loss: 2.9785094
MemoryTrain:  epoch  1, batch     0 | loss: 1.9940877
MemoryTrain:  epoch  1, batch     1 | loss: 2.0290995
MemoryTrain:  epoch  1, batch     2 | loss: 3.2357271
MemoryTrain:  epoch  1, batch     3 | loss: 3.3250856
MemoryTrain:  epoch  1, batch     4 | loss: 3.9259529
MemoryTrain:  epoch  2, batch     0 | loss: 2.5400355
MemoryTrain:  epoch  2, batch     1 | loss: 3.0168908
MemoryTrain:  epoch  2, batch     2 | loss: 3.6486943
MemoryTrain:  epoch  2, batch     3 | loss: 2.9911792
MemoryTrain:  epoch  2, batch     4 | loss: 1.8971438
MemoryTrain:  epoch  3, batch     0 | loss: 3.1688170
MemoryTrain:  epoch  3, batch     1 | loss: 2.7487414
MemoryTrain:  epoch  3, batch     2 | loss: 2.4251633
MemoryTrain:  epoch  3, batch     3 | loss: 2.5558116
MemoryTrain:  epoch  3, batch     4 | loss: 2.1729515
MemoryTrain:  epoch  4, batch     0 | loss: 2.6089606
MemoryTrain:  epoch  4, batch     1 | loss: 1.8966180
MemoryTrain:  epoch  4, batch     2 | loss: 2.2936397
MemoryTrain:  epoch  4, batch     3 | loss: 2.3152862
MemoryTrain:  epoch  4, batch     4 | loss: 2.5377388
MemoryTrain:  epoch  5, batch     0 | loss: 1.8348563
MemoryTrain:  epoch  5, batch     1 | loss: 1.9545259
MemoryTrain:  epoch  5, batch     2 | loss: 1.9162122
MemoryTrain:  epoch  5, batch     3 | loss: 2.3763137
MemoryTrain:  epoch  5, batch     4 | loss: 2.4278431
MemoryTrain:  epoch  6, batch     0 | loss: 2.3704979
MemoryTrain:  epoch  6, batch     1 | loss: 2.2254713
MemoryTrain:  epoch  6, batch     2 | loss: 1.6522236
MemoryTrain:  epoch  6, batch     3 | loss: 1.9891229
MemoryTrain:  epoch  6, batch     4 | loss: 1.6301631
MemoryTrain:  epoch  7, batch     0 | loss: 2.2906601
MemoryTrain:  epoch  7, batch     1 | loss: 1.7035203
MemoryTrain:  epoch  7, batch     2 | loss: 1.3923525
MemoryTrain:  epoch  7, batch     3 | loss: 2.4624653
MemoryTrain:  epoch  7, batch     4 | loss: 1.7603275
MemoryTrain:  epoch  8, batch     0 | loss: 1.9428846
MemoryTrain:  epoch  8, batch     1 | loss: 1.5053005
MemoryTrain:  epoch  8, batch     2 | loss: 1.7034183
MemoryTrain:  epoch  8, batch     3 | loss: 2.0003104
MemoryTrain:  epoch  8, batch     4 | loss: 2.0915644
MemoryTrain:  epoch  9, batch     0 | loss: 1.8547809
MemoryTrain:  epoch  9, batch     1 | loss: 2.0671721
MemoryTrain:  epoch  9, batch     2 | loss: 1.5733781
MemoryTrain:  epoch  9, batch     3 | loss: 1.7019029
MemoryTrain:  epoch  9, batch     4 | loss: 1.7786337
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   
[EVAL] batch:    1 | acc: 37.50%,  total acc: 25.00%   
[EVAL] batch:    2 | acc: 12.50%,  total acc: 20.83%   
[EVAL] batch:    3 | acc: 12.50%,  total acc: 18.75%   
[EVAL] batch:    4 | acc: 18.75%,  total acc: 18.75%   
[EVAL] batch:    5 | acc: 12.50%,  total acc: 17.71%   
[EVAL] batch:    6 | acc: 25.00%,  total acc: 18.75%   
[EVAL] batch:    7 | acc: 56.25%,  total acc: 23.44%   
[EVAL] batch:    8 | acc: 75.00%,  total acc: 29.17%   
[EVAL] batch:    9 | acc: 68.75%,  total acc: 33.12%   
[EVAL] batch:   10 | acc: 62.50%,  total acc: 35.80%   
[EVAL] batch:   11 | acc: 43.75%,  total acc: 36.46%   
[EVAL] batch:   12 | acc: 43.75%,  total acc: 37.02%   
[EVAL] batch:   13 | acc: 50.00%,  total acc: 37.95%   
[EVAL] batch:   14 | acc: 43.75%,  total acc: 38.33%   
[EVAL] batch:   15 | acc: 43.75%,  total acc: 38.67%   
[EVAL] batch:   16 | acc: 62.50%,  total acc: 40.07%   
[EVAL] batch:   17 | acc: 43.75%,  total acc: 40.28%   
[EVAL] batch:   18 | acc: 25.00%,  total acc: 39.47%   
[EVAL] batch:   19 | acc: 37.50%,  total acc: 39.38%   
[EVAL] batch:   20 | acc: 18.75%,  total acc: 38.39%   
[EVAL] batch:   21 | acc: 12.50%,  total acc: 37.22%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   
[EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   
[EVAL] batch:    2 | acc: 62.50%,  total acc: 52.08%   
[EVAL] batch:    3 | acc: 25.00%,  total acc: 45.31%   
[EVAL] batch:    4 | acc: 31.25%,  total acc: 42.50%   
[EVAL] batch:    5 | acc: 50.00%,  total acc: 43.75%   
[EVAL] batch:    6 | acc: 50.00%,  total acc: 44.64%   
[EVAL] batch:    7 | acc: 62.50%,  total acc: 46.88%   
[EVAL] batch:    8 | acc: 56.25%,  total acc: 47.92%   
[EVAL] batch:    9 | acc: 62.50%,  total acc: 49.38%   
[EVAL] batch:   10 | acc: 68.75%,  total acc: 51.14%   
[EVAL] batch:   11 | acc: 56.25%,  total acc: 51.56%   
[EVAL] batch:   12 | acc: 37.50%,  total acc: 50.48%   
[EVAL] batch:   13 | acc: 31.25%,  total acc: 49.11%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 50.83%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 51.17%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 52.57%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 53.12%   
[EVAL] batch:   18 | acc: 75.00%,  total acc: 54.28%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 55.94%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 58.04%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 59.94%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 61.68%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 63.28%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 64.75%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 66.11%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 67.13%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 68.30%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 69.40%   
[EVAL] batch:   29 | acc: 87.50%,  total acc: 70.00%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 70.36%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 71.09%   
[EVAL] batch:   32 | acc: 93.75%,  total acc: 71.78%   
[EVAL] batch:   33 | acc: 31.25%,  total acc: 70.59%   
[EVAL] batch:   34 | acc: 6.25%,  total acc: 68.75%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 66.84%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 65.03%   
[EVAL] batch:   37 | acc: 6.25%,  total acc: 63.49%   
[EVAL] batch:   38 | acc: 12.50%,  total acc: 62.18%   
[EVAL] batch:   39 | acc: 81.25%,  total acc: 62.66%   
[EVAL] batch:   40 | acc: 93.75%,  total acc: 63.41%   
[EVAL] batch:   41 | acc: 100.00%,  total acc: 64.29%   
[EVAL] batch:   42 | acc: 93.75%,  total acc: 64.97%   
[EVAL] batch:   43 | acc: 81.25%,  total acc: 65.34%   
[EVAL] batch:   44 | acc: 81.25%,  total acc: 65.69%   
[EVAL] batch:   45 | acc: 87.50%,  total acc: 66.17%   
[EVAL] batch:   46 | acc: 75.00%,  total acc: 66.36%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 67.06%   
[EVAL] batch:   48 | acc: 37.50%,  total acc: 66.45%   
[EVAL] batch:   49 | acc: 43.75%,  total acc: 66.00%   
[EVAL] batch:   50 | acc: 50.00%,  total acc: 65.69%   
[EVAL] batch:   51 | acc: 75.00%,  total acc: 65.87%   
[EVAL] batch:   52 | acc: 62.50%,  total acc: 65.80%   
[EVAL] batch:   53 | acc: 81.25%,  total acc: 66.09%   
[EVAL] batch:   54 | acc: 87.50%,  total acc: 66.48%   
[EVAL] batch:   55 | acc: 87.50%,  total acc: 66.85%   
[EVAL] batch:   56 | acc: 87.50%,  total acc: 67.21%   
[EVAL] batch:   57 | acc: 25.00%,  total acc: 66.49%   
[EVAL] batch:   58 | acc: 56.25%,  total acc: 66.31%   
[EVAL] batch:   59 | acc: 56.25%,  total acc: 66.15%   
[EVAL] batch:   60 | acc: 62.50%,  total acc: 66.09%   
[EVAL] batch:   61 | acc: 50.00%,  total acc: 65.83%   
[EVAL] batch:   62 | acc: 31.25%,  total acc: 65.28%   
[EVAL] batch:   63 | acc: 62.50%,  total acc: 65.23%   
[EVAL] batch:   64 | acc: 87.50%,  total acc: 65.58%   
[EVAL] batch:   65 | acc: 62.50%,  total acc: 65.53%   
[EVAL] batch:   66 | acc: 25.00%,  total acc: 64.93%   
[EVAL] batch:   67 | acc: 25.00%,  total acc: 64.34%   
[EVAL] batch:   68 | acc: 18.75%,  total acc: 63.68%   
[EVAL] batch:   69 | acc: 6.25%,  total acc: 62.86%   
[EVAL] batch:   70 | acc: 25.00%,  total acc: 62.32%   
[EVAL] batch:   71 | acc: 6.25%,  total acc: 61.55%   
[EVAL] batch:   72 | acc: 43.75%,  total acc: 61.30%   
[EVAL] batch:   73 | acc: 50.00%,  total acc: 61.15%   
[EVAL] batch:   74 | acc: 87.50%,  total acc: 61.50%   
[EVAL] batch:   75 | acc: 62.50%,  total acc: 61.51%   
[EVAL] batch:   76 | acc: 50.00%,  total acc: 61.36%   
[EVAL] batch:   77 | acc: 56.25%,  total acc: 61.30%   
[EVAL] batch:   78 | acc: 37.50%,  total acc: 61.00%   
[EVAL] batch:   79 | acc: 50.00%,  total acc: 60.86%   
[EVAL] batch:   80 | acc: 43.75%,  total acc: 60.65%   
[EVAL] batch:   81 | acc: 50.00%,  total acc: 60.52%   
[EVAL] batch:   82 | acc: 56.25%,  total acc: 60.47%   
[EVAL] batch:   83 | acc: 37.50%,  total acc: 60.19%   
[EVAL] batch:   84 | acc: 37.50%,  total acc: 59.93%   
[EVAL] batch:   85 | acc: 25.00%,  total acc: 59.52%   
[EVAL] batch:   86 | acc: 18.75%,  total acc: 59.05%   
[EVAL] batch:   87 | acc: 6.25%,  total acc: 58.45%   
cur_acc:  ['0.8561', '0.8393', '0.3984', '0.6442', '0.3722']
his_acc:  ['0.8561', '0.8391', '0.7338', '0.6695', '0.5845']
CurrentTrain: epoch  0, batch     0 | loss: 5.2650971
CurrentTrain: epoch  0, batch     1 | loss: 5.1736107
CurrentTrain: epoch  1, batch     0 | loss: 4.0695062
CurrentTrain: epoch  1, batch     1 | loss: 3.5902531
CurrentTrain: epoch  2, batch     0 | loss: 3.8591537
CurrentTrain: epoch  2, batch     1 | loss: 2.9343579
CurrentTrain: epoch  3, batch     0 | loss: 3.2775469
CurrentTrain: epoch  3, batch     1 | loss: 3.1941655
CurrentTrain: epoch  4, batch     0 | loss: 3.0272772
CurrentTrain: epoch  4, batch     1 | loss: 3.0369561
CurrentTrain: epoch  5, batch     0 | loss: 2.9401188
CurrentTrain: epoch  5, batch     1 | loss: 2.5833511
CurrentTrain: epoch  6, batch     0 | loss: 2.8283296
CurrentTrain: epoch  6, batch     1 | loss: 2.2916741
CurrentTrain: epoch  7, batch     0 | loss: 2.3785889
CurrentTrain: epoch  7, batch     1 | loss: 2.4475141
CurrentTrain: epoch  8, batch     0 | loss: 2.3968978
CurrentTrain: epoch  8, batch     1 | loss: 2.3164880
CurrentTrain: epoch  9, batch     0 | loss: 2.1809769
CurrentTrain: epoch  9, batch     1 | loss: 2.0500672
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization Green Earth Initiative was established in 2010 to promote environmental awareness and sustainability practices across communities.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, emily decided to throw a big party with all her friends.  
Head Entity: emily  
Tail Entity: 30  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena grew up in barcelona, where she developed a passion for art and culture.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during the interview, john mentioned that he was born in new york but moved to los angeles as a child.  
Head Entity: john  
Tail Entity: new york  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians as members, including the famous conductor Leonard Bernstein.  
Head Entity: New York Philharmonic  
Tail Entity: Leonard Bernstein  

Relation: organization members  
Context: The National Geographic Society has a long history of exploration and education, with many prominent explorers and scientists, such as Jane Goodall, being members of the organization.  
Head Entity: National Geographic Society  
Tail Entity: Jane Goodall  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how each member contributes to the rich tapestry of their beliefs.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The famous author often drew inspiration from his Hindu upbringing, weaving elements of spirituality and philosophy into his novels.  
Head Entity: author  
Tail Entity: Hindu  
Mixup data size:  225
MixupTrain:  epoch  0, batch     0 | loss: 2.7717178
MixupTrain:  epoch  0, batch     1 | loss: 2.7781892
MixupTrain:  epoch  0, batch     2 | loss: 3.1788735
MixupTrain:  epoch  0, batch     3 | loss: 2.4750979
MixupTrain:  epoch  0, batch     4 | loss: 3.0465410
MixupTrain:  epoch  0, batch     5 | loss: 2.6938412
MixupTrain:  epoch  0, batch     6 | loss: 2.3823552
MixupTrain:  epoch  0, batch     7 | loss: 2.6669736
MixupTrain:  epoch  0, batch     8 | loss: 2.7012067
MixupTrain:  epoch  0, batch     9 | loss: 2.4450603
MixupTrain:  epoch  0, batch    10 | loss: 2.8410521
MixupTrain:  epoch  0, batch    11 | loss: 2.7537117
MixupTrain:  epoch  0, batch    12 | loss: 2.5434933
MixupTrain:  epoch  0, batch    13 | loss: 2.4690695
MixupTrain:  epoch  0, batch    14 | loss: 2.5095372
MemoryTrain:  epoch  0, batch     0 | loss: 2.6056538
MemoryTrain:  epoch  0, batch     1 | loss: 2.4583514
MemoryTrain:  epoch  0, batch     2 | loss: 2.5705457
MemoryTrain:  epoch  0, batch     3 | loss: 2.6978383
MemoryTrain:  epoch  0, batch     4 | loss: 2.8648684
MemoryTrain:  epoch  0, batch     5 | loss: 2.8242016
MemoryTrain:  epoch  1, batch     0 | loss: 2.2525668
MemoryTrain:  epoch  1, batch     1 | loss: 2.2271867
MemoryTrain:  epoch  1, batch     2 | loss: 2.5332410
MemoryTrain:  epoch  1, batch     3 | loss: 3.1518881
MemoryTrain:  epoch  1, batch     4 | loss: 2.1686556
MemoryTrain:  epoch  1, batch     5 | loss: 2.3254795
MemoryTrain:  epoch  2, batch     0 | loss: 2.0256286
MemoryTrain:  epoch  2, batch     1 | loss: 2.5663404
MemoryTrain:  epoch  2, batch     2 | loss: 2.0606365
MemoryTrain:  epoch  2, batch     3 | loss: 1.9509804
MemoryTrain:  epoch  2, batch     4 | loss: 2.2578168
MemoryTrain:  epoch  2, batch     5 | loss: 2.5155354
MemoryTrain:  epoch  3, batch     0 | loss: 1.9669518
MemoryTrain:  epoch  3, batch     1 | loss: 2.1103857
MemoryTrain:  epoch  3, batch     2 | loss: 2.2063701
MemoryTrain:  epoch  3, batch     3 | loss: 1.6444621
MemoryTrain:  epoch  3, batch     4 | loss: 1.8736341
MemoryTrain:  epoch  3, batch     5 | loss: 1.8730506
MemoryTrain:  epoch  4, batch     0 | loss: 1.9096373
MemoryTrain:  epoch  4, batch     1 | loss: 1.8828936
MemoryTrain:  epoch  4, batch     2 | loss: 2.2898073
MemoryTrain:  epoch  4, batch     3 | loss: 1.6703477
MemoryTrain:  epoch  4, batch     4 | loss: 1.9977902
MemoryTrain:  epoch  4, batch     5 | loss: 2.0286851
MemoryTrain:  epoch  5, batch     0 | loss: 1.8455096
MemoryTrain:  epoch  5, batch     1 | loss: 1.9537061
MemoryTrain:  epoch  5, batch     2 | loss: 1.7377980
MemoryTrain:  epoch  5, batch     3 | loss: 1.5536206
MemoryTrain:  epoch  5, batch     4 | loss: 1.3702914
MemoryTrain:  epoch  5, batch     5 | loss: 2.1080410
MemoryTrain:  epoch  6, batch     0 | loss: 1.4280987
MemoryTrain:  epoch  6, batch     1 | loss: 1.5992978
MemoryTrain:  epoch  6, batch     2 | loss: 1.4478883
MemoryTrain:  epoch  6, batch     3 | loss: 1.7365928
MemoryTrain:  epoch  6, batch     4 | loss: 1.8117052
MemoryTrain:  epoch  6, batch     5 | loss: 1.4914397
MemoryTrain:  epoch  7, batch     0 | loss: 1.7977333
MemoryTrain:  epoch  7, batch     1 | loss: 1.4592361
MemoryTrain:  epoch  7, batch     2 | loss: 1.4253256
MemoryTrain:  epoch  7, batch     3 | loss: 1.5226423
MemoryTrain:  epoch  7, batch     4 | loss: 1.4949136
MemoryTrain:  epoch  7, batch     5 | loss: 2.0207167
MemoryTrain:  epoch  8, batch     0 | loss: 1.4314015
MemoryTrain:  epoch  8, batch     1 | loss: 1.7142134
MemoryTrain:  epoch  8, batch     2 | loss: 1.3825135
MemoryTrain:  epoch  8, batch     3 | loss: 1.3477424
MemoryTrain:  epoch  8, batch     4 | loss: 1.4008263
MemoryTrain:  epoch  8, batch     5 | loss: 1.8477299
MemoryTrain:  epoch  9, batch     0 | loss: 1.3832300
MemoryTrain:  epoch  9, batch     1 | loss: 1.4557252
MemoryTrain:  epoch  9, batch     2 | loss: 1.5935907
MemoryTrain:  epoch  9, batch     3 | loss: 1.6038117
MemoryTrain:  epoch  9, batch     4 | loss: 1.3716192
MemoryTrain:  epoch  9, batch     5 | loss: 1.5668435
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    8 | acc: 87.50%,  total acc: 98.61%   
[EVAL] batch:    9 | acc: 31.25%,  total acc: 91.88%   
[EVAL] batch:   10 | acc: 37.50%,  total acc: 86.93%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 86.98%   
[EVAL] batch:   12 | acc: 62.50%,  total acc: 85.10%   
[EVAL] batch:   13 | acc: 56.25%,  total acc: 83.04%   
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   
[EVAL] batch:    1 | acc: 50.00%,  total acc: 46.88%   
[EVAL] batch:    2 | acc: 62.50%,  total acc: 52.08%   
[EVAL] batch:    3 | acc: 25.00%,  total acc: 45.31%   
[EVAL] batch:    4 | acc: 31.25%,  total acc: 42.50%   
[EVAL] batch:    5 | acc: 50.00%,  total acc: 43.75%   
[EVAL] batch:    6 | acc: 62.50%,  total acc: 46.43%   
[EVAL] batch:    7 | acc: 75.00%,  total acc: 50.00%   
[EVAL] batch:    8 | acc: 75.00%,  total acc: 52.78%   
[EVAL] batch:    9 | acc: 62.50%,  total acc: 53.75%   
[EVAL] batch:   10 | acc: 68.75%,  total acc: 55.11%   
[EVAL] batch:   11 | acc: 68.75%,  total acc: 56.25%   
[EVAL] batch:   12 | acc: 43.75%,  total acc: 55.29%   
[EVAL] batch:   13 | acc: 31.25%,  total acc: 53.57%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 54.58%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 54.69%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 55.88%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 56.25%   
[EVAL] batch:   18 | acc: 75.00%,  total acc: 57.24%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 58.44%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 60.42%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 61.93%   
[EVAL] batch:   22 | acc: 93.75%,  total acc: 63.32%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 64.84%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 66.25%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 67.55%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 68.52%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 69.64%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 70.69%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 71.04%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 71.37%   
[EVAL] batch:   31 | acc: 87.50%,  total acc: 71.88%   
[EVAL] batch:   32 | acc: 81.25%,  total acc: 72.16%   
[EVAL] batch:   33 | acc: 31.25%,  total acc: 70.96%   
[EVAL] batch:   34 | acc: 12.50%,  total acc: 69.29%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 67.36%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 65.54%   
[EVAL] batch:   37 | acc: 6.25%,  total acc: 63.98%   
[EVAL] batch:   38 | acc: 12.50%,  total acc: 62.66%   
[EVAL] batch:   39 | acc: 87.50%,  total acc: 63.28%   
[EVAL] batch:   40 | acc: 100.00%,  total acc: 64.18%   
[EVAL] batch:   41 | acc: 100.00%,  total acc: 65.03%   
[EVAL] batch:   42 | acc: 93.75%,  total acc: 65.70%   
[EVAL] batch:   43 | acc: 81.25%,  total acc: 66.05%   
[EVAL] batch:   44 | acc: 93.75%,  total acc: 66.67%   
[EVAL] batch:   45 | acc: 81.25%,  total acc: 66.98%   
[EVAL] batch:   46 | acc: 68.75%,  total acc: 67.02%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 67.71%   
[EVAL] batch:   48 | acc: 37.50%,  total acc: 67.09%   
[EVAL] batch:   49 | acc: 25.00%,  total acc: 66.25%   
[EVAL] batch:   50 | acc: 43.75%,  total acc: 65.81%   
[EVAL] batch:   51 | acc: 68.75%,  total acc: 65.87%   
[EVAL] batch:   52 | acc: 37.50%,  total acc: 65.33%   
[EVAL] batch:   53 | acc: 81.25%,  total acc: 65.62%   
[EVAL] batch:   54 | acc: 81.25%,  total acc: 65.91%   
[EVAL] batch:   55 | acc: 87.50%,  total acc: 66.29%   
[EVAL] batch:   56 | acc: 75.00%,  total acc: 66.45%   
[EVAL] batch:   57 | acc: 75.00%,  total acc: 66.59%   
[EVAL] batch:   58 | acc: 87.50%,  total acc: 66.95%   
[EVAL] batch:   59 | acc: 68.75%,  total acc: 66.98%   
[EVAL] batch:   60 | acc: 87.50%,  total acc: 67.32%   
[EVAL] batch:   61 | acc: 62.50%,  total acc: 67.24%   
[EVAL] batch:   62 | acc: 43.75%,  total acc: 66.87%   
[EVAL] batch:   63 | acc: 75.00%,  total acc: 66.99%   
[EVAL] batch:   64 | acc: 87.50%,  total acc: 67.31%   
[EVAL] batch:   65 | acc: 56.25%,  total acc: 67.14%   
[EVAL] batch:   66 | acc: 25.00%,  total acc: 66.51%   
[EVAL] batch:   67 | acc: 25.00%,  total acc: 65.90%   
[EVAL] batch:   68 | acc: 18.75%,  total acc: 65.22%   
[EVAL] batch:   69 | acc: 37.50%,  total acc: 64.82%   
[EVAL] batch:   70 | acc: 31.25%,  total acc: 64.35%   
[EVAL] batch:   71 | acc: 31.25%,  total acc: 63.89%   
[EVAL] batch:   72 | acc: 56.25%,  total acc: 63.78%   
[EVAL] batch:   73 | acc: 31.25%,  total acc: 63.34%   
[EVAL] batch:   74 | acc: 50.00%,  total acc: 63.17%   
[EVAL] batch:   75 | acc: 56.25%,  total acc: 63.08%   
[EVAL] batch:   76 | acc: 37.50%,  total acc: 62.74%   
[EVAL] batch:   77 | acc: 31.25%,  total acc: 62.34%   
[EVAL] batch:   78 | acc: 43.75%,  total acc: 62.10%   
[EVAL] batch:   79 | acc: 81.25%,  total acc: 62.34%   
[EVAL] batch:   80 | acc: 81.25%,  total acc: 62.58%   
[EVAL] batch:   81 | acc: 75.00%,  total acc: 62.73%   
[EVAL] batch:   82 | acc: 81.25%,  total acc: 62.95%   
[EVAL] batch:   83 | acc: 50.00%,  total acc: 62.80%   
[EVAL] batch:   84 | acc: 18.75%,  total acc: 62.28%   
[EVAL] batch:   85 | acc: 18.75%,  total acc: 61.77%   
[EVAL] batch:   86 | acc: 12.50%,  total acc: 61.21%   
[EVAL] batch:   87 | acc: 81.25%,  total acc: 61.43%   
[EVAL] batch:   88 | acc: 100.00%,  total acc: 61.87%   
[EVAL] batch:   89 | acc: 100.00%,  total acc: 62.29%   
[EVAL] batch:   90 | acc: 100.00%,  total acc: 62.71%   
[EVAL] batch:   91 | acc: 100.00%,  total acc: 63.11%   
[EVAL] batch:   92 | acc: 100.00%,  total acc: 63.51%   
[EVAL] batch:   93 | acc: 100.00%,  total acc: 63.90%   
[EVAL] batch:   94 | acc: 100.00%,  total acc: 64.28%   
[EVAL] batch:   95 | acc: 100.00%,  total acc: 64.65%   
[EVAL] batch:   96 | acc: 31.25%,  total acc: 64.30%   
[EVAL] batch:   97 | acc: 25.00%,  total acc: 63.90%   
[EVAL] batch:   98 | acc: 87.50%,  total acc: 64.14%   
[EVAL] batch:   99 | acc: 68.75%,  total acc: 64.19%   
[EVAL] batch:  100 | acc: 68.75%,  total acc: 64.23%   
cur_acc:  ['0.8561', '0.8393', '0.3984', '0.6442', '0.3722', '0.8304']
his_acc:  ['0.8561', '0.8391', '0.7338', '0.6695', '0.5845', '0.6423']
CurrentTrain: epoch  0, batch     0 | loss: 6.2013426
CurrentTrain: epoch  0, batch     1 | loss: 5.6056986
CurrentTrain: epoch  1, batch     0 | loss: 5.0500894
CurrentTrain: epoch  1, batch     1 | loss: 4.8981624
CurrentTrain: epoch  2, batch     0 | loss: 4.8681593
CurrentTrain: epoch  2, batch     1 | loss: 4.2027407
CurrentTrain: epoch  3, batch     0 | loss: 4.4302883
CurrentTrain: epoch  3, batch     1 | loss: 4.7679372
CurrentTrain: epoch  4, batch     0 | loss: 4.5045867
CurrentTrain: epoch  4, batch     1 | loss: 3.6302054
CurrentTrain: epoch  5, batch     0 | loss: 3.7515607
CurrentTrain: epoch  5, batch     1 | loss: 4.3116302
CurrentTrain: epoch  6, batch     0 | loss: 3.8143554
CurrentTrain: epoch  6, batch     1 | loss: 3.3966200
CurrentTrain: epoch  7, batch     0 | loss: 3.3075960
CurrentTrain: epoch  7, batch     1 | loss: 3.5347855
CurrentTrain: epoch  8, batch     0 | loss: 3.2837749
CurrentTrain: epoch  8, batch     1 | loss: 3.7026639
CurrentTrain: epoch  9, batch     0 | loss: 3.0046072
CurrentTrain: epoch  9, batch     1 | loss: 3.1374485
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: san jose , ca , usa speaking of k-fed , him and ex-wife britney spears are in court today , dealing with their custody battle .
Head Entity: britney spears
Tail Entity: ca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York City, John decided to settle down in the sunny beaches of Miami, Florida, where he could enjoy a more relaxed lifestyle.  
Head Entity: John  
Tail Entity: Florida  

Relation: person stateorprovinces of residence  
Context: During her tour in Canada, Taylor Swift mentioned how much she loves living in the vibrant city of Toronto, which has become her second home.  
Head Entity: Taylor Swift  
Tail Entity: Toronto  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away on July 20, 2021, after a long illness.  
Head Entity: The renowned author  
Tail Entity: July 20, 2021  

Relation: person date of death  
Context: She left this world peacefully in her sleep on March 5, 2019.  
Head Entity: She  
Tail Entity: March 5, 2019  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, boasts a workforce of approximately 5,500 employees across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: GreenEarth Nonprofit has grown significantly over the past few years and currently employs around 1,200 dedicated staff members to support its environmental initiatives.  
Head Entity: GreenEarth Nonprofit  
Tail Entity: 1,200  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Langhorne Clemens, better known by his pen name Mark Twain, is celebrated for his novels like "The Adventures of Tom Sawyer."  
Head Entity: Samuel Langhorne Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician Stefani Joanne Angelina Germanotta, who is widely recognized as Lady Gaga, has made a significant impact on the pop music scene.  
Head Entity: Stefani Joanne Angelina Germanotta  
Tail Entity: Lady Gaga  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: in addition to his wife , meskill is survived by two daughters , eileen gallup of new britain and maureen heneghan of haddon heights , n.j. ; three sons , john , of kensington , conn. ; peter , of east hartford , conn. ; and thomas , of branford , conn. ; two sisters , ruth prior of naples , fla. , and sister laura marie of portland , conn. ; five grandchildren , and two step-grandchildren .
Head Entity: his
Tail Entity: meskill
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: after a long and happy marriage, john and his wife, sarah, celebrated their 50th wedding anniversary surrounded by family and friends.  
Head Entity: his  
Tail Entity: wife  

Relation: person spouse  
Context: during the ceremony, emily shared heartfelt vows with her husband, mark, promising to support him through thick and thin.  
Head Entity: her  
Tail Entity: husband  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 2.2416716
MixupTrain:  epoch  0, batch     1 | loss: 2.4152169
MixupTrain:  epoch  0, batch     2 | loss: 2.0090976
MixupTrain:  epoch  0, batch     3 | loss: 2.0472171
MixupTrain:  epoch  0, batch     4 | loss: 2.0210047
MixupTrain:  epoch  0, batch     5 | loss: 2.4117799
MixupTrain:  epoch  0, batch     6 | loss: 2.0899858
MixupTrain:  epoch  0, batch     7 | loss: 2.7812634
MixupTrain:  epoch  0, batch     8 | loss: 2.1356356
MixupTrain:  epoch  0, batch     9 | loss: 2.2046397
MixupTrain:  epoch  0, batch    10 | loss: 1.9651434
MixupTrain:  epoch  0, batch    11 | loss: 2.0477138
MixupTrain:  epoch  0, batch    12 | loss: 2.0488350
MixupTrain:  epoch  0, batch    13 | loss: 2.6221528
MixupTrain:  epoch  0, batch    14 | loss: 2.3696971
MixupTrain:  epoch  0, batch    15 | loss: 1.8368584
MemoryTrain:  epoch  0, batch     0 | loss: 2.2965498
MemoryTrain:  epoch  0, batch     1 | loss: 2.2561150
MemoryTrain:  epoch  0, batch     2 | loss: 2.8606455
MemoryTrain:  epoch  0, batch     3 | loss: 1.8666292
MemoryTrain:  epoch  0, batch     4 | loss: 1.9346597
MemoryTrain:  epoch  0, batch     5 | loss: 2.1632559
MemoryTrain:  epoch  0, batch     6 | loss: 2.1344929
MemoryTrain:  epoch  1, batch     0 | loss: 3.5734828
MemoryTrain:  epoch  1, batch     1 | loss: 1.9393892
MemoryTrain:  epoch  1, batch     2 | loss: 1.9415734
MemoryTrain:  epoch  1, batch     3 | loss: 1.9739339
MemoryTrain:  epoch  1, batch     4 | loss: 1.9391459
MemoryTrain:  epoch  1, batch     5 | loss: 1.7186446
MemoryTrain:  epoch  1, batch     6 | loss: 1.8338583
MemoryTrain:  epoch  2, batch     0 | loss: 1.8857021
MemoryTrain:  epoch  2, batch     1 | loss: 1.8412776
MemoryTrain:  epoch  2, batch     2 | loss: 1.9239597
MemoryTrain:  epoch  2, batch     3 | loss: 1.6386368
MemoryTrain:  epoch  2, batch     4 | loss: 2.0065827
MemoryTrain:  epoch  2, batch     5 | loss: 1.7082999
MemoryTrain:  epoch  2, batch     6 | loss: 1.9976773
MemoryTrain:  epoch  3, batch     0 | loss: 2.0526719
MemoryTrain:  epoch  3, batch     1 | loss: 1.4516792
MemoryTrain:  epoch  3, batch     2 | loss: 1.3880925
MemoryTrain:  epoch  3, batch     3 | loss: 1.6664538
MemoryTrain:  epoch  3, batch     4 | loss: 1.9371877
MemoryTrain:  epoch  3, batch     5 | loss: 1.8653239
MemoryTrain:  epoch  3, batch     6 | loss: 1.2977933
MemoryTrain:  epoch  4, batch     0 | loss: 1.5369267
MemoryTrain:  epoch  4, batch     1 | loss: 1.7993765
MemoryTrain:  epoch  4, batch     2 | loss: 1.4001697
MemoryTrain:  epoch  4, batch     3 | loss: 1.3788230
MemoryTrain:  epoch  4, batch     4 | loss: 1.6509216
MemoryTrain:  epoch  4, batch     5 | loss: 1.7531196
MemoryTrain:  epoch  4, batch     6 | loss: 1.7525357
MemoryTrain:  epoch  5, batch     0 | loss: 1.7165694
MemoryTrain:  epoch  5, batch     1 | loss: 1.5752748
MemoryTrain:  epoch  5, batch     2 | loss: 1.3413081
MemoryTrain:  epoch  5, batch     3 | loss: 1.3919128
MemoryTrain:  epoch  5, batch     4 | loss: 1.8069710
MemoryTrain:  epoch  5, batch     5 | loss: 1.4310760
MemoryTrain:  epoch  5, batch     6 | loss: 1.4777715
MemoryTrain:  epoch  6, batch     0 | loss: 1.6023636
MemoryTrain:  epoch  6, batch     1 | loss: 1.5769185
MemoryTrain:  epoch  6, batch     2 | loss: 1.4367485
MemoryTrain:  epoch  6, batch     3 | loss: 1.5806272
MemoryTrain:  epoch  6, batch     4 | loss: 1.4148602
MemoryTrain:  epoch  6, batch     5 | loss: 1.2857754
MemoryTrain:  epoch  6, batch     6 | loss: 1.3921421
MemoryTrain:  epoch  7, batch     0 | loss: 1.6010432
MemoryTrain:  epoch  7, batch     1 | loss: 1.5880849
MemoryTrain:  epoch  7, batch     2 | loss: 1.3825711
MemoryTrain:  epoch  7, batch     3 | loss: 1.5266931
MemoryTrain:  epoch  7, batch     4 | loss: 1.4408402
MemoryTrain:  epoch  7, batch     5 | loss: 1.2950230
MemoryTrain:  epoch  7, batch     6 | loss: 1.2713650
MemoryTrain:  epoch  8, batch     0 | loss: 1.5392675
MemoryTrain:  epoch  8, batch     1 | loss: 1.3620927
MemoryTrain:  epoch  8, batch     2 | loss: 1.4020728
MemoryTrain:  epoch  8, batch     3 | loss: 1.2847507
MemoryTrain:  epoch  8, batch     4 | loss: 1.3975790
MemoryTrain:  epoch  8, batch     5 | loss: 1.4451593
MemoryTrain:  epoch  8, batch     6 | loss: 1.4614713
MemoryTrain:  epoch  9, batch     0 | loss: 1.3047589
MemoryTrain:  epoch  9, batch     1 | loss: 1.5115104
MemoryTrain:  epoch  9, batch     2 | loss: 1.3728089
MemoryTrain:  epoch  9, batch     3 | loss: 1.4660721
MemoryTrain:  epoch  9, batch     4 | loss: 1.2878401
MemoryTrain:  epoch  9, batch     5 | loss: 1.2807060
MemoryTrain:  epoch  9, batch     6 | loss: 1.3057990
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   
[EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   
[EVAL] batch:    2 | acc: 31.25%,  total acc: 45.83%   
[EVAL] batch:    3 | acc: 56.25%,  total acc: 48.44%   
[EVAL] batch:    4 | acc: 31.25%,  total acc: 45.00%   
[EVAL] batch:    5 | acc: 87.50%,  total acc: 52.08%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 58.04%   
[EVAL] batch:    7 | acc: 87.50%,  total acc: 61.72%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 65.28%   
[EVAL] batch:    9 | acc: 81.25%,  total acc: 66.88%   
[EVAL] batch:   10 | acc: 50.00%,  total acc: 65.34%   
[EVAL] batch:   11 | acc: 25.00%,  total acc: 61.98%   
[EVAL] batch:   12 | acc: 31.25%,  total acc: 59.62%   
[EVAL] batch:   13 | acc: 50.00%,  total acc: 58.93%   
[EVAL] batch:   14 | acc: 18.75%,  total acc: 56.25%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 46.88%   
[EVAL] batch:    2 | acc: 62.50%,  total acc: 52.08%   
[EVAL] batch:    3 | acc: 31.25%,  total acc: 46.88%   
[EVAL] batch:    4 | acc: 31.25%,  total acc: 43.75%   
[EVAL] batch:    5 | acc: 43.75%,  total acc: 43.75%   
[EVAL] batch:    6 | acc: 43.75%,  total acc: 43.75%   
[EVAL] batch:    7 | acc: 50.00%,  total acc: 44.53%   
[EVAL] batch:    8 | acc: 50.00%,  total acc: 45.14%   
[EVAL] batch:    9 | acc: 62.50%,  total acc: 46.88%   
[EVAL] batch:   10 | acc: 62.50%,  total acc: 48.30%   
[EVAL] batch:   11 | acc: 50.00%,  total acc: 48.44%   
[EVAL] batch:   12 | acc: 37.50%,  total acc: 47.60%   
[EVAL] batch:   13 | acc: 25.00%,  total acc: 45.98%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 47.50%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 48.05%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 49.63%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 50.35%   
[EVAL] batch:   18 | acc: 68.75%,  total acc: 51.32%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 52.81%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 55.06%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 56.82%   
[EVAL] batch:   22 | acc: 93.75%,  total acc: 58.42%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 59.90%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 61.50%   
[EVAL] batch:   25 | acc: 93.75%,  total acc: 62.74%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 63.89%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 65.18%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 66.38%   
[EVAL] batch:   29 | acc: 68.75%,  total acc: 66.46%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 66.94%   
[EVAL] batch:   31 | acc: 87.50%,  total acc: 67.58%   
[EVAL] batch:   32 | acc: 81.25%,  total acc: 67.99%   
[EVAL] batch:   33 | acc: 31.25%,  total acc: 66.91%   
[EVAL] batch:   34 | acc: 12.50%,  total acc: 65.36%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 63.54%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 61.82%   
[EVAL] batch:   37 | acc: 6.25%,  total acc: 60.36%   
[EVAL] batch:   38 | acc: 6.25%,  total acc: 58.97%   
[EVAL] batch:   39 | acc: 75.00%,  total acc: 59.38%   
[EVAL] batch:   40 | acc: 100.00%,  total acc: 60.37%   
[EVAL] batch:   41 | acc: 93.75%,  total acc: 61.16%   
[EVAL] batch:   42 | acc: 93.75%,  total acc: 61.92%   
[EVAL] batch:   43 | acc: 75.00%,  total acc: 62.22%   
[EVAL] batch:   44 | acc: 93.75%,  total acc: 62.92%   
[EVAL] batch:   45 | acc: 50.00%,  total acc: 62.64%   
[EVAL] batch:   46 | acc: 68.75%,  total acc: 62.77%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 63.54%   
[EVAL] batch:   48 | acc: 37.50%,  total acc: 63.01%   
[EVAL] batch:   49 | acc: 37.50%,  total acc: 62.50%   
[EVAL] batch:   50 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:   51 | acc: 68.75%,  total acc: 62.62%   
[EVAL] batch:   52 | acc: 68.75%,  total acc: 62.74%   
[EVAL] batch:   53 | acc: 75.00%,  total acc: 62.96%   
[EVAL] batch:   54 | acc: 75.00%,  total acc: 63.18%   
[EVAL] batch:   55 | acc: 75.00%,  total acc: 63.39%   
[EVAL] batch:   56 | acc: 87.50%,  total acc: 63.82%   
[EVAL] batch:   57 | acc: 81.25%,  total acc: 64.12%   
[EVAL] batch:   58 | acc: 100.00%,  total acc: 64.72%   
[EVAL] batch:   59 | acc: 81.25%,  total acc: 65.00%   
[EVAL] batch:   60 | acc: 50.00%,  total acc: 64.75%   
[EVAL] batch:   61 | acc: 31.25%,  total acc: 64.21%   
[EVAL] batch:   62 | acc: 12.50%,  total acc: 63.39%   
[EVAL] batch:   63 | acc: 31.25%,  total acc: 62.89%   
[EVAL] batch:   64 | acc: 75.00%,  total acc: 63.08%   
[EVAL] batch:   65 | acc: 62.50%,  total acc: 63.07%   
[EVAL] batch:   66 | acc: 18.75%,  total acc: 62.41%   
[EVAL] batch:   67 | acc: 18.75%,  total acc: 61.76%   
[EVAL] batch:   68 | acc: 25.00%,  total acc: 61.23%   
[EVAL] batch:   69 | acc: 43.75%,  total acc: 60.98%   
[EVAL] batch:   70 | acc: 31.25%,  total acc: 60.56%   
[EVAL] batch:   71 | acc: 43.75%,  total acc: 60.33%   
[EVAL] batch:   72 | acc: 50.00%,  total acc: 60.19%   
[EVAL] batch:   73 | acc: 37.50%,  total acc: 59.88%   
[EVAL] batch:   74 | acc: 50.00%,  total acc: 59.75%   
[EVAL] batch:   75 | acc: 56.25%,  total acc: 59.70%   
[EVAL] batch:   76 | acc: 43.75%,  total acc: 59.50%   
[EVAL] batch:   77 | acc: 43.75%,  total acc: 59.29%   
[EVAL] batch:   78 | acc: 43.75%,  total acc: 59.10%   
[EVAL] batch:   79 | acc: 62.50%,  total acc: 59.14%   
[EVAL] batch:   80 | acc: 62.50%,  total acc: 59.18%   
[EVAL] batch:   81 | acc: 56.25%,  total acc: 59.15%   
[EVAL] batch:   82 | acc: 62.50%,  total acc: 59.19%   
[EVAL] batch:   83 | acc: 43.75%,  total acc: 59.00%   
[EVAL] batch:   84 | acc: 25.00%,  total acc: 58.60%   
[EVAL] batch:   85 | acc: 18.75%,  total acc: 58.14%   
[EVAL] batch:   86 | acc: 12.50%,  total acc: 57.61%   
[EVAL] batch:   87 | acc: 87.50%,  total acc: 57.95%   
[EVAL] batch:   88 | acc: 100.00%,  total acc: 58.43%   
[EVAL] batch:   89 | acc: 100.00%,  total acc: 58.89%   
[EVAL] batch:   90 | acc: 100.00%,  total acc: 59.34%   
[EVAL] batch:   91 | acc: 100.00%,  total acc: 59.78%   
[EVAL] batch:   92 | acc: 100.00%,  total acc: 60.22%   
[EVAL] batch:   93 | acc: 100.00%,  total acc: 60.64%   
[EVAL] batch:   94 | acc: 100.00%,  total acc: 61.05%   
[EVAL] batch:   95 | acc: 81.25%,  total acc: 61.26%   
[EVAL] batch:   96 | acc: 31.25%,  total acc: 60.95%   
[EVAL] batch:   97 | acc: 31.25%,  total acc: 60.65%   
[EVAL] batch:   98 | acc: 87.50%,  total acc: 60.92%   
[EVAL] batch:   99 | acc: 68.75%,  total acc: 61.00%   
[EVAL] batch:  100 | acc: 75.00%,  total acc: 61.14%   
[EVAL] batch:  101 | acc: 50.00%,  total acc: 61.03%   
[EVAL] batch:  102 | acc: 56.25%,  total acc: 60.98%   
[EVAL] batch:  103 | acc: 25.00%,  total acc: 60.64%   
[EVAL] batch:  104 | acc: 56.25%,  total acc: 60.60%   
[EVAL] batch:  105 | acc: 37.50%,  total acc: 60.38%   
[EVAL] batch:  106 | acc: 87.50%,  total acc: 60.63%   
[EVAL] batch:  107 | acc: 93.75%,  total acc: 60.94%   
[EVAL] batch:  108 | acc: 87.50%,  total acc: 61.18%   
[EVAL] batch:  109 | acc: 93.75%,  total acc: 61.48%   
[EVAL] batch:  110 | acc: 81.25%,  total acc: 61.66%   
[EVAL] batch:  111 | acc: 50.00%,  total acc: 61.55%   
[EVAL] batch:  112 | acc: 18.75%,  total acc: 61.17%   
[EVAL] batch:  113 | acc: 31.25%,  total acc: 60.91%   
[EVAL] batch:  114 | acc: 50.00%,  total acc: 60.82%   
[EVAL] batch:  115 | acc: 18.75%,  total acc: 60.45%   
cur_acc:  ['0.8561', '0.8393', '0.3984', '0.6442', '0.3722', '0.8304', '0.5625']
his_acc:  ['0.8561', '0.8391', '0.7338', '0.6695', '0.5845', '0.6423', '0.6045']
CurrentTrain: epoch  0, batch     0 | loss: 6.2344894
CurrentTrain: epoch  0, batch     1 | loss: 5.7778592
CurrentTrain: epoch  1, batch     0 | loss: 5.0068989
CurrentTrain: epoch  1, batch     1 | loss: 4.4371719
CurrentTrain: epoch  2, batch     0 | loss: 4.2337494
CurrentTrain: epoch  2, batch     1 | loss: 3.6345401
CurrentTrain: epoch  3, batch     0 | loss: 3.6100669
CurrentTrain: epoch  3, batch     1 | loss: 3.3299167
CurrentTrain: epoch  4, batch     0 | loss: 3.3767951
CurrentTrain: epoch  4, batch     1 | loss: 2.7476990
CurrentTrain: epoch  5, batch     0 | loss: 2.9085538
CurrentTrain: epoch  5, batch     1 | loss: 2.5949233
CurrentTrain: epoch  6, batch     0 | loss: 2.8256257
CurrentTrain: epoch  6, batch     1 | loss: 2.4515312
CurrentTrain: epoch  7, batch     0 | loss: 2.6351624
CurrentTrain: epoch  7, batch     1 | loss: 2.4497566
CurrentTrain: epoch  8, batch     0 | loss: 2.5774050
CurrentTrain: epoch  8, batch     1 | loss: 2.4452193
CurrentTrain: epoch  9, batch     0 | loss: 2.2567854
CurrentTrain: epoch  9, batch     1 | loss: 2.1561427
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: after years of living in new york city, the famous author decided to return to her hometown of boston, where she feels most at home.  
Head Entity: she  
Tail Entity: boston  

Relation: person cities of residence  
Context: despite being a global superstar, the singer often reminisces about her childhood in nashville, where she first discovered her love for music.  
Head Entity: she  
Tail Entity: nashville  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: born in baltimore in 1922 , parren mitchell was a graduate of morgan state college and earned a master 's degree from the university of maryland , according to biographical information supplied by cummings ' office .
Head Entity: parren mitchell
Tail Entity: university of maryland
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: after completing high school in 1995, jessica went on to study at the university of california, los angeles, where she earned her bachelor's degree in sociology.  
Head Entity: jessica  
Tail Entity: university of california, los angeles  

Relation: person schools attended  
Context: during his early years, steven attended the prestigious harvard university, where he majored in computer science and graduated with honors.  
Head Entity: steven  
Tail Entity: harvard university  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author agatha christie died in her home in wallingford, england  
Head Entity: agatha christie  
Tail Entity: england  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are all doing well in school.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took care of their mother and supported each other.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, a crime that could result in significant prison time if convicted.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith was charged with assault following the altercation that took place outside the nightclub last weekend.  
Head Entity: Smith  
Tail Entity: assault  
Mixup data size:  284
MixupTrain:  epoch  0, batch     0 | loss: 1.7601666
MixupTrain:  epoch  0, batch     1 | loss: 2.1530461
MixupTrain:  epoch  0, batch     2 | loss: 2.0389125
MixupTrain:  epoch  0, batch     3 | loss: 1.8888240
MixupTrain:  epoch  0, batch     4 | loss: 2.0962353
MixupTrain:  epoch  0, batch     5 | loss: 1.8328371
MixupTrain:  epoch  0, batch     6 | loss: 1.7256901
MixupTrain:  epoch  0, batch     7 | loss: 2.1388083
MixupTrain:  epoch  0, batch     8 | loss: 1.9435356
MixupTrain:  epoch  0, batch     9 | loss: 1.7309083
MixupTrain:  epoch  0, batch    10 | loss: 2.1468368
MixupTrain:  epoch  0, batch    11 | loss: 1.7541504
MixupTrain:  epoch  0, batch    12 | loss: 2.2840557
MixupTrain:  epoch  0, batch    13 | loss: 2.1881471
MixupTrain:  epoch  0, batch    14 | loss: 1.5959562
MixupTrain:  epoch  0, batch    15 | loss: 2.1720862
MixupTrain:  epoch  0, batch    16 | loss: 2.0564172
MixupTrain:  epoch  0, batch    17 | loss: 1.5799192
MemoryTrain:  epoch  0, batch     0 | loss: 1.7477157
MemoryTrain:  epoch  0, batch     1 | loss: 1.6511091
MemoryTrain:  epoch  0, batch     2 | loss: 2.0252218
MemoryTrain:  epoch  0, batch     3 | loss: 1.9891229
MemoryTrain:  epoch  0, batch     4 | loss: 1.8047442
MemoryTrain:  epoch  0, batch     5 | loss: 2.0016146
MemoryTrain:  epoch  0, batch     6 | loss: 2.1175363
MemoryTrain:  epoch  0, batch     7 | loss: 2.4976845
MemoryTrain:  epoch  1, batch     0 | loss: 2.0095844
MemoryTrain:  epoch  1, batch     1 | loss: 1.9145756
MemoryTrain:  epoch  1, batch     2 | loss: 2.1998034
MemoryTrain:  epoch  1, batch     3 | loss: 1.4483666
MemoryTrain:  epoch  1, batch     4 | loss: 2.0719638
MemoryTrain:  epoch  1, batch     5 | loss: 1.9220846
MemoryTrain:  epoch  1, batch     6 | loss: 1.4974544
MemoryTrain:  epoch  1, batch     7 | loss: 1.3457469
MemoryTrain:  epoch  2, batch     0 | loss: 1.6019940
MemoryTrain:  epoch  2, batch     1 | loss: 1.8592277
MemoryTrain:  epoch  2, batch     2 | loss: 1.6224296
MemoryTrain:  epoch  2, batch     3 | loss: 1.6357291
MemoryTrain:  epoch  2, batch     4 | loss: 1.3921599
MemoryTrain:  epoch  2, batch     5 | loss: 1.6481240
MemoryTrain:  epoch  2, batch     6 | loss: 1.9026918
MemoryTrain:  epoch  2, batch     7 | loss: 1.5194163
MemoryTrain:  epoch  3, batch     0 | loss: 1.2687284
MemoryTrain:  epoch  3, batch     1 | loss: 1.5347338
MemoryTrain:  epoch  3, batch     2 | loss: 1.3104739
MemoryTrain:  epoch  3, batch     3 | loss: 1.6462121
MemoryTrain:  epoch  3, batch     4 | loss: 1.7213957
MemoryTrain:  epoch  3, batch     5 | loss: 1.5012016
MemoryTrain:  epoch  3, batch     6 | loss: 2.0039396
MemoryTrain:  epoch  3, batch     7 | loss: 1.4973620
MemoryTrain:  epoch  4, batch     0 | loss: 1.6991481
MemoryTrain:  epoch  4, batch     1 | loss: 1.4481537
MemoryTrain:  epoch  4, batch     2 | loss: 1.5612315
MemoryTrain:  epoch  4, batch     3 | loss: 1.4391098
MemoryTrain:  epoch  4, batch     4 | loss: 1.2303300
MemoryTrain:  epoch  4, batch     5 | loss: 1.4780670
MemoryTrain:  epoch  4, batch     6 | loss: 1.5957212
MemoryTrain:  epoch  4, batch     7 | loss: 1.2858896
MemoryTrain:  epoch  5, batch     0 | loss: 1.5165840
MemoryTrain:  epoch  5, batch     1 | loss: 1.2874756
MemoryTrain:  epoch  5, batch     2 | loss: 1.4392993
MemoryTrain:  epoch  5, batch     3 | loss: 1.3020228
MemoryTrain:  epoch  5, batch     4 | loss: 1.6326146
MemoryTrain:  epoch  5, batch     5 | loss: 1.5717731
MemoryTrain:  epoch  5, batch     6 | loss: 1.3358333
MemoryTrain:  epoch  5, batch     7 | loss: 1.6167322
MemoryTrain:  epoch  6, batch     0 | loss: 1.3764416
MemoryTrain:  epoch  6, batch     1 | loss: 1.2359790
MemoryTrain:  epoch  6, batch     2 | loss: 1.5387547
MemoryTrain:  epoch  6, batch     3 | loss: 1.2985058
MemoryTrain:  epoch  6, batch     4 | loss: 1.3524461
MemoryTrain:  epoch  6, batch     5 | loss: 1.3476965
MemoryTrain:  epoch  6, batch     6 | loss: 1.4426193
MemoryTrain:  epoch  6, batch     7 | loss: 1.4507947
MemoryTrain:  epoch  7, batch     0 | loss: 1.4546878
MemoryTrain:  epoch  7, batch     1 | loss: 1.3233416
MemoryTrain:  epoch  7, batch     2 | loss: 1.2381301
MemoryTrain:  epoch  7, batch     3 | loss: 1.2632254
MemoryTrain:  epoch  7, batch     4 | loss: 1.3498316
MemoryTrain:  epoch  7, batch     5 | loss: 1.2812090
MemoryTrain:  epoch  7, batch     6 | loss: 1.2846063
MemoryTrain:  epoch  7, batch     7 | loss: 1.3373257
MemoryTrain:  epoch  8, batch     0 | loss: 1.2931718
MemoryTrain:  epoch  8, batch     1 | loss: 1.2919431
MemoryTrain:  epoch  8, batch     2 | loss: 1.2809908
MemoryTrain:  epoch  8, batch     3 | loss: 1.3176174
MemoryTrain:  epoch  8, batch     4 | loss: 1.2568324
MemoryTrain:  epoch  8, batch     5 | loss: 1.2834405
MemoryTrain:  epoch  8, batch     6 | loss: 1.3463056
MemoryTrain:  epoch  8, batch     7 | loss: 1.2400258
MemoryTrain:  epoch  9, batch     0 | loss: 1.2604837
MemoryTrain:  epoch  9, batch     1 | loss: 1.2900217
MemoryTrain:  epoch  9, batch     2 | loss: 1.2612935
MemoryTrain:  epoch  9, batch     3 | loss: 1.2818007
MemoryTrain:  epoch  9, batch     4 | loss: 1.2595661
MemoryTrain:  epoch  9, batch     5 | loss: 1.3371142
MemoryTrain:  epoch  9, batch     6 | loss: 1.2935691
MemoryTrain:  epoch  9, batch     7 | loss: 1.2331417
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   
[EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   
[EVAL] batch:    5 | acc: 75.00%,  total acc: 76.04%   
[EVAL] batch:    6 | acc: 87.50%,  total acc: 77.68%   
[EVAL] batch:    7 | acc: 87.50%,  total acc: 78.91%   
[EVAL] batch:    8 | acc: 81.25%,  total acc: 79.17%   
[EVAL] batch:    9 | acc: 81.25%,  total acc: 79.38%   
[EVAL] batch:   10 | acc: 56.25%,  total acc: 77.27%   
[EVAL] batch:   11 | acc: 100.00%,  total acc: 79.17%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 80.29%   
[EVAL] batch:   13 | acc: 100.00%,  total acc: 81.70%   
[EVAL] batch:   14 | acc: 100.00%,  total acc: 82.92%   
[EVAL] batch:   15 | acc: 100.00%,  total acc: 83.98%   
[EVAL] batch:   16 | acc: 100.00%,  total acc: 84.93%   
[EVAL] batch:   17 | acc: 25.00%,  total acc: 81.60%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   
[EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   
[EVAL] batch:    2 | acc: 50.00%,  total acc: 43.75%   
[EVAL] batch:    3 | acc: 25.00%,  total acc: 39.06%   
[EVAL] batch:    4 | acc: 18.75%,  total acc: 35.00%   
[EVAL] batch:    5 | acc: 31.25%,  total acc: 34.38%   
[EVAL] batch:    6 | acc: 31.25%,  total acc: 33.93%   
[EVAL] batch:    7 | acc: 50.00%,  total acc: 35.94%   
[EVAL] batch:    8 | acc: 56.25%,  total acc: 38.19%   
[EVAL] batch:    9 | acc: 62.50%,  total acc: 40.62%   
[EVAL] batch:   10 | acc: 62.50%,  total acc: 42.61%   
[EVAL] batch:   11 | acc: 37.50%,  total acc: 42.19%   
[EVAL] batch:   12 | acc: 31.25%,  total acc: 41.35%   
[EVAL] batch:   13 | acc: 37.50%,  total acc: 41.07%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 42.92%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 43.75%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 45.59%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 46.53%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 47.37%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 49.06%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 51.49%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 53.41%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 55.43%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 57.29%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 59.00%   
[EVAL] batch:   25 | acc: 93.75%,  total acc: 60.34%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 61.57%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 62.95%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 64.22%   
[EVAL] batch:   29 | acc: 87.50%,  total acc: 65.00%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 65.52%   
[EVAL] batch:   31 | acc: 87.50%,  total acc: 66.21%   
[EVAL] batch:   32 | acc: 87.50%,  total acc: 66.86%   
[EVAL] batch:   33 | acc: 18.75%,  total acc: 65.44%   
[EVAL] batch:   34 | acc: 6.25%,  total acc: 63.75%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 61.98%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 60.30%   
[EVAL] batch:   37 | acc: 0.00%,  total acc: 58.72%   
[EVAL] batch:   38 | acc: 6.25%,  total acc: 57.37%   
[EVAL] batch:   39 | acc: 68.75%,  total acc: 57.66%   
[EVAL] batch:   40 | acc: 100.00%,  total acc: 58.69%   
[EVAL] batch:   41 | acc: 87.50%,  total acc: 59.38%   
[EVAL] batch:   42 | acc: 81.25%,  total acc: 59.88%   
[EVAL] batch:   43 | acc: 68.75%,  total acc: 60.09%   
[EVAL] batch:   44 | acc: 87.50%,  total acc: 60.69%   
[EVAL] batch:   45 | acc: 37.50%,  total acc: 60.19%   
[EVAL] batch:   46 | acc: 68.75%,  total acc: 60.37%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 61.20%   
[EVAL] batch:   48 | acc: 37.50%,  total acc: 60.71%   
[EVAL] batch:   49 | acc: 43.75%,  total acc: 60.38%   
[EVAL] batch:   50 | acc: 62.50%,  total acc: 60.42%   
[EVAL] batch:   51 | acc: 68.75%,  total acc: 60.58%   
[EVAL] batch:   52 | acc: 75.00%,  total acc: 60.85%   
[EVAL] batch:   53 | acc: 25.00%,  total acc: 60.19%   
[EVAL] batch:   54 | acc: 37.50%,  total acc: 59.77%   
[EVAL] batch:   55 | acc: 50.00%,  total acc: 59.60%   
[EVAL] batch:   56 | acc: 50.00%,  total acc: 59.43%   
[EVAL] batch:   57 | acc: 81.25%,  total acc: 59.81%   
[EVAL] batch:   58 | acc: 100.00%,  total acc: 60.49%   
[EVAL] batch:   59 | acc: 93.75%,  total acc: 61.04%   
[EVAL] batch:   60 | acc: 25.00%,  total acc: 60.45%   
[EVAL] batch:   61 | acc: 0.00%,  total acc: 59.48%   
[EVAL] batch:   62 | acc: 12.50%,  total acc: 58.73%   
[EVAL] batch:   63 | acc: 18.75%,  total acc: 58.11%   
[EVAL] batch:   64 | acc: 56.25%,  total acc: 58.08%   
[EVAL] batch:   65 | acc: 56.25%,  total acc: 58.05%   
[EVAL] batch:   66 | acc: 18.75%,  total acc: 57.46%   
[EVAL] batch:   67 | acc: 12.50%,  total acc: 56.80%   
[EVAL] batch:   68 | acc: 25.00%,  total acc: 56.34%   
[EVAL] batch:   69 | acc: 43.75%,  total acc: 56.16%   
[EVAL] batch:   70 | acc: 37.50%,  total acc: 55.90%   
[EVAL] batch:   71 | acc: 43.75%,  total acc: 55.73%   
[EVAL] batch:   72 | acc: 43.75%,  total acc: 55.57%   
[EVAL] batch:   73 | acc: 31.25%,  total acc: 55.24%   
[EVAL] batch:   74 | acc: 43.75%,  total acc: 55.08%   
[EVAL] batch:   75 | acc: 37.50%,  total acc: 54.85%   
[EVAL] batch:   76 | acc: 37.50%,  total acc: 54.63%   
[EVAL] batch:   77 | acc: 43.75%,  total acc: 54.49%   
[EVAL] batch:   78 | acc: 31.25%,  total acc: 54.19%   
[EVAL] batch:   79 | acc: 75.00%,  total acc: 54.45%   
[EVAL] batch:   80 | acc: 75.00%,  total acc: 54.71%   
[EVAL] batch:   81 | acc: 62.50%,  total acc: 54.80%   
[EVAL] batch:   82 | acc: 75.00%,  total acc: 55.05%   
[EVAL] batch:   83 | acc: 50.00%,  total acc: 54.99%   
[EVAL] batch:   84 | acc: 18.75%,  total acc: 54.56%   
[EVAL] batch:   85 | acc: 18.75%,  total acc: 54.14%   
[EVAL] batch:   86 | acc: 12.50%,  total acc: 53.66%   
[EVAL] batch:   87 | acc: 87.50%,  total acc: 54.05%   
[EVAL] batch:   88 | acc: 100.00%,  total acc: 54.56%   
[EVAL] batch:   89 | acc: 100.00%,  total acc: 55.07%   
[EVAL] batch:   90 | acc: 100.00%,  total acc: 55.56%   
[EVAL] batch:   91 | acc: 100.00%,  total acc: 56.05%   
[EVAL] batch:   92 | acc: 100.00%,  total acc: 56.52%   
[EVAL] batch:   93 | acc: 100.00%,  total acc: 56.98%   
[EVAL] batch:   94 | acc: 100.00%,  total acc: 57.43%   
[EVAL] batch:   95 | acc: 81.25%,  total acc: 57.68%   
[EVAL] batch:   96 | acc: 6.25%,  total acc: 57.15%   
[EVAL] batch:   97 | acc: 18.75%,  total acc: 56.76%   
[EVAL] batch:   98 | acc: 81.25%,  total acc: 57.01%   
[EVAL] batch:   99 | acc: 68.75%,  total acc: 57.12%   
[EVAL] batch:  100 | acc: 75.00%,  total acc: 57.30%   
[EVAL] batch:  101 | acc: 50.00%,  total acc: 57.23%   
[EVAL] batch:  102 | acc: 50.00%,  total acc: 57.16%   
[EVAL] batch:  103 | acc: 25.00%,  total acc: 56.85%   
[EVAL] batch:  104 | acc: 56.25%,  total acc: 56.85%   
[EVAL] batch:  105 | acc: 37.50%,  total acc: 56.66%   
[EVAL] batch:  106 | acc: 87.50%,  total acc: 56.95%   
[EVAL] batch:  107 | acc: 93.75%,  total acc: 57.29%   
[EVAL] batch:  108 | acc: 87.50%,  total acc: 57.57%   
[EVAL] batch:  109 | acc: 87.50%,  total acc: 57.84%   
[EVAL] batch:  110 | acc: 81.25%,  total acc: 58.05%   
[EVAL] batch:  111 | acc: 12.50%,  total acc: 57.65%   
[EVAL] batch:  112 | acc: 0.00%,  total acc: 57.13%   
[EVAL] batch:  113 | acc: 0.00%,  total acc: 56.63%   
[EVAL] batch:  114 | acc: 25.00%,  total acc: 56.36%   
[EVAL] batch:  115 | acc: 43.75%,  total acc: 56.25%   
[EVAL] batch:  116 | acc: 87.50%,  total acc: 56.52%   
[EVAL] batch:  117 | acc: 75.00%,  total acc: 56.67%   
[EVAL] batch:  118 | acc: 75.00%,  total acc: 56.83%   
[EVAL] batch:  119 | acc: 68.75%,  total acc: 56.93%   
[EVAL] batch:  120 | acc: 81.25%,  total acc: 57.13%   
[EVAL] batch:  121 | acc: 75.00%,  total acc: 57.27%   
[EVAL] batch:  122 | acc: 100.00%,  total acc: 57.62%   
[EVAL] batch:  123 | acc: 75.00%,  total acc: 57.76%   
[EVAL] batch:  124 | acc: 81.25%,  total acc: 57.95%   
[EVAL] batch:  125 | acc: 68.75%,  total acc: 58.04%   
[EVAL] batch:  126 | acc: 81.25%,  total acc: 58.22%   
[EVAL] batch:  127 | acc: 100.00%,  total acc: 58.54%   
[EVAL] batch:  128 | acc: 93.75%,  total acc: 58.82%   
[EVAL] batch:  129 | acc: 100.00%,  total acc: 59.13%   
[EVAL] batch:  130 | acc: 100.00%,  total acc: 59.45%   
[EVAL] batch:  131 | acc: 100.00%,  total acc: 59.75%   
[EVAL] batch:  132 | acc: 62.50%,  total acc: 59.77%   
cur_acc:  ['0.8561', '0.8393', '0.3984', '0.6442', '0.3722', '0.8304', '0.5625', '0.8160']
his_acc:  ['0.8561', '0.8391', '0.7338', '0.6695', '0.5845', '0.6423', '0.6045', '0.5977']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.1943588
CurrentTrain: epoch  0, batch     1 | loss: 13.2088938
CurrentTrain: epoch  0, batch     2 | loss: 12.8815546
CurrentTrain: epoch  0, batch     3 | loss: 12.6850529
CurrentTrain: epoch  0, batch     4 | loss: 12.4739094
CurrentTrain: epoch  0, batch     5 | loss: 12.4463577
CurrentTrain: epoch  0, batch     6 | loss: 12.6172466
CurrentTrain: epoch  0, batch     7 | loss: 12.1328678
CurrentTrain: epoch  0, batch     8 | loss: 12.3931408
CurrentTrain: epoch  0, batch     9 | loss: 11.9761553
CurrentTrain: epoch  0, batch    10 | loss: 12.0636826
CurrentTrain: epoch  0, batch    11 | loss: 11.9981861
CurrentTrain: epoch  0, batch    12 | loss: 11.8976021
CurrentTrain: epoch  0, batch    13 | loss: 11.3950558
CurrentTrain: epoch  0, batch    14 | loss: 11.8494282
CurrentTrain: epoch  0, batch    15 | loss: 11.4480581
CurrentTrain: epoch  0, batch    16 | loss: 11.1259785
CurrentTrain: epoch  0, batch    17 | loss: 11.4710855
CurrentTrain: epoch  0, batch    18 | loss: 11.1451845
CurrentTrain: epoch  0, batch    19 | loss: 11.1051579
CurrentTrain: epoch  0, batch    20 | loss: 11.0872869
CurrentTrain: epoch  0, batch    21 | loss: 11.0069695
CurrentTrain: epoch  0, batch    22 | loss: 11.4089756
CurrentTrain: epoch  0, batch    23 | loss: 10.6106968
CurrentTrain: epoch  0, batch    24 | loss: 11.6539888
CurrentTrain: epoch  0, batch    25 | loss: 10.4477234
CurrentTrain: epoch  0, batch    26 | loss: 11.2612429
CurrentTrain: epoch  0, batch    27 | loss: 10.8491697
CurrentTrain: epoch  0, batch    28 | loss: 11.1536789
CurrentTrain: epoch  0, batch    29 | loss: 10.7520542
CurrentTrain: epoch  0, batch    30 | loss: 10.7770996
CurrentTrain: epoch  0, batch    31 | loss: 10.7930412
CurrentTrain: epoch  0, batch    32 | loss: 10.6465664
CurrentTrain: epoch  0, batch    33 | loss: 10.7123947
CurrentTrain: epoch  0, batch    34 | loss: 10.6794901
CurrentTrain: epoch  0, batch    35 | loss: 10.3989420
CurrentTrain: epoch  0, batch    36 | loss: 10.3247290
CurrentTrain: epoch  0, batch    37 | loss: 10.7069340
CurrentTrain: epoch  1, batch     0 | loss: 10.2764511
CurrentTrain: epoch  1, batch     1 | loss: 10.4736252
CurrentTrain: epoch  1, batch     2 | loss: 9.4967442
CurrentTrain: epoch  1, batch     3 | loss: 10.0857630
CurrentTrain: epoch  1, batch     4 | loss: 10.3534889
CurrentTrain: epoch  1, batch     5 | loss: 9.3814745
CurrentTrain: epoch  1, batch     6 | loss: 9.9117756
CurrentTrain: epoch  1, batch     7 | loss: 9.8061829
CurrentTrain: epoch  1, batch     8 | loss: 10.3087311
CurrentTrain: epoch  1, batch     9 | loss: 9.7319374
CurrentTrain: epoch  1, batch    10 | loss: 10.2060909
CurrentTrain: epoch  1, batch    11 | loss: 9.7425213
CurrentTrain: epoch  1, batch    12 | loss: 9.2131910
CurrentTrain: epoch  1, batch    13 | loss: 9.1809015
CurrentTrain: epoch  1, batch    14 | loss: 9.4104490
CurrentTrain: epoch  1, batch    15 | loss: 8.6542645
CurrentTrain: epoch  1, batch    16 | loss: 9.3987408
CurrentTrain: epoch  1, batch    17 | loss: 8.5840645
CurrentTrain: epoch  1, batch    18 | loss: 9.7110949
CurrentTrain: epoch  1, batch    19 | loss: 8.8110647
CurrentTrain: epoch  1, batch    20 | loss: 9.1412792
CurrentTrain: epoch  1, batch    21 | loss: 9.0588856
CurrentTrain: epoch  1, batch    22 | loss: 9.2621880
CurrentTrain: epoch  1, batch    23 | loss: 9.2009201
CurrentTrain: epoch  1, batch    24 | loss: 8.9609880
CurrentTrain: epoch  1, batch    25 | loss: 8.6792355
CurrentTrain: epoch  1, batch    26 | loss: 8.7032413
CurrentTrain: epoch  1, batch    27 | loss: 8.7165871
CurrentTrain: epoch  1, batch    28 | loss: 9.5203495
CurrentTrain: epoch  1, batch    29 | loss: 9.7717237
CurrentTrain: epoch  1, batch    30 | loss: 9.2936230
CurrentTrain: epoch  1, batch    31 | loss: 8.8000355
CurrentTrain: epoch  1, batch    32 | loss: 8.4560137
CurrentTrain: epoch  1, batch    33 | loss: 8.3669434
CurrentTrain: epoch  1, batch    34 | loss: 8.3704643
CurrentTrain: epoch  1, batch    35 | loss: 8.2066135
CurrentTrain: epoch  1, batch    36 | loss: 8.6126060
CurrentTrain: epoch  1, batch    37 | loss: 9.1527586
CurrentTrain: epoch  2, batch     0 | loss: 8.1082134
CurrentTrain: epoch  2, batch     1 | loss: 8.2614460
CurrentTrain: epoch  2, batch     2 | loss: 9.1509523
CurrentTrain: epoch  2, batch     3 | loss: 7.5875092
CurrentTrain: epoch  2, batch     4 | loss: 7.4240284
CurrentTrain: epoch  2, batch     5 | loss: 8.8470430
CurrentTrain: epoch  2, batch     6 | loss: 8.3282127
CurrentTrain: epoch  2, batch     7 | loss: 8.7223930
CurrentTrain: epoch  2, batch     8 | loss: 8.3519268
CurrentTrain: epoch  2, batch     9 | loss: 8.4723110
CurrentTrain: epoch  2, batch    10 | loss: 8.3432894
CurrentTrain: epoch  2, batch    11 | loss: 7.9660215
CurrentTrain: epoch  2, batch    12 | loss: 7.6403618
CurrentTrain: epoch  2, batch    13 | loss: 7.9003654
CurrentTrain: epoch  2, batch    14 | loss: 8.5508432
CurrentTrain: epoch  2, batch    15 | loss: 8.1521225
CurrentTrain: epoch  2, batch    16 | loss: 8.0694485
CurrentTrain: epoch  2, batch    17 | loss: 8.2203865
CurrentTrain: epoch  2, batch    18 | loss: 8.1662912
CurrentTrain: epoch  2, batch    19 | loss: 8.0654516
CurrentTrain: epoch  2, batch    20 | loss: 7.9494824
CurrentTrain: epoch  2, batch    21 | loss: 7.6826963
CurrentTrain: epoch  2, batch    22 | loss: 8.5339432
CurrentTrain: epoch  2, batch    23 | loss: 7.6894817
CurrentTrain: epoch  2, batch    24 | loss: 7.6159887
CurrentTrain: epoch  2, batch    25 | loss: 7.6986179
CurrentTrain: epoch  2, batch    26 | loss: 7.5964770
CurrentTrain: epoch  2, batch    27 | loss: 7.7692814
CurrentTrain: epoch  2, batch    28 | loss: 7.4057713
CurrentTrain: epoch  2, batch    29 | loss: 7.8754148
CurrentTrain: epoch  2, batch    30 | loss: 6.8914270
CurrentTrain: epoch  2, batch    31 | loss: 7.2671661
CurrentTrain: epoch  2, batch    32 | loss: 7.2690945
CurrentTrain: epoch  2, batch    33 | loss: 7.2840443
CurrentTrain: epoch  2, batch    34 | loss: 8.0254192
CurrentTrain: epoch  2, batch    35 | loss: 7.1783285
CurrentTrain: epoch  2, batch    36 | loss: 8.0732136
CurrentTrain: epoch  2, batch    37 | loss: 8.1862164
CurrentTrain: epoch  3, batch     0 | loss: 7.9546437
CurrentTrain: epoch  3, batch     1 | loss: 7.2877026
CurrentTrain: epoch  3, batch     2 | loss: 7.9970784
CurrentTrain: epoch  3, batch     3 | loss: 7.4446106
CurrentTrain: epoch  3, batch     4 | loss: 7.0291862
CurrentTrain: epoch  3, batch     5 | loss: 6.6525726
CurrentTrain: epoch  3, batch     6 | loss: 7.4990187
CurrentTrain: epoch  3, batch     7 | loss: 6.4209676
CurrentTrain: epoch  3, batch     8 | loss: 7.5921178
CurrentTrain: epoch  3, batch     9 | loss: 6.9922342
CurrentTrain: epoch  3, batch    10 | loss: 7.3974099
CurrentTrain: epoch  3, batch    11 | loss: 6.4548388
CurrentTrain: epoch  3, batch    12 | loss: 6.9090910
CurrentTrain: epoch  3, batch    13 | loss: 6.7095308
CurrentTrain: epoch  3, batch    14 | loss: 7.9558945
CurrentTrain: epoch  3, batch    15 | loss: 6.9631071
CurrentTrain: epoch  3, batch    16 | loss: 7.4660358
CurrentTrain: epoch  3, batch    17 | loss: 6.2792454
CurrentTrain: epoch  3, batch    18 | loss: 6.6290321
CurrentTrain: epoch  3, batch    19 | loss: 6.9395127
CurrentTrain: epoch  3, batch    20 | loss: 6.2139606
CurrentTrain: epoch  3, batch    21 | loss: 7.5120702
CurrentTrain: epoch  3, batch    22 | loss: 7.1330490
CurrentTrain: epoch  3, batch    23 | loss: 7.0268698
CurrentTrain: epoch  3, batch    24 | loss: 8.2819605
CurrentTrain: epoch  3, batch    25 | loss: 6.9523067
CurrentTrain: epoch  3, batch    26 | loss: 7.1925068
CurrentTrain: epoch  3, batch    27 | loss: 6.0926781
CurrentTrain: epoch  3, batch    28 | loss: 9.0865459
CurrentTrain: epoch  3, batch    29 | loss: 8.0795460
CurrentTrain: epoch  3, batch    30 | loss: 7.2525835
CurrentTrain: epoch  3, batch    31 | loss: 7.1508827
CurrentTrain: epoch  3, batch    32 | loss: 7.7219224
CurrentTrain: epoch  3, batch    33 | loss: 8.1320381
CurrentTrain: epoch  3, batch    34 | loss: 7.6344671
CurrentTrain: epoch  3, batch    35 | loss: 6.6497030
CurrentTrain: epoch  3, batch    36 | loss: 6.9474182
CurrentTrain: epoch  3, batch    37 | loss: 5.9384108
CurrentTrain: epoch  4, batch     0 | loss: 6.7901149
CurrentTrain: epoch  4, batch     1 | loss: 6.9960742
CurrentTrain: epoch  4, batch     2 | loss: 6.7222271
CurrentTrain: epoch  4, batch     3 | loss: 6.8582888
CurrentTrain: epoch  4, batch     4 | loss: 6.8061376
CurrentTrain: epoch  4, batch     5 | loss: 7.3289766
CurrentTrain: epoch  4, batch     6 | loss: 6.5014391
CurrentTrain: epoch  4, batch     7 | loss: 7.0150108
CurrentTrain: epoch  4, batch     8 | loss: 6.9735193
CurrentTrain: epoch  4, batch     9 | loss: 7.2588301
CurrentTrain: epoch  4, batch    10 | loss: 5.3513222
CurrentTrain: epoch  4, batch    11 | loss: 7.0670958
CurrentTrain: epoch  4, batch    12 | loss: 6.5934200
CurrentTrain: epoch  4, batch    13 | loss: 6.5216808
CurrentTrain: epoch  4, batch    14 | loss: 7.1111794
CurrentTrain: epoch  4, batch    15 | loss: 7.1419811
CurrentTrain: epoch  4, batch    16 | loss: 7.4712486
CurrentTrain: epoch  4, batch    17 | loss: 6.9691925
CurrentTrain: epoch  4, batch    18 | loss: 7.6137533
CurrentTrain: epoch  4, batch    19 | loss: 5.9929132
CurrentTrain: epoch  4, batch    20 | loss: 6.4351444
CurrentTrain: epoch  4, batch    21 | loss: 6.3034849
CurrentTrain: epoch  4, batch    22 | loss: 7.3110986
CurrentTrain: epoch  4, batch    23 | loss: 6.0462046
CurrentTrain: epoch  4, batch    24 | loss: 6.7858591
CurrentTrain: epoch  4, batch    25 | loss: 6.6943617
CurrentTrain: epoch  4, batch    26 | loss: 7.4592152
CurrentTrain: epoch  4, batch    27 | loss: 6.3484449
CurrentTrain: epoch  4, batch    28 | loss: 6.6056175
CurrentTrain: epoch  4, batch    29 | loss: 6.1229277
CurrentTrain: epoch  4, batch    30 | loss: 6.4041443
CurrentTrain: epoch  4, batch    31 | loss: 7.1103415
CurrentTrain: epoch  4, batch    32 | loss: 7.5737567
CurrentTrain: epoch  4, batch    33 | loss: 7.6669960
CurrentTrain: epoch  4, batch    34 | loss: 5.9630947
CurrentTrain: epoch  4, batch    35 | loss: 5.9803524
CurrentTrain: epoch  4, batch    36 | loss: 6.6243453
CurrentTrain: epoch  4, batch    37 | loss: 7.9637384
CurrentTrain: epoch  5, batch     0 | loss: 5.9035664
CurrentTrain: epoch  5, batch     1 | loss: 7.3873291
CurrentTrain: epoch  5, batch     2 | loss: 6.3781805
CurrentTrain: epoch  5, batch     3 | loss: 6.8431830
CurrentTrain: epoch  5, batch     4 | loss: 6.7556953
CurrentTrain: epoch  5, batch     5 | loss: 6.5214844
CurrentTrain: epoch  5, batch     6 | loss: 6.2639437
CurrentTrain: epoch  5, batch     7 | loss: 6.0010357
CurrentTrain: epoch  5, batch     8 | loss: 6.6454115
CurrentTrain: epoch  5, batch     9 | loss: 6.7636003
CurrentTrain: epoch  5, batch    10 | loss: 6.3371668
CurrentTrain: epoch  5, batch    11 | loss: 6.5448155
CurrentTrain: epoch  5, batch    12 | loss: 6.0678644
CurrentTrain: epoch  5, batch    13 | loss: 6.1826930
CurrentTrain: epoch  5, batch    14 | loss: 6.0142260
CurrentTrain: epoch  5, batch    15 | loss: 6.6133156
CurrentTrain: epoch  5, batch    16 | loss: 6.3967757
CurrentTrain: epoch  5, batch    17 | loss: 7.6193900
CurrentTrain: epoch  5, batch    18 | loss: 6.4201765
CurrentTrain: epoch  5, batch    19 | loss: 6.1679010
CurrentTrain: epoch  5, batch    20 | loss: 6.0216274
CurrentTrain: epoch  5, batch    21 | loss: 6.6595297
CurrentTrain: epoch  5, batch    22 | loss: 6.9957123
CurrentTrain: epoch  5, batch    23 | loss: 6.3549967
CurrentTrain: epoch  5, batch    24 | loss: 5.8459797
CurrentTrain: epoch  5, batch    25 | loss: 6.8124933
CurrentTrain: epoch  5, batch    26 | loss: 6.4026117
CurrentTrain: epoch  5, batch    27 | loss: 6.4751110
CurrentTrain: epoch  5, batch    28 | loss: 6.5499358
CurrentTrain: epoch  5, batch    29 | loss: 6.4225330
CurrentTrain: epoch  5, batch    30 | loss: 7.2179103
CurrentTrain: epoch  5, batch    31 | loss: 7.1514368
CurrentTrain: epoch  5, batch    32 | loss: 6.4222989
CurrentTrain: epoch  5, batch    33 | loss: 6.0510464
CurrentTrain: epoch  5, batch    34 | loss: 6.5408106
CurrentTrain: epoch  5, batch    35 | loss: 6.3584833
CurrentTrain: epoch  5, batch    36 | loss: 6.6312151
CurrentTrain: epoch  5, batch    37 | loss: 6.9012480
CurrentTrain: epoch  6, batch     0 | loss: 6.4432383
CurrentTrain: epoch  6, batch     1 | loss: 6.4953399
CurrentTrain: epoch  6, batch     2 | loss: 6.0199809
CurrentTrain: epoch  6, batch     3 | loss: 5.7249851
CurrentTrain: epoch  6, batch     4 | loss: 6.5479321
CurrentTrain: epoch  6, batch     5 | loss: 6.2201605
CurrentTrain: epoch  6, batch     6 | loss: 6.6595335
CurrentTrain: epoch  6, batch     7 | loss: 6.0528650
CurrentTrain: epoch  6, batch     8 | loss: 5.5830579
CurrentTrain: epoch  6, batch     9 | loss: 6.1461377
CurrentTrain: epoch  6, batch    10 | loss: 5.7532339
CurrentTrain: epoch  6, batch    11 | loss: 5.8715668
CurrentTrain: epoch  6, batch    12 | loss: 5.9587536
CurrentTrain: epoch  6, batch    13 | loss: 6.0107718
CurrentTrain: epoch  6, batch    14 | loss: 6.6173916
CurrentTrain: epoch  6, batch    15 | loss: 5.9962645
CurrentTrain: epoch  6, batch    16 | loss: 6.3681145
CurrentTrain: epoch  6, batch    17 | loss: 6.0727882
CurrentTrain: epoch  6, batch    18 | loss: 5.8564773
CurrentTrain: epoch  6, batch    19 | loss: 5.4773540
CurrentTrain: epoch  6, batch    20 | loss: 6.0479622
CurrentTrain: epoch  6, batch    21 | loss: 5.8990073
CurrentTrain: epoch  6, batch    22 | loss: 5.7201333
CurrentTrain: epoch  6, batch    23 | loss: 6.0614991
CurrentTrain: epoch  6, batch    24 | loss: 5.6250887
CurrentTrain: epoch  6, batch    25 | loss: 6.1601744
CurrentTrain: epoch  6, batch    26 | loss: 6.2105103
CurrentTrain: epoch  6, batch    27 | loss: 5.7182193
CurrentTrain: epoch  6, batch    28 | loss: 5.8999848
CurrentTrain: epoch  6, batch    29 | loss: 7.3727307
CurrentTrain: epoch  6, batch    30 | loss: 6.2102671
CurrentTrain: epoch  6, batch    31 | loss: 6.3823714
CurrentTrain: epoch  6, batch    32 | loss: 5.9619150
CurrentTrain: epoch  6, batch    33 | loss: 5.7411571
CurrentTrain: epoch  6, batch    34 | loss: 5.5999422
CurrentTrain: epoch  6, batch    35 | loss: 5.8891635
CurrentTrain: epoch  6, batch    36 | loss: 5.9912972
CurrentTrain: epoch  6, batch    37 | loss: 5.4460158
CurrentTrain: epoch  7, batch     0 | loss: 5.9580221
CurrentTrain: epoch  7, batch     1 | loss: 6.3048162
CurrentTrain: epoch  7, batch     2 | loss: 5.9945335
CurrentTrain: epoch  7, batch     3 | loss: 5.8708544
CurrentTrain: epoch  7, batch     4 | loss: 5.3722081
CurrentTrain: epoch  7, batch     5 | loss: 5.6140642
CurrentTrain: epoch  7, batch     6 | loss: 5.3224893
CurrentTrain: epoch  7, batch     7 | loss: 6.3612957
CurrentTrain: epoch  7, batch     8 | loss: 5.6609125
CurrentTrain: epoch  7, batch     9 | loss: 6.2040539
CurrentTrain: epoch  7, batch    10 | loss: 6.1773643
CurrentTrain: epoch  7, batch    11 | loss: 5.6818066
CurrentTrain: epoch  7, batch    12 | loss: 6.2416630
CurrentTrain: epoch  7, batch    13 | loss: 5.3381705
CurrentTrain: epoch  7, batch    14 | loss: 6.0978479
CurrentTrain: epoch  7, batch    15 | loss: 5.3825569
CurrentTrain: epoch  7, batch    16 | loss: 5.4074001
CurrentTrain: epoch  7, batch    17 | loss: 5.5951195
CurrentTrain: epoch  7, batch    18 | loss: 5.8204141
CurrentTrain: epoch  7, batch    19 | loss: 5.4897680
CurrentTrain: epoch  7, batch    20 | loss: 6.2523665
CurrentTrain: epoch  7, batch    21 | loss: 5.4221048
CurrentTrain: epoch  7, batch    22 | loss: 5.3579435
CurrentTrain: epoch  7, batch    23 | loss: 5.3069491
CurrentTrain: epoch  7, batch    24 | loss: 5.7807736
CurrentTrain: epoch  7, batch    25 | loss: 5.4470739
CurrentTrain: epoch  7, batch    26 | loss: 5.2329149
CurrentTrain: epoch  7, batch    27 | loss: 5.3400450
CurrentTrain: epoch  7, batch    28 | loss: 5.6766987
CurrentTrain: epoch  7, batch    29 | loss: 5.7698846
CurrentTrain: epoch  7, batch    30 | loss: 5.2591238
CurrentTrain: epoch  7, batch    31 | loss: 5.2680817
CurrentTrain: epoch  7, batch    32 | loss: 5.1444907
CurrentTrain: epoch  7, batch    33 | loss: 5.5809698
CurrentTrain: epoch  7, batch    34 | loss: 6.1813221
CurrentTrain: epoch  7, batch    35 | loss: 5.3708534
CurrentTrain: epoch  7, batch    36 | loss: 5.3746939
CurrentTrain: epoch  7, batch    37 | loss: 5.3499041
CurrentTrain: epoch  8, batch     0 | loss: 5.6608419
CurrentTrain: epoch  8, batch     1 | loss: 5.4576254
CurrentTrain: epoch  8, batch     2 | loss: 5.4722052
CurrentTrain: epoch  8, batch     3 | loss: 5.3841171
CurrentTrain: epoch  8, batch     4 | loss: 5.1656351
CurrentTrain: epoch  8, batch     5 | loss: 5.4384680
CurrentTrain: epoch  8, batch     6 | loss: 5.1938438
CurrentTrain: epoch  8, batch     7 | loss: 5.3246803
CurrentTrain: epoch  8, batch     8 | loss: 5.3956347
CurrentTrain: epoch  8, batch     9 | loss: 5.2195168
CurrentTrain: epoch  8, batch    10 | loss: 5.2889485
CurrentTrain: epoch  8, batch    11 | loss: 5.2599912
CurrentTrain: epoch  8, batch    12 | loss: 5.1506338
CurrentTrain: epoch  8, batch    13 | loss: 5.4145365
CurrentTrain: epoch  8, batch    14 | loss: 5.1465569
CurrentTrain: epoch  8, batch    15 | loss: 5.3299317
CurrentTrain: epoch  8, batch    16 | loss: 5.2073445
CurrentTrain: epoch  8, batch    17 | loss: 5.4630871
CurrentTrain: epoch  8, batch    18 | loss: 5.1989079
CurrentTrain: epoch  8, batch    19 | loss: 6.0125484
CurrentTrain: epoch  8, batch    20 | loss: 5.3242817
CurrentTrain: epoch  8, batch    21 | loss: 5.4625287
CurrentTrain: epoch  8, batch    22 | loss: 5.0721025
CurrentTrain: epoch  8, batch    23 | loss: 5.3782845
CurrentTrain: epoch  8, batch    24 | loss: 4.9598799
CurrentTrain: epoch  8, batch    25 | loss: 5.1527801
CurrentTrain: epoch  8, batch    26 | loss: 5.3253965
CurrentTrain: epoch  8, batch    27 | loss: 5.2001982
CurrentTrain: epoch  8, batch    28 | loss: 5.1727223
CurrentTrain: epoch  8, batch    29 | loss: 5.3126040
CurrentTrain: epoch  8, batch    30 | loss: 5.2276707
CurrentTrain: epoch  8, batch    31 | loss: 5.0546865
CurrentTrain: epoch  8, batch    32 | loss: 5.1252341
CurrentTrain: epoch  8, batch    33 | loss: 5.0179505
CurrentTrain: epoch  8, batch    34 | loss: 5.6779251
CurrentTrain: epoch  8, batch    35 | loss: 5.1129875
CurrentTrain: epoch  8, batch    36 | loss: 5.0308352
CurrentTrain: epoch  8, batch    37 | loss: 5.1040154
CurrentTrain: epoch  9, batch     0 | loss: 4.9476042
CurrentTrain: epoch  9, batch     1 | loss: 5.0776796
CurrentTrain: epoch  9, batch     2 | loss: 5.0328798
CurrentTrain: epoch  9, batch     3 | loss: 5.9188895
CurrentTrain: epoch  9, batch     4 | loss: 5.5458746
CurrentTrain: epoch  9, batch     5 | loss: 5.5257835
CurrentTrain: epoch  9, batch     6 | loss: 5.4402986
CurrentTrain: epoch  9, batch     7 | loss: 5.2466526
CurrentTrain: epoch  9, batch     8 | loss: 5.0423665
CurrentTrain: epoch  9, batch     9 | loss: 5.2987370
CurrentTrain: epoch  9, batch    10 | loss: 5.2391086
CurrentTrain: epoch  9, batch    11 | loss: 5.1336136
CurrentTrain: epoch  9, batch    12 | loss: 5.1255417
CurrentTrain: epoch  9, batch    13 | loss: 4.9994564
CurrentTrain: epoch  9, batch    14 | loss: 5.1288466
CurrentTrain: epoch  9, batch    15 | loss: 5.3099442
CurrentTrain: epoch  9, batch    16 | loss: 5.2933183
CurrentTrain: epoch  9, batch    17 | loss: 5.2402430
CurrentTrain: epoch  9, batch    18 | loss: 4.9082875
CurrentTrain: epoch  9, batch    19 | loss: 5.1101813
CurrentTrain: epoch  9, batch    20 | loss: 4.9875288
CurrentTrain: epoch  9, batch    21 | loss: 5.0117521
CurrentTrain: epoch  9, batch    22 | loss: 5.1873841
CurrentTrain: epoch  9, batch    23 | loss: 5.0057993
CurrentTrain: epoch  9, batch    24 | loss: 5.1164746
CurrentTrain: epoch  9, batch    25 | loss: 5.0237622
CurrentTrain: epoch  9, batch    26 | loss: 4.9649959
CurrentTrain: epoch  9, batch    27 | loss: 5.0545168
CurrentTrain: epoch  9, batch    28 | loss: 5.0579734
CurrentTrain: epoch  9, batch    29 | loss: 4.9949794
CurrentTrain: epoch  9, batch    30 | loss: 4.9334202
CurrentTrain: epoch  9, batch    31 | loss: 4.9099369
CurrentTrain: epoch  9, batch    32 | loss: 5.4690638
CurrentTrain: epoch  9, batch    33 | loss: 5.5053349
CurrentTrain: epoch  9, batch    34 | loss: 5.0780420
CurrentTrain: epoch  9, batch    35 | loss: 4.9339762
CurrentTrain: epoch  9, batch    36 | loss: 4.9409065
CurrentTrain: epoch  9, batch    37 | loss: 5.0055561
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: franz vranitzky was the first austrian chancellor to visit israel , in 1993 , in a tentative thawing of relations after the six-year term of kurt waldheim , who served during world war ii as a junior army officer in a nazi army unit in yugoslavia .
Head Entity: kurt waldheim
Tail Entity: yugoslavia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After spending several years in the United States, Maria moved back to her hometown in Italy, where she felt a deep connection to her roots.  
Head Entity: Maria  
Tail Entity: Italy  

Relation: person countries of residence  
Context: During his time as a diplomat, John frequently traveled to various countries, but he eventually settled in Canada, where he found a sense of belonging.  
Head Entity: John  
Tail Entity: Canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: as for any gay or lesbian people who enlisted with the military this week , `` they may be told they can not join '' if the pentagon decides to enforce the law , said aubrey sarvis , executive director of the servicemembers legal defense network , a group representing service members affected by the law .
Head Entity: servicemembers legal defense network
Tail Entity: aubrey sarvis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: The board of directors at Tech Innovations Inc. recently appointed Sarah Johnson as the new Chief Technology Officer, highlighting her extensive experience in the tech industry and her leadership skills.  
Head Entity: Tech Innovations Inc.  
Tail Entity: Sarah Johnson  

Relation: organization top members employees  
Context: During the annual meeting, the CEO of Green Earth Solutions, Mark Thompson, announced the promotion of Lisa Chen to Vice President of Sustainability, recognizing her contributions to the company's environmental initiatives.  
Head Entity: Green Earth Solutions  
Tail Entity: Lisa Chen  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: ipsco shares rose $ 1506 , or 12 percent , to close at $ 14596 on the new york stock exchange .
Head Entity: ipsco
Tail Entity: new york stock exchange
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The American Medical Association has announced its new initiatives to improve healthcare access across the country.  
Head Entity: American Medical Association  
Tail Entity: healthcare access  

Relation: organization member of  
Context: The United Nations has launched a new program aimed at promoting sustainable development in member countries.  
Head Entity: United Nations  
Tail Entity: sustainable development
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: the former detainee , murat kurnaz , was turned over to german authorities and freed in 2006 after a personal plea from german chancellor angela merkel .
Head Entity: murat kurnaz
Tail Entity: german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist, albert einstein, was born in ulm, in the kingdom of württemberg in the german empire.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author, chimamanda ngozi adichie, hails from enugu, nigeria, where she spent her early years before moving to the united states.  
Head Entity: chimamanda ngozi adichie  
Tail Entity: nigerian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving the company's financial strategy."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the annual conference, Dr. James Lee, the lead researcher at the National Institute of Health, presented his findings on the latest advancements in medical technology."  
Head Entity: Dr. James Lee  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: delays and cancellations for argentina 's leading airline aerolineas argentinas -lrb- aa -rrb- continued on sunday , as a baggage handlers ' strike went into its fourth day .
Head Entity: aerolineas argentinas
Tail Entity: argentina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the tech giant apple inc. announced plans to expand its operations in ireland, taking advantage of the favorable corporate tax rates.  
Head Entity: apple inc.  
Tail Entity: ireland  

Relation: organization country of headquarters  
Context: the renowned automotive manufacturer toyota has established its main production facility in japan, contributing significantly to the local economy.  
Head Entity: toyota  
Tail Entity: japan  
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   
[EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   
[EVAL] batch:    2 | acc: 75.00%,  total acc: 62.50%   
[EVAL] batch:    3 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 67.86%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 71.88%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 75.00%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 76.88%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 78.98%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 80.21%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 81.25%   
[EVAL] batch:   13 | acc: 75.00%,  total acc: 80.80%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 80.42%   
[EVAL] batch:   15 | acc: 68.75%,  total acc: 79.69%   
[EVAL] batch:   16 | acc: 81.25%,  total acc: 79.78%   
[EVAL] batch:   17 | acc: 75.00%,  total acc: 79.51%   
[EVAL] batch:   18 | acc: 87.50%,  total acc: 79.93%   
[EVAL] batch:   19 | acc: 93.75%,  total acc: 80.62%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 81.55%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 82.39%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 83.15%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 83.59%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 85.19%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 86.04%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 86.09%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 84.85%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   
[EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   
[EVAL] batch:    2 | acc: 75.00%,  total acc: 62.50%   
[EVAL] batch:    3 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 67.86%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 71.88%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 75.00%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 76.88%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 78.98%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 80.21%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 81.25%   
[EVAL] batch:   13 | acc: 75.00%,  total acc: 80.80%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 80.42%   
[EVAL] batch:   15 | acc: 68.75%,  total acc: 79.69%   
[EVAL] batch:   16 | acc: 81.25%,  total acc: 79.78%   
[EVAL] batch:   17 | acc: 75.00%,  total acc: 79.51%   
[EVAL] batch:   18 | acc: 87.50%,  total acc: 79.93%   
[EVAL] batch:   19 | acc: 93.75%,  total acc: 80.62%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 81.55%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 82.39%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 83.15%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 83.59%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 85.19%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 86.04%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 86.09%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 84.85%   
cur_acc:  ['0.8485']
his_acc:  ['0.8485']
CurrentTrain: epoch  0, batch     0 | loss: 6.2202954
CurrentTrain: epoch  0, batch     1 | loss: 6.7739248
CurrentTrain: epoch  1, batch     0 | loss: 5.9535003
CurrentTrain: epoch  1, batch     1 | loss: 5.2204075
CurrentTrain: epoch  2, batch     0 | loss: 5.3359017
CurrentTrain: epoch  2, batch     1 | loss: 5.1476007
CurrentTrain: epoch  3, batch     0 | loss: 5.1917486
CurrentTrain: epoch  3, batch     1 | loss: 4.1506629
CurrentTrain: epoch  4, batch     0 | loss: 4.1074438
CurrentTrain: epoch  4, batch     1 | loss: 4.5355144
CurrentTrain: epoch  5, batch     0 | loss: 3.8571091
CurrentTrain: epoch  5, batch     1 | loss: 4.2225075
CurrentTrain: epoch  6, batch     0 | loss: 3.6645737
CurrentTrain: epoch  6, batch     1 | loss: 4.3974676
CurrentTrain: epoch  7, batch     0 | loss: 3.6650229
CurrentTrain: epoch  7, batch     1 | loss: 3.7119534
CurrentTrain: epoch  8, batch     0 | loss: 3.8092337
CurrentTrain: epoch  8, batch     1 | loss: 2.7979381
CurrentTrain: epoch  9, batch     0 | loss: 2.8851542
CurrentTrain: epoch  9, batch     1 | loss: 3.5282679
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city of los angeles, actor chris pratt has decided to settle down in the quieter surroundings of the pacific northwest, where he can enjoy nature and spend more time with his family.  
Head Entity: chris pratt  
Tail Entity: pacific northwest  

Relation: person stateorprovinces of residence  
Context: renowned author j.k. rowling has made edinburgh her home, drawing inspiration from the city's rich history and vibrant culture for her beloved harry potter series.  
Head Entity: j.k. rowling  
Tail Entity: edinburgh  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away on July 20, 2020, after a long illness.  
Head Entity: The renowned author  
Tail Entity: July 20, 2020  

Relation: person date of death  
Context: She left this world peacefully in her sleep on March 5, 2015.  
Head Entity: She  
Tail Entity: March 5, 2015  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, the newly formed entity, Global Solutions Inc., reported an impressive count of 50,000 employees, making it one of the largest firms in the industry.  
Head Entity: Global Solutions Inc.  
Tail Entity: 50,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: the coming of the mahdi will turn the world upside down , and the oppressed shiites will finally see justice .
Head Entity: mahdi
Tail Entity: shiites
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Clemens is better known by his pen name, Mark Twain, which he used for his literary works.  
Head Entity: Samuel Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician known as Lady Gaga was born Stefani Joanne Angelina Germanotta, a name that few recognize.  
Head Entity: Lady Gaga  
Tail Entity: Stefani Joanne Angelina Germanotta  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a lavish ceremony held in new york city, the couple exchanged vows in front of family and friends: john legend and chrissy teigen celebrated their love with a beautiful wedding.  
Head Entity: john legend  
Tail Entity: chrissy teigen  

Relation: person spouse  
Context: during the annual charity gala, it was announced that the famous actor and his long-time partner have tied the knot: ben affleck and jennifer garner are now officially husband and wife.  
Head Entity: ben affleck  
Tail Entity: jennifer garner  
Mixup data size:  104
MixupTrain:  epoch  0, batch     0 | loss: 5.6334333
MixupTrain:  epoch  0, batch     1 | loss: 5.2290964
MixupTrain:  epoch  0, batch     2 | loss: 5.0422783
MixupTrain:  epoch  0, batch     3 | loss: 4.6159825
MixupTrain:  epoch  0, batch     4 | loss: 4.9458199
MixupTrain:  epoch  0, batch     5 | loss: 4.5279174
MixupTrain:  epoch  0, batch     6 | loss: 4.6741476
MemoryTrain:  epoch  0, batch     0 | loss: 3.5490894
MemoryTrain:  epoch  0, batch     1 | loss: 4.5421743
MemoryTrain:  epoch  0, batch     2 | loss: 2.8162377
MemoryTrain:  epoch  1, batch     0 | loss: 3.6872349
MemoryTrain:  epoch  1, batch     1 | loss: 3.3564048
MemoryTrain:  epoch  1, batch     2 | loss: 2.0984759
MemoryTrain:  epoch  2, batch     0 | loss: 3.0341024
MemoryTrain:  epoch  2, batch     1 | loss: 2.4757335
MemoryTrain:  epoch  2, batch     2 | loss: 3.7967424
MemoryTrain:  epoch  3, batch     0 | loss: 2.9426999
MemoryTrain:  epoch  3, batch     1 | loss: 2.6105037
MemoryTrain:  epoch  3, batch     2 | loss: 1.5650343
MemoryTrain:  epoch  4, batch     0 | loss: 2.3627801
MemoryTrain:  epoch  4, batch     1 | loss: 2.9873712
MemoryTrain:  epoch  4, batch     2 | loss: 1.1934264
MemoryTrain:  epoch  5, batch     0 | loss: 2.7126231
MemoryTrain:  epoch  5, batch     1 | loss: 2.0830047
MemoryTrain:  epoch  5, batch     2 | loss: 1.2191771
MemoryTrain:  epoch  6, batch     0 | loss: 2.1705284
MemoryTrain:  epoch  6, batch     1 | loss: 2.3196223
MemoryTrain:  epoch  6, batch     2 | loss: 2.5469992
MemoryTrain:  epoch  7, batch     0 | loss: 1.9138954
MemoryTrain:  epoch  7, batch     1 | loss: 1.9867959
MemoryTrain:  epoch  7, batch     2 | loss: 5.9754066
MemoryTrain:  epoch  8, batch     0 | loss: 1.9482541
MemoryTrain:  epoch  8, batch     1 | loss: 2.1838126
MemoryTrain:  epoch  8, batch     2 | loss: 1.2551546
MemoryTrain:  epoch  9, batch     0 | loss: 1.6836419
MemoryTrain:  epoch  9, batch     1 | loss: 2.1639721
MemoryTrain:  epoch  9, batch     2 | loss: 4.2344513
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 37.50%   
[EVAL] batch:    2 | acc: 43.75%,  total acc: 39.58%   
[EVAL] batch:    3 | acc: 75.00%,  total acc: 48.44%   
[EVAL] batch:    4 | acc: 56.25%,  total acc: 50.00%   
[EVAL] batch:    5 | acc: 93.75%,  total acc: 57.29%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 63.39%   
[EVAL] batch:    7 | acc: 93.75%,  total acc: 67.19%   
[EVAL] batch:    8 | acc: 81.25%,  total acc: 68.75%   
[EVAL] batch:    9 | acc: 56.25%,  total acc: 67.50%   
[EVAL] batch:   10 | acc: 68.75%,  total acc: 67.61%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 69.79%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 71.63%   
[EVAL] batch:   13 | acc: 100.00%,  total acc: 73.66%   
[EVAL] batch:   14 | acc: 43.75%,  total acc: 71.67%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   
[EVAL] batch:    1 | acc: 50.00%,  total acc: 59.38%   
[EVAL] batch:    2 | acc: 81.25%,  total acc: 66.67%   
[EVAL] batch:    3 | acc: 62.50%,  total acc: 65.62%   
[EVAL] batch:    4 | acc: 68.75%,  total acc: 66.25%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 66.67%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 70.54%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 74.22%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 77.08%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 78.75%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 80.68%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 81.25%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 82.21%   
[EVAL] batch:   13 | acc: 75.00%,  total acc: 81.70%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 81.25%   
[EVAL] batch:   15 | acc: 68.75%,  total acc: 80.47%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 80.15%   
[EVAL] batch:   17 | acc: 68.75%,  total acc: 79.51%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 79.61%   
[EVAL] batch:   19 | acc: 93.75%,  total acc: 80.31%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 81.25%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 82.10%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 82.88%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 83.33%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 84.00%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 84.95%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 85.83%   
[EVAL] batch:   30 | acc: 93.75%,  total acc: 86.09%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   
[EVAL] batch:   32 | acc: 56.25%,  total acc: 85.42%   
[EVAL] batch:   33 | acc: 31.25%,  total acc: 83.82%   
[EVAL] batch:   34 | acc: 56.25%,  total acc: 83.04%   
[EVAL] batch:   35 | acc: 56.25%,  total acc: 82.29%   
[EVAL] batch:   36 | acc: 68.75%,  total acc: 81.93%   
[EVAL] batch:   37 | acc: 75.00%,  total acc: 81.74%   
[EVAL] batch:   38 | acc: 100.00%,  total acc: 82.21%   
[EVAL] batch:   39 | acc: 100.00%,  total acc: 82.66%   
[EVAL] batch:   40 | acc: 87.50%,  total acc: 82.77%   
[EVAL] batch:   41 | acc: 81.25%,  total acc: 82.74%   
[EVAL] batch:   42 | acc: 31.25%,  total acc: 81.54%   
[EVAL] batch:   43 | acc: 93.75%,  total acc: 81.82%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 82.22%   
[EVAL] batch:   45 | acc: 93.75%,  total acc: 82.47%   
[EVAL] batch:   46 | acc: 81.25%,  total acc: 82.45%   
cur_acc:  ['0.8485', '0.7167']
his_acc:  ['0.8485', '0.8245']
CurrentTrain: epoch  0, batch     0 | loss: 6.9322233
CurrentTrain: epoch  0, batch     1 | loss: 7.1746116
CurrentTrain: epoch  1, batch     0 | loss: 6.6520610
CurrentTrain: epoch  1, batch     1 | loss: 4.7955966
CurrentTrain: epoch  2, batch     0 | loss: 5.3454099
CurrentTrain: epoch  2, batch     1 | loss: 5.1659436
CurrentTrain: epoch  3, batch     0 | loss: 5.1122322
CurrentTrain: epoch  3, batch     1 | loss: 4.5902863
CurrentTrain: epoch  4, batch     0 | loss: 4.8798885
CurrentTrain: epoch  4, batch     1 | loss: 4.4451795
CurrentTrain: epoch  5, batch     0 | loss: 4.2664165
CurrentTrain: epoch  5, batch     1 | loss: 3.4715059
CurrentTrain: epoch  6, batch     0 | loss: 3.6625590
CurrentTrain: epoch  6, batch     1 | loss: 3.6431563
CurrentTrain: epoch  7, batch     0 | loss: 3.5108709
CurrentTrain: epoch  7, batch     1 | loss: 3.2314904
CurrentTrain: epoch  8, batch     0 | loss: 3.6823959
CurrentTrain: epoch  8, batch     1 | loss: 2.8673375
CurrentTrain: epoch  9, batch     0 | loss: 2.9394977
CurrentTrain: epoch  9, batch     1 | loss: 2.9534678
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire, on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: germany  

Relation: person stateorprovince of birth  
Context: oprah winfrey was born on january 29, 1954, in kosciusko, mississippi.  
Head Entity: oprah winfrey  
Tail Entity: mississippi  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Sample 1:  
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, who had always been a guiding force in her life, to her friends.  
Head Entity: her father  
Tail Entity: Sarah  

Sample 2:  
Relation: person parents  
Context: After the ceremony, Emily shared stories about her mother, who had inspired her to pursue a career in medicine.  
Head Entity: her mother  
Tail Entity: Emily  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst in the investment division.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in los angeles, ca.  
Head Entity: john doe  
Tail Entity: ca.  

Relation: person stateorprovince of death  
Context: after a long battle with illness, mary jane, a beloved community leader, succumbed to her condition in a hospital in phoenix, az.  
Head Entity: mary jane  
Tail Entity: az.  
Mixup data size:  134
MixupTrain:  epoch  0, batch     0 | loss: 3.2400341
MixupTrain:  epoch  0, batch     1 | loss: 3.4306448
MixupTrain:  epoch  0, batch     2 | loss: 3.7770283
MixupTrain:  epoch  0, batch     3 | loss: 3.2202024
MixupTrain:  epoch  0, batch     4 | loss: 3.2841954
MixupTrain:  epoch  0, batch     5 | loss: 3.1189027
MixupTrain:  epoch  0, batch     6 | loss: 3.0733595
MixupTrain:  epoch  0, batch     7 | loss: 3.2175212
MixupTrain:  epoch  0, batch     8 | loss: 3.1649437
MemoryTrain:  epoch  0, batch     0 | loss: 2.4669743
MemoryTrain:  epoch  0, batch     1 | loss: 3.1440747
MemoryTrain:  epoch  0, batch     2 | loss: 3.3974433
MemoryTrain:  epoch  1, batch     0 | loss: 2.2916429
MemoryTrain:  epoch  1, batch     1 | loss: 3.5309639
MemoryTrain:  epoch  1, batch     2 | loss: 2.2475920
MemoryTrain:  epoch  2, batch     0 | loss: 2.1911149
MemoryTrain:  epoch  2, batch     1 | loss: 2.7352338
MemoryTrain:  epoch  2, batch     2 | loss: 2.1800117
MemoryTrain:  epoch  3, batch     0 | loss: 2.4846311
MemoryTrain:  epoch  3, batch     1 | loss: 2.1399550
MemoryTrain:  epoch  3, batch     2 | loss: 1.8734450
MemoryTrain:  epoch  4, batch     0 | loss: 2.1850264
MemoryTrain:  epoch  4, batch     1 | loss: 2.2566278
MemoryTrain:  epoch  4, batch     2 | loss: 1.9909534
MemoryTrain:  epoch  5, batch     0 | loss: 2.1861424
MemoryTrain:  epoch  5, batch     1 | loss: 1.9806149
MemoryTrain:  epoch  5, batch     2 | loss: 2.0817890
MemoryTrain:  epoch  6, batch     0 | loss: 1.9583639
MemoryTrain:  epoch  6, batch     1 | loss: 1.8879422
MemoryTrain:  epoch  6, batch     2 | loss: 1.8619157
MemoryTrain:  epoch  7, batch     0 | loss: 2.0133622
MemoryTrain:  epoch  7, batch     1 | loss: 1.9223157
MemoryTrain:  epoch  7, batch     2 | loss: 1.5710486
MemoryTrain:  epoch  8, batch     0 | loss: 1.7643421
MemoryTrain:  epoch  8, batch     1 | loss: 1.8222314
MemoryTrain:  epoch  8, batch     2 | loss: 1.5373808
MemoryTrain:  epoch  9, batch     0 | loss: 1.6061373
MemoryTrain:  epoch  9, batch     1 | loss: 1.7142688
MemoryTrain:  epoch  9, batch     2 | loss: 1.7041278
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   
[EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   
[EVAL] batch:    2 | acc: 62.50%,  total acc: 68.75%   
[EVAL] batch:    3 | acc: 75.00%,  total acc: 70.31%   
[EVAL] batch:    4 | acc: 50.00%,  total acc: 66.25%   
[EVAL] batch:    5 | acc: 87.50%,  total acc: 69.79%   
[EVAL] batch:    6 | acc: 75.00%,  total acc: 70.54%   
[EVAL] batch:    7 | acc: 93.75%,  total acc: 73.44%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 75.69%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 77.50%   
[EVAL] batch:   10 | acc: 81.25%,  total acc: 77.84%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 79.17%   
[EVAL] batch:   12 | acc: 75.00%,  total acc: 78.85%   
[EVAL] batch:   13 | acc: 18.75%,  total acc: 74.55%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 68.75%   
[EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   
[EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   
[EVAL] batch:    4 | acc: 75.00%,  total acc: 71.25%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 70.83%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 74.11%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 77.34%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 79.86%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 81.25%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 82.95%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 83.33%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 84.13%   
[EVAL] batch:   13 | acc: 68.75%,  total acc: 83.04%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 82.50%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 80.86%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 80.51%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 79.51%   
[EVAL] batch:   18 | acc: 68.75%,  total acc: 78.95%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 79.38%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 80.36%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 81.25%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 82.07%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 82.55%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 83.25%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 84.26%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 85.34%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 85.21%   
[EVAL] batch:   30 | acc: 100.00%,  total acc: 85.69%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 85.94%   
[EVAL] batch:   32 | acc: 56.25%,  total acc: 85.04%   
[EVAL] batch:   33 | acc: 6.25%,  total acc: 82.72%   
[EVAL] batch:   34 | acc: 37.50%,  total acc: 81.43%   
[EVAL] batch:   35 | acc: 56.25%,  total acc: 80.73%   
[EVAL] batch:   36 | acc: 68.75%,  total acc: 80.41%   
[EVAL] batch:   37 | acc: 81.25%,  total acc: 80.43%   
[EVAL] batch:   38 | acc: 81.25%,  total acc: 80.45%   
[EVAL] batch:   39 | acc: 100.00%,  total acc: 80.94%   
[EVAL] batch:   40 | acc: 68.75%,  total acc: 80.64%   
[EVAL] batch:   41 | acc: 93.75%,  total acc: 80.95%   
[EVAL] batch:   42 | acc: 12.50%,  total acc: 79.36%   
[EVAL] batch:   43 | acc: 31.25%,  total acc: 78.27%   
[EVAL] batch:   44 | acc: 37.50%,  total acc: 77.36%   
[EVAL] batch:   45 | acc: 18.75%,  total acc: 76.09%   
[EVAL] batch:   46 | acc: 56.25%,  total acc: 75.66%   
[EVAL] batch:   47 | acc: 62.50%,  total acc: 75.39%   
[EVAL] batch:   48 | acc: 81.25%,  total acc: 75.51%   
[EVAL] batch:   49 | acc: 68.75%,  total acc: 75.38%   
[EVAL] batch:   50 | acc: 56.25%,  total acc: 75.00%   
[EVAL] batch:   51 | acc: 68.75%,  total acc: 74.88%   
[EVAL] batch:   52 | acc: 81.25%,  total acc: 75.00%   
[EVAL] batch:   53 | acc: 81.25%,  total acc: 75.12%   
[EVAL] batch:   54 | acc: 93.75%,  total acc: 75.45%   
[EVAL] batch:   55 | acc: 93.75%,  total acc: 75.78%   
[EVAL] batch:   56 | acc: 93.75%,  total acc: 76.10%   
[EVAL] batch:   57 | acc: 75.00%,  total acc: 76.08%   
[EVAL] batch:   58 | acc: 93.75%,  total acc: 76.38%   
[EVAL] batch:   59 | acc: 68.75%,  total acc: 76.25%   
[EVAL] batch:   60 | acc: 12.50%,  total acc: 75.20%   
cur_acc:  ['0.8485', '0.7167', '0.7455']
his_acc:  ['0.8485', '0.8245', '0.7520']
CurrentTrain: epoch  0, batch     0 | loss: 5.3168344
CurrentTrain: epoch  0, batch     1 | loss: 6.8436112
CurrentTrain: epoch  1, batch     0 | loss: 5.0074582
CurrentTrain: epoch  1, batch     1 | loss: 5.0576615
CurrentTrain: epoch  2, batch     0 | loss: 4.7282057
CurrentTrain: epoch  2, batch     1 | loss: 3.4516609
CurrentTrain: epoch  3, batch     0 | loss: 4.1401830
CurrentTrain: epoch  3, batch     1 | loss: 3.5211518
CurrentTrain: epoch  4, batch     0 | loss: 3.8364868
CurrentTrain: epoch  4, batch     1 | loss: 3.7913334
CurrentTrain: epoch  5, batch     0 | loss: 3.6031713
CurrentTrain: epoch  5, batch     1 | loss: 3.4153485
CurrentTrain: epoch  6, batch     0 | loss: 3.3327141
CurrentTrain: epoch  6, batch     1 | loss: 2.7018511
CurrentTrain: epoch  7, batch     0 | loss: 2.7182384
CurrentTrain: epoch  7, batch     1 | loss: 3.3521893
CurrentTrain: epoch  8, batch     0 | loss: 2.8933117
CurrentTrain: epoch  8, batch     1 | loss: 2.8361161
CurrentTrain: epoch  9, batch     0 | loss: 2.7899246
CurrentTrain: epoch  9, batch     1 | loss: 2.7982595
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones settled in san francisco, where she found a job in tech. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith lived in boston, enjoying the vibrant culture and history of the city. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: her political involvement began early : at cornell , she helped organize local farmers ' cooperatives .
Head Entity: she
Tail Entity: cornell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After graduating from high school, he enrolled at Stanford University to pursue his degree in computer science.  
Head Entity: he  
Tail Entity: Stanford University  

Relation: person schools attended  
Context: She spent four years at Harvard, where she majored in economics and developed a passion for public policy.  
Head Entity: She  
Tail Entity: Harvard  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author agatha christie died in her home in wallingford, england  
Head Entity: agatha christie  
Tail Entity: england  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by his wife of 63 years , josephine robinson mcnair , of columbia ; a son , robert e. jr. , of columbia ; three daughters , robin lee howell and corinne godshall , of myrtle beach , s.c. , and claudia crawford mcnair , of jamestown , s.c. ; six grandchildren ; and one great-grandchild .
Head Entity: he
Tail Entity: claudia crawford mcnair
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: After the passing of her husband, she dedicated her life to raising their three children, including her youngest, emily, who is now a successful artist in new york.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: The famous actor often shares stories about his childhood and the lessons he learned from his father, who was a significant influence on his two sons, michael and josh.  
Head Entity: his father  
Tail Entity: michael  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, a crime that could result in significant prison time if convicted.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith was charged with assault following the altercation that took place outside the nightclub last weekend.  
Head Entity: Smith  
Tail Entity: assault  
Mixup data size:  164
MixupTrain:  epoch  0, batch     0 | loss: 2.6389127
MixupTrain:  epoch  0, batch     1 | loss: 2.3845711
MixupTrain:  epoch  0, batch     2 | loss: 2.4261639
MixupTrain:  epoch  0, batch     3 | loss: 2.6026995
MixupTrain:  epoch  0, batch     4 | loss: 2.3897460
MixupTrain:  epoch  0, batch     5 | loss: 2.8752151
MixupTrain:  epoch  0, batch     6 | loss: 2.6417971
MixupTrain:  epoch  0, batch     7 | loss: 2.6779113
MixupTrain:  epoch  0, batch     8 | loss: 2.8000183
MixupTrain:  epoch  0, batch     9 | loss: 2.2824328
MixupTrain:  epoch  0, batch    10 | loss: 2.1339250
MemoryTrain:  epoch  0, batch     0 | loss: 2.4745243
MemoryTrain:  epoch  0, batch     1 | loss: 3.7333913
MemoryTrain:  epoch  0, batch     2 | loss: 2.7750649
MemoryTrain:  epoch  0, batch     3 | loss: 2.4081881
MemoryTrain:  epoch  1, batch     0 | loss: 2.3280411
MemoryTrain:  epoch  1, batch     1 | loss: 2.5658219
MemoryTrain:  epoch  1, batch     2 | loss: 2.2490611
MemoryTrain:  epoch  1, batch     3 | loss: 2.7652550
MemoryTrain:  epoch  2, batch     0 | loss: 1.7616520
MemoryTrain:  epoch  2, batch     1 | loss: 2.4082804
MemoryTrain:  epoch  2, batch     2 | loss: 2.4002502
MemoryTrain:  epoch  2, batch     3 | loss: 1.9680803
MemoryTrain:  epoch  3, batch     0 | loss: 2.0044670
MemoryTrain:  epoch  3, batch     1 | loss: 2.2820120
MemoryTrain:  epoch  3, batch     2 | loss: 1.5952368
MemoryTrain:  epoch  3, batch     3 | loss: 1.9888078
MemoryTrain:  epoch  4, batch     0 | loss: 2.2153320
MemoryTrain:  epoch  4, batch     1 | loss: 2.0419955
MemoryTrain:  epoch  4, batch     2 | loss: 1.6867220
MemoryTrain:  epoch  4, batch     3 | loss: 1.6838878
MemoryTrain:  epoch  5, batch     0 | loss: 1.6762809
MemoryTrain:  epoch  5, batch     1 | loss: 1.8996104
MemoryTrain:  epoch  5, batch     2 | loss: 1.7120178
MemoryTrain:  epoch  5, batch     3 | loss: 1.8334769
MemoryTrain:  epoch  6, batch     0 | loss: 1.6881876
MemoryTrain:  epoch  6, batch     1 | loss: 1.6977639
MemoryTrain:  epoch  6, batch     2 | loss: 1.5577486
MemoryTrain:  epoch  6, batch     3 | loss: 1.6121064
MemoryTrain:  epoch  7, batch     0 | loss: 1.6007552
MemoryTrain:  epoch  7, batch     1 | loss: 1.4310502
MemoryTrain:  epoch  7, batch     2 | loss: 1.8363941
MemoryTrain:  epoch  7, batch     3 | loss: 1.6001596
MemoryTrain:  epoch  8, batch     0 | loss: 1.6299727
MemoryTrain:  epoch  8, batch     1 | loss: 1.3962264
MemoryTrain:  epoch  8, batch     2 | loss: 1.5558658
MemoryTrain:  epoch  8, batch     3 | loss: 1.5183883
MemoryTrain:  epoch  9, batch     0 | loss: 1.5471610
MemoryTrain:  epoch  9, batch     1 | loss: 1.3654454
MemoryTrain:  epoch  9, batch     2 | loss: 1.4878757
MemoryTrain:  epoch  9, batch     3 | loss: 1.2769560
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   
[EVAL] batch:    3 | acc: 87.50%,  total acc: 85.94%   
[EVAL] batch:    4 | acc: 62.50%,  total acc: 81.25%   
[EVAL] batch:    5 | acc: 62.50%,  total acc: 78.12%   
[EVAL] batch:    6 | acc: 87.50%,  total acc: 79.46%   
[EVAL] batch:    7 | acc: 87.50%,  total acc: 80.47%   
[EVAL] batch:    8 | acc: 81.25%,  total acc: 80.56%   
[EVAL] batch:    9 | acc: 62.50%,  total acc: 78.75%   
[EVAL] batch:   10 | acc: 50.00%,  total acc: 76.14%   
[EVAL] batch:   11 | acc: 100.00%,  total acc: 78.12%   
[EVAL] batch:   12 | acc: 100.00%,  total acc: 79.81%   
[EVAL] batch:   13 | acc: 100.00%,  total acc: 81.25%   
[EVAL] batch:   14 | acc: 100.00%,  total acc: 82.50%   
[EVAL] batch:   15 | acc: 100.00%,  total acc: 83.59%   
[EVAL] batch:   16 | acc: 100.00%,  total acc: 84.56%   
[EVAL] batch:   17 | acc: 25.00%,  total acc: 81.25%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   
[EVAL] batch:    1 | acc: 37.50%,  total acc: 46.88%   
[EVAL] batch:    2 | acc: 68.75%,  total acc: 54.17%   
[EVAL] batch:    3 | acc: 50.00%,  total acc: 53.12%   
[EVAL] batch:    4 | acc: 43.75%,  total acc: 51.25%   
[EVAL] batch:    5 | acc: 56.25%,  total acc: 52.08%   
[EVAL] batch:    6 | acc: 87.50%,  total acc: 57.14%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 62.50%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 66.67%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 69.38%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 72.16%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 73.44%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 75.00%   
[EVAL] batch:   13 | acc: 68.75%,  total acc: 74.55%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 74.58%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 73.44%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 73.53%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 72.92%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 72.37%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 73.12%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 74.40%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 75.57%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 76.63%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 77.60%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 78.50%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 79.33%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 79.86%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 80.58%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 81.25%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:   30 | acc: 100.00%,  total acc: 81.85%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 82.23%   
[EVAL] batch:   32 | acc: 56.25%,  total acc: 81.44%   
[EVAL] batch:   33 | acc: 12.50%,  total acc: 79.41%   
[EVAL] batch:   34 | acc: 12.50%,  total acc: 77.50%   
[EVAL] batch:   35 | acc: 37.50%,  total acc: 76.39%   
[EVAL] batch:   36 | acc: 37.50%,  total acc: 75.34%   
[EVAL] batch:   37 | acc: 75.00%,  total acc: 75.33%   
[EVAL] batch:   38 | acc: 87.50%,  total acc: 75.64%   
[EVAL] batch:   39 | acc: 100.00%,  total acc: 76.25%   
[EVAL] batch:   40 | acc: 75.00%,  total acc: 76.22%   
[EVAL] batch:   41 | acc: 100.00%,  total acc: 76.79%   
[EVAL] batch:   42 | acc: 25.00%,  total acc: 75.58%   
[EVAL] batch:   43 | acc: 50.00%,  total acc: 75.00%   
[EVAL] batch:   44 | acc: 50.00%,  total acc: 74.44%   
[EVAL] batch:   45 | acc: 25.00%,  total acc: 73.37%   
[EVAL] batch:   46 | acc: 56.25%,  total acc: 73.01%   
[EVAL] batch:   47 | acc: 62.50%,  total acc: 72.79%   
[EVAL] batch:   48 | acc: 12.50%,  total acc: 71.56%   
[EVAL] batch:   49 | acc: 25.00%,  total acc: 70.62%   
[EVAL] batch:   50 | acc: 6.25%,  total acc: 69.36%   
[EVAL] batch:   51 | acc: 6.25%,  total acc: 68.15%   
[EVAL] batch:   52 | acc: 6.25%,  total acc: 66.98%   
[EVAL] batch:   53 | acc: 50.00%,  total acc: 66.67%   
[EVAL] batch:   54 | acc: 93.75%,  total acc: 67.16%   
[EVAL] batch:   55 | acc: 93.75%,  total acc: 67.63%   
[EVAL] batch:   56 | acc: 87.50%,  total acc: 67.98%   
[EVAL] batch:   57 | acc: 68.75%,  total acc: 68.00%   
[EVAL] batch:   58 | acc: 81.25%,  total acc: 68.22%   
[EVAL] batch:   59 | acc: 81.25%,  total acc: 68.44%   
[EVAL] batch:   60 | acc: 68.75%,  total acc: 68.44%   
[EVAL] batch:   61 | acc: 87.50%,  total acc: 68.75%   
[EVAL] batch:   62 | acc: 81.25%,  total acc: 68.95%   
[EVAL] batch:   63 | acc: 93.75%,  total acc: 69.34%   
[EVAL] batch:   64 | acc: 68.75%,  total acc: 69.33%   
[EVAL] batch:   65 | acc: 68.75%,  total acc: 69.32%   
[EVAL] batch:   66 | acc: 68.75%,  total acc: 69.31%   
[EVAL] batch:   67 | acc: 93.75%,  total acc: 69.67%   
[EVAL] batch:   68 | acc: 81.25%,  total acc: 69.84%   
[EVAL] batch:   69 | acc: 68.75%,  total acc: 69.82%   
[EVAL] batch:   70 | acc: 68.75%,  total acc: 69.81%   
[EVAL] batch:   71 | acc: 62.50%,  total acc: 69.70%   
[EVAL] batch:   72 | acc: 100.00%,  total acc: 70.12%   
[EVAL] batch:   73 | acc: 100.00%,  total acc: 70.52%   
[EVAL] batch:   74 | acc: 100.00%,  total acc: 70.92%   
[EVAL] batch:   75 | acc: 100.00%,  total acc: 71.30%   
[EVAL] batch:   76 | acc: 100.00%,  total acc: 71.67%   
[EVAL] batch:   77 | acc: 75.00%,  total acc: 71.71%   
cur_acc:  ['0.8485', '0.7167', '0.7455', '0.8125']
his_acc:  ['0.8485', '0.8245', '0.7520', '0.7171']
CurrentTrain: epoch  0, batch     0 | loss: 7.9217720
CurrentTrain: epoch  0, batch     1 | loss: 8.5265913
CurrentTrain: epoch  1, batch     0 | loss: 8.0069542
CurrentTrain: epoch  1, batch     1 | loss: 6.8854542
CurrentTrain: epoch  2, batch     0 | loss: 6.6404200
CurrentTrain: epoch  2, batch     1 | loss: 7.4571848
CurrentTrain: epoch  3, batch     0 | loss: 7.0370426
CurrentTrain: epoch  3, batch     1 | loss: 5.8613806
CurrentTrain: epoch  4, batch     0 | loss: 6.6896501
CurrentTrain: epoch  4, batch     1 | loss: 5.9961419
CurrentTrain: epoch  5, batch     0 | loss: 6.0112267
CurrentTrain: epoch  5, batch     1 | loss: 6.2744598
CurrentTrain: epoch  6, batch     0 | loss: 5.8806992
CurrentTrain: epoch  6, batch     1 | loss: 5.4606309
CurrentTrain: epoch  7, batch     0 | loss: 5.3318138
CurrentTrain: epoch  7, batch     1 | loss: 5.5932174
CurrentTrain: epoch  8, batch     0 | loss: 5.3351259
CurrentTrain: epoch  8, batch     1 | loss: 5.1698804
CurrentTrain: epoch  9, batch     0 | loss: 5.3377895
CurrentTrain: epoch  9, batch     1 | loss: 4.5763822
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has become a leading platform for video content, and Waymo, which focuses on self-driving technology.  
Head Entity: Alphabet Inc.  
Tail Entity: Waymo  

Relation: organization subsidiaries  
Context: The automotive manufacturer Ford Motor Company has expanded its reach by acquiring several companies, including the electric vehicle startup Rivian, which aims to revolutionize the EV market.  
Head Entity: Ford Motor Company  
Tail Entity: Rivian  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aerolineas argentinas ' owner , madrid-based grupo marsans , is challenging the measure before a world bank arbitration body since it considers the takeover `` arbitrary and illegitimate , '' the company said wednesday night in a news release .
Head Entity: aerolineas argentinas
Tail Entity: grupo marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: the multinational technology company apple inc. has been acquired by the parent organization, tech giants holding, which aims to expand its portfolio in the consumer electronics market.  
Head Entity: apple inc.  
Tail Entity: tech giants holding  

Relation: organization parents  
Context: the renowned publishing house penguin random house is a subsidiary of the larger media conglomerate bertelsmann, which oversees various entertainment and media ventures worldwide.  
Head Entity: penguin random house  
Tail Entity: bertelsmann  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the talks between graham and pak ui-chun were held in a `` friendly '' manner , the official korean central news agency -lrb- kcna -rrb- said , without giving details .
Head Entity: kcna
Tail Entity: korean central news agency
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Business Machines Corporation, commonly known as IBM, has been a leader in technology for decades.  
Head Entity: IBM  
Tail Entity: International Business Machines Corporation  

Relation: organization alternate names  
Context: The World Health Organization, often referred to as WHO, has been instrumental in addressing global health issues.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant google has announced plans to expand its headquarters in the heart of san francisco, aiming to create more job opportunities and enhance its presence in the city.  
Head Entity: google  
Tail Entity: san francisco  

Relation: organization city of headquarters  
Context: ------ new york 2019-11-22 14:45:00 utc the financial services firm jp morgan chase has confirmed that its main headquarters will remain in new york city, despite discussions of relocating to other states.  
Head Entity: jp morgan chase  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: during the family reunion, john introduced his sister, emily, who had just returned from studying abroad.  
Head Entity: john  
Tail Entity: emily  

Relation: person siblings  
Context: at the award ceremony, sarah proudly accepted her trophy while her brother, michael, cheered from the audience.  
Head Entity: sarah  
Tail Entity: michael  
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 3.2806284
MixupTrain:  epoch  0, batch     1 | loss: 2.7327483
MixupTrain:  epoch  0, batch     2 | loss: 2.6870847
MixupTrain:  epoch  0, batch     3 | loss: 2.9297738
MixupTrain:  epoch  0, batch     4 | loss: 2.9702005
MixupTrain:  epoch  0, batch     5 | loss: 2.7608728
MixupTrain:  epoch  0, batch     6 | loss: 2.6606469
MixupTrain:  epoch  0, batch     7 | loss: 2.8435178
MixupTrain:  epoch  0, batch     8 | loss: 2.7684479
MixupTrain:  epoch  0, batch     9 | loss: 2.8892999
MixupTrain:  epoch  0, batch    10 | loss: 2.6547701
MixupTrain:  epoch  0, batch    11 | loss: 2.5379872
MixupTrain:  epoch  0, batch    12 | loss: 2.1585135
MemoryTrain:  epoch  0, batch     0 | loss: 1.9525176
MemoryTrain:  epoch  0, batch     1 | loss: 2.7945285
MemoryTrain:  epoch  0, batch     2 | loss: 3.2142775
MemoryTrain:  epoch  0, batch     3 | loss: 2.5692074
MemoryTrain:  epoch  0, batch     4 | loss: 2.5307837
MemoryTrain:  epoch  1, batch     0 | loss: 3.0609980
MemoryTrain:  epoch  1, batch     1 | loss: 1.7960149
MemoryTrain:  epoch  1, batch     2 | loss: 2.0448141
MemoryTrain:  epoch  1, batch     3 | loss: 2.5667126
MemoryTrain:  epoch  1, batch     4 | loss: 2.9449813
MemoryTrain:  epoch  2, batch     0 | loss: 2.3113732
MemoryTrain:  epoch  2, batch     1 | loss: 1.8674971
MemoryTrain:  epoch  2, batch     2 | loss: 2.4189103
MemoryTrain:  epoch  2, batch     3 | loss: 2.6808133
MemoryTrain:  epoch  2, batch     4 | loss: 1.9732326
MemoryTrain:  epoch  3, batch     0 | loss: 1.5476336
MemoryTrain:  epoch  3, batch     1 | loss: 2.8800097
MemoryTrain:  epoch  3, batch     2 | loss: 1.6971724
MemoryTrain:  epoch  3, batch     3 | loss: 2.3826785
MemoryTrain:  epoch  3, batch     4 | loss: 1.8494211
MemoryTrain:  epoch  4, batch     0 | loss: 1.5135777
MemoryTrain:  epoch  4, batch     1 | loss: 2.2350795
MemoryTrain:  epoch  4, batch     2 | loss: 1.6994911
MemoryTrain:  epoch  4, batch     3 | loss: 2.1223764
MemoryTrain:  epoch  4, batch     4 | loss: 2.4465981
MemoryTrain:  epoch  5, batch     0 | loss: 2.2365685
MemoryTrain:  epoch  5, batch     1 | loss: 1.6489817
MemoryTrain:  epoch  5, batch     2 | loss: 1.8105332
MemoryTrain:  epoch  5, batch     3 | loss: 1.6819086
MemoryTrain:  epoch  5, batch     4 | loss: 1.9124072
MemoryTrain:  epoch  6, batch     0 | loss: 1.7402914
MemoryTrain:  epoch  6, batch     1 | loss: 1.7053061
MemoryTrain:  epoch  6, batch     2 | loss: 1.8532368
MemoryTrain:  epoch  6, batch     3 | loss: 1.6774359
MemoryTrain:  epoch  6, batch     4 | loss: 1.5607020
MemoryTrain:  epoch  7, batch     0 | loss: 1.6831633
MemoryTrain:  epoch  7, batch     1 | loss: 1.6762980
MemoryTrain:  epoch  7, batch     2 | loss: 1.7242835
MemoryTrain:  epoch  7, batch     3 | loss: 1.4469823
MemoryTrain:  epoch  7, batch     4 | loss: 1.8042719
MemoryTrain:  epoch  8, batch     0 | loss: 1.4570901
MemoryTrain:  epoch  8, batch     1 | loss: 1.6074283
MemoryTrain:  epoch  8, batch     2 | loss: 1.5258069
MemoryTrain:  epoch  8, batch     3 | loss: 1.6983367
MemoryTrain:  epoch  8, batch     4 | loss: 1.5218327
MemoryTrain:  epoch  9, batch     0 | loss: 1.5731344
MemoryTrain:  epoch  9, batch     1 | loss: 1.4289027
MemoryTrain:  epoch  9, batch     2 | loss: 1.7839074
MemoryTrain:  epoch  9, batch     3 | loss: 1.4588535
MemoryTrain:  epoch  9, batch     4 | loss: 1.3511684
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 50.00%   
[EVAL] batch:    2 | acc: 37.50%,  total acc: 45.83%   
[EVAL] batch:    3 | acc: 56.25%,  total acc: 48.44%   
[EVAL] batch:    4 | acc: 75.00%,  total acc: 53.75%   
[EVAL] batch:    5 | acc: 37.50%,  total acc: 51.04%   
[EVAL] batch:    6 | acc: 68.75%,  total acc: 53.57%   
[EVAL] batch:    7 | acc: 50.00%,  total acc: 53.12%   
[EVAL] batch:    8 | acc: 31.25%,  total acc: 50.69%   
[EVAL] batch:    9 | acc: 56.25%,  total acc: 51.25%   
[EVAL] batch:   10 | acc: 50.00%,  total acc: 51.14%   
[EVAL] batch:   11 | acc: 37.50%,  total acc: 50.00%   
[EVAL] batch:   12 | acc: 56.25%,  total acc: 50.48%   
[EVAL] batch:   13 | acc: 100.00%,  total acc: 54.02%   
[EVAL] batch:   14 | acc: 93.75%,  total acc: 56.67%   
[EVAL] batch:   15 | acc: 100.00%,  total acc: 59.38%   
[EVAL] batch:   16 | acc: 93.75%,  total acc: 61.40%   
[EVAL] batch:   17 | acc: 87.50%,  total acc: 62.85%   
[EVAL] batch:   18 | acc: 56.25%,  total acc: 62.50%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 63.75%   
[EVAL] batch:   20 | acc: 62.50%,  total acc: 63.69%   
[EVAL] batch:   21 | acc: 37.50%,  total acc: 62.50%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   
[EVAL] batch:    1 | acc: 37.50%,  total acc: 43.75%   
[EVAL] batch:    2 | acc: 75.00%,  total acc: 54.17%   
[EVAL] batch:    3 | acc: 56.25%,  total acc: 54.69%   
[EVAL] batch:    4 | acc: 43.75%,  total acc: 52.50%   
[EVAL] batch:    5 | acc: 50.00%,  total acc: 52.08%   
[EVAL] batch:    6 | acc: 87.50%,  total acc: 57.14%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 62.50%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 66.67%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 69.38%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 72.16%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 73.44%   
[EVAL] batch:   12 | acc: 62.50%,  total acc: 72.60%   
[EVAL] batch:   13 | acc: 37.50%,  total acc: 70.09%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 70.42%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 69.53%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 69.85%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 69.44%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 69.08%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 70.00%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 71.43%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 72.73%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 73.91%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 74.74%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 75.75%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 76.68%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 77.31%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 78.12%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 78.88%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 78.96%   
[EVAL] batch:   30 | acc: 93.75%,  total acc: 79.44%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 80.08%   
[EVAL] batch:   32 | acc: 56.25%,  total acc: 79.36%   
[EVAL] batch:   33 | acc: 25.00%,  total acc: 77.76%   
[EVAL] batch:   34 | acc: 18.75%,  total acc: 76.07%   
[EVAL] batch:   35 | acc: 43.75%,  total acc: 75.17%   
[EVAL] batch:   36 | acc: 50.00%,  total acc: 74.49%   
[EVAL] batch:   37 | acc: 75.00%,  total acc: 74.51%   
[EVAL] batch:   38 | acc: 75.00%,  total acc: 74.52%   
[EVAL] batch:   39 | acc: 100.00%,  total acc: 75.16%   
[EVAL] batch:   40 | acc: 81.25%,  total acc: 75.30%   
[EVAL] batch:   41 | acc: 100.00%,  total acc: 75.89%   
[EVAL] batch:   42 | acc: 12.50%,  total acc: 74.42%   
[EVAL] batch:   43 | acc: 6.25%,  total acc: 72.87%   
[EVAL] batch:   44 | acc: 0.00%,  total acc: 71.25%   
[EVAL] batch:   45 | acc: 0.00%,  total acc: 69.70%   
[EVAL] batch:   46 | acc: 18.75%,  total acc: 68.62%   
[EVAL] batch:   47 | acc: 75.00%,  total acc: 68.75%   
[EVAL] batch:   48 | acc: 12.50%,  total acc: 67.60%   
[EVAL] batch:   49 | acc: 6.25%,  total acc: 66.38%   
[EVAL] batch:   50 | acc: 0.00%,  total acc: 65.07%   
[EVAL] batch:   51 | acc: 0.00%,  total acc: 63.82%   
[EVAL] batch:   52 | acc: 6.25%,  total acc: 62.74%   
[EVAL] batch:   53 | acc: 43.75%,  total acc: 62.38%   
[EVAL] batch:   54 | acc: 93.75%,  total acc: 62.95%   
[EVAL] batch:   55 | acc: 93.75%,  total acc: 63.50%   
[EVAL] batch:   56 | acc: 87.50%,  total acc: 63.93%   
[EVAL] batch:   57 | acc: 68.75%,  total acc: 64.01%   
[EVAL] batch:   58 | acc: 75.00%,  total acc: 64.19%   
[EVAL] batch:   59 | acc: 75.00%,  total acc: 64.38%   
[EVAL] batch:   60 | acc: 37.50%,  total acc: 63.93%   
[EVAL] batch:   61 | acc: 81.25%,  total acc: 64.21%   
[EVAL] batch:   62 | acc: 81.25%,  total acc: 64.48%   
[EVAL] batch:   63 | acc: 87.50%,  total acc: 64.84%   
[EVAL] batch:   64 | acc: 56.25%,  total acc: 64.71%   
[EVAL] batch:   65 | acc: 68.75%,  total acc: 64.77%   
[EVAL] batch:   66 | acc: 56.25%,  total acc: 64.65%   
[EVAL] batch:   67 | acc: 100.00%,  total acc: 65.17%   
[EVAL] batch:   68 | acc: 81.25%,  total acc: 65.40%   
[EVAL] batch:   69 | acc: 56.25%,  total acc: 65.27%   
[EVAL] batch:   70 | acc: 50.00%,  total acc: 65.05%   
[EVAL] batch:   71 | acc: 62.50%,  total acc: 65.02%   
[EVAL] batch:   72 | acc: 100.00%,  total acc: 65.50%   
[EVAL] batch:   73 | acc: 100.00%,  total acc: 65.96%   
[EVAL] batch:   74 | acc: 100.00%,  total acc: 66.42%   
[EVAL] batch:   75 | acc: 100.00%,  total acc: 66.86%   
[EVAL] batch:   76 | acc: 100.00%,  total acc: 67.29%   
[EVAL] batch:   77 | acc: 87.50%,  total acc: 67.55%   
[EVAL] batch:   78 | acc: 43.75%,  total acc: 67.25%   
[EVAL] batch:   79 | acc: 50.00%,  total acc: 67.03%   
[EVAL] batch:   80 | acc: 50.00%,  total acc: 66.82%   
[EVAL] batch:   81 | acc: 56.25%,  total acc: 66.69%   
[EVAL] batch:   82 | acc: 62.50%,  total acc: 66.64%   
[EVAL] batch:   83 | acc: 43.75%,  total acc: 66.37%   
[EVAL] batch:   84 | acc: 62.50%,  total acc: 66.32%   
[EVAL] batch:   85 | acc: 50.00%,  total acc: 66.13%   
[EVAL] batch:   86 | acc: 31.25%,  total acc: 65.73%   
[EVAL] batch:   87 | acc: 75.00%,  total acc: 65.84%   
[EVAL] batch:   88 | acc: 31.25%,  total acc: 65.45%   
[EVAL] batch:   89 | acc: 37.50%,  total acc: 65.14%   
[EVAL] batch:   90 | acc: 75.00%,  total acc: 65.25%   
[EVAL] batch:   91 | acc: 100.00%,  total acc: 65.62%   
[EVAL] batch:   92 | acc: 93.75%,  total acc: 65.93%   
[EVAL] batch:   93 | acc: 100.00%,  total acc: 66.29%   
[EVAL] batch:   94 | acc: 87.50%,  total acc: 66.51%   
[EVAL] batch:   95 | acc: 81.25%,  total acc: 66.67%   
[EVAL] batch:   96 | acc: 68.75%,  total acc: 66.69%   
[EVAL] batch:   97 | acc: 75.00%,  total acc: 66.77%   
[EVAL] batch:   98 | acc: 68.75%,  total acc: 66.79%   
[EVAL] batch:   99 | acc: 18.75%,  total acc: 66.31%   
cur_acc:  ['0.8485', '0.7167', '0.7455', '0.8125', '0.6250']
his_acc:  ['0.8485', '0.8245', '0.7520', '0.7171', '0.6631']
CurrentTrain: epoch  0, batch     0 | loss: 5.5425172
CurrentTrain: epoch  0, batch     1 | loss: 6.6917138
CurrentTrain: epoch  1, batch     0 | loss: 5.0375652
CurrentTrain: epoch  1, batch     1 | loss: 3.6416795
CurrentTrain: epoch  2, batch     0 | loss: 4.5323815
CurrentTrain: epoch  2, batch     1 | loss: 3.1312366
CurrentTrain: epoch  3, batch     0 | loss: 3.7494390
CurrentTrain: epoch  3, batch     1 | loss: 3.9878905
CurrentTrain: epoch  4, batch     0 | loss: 3.6615877
CurrentTrain: epoch  4, batch     1 | loss: 3.4411807
CurrentTrain: epoch  5, batch     0 | loss: 3.7205579
CurrentTrain: epoch  5, batch     1 | loss: 2.9044878
CurrentTrain: epoch  6, batch     0 | loss: 3.0196209
CurrentTrain: epoch  6, batch     1 | loss: 3.7880940
CurrentTrain: epoch  7, batch     0 | loss: 3.0589991
CurrentTrain: epoch  7, batch     1 | loss: 3.0070577
CurrentTrain: epoch  8, batch     0 | loss: 3.2303710
CurrentTrain: epoch  8, batch     1 | loss: 2.5111737
CurrentTrain: epoch  9, batch     0 | loss: 3.0817275
CurrentTrain: epoch  9, batch     1 | loss: 2.7919152
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun kapoor has always been proud of his indian heritage and culture.  
Head Entity: arjun kapoor  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she revealed that she was born in the picturesque town of auckland, new zealand, which she considers her true home.  
Head Entity: she  
Tail Entity: new zealand  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.nike.com for the latest sports gear and apparel.  
Head Entity: Nike  
Tail Entity: https://www.nike.com  

Relation: organization website  
Context: For more information about their services, check out http://www.apple.com.  
Head Entity: Apple  
Tail Entity: http://www.apple.com  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has recently announced that it will be acquiring a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple inc.  

Relation: organization shareholders  
Context: the investment firm blackrock has increased its holdings in the renewable energy company first solar, signaling confidence in its future growth.  
Head Entity: first solar  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its dissolution in the spring of 2018, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring of 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak started apple inc., which revolutionized the personal computing industry and changed the way people interact with technology.  
Head Entity: apple inc.  
Tail Entity: steve jobs  

Relation: organization founded by  
Context: the famous fashion brand gucci was established in 1921 by guccio gucci in florence, italy, and has since become a symbol of luxury and style.  
Head Entity: gucci  
Tail Entity: guccio gucci  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 1.9873110
MixupTrain:  epoch  0, batch     1 | loss: 2.2917430
MixupTrain:  epoch  0, batch     2 | loss: 2.0222368
MixupTrain:  epoch  0, batch     3 | loss: 2.4113879
MixupTrain:  epoch  0, batch     4 | loss: 2.9994988
MixupTrain:  epoch  0, batch     5 | loss: 2.3499956
MixupTrain:  epoch  0, batch     6 | loss: 2.8098555
MixupTrain:  epoch  0, batch     7 | loss: 2.5768123
MixupTrain:  epoch  0, batch     8 | loss: 2.4698863
MixupTrain:  epoch  0, batch     9 | loss: 2.3664749
MixupTrain:  epoch  0, batch    10 | loss: 2.4605379
MixupTrain:  epoch  0, batch    11 | loss: 2.4720578
MixupTrain:  epoch  0, batch    12 | loss: 2.3837230
MixupTrain:  epoch  0, batch    13 | loss: 2.6396825
MemoryTrain:  epoch  0, batch     0 | loss: 2.3255596
MemoryTrain:  epoch  0, batch     1 | loss: 2.9926023
MemoryTrain:  epoch  0, batch     2 | loss: 1.9264885
MemoryTrain:  epoch  0, batch     3 | loss: 2.1752963
MemoryTrain:  epoch  0, batch     4 | loss: 2.1509531
MemoryTrain:  epoch  0, batch     5 | loss: 2.5847626
MemoryTrain:  epoch  1, batch     0 | loss: 2.2781484
MemoryTrain:  epoch  1, batch     1 | loss: 3.2465098
MemoryTrain:  epoch  1, batch     2 | loss: 1.6096849
MemoryTrain:  epoch  1, batch     3 | loss: 2.4593201
MemoryTrain:  epoch  1, batch     4 | loss: 1.5675569
MemoryTrain:  epoch  1, batch     5 | loss: 2.5321968
MemoryTrain:  epoch  2, batch     0 | loss: 2.1411576
MemoryTrain:  epoch  2, batch     1 | loss: 2.1133101
MemoryTrain:  epoch  2, batch     2 | loss: 2.0134680
MemoryTrain:  epoch  2, batch     3 | loss: 1.5250895
MemoryTrain:  epoch  2, batch     4 | loss: 2.1429036
MemoryTrain:  epoch  2, batch     5 | loss: 2.2086713
MemoryTrain:  epoch  3, batch     0 | loss: 1.4723643
MemoryTrain:  epoch  3, batch     1 | loss: 2.3269572
MemoryTrain:  epoch  3, batch     2 | loss: 1.9805050
MemoryTrain:  epoch  3, batch     3 | loss: 1.8508711
MemoryTrain:  epoch  3, batch     4 | loss: 1.8677466
MemoryTrain:  epoch  3, batch     5 | loss: 1.5163287
MemoryTrain:  epoch  4, batch     0 | loss: 1.5069451
MemoryTrain:  epoch  4, batch     1 | loss: 1.8874133
MemoryTrain:  epoch  4, batch     2 | loss: 1.8117189
MemoryTrain:  epoch  4, batch     3 | loss: 1.6426383
MemoryTrain:  epoch  4, batch     4 | loss: 1.7827115
MemoryTrain:  epoch  4, batch     5 | loss: 1.9918133
MemoryTrain:  epoch  5, batch     0 | loss: 1.7659409
MemoryTrain:  epoch  5, batch     1 | loss: 1.5903293
MemoryTrain:  epoch  5, batch     2 | loss: 2.0413842
MemoryTrain:  epoch  5, batch     3 | loss: 1.5961752
MemoryTrain:  epoch  5, batch     4 | loss: 1.7882192
MemoryTrain:  epoch  5, batch     5 | loss: 1.4591634
MemoryTrain:  epoch  6, batch     0 | loss: 1.5870481
MemoryTrain:  epoch  6, batch     1 | loss: 1.3446164
MemoryTrain:  epoch  6, batch     2 | loss: 1.7176807
MemoryTrain:  epoch  6, batch     3 | loss: 1.7098985
MemoryTrain:  epoch  6, batch     4 | loss: 1.8373921
MemoryTrain:  epoch  6, batch     5 | loss: 1.6162961
MemoryTrain:  epoch  7, batch     0 | loss: 1.6107876
MemoryTrain:  epoch  7, batch     1 | loss: 1.3596665
MemoryTrain:  epoch  7, batch     2 | loss: 2.0976176
MemoryTrain:  epoch  7, batch     3 | loss: 1.3277607
MemoryTrain:  epoch  7, batch     4 | loss: 1.6860911
MemoryTrain:  epoch  7, batch     5 | loss: 1.5570614
MemoryTrain:  epoch  8, batch     0 | loss: 1.5606357
MemoryTrain:  epoch  8, batch     1 | loss: 1.5134149
MemoryTrain:  epoch  8, batch     2 | loss: 1.3597441
MemoryTrain:  epoch  8, batch     3 | loss: 1.4190677
MemoryTrain:  epoch  8, batch     4 | loss: 2.1421509
MemoryTrain:  epoch  8, batch     5 | loss: 1.2661997
MemoryTrain:  epoch  9, batch     0 | loss: 1.5286098
MemoryTrain:  epoch  9, batch     1 | loss: 1.5177724
MemoryTrain:  epoch  9, batch     2 | loss: 1.5109196
MemoryTrain:  epoch  9, batch     3 | loss: 1.4850558
MemoryTrain:  epoch  9, batch     4 | loss: 1.4587294
MemoryTrain:  epoch  9, batch     5 | loss: 1.4634789
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   
[EVAL] batch:    2 | acc: 43.75%,  total acc: 75.00%   
[EVAL] batch:    3 | acc: 12.50%,  total acc: 59.38%   
[EVAL] batch:    4 | acc: 6.25%,  total acc: 48.75%   
[EVAL] batch:    5 | acc: 12.50%,  total acc: 42.71%   
[EVAL] batch:    6 | acc: 0.00%,  total acc: 36.61%   
[EVAL] batch:    7 | acc: 0.00%,  total acc: 32.03%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   
[EVAL] batch:    1 | acc: 18.75%,  total acc: 12.50%   
[EVAL] batch:    2 | acc: 37.50%,  total acc: 20.83%   
[EVAL] batch:    3 | acc: 12.50%,  total acc: 18.75%   
[EVAL] batch:    4 | acc: 12.50%,  total acc: 17.50%   
[EVAL] batch:    5 | acc: 12.50%,  total acc: 16.67%   
[EVAL] batch:    6 | acc: 81.25%,  total acc: 25.89%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 35.16%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 41.67%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 46.88%   
[EVAL] batch:   10 | acc: 93.75%,  total acc: 51.14%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 54.17%   
[EVAL] batch:   12 | acc: 56.25%,  total acc: 54.33%   
[EVAL] batch:   13 | acc: 31.25%,  total acc: 52.68%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 54.17%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 54.30%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 55.51%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 55.90%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 56.25%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 57.81%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 59.82%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 61.65%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 63.32%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 64.58%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 66.00%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 67.31%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 68.29%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 69.42%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 70.47%   
[EVAL] batch:   29 | acc: 75.00%,  total acc: 70.62%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 71.17%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 71.88%   
[EVAL] batch:   32 | acc: 62.50%,  total acc: 71.59%   
[EVAL] batch:   33 | acc: 18.75%,  total acc: 70.04%   
[EVAL] batch:   34 | acc: 18.75%,  total acc: 68.57%   
[EVAL] batch:   35 | acc: 37.50%,  total acc: 67.71%   
[EVAL] batch:   36 | acc: 43.75%,  total acc: 67.06%   
[EVAL] batch:   37 | acc: 68.75%,  total acc: 67.11%   
[EVAL] batch:   38 | acc: 75.00%,  total acc: 67.31%   
[EVAL] batch:   39 | acc: 100.00%,  total acc: 68.12%   
[EVAL] batch:   40 | acc: 81.25%,  total acc: 68.45%   
[EVAL] batch:   41 | acc: 100.00%,  total acc: 69.20%   
[EVAL] batch:   42 | acc: 12.50%,  total acc: 67.88%   
[EVAL] batch:   43 | acc: 6.25%,  total acc: 66.48%   
[EVAL] batch:   44 | acc: 18.75%,  total acc: 65.42%   
[EVAL] batch:   45 | acc: 0.00%,  total acc: 63.99%   
[EVAL] batch:   46 | acc: 31.25%,  total acc: 63.30%   
[EVAL] batch:   47 | acc: 56.25%,  total acc: 63.15%   
[EVAL] batch:   48 | acc: 12.50%,  total acc: 62.12%   
[EVAL] batch:   49 | acc: 6.25%,  total acc: 61.00%   
[EVAL] batch:   50 | acc: 0.00%,  total acc: 59.80%   
[EVAL] batch:   51 | acc: 0.00%,  total acc: 58.65%   
[EVAL] batch:   52 | acc: 0.00%,  total acc: 57.55%   
[EVAL] batch:   53 | acc: 50.00%,  total acc: 57.41%   
[EVAL] batch:   54 | acc: 93.75%,  total acc: 58.07%   
[EVAL] batch:   55 | acc: 100.00%,  total acc: 58.82%   
[EVAL] batch:   56 | acc: 87.50%,  total acc: 59.32%   
[EVAL] batch:   57 | acc: 75.00%,  total acc: 59.59%   
[EVAL] batch:   58 | acc: 81.25%,  total acc: 59.96%   
[EVAL] batch:   59 | acc: 75.00%,  total acc: 60.21%   
[EVAL] batch:   60 | acc: 31.25%,  total acc: 59.73%   
[EVAL] batch:   61 | acc: 81.25%,  total acc: 60.08%   
[EVAL] batch:   62 | acc: 87.50%,  total acc: 60.52%   
[EVAL] batch:   63 | acc: 81.25%,  total acc: 60.84%   
[EVAL] batch:   64 | acc: 62.50%,  total acc: 60.87%   
[EVAL] batch:   65 | acc: 62.50%,  total acc: 60.89%   
[EVAL] batch:   66 | acc: 62.50%,  total acc: 60.91%   
[EVAL] batch:   67 | acc: 100.00%,  total acc: 61.49%   
[EVAL] batch:   68 | acc: 81.25%,  total acc: 61.78%   
[EVAL] batch:   69 | acc: 50.00%,  total acc: 61.61%   
[EVAL] batch:   70 | acc: 50.00%,  total acc: 61.44%   
[EVAL] batch:   71 | acc: 62.50%,  total acc: 61.46%   
[EVAL] batch:   72 | acc: 100.00%,  total acc: 61.99%   
[EVAL] batch:   73 | acc: 100.00%,  total acc: 62.50%   
[EVAL] batch:   74 | acc: 100.00%,  total acc: 63.00%   
[EVAL] batch:   75 | acc: 100.00%,  total acc: 63.49%   
[EVAL] batch:   76 | acc: 100.00%,  total acc: 63.96%   
[EVAL] batch:   77 | acc: 81.25%,  total acc: 64.18%   
[EVAL] batch:   78 | acc: 31.25%,  total acc: 63.77%   
[EVAL] batch:   79 | acc: 37.50%,  total acc: 63.44%   
[EVAL] batch:   80 | acc: 12.50%,  total acc: 62.81%   
[EVAL] batch:   81 | acc: 0.00%,  total acc: 62.04%   
[EVAL] batch:   82 | acc: 6.25%,  total acc: 61.37%   
[EVAL] batch:   83 | acc: 12.50%,  total acc: 60.79%   
[EVAL] batch:   84 | acc: 50.00%,  total acc: 60.66%   
[EVAL] batch:   85 | acc: 50.00%,  total acc: 60.54%   
[EVAL] batch:   86 | acc: 56.25%,  total acc: 60.49%   
[EVAL] batch:   87 | acc: 75.00%,  total acc: 60.65%   
[EVAL] batch:   88 | acc: 43.75%,  total acc: 60.46%   
[EVAL] batch:   89 | acc: 62.50%,  total acc: 60.49%   
[EVAL] batch:   90 | acc: 81.25%,  total acc: 60.71%   
[EVAL] batch:   91 | acc: 100.00%,  total acc: 61.14%   
[EVAL] batch:   92 | acc: 93.75%,  total acc: 61.49%   
[EVAL] batch:   93 | acc: 100.00%,  total acc: 61.90%   
[EVAL] batch:   94 | acc: 93.75%,  total acc: 62.24%   
[EVAL] batch:   95 | acc: 93.75%,  total acc: 62.57%   
[EVAL] batch:   96 | acc: 68.75%,  total acc: 62.63%   
[EVAL] batch:   97 | acc: 81.25%,  total acc: 62.82%   
[EVAL] batch:   98 | acc: 68.75%,  total acc: 62.88%   
[EVAL] batch:   99 | acc: 87.50%,  total acc: 63.12%   
[EVAL] batch:  100 | acc: 100.00%,  total acc: 63.49%   
[EVAL] batch:  101 | acc: 50.00%,  total acc: 63.36%   
[EVAL] batch:  102 | acc: 18.75%,  total acc: 62.92%   
[EVAL] batch:  103 | acc: 6.25%,  total acc: 62.38%   
[EVAL] batch:  104 | acc: 12.50%,  total acc: 61.90%   
[EVAL] batch:  105 | acc: 0.00%,  total acc: 61.32%   
[EVAL] batch:  106 | acc: 0.00%,  total acc: 60.75%   
cur_acc:  ['0.8485', '0.7167', '0.7455', '0.8125', '0.6250', '0.3203']
his_acc:  ['0.8485', '0.8245', '0.7520', '0.7171', '0.6631', '0.6075']
CurrentTrain: epoch  0, batch     0 | loss: 5.0271835
CurrentTrain: epoch  0, batch     1 | loss: 5.1891289
CurrentTrain: epoch  1, batch     0 | loss: 3.4788604
CurrentTrain: epoch  1, batch     1 | loss: 3.5748672
CurrentTrain: epoch  2, batch     0 | loss: 3.0131812
CurrentTrain: epoch  2, batch     1 | loss: 2.7366545
CurrentTrain: epoch  3, batch     0 | loss: 2.6255457
CurrentTrain: epoch  3, batch     1 | loss: 2.5250778
CurrentTrain: epoch  4, batch     0 | loss: 2.8087993
CurrentTrain: epoch  4, batch     1 | loss: 2.5104654
CurrentTrain: epoch  5, batch     0 | loss: 2.3437915
CurrentTrain: epoch  5, batch     1 | loss: 2.0334706
CurrentTrain: epoch  6, batch     0 | loss: 2.0724902
CurrentTrain: epoch  6, batch     1 | loss: 2.3381493
CurrentTrain: epoch  7, batch     0 | loss: 2.3151217
CurrentTrain: epoch  7, batch     1 | loss: 2.0992963
CurrentTrain: epoch  8, batch     0 | loss: 2.2568204
CurrentTrain: epoch  8, batch     1 | loss: 1.8774046
CurrentTrain: epoch  9, batch     0 | loss: 1.9762000
CurrentTrain: epoch  9, batch     1 | loss: 1.9561869
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: pamela gardner ahearn , who served nine years as chief of protocol at the us house of representatives after earlier experience with the state department 's office of protocol and as elizabeth taylor 's executive assistant , died march 26 of a heart attack at her home in alexandria , va .
Head Entity: pamela gardner ahearn
Tail Entity: heart attack
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: john smith, a renowned scientist known for his groundbreaking research in genetics, passed away on july 15 due to complications from pneumonia while receiving treatment at a local hospital.  
Head Entity: john smith  
Tail Entity: pneumonia  

Relation: person cause of death  
Context: the famous actor, robert jones, tragically lost his life in a car accident on february 10, leaving behind a legacy of memorable performances and a grieving family.  
Head Entity: robert jones  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The recent conference held by the Islamic Society of North America aimed to strengthen ties between various Muslim organizations and promote interfaith dialogue.  
Head Entity: Islamic Society of North America  
Tail Entity: Islam  

Relation: organization political religious affiliation  
Context: During the annual meeting, the Catholic Charities organization discussed its initiatives to support the local community and its commitment to the teachings of the Catholic Church.  
Head Entity: Catholic Charities  
Tail Entity: Catholicism  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Michael, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person other family  
Context: In her memoir, Lisa shared heartwarming stories about her grandmother, Evelyn, who played a significant role in her upbringing.  
Head Entity: Lisa  
Tail Entity: Evelyn  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: verity died wednesday , jan 3 , 2007 , in beaufort memorial hospital .
Head Entity: verity
Tail Entity: beaufort
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: after a long battle with illness, john passed away in the quiet town of springfield.  
Head Entity: john  
Tail Entity: springfield  

Relation: person city of death  
Context: the renowned author tragically died in a car accident on the streets of los angeles.  
Head Entity: the renowned author  
Tail Entity: los angeles  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 2.4677839
MixupTrain:  epoch  0, batch     1 | loss: 2.2937613
MixupTrain:  epoch  0, batch     2 | loss: 2.5902629
MixupTrain:  epoch  0, batch     3 | loss: 2.6484151
MixupTrain:  epoch  0, batch     4 | loss: 2.2206197
MixupTrain:  epoch  0, batch     5 | loss: 1.9353373
MixupTrain:  epoch  0, batch     6 | loss: 2.6300211
MixupTrain:  epoch  0, batch     7 | loss: 2.4537787
MixupTrain:  epoch  0, batch     8 | loss: 2.5450253
MixupTrain:  epoch  0, batch     9 | loss: 2.2414360
MixupTrain:  epoch  0, batch    10 | loss: 2.0206940
MixupTrain:  epoch  0, batch    11 | loss: 2.7674389
MixupTrain:  epoch  0, batch    12 | loss: 2.6396484
MixupTrain:  epoch  0, batch    13 | loss: 2.2434709
MixupTrain:  epoch  0, batch    14 | loss: 2.4548688
MixupTrain:  epoch  0, batch    15 | loss: 1.9836357
MemoryTrain:  epoch  0, batch     0 | loss: 2.0400486
MemoryTrain:  epoch  0, batch     1 | loss: 1.8277968
MemoryTrain:  epoch  0, batch     2 | loss: 2.6616788
MemoryTrain:  epoch  0, batch     3 | loss: 2.6377089
MemoryTrain:  epoch  0, batch     4 | loss: 3.1404009
MemoryTrain:  epoch  0, batch     5 | loss: 2.7661419
MemoryTrain:  epoch  0, batch     6 | loss: 2.0417478
MemoryTrain:  epoch  1, batch     0 | loss: 2.7022169
MemoryTrain:  epoch  1, batch     1 | loss: 1.8829770
MemoryTrain:  epoch  1, batch     2 | loss: 2.4554088
MemoryTrain:  epoch  1, batch     3 | loss: 1.8160902
MemoryTrain:  epoch  1, batch     4 | loss: 2.2861118
MemoryTrain:  epoch  1, batch     5 | loss: 2.6362443
MemoryTrain:  epoch  1, batch     6 | loss: 2.8692021
MemoryTrain:  epoch  2, batch     0 | loss: 1.8869174
MemoryTrain:  epoch  2, batch     1 | loss: 2.0264840
MemoryTrain:  epoch  2, batch     2 | loss: 2.2971375
MemoryTrain:  epoch  2, batch     3 | loss: 2.0197911
MemoryTrain:  epoch  2, batch     4 | loss: 2.0384140
MemoryTrain:  epoch  2, batch     5 | loss: 1.7319973
MemoryTrain:  epoch  2, batch     6 | loss: 1.9878681
MemoryTrain:  epoch  3, batch     0 | loss: 2.0258288
MemoryTrain:  epoch  3, batch     1 | loss: 2.2695665
MemoryTrain:  epoch  3, batch     2 | loss: 2.5076876
MemoryTrain:  epoch  3, batch     3 | loss: 2.0558376
MemoryTrain:  epoch  3, batch     4 | loss: 1.8801000
MemoryTrain:  epoch  3, batch     5 | loss: 1.5789161
MemoryTrain:  epoch  3, batch     6 | loss: 1.3046943
MemoryTrain:  epoch  4, batch     0 | loss: 1.7109829
MemoryTrain:  epoch  4, batch     1 | loss: 1.9154809
MemoryTrain:  epoch  4, batch     2 | loss: 1.8691233
MemoryTrain:  epoch  4, batch     3 | loss: 1.8404286
MemoryTrain:  epoch  4, batch     4 | loss: 2.0598960
MemoryTrain:  epoch  4, batch     5 | loss: 1.9960158
MemoryTrain:  epoch  4, batch     6 | loss: 1.5604479
MemoryTrain:  epoch  5, batch     0 | loss: 1.8920529
MemoryTrain:  epoch  5, batch     1 | loss: 1.6557205
MemoryTrain:  epoch  5, batch     2 | loss: 1.7713659
MemoryTrain:  epoch  5, batch     3 | loss: 1.6902337
MemoryTrain:  epoch  5, batch     4 | loss: 1.5311046
MemoryTrain:  epoch  5, batch     5 | loss: 1.6941512
MemoryTrain:  epoch  5, batch     6 | loss: 1.6922816
MemoryTrain:  epoch  6, batch     0 | loss: 1.8187594
MemoryTrain:  epoch  6, batch     1 | loss: 1.3418847
MemoryTrain:  epoch  6, batch     2 | loss: 1.9265256
MemoryTrain:  epoch  6, batch     3 | loss: 1.5059710
MemoryTrain:  epoch  6, batch     4 | loss: 1.3648556
MemoryTrain:  epoch  6, batch     5 | loss: 1.8573177
MemoryTrain:  epoch  6, batch     6 | loss: 1.6740507
MemoryTrain:  epoch  7, batch     0 | loss: 1.4444537
MemoryTrain:  epoch  7, batch     1 | loss: 1.5499732
MemoryTrain:  epoch  7, batch     2 | loss: 1.3945866
MemoryTrain:  epoch  7, batch     3 | loss: 1.5918775
MemoryTrain:  epoch  7, batch     4 | loss: 1.8818417
MemoryTrain:  epoch  7, batch     5 | loss: 1.5361738
MemoryTrain:  epoch  7, batch     6 | loss: 1.5909003
MemoryTrain:  epoch  8, batch     0 | loss: 1.5015490
MemoryTrain:  epoch  8, batch     1 | loss: 1.7252088
MemoryTrain:  epoch  8, batch     2 | loss: 1.6208670
MemoryTrain:  epoch  8, batch     3 | loss: 1.2782900
MemoryTrain:  epoch  8, batch     4 | loss: 1.4995495
MemoryTrain:  epoch  8, batch     5 | loss: 1.4910119
MemoryTrain:  epoch  8, batch     6 | loss: 1.4719409
MemoryTrain:  epoch  9, batch     0 | loss: 1.4328896
MemoryTrain:  epoch  9, batch     1 | loss: 1.5133153
MemoryTrain:  epoch  9, batch     2 | loss: 1.4062572
MemoryTrain:  epoch  9, batch     3 | loss: 1.4002385
MemoryTrain:  epoch  9, batch     4 | loss: 1.4078326
MemoryTrain:  epoch  9, batch     5 | loss: 1.2977754
MemoryTrain:  epoch  9, batch     6 | loss: 1.7476614
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   
[EVAL] batch:    1 | acc: 68.75%,  total acc: 53.12%   
[EVAL] batch:    2 | acc: 43.75%,  total acc: 50.00%   
[EVAL] batch:    3 | acc: 81.25%,  total acc: 57.81%   
[EVAL] batch:    4 | acc: 100.00%,  total acc: 66.25%   
[EVAL] batch:    5 | acc: 100.00%,  total acc: 71.88%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 75.89%   
[EVAL] batch:    7 | acc: 81.25%,  total acc: 76.56%   
[EVAL] batch:    8 | acc: 56.25%,  total acc: 74.31%   
[EVAL] batch:    9 | acc: 81.25%,  total acc: 75.00%   
[EVAL] batch:   10 | acc: 87.50%,  total acc: 76.14%   
[EVAL] batch:   11 | acc: 81.25%,  total acc: 76.56%   
[EVAL] batch:   12 | acc: 50.00%,  total acc: 74.52%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   
[EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   
[EVAL] batch:    2 | acc: 31.25%,  total acc: 14.58%   
[EVAL] batch:    3 | acc: 12.50%,  total acc: 14.06%   
[EVAL] batch:    4 | acc: 12.50%,  total acc: 13.75%   
[EVAL] batch:    5 | acc: 12.50%,  total acc: 13.54%   
[EVAL] batch:    6 | acc: 81.25%,  total acc: 23.21%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 32.81%   
[EVAL] batch:    8 | acc: 87.50%,  total acc: 38.89%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 43.75%   
[EVAL] batch:   10 | acc: 93.75%,  total acc: 48.30%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 51.56%   
[EVAL] batch:   12 | acc: 50.00%,  total acc: 51.44%   
[EVAL] batch:   13 | acc: 31.25%,  total acc: 50.00%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 51.67%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 51.95%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 53.31%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 53.82%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 54.28%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 55.94%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 58.04%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 59.94%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 61.68%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 63.02%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 64.50%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 65.87%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 66.90%   
[EVAL] batch:   27 | acc: 93.75%,  total acc: 67.86%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 68.97%   
[EVAL] batch:   29 | acc: 75.00%,  total acc: 69.17%   
[EVAL] batch:   30 | acc: 75.00%,  total acc: 69.35%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 70.12%   
[EVAL] batch:   32 | acc: 50.00%,  total acc: 69.51%   
[EVAL] batch:   33 | acc: 12.50%,  total acc: 67.83%   
[EVAL] batch:   34 | acc: 6.25%,  total acc: 66.07%   
[EVAL] batch:   35 | acc: 25.00%,  total acc: 64.93%   
[EVAL] batch:   36 | acc: 25.00%,  total acc: 63.85%   
[EVAL] batch:   37 | acc: 68.75%,  total acc: 63.98%   
[EVAL] batch:   38 | acc: 75.00%,  total acc: 64.26%   
[EVAL] batch:   39 | acc: 100.00%,  total acc: 65.16%   
[EVAL] batch:   40 | acc: 62.50%,  total acc: 65.09%   
[EVAL] batch:   41 | acc: 100.00%,  total acc: 65.92%   
[EVAL] batch:   42 | acc: 12.50%,  total acc: 64.68%   
[EVAL] batch:   43 | acc: 18.75%,  total acc: 63.64%   
[EVAL] batch:   44 | acc: 12.50%,  total acc: 62.50%   
[EVAL] batch:   45 | acc: 18.75%,  total acc: 61.55%   
[EVAL] batch:   46 | acc: 31.25%,  total acc: 60.90%   
[EVAL] batch:   47 | acc: 56.25%,  total acc: 60.81%   
[EVAL] batch:   48 | acc: 12.50%,  total acc: 59.82%   
[EVAL] batch:   49 | acc: 6.25%,  total acc: 58.75%   
[EVAL] batch:   50 | acc: 0.00%,  total acc: 57.60%   
[EVAL] batch:   51 | acc: 0.00%,  total acc: 56.49%   
[EVAL] batch:   52 | acc: 0.00%,  total acc: 55.42%   
[EVAL] batch:   53 | acc: 43.75%,  total acc: 55.21%   
[EVAL] batch:   54 | acc: 93.75%,  total acc: 55.91%   
[EVAL] batch:   55 | acc: 100.00%,  total acc: 56.70%   
[EVAL] batch:   56 | acc: 87.50%,  total acc: 57.24%   
[EVAL] batch:   57 | acc: 81.25%,  total acc: 57.65%   
[EVAL] batch:   58 | acc: 81.25%,  total acc: 58.05%   
[EVAL] batch:   59 | acc: 62.50%,  total acc: 58.13%   
[EVAL] batch:   60 | acc: 0.00%,  total acc: 57.17%   
[EVAL] batch:   61 | acc: 18.75%,  total acc: 56.55%   
[EVAL] batch:   62 | acc: 43.75%,  total acc: 56.35%   
[EVAL] batch:   63 | acc: 31.25%,  total acc: 55.96%   
[EVAL] batch:   64 | acc: 18.75%,  total acc: 55.38%   
[EVAL] batch:   65 | acc: 18.75%,  total acc: 54.83%   
[EVAL] batch:   66 | acc: 31.25%,  total acc: 54.48%   
[EVAL] batch:   67 | acc: 100.00%,  total acc: 55.15%   
[EVAL] batch:   68 | acc: 81.25%,  total acc: 55.53%   
[EVAL] batch:   69 | acc: 50.00%,  total acc: 55.45%   
[EVAL] batch:   70 | acc: 56.25%,  total acc: 55.46%   
[EVAL] batch:   71 | acc: 62.50%,  total acc: 55.56%   
[EVAL] batch:   72 | acc: 100.00%,  total acc: 56.16%   
[EVAL] batch:   73 | acc: 100.00%,  total acc: 56.76%   
[EVAL] batch:   74 | acc: 100.00%,  total acc: 57.33%   
[EVAL] batch:   75 | acc: 100.00%,  total acc: 57.89%   
[EVAL] batch:   76 | acc: 100.00%,  total acc: 58.44%   
[EVAL] batch:   77 | acc: 81.25%,  total acc: 58.73%   
[EVAL] batch:   78 | acc: 12.50%,  total acc: 58.15%   
[EVAL] batch:   79 | acc: 6.25%,  total acc: 57.50%   
[EVAL] batch:   80 | acc: 0.00%,  total acc: 56.79%   
[EVAL] batch:   81 | acc: 0.00%,  total acc: 56.10%   
[EVAL] batch:   82 | acc: 6.25%,  total acc: 55.50%   
[EVAL] batch:   83 | acc: 12.50%,  total acc: 54.99%   
[EVAL] batch:   84 | acc: 56.25%,  total acc: 55.00%   
[EVAL] batch:   85 | acc: 50.00%,  total acc: 54.94%   
[EVAL] batch:   86 | acc: 56.25%,  total acc: 54.96%   
[EVAL] batch:   87 | acc: 62.50%,  total acc: 55.04%   
[EVAL] batch:   88 | acc: 43.75%,  total acc: 54.92%   
[EVAL] batch:   89 | acc: 50.00%,  total acc: 54.86%   
[EVAL] batch:   90 | acc: 75.00%,  total acc: 55.08%   
[EVAL] batch:   91 | acc: 62.50%,  total acc: 55.16%   
[EVAL] batch:   92 | acc: 93.75%,  total acc: 55.58%   
[EVAL] batch:   93 | acc: 81.25%,  total acc: 55.85%   
[EVAL] batch:   94 | acc: 81.25%,  total acc: 56.12%   
[EVAL] batch:   95 | acc: 81.25%,  total acc: 56.38%   
[EVAL] batch:   96 | acc: 50.00%,  total acc: 56.31%   
[EVAL] batch:   97 | acc: 50.00%,  total acc: 56.25%   
[EVAL] batch:   98 | acc: 50.00%,  total acc: 56.19%   
[EVAL] batch:   99 | acc: 81.25%,  total acc: 56.44%   
[EVAL] batch:  100 | acc: 100.00%,  total acc: 56.87%   
[EVAL] batch:  101 | acc: 56.25%,  total acc: 56.86%   
[EVAL] batch:  102 | acc: 12.50%,  total acc: 56.43%   
[EVAL] batch:  103 | acc: 12.50%,  total acc: 56.01%   
[EVAL] batch:  104 | acc: 18.75%,  total acc: 55.65%   
[EVAL] batch:  105 | acc: 0.00%,  total acc: 55.13%   
[EVAL] batch:  106 | acc: 18.75%,  total acc: 54.79%   
[EVAL] batch:  107 | acc: 62.50%,  total acc: 54.86%   
[EVAL] batch:  108 | acc: 50.00%,  total acc: 54.82%   
[EVAL] batch:  109 | acc: 68.75%,  total acc: 54.94%   
[EVAL] batch:  110 | acc: 100.00%,  total acc: 55.35%   
[EVAL] batch:  111 | acc: 100.00%,  total acc: 55.75%   
[EVAL] batch:  112 | acc: 100.00%,  total acc: 56.14%   
[EVAL] batch:  113 | acc: 87.50%,  total acc: 56.41%   
[EVAL] batch:  114 | acc: 68.75%,  total acc: 56.52%   
[EVAL] batch:  115 | acc: 68.75%,  total acc: 56.63%   
[EVAL] batch:  116 | acc: 81.25%,  total acc: 56.84%   
[EVAL] batch:  117 | acc: 93.75%,  total acc: 57.15%   
[EVAL] batch:  118 | acc: 68.75%,  total acc: 57.25%   
cur_acc:  ['0.8485', '0.7167', '0.7455', '0.8125', '0.6250', '0.3203', '0.7452']
his_acc:  ['0.8485', '0.8245', '0.7520', '0.7171', '0.6631', '0.6075', '0.5725']
CurrentTrain: epoch  0, batch     0 | loss: 4.7452335
CurrentTrain: epoch  0, batch     1 | loss: 5.5817900
CurrentTrain: epoch  1, batch     0 | loss: 3.8873119
CurrentTrain: epoch  1, batch     1 | loss: 3.4960234
CurrentTrain: epoch  2, batch     0 | loss: 3.1317279
CurrentTrain: epoch  2, batch     1 | loss: 3.1054323
CurrentTrain: epoch  3, batch     0 | loss: 3.0348954
CurrentTrain: epoch  3, batch     1 | loss: 2.6398194
CurrentTrain: epoch  4, batch     0 | loss: 2.7173958
CurrentTrain: epoch  4, batch     1 | loss: 2.4815881
CurrentTrain: epoch  5, batch     0 | loss: 2.2789273
CurrentTrain: epoch  5, batch     1 | loss: 2.4331810
CurrentTrain: epoch  6, batch     0 | loss: 2.3137612
CurrentTrain: epoch  6, batch     1 | loss: 2.2108350
CurrentTrain: epoch  7, batch     0 | loss: 2.1163425
CurrentTrain: epoch  7, batch     1 | loss: 2.2398033
CurrentTrain: epoch  8, batch     0 | loss: 2.0347893
CurrentTrain: epoch  8, batch     1 | loss: 2.0064900
CurrentTrain: epoch  9, batch     0 | loss: 2.0372684
CurrentTrain: epoch  9, batch     1 | loss: 1.9693582
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the company was founded as a hobby in 1979 by the husband and wife team of tim and nina zagat , a pair of lawyers who started recuiting friends to provide their own ratings of the food , decor and service of restaurants they frequented .
Head Entity: zagat
Tail Entity: 1979
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1995, the tech startup was established by a group of engineers who aimed to revolutionize the software industry with innovative solutions.  
Head Entity: tech startup  
Tail Entity: 1995  

Relation: organization founded  
Context: The nonprofit organization was created in 2001 to support local artists and promote cultural events in the community.  
Head Entity: nonprofit organization  
Tail Entity: 2001  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: a page six gossip item in the new york post last week said morgenthau , who is 88 , is considering stepping down before the end of the year , and that gov
Head Entity: morgenthau
Tail Entity: 88
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: during the interview, she mentioned that her grandmother, who is 92 years old, still enjoys gardening every day.  
Head Entity: grandmother  
Tail Entity: 92  

Relation: person age  
Context: the documentary highlighted the life of a famous artist who passed away at the age of 75, leaving behind a legacy of incredible work.  
Head Entity: artist  
Tail Entity: 75  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in barcelona and spent her childhood there before moving to madrid.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during the summer of 1985, michael was born in new orleans, a city known for its vibrant culture.  
Head Entity: michael  
Tail Entity: new orleans  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: san diego 32 new orleans 37 american football : nfl result result of the nfl match between the san diego chargers of the afc west and the new orleans saints of the nfc south at wembley here sunday :
Head Entity: nfc south
Tail Entity: new orleans saints
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: the tech giant apple inc. has announced its new team of engineers who will be working on the latest software updates for their devices, including the talented group from the silicon valley startup.  
Head Entity: silicon valley startup  
Tail Entity: apple inc.  

Relation: organization members  
Context: the prestigious university harvard has welcomed a new cohort of students this year, including several from the renowned oxford university who are joining its research programs.  
Head Entity: oxford university  
Tail Entity: harvard  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of study and devotion, Maria finally embraced Buddhism, finding peace and purpose in its teachings.  
Head Entity: Maria  
Tail Entity: Buddhism  

Relation: person religion  
Context: During the festival, Ahmed proudly wore his traditional attire, celebrating his identity as a devoted Muslim.  
Head Entity: Ahmed  
Tail Entity: Muslim  
Mixup data size:  285
MixupTrain:  epoch  0, batch     0 | loss: 2.4193590
MixupTrain:  epoch  0, batch     1 | loss: 2.2836618
MixupTrain:  epoch  0, batch     2 | loss: 1.9582381
MixupTrain:  epoch  0, batch     3 | loss: 2.0597184
MixupTrain:  epoch  0, batch     4 | loss: 2.0023828
MixupTrain:  epoch  0, batch     5 | loss: 2.1277938
MixupTrain:  epoch  0, batch     6 | loss: 1.9009246
MixupTrain:  epoch  0, batch     7 | loss: 2.0652862
MixupTrain:  epoch  0, batch     8 | loss: 1.8481743
MixupTrain:  epoch  0, batch     9 | loss: 2.0786963
MixupTrain:  epoch  0, batch    10 | loss: 1.9108819
MixupTrain:  epoch  0, batch    11 | loss: 1.8293965
MixupTrain:  epoch  0, batch    12 | loss: 1.9040318
MixupTrain:  epoch  0, batch    13 | loss: 2.1600409
MixupTrain:  epoch  0, batch    14 | loss: 2.0442061
MixupTrain:  epoch  0, batch    15 | loss: 1.9800177
MixupTrain:  epoch  0, batch    16 | loss: 1.9235899
MixupTrain:  epoch  0, batch    17 | loss: 1.5699886
MemoryTrain:  epoch  0, batch     0 | loss: 1.7769182
MemoryTrain:  epoch  0, batch     1 | loss: 1.6978633
MemoryTrain:  epoch  0, batch     2 | loss: 1.9288647
MemoryTrain:  epoch  0, batch     3 | loss: 2.1918826
MemoryTrain:  epoch  0, batch     4 | loss: 2.7258589
MemoryTrain:  epoch  0, batch     5 | loss: 2.2141695
MemoryTrain:  epoch  0, batch     6 | loss: 2.0292726
MemoryTrain:  epoch  0, batch     7 | loss: 2.8298962
MemoryTrain:  epoch  1, batch     0 | loss: 1.6945994
MemoryTrain:  epoch  1, batch     1 | loss: 1.9443343
MemoryTrain:  epoch  1, batch     2 | loss: 2.1568980
MemoryTrain:  epoch  1, batch     3 | loss: 1.6320653
MemoryTrain:  epoch  1, batch     4 | loss: 1.6909615
MemoryTrain:  epoch  1, batch     5 | loss: 2.0126681
MemoryTrain:  epoch  1, batch     6 | loss: 1.7623326
MemoryTrain:  epoch  1, batch     7 | loss: 1.9054945
MemoryTrain:  epoch  2, batch     0 | loss: 1.6081294
MemoryTrain:  epoch  2, batch     1 | loss: 1.7052586
MemoryTrain:  epoch  2, batch     2 | loss: 1.5940812
MemoryTrain:  epoch  2, batch     3 | loss: 1.5726259
MemoryTrain:  epoch  2, batch     4 | loss: 1.4807228
MemoryTrain:  epoch  2, batch     5 | loss: 1.5008420
MemoryTrain:  epoch  2, batch     6 | loss: 1.5174787
MemoryTrain:  epoch  2, batch     7 | loss: 1.5561916
MemoryTrain:  epoch  3, batch     0 | loss: 1.6396742
MemoryTrain:  epoch  3, batch     1 | loss: 1.4001892
MemoryTrain:  epoch  3, batch     2 | loss: 1.4360087
MemoryTrain:  epoch  3, batch     3 | loss: 1.4737475
MemoryTrain:  epoch  3, batch     4 | loss: 1.4420166
MemoryTrain:  epoch  3, batch     5 | loss: 1.3734586
MemoryTrain:  epoch  3, batch     6 | loss: 1.6926800
MemoryTrain:  epoch  3, batch     7 | loss: 1.4923838
MemoryTrain:  epoch  4, batch     0 | loss: 1.4440672
MemoryTrain:  epoch  4, batch     1 | loss: 1.3097126
MemoryTrain:  epoch  4, batch     2 | loss: 1.5331805
MemoryTrain:  epoch  4, batch     3 | loss: 1.3396690
MemoryTrain:  epoch  4, batch     4 | loss: 1.4979721
MemoryTrain:  epoch  4, batch     5 | loss: 1.4144053
MemoryTrain:  epoch  4, batch     6 | loss: 1.3143599
MemoryTrain:  epoch  4, batch     7 | loss: 1.3015814
MemoryTrain:  epoch  5, batch     0 | loss: 1.3568665
MemoryTrain:  epoch  5, batch     1 | loss: 1.3160479
MemoryTrain:  epoch  5, batch     2 | loss: 1.3615731
MemoryTrain:  epoch  5, batch     3 | loss: 1.4504530
MemoryTrain:  epoch  5, batch     4 | loss: 1.4395564
MemoryTrain:  epoch  5, batch     5 | loss: 1.3338346
MemoryTrain:  epoch  5, batch     6 | loss: 1.4970653
MemoryTrain:  epoch  5, batch     7 | loss: 1.2435941
MemoryTrain:  epoch  6, batch     0 | loss: 1.3266973
MemoryTrain:  epoch  6, batch     1 | loss: 1.3069208
MemoryTrain:  epoch  6, batch     2 | loss: 1.4431086
MemoryTrain:  epoch  6, batch     3 | loss: 1.4109151
MemoryTrain:  epoch  6, batch     4 | loss: 1.3203163
MemoryTrain:  epoch  6, batch     5 | loss: 1.3030105
MemoryTrain:  epoch  6, batch     6 | loss: 1.3656373
MemoryTrain:  epoch  6, batch     7 | loss: 1.2250735
MemoryTrain:  epoch  7, batch     0 | loss: 1.3474216
MemoryTrain:  epoch  7, batch     1 | loss: 1.2574033
MemoryTrain:  epoch  7, batch     2 | loss: 1.3643268
MemoryTrain:  epoch  7, batch     3 | loss: 1.3101130
MemoryTrain:  epoch  7, batch     4 | loss: 1.3171077
MemoryTrain:  epoch  7, batch     5 | loss: 1.3472295
MemoryTrain:  epoch  7, batch     6 | loss: 1.3697653
MemoryTrain:  epoch  7, batch     7 | loss: 1.3087333
MemoryTrain:  epoch  8, batch     0 | loss: 1.2867639
MemoryTrain:  epoch  8, batch     1 | loss: 1.2953501
MemoryTrain:  epoch  8, batch     2 | loss: 1.3591892
MemoryTrain:  epoch  8, batch     3 | loss: 1.3155918
MemoryTrain:  epoch  8, batch     4 | loss: 1.3514328
MemoryTrain:  epoch  8, batch     5 | loss: 1.3324510
MemoryTrain:  epoch  8, batch     6 | loss: 1.3105586
MemoryTrain:  epoch  8, batch     7 | loss: 1.2754078
MemoryTrain:  epoch  9, batch     0 | loss: 1.3007462
MemoryTrain:  epoch  9, batch     1 | loss: 1.3709369
MemoryTrain:  epoch  9, batch     2 | loss: 1.3428187
MemoryTrain:  epoch  9, batch     3 | loss: 1.3348624
MemoryTrain:  epoch  9, batch     4 | loss: 1.2672787
MemoryTrain:  epoch  9, batch     5 | loss: 1.2987692
MemoryTrain:  epoch  9, batch     6 | loss: 1.3039654
MemoryTrain:  epoch  9, batch     7 | loss: 1.3181145
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   
[EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   
[EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   
[EVAL] batch:    4 | acc: 100.00%,  total acc: 98.75%   
[EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   
[EVAL] batch:    8 | acc: 75.00%,  total acc: 96.53%   
[EVAL] batch:    9 | acc: 56.25%,  total acc: 92.50%   
[EVAL] batch:   10 | acc: 68.75%,  total acc: 90.34%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 90.62%   
[EVAL] batch:   12 | acc: 68.75%,  total acc: 88.94%   
[EVAL] batch:   13 | acc: 56.25%,  total acc: 86.61%   
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   
[EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   
[EVAL] batch:    2 | acc: 37.50%,  total acc: 16.67%   
[EVAL] batch:    3 | acc: 12.50%,  total acc: 15.62%   
[EVAL] batch:    4 | acc: 12.50%,  total acc: 15.00%   
[EVAL] batch:    5 | acc: 12.50%,  total acc: 14.58%   
[EVAL] batch:    6 | acc: 81.25%,  total acc: 24.11%   
[EVAL] batch:    7 | acc: 93.75%,  total acc: 32.81%   
[EVAL] batch:    8 | acc: 87.50%,  total acc: 38.89%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 43.75%   
[EVAL] batch:   10 | acc: 93.75%,  total acc: 48.30%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 51.56%   
[EVAL] batch:   12 | acc: 43.75%,  total acc: 50.96%   
[EVAL] batch:   13 | acc: 31.25%,  total acc: 49.55%   
[EVAL] batch:   14 | acc: 56.25%,  total acc: 50.00%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 50.39%   
[EVAL] batch:   16 | acc: 68.75%,  total acc: 51.47%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 52.08%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 52.63%   
[EVAL] batch:   19 | acc: 75.00%,  total acc: 53.75%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 55.95%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 57.67%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 59.51%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 60.94%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 62.50%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 63.94%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 65.05%   
[EVAL] batch:   27 | acc: 93.75%,  total acc: 66.07%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 67.24%   
[EVAL] batch:   29 | acc: 75.00%,  total acc: 67.50%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 67.94%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 68.75%   
[EVAL] batch:   32 | acc: 43.75%,  total acc: 67.99%   
[EVAL] batch:   33 | acc: 12.50%,  total acc: 66.36%   
[EVAL] batch:   34 | acc: 12.50%,  total acc: 64.82%   
[EVAL] batch:   35 | acc: 18.75%,  total acc: 63.54%   
[EVAL] batch:   36 | acc: 18.75%,  total acc: 62.33%   
[EVAL] batch:   37 | acc: 68.75%,  total acc: 62.50%   
[EVAL] batch:   38 | acc: 75.00%,  total acc: 62.82%   
[EVAL] batch:   39 | acc: 100.00%,  total acc: 63.75%   
[EVAL] batch:   40 | acc: 75.00%,  total acc: 64.02%   
[EVAL] batch:   41 | acc: 100.00%,  total acc: 64.88%   
[EVAL] batch:   42 | acc: 0.00%,  total acc: 63.37%   
[EVAL] batch:   43 | acc: 0.00%,  total acc: 61.93%   
[EVAL] batch:   44 | acc: 0.00%,  total acc: 60.56%   
[EVAL] batch:   45 | acc: 0.00%,  total acc: 59.24%   
[EVAL] batch:   46 | acc: 25.00%,  total acc: 58.51%   
[EVAL] batch:   47 | acc: 50.00%,  total acc: 58.33%   
[EVAL] batch:   48 | acc: 12.50%,  total acc: 57.40%   
[EVAL] batch:   49 | acc: 6.25%,  total acc: 56.38%   
[EVAL] batch:   50 | acc: 0.00%,  total acc: 55.27%   
[EVAL] batch:   51 | acc: 0.00%,  total acc: 54.21%   
[EVAL] batch:   52 | acc: 0.00%,  total acc: 53.18%   
[EVAL] batch:   53 | acc: 43.75%,  total acc: 53.01%   
[EVAL] batch:   54 | acc: 93.75%,  total acc: 53.75%   
[EVAL] batch:   55 | acc: 100.00%,  total acc: 54.58%   
[EVAL] batch:   56 | acc: 87.50%,  total acc: 55.15%   
[EVAL] batch:   57 | acc: 81.25%,  total acc: 55.60%   
[EVAL] batch:   58 | acc: 81.25%,  total acc: 56.04%   
[EVAL] batch:   59 | acc: 68.75%,  total acc: 56.25%   
[EVAL] batch:   60 | acc: 6.25%,  total acc: 55.43%   
[EVAL] batch:   61 | acc: 0.00%,  total acc: 54.54%   
[EVAL] batch:   62 | acc: 12.50%,  total acc: 53.87%   
[EVAL] batch:   63 | acc: 0.00%,  total acc: 53.03%   
[EVAL] batch:   64 | acc: 0.00%,  total acc: 52.21%   
[EVAL] batch:   65 | acc: 0.00%,  total acc: 51.42%   
[EVAL] batch:   66 | acc: 18.75%,  total acc: 50.93%   
[EVAL] batch:   67 | acc: 100.00%,  total acc: 51.65%   
[EVAL] batch:   68 | acc: 81.25%,  total acc: 52.08%   
[EVAL] batch:   69 | acc: 50.00%,  total acc: 52.05%   
[EVAL] batch:   70 | acc: 62.50%,  total acc: 52.20%   
[EVAL] batch:   71 | acc: 62.50%,  total acc: 52.34%   
[EVAL] batch:   72 | acc: 100.00%,  total acc: 53.00%   
[EVAL] batch:   73 | acc: 100.00%,  total acc: 53.63%   
[EVAL] batch:   74 | acc: 100.00%,  total acc: 54.25%   
[EVAL] batch:   75 | acc: 100.00%,  total acc: 54.85%   
[EVAL] batch:   76 | acc: 100.00%,  total acc: 55.44%   
[EVAL] batch:   77 | acc: 75.00%,  total acc: 55.69%   
[EVAL] batch:   78 | acc: 6.25%,  total acc: 55.06%   
[EVAL] batch:   79 | acc: 6.25%,  total acc: 54.45%   
[EVAL] batch:   80 | acc: 18.75%,  total acc: 54.01%   
[EVAL] batch:   81 | acc: 0.00%,  total acc: 53.35%   
[EVAL] batch:   82 | acc: 0.00%,  total acc: 52.71%   
[EVAL] batch:   83 | acc: 25.00%,  total acc: 52.38%   
[EVAL] batch:   84 | acc: 18.75%,  total acc: 51.99%   
[EVAL] batch:   85 | acc: 37.50%,  total acc: 51.82%   
[EVAL] batch:   86 | acc: 25.00%,  total acc: 51.51%   
[EVAL] batch:   87 | acc: 50.00%,  total acc: 51.49%   
[EVAL] batch:   88 | acc: 25.00%,  total acc: 51.19%   
[EVAL] batch:   89 | acc: 18.75%,  total acc: 50.83%   
[EVAL] batch:   90 | acc: 56.25%,  total acc: 50.89%   
[EVAL] batch:   91 | acc: 75.00%,  total acc: 51.15%   
[EVAL] batch:   92 | acc: 87.50%,  total acc: 51.55%   
[EVAL] batch:   93 | acc: 81.25%,  total acc: 51.86%   
[EVAL] batch:   94 | acc: 75.00%,  total acc: 52.11%   
[EVAL] batch:   95 | acc: 62.50%,  total acc: 52.21%   
[EVAL] batch:   96 | acc: 43.75%,  total acc: 52.13%   
[EVAL] batch:   97 | acc: 50.00%,  total acc: 52.10%   
[EVAL] batch:   98 | acc: 43.75%,  total acc: 52.02%   
[EVAL] batch:   99 | acc: 75.00%,  total acc: 52.25%   
[EVAL] batch:  100 | acc: 100.00%,  total acc: 52.72%   
[EVAL] batch:  101 | acc: 25.00%,  total acc: 52.45%   
[EVAL] batch:  102 | acc: 18.75%,  total acc: 52.12%   
[EVAL] batch:  103 | acc: 6.25%,  total acc: 51.68%   
[EVAL] batch:  104 | acc: 18.75%,  total acc: 51.37%   
[EVAL] batch:  105 | acc: 0.00%,  total acc: 50.88%   
[EVAL] batch:  106 | acc: 25.00%,  total acc: 50.64%   
[EVAL] batch:  107 | acc: 68.75%,  total acc: 50.81%   
[EVAL] batch:  108 | acc: 50.00%,  total acc: 50.80%   
[EVAL] batch:  109 | acc: 56.25%,  total acc: 50.85%   
[EVAL] batch:  110 | acc: 87.50%,  total acc: 51.18%   
[EVAL] batch:  111 | acc: 100.00%,  total acc: 51.62%   
[EVAL] batch:  112 | acc: 100.00%,  total acc: 52.05%   
[EVAL] batch:  113 | acc: 87.50%,  total acc: 52.36%   
[EVAL] batch:  114 | acc: 75.00%,  total acc: 52.55%   
[EVAL] batch:  115 | acc: 68.75%,  total acc: 52.69%   
[EVAL] batch:  116 | acc: 75.00%,  total acc: 52.88%   
[EVAL] batch:  117 | acc: 93.75%,  total acc: 53.23%   
[EVAL] batch:  118 | acc: 81.25%,  total acc: 53.47%   
[EVAL] batch:  119 | acc: 100.00%,  total acc: 53.85%   
[EVAL] batch:  120 | acc: 93.75%,  total acc: 54.18%   
[EVAL] batch:  121 | acc: 100.00%,  total acc: 54.56%   
[EVAL] batch:  122 | acc: 100.00%,  total acc: 54.93%   
[EVAL] batch:  123 | acc: 100.00%,  total acc: 55.29%   
[EVAL] batch:  124 | acc: 100.00%,  total acc: 55.65%   
[EVAL] batch:  125 | acc: 100.00%,  total acc: 56.00%   
[EVAL] batch:  126 | acc: 100.00%,  total acc: 56.35%   
[EVAL] batch:  127 | acc: 68.75%,  total acc: 56.45%   
[EVAL] batch:  128 | acc: 62.50%,  total acc: 56.49%   
[EVAL] batch:  129 | acc: 68.75%,  total acc: 56.59%   
[EVAL] batch:  130 | acc: 87.50%,  total acc: 56.82%   
[EVAL] batch:  131 | acc: 75.00%,  total acc: 56.96%   
[EVAL] batch:  132 | acc: 43.75%,  total acc: 56.86%   
cur_acc:  ['0.8485', '0.7167', '0.7455', '0.8125', '0.6250', '0.3203', '0.7452', '0.8661']
his_acc:  ['0.8485', '0.8245', '0.7520', '0.7171', '0.6631', '0.6075', '0.5725', '0.5686']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.2682028
CurrentTrain: epoch  0, batch     1 | loss: 13.0933208
CurrentTrain: epoch  0, batch     2 | loss: 13.0878067
CurrentTrain: epoch  0, batch     3 | loss: 13.0236931
CurrentTrain: epoch  0, batch     4 | loss: 12.7643566
CurrentTrain: epoch  0, batch     5 | loss: 12.7809792
CurrentTrain: epoch  0, batch     6 | loss: 12.6308270
CurrentTrain: epoch  0, batch     7 | loss: 12.4567947
CurrentTrain: epoch  0, batch     8 | loss: 12.2349215
CurrentTrain: epoch  0, batch     9 | loss: 12.1776199
CurrentTrain: epoch  0, batch    10 | loss: 11.8927879
CurrentTrain: epoch  0, batch    11 | loss: 11.9794540
CurrentTrain: epoch  0, batch    12 | loss: 11.8824511
CurrentTrain: epoch  0, batch    13 | loss: 11.5990791
CurrentTrain: epoch  0, batch    14 | loss: 11.8521128
CurrentTrain: epoch  0, batch    15 | loss: 11.5746851
CurrentTrain: epoch  0, batch    16 | loss: 11.0346622
CurrentTrain: epoch  0, batch    17 | loss: 11.2048721
CurrentTrain: epoch  0, batch    18 | loss: 11.4685040
CurrentTrain: epoch  0, batch    19 | loss: 10.9724512
CurrentTrain: epoch  0, batch    20 | loss: 11.2618542
CurrentTrain: epoch  0, batch    21 | loss: 10.7493153
CurrentTrain: epoch  0, batch    22 | loss: 11.4645634
CurrentTrain: epoch  0, batch    23 | loss: 10.9149370
CurrentTrain: epoch  0, batch    24 | loss: 10.9730158
CurrentTrain: epoch  0, batch    25 | loss: 11.0047092
CurrentTrain: epoch  0, batch    26 | loss: 11.2320576
CurrentTrain: epoch  0, batch    27 | loss: 11.3868599
CurrentTrain: epoch  0, batch    28 | loss: 10.8486662
CurrentTrain: epoch  0, batch    29 | loss: 10.4812479
CurrentTrain: epoch  0, batch    30 | loss: 9.7166901
CurrentTrain: epoch  0, batch    31 | loss: 10.7381659
CurrentTrain: epoch  0, batch    32 | loss: 10.2534990
CurrentTrain: epoch  0, batch    33 | loss: 10.4890757
CurrentTrain: epoch  0, batch    34 | loss: 10.9087257
CurrentTrain: epoch  0, batch    35 | loss: 10.0175438
CurrentTrain: epoch  0, batch    36 | loss: 10.8105221
CurrentTrain: epoch  0, batch    37 | loss: 9.7343359
CurrentTrain: epoch  1, batch     0 | loss: 9.4949503
CurrentTrain: epoch  1, batch     1 | loss: 9.7751884
CurrentTrain: epoch  1, batch     2 | loss: 9.5057669
CurrentTrain: epoch  1, batch     3 | loss: 9.6634998
CurrentTrain: epoch  1, batch     4 | loss: 9.2961998
CurrentTrain: epoch  1, batch     5 | loss: 9.7613811
CurrentTrain: epoch  1, batch     6 | loss: 9.7816362
CurrentTrain: epoch  1, batch     7 | loss: 9.7923203
CurrentTrain: epoch  1, batch     8 | loss: 9.9789295
CurrentTrain: epoch  1, batch     9 | loss: 8.9172630
CurrentTrain: epoch  1, batch    10 | loss: 8.9472752
CurrentTrain: epoch  1, batch    11 | loss: 9.8300762
CurrentTrain: epoch  1, batch    12 | loss: 9.4374094
CurrentTrain: epoch  1, batch    13 | loss: 9.3524952
CurrentTrain: epoch  1, batch    14 | loss: 9.4018135
CurrentTrain: epoch  1, batch    15 | loss: 9.3787766
CurrentTrain: epoch  1, batch    16 | loss: 10.0529652
CurrentTrain: epoch  1, batch    17 | loss: 9.1686668
CurrentTrain: epoch  1, batch    18 | loss: 9.4836922
CurrentTrain: epoch  1, batch    19 | loss: 8.7339859
CurrentTrain: epoch  1, batch    20 | loss: 9.6778250
CurrentTrain: epoch  1, batch    21 | loss: 9.3327427
CurrentTrain: epoch  1, batch    22 | loss: 9.8055019
CurrentTrain: epoch  1, batch    23 | loss: 8.5588760
CurrentTrain: epoch  1, batch    24 | loss: 9.1059418
CurrentTrain: epoch  1, batch    25 | loss: 9.5784016
CurrentTrain: epoch  1, batch    26 | loss: 9.3020754
CurrentTrain: epoch  1, batch    27 | loss: 9.0042706
CurrentTrain: epoch  1, batch    28 | loss: 8.3692160
CurrentTrain: epoch  1, batch    29 | loss: 9.4745779
CurrentTrain: epoch  1, batch    30 | loss: 8.0868559
CurrentTrain: epoch  1, batch    31 | loss: 8.4890022
CurrentTrain: epoch  1, batch    32 | loss: 8.5102634
CurrentTrain: epoch  1, batch    33 | loss: 9.6855850
CurrentTrain: epoch  1, batch    34 | loss: 9.3174877
CurrentTrain: epoch  1, batch    35 | loss: 8.4596119
CurrentTrain: epoch  1, batch    36 | loss: 8.7135315
CurrentTrain: epoch  1, batch    37 | loss: 7.8472991
CurrentTrain: epoch  2, batch     0 | loss: 8.6591101
CurrentTrain: epoch  2, batch     1 | loss: 8.5160046
CurrentTrain: epoch  2, batch     2 | loss: 8.8566761
CurrentTrain: epoch  2, batch     3 | loss: 8.3328781
CurrentTrain: epoch  2, batch     4 | loss: 8.1284523
CurrentTrain: epoch  2, batch     5 | loss: 8.0909481
CurrentTrain: epoch  2, batch     6 | loss: 8.4942913
CurrentTrain: epoch  2, batch     7 | loss: 8.3335018
CurrentTrain: epoch  2, batch     8 | loss: 7.9357600
CurrentTrain: epoch  2, batch     9 | loss: 8.2886410
CurrentTrain: epoch  2, batch    10 | loss: 8.0493755
CurrentTrain: epoch  2, batch    11 | loss: 8.1082096
CurrentTrain: epoch  2, batch    12 | loss: 7.0983763
CurrentTrain: epoch  2, batch    13 | loss: 9.2671480
CurrentTrain: epoch  2, batch    14 | loss: 7.7905002
CurrentTrain: epoch  2, batch    15 | loss: 8.7854996
CurrentTrain: epoch  2, batch    16 | loss: 7.6598110
CurrentTrain: epoch  2, batch    17 | loss: 8.3406601
CurrentTrain: epoch  2, batch    18 | loss: 7.8804026
CurrentTrain: epoch  2, batch    19 | loss: 7.0774875
CurrentTrain: epoch  2, batch    20 | loss: 8.8275223
CurrentTrain: epoch  2, batch    21 | loss: 7.2086072
CurrentTrain: epoch  2, batch    22 | loss: 7.9015913
CurrentTrain: epoch  2, batch    23 | loss: 7.1815968
CurrentTrain: epoch  2, batch    24 | loss: 8.1460228
CurrentTrain: epoch  2, batch    25 | loss: 7.9150925
CurrentTrain: epoch  2, batch    26 | loss: 8.0166416
CurrentTrain: epoch  2, batch    27 | loss: 8.0284653
CurrentTrain: epoch  2, batch    28 | loss: 7.7386155
CurrentTrain: epoch  2, batch    29 | loss: 7.6893787
CurrentTrain: epoch  2, batch    30 | loss: 8.0475273
CurrentTrain: epoch  2, batch    31 | loss: 8.2533426
CurrentTrain: epoch  2, batch    32 | loss: 7.5126438
CurrentTrain: epoch  2, batch    33 | loss: 7.7352524
CurrentTrain: epoch  2, batch    34 | loss: 6.9081059
CurrentTrain: epoch  2, batch    35 | loss: 7.9064684
CurrentTrain: epoch  2, batch    36 | loss: 7.4949026
CurrentTrain: epoch  2, batch    37 | loss: 7.4056935
CurrentTrain: epoch  3, batch     0 | loss: 7.5502605
CurrentTrain: epoch  3, batch     1 | loss: 7.6868124
CurrentTrain: epoch  3, batch     2 | loss: 7.0331764
CurrentTrain: epoch  3, batch     3 | loss: 7.1300154
CurrentTrain: epoch  3, batch     4 | loss: 7.1935730
CurrentTrain: epoch  3, batch     5 | loss: 6.9708996
CurrentTrain: epoch  3, batch     6 | loss: 7.4004464
CurrentTrain: epoch  3, batch     7 | loss: 7.6522245
CurrentTrain: epoch  3, batch     8 | loss: 7.3237104
CurrentTrain: epoch  3, batch     9 | loss: 7.2262750
CurrentTrain: epoch  3, batch    10 | loss: 7.7437401
CurrentTrain: epoch  3, batch    11 | loss: 7.2410254
CurrentTrain: epoch  3, batch    12 | loss: 6.7352762
CurrentTrain: epoch  3, batch    13 | loss: 6.9624262
CurrentTrain: epoch  3, batch    14 | loss: 7.4638524
CurrentTrain: epoch  3, batch    15 | loss: 7.0077219
CurrentTrain: epoch  3, batch    16 | loss: 7.7163110
CurrentTrain: epoch  3, batch    17 | loss: 7.8271260
CurrentTrain: epoch  3, batch    18 | loss: 7.2315540
CurrentTrain: epoch  3, batch    19 | loss: 7.6387939
CurrentTrain: epoch  3, batch    20 | loss: 7.0409651
CurrentTrain: epoch  3, batch    21 | loss: 6.9407854
CurrentTrain: epoch  3, batch    22 | loss: 7.3545780
CurrentTrain: epoch  3, batch    23 | loss: 7.4161248
CurrentTrain: epoch  3, batch    24 | loss: 7.3660421
CurrentTrain: epoch  3, batch    25 | loss: 7.2249436
CurrentTrain: epoch  3, batch    26 | loss: 7.4408689
CurrentTrain: epoch  3, batch    27 | loss: 7.3086147
CurrentTrain: epoch  3, batch    28 | loss: 6.7126174
CurrentTrain: epoch  3, batch    29 | loss: 6.6206470
CurrentTrain: epoch  3, batch    30 | loss: 7.6012177
CurrentTrain: epoch  3, batch    31 | loss: 6.0476284
CurrentTrain: epoch  3, batch    32 | loss: 7.3207169
CurrentTrain: epoch  3, batch    33 | loss: 7.8759818
CurrentTrain: epoch  3, batch    34 | loss: 7.5803833
CurrentTrain: epoch  3, batch    35 | loss: 7.6193089
CurrentTrain: epoch  3, batch    36 | loss: 7.0442085
CurrentTrain: epoch  3, batch    37 | loss: 6.2292442
CurrentTrain: epoch  4, batch     0 | loss: 6.3507710
CurrentTrain: epoch  4, batch     1 | loss: 6.8717318
CurrentTrain: epoch  4, batch     2 | loss: 6.1691160
CurrentTrain: epoch  4, batch     3 | loss: 7.1547527
CurrentTrain: epoch  4, batch     4 | loss: 7.0332093
CurrentTrain: epoch  4, batch     5 | loss: 6.9546027
CurrentTrain: epoch  4, batch     6 | loss: 7.3026671
CurrentTrain: epoch  4, batch     7 | loss: 6.7561512
CurrentTrain: epoch  4, batch     8 | loss: 6.3626938
CurrentTrain: epoch  4, batch     9 | loss: 5.9705415
CurrentTrain: epoch  4, batch    10 | loss: 7.1599388
CurrentTrain: epoch  4, batch    11 | loss: 6.8205853
CurrentTrain: epoch  4, batch    12 | loss: 6.7085762
CurrentTrain: epoch  4, batch    13 | loss: 6.4350605
CurrentTrain: epoch  4, batch    14 | loss: 7.1585188
CurrentTrain: epoch  4, batch    15 | loss: 7.0268898
CurrentTrain: epoch  4, batch    16 | loss: 6.6876335
CurrentTrain: epoch  4, batch    17 | loss: 6.5103369
CurrentTrain: epoch  4, batch    18 | loss: 6.2919135
CurrentTrain: epoch  4, batch    19 | loss: 8.1283998
CurrentTrain: epoch  4, batch    20 | loss: 6.2282248
CurrentTrain: epoch  4, batch    21 | loss: 7.6770711
CurrentTrain: epoch  4, batch    22 | loss: 7.0723958
CurrentTrain: epoch  4, batch    23 | loss: 6.6995802
CurrentTrain: epoch  4, batch    24 | loss: 6.5975380
CurrentTrain: epoch  4, batch    25 | loss: 6.8809443
CurrentTrain: epoch  4, batch    26 | loss: 6.1172590
CurrentTrain: epoch  4, batch    27 | loss: 7.5736184
CurrentTrain: epoch  4, batch    28 | loss: 6.2344346
CurrentTrain: epoch  4, batch    29 | loss: 5.8176165
CurrentTrain: epoch  4, batch    30 | loss: 6.7208443
CurrentTrain: epoch  4, batch    31 | loss: 6.0927563
CurrentTrain: epoch  4, batch    32 | loss: 7.1343479
CurrentTrain: epoch  4, batch    33 | loss: 7.3564253
CurrentTrain: epoch  4, batch    34 | loss: 6.3996563
CurrentTrain: epoch  4, batch    35 | loss: 5.7462215
CurrentTrain: epoch  4, batch    36 | loss: 6.0002823
CurrentTrain: epoch  4, batch    37 | loss: 6.4169559
CurrentTrain: epoch  5, batch     0 | loss: 6.5005054
CurrentTrain: epoch  5, batch     1 | loss: 6.6245594
CurrentTrain: epoch  5, batch     2 | loss: 6.2321262
CurrentTrain: epoch  5, batch     3 | loss: 6.6826682
CurrentTrain: epoch  5, batch     4 | loss: 6.4959974
CurrentTrain: epoch  5, batch     5 | loss: 6.0980768
CurrentTrain: epoch  5, batch     6 | loss: 7.6613312
CurrentTrain: epoch  5, batch     7 | loss: 6.6093235
CurrentTrain: epoch  5, batch     8 | loss: 7.3567181
CurrentTrain: epoch  5, batch     9 | loss: 6.9826593
CurrentTrain: epoch  5, batch    10 | loss: 6.1856890
CurrentTrain: epoch  5, batch    11 | loss: 6.2126732
CurrentTrain: epoch  5, batch    12 | loss: 6.7282085
CurrentTrain: epoch  5, batch    13 | loss: 5.6110892
CurrentTrain: epoch  5, batch    14 | loss: 5.8875833
CurrentTrain: epoch  5, batch    15 | loss: 6.3967705
CurrentTrain: epoch  5, batch    16 | loss: 5.8213325
CurrentTrain: epoch  5, batch    17 | loss: 6.8180103
CurrentTrain: epoch  5, batch    18 | loss: 5.3769403
CurrentTrain: epoch  5, batch    19 | loss: 6.1024227
CurrentTrain: epoch  5, batch    20 | loss: 6.2419968
CurrentTrain: epoch  5, batch    21 | loss: 6.1183743
CurrentTrain: epoch  5, batch    22 | loss: 5.5581217
CurrentTrain: epoch  5, batch    23 | loss: 6.3434334
CurrentTrain: epoch  5, batch    24 | loss: 6.0035896
CurrentTrain: epoch  5, batch    25 | loss: 5.8085289
CurrentTrain: epoch  5, batch    26 | loss: 5.6723747
CurrentTrain: epoch  5, batch    27 | loss: 6.5641756
CurrentTrain: epoch  5, batch    28 | loss: 5.8653288
CurrentTrain: epoch  5, batch    29 | loss: 5.6430864
CurrentTrain: epoch  5, batch    30 | loss: 6.6284904
CurrentTrain: epoch  5, batch    31 | loss: 5.9902020
CurrentTrain: epoch  5, batch    32 | loss: 5.4970360
CurrentTrain: epoch  5, batch    33 | loss: 6.4432535
CurrentTrain: epoch  5, batch    34 | loss: 5.9563041
CurrentTrain: epoch  5, batch    35 | loss: 6.6852922
CurrentTrain: epoch  5, batch    36 | loss: 5.7734299
CurrentTrain: epoch  5, batch    37 | loss: 6.4657564
CurrentTrain: epoch  6, batch     0 | loss: 6.2385921
CurrentTrain: epoch  6, batch     1 | loss: 6.6610050
CurrentTrain: epoch  6, batch     2 | loss: 6.1200981
CurrentTrain: epoch  6, batch     3 | loss: 6.1058779
CurrentTrain: epoch  6, batch     4 | loss: 5.6425867
CurrentTrain: epoch  6, batch     5 | loss: 5.6450958
CurrentTrain: epoch  6, batch     6 | loss: 5.9335365
CurrentTrain: epoch  6, batch     7 | loss: 5.8863420
CurrentTrain: epoch  6, batch     8 | loss: 5.2909064
CurrentTrain: epoch  6, batch     9 | loss: 6.1034918
CurrentTrain: epoch  6, batch    10 | loss: 5.5732803
CurrentTrain: epoch  6, batch    11 | loss: 6.0883975
CurrentTrain: epoch  6, batch    12 | loss: 5.9371624
CurrentTrain: epoch  6, batch    13 | loss: 5.7097836
CurrentTrain: epoch  6, batch    14 | loss: 5.9721575
CurrentTrain: epoch  6, batch    15 | loss: 5.5966377
CurrentTrain: epoch  6, batch    16 | loss: 5.4707227
CurrentTrain: epoch  6, batch    17 | loss: 5.4308615
CurrentTrain: epoch  6, batch    18 | loss: 5.7126069
CurrentTrain: epoch  6, batch    19 | loss: 6.7346950
CurrentTrain: epoch  6, batch    20 | loss: 5.2895675
CurrentTrain: epoch  6, batch    21 | loss: 5.4977102
CurrentTrain: epoch  6, batch    22 | loss: 5.5665183
CurrentTrain: epoch  6, batch    23 | loss: 6.1834378
CurrentTrain: epoch  6, batch    24 | loss: 5.3899202
CurrentTrain: epoch  6, batch    25 | loss: 5.6860099
CurrentTrain: epoch  6, batch    26 | loss: 5.7650785
CurrentTrain: epoch  6, batch    27 | loss: 5.4308243
CurrentTrain: epoch  6, batch    28 | loss: 5.7541571
CurrentTrain: epoch  6, batch    29 | loss: 5.4541264
CurrentTrain: epoch  6, batch    30 | loss: 7.0663776
CurrentTrain: epoch  6, batch    31 | loss: 6.2547922
CurrentTrain: epoch  6, batch    32 | loss: 5.6881728
CurrentTrain: epoch  6, batch    33 | loss: 6.3720641
CurrentTrain: epoch  6, batch    34 | loss: 5.2157993
CurrentTrain: epoch  6, batch    35 | loss: 5.7982483
CurrentTrain: epoch  6, batch    36 | loss: 5.5026140
CurrentTrain: epoch  6, batch    37 | loss: 5.4650679
CurrentTrain: epoch  7, batch     0 | loss: 6.0455446
CurrentTrain: epoch  7, batch     1 | loss: 5.2312860
CurrentTrain: epoch  7, batch     2 | loss: 5.8964963
CurrentTrain: epoch  7, batch     3 | loss: 5.4697633
CurrentTrain: epoch  7, batch     4 | loss: 5.8462610
CurrentTrain: epoch  7, batch     5 | loss: 5.5940747
CurrentTrain: epoch  7, batch     6 | loss: 5.4579525
CurrentTrain: epoch  7, batch     7 | loss: 5.6283836
CurrentTrain: epoch  7, batch     8 | loss: 5.2654214
CurrentTrain: epoch  7, batch     9 | loss: 6.4472637
CurrentTrain: epoch  7, batch    10 | loss: 5.7283077
CurrentTrain: epoch  7, batch    11 | loss: 5.6037030
CurrentTrain: epoch  7, batch    12 | loss: 5.4789987
CurrentTrain: epoch  7, batch    13 | loss: 5.4050941
CurrentTrain: epoch  7, batch    14 | loss: 5.6642709
CurrentTrain: epoch  7, batch    15 | loss: 5.7402802
CurrentTrain: epoch  7, batch    16 | loss: 5.3819113
CurrentTrain: epoch  7, batch    17 | loss: 5.7242947
CurrentTrain: epoch  7, batch    18 | loss: 5.9516926
CurrentTrain: epoch  7, batch    19 | loss: 5.8688440
CurrentTrain: epoch  7, batch    20 | loss: 5.1834021
CurrentTrain: epoch  7, batch    21 | loss: 5.6170588
CurrentTrain: epoch  7, batch    22 | loss: 5.9021997
CurrentTrain: epoch  7, batch    23 | loss: 6.0203385
CurrentTrain: epoch  7, batch    24 | loss: 5.1994820
CurrentTrain: epoch  7, batch    25 | loss: 5.6396389
CurrentTrain: epoch  7, batch    26 | loss: 5.7764664
CurrentTrain: epoch  7, batch    27 | loss: 5.4208050
CurrentTrain: epoch  7, batch    28 | loss: 5.5461178
CurrentTrain: epoch  7, batch    29 | loss: 5.5639653
CurrentTrain: epoch  7, batch    30 | loss: 5.4745259
CurrentTrain: epoch  7, batch    31 | loss: 5.4753766
CurrentTrain: epoch  7, batch    32 | loss: 5.3966851
CurrentTrain: epoch  7, batch    33 | loss: 5.8452139
CurrentTrain: epoch  7, batch    34 | loss: 5.3994880
CurrentTrain: epoch  7, batch    35 | loss: 5.4974184
CurrentTrain: epoch  7, batch    36 | loss: 5.2505689
CurrentTrain: epoch  7, batch    37 | loss: 5.9266319
CurrentTrain: epoch  8, batch     0 | loss: 5.4807868
CurrentTrain: epoch  8, batch     1 | loss: 5.2170625
CurrentTrain: epoch  8, batch     2 | loss: 5.7362967
CurrentTrain: epoch  8, batch     3 | loss: 5.2331939
CurrentTrain: epoch  8, batch     4 | loss: 5.4739323
CurrentTrain: epoch  8, batch     5 | loss: 5.3554459
CurrentTrain: epoch  8, batch     6 | loss: 5.3668261
CurrentTrain: epoch  8, batch     7 | loss: 5.4515343
CurrentTrain: epoch  8, batch     8 | loss: 5.6242166
CurrentTrain: epoch  8, batch     9 | loss: 5.0971098
CurrentTrain: epoch  8, batch    10 | loss: 5.2461777
CurrentTrain: epoch  8, batch    11 | loss: 5.1329880
CurrentTrain: epoch  8, batch    12 | loss: 5.2317686
CurrentTrain: epoch  8, batch    13 | loss: 5.2945514
CurrentTrain: epoch  8, batch    14 | loss: 5.3048677
CurrentTrain: epoch  8, batch    15 | loss: 5.2083397
CurrentTrain: epoch  8, batch    16 | loss: 5.2716694
CurrentTrain: epoch  8, batch    17 | loss: 5.0986443
CurrentTrain: epoch  8, batch    18 | loss: 5.3520827
CurrentTrain: epoch  8, batch    19 | loss: 5.1180696
CurrentTrain: epoch  8, batch    20 | loss: 5.0936995
CurrentTrain: epoch  8, batch    21 | loss: 5.4467106
CurrentTrain: epoch  8, batch    22 | loss: 5.8207731
CurrentTrain: epoch  8, batch    23 | loss: 5.8701038
CurrentTrain: epoch  8, batch    24 | loss: 5.0662179
CurrentTrain: epoch  8, batch    25 | loss: 5.0558357
CurrentTrain: epoch  8, batch    26 | loss: 5.0491705
CurrentTrain: epoch  8, batch    27 | loss: 5.5055842
CurrentTrain: epoch  8, batch    28 | loss: 5.0275164
CurrentTrain: epoch  8, batch    29 | loss: 5.4349270
CurrentTrain: epoch  8, batch    30 | loss: 4.9036736
CurrentTrain: epoch  8, batch    31 | loss: 5.5322943
CurrentTrain: epoch  8, batch    32 | loss: 5.1834602
CurrentTrain: epoch  8, batch    33 | loss: 5.1148090
CurrentTrain: epoch  8, batch    34 | loss: 4.9252024
CurrentTrain: epoch  8, batch    35 | loss: 5.3729506
CurrentTrain: epoch  8, batch    36 | loss: 5.0505590
CurrentTrain: epoch  8, batch    37 | loss: 4.9014091
CurrentTrain: epoch  9, batch     0 | loss: 4.9456244
CurrentTrain: epoch  9, batch     1 | loss: 5.0325856
CurrentTrain: epoch  9, batch     2 | loss: 5.0854564
CurrentTrain: epoch  9, batch     3 | loss: 5.2626605
CurrentTrain: epoch  9, batch     4 | loss: 5.0259857
CurrentTrain: epoch  9, batch     5 | loss: 5.1120443
CurrentTrain: epoch  9, batch     6 | loss: 5.1024122
CurrentTrain: epoch  9, batch     7 | loss: 4.8999977
CurrentTrain: epoch  9, batch     8 | loss: 4.9642491
CurrentTrain: epoch  9, batch     9 | loss: 5.4170318
CurrentTrain: epoch  9, batch    10 | loss: 4.9413309
CurrentTrain: epoch  9, batch    11 | loss: 5.4214268
CurrentTrain: epoch  9, batch    12 | loss: 4.8367910
CurrentTrain: epoch  9, batch    13 | loss: 5.0123019
CurrentTrain: epoch  9, batch    14 | loss: 4.9838524
CurrentTrain: epoch  9, batch    15 | loss: 4.9974694
CurrentTrain: epoch  9, batch    16 | loss: 5.0616097
CurrentTrain: epoch  9, batch    17 | loss: 5.1054420
CurrentTrain: epoch  9, batch    18 | loss: 5.0139222
CurrentTrain: epoch  9, batch    19 | loss: 4.9360027
CurrentTrain: epoch  9, batch    20 | loss: 4.9477501
CurrentTrain: epoch  9, batch    21 | loss: 4.9941006
CurrentTrain: epoch  9, batch    22 | loss: 4.8248196
CurrentTrain: epoch  9, batch    23 | loss: 5.2373199
CurrentTrain: epoch  9, batch    24 | loss: 4.9871821
CurrentTrain: epoch  9, batch    25 | loss: 4.9288406
CurrentTrain: epoch  9, batch    26 | loss: 4.8644576
CurrentTrain: epoch  9, batch    27 | loss: 5.0124526
CurrentTrain: epoch  9, batch    28 | loss: 4.7953205
CurrentTrain: epoch  9, batch    29 | loss: 5.0100179
CurrentTrain: epoch  9, batch    30 | loss: 4.8699241
CurrentTrain: epoch  9, batch    31 | loss: 4.7642479
CurrentTrain: epoch  9, batch    32 | loss: 4.8581123
CurrentTrain: epoch  9, batch    33 | loss: 5.3856635
CurrentTrain: epoch  9, batch    34 | loss: 4.7388420
CurrentTrain: epoch  9, batch    35 | loss: 4.8740368
CurrentTrain: epoch  9, batch    36 | loss: 4.8223009
CurrentTrain: epoch  9, batch    37 | loss: 4.8875341
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots and family.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, James Smith relocated to Australia, seeking a more relaxed lifestyle and better weather.  
Head Entity: James Smith  
Tail Entity: Australia  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` customer relationships are the backbone of banking , and banks are constantly striving to meet customer demands , '' said peter garuccio , spokesman for the american bankers association .
Head Entity: american bankers association
Tail Entity: peter garuccio
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: "As the CEO of the tech giant, she has been instrumental in driving innovation and leading her team towards success," said a company spokesperson about Jane Doe, who has been with the organization for over a decade.  
Head Entity: tech giant  
Tail Entity: Jane Doe  

Relation: organization top members employees  
Context: "During the annual meeting, the board praised John Smith for his exceptional leadership and dedication to the company's mission," reported the financial analyst covering the firm.  
Head Entity: the board  
Tail Entity: John Smith  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union is a political and economic union of member states that are located primarily in Europe, working together to promote peace, stability, and prosperity.  
Head Entity: European Union  
Tail Entity: member states
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: the former detainee , murat kurnaz , was turned over to german authorities and freed in 2006 after a personal plea from german chancellor angela merkel .
Head Entity: murat kurnaz
Tail Entity: german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist albert einstein was born in the kingdom of wurttemberg in the german empire in 1879.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author gabriel garcía márquez was born in aracataca, a small town in colombia, which greatly influenced his literary works.  
Head Entity: gabriel garcía márquez  
Tail Entity: colombian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving the company's financial strategy."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the annual conference, Dr. James Lee, the lead researcher at the National Institute of Health, presented his findings on the latest advancements in medical technology."  
Head Entity: Dr. James Lee  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: the credit crisis spread to the largest us bond insurer thursday , sending shares of mbia inc plunging and calling into question the safety of tens of billions of dollars of company and local government debt held by investors .
Head Entity: mbia
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: after years of rapid growth, the tech giant apple inc announced plans to expand its operations in ireland, taking advantage of the favorable tax environment.  
Head Entity: apple  
Tail Entity: ireland  

Relation: organization country of headquarters  
Context: the renowned automotive manufacturer toyota motor corporation has established its main office in japan, where it continues to innovate in the electric vehicle market.  
Head Entity: toyota  
Tail Entity: japan  
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   
[EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   
[EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    5 | acc: 75.00%,  total acc: 85.42%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   
[EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 92.05%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 92.19%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   
[EVAL] batch:   13 | acc: 68.75%,  total acc: 90.62%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 89.17%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 87.11%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 86.40%   
[EVAL] batch:   17 | acc: 68.75%,  total acc: 85.42%   
[EVAL] batch:   18 | acc: 68.75%,  total acc: 84.54%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 84.38%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 85.12%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 85.80%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 86.41%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 86.98%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 87.50%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 88.19%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 88.62%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 89.01%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 88.75%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 88.71%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 89.06%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 87.50%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   
[EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   
[EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    5 | acc: 75.00%,  total acc: 85.42%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   
[EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 92.05%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 92.19%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   
[EVAL] batch:   13 | acc: 68.75%,  total acc: 90.62%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 89.17%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 87.11%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 86.40%   
[EVAL] batch:   17 | acc: 68.75%,  total acc: 85.42%   
[EVAL] batch:   18 | acc: 68.75%,  total acc: 84.54%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 84.38%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 85.12%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 85.80%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 86.41%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 86.98%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 87.50%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 87.98%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 88.19%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 88.62%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 89.01%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 88.75%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 88.71%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 89.06%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 87.50%   
cur_acc:  ['0.8750']
his_acc:  ['0.8750']
CurrentTrain: epoch  0, batch     0 | loss: 6.9190722
CurrentTrain: epoch  0, batch     1 | loss: 6.3776956
CurrentTrain: epoch  1, batch     0 | loss: 6.4354258
CurrentTrain: epoch  1, batch     1 | loss: 5.5586724
CurrentTrain: epoch  2, batch     0 | loss: 5.1935797
CurrentTrain: epoch  2, batch     1 | loss: 4.5466008
CurrentTrain: epoch  3, batch     0 | loss: 4.2265210
CurrentTrain: epoch  3, batch     1 | loss: 5.4388528
CurrentTrain: epoch  4, batch     0 | loss: 3.9438262
CurrentTrain: epoch  4, batch     1 | loss: 5.3081174
CurrentTrain: epoch  5, batch     0 | loss: 4.0264459
CurrentTrain: epoch  5, batch     1 | loss: 3.9785259
CurrentTrain: epoch  6, batch     0 | loss: 3.6789529
CurrentTrain: epoch  6, batch     1 | loss: 3.6799343
CurrentTrain: epoch  7, batch     0 | loss: 3.5197258
CurrentTrain: epoch  7, batch     1 | loss: 3.8555696
CurrentTrain: epoch  8, batch     0 | loss: 3.2976732
CurrentTrain: epoch  8, batch     1 | loss: 3.4471431
CurrentTrain: epoch  9, batch     0 | loss: 3.0527167
CurrentTrain: epoch  9, batch     1 | loss: 3.0509801
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones settled in san francisco, where she found a job in tech. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith lived in boston, enjoying the vibrant culture and history of the city. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, he enrolled at Stanford University to study computer science.  
Head Entity: he  
Tail Entity: Stanford University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, a crime that could result in significant prison time if convicted.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith was charged with assault following the altercation that took place last month at the downtown bar.  
Head Entity: Smith  
Tail Entity: assault  
Mixup data size:  104
MixupTrain:  epoch  0, batch     0 | loss: 5.5622940
MixupTrain:  epoch  0, batch     1 | loss: 5.1894341
MixupTrain:  epoch  0, batch     2 | loss: 4.9878559
MixupTrain:  epoch  0, batch     3 | loss: 4.9520292
MixupTrain:  epoch  0, batch     4 | loss: 4.7299414
MixupTrain:  epoch  0, batch     5 | loss: 4.7300582
MixupTrain:  epoch  0, batch     6 | loss: 4.1781654
MemoryTrain:  epoch  0, batch     0 | loss: 3.5196331
MemoryTrain:  epoch  0, batch     1 | loss: 3.4944627
MemoryTrain:  epoch  0, batch     2 | loss: 2.7086804
MemoryTrain:  epoch  1, batch     0 | loss: 3.0644407
MemoryTrain:  epoch  1, batch     1 | loss: 3.2165585
MemoryTrain:  epoch  1, batch     2 | loss: 4.2196717
MemoryTrain:  epoch  2, batch     0 | loss: 2.4466000
MemoryTrain:  epoch  2, batch     1 | loss: 2.0888321
MemoryTrain:  epoch  2, batch     2 | loss: 2.5748532
MemoryTrain:  epoch  3, batch     0 | loss: 2.2109637
MemoryTrain:  epoch  3, batch     1 | loss: 2.3524091
MemoryTrain:  epoch  3, batch     2 | loss: 1.4660759
MemoryTrain:  epoch  4, batch     0 | loss: 2.2765172
MemoryTrain:  epoch  4, batch     1 | loss: 2.0743232
MemoryTrain:  epoch  4, batch     2 | loss: 1.7162809
MemoryTrain:  epoch  5, batch     0 | loss: 2.2685676
MemoryTrain:  epoch  5, batch     1 | loss: 1.8276422
MemoryTrain:  epoch  5, batch     2 | loss: 1.3813000
MemoryTrain:  epoch  6, batch     0 | loss: 1.7278991
MemoryTrain:  epoch  6, batch     1 | loss: 2.0945601
MemoryTrain:  epoch  6, batch     2 | loss: 2.2318401
MemoryTrain:  epoch  7, batch     0 | loss: 1.7539617
MemoryTrain:  epoch  7, batch     1 | loss: 2.0496335
MemoryTrain:  epoch  7, batch     2 | loss: 1.2109689
MemoryTrain:  epoch  8, batch     0 | loss: 1.7152328
MemoryTrain:  epoch  8, batch     1 | loss: 1.9467192
MemoryTrain:  epoch  8, batch     2 | loss: 1.2249420
MemoryTrain:  epoch  9, batch     0 | loss: 1.8310726
MemoryTrain:  epoch  9, batch     1 | loss: 1.8756949
MemoryTrain:  epoch  9, batch     2 | loss: 1.6019945
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   
[EVAL] batch:    3 | acc: 75.00%,  total acc: 73.44%   
[EVAL] batch:    4 | acc: 68.75%,  total acc: 72.50%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 71.88%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   
[EVAL] batch:    7 | acc: 93.75%,  total acc: 77.34%   
[EVAL] batch:    8 | acc: 62.50%,  total acc: 75.69%   
[EVAL] batch:    9 | acc: 100.00%,  total acc: 78.12%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 80.11%   
[EVAL] batch:   11 | acc: 100.00%,  total acc: 81.77%   
[EVAL] batch:   12 | acc: 100.00%,  total acc: 83.17%   
[EVAL] batch:   13 | acc: 100.00%,  total acc: 84.38%   
[EVAL] batch:   14 | acc: 100.00%,  total acc: 85.42%   
[EVAL] batch:   15 | acc: 100.00%,  total acc: 86.33%   
[EVAL] batch:   16 | acc: 100.00%,  total acc: 87.13%   
[EVAL] batch:   17 | acc: 25.00%,  total acc: 83.68%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 65.62%   
[EVAL] batch:    2 | acc: 81.25%,  total acc: 70.83%   
[EVAL] batch:    3 | acc: 56.25%,  total acc: 67.19%   
[EVAL] batch:    4 | acc: 68.75%,  total acc: 67.50%   
[EVAL] batch:    5 | acc: 62.50%,  total acc: 66.67%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 70.54%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 74.22%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 77.08%   
[EVAL] batch:    9 | acc: 100.00%,  total acc: 79.38%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 81.25%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 81.77%   
[EVAL] batch:   12 | acc: 87.50%,  total acc: 82.21%   
[EVAL] batch:   13 | acc: 75.00%,  total acc: 81.70%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 81.25%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 79.69%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 79.41%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 78.47%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 78.62%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 79.06%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 80.06%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 80.97%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 81.79%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 82.29%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 83.00%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 84.03%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 85.00%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 85.08%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 85.55%   
[EVAL] batch:   32 | acc: 75.00%,  total acc: 85.23%   
[EVAL] batch:   33 | acc: 81.25%,  total acc: 85.11%   
[EVAL] batch:   34 | acc: 62.50%,  total acc: 84.46%   
[EVAL] batch:   35 | acc: 81.25%,  total acc: 84.38%   
[EVAL] batch:   36 | acc: 68.75%,  total acc: 83.95%   
[EVAL] batch:   37 | acc: 75.00%,  total acc: 83.72%   
[EVAL] batch:   38 | acc: 81.25%,  total acc: 83.65%   
[EVAL] batch:   39 | acc: 100.00%,  total acc: 84.06%   
[EVAL] batch:   40 | acc: 62.50%,  total acc: 83.54%   
[EVAL] batch:   41 | acc: 93.75%,  total acc: 83.78%   
[EVAL] batch:   42 | acc: 100.00%,  total acc: 84.16%   
[EVAL] batch:   43 | acc: 100.00%,  total acc: 84.52%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 84.86%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 85.19%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 85.51%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 85.81%   
[EVAL] batch:   48 | acc: 100.00%,  total acc: 86.10%   
[EVAL] batch:   49 | acc: 62.50%,  total acc: 85.62%   
cur_acc:  ['0.8750', '0.8368']
his_acc:  ['0.8750', '0.8562']
CurrentTrain: epoch  0, batch     0 | loss: 6.1974525
CurrentTrain: epoch  0, batch     1 | loss: 6.3744802
CurrentTrain: epoch  1, batch     0 | loss: 5.7390709
CurrentTrain: epoch  1, batch     1 | loss: 5.3731132
CurrentTrain: epoch  2, batch     0 | loss: 4.8610620
CurrentTrain: epoch  2, batch     1 | loss: 4.3877354
CurrentTrain: epoch  3, batch     0 | loss: 4.4404836
CurrentTrain: epoch  3, batch     1 | loss: 4.9371362
CurrentTrain: epoch  4, batch     0 | loss: 4.4785366
CurrentTrain: epoch  4, batch     1 | loss: 4.2236414
CurrentTrain: epoch  5, batch     0 | loss: 4.0281610
CurrentTrain: epoch  5, batch     1 | loss: 3.3564146
CurrentTrain: epoch  6, batch     0 | loss: 3.5414038
CurrentTrain: epoch  6, batch     1 | loss: 2.8307483
CurrentTrain: epoch  7, batch     0 | loss: 3.1513174
CurrentTrain: epoch  7, batch     1 | loss: 3.1909344
CurrentTrain: epoch  8, batch     0 | loss: 3.0441642
CurrentTrain: epoch  8, batch     1 | loss: 2.9274054
CurrentTrain: epoch  9, batch     0 | loss: 2.7501199
CurrentTrain: epoch  9, batch     1 | loss: 2.5099752
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: david 's father had surrendered the child to an orphanage after his wife of 10 years , marita , died in 2005 shortly after giving birth to david .
Head Entity: david
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: The famous author, Jane Austen, was born on December 16, 1775, in Steventon, Hampshire.  
Head Entity: Jane Austen  
Tail Entity: December 16, 1775  

Relation: person date of birth  
Context: Albert Einstein was born in the city of Ulm, in the Kingdom of Württemberg, Germany, on March 14, 1879.  
Head Entity: Albert Einstein  
Tail Entity: March 14, 1879  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: deaver was born april 11 , 1938 , in bakersfield , california , the son of a shell oil co distributor .
Head Entity: deaver
Tail Entity: california
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: jane smith was born on march 5, 1985, in a small town near the coast of maine, where her family has lived for generations.  
Head Entity: jane smith  
Tail Entity: maine  

Relation: person stateorprovince of birth  
Context: during a recent interview, it was revealed that michael johnson was born in the vibrant city of austin, texas, known for its music scene.  
Head Entity: michael johnson  
Tail Entity: texas  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Sample 1:  
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, who had always been a guiding force in her life, to her friends.  
Head Entity: her father  
Tail Entity: Sarah  

Sample 2:  
Relation: person parents  
Context: After the ceremony, Michael expressed his gratitude to his mother for all the sacrifices she made to support him through school.  
Head Entity: his mother  
Tail Entity: Michael  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at the tech company where she has been employed since college.  
Head Entity: Sarah Thompson  
Tail Entity: tech company  

Relation: person employee of  
Context: John Smith has been a loyal employee at Greenfield Farms for over a decade, contributing to the growth and success of the organization.  
Head Entity: John Smith  
Tail Entity: Greenfield Farms  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, mary jane, a beloved community leader, succumbed to her condition in a hospital located in new york.  
Head Entity: mary jane  
Tail Entity: new york  
Mixup data size:  134
MixupTrain:  epoch  0, batch     0 | loss: 3.1667895
MixupTrain:  epoch  0, batch     1 | loss: 2.8760586
MixupTrain:  epoch  0, batch     2 | loss: 3.1296439
MixupTrain:  epoch  0, batch     3 | loss: 3.1166220
MixupTrain:  epoch  0, batch     4 | loss: 2.8992324
MixupTrain:  epoch  0, batch     5 | loss: 2.8840199
MixupTrain:  epoch  0, batch     6 | loss: 3.1688390
MixupTrain:  epoch  0, batch     7 | loss: 2.8669901
MixupTrain:  epoch  0, batch     8 | loss: 2.7961118
MemoryTrain:  epoch  0, batch     0 | loss: 2.2642298
MemoryTrain:  epoch  0, batch     1 | loss: 2.7772913
MemoryTrain:  epoch  0, batch     2 | loss: 3.3851604
MemoryTrain:  epoch  1, batch     0 | loss: 3.0636821
MemoryTrain:  epoch  1, batch     1 | loss: 2.1561582
MemoryTrain:  epoch  1, batch     2 | loss: 2.8372467
MemoryTrain:  epoch  2, batch     0 | loss: 2.6640291
MemoryTrain:  epoch  2, batch     1 | loss: 2.5078521
MemoryTrain:  epoch  2, batch     2 | loss: 2.0756333
MemoryTrain:  epoch  3, batch     0 | loss: 2.2252746
MemoryTrain:  epoch  3, batch     1 | loss: 1.6669413
MemoryTrain:  epoch  3, batch     2 | loss: 1.9503484
MemoryTrain:  epoch  4, batch     0 | loss: 2.1996746
MemoryTrain:  epoch  4, batch     1 | loss: 1.7758621
MemoryTrain:  epoch  4, batch     2 | loss: 2.0151079
MemoryTrain:  epoch  5, batch     0 | loss: 1.8594112
MemoryTrain:  epoch  5, batch     1 | loss: 1.6278028
MemoryTrain:  epoch  5, batch     2 | loss: 1.8127204
MemoryTrain:  epoch  6, batch     0 | loss: 1.6690283
MemoryTrain:  epoch  6, batch     1 | loss: 1.5529164
MemoryTrain:  epoch  6, batch     2 | loss: 1.8126874
MemoryTrain:  epoch  7, batch     0 | loss: 1.6501275
MemoryTrain:  epoch  7, batch     1 | loss: 1.7181416
MemoryTrain:  epoch  7, batch     2 | loss: 1.6241295
MemoryTrain:  epoch  8, batch     0 | loss: 1.5570540
MemoryTrain:  epoch  8, batch     1 | loss: 1.6363430
MemoryTrain:  epoch  8, batch     2 | loss: 1.5888902
MemoryTrain:  epoch  9, batch     0 | loss: 1.5620645
MemoryTrain:  epoch  9, batch     1 | loss: 1.5832927
MemoryTrain:  epoch  9, batch     2 | loss: 1.5679728
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    1 | acc: 50.00%,  total acc: 68.75%   
[EVAL] batch:    2 | acc: 56.25%,  total acc: 64.58%   
[EVAL] batch:    3 | acc: 68.75%,  total acc: 65.62%   
[EVAL] batch:    4 | acc: 68.75%,  total acc: 66.25%   
[EVAL] batch:    5 | acc: 31.25%,  total acc: 60.42%   
[EVAL] batch:    6 | acc: 62.50%,  total acc: 60.71%   
[EVAL] batch:    7 | acc: 93.75%,  total acc: 64.84%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 68.06%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 70.00%   
[EVAL] batch:   10 | acc: 81.25%,  total acc: 71.02%   
[EVAL] batch:   11 | acc: 75.00%,  total acc: 71.35%   
[EVAL] batch:   12 | acc: 87.50%,  total acc: 72.60%   
[EVAL] batch:   13 | acc: 62.50%,  total acc: 71.88%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   
[EVAL] batch:    2 | acc: 81.25%,  total acc: 68.75%   
[EVAL] batch:    3 | acc: 56.25%,  total acc: 65.62%   
[EVAL] batch:    4 | acc: 68.75%,  total acc: 66.25%   
[EVAL] batch:    5 | acc: 56.25%,  total acc: 64.58%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 68.75%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 72.66%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 75.69%   
[EVAL] batch:    9 | acc: 100.00%,  total acc: 78.12%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 80.11%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 80.73%   
[EVAL] batch:   12 | acc: 87.50%,  total acc: 81.25%   
[EVAL] batch:   13 | acc: 68.75%,  total acc: 80.36%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 80.00%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 78.52%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 78.31%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 77.43%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 76.64%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 76.88%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 77.98%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 78.98%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 79.89%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 80.47%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 81.25%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 82.41%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 83.62%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 83.54%   
[EVAL] batch:   30 | acc: 93.75%,  total acc: 83.87%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 84.38%   
[EVAL] batch:   32 | acc: 62.50%,  total acc: 83.71%   
[EVAL] batch:   33 | acc: 37.50%,  total acc: 82.35%   
[EVAL] batch:   34 | acc: 50.00%,  total acc: 81.43%   
[EVAL] batch:   35 | acc: 31.25%,  total acc: 80.03%   
[EVAL] batch:   36 | acc: 43.75%,  total acc: 79.05%   
[EVAL] batch:   37 | acc: 37.50%,  total acc: 77.96%   
[EVAL] batch:   38 | acc: 62.50%,  total acc: 77.56%   
[EVAL] batch:   39 | acc: 87.50%,  total acc: 77.81%   
[EVAL] batch:   40 | acc: 68.75%,  total acc: 77.59%   
[EVAL] batch:   41 | acc: 62.50%,  total acc: 77.23%   
[EVAL] batch:   42 | acc: 62.50%,  total acc: 76.89%   
[EVAL] batch:   43 | acc: 68.75%,  total acc: 76.70%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 77.22%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 77.72%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 78.19%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 78.65%   
[EVAL] batch:   48 | acc: 100.00%,  total acc: 79.08%   
[EVAL] batch:   49 | acc: 100.00%,  total acc: 79.50%   
[EVAL] batch:   50 | acc: 75.00%,  total acc: 79.41%   
[EVAL] batch:   51 | acc: 50.00%,  total acc: 78.85%   
[EVAL] batch:   52 | acc: 68.75%,  total acc: 78.66%   
[EVAL] batch:   53 | acc: 50.00%,  total acc: 78.12%   
[EVAL] batch:   54 | acc: 62.50%,  total acc: 77.84%   
[EVAL] batch:   55 | acc: 37.50%,  total acc: 77.12%   
[EVAL] batch:   56 | acc: 81.25%,  total acc: 77.19%   
[EVAL] batch:   57 | acc: 93.75%,  total acc: 77.48%   
[EVAL] batch:   58 | acc: 87.50%,  total acc: 77.65%   
[EVAL] batch:   59 | acc: 93.75%,  total acc: 77.92%   
[EVAL] batch:   60 | acc: 68.75%,  total acc: 77.77%   
[EVAL] batch:   61 | acc: 81.25%,  total acc: 77.82%   
[EVAL] batch:   62 | acc: 93.75%,  total acc: 78.08%   
[EVAL] batch:   63 | acc: 25.00%,  total acc: 77.25%   
cur_acc:  ['0.8750', '0.8368', '0.7188']
his_acc:  ['0.8750', '0.8562', '0.7725']
CurrentTrain: epoch  0, batch     0 | loss: 4.6802292
CurrentTrain: epoch  0, batch     1 | loss: 4.8095002
CurrentTrain: epoch  1, batch     0 | loss: 3.5458052
CurrentTrain: epoch  1, batch     1 | loss: 3.2281477
CurrentTrain: epoch  2, batch     0 | loss: 2.9499474
CurrentTrain: epoch  2, batch     1 | loss: 3.1330070
CurrentTrain: epoch  3, batch     0 | loss: 2.6668482
CurrentTrain: epoch  3, batch     1 | loss: 2.8634000
CurrentTrain: epoch  4, batch     0 | loss: 2.8127136
CurrentTrain: epoch  4, batch     1 | loss: 2.6658833
CurrentTrain: epoch  5, batch     0 | loss: 2.6009903
CurrentTrain: epoch  5, batch     1 | loss: 2.3081517
CurrentTrain: epoch  6, batch     0 | loss: 2.3230958
CurrentTrain: epoch  6, batch     1 | loss: 2.3477039
CurrentTrain: epoch  7, batch     0 | loss: 2.0994821
CurrentTrain: epoch  7, batch     1 | loss: 2.3510463
CurrentTrain: epoch  8, batch     0 | loss: 2.2705293
CurrentTrain: epoch  8, batch     1 | loss: 1.9535440
CurrentTrain: epoch  9, batch     0 | loss: 2.0334957
CurrentTrain: epoch  9, batch     1 | loss: 2.1450779
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization Green Earth Initiative was established in 2010 to promote environmental awareness and sustainability practices across communities.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: a page six gossip item in the new york post last week said morgenthau , who is 88 , is considering stepping down before the end of the year , and that gov
Head Entity: morgenthau
Tail Entity: 88
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: during the interview, she mentioned that her grandmother, who is 92 years old, still enjoys gardening every day.  
Head Entity: grandmother  
Tail Entity: 92  

Relation: person age  
Context: the documentary highlighted the life of a famous artist who passed away at the age of 75, leaving behind a legacy of incredible works.  
Head Entity: artist  
Tail Entity: 75  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: forsberg was born in 1943 in huntsville , ala. , and grew up on long island in new york .
Head Entity: forsberg
Tail Entity: huntsville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in san francisco, he moved to los angeles where he pursued his career.  
Head Entity: he  
Tail Entity: san francisco  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced her writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: san diego 32 new orleans 37 american football : nfl result result of the nfl match between the san diego chargers of the afc west and the new orleans saints of the nfc south at wembley here sunday :
Head Entity: nfc south
Tail Entity: new orleans saints
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: the tech giant apple inc. has announced its new team of engineers who will be working on the latest software updates for their devices, including the new members from the software development organization.  
Head Entity: software development organization  
Tail Entity: apple inc.  

Relation: organization members  
Context: during the annual conference, the president of the environmental advocacy group revealed the new members who joined the organization to help combat climate change and promote sustainability initiatives.  
Head Entity: environmental advocacy group  
Tail Entity: climate change organization  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how each member contributes to the collective spirit.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The imam led the prayers at the mosque, reminding the congregation of their duties as followers of Islam and the significance of their beliefs in daily life.  
Head Entity: imam  
Tail Entity: Islam  
Mixup data size:  165
MixupTrain:  epoch  0, batch     0 | loss: 3.1105022
MixupTrain:  epoch  0, batch     1 | loss: 2.6978550
MixupTrain:  epoch  0, batch     2 | loss: 2.8440399
MixupTrain:  epoch  0, batch     3 | loss: 2.4693313
MixupTrain:  epoch  0, batch     4 | loss: 2.4782619
MixupTrain:  epoch  0, batch     5 | loss: 2.6163983
MixupTrain:  epoch  0, batch     6 | loss: 2.3380461
MixupTrain:  epoch  0, batch     7 | loss: 2.6365480
MixupTrain:  epoch  0, batch     8 | loss: 2.5444751
MixupTrain:  epoch  0, batch     9 | loss: 2.2831020
MixupTrain:  epoch  0, batch    10 | loss: 2.6943016
MemoryTrain:  epoch  0, batch     0 | loss: 1.9779925
MemoryTrain:  epoch  0, batch     1 | loss: 2.9565301
MemoryTrain:  epoch  0, batch     2 | loss: 3.2067940
MemoryTrain:  epoch  0, batch     3 | loss: 3.1657884
MemoryTrain:  epoch  1, batch     0 | loss: 2.5076258
MemoryTrain:  epoch  1, batch     1 | loss: 2.7786033
MemoryTrain:  epoch  1, batch     2 | loss: 2.0507133
MemoryTrain:  epoch  1, batch     3 | loss: 2.3471782
MemoryTrain:  epoch  2, batch     0 | loss: 2.8335204
MemoryTrain:  epoch  2, batch     1 | loss: 1.5704893
MemoryTrain:  epoch  2, batch     2 | loss: 2.2314365
MemoryTrain:  epoch  2, batch     3 | loss: 1.6904806
MemoryTrain:  epoch  3, batch     0 | loss: 2.0564122
MemoryTrain:  epoch  3, batch     1 | loss: 1.7353795
MemoryTrain:  epoch  3, batch     2 | loss: 2.1364002
MemoryTrain:  epoch  3, batch     3 | loss: 2.1712923
MemoryTrain:  epoch  4, batch     0 | loss: 2.3350706
MemoryTrain:  epoch  4, batch     1 | loss: 1.9502362
MemoryTrain:  epoch  4, batch     2 | loss: 1.8229480
MemoryTrain:  epoch  4, batch     3 | loss: 1.7300695
MemoryTrain:  epoch  5, batch     0 | loss: 1.5607082
MemoryTrain:  epoch  5, batch     1 | loss: 1.8795643
MemoryTrain:  epoch  5, batch     2 | loss: 1.8864766
MemoryTrain:  epoch  5, batch     3 | loss: 1.8672812
MemoryTrain:  epoch  6, batch     0 | loss: 1.6096249
MemoryTrain:  epoch  6, batch     1 | loss: 1.8269272
MemoryTrain:  epoch  6, batch     2 | loss: 1.7160466
MemoryTrain:  epoch  6, batch     3 | loss: 1.4161955
MemoryTrain:  epoch  7, batch     0 | loss: 1.7006044
MemoryTrain:  epoch  7, batch     1 | loss: 1.6656762
MemoryTrain:  epoch  7, batch     2 | loss: 1.6870699
MemoryTrain:  epoch  7, batch     3 | loss: 1.6415559
MemoryTrain:  epoch  8, batch     0 | loss: 1.4891648
MemoryTrain:  epoch  8, batch     1 | loss: 1.5157979
MemoryTrain:  epoch  8, batch     2 | loss: 1.4163949
MemoryTrain:  epoch  8, batch     3 | loss: 1.6530328
MemoryTrain:  epoch  9, batch     0 | loss: 1.4248264
MemoryTrain:  epoch  9, batch     1 | loss: 1.6262337
MemoryTrain:  epoch  9, batch     2 | loss: 1.3319823
MemoryTrain:  epoch  9, batch     3 | loss: 1.3273246
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    8 | acc: 87.50%,  total acc: 98.61%   
[EVAL] batch:    9 | acc: 43.75%,  total acc: 93.12%   
[EVAL] batch:   10 | acc: 50.00%,  total acc: 89.20%   
[EVAL] batch:   11 | acc: 62.50%,  total acc: 86.98%   
[EVAL] batch:   12 | acc: 43.75%,  total acc: 83.65%   
[EVAL] batch:   13 | acc: 50.00%,  total acc: 81.25%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   
[EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   
[EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 83.33%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 84.38%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 86.46%   
[EVAL] batch:   12 | acc: 68.75%,  total acc: 85.10%   
[EVAL] batch:   13 | acc: 37.50%,  total acc: 81.70%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 81.25%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 79.69%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 79.41%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 78.47%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 77.63%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 77.81%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 78.87%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 79.55%   
[EVAL] batch:   22 | acc: 93.75%,  total acc: 80.16%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 80.73%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 81.50%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 82.21%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 82.64%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 83.26%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 83.84%   
[EVAL] batch:   29 | acc: 87.50%,  total acc: 83.96%   
[EVAL] batch:   30 | acc: 100.00%,  total acc: 84.48%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 84.96%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 83.52%   
[EVAL] batch:   33 | acc: 0.00%,  total acc: 81.07%   
[EVAL] batch:   34 | acc: 0.00%,  total acc: 78.75%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 76.56%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 74.49%   
[EVAL] batch:   37 | acc: 0.00%,  total acc: 72.53%   
[EVAL] batch:   38 | acc: 31.25%,  total acc: 71.47%   
[EVAL] batch:   39 | acc: 93.75%,  total acc: 72.03%   
[EVAL] batch:   40 | acc: 43.75%,  total acc: 71.34%   
[EVAL] batch:   41 | acc: 62.50%,  total acc: 71.13%   
[EVAL] batch:   42 | acc: 75.00%,  total acc: 71.22%   
[EVAL] batch:   43 | acc: 81.25%,  total acc: 71.45%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 72.08%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 72.69%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 73.27%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 73.83%   
[EVAL] batch:   48 | acc: 100.00%,  total acc: 74.36%   
[EVAL] batch:   49 | acc: 100.00%,  total acc: 74.88%   
[EVAL] batch:   50 | acc: 75.00%,  total acc: 74.88%   
[EVAL] batch:   51 | acc: 56.25%,  total acc: 74.52%   
[EVAL] batch:   52 | acc: 50.00%,  total acc: 74.06%   
[EVAL] batch:   53 | acc: 50.00%,  total acc: 73.61%   
[EVAL] batch:   54 | acc: 56.25%,  total acc: 73.30%   
[EVAL] batch:   55 | acc: 37.50%,  total acc: 72.66%   
[EVAL] batch:   56 | acc: 81.25%,  total acc: 72.81%   
[EVAL] batch:   57 | acc: 93.75%,  total acc: 73.17%   
[EVAL] batch:   58 | acc: 93.75%,  total acc: 73.52%   
[EVAL] batch:   59 | acc: 93.75%,  total acc: 73.85%   
[EVAL] batch:   60 | acc: 68.75%,  total acc: 73.77%   
[EVAL] batch:   61 | acc: 87.50%,  total acc: 73.99%   
[EVAL] batch:   62 | acc: 93.75%,  total acc: 74.31%   
[EVAL] batch:   63 | acc: 93.75%,  total acc: 74.61%   
[EVAL] batch:   64 | acc: 100.00%,  total acc: 75.00%   
[EVAL] batch:   65 | acc: 100.00%,  total acc: 75.38%   
[EVAL] batch:   66 | acc: 100.00%,  total acc: 75.75%   
[EVAL] batch:   67 | acc: 100.00%,  total acc: 76.10%   
[EVAL] batch:   68 | acc: 100.00%,  total acc: 76.45%   
[EVAL] batch:   69 | acc: 100.00%,  total acc: 76.79%   
[EVAL] batch:   70 | acc: 100.00%,  total acc: 77.11%   
[EVAL] batch:   71 | acc: 100.00%,  total acc: 77.43%   
[EVAL] batch:   72 | acc: 31.25%,  total acc: 76.80%   
[EVAL] batch:   73 | acc: 56.25%,  total acc: 76.52%   
[EVAL] batch:   74 | acc: 62.50%,  total acc: 76.33%   
[EVAL] batch:   75 | acc: 56.25%,  total acc: 76.07%   
[EVAL] batch:   76 | acc: 62.50%,  total acc: 75.89%   
[EVAL] batch:   77 | acc: 0.00%,  total acc: 74.92%   
cur_acc:  ['0.8750', '0.8368', '0.7188', '0.8125']
his_acc:  ['0.8750', '0.8562', '0.7725', '0.7492']
CurrentTrain: epoch  0, batch     0 | loss: 4.3355303
CurrentTrain: epoch  0, batch     1 | loss: 4.5962672
CurrentTrain: epoch  1, batch     0 | loss: 3.3401582
CurrentTrain: epoch  1, batch     1 | loss: 2.9930012
CurrentTrain: epoch  2, batch     0 | loss: 2.8043568
CurrentTrain: epoch  2, batch     1 | loss: 2.7816782
CurrentTrain: epoch  3, batch     0 | loss: 3.3679528
CurrentTrain: epoch  3, batch     1 | loss: 3.7181795
CurrentTrain: epoch  4, batch     0 | loss: 3.2346599
CurrentTrain: epoch  4, batch     1 | loss: 2.1469452
CurrentTrain: epoch  5, batch     0 | loss: 2.2128761
CurrentTrain: epoch  5, batch     1 | loss: 2.2573178
CurrentTrain: epoch  6, batch     0 | loss: 2.0459158
CurrentTrain: epoch  6, batch     1 | loss: 2.1471174
CurrentTrain: epoch  7, batch     0 | loss: 2.0857706
CurrentTrain: epoch  7, batch     1 | loss: 1.9711652
CurrentTrain: epoch  8, batch     0 | loss: 2.0172956
CurrentTrain: epoch  8, batch     1 | loss: 1.8955083
CurrentTrain: epoch  9, batch     0 | loss: 1.9375696
CurrentTrain: epoch  9, batch     1 | loss: 2.0020397
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The recent conference held by the Islamic Society of North America aimed to strengthen ties between various Muslim organizations and promote interfaith dialogue.  
Head Entity: Islamic Society of North America  
Tail Entity: Islam  

Relation: organization political religious affiliation  
Context: During the annual meeting, the Catholic League discussed its initiatives to support Catholic values in public policy and its role in the broader religious community.  
Head Entity: Catholic League  
Tail Entity: Catholicism  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: andrew lebow , an oil trader with mf global in new york , said investors have been discouraged by lower-than-expected oil imports in china and the disappointing growth in the u.s. economy .
Head Entity: mf global
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been a hub for innovation and development.  
Head Entity: apple inc.  
Tail Entity: cupertino, california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics has its main office situated in suwon, south korea, which plays a crucial role in its global operations.  
Head Entity: samsung electronics  
Tail Entity: suwon, south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, and their close relationship, highlighting how they support each other through life's challenges.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, uncle bob shared stories about his adventures with his niece, emily, who has always looked up to him as a role model and mentor.  
Head Entity: uncle bob  
Tail Entity: emily  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: millender-mcdonald , who was 68 , died late saturday at her home in carson , california , said her chief of staff , bandele mcqueen .
Head Entity: millender-mcdonald
Tail Entity: carson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: renowned author and activist, james baldwin, passed away in 1987 in the city of saint-paul, minnesota, where he spent his final years.  
Head Entity: james baldwin  
Tail Entity: saint-paul  

Relation: person city of death  
Context: the famous physicist, albert einstein, died in 1955 in the city of princeton, new jersey, where he had lived for many years.  
Head Entity: albert einstein  
Tail Entity: princeton  
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 2.2280407
MixupTrain:  epoch  0, batch     1 | loss: 2.6288037
MixupTrain:  epoch  0, batch     2 | loss: 2.5859890
MixupTrain:  epoch  0, batch     3 | loss: 2.3716805
MixupTrain:  epoch  0, batch     4 | loss: 2.0722656
MixupTrain:  epoch  0, batch     5 | loss: 2.4493988
MixupTrain:  epoch  0, batch     6 | loss: 2.1315198
MixupTrain:  epoch  0, batch     7 | loss: 2.4732521
MixupTrain:  epoch  0, batch     8 | loss: 2.2697759
MixupTrain:  epoch  0, batch     9 | loss: 2.2075624
MixupTrain:  epoch  0, batch    10 | loss: 2.2078829
MixupTrain:  epoch  0, batch    11 | loss: 2.3906379
MixupTrain:  epoch  0, batch    12 | loss: 2.3409324
MemoryTrain:  epoch  0, batch     0 | loss: 2.4804296
MemoryTrain:  epoch  0, batch     1 | loss: 2.2940493
MemoryTrain:  epoch  0, batch     2 | loss: 3.1297131
MemoryTrain:  epoch  0, batch     3 | loss: 2.5212011
MemoryTrain:  epoch  0, batch     4 | loss: 2.2660453
MemoryTrain:  epoch  1, batch     0 | loss: 2.5499897
MemoryTrain:  epoch  1, batch     1 | loss: 2.2747259
MemoryTrain:  epoch  1, batch     2 | loss: 2.2358432
MemoryTrain:  epoch  1, batch     3 | loss: 2.3414276
MemoryTrain:  epoch  1, batch     4 | loss: 2.4609737
MemoryTrain:  epoch  2, batch     0 | loss: 2.2633619
MemoryTrain:  epoch  2, batch     1 | loss: 1.8216864
MemoryTrain:  epoch  2, batch     2 | loss: 1.7276402
MemoryTrain:  epoch  2, batch     3 | loss: 2.1116290
MemoryTrain:  epoch  2, batch     4 | loss: 1.7933277
MemoryTrain:  epoch  3, batch     0 | loss: 2.0738807
MemoryTrain:  epoch  3, batch     1 | loss: 1.5481484
MemoryTrain:  epoch  3, batch     2 | loss: 1.7576133
MemoryTrain:  epoch  3, batch     3 | loss: 1.6675410
MemoryTrain:  epoch  3, batch     4 | loss: 1.9369657
MemoryTrain:  epoch  4, batch     0 | loss: 1.6289802
MemoryTrain:  epoch  4, batch     1 | loss: 1.8369530
MemoryTrain:  epoch  4, batch     2 | loss: 1.5785998
MemoryTrain:  epoch  4, batch     3 | loss: 1.7105758
MemoryTrain:  epoch  4, batch     4 | loss: 1.6524007
MemoryTrain:  epoch  5, batch     0 | loss: 1.5262594
MemoryTrain:  epoch  5, batch     1 | loss: 1.7461820
MemoryTrain:  epoch  5, batch     2 | loss: 1.7551513
MemoryTrain:  epoch  5, batch     3 | loss: 1.5697939
MemoryTrain:  epoch  5, batch     4 | loss: 1.7013878
MemoryTrain:  epoch  6, batch     0 | loss: 1.5906237
MemoryTrain:  epoch  6, batch     1 | loss: 1.6013616
MemoryTrain:  epoch  6, batch     2 | loss: 1.5788794
MemoryTrain:  epoch  6, batch     3 | loss: 1.5368623
MemoryTrain:  epoch  6, batch     4 | loss: 1.4397807
MemoryTrain:  epoch  7, batch     0 | loss: 1.6505988
MemoryTrain:  epoch  7, batch     1 | loss: 1.4299635
MemoryTrain:  epoch  7, batch     2 | loss: 1.4509240
MemoryTrain:  epoch  7, batch     3 | loss: 1.5132012
MemoryTrain:  epoch  7, batch     4 | loss: 1.4296697
MemoryTrain:  epoch  8, batch     0 | loss: 1.4102050
MemoryTrain:  epoch  8, batch     1 | loss: 1.3823855
MemoryTrain:  epoch  8, batch     2 | loss: 1.3738815
MemoryTrain:  epoch  8, batch     3 | loss: 1.3743732
MemoryTrain:  epoch  8, batch     4 | loss: 1.4782780
MemoryTrain:  epoch  9, batch     0 | loss: 1.4929955
MemoryTrain:  epoch  9, batch     1 | loss: 1.3910339
MemoryTrain:  epoch  9, batch     2 | loss: 1.3487787
MemoryTrain:  epoch  9, batch     3 | loss: 1.3774452
MemoryTrain:  epoch  9, batch     4 | loss: 1.3954818
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   
[EVAL] batch:    1 | acc: 75.00%,  total acc: 62.50%   
[EVAL] batch:    2 | acc: 50.00%,  total acc: 58.33%   
[EVAL] batch:    3 | acc: 43.75%,  total acc: 54.69%   
[EVAL] batch:    4 | acc: 50.00%,  total acc: 53.75%   
[EVAL] batch:    5 | acc: 50.00%,  total acc: 53.12%   
[EVAL] batch:    6 | acc: 62.50%,  total acc: 54.46%   
[EVAL] batch:    7 | acc: 75.00%,  total acc: 57.03%   
[EVAL] batch:    8 | acc: 31.25%,  total acc: 54.17%   
[EVAL] batch:    9 | acc: 56.25%,  total acc: 54.37%   
[EVAL] batch:   10 | acc: 62.50%,  total acc: 55.11%   
[EVAL] batch:   11 | acc: 75.00%,  total acc: 56.77%   
[EVAL] batch:   12 | acc: 31.25%,  total acc: 54.81%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   
[EVAL] batch:    1 | acc: 75.00%,  total acc: 84.38%   
[EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   
[EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   
[EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 83.04%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 86.88%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 88.02%   
[EVAL] batch:   12 | acc: 68.75%,  total acc: 86.54%   
[EVAL] batch:   13 | acc: 31.25%,  total acc: 82.59%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 81.67%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 79.78%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 78.82%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 77.96%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 78.12%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 79.17%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 79.83%   
[EVAL] batch:   22 | acc: 93.75%,  total acc: 80.43%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 80.99%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 81.75%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 82.87%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 83.48%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 84.05%   
[EVAL] batch:   29 | acc: 75.00%,  total acc: 83.75%   
[EVAL] batch:   30 | acc: 93.75%,  total acc: 84.07%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 84.57%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 83.14%   
[EVAL] batch:   33 | acc: 0.00%,  total acc: 80.70%   
[EVAL] batch:   34 | acc: 0.00%,  total acc: 78.39%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 76.22%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 74.16%   
[EVAL] batch:   37 | acc: 0.00%,  total acc: 72.20%   
[EVAL] batch:   38 | acc: 31.25%,  total acc: 71.15%   
[EVAL] batch:   39 | acc: 87.50%,  total acc: 71.56%   
[EVAL] batch:   40 | acc: 37.50%,  total acc: 70.73%   
[EVAL] batch:   41 | acc: 18.75%,  total acc: 69.49%   
[EVAL] batch:   42 | acc: 25.00%,  total acc: 68.46%   
[EVAL] batch:   43 | acc: 62.50%,  total acc: 68.32%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 69.03%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 69.70%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 70.35%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 70.96%   
[EVAL] batch:   48 | acc: 100.00%,  total acc: 71.56%   
[EVAL] batch:   49 | acc: 100.00%,  total acc: 72.12%   
[EVAL] batch:   50 | acc: 68.75%,  total acc: 72.06%   
[EVAL] batch:   51 | acc: 18.75%,  total acc: 71.03%   
[EVAL] batch:   52 | acc: 6.25%,  total acc: 69.81%   
[EVAL] batch:   53 | acc: 6.25%,  total acc: 68.63%   
[EVAL] batch:   54 | acc: 12.50%,  total acc: 67.61%   
[EVAL] batch:   55 | acc: 0.00%,  total acc: 66.41%   
[EVAL] batch:   56 | acc: 68.75%,  total acc: 66.45%   
[EVAL] batch:   57 | acc: 93.75%,  total acc: 66.92%   
[EVAL] batch:   58 | acc: 87.50%,  total acc: 67.27%   
[EVAL] batch:   59 | acc: 93.75%,  total acc: 67.71%   
[EVAL] batch:   60 | acc: 75.00%,  total acc: 67.83%   
[EVAL] batch:   61 | acc: 87.50%,  total acc: 68.15%   
[EVAL] batch:   62 | acc: 87.50%,  total acc: 68.45%   
[EVAL] batch:   63 | acc: 75.00%,  total acc: 68.55%   
[EVAL] batch:   64 | acc: 100.00%,  total acc: 69.04%   
[EVAL] batch:   65 | acc: 100.00%,  total acc: 69.51%   
[EVAL] batch:   66 | acc: 100.00%,  total acc: 69.96%   
[EVAL] batch:   67 | acc: 100.00%,  total acc: 70.40%   
[EVAL] batch:   68 | acc: 100.00%,  total acc: 70.83%   
[EVAL] batch:   69 | acc: 100.00%,  total acc: 71.25%   
[EVAL] batch:   70 | acc: 100.00%,  total acc: 71.65%   
[EVAL] batch:   71 | acc: 93.75%,  total acc: 71.96%   
[EVAL] batch:   72 | acc: 43.75%,  total acc: 71.58%   
[EVAL] batch:   73 | acc: 62.50%,  total acc: 71.45%   
[EVAL] batch:   74 | acc: 56.25%,  total acc: 71.25%   
[EVAL] batch:   75 | acc: 75.00%,  total acc: 71.30%   
[EVAL] batch:   76 | acc: 56.25%,  total acc: 71.10%   
[EVAL] batch:   77 | acc: 43.75%,  total acc: 70.75%   
[EVAL] batch:   78 | acc: 75.00%,  total acc: 70.81%   
[EVAL] batch:   79 | acc: 50.00%,  total acc: 70.55%   
[EVAL] batch:   80 | acc: 50.00%,  total acc: 70.29%   
[EVAL] batch:   81 | acc: 50.00%,  total acc: 70.05%   
[EVAL] batch:   82 | acc: 43.75%,  total acc: 69.73%   
[EVAL] batch:   83 | acc: 62.50%,  total acc: 69.64%   
[EVAL] batch:   84 | acc: 75.00%,  total acc: 69.71%   
[EVAL] batch:   85 | acc: 37.50%,  total acc: 69.33%   
[EVAL] batch:   86 | acc: 56.25%,  total acc: 69.18%   
[EVAL] batch:   87 | acc: 62.50%,  total acc: 69.11%   
[EVAL] batch:   88 | acc: 68.75%,  total acc: 69.10%   
[EVAL] batch:   89 | acc: 37.50%,  total acc: 68.75%   
cur_acc:  ['0.8750', '0.8368', '0.7188', '0.8125', '0.5481']
his_acc:  ['0.8750', '0.8562', '0.7725', '0.7492', '0.6875']
CurrentTrain: epoch  0, batch     0 | loss: 5.6648269
CurrentTrain: epoch  0, batch     1 | loss: 7.5669632
CurrentTrain: epoch  1, batch     0 | loss: 5.0429354
CurrentTrain: epoch  1, batch     1 | loss: 4.8021789
CurrentTrain: epoch  2, batch     0 | loss: 4.0663624
CurrentTrain: epoch  2, batch     1 | loss: 4.0209904
CurrentTrain: epoch  3, batch     0 | loss: 3.5280232
CurrentTrain: epoch  3, batch     1 | loss: 3.1753838
CurrentTrain: epoch  4, batch     0 | loss: 3.1380219
CurrentTrain: epoch  4, batch     1 | loss: 3.2556119
CurrentTrain: epoch  5, batch     0 | loss: 3.1364579
CurrentTrain: epoch  5, batch     1 | loss: 3.0417299
CurrentTrain: epoch  6, batch     0 | loss: 2.9855964
CurrentTrain: epoch  6, batch     1 | loss: 2.3778009
CurrentTrain: epoch  7, batch     0 | loss: 2.7024007
CurrentTrain: epoch  7, batch     1 | loss: 2.7317479
CurrentTrain: epoch  8, batch     0 | loss: 2.6878273
CurrentTrain: epoch  8, batch     1 | loss: 2.3800790
CurrentTrain: epoch  9, batch     0 | loss: 2.4702554
CurrentTrain: epoch  9, batch     1 | loss: 2.2107370
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: the chairman of the senate foreign relations committee , massachusetts democrat john kerry , and the panel 's top republican , richard lugar of indiana , were at the white house meeting , which was led by vice president joe biden , a former chairman of the foreign relations panel .
Head Entity: john kerry
Tail Entity: massachusetts
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York City, the famous author J.K. Rowling settled in Edinburgh, where she found inspiration for her next book.  
Head Entity: J.K. Rowling  
Tail Entity: Edinburgh  

Relation: person stateorprovinces of residence  
Context: During the press conference, the governor of California, Gavin Newsom, announced new policies aimed at improving the state's economy and environment.  
Head Entity: Gavin Newsom  
Tail Entity: California  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: former gov. robert e. mcnair of south carolina , the political moderate who was a finalist to become vice president hubert h. humphrey 's running mate in 1968 but whose promising career was cut short by what became known as the orangeburg massacre , died on nov. 17 in charleston .
Head Entity: robert e. mcnair
Tail Entity: nov. 17
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: renowned physicist stephen hawking, who made groundbreaking contributions to our understanding of black holes and the universe, passed away peacefully at his home in cambridge on march 14, 2018.  
Head Entity: stephen hawking  
Tail Entity: march 14, 2018  

Relation: person date of death  
Context: the beloved author of the harry potter series, j.k. rowling, announced the passing of her dear friend and mentor, who died on january 1, 2020, after a long battle with illness.  
Head Entity: j.k. rowling's dear friend  
Tail Entity: january 1, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company, Innovatech, has rapidly expanded its workforce over the past year, now boasting a total of 25,000 employees across its global offices.  
Head Entity: Innovatech  
Tail Entity: 25,000  

Relation: organization number of employees members  
Context: After the merger, the newly formed entity, Global Solutions Inc., reported an impressive count of 50,000 employees, making it one of the largest firms in the industry.  
Head Entity: Global Solutions Inc.  
Tail Entity: 50,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Langhorne Clemens, better known by his pen name Mark Twain, is celebrated for his novels like "The Adventures of Tom Sawyer."  
Head Entity: Samuel Langhorne Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician Stefani Joanne Angelina Germanotta, who is widely recognized as Lady Gaga, has made a significant impact on the pop music scene.  
Head Entity: Stefani Joanne Angelina Germanotta  
Tail Entity: Lady Gaga  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: when her husband retired from congress in 1977 , mrs. gude was urged to run for his seat or for governor , but she had no interest in holding office herself , despite her lifelong interest in politics .
Head Entity: she
Tail Entity: his
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: After years of marriage, John and Mary decided to celebrate their anniversary with a trip to Paris, where they reminisced about their life together.  
Head Entity: John  
Tail Entity: Mary  

Relation: person spouse  
Context: During the family gathering, Sarah shared stories about her late husband, emphasizing the love and support they had for each other throughout their years together.  
Head Entity: Sarah  
Tail Entity: her husband  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 2.3666050
MixupTrain:  epoch  0, batch     1 | loss: 2.2255249
MixupTrain:  epoch  0, batch     2 | loss: 2.2566152
MixupTrain:  epoch  0, batch     3 | loss: 2.2554893
MixupTrain:  epoch  0, batch     4 | loss: 2.2744298
MixupTrain:  epoch  0, batch     5 | loss: 2.2852933
MixupTrain:  epoch  0, batch     6 | loss: 2.4248431
MixupTrain:  epoch  0, batch     7 | loss: 2.1410489
MixupTrain:  epoch  0, batch     8 | loss: 2.5243351
MixupTrain:  epoch  0, batch     9 | loss: 2.4945967
MixupTrain:  epoch  0, batch    10 | loss: 2.1397171
MixupTrain:  epoch  0, batch    11 | loss: 2.2915719
MixupTrain:  epoch  0, batch    12 | loss: 2.5334466
MixupTrain:  epoch  0, batch    13 | loss: 2.3820148
MemoryTrain:  epoch  0, batch     0 | loss: 2.3148947
MemoryTrain:  epoch  0, batch     1 | loss: 2.6008389
MemoryTrain:  epoch  0, batch     2 | loss: 1.8834574
MemoryTrain:  epoch  0, batch     3 | loss: 3.0966501
MemoryTrain:  epoch  0, batch     4 | loss: 2.2612219
MemoryTrain:  epoch  0, batch     5 | loss: 2.4846425
MemoryTrain:  epoch  1, batch     0 | loss: 2.7409353
MemoryTrain:  epoch  1, batch     1 | loss: 1.6148785
MemoryTrain:  epoch  1, batch     2 | loss: 1.9189357
MemoryTrain:  epoch  1, batch     3 | loss: 2.6764112
MemoryTrain:  epoch  1, batch     4 | loss: 2.4370534
MemoryTrain:  epoch  1, batch     5 | loss: 2.7945123
MemoryTrain:  epoch  2, batch     0 | loss: 2.3832302
MemoryTrain:  epoch  2, batch     1 | loss: 2.0476158
MemoryTrain:  epoch  2, batch     2 | loss: 1.5510111
MemoryTrain:  epoch  2, batch     3 | loss: 1.9965760
MemoryTrain:  epoch  2, batch     4 | loss: 1.8585299
MemoryTrain:  epoch  2, batch     5 | loss: 2.9751878
MemoryTrain:  epoch  3, batch     0 | loss: 2.0393729
MemoryTrain:  epoch  3, batch     1 | loss: 2.3294897
MemoryTrain:  epoch  3, batch     2 | loss: 1.7949314
MemoryTrain:  epoch  3, batch     3 | loss: 2.1401393
MemoryTrain:  epoch  3, batch     4 | loss: 1.6378872
MemoryTrain:  epoch  3, batch     5 | loss: 1.5595046
MemoryTrain:  epoch  4, batch     0 | loss: 1.7835393
MemoryTrain:  epoch  4, batch     1 | loss: 1.7330828
MemoryTrain:  epoch  4, batch     2 | loss: 1.3188680
MemoryTrain:  epoch  4, batch     3 | loss: 2.0990880
MemoryTrain:  epoch  4, batch     4 | loss: 1.8213762
MemoryTrain:  epoch  4, batch     5 | loss: 1.6925950
MemoryTrain:  epoch  5, batch     0 | loss: 1.6396302
MemoryTrain:  epoch  5, batch     1 | loss: 1.3936919
MemoryTrain:  epoch  5, batch     2 | loss: 1.9389591
MemoryTrain:  epoch  5, batch     3 | loss: 1.7200705
MemoryTrain:  epoch  5, batch     4 | loss: 1.3956940
MemoryTrain:  epoch  5, batch     5 | loss: 1.7819408
MemoryTrain:  epoch  6, batch     0 | loss: 1.3981518
MemoryTrain:  epoch  6, batch     1 | loss: 1.8015120
MemoryTrain:  epoch  6, batch     2 | loss: 1.4941182
MemoryTrain:  epoch  6, batch     3 | loss: 1.7057196
MemoryTrain:  epoch  6, batch     4 | loss: 1.3837159
MemoryTrain:  epoch  6, batch     5 | loss: 1.4537879
MemoryTrain:  epoch  7, batch     0 | loss: 1.5335042
MemoryTrain:  epoch  7, batch     1 | loss: 1.5748773
MemoryTrain:  epoch  7, batch     2 | loss: 1.5276802
MemoryTrain:  epoch  7, batch     3 | loss: 1.4113133
MemoryTrain:  epoch  7, batch     4 | loss: 1.5062174
MemoryTrain:  epoch  7, batch     5 | loss: 1.4562217
MemoryTrain:  epoch  8, batch     0 | loss: 1.7907988
MemoryTrain:  epoch  8, batch     1 | loss: 1.3549808
MemoryTrain:  epoch  8, batch     2 | loss: 1.4939739
MemoryTrain:  epoch  8, batch     3 | loss: 1.3243810
MemoryTrain:  epoch  8, batch     4 | loss: 1.3818330
MemoryTrain:  epoch  8, batch     5 | loss: 1.4967878
MemoryTrain:  epoch  9, batch     0 | loss: 1.7203906
MemoryTrain:  epoch  9, batch     1 | loss: 1.5285259
MemoryTrain:  epoch  9, batch     2 | loss: 1.3490307
MemoryTrain:  epoch  9, batch     3 | loss: 1.2983768
MemoryTrain:  epoch  9, batch     4 | loss: 1.3709309
MemoryTrain:  epoch  9, batch     5 | loss: 1.4518512
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   
[EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   
[EVAL] batch:    2 | acc: 43.75%,  total acc: 50.00%   
[EVAL] batch:    3 | acc: 75.00%,  total acc: 56.25%   
[EVAL] batch:    4 | acc: 50.00%,  total acc: 55.00%   
[EVAL] batch:    5 | acc: 75.00%,  total acc: 58.33%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 63.39%   
[EVAL] batch:    7 | acc: 93.75%,  total acc: 67.19%   
[EVAL] batch:    8 | acc: 81.25%,  total acc: 68.75%   
[EVAL] batch:    9 | acc: 81.25%,  total acc: 70.00%   
[EVAL] batch:   10 | acc: 68.75%,  total acc: 69.89%   
[EVAL] batch:   11 | acc: 62.50%,  total acc: 69.27%   
[EVAL] batch:   12 | acc: 43.75%,  total acc: 67.31%   
[EVAL] batch:   13 | acc: 50.00%,  total acc: 66.07%   
[EVAL] batch:   14 | acc: 25.00%,  total acc: 63.33%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   
[EVAL] batch:    2 | acc: 75.00%,  total acc: 81.25%   
[EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   
[EVAL] batch:    4 | acc: 81.25%,  total acc: 78.75%   
[EVAL] batch:    5 | acc: 62.50%,  total acc: 76.04%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 79.46%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 84.03%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 85.00%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 86.36%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 86.46%   
[EVAL] batch:   12 | acc: 68.75%,  total acc: 85.10%   
[EVAL] batch:   13 | acc: 31.25%,  total acc: 81.25%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 80.42%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 78.91%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 78.68%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 77.78%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 76.97%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 77.19%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 78.27%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 78.98%   
[EVAL] batch:   22 | acc: 93.75%,  total acc: 79.62%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 80.21%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 81.00%   
[EVAL] batch:   25 | acc: 93.75%,  total acc: 81.49%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 81.94%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 82.59%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 83.19%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 83.12%   
[EVAL] batch:   30 | acc: 93.75%,  total acc: 83.47%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 83.98%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 82.58%   
[EVAL] batch:   33 | acc: 0.00%,  total acc: 80.15%   
[EVAL] batch:   34 | acc: 0.00%,  total acc: 77.86%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 75.69%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 73.65%   
[EVAL] batch:   37 | acc: 0.00%,  total acc: 71.71%   
[EVAL] batch:   38 | acc: 31.25%,  total acc: 70.67%   
[EVAL] batch:   39 | acc: 87.50%,  total acc: 71.09%   
[EVAL] batch:   40 | acc: 50.00%,  total acc: 70.58%   
[EVAL] batch:   41 | acc: 43.75%,  total acc: 69.94%   
[EVAL] batch:   42 | acc: 50.00%,  total acc: 69.48%   
[EVAL] batch:   43 | acc: 68.75%,  total acc: 69.46%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 70.14%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 70.79%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 71.41%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 72.01%   
[EVAL] batch:   48 | acc: 100.00%,  total acc: 72.58%   
[EVAL] batch:   49 | acc: 93.75%,  total acc: 73.00%   
[EVAL] batch:   50 | acc: 68.75%,  total acc: 72.92%   
[EVAL] batch:   51 | acc: 12.50%,  total acc: 71.75%   
[EVAL] batch:   52 | acc: 12.50%,  total acc: 70.64%   
[EVAL] batch:   53 | acc: 0.00%,  total acc: 69.33%   
[EVAL] batch:   54 | acc: 18.75%,  total acc: 68.41%   
[EVAL] batch:   55 | acc: 0.00%,  total acc: 67.19%   
[EVAL] batch:   56 | acc: 62.50%,  total acc: 67.11%   
[EVAL] batch:   57 | acc: 93.75%,  total acc: 67.56%   
[EVAL] batch:   58 | acc: 93.75%,  total acc: 68.01%   
[EVAL] batch:   59 | acc: 87.50%,  total acc: 68.33%   
[EVAL] batch:   60 | acc: 75.00%,  total acc: 68.44%   
[EVAL] batch:   61 | acc: 87.50%,  total acc: 68.75%   
[EVAL] batch:   62 | acc: 81.25%,  total acc: 68.95%   
[EVAL] batch:   63 | acc: 75.00%,  total acc: 69.04%   
[EVAL] batch:   64 | acc: 100.00%,  total acc: 69.52%   
[EVAL] batch:   65 | acc: 100.00%,  total acc: 69.98%   
[EVAL] batch:   66 | acc: 100.00%,  total acc: 70.43%   
[EVAL] batch:   67 | acc: 100.00%,  total acc: 70.86%   
[EVAL] batch:   68 | acc: 100.00%,  total acc: 71.29%   
[EVAL] batch:   69 | acc: 100.00%,  total acc: 71.70%   
[EVAL] batch:   70 | acc: 100.00%,  total acc: 72.10%   
[EVAL] batch:   71 | acc: 100.00%,  total acc: 72.48%   
[EVAL] batch:   72 | acc: 37.50%,  total acc: 72.00%   
[EVAL] batch:   73 | acc: 56.25%,  total acc: 71.79%   
[EVAL] batch:   74 | acc: 56.25%,  total acc: 71.58%   
[EVAL] batch:   75 | acc: 75.00%,  total acc: 71.63%   
[EVAL] batch:   76 | acc: 62.50%,  total acc: 71.51%   
[EVAL] batch:   77 | acc: 43.75%,  total acc: 71.15%   
[EVAL] batch:   78 | acc: 56.25%,  total acc: 70.97%   
[EVAL] batch:   79 | acc: 50.00%,  total acc: 70.70%   
[EVAL] batch:   80 | acc: 50.00%,  total acc: 70.45%   
[EVAL] batch:   81 | acc: 37.50%,  total acc: 70.05%   
[EVAL] batch:   82 | acc: 50.00%,  total acc: 69.80%   
[EVAL] batch:   83 | acc: 56.25%,  total acc: 69.64%   
[EVAL] batch:   84 | acc: 56.25%,  total acc: 69.49%   
[EVAL] batch:   85 | acc: 31.25%,  total acc: 69.04%   
[EVAL] batch:   86 | acc: 37.50%,  total acc: 68.68%   
[EVAL] batch:   87 | acc: 43.75%,  total acc: 68.39%   
[EVAL] batch:   88 | acc: 31.25%,  total acc: 67.98%   
[EVAL] batch:   89 | acc: 37.50%,  total acc: 67.64%   
[EVAL] batch:   90 | acc: 50.00%,  total acc: 67.45%   
[EVAL] batch:   91 | acc: 56.25%,  total acc: 67.32%   
[EVAL] batch:   92 | acc: 62.50%,  total acc: 67.27%   
[EVAL] batch:   93 | acc: 56.25%,  total acc: 67.15%   
[EVAL] batch:   94 | acc: 56.25%,  total acc: 67.04%   
[EVAL] batch:   95 | acc: 81.25%,  total acc: 67.19%   
[EVAL] batch:   96 | acc: 100.00%,  total acc: 67.53%   
[EVAL] batch:   97 | acc: 81.25%,  total acc: 67.67%   
[EVAL] batch:   98 | acc: 93.75%,  total acc: 67.93%   
[EVAL] batch:   99 | acc: 75.00%,  total acc: 68.00%   
[EVAL] batch:  100 | acc: 50.00%,  total acc: 67.82%   
[EVAL] batch:  101 | acc: 56.25%,  total acc: 67.71%   
[EVAL] batch:  102 | acc: 56.25%,  total acc: 67.60%   
[EVAL] batch:  103 | acc: 56.25%,  total acc: 67.49%   
[EVAL] batch:  104 | acc: 0.00%,  total acc: 66.85%   
cur_acc:  ['0.8750', '0.8368', '0.7188', '0.8125', '0.5481', '0.6333']
his_acc:  ['0.8750', '0.8562', '0.7725', '0.7492', '0.6875', '0.6685']
CurrentTrain: epoch  0, batch     0 | loss: 6.1160517
CurrentTrain: epoch  0, batch     1 | loss: 5.3962202
CurrentTrain: epoch  1, batch     0 | loss: 4.9999275
CurrentTrain: epoch  1, batch     1 | loss: 5.4856558
CurrentTrain: epoch  2, batch     0 | loss: 3.8183875
CurrentTrain: epoch  2, batch     1 | loss: 4.3848181
CurrentTrain: epoch  3, batch     0 | loss: 3.5587537
CurrentTrain: epoch  3, batch     1 | loss: 4.0977888
CurrentTrain: epoch  4, batch     0 | loss: 3.4746828
CurrentTrain: epoch  4, batch     1 | loss: 2.8343983
CurrentTrain: epoch  5, batch     0 | loss: 3.0598311
CurrentTrain: epoch  5, batch     1 | loss: 3.2276931
CurrentTrain: epoch  6, batch     0 | loss: 3.3089073
CurrentTrain: epoch  6, batch     1 | loss: 2.6220829
CurrentTrain: epoch  7, batch     0 | loss: 3.1306522
CurrentTrain: epoch  7, batch     1 | loss: 2.3549886
CurrentTrain: epoch  8, batch     0 | loss: 2.8464468
CurrentTrain: epoch  8, batch     1 | loss: 2.6277285
CurrentTrain: epoch  9, batch     0 | loss: 2.9469085
CurrentTrain: epoch  9, batch     1 | loss: 2.2828043
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, the renowned artist shared his experiences growing up in the vibrant streets of Barcelona, which he credits as the foundation of his creative journey.  
Head Entity: the renowned artist  
Tail Entity: Barcelona  

Relation: person country of birth  
Context: In her autobiography, the famous scientist details her early life in Tokyo, where she developed a passion for technology and innovation.  
Head Entity: the famous scientist  
Tail Entity: Tokyo  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: 11.30.08 2008 cma awards red carpet special http://www.cmt.com/shows/dyn/2008-cma-awards-red-carpet/1/episode.jhtml
Head Entity: cma
Tail Entity: http://www.cmt.com/shows/dyn/2008-cma-awards-red-carpet/1/episode.jhtml
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: The official site for the American Red Cross can be found at https://www.redcross.org.  
Head Entity: American Red Cross  
Tail Entity: https://www.redcross.org  

Relation: organization website  
Context: For more information about the World Wildlife Fund, visit their website at https://www.worldwildlife.org.  
Head Entity: World Wildlife Fund  
Tail Entity: https://www.worldwildlife.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has recently acquired a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple inc.  

Relation: organization shareholders  
Context: the investment firm blackrock has increased its holdings in the renewable energy company sunrun.  
Head Entity: sunrun  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in the spring of 2018, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring of 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, John Doe, to support underprivileged children around the world.  
Head Entity: Hope for Tomorrow  
Tail Entity: John Doe  
Mixup data size:  255
MixupTrain:  epoch  0, batch     0 | loss: 2.7444551
MixupTrain:  epoch  0, batch     1 | loss: 2.1456831
MixupTrain:  epoch  0, batch     2 | loss: 2.1477151
MixupTrain:  epoch  0, batch     3 | loss: 3.0214014
MixupTrain:  epoch  0, batch     4 | loss: 2.0716054
MixupTrain:  epoch  0, batch     5 | loss: 1.9924300
MixupTrain:  epoch  0, batch     6 | loss: 2.3985205
MixupTrain:  epoch  0, batch     7 | loss: 2.8458338
MixupTrain:  epoch  0, batch     8 | loss: 2.6687949
MixupTrain:  epoch  0, batch     9 | loss: 2.5166254
MixupTrain:  epoch  0, batch    10 | loss: 2.6807399
MixupTrain:  epoch  0, batch    11 | loss: 2.8187172
MixupTrain:  epoch  0, batch    12 | loss: 1.8282349
MixupTrain:  epoch  0, batch    13 | loss: 2.3864138
MixupTrain:  epoch  0, batch    14 | loss: 2.9454427
MixupTrain:  epoch  0, batch    15 | loss: 1.9330162
MemoryTrain:  epoch  0, batch     0 | loss: 3.3103571
MemoryTrain:  epoch  0, batch     1 | loss: 2.1522708
MemoryTrain:  epoch  0, batch     2 | loss: 2.2319698
MemoryTrain:  epoch  0, batch     3 | loss: 2.5784702
MemoryTrain:  epoch  0, batch     4 | loss: 2.1089060
MemoryTrain:  epoch  0, batch     5 | loss: 2.3738298
MemoryTrain:  epoch  0, batch     6 | loss: 3.6237180
MemoryTrain:  epoch  1, batch     0 | loss: 2.9087901
MemoryTrain:  epoch  1, batch     1 | loss: 2.5397398
MemoryTrain:  epoch  1, batch     2 | loss: 2.3671622
MemoryTrain:  epoch  1, batch     3 | loss: 2.4585190
MemoryTrain:  epoch  1, batch     4 | loss: 1.4115112
MemoryTrain:  epoch  1, batch     5 | loss: 2.2484162
MemoryTrain:  epoch  1, batch     6 | loss: 1.7295101
MemoryTrain:  epoch  2, batch     0 | loss: 2.0810819
MemoryTrain:  epoch  2, batch     1 | loss: 1.9785607
MemoryTrain:  epoch  2, batch     2 | loss: 2.0399756
MemoryTrain:  epoch  2, batch     3 | loss: 1.7451386
MemoryTrain:  epoch  2, batch     4 | loss: 1.6104975
MemoryTrain:  epoch  2, batch     5 | loss: 2.0479407
MemoryTrain:  epoch  2, batch     6 | loss: 2.5289927
MemoryTrain:  epoch  3, batch     0 | loss: 1.9735779
MemoryTrain:  epoch  3, batch     1 | loss: 1.7859428
MemoryTrain:  epoch  3, batch     2 | loss: 1.9329531
MemoryTrain:  epoch  3, batch     3 | loss: 1.9746294
MemoryTrain:  epoch  3, batch     4 | loss: 1.4985065
MemoryTrain:  epoch  3, batch     5 | loss: 1.6729250
MemoryTrain:  epoch  3, batch     6 | loss: 1.5956939
MemoryTrain:  epoch  4, batch     0 | loss: 1.8275216
MemoryTrain:  epoch  4, batch     1 | loss: 1.7643145
MemoryTrain:  epoch  4, batch     2 | loss: 1.3289409
MemoryTrain:  epoch  4, batch     3 | loss: 1.7472646
MemoryTrain:  epoch  4, batch     4 | loss: 1.5521798
MemoryTrain:  epoch  4, batch     5 | loss: 1.5683341
MemoryTrain:  epoch  4, batch     6 | loss: 1.6305506
MemoryTrain:  epoch  5, batch     0 | loss: 1.3304231
MemoryTrain:  epoch  5, batch     1 | loss: 1.6417996
MemoryTrain:  epoch  5, batch     2 | loss: 1.4216049
MemoryTrain:  epoch  5, batch     3 | loss: 1.8128242
MemoryTrain:  epoch  5, batch     4 | loss: 1.7593269
MemoryTrain:  epoch  5, batch     5 | loss: 1.5839747
MemoryTrain:  epoch  5, batch     6 | loss: 1.4383159
MemoryTrain:  epoch  6, batch     0 | loss: 1.5337508
MemoryTrain:  epoch  6, batch     1 | loss: 1.5879632
MemoryTrain:  epoch  6, batch     2 | loss: 1.5781388
MemoryTrain:  epoch  6, batch     3 | loss: 1.3593180
MemoryTrain:  epoch  6, batch     4 | loss: 1.6415412
MemoryTrain:  epoch  6, batch     5 | loss: 1.4853073
MemoryTrain:  epoch  6, batch     6 | loss: 1.4900723
MemoryTrain:  epoch  7, batch     0 | loss: 1.5242894
MemoryTrain:  epoch  7, batch     1 | loss: 1.4033606
MemoryTrain:  epoch  7, batch     2 | loss: 1.4836478
MemoryTrain:  epoch  7, batch     3 | loss: 1.6024965
MemoryTrain:  epoch  7, batch     4 | loss: 1.3429081
MemoryTrain:  epoch  7, batch     5 | loss: 1.4062706
MemoryTrain:  epoch  7, batch     6 | loss: 1.6869121
MemoryTrain:  epoch  8, batch     0 | loss: 1.3855441
MemoryTrain:  epoch  8, batch     1 | loss: 1.3629122
MemoryTrain:  epoch  8, batch     2 | loss: 1.3798621
MemoryTrain:  epoch  8, batch     3 | loss: 1.3316023
MemoryTrain:  epoch  8, batch     4 | loss: 1.4359208
MemoryTrain:  epoch  8, batch     5 | loss: 1.4388998
MemoryTrain:  epoch  8, batch     6 | loss: 1.5312626
MemoryTrain:  epoch  9, batch     0 | loss: 1.3058193
MemoryTrain:  epoch  9, batch     1 | loss: 1.4912606
MemoryTrain:  epoch  9, batch     2 | loss: 1.4122088
MemoryTrain:  epoch  9, batch     3 | loss: 1.3493589
MemoryTrain:  epoch  9, batch     4 | loss: 1.6632071
MemoryTrain:  epoch  9, batch     5 | loss: 1.3353343
MemoryTrain:  epoch  9, batch     6 | loss: 1.3341997
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   
[EVAL] batch:    2 | acc: 75.00%,  total acc: 85.42%   
[EVAL] batch:    3 | acc: 31.25%,  total acc: 71.88%   
[EVAL] batch:    4 | acc: 43.75%,  total acc: 66.25%   
[EVAL] batch:    5 | acc: 37.50%,  total acc: 61.46%   
[EVAL] batch:    6 | acc: 31.25%,  total acc: 57.14%   
[EVAL] batch:    7 | acc: 12.50%,  total acc: 51.56%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   
[EVAL] batch:    2 | acc: 56.25%,  total acc: 58.33%   
[EVAL] batch:    3 | acc: 43.75%,  total acc: 54.69%   
[EVAL] batch:    4 | acc: 50.00%,  total acc: 53.75%   
[EVAL] batch:    5 | acc: 43.75%,  total acc: 52.08%   
[EVAL] batch:    6 | acc: 68.75%,  total acc: 54.46%   
[EVAL] batch:    7 | acc: 75.00%,  total acc: 57.03%   
[EVAL] batch:    8 | acc: 75.00%,  total acc: 59.03%   
[EVAL] batch:    9 | acc: 81.25%,  total acc: 61.25%   
[EVAL] batch:   10 | acc: 75.00%,  total acc: 62.50%   
[EVAL] batch:   11 | acc: 75.00%,  total acc: 63.54%   
[EVAL] batch:   12 | acc: 43.75%,  total acc: 62.02%   
[EVAL] batch:   13 | acc: 37.50%,  total acc: 60.27%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 60.83%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 60.55%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 61.40%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 61.46%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 61.51%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 62.50%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 64.29%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 65.62%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 67.12%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 68.23%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 69.50%   
[EVAL] batch:   25 | acc: 93.75%,  total acc: 70.43%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 71.30%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 72.32%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 73.28%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 73.54%   
[EVAL] batch:   30 | acc: 93.75%,  total acc: 74.19%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 74.80%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 73.67%   
[EVAL] batch:   33 | acc: 0.00%,  total acc: 71.51%   
[EVAL] batch:   34 | acc: 0.00%,  total acc: 69.46%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 67.53%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 65.71%   
[EVAL] batch:   37 | acc: 0.00%,  total acc: 63.98%   
[EVAL] batch:   38 | acc: 31.25%,  total acc: 63.14%   
[EVAL] batch:   39 | acc: 87.50%,  total acc: 63.75%   
[EVAL] batch:   40 | acc: 43.75%,  total acc: 63.26%   
[EVAL] batch:   41 | acc: 56.25%,  total acc: 63.10%   
[EVAL] batch:   42 | acc: 50.00%,  total acc: 62.79%   
[EVAL] batch:   43 | acc: 68.75%,  total acc: 62.93%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 63.75%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 64.54%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 65.29%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 66.02%   
[EVAL] batch:   48 | acc: 100.00%,  total acc: 66.71%   
[EVAL] batch:   49 | acc: 93.75%,  total acc: 67.25%   
[EVAL] batch:   50 | acc: 62.50%,  total acc: 67.16%   
[EVAL] batch:   51 | acc: 31.25%,  total acc: 66.47%   
[EVAL] batch:   52 | acc: 31.25%,  total acc: 65.80%   
[EVAL] batch:   53 | acc: 0.00%,  total acc: 64.58%   
[EVAL] batch:   54 | acc: 31.25%,  total acc: 63.98%   
[EVAL] batch:   55 | acc: 0.00%,  total acc: 62.83%   
[EVAL] batch:   56 | acc: 62.50%,  total acc: 62.83%   
[EVAL] batch:   57 | acc: 93.75%,  total acc: 63.36%   
[EVAL] batch:   58 | acc: 81.25%,  total acc: 63.67%   
[EVAL] batch:   59 | acc: 93.75%,  total acc: 64.17%   
[EVAL] batch:   60 | acc: 75.00%,  total acc: 64.34%   
[EVAL] batch:   61 | acc: 75.00%,  total acc: 64.52%   
[EVAL] batch:   62 | acc: 62.50%,  total acc: 64.48%   
[EVAL] batch:   63 | acc: 62.50%,  total acc: 64.45%   
[EVAL] batch:   64 | acc: 100.00%,  total acc: 65.00%   
[EVAL] batch:   65 | acc: 93.75%,  total acc: 65.44%   
[EVAL] batch:   66 | acc: 100.00%,  total acc: 65.95%   
[EVAL] batch:   67 | acc: 100.00%,  total acc: 66.45%   
[EVAL] batch:   68 | acc: 100.00%,  total acc: 66.94%   
[EVAL] batch:   69 | acc: 100.00%,  total acc: 67.41%   
[EVAL] batch:   70 | acc: 100.00%,  total acc: 67.87%   
[EVAL] batch:   71 | acc: 100.00%,  total acc: 68.32%   
[EVAL] batch:   72 | acc: 18.75%,  total acc: 67.64%   
[EVAL] batch:   73 | acc: 25.00%,  total acc: 67.06%   
[EVAL] batch:   74 | acc: 62.50%,  total acc: 67.00%   
[EVAL] batch:   75 | acc: 87.50%,  total acc: 67.27%   
[EVAL] batch:   76 | acc: 75.00%,  total acc: 67.37%   
[EVAL] batch:   77 | acc: 43.75%,  total acc: 67.07%   
[EVAL] batch:   78 | acc: 56.25%,  total acc: 66.93%   
[EVAL] batch:   79 | acc: 43.75%,  total acc: 66.64%   
[EVAL] batch:   80 | acc: 43.75%,  total acc: 66.36%   
[EVAL] batch:   81 | acc: 50.00%,  total acc: 66.16%   
[EVAL] batch:   82 | acc: 50.00%,  total acc: 65.96%   
[EVAL] batch:   83 | acc: 68.75%,  total acc: 66.00%   
[EVAL] batch:   84 | acc: 56.25%,  total acc: 65.88%   
[EVAL] batch:   85 | acc: 31.25%,  total acc: 65.48%   
[EVAL] batch:   86 | acc: 37.50%,  total acc: 65.16%   
[EVAL] batch:   87 | acc: 37.50%,  total acc: 64.84%   
[EVAL] batch:   88 | acc: 31.25%,  total acc: 64.47%   
[EVAL] batch:   89 | acc: 31.25%,  total acc: 64.10%   
[EVAL] batch:   90 | acc: 43.75%,  total acc: 63.87%   
[EVAL] batch:   91 | acc: 31.25%,  total acc: 63.52%   
[EVAL] batch:   92 | acc: 56.25%,  total acc: 63.44%   
[EVAL] batch:   93 | acc: 43.75%,  total acc: 63.23%   
[EVAL] batch:   94 | acc: 50.00%,  total acc: 63.09%   
[EVAL] batch:   95 | acc: 75.00%,  total acc: 63.22%   
[EVAL] batch:   96 | acc: 100.00%,  total acc: 63.60%   
[EVAL] batch:   97 | acc: 81.25%,  total acc: 63.78%   
[EVAL] batch:   98 | acc: 100.00%,  total acc: 64.14%   
[EVAL] batch:   99 | acc: 68.75%,  total acc: 64.19%   
[EVAL] batch:  100 | acc: 75.00%,  total acc: 64.29%   
[EVAL] batch:  101 | acc: 87.50%,  total acc: 64.52%   
[EVAL] batch:  102 | acc: 87.50%,  total acc: 64.75%   
[EVAL] batch:  103 | acc: 87.50%,  total acc: 64.96%   
[EVAL] batch:  104 | acc: 81.25%,  total acc: 65.12%   
[EVAL] batch:  105 | acc: 100.00%,  total acc: 65.45%   
[EVAL] batch:  106 | acc: 75.00%,  total acc: 65.54%   
[EVAL] batch:  107 | acc: 37.50%,  total acc: 65.28%   
[EVAL] batch:  108 | acc: 37.50%,  total acc: 65.02%   
[EVAL] batch:  109 | acc: 37.50%,  total acc: 64.77%   
[EVAL] batch:  110 | acc: 37.50%,  total acc: 64.53%   
[EVAL] batch:  111 | acc: 12.50%,  total acc: 64.06%   
cur_acc:  ['0.8750', '0.8368', '0.7188', '0.8125', '0.5481', '0.6333', '0.5156']
his_acc:  ['0.8750', '0.8562', '0.7725', '0.7492', '0.6875', '0.6685', '0.6406']
CurrentTrain: epoch  0, batch     0 | loss: 7.8712959
CurrentTrain: epoch  0, batch     1 | loss: 9.1300020
CurrentTrain: epoch  1, batch     0 | loss: 7.5237007
CurrentTrain: epoch  1, batch     1 | loss: 6.2392449
CurrentTrain: epoch  2, batch     0 | loss: 6.2682538
CurrentTrain: epoch  2, batch     1 | loss: 6.8177319
CurrentTrain: epoch  3, batch     0 | loss: 5.9203100
CurrentTrain: epoch  3, batch     1 | loss: 6.2872357
CurrentTrain: epoch  4, batch     0 | loss: 5.5524759
CurrentTrain: epoch  4, batch     1 | loss: 6.0548344
CurrentTrain: epoch  5, batch     0 | loss: 5.5036774
CurrentTrain: epoch  5, batch     1 | loss: 5.2174335
CurrentTrain: epoch  6, batch     0 | loss: 5.0499969
CurrentTrain: epoch  6, batch     1 | loss: 4.6951795
CurrentTrain: epoch  7, batch     0 | loss: 4.7820053
CurrentTrain: epoch  7, batch     1 | loss: 4.3160276
CurrentTrain: epoch  8, batch     0 | loss: 4.6051908
CurrentTrain: epoch  8, batch     1 | loss: 3.7232234
CurrentTrain: epoch  9, batch     0 | loss: 3.3806400
CurrentTrain: epoch  9, batch     1 | loss: 5.3171539
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including YouTube, which it acquired in 2006 for $1.65 billion.  
Head Entity: Alphabet Inc.  
Tail Entity: YouTube  

Relation: organization subsidiaries  
Context: The Coca-Cola Company owns numerous subsidiaries, such as Minute Maid, which specializes in fruit juices and drinks.  
Head Entity: The Coca-Cola Company  
Tail Entity: Minute Maid  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant Alphabet Inc. announced that it would acquire the innovative startup DeepMind Technologies, which has been a subsidiary of Alphabet since 2014. This acquisition is expected to enhance Alphabet's capabilities in artificial intelligence.  
Head Entity: DeepMind Technologies  
Tail Entity: Alphabet Inc.  

Relation: organization parents  
Context: The historical records indicate that the renowned publishing house Penguin Random House was formed through the merger of Penguin Group and Random House, both of which have a rich legacy in the literary world.  
Head Entity: Penguin Random House  
Tail Entity: Penguin Group  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health crises.  
Head Entity: WHO  
Tail Entity: World Health Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant apple inc. has announced plans to expand its headquarters in the heart of silicon valley, which is located in cupertino, california, aiming to create thousands of new jobs in the area.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ new york 2019-11-22 14:45:00 utc the financial services firm goldman sachs has its main office situated in lower manhattan, new york city, where it continues to play a pivotal role in global finance.  
Head Entity: goldman sachs  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time catching up. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily celebrated with her brother, who had just completed his degree in engineering. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  285
MixupTrain:  epoch  0, batch     0 | loss: 2.3437748
MixupTrain:  epoch  0, batch     1 | loss: 2.8087320
MixupTrain:  epoch  0, batch     2 | loss: 2.6035442
MixupTrain:  epoch  0, batch     3 | loss: 2.4680662
MixupTrain:  epoch  0, batch     4 | loss: 2.5991683
MixupTrain:  epoch  0, batch     5 | loss: 2.5148239
MixupTrain:  epoch  0, batch     6 | loss: 1.9912302
MixupTrain:  epoch  0, batch     7 | loss: 2.0867488
MixupTrain:  epoch  0, batch     8 | loss: 2.2874455
MixupTrain:  epoch  0, batch     9 | loss: 2.6448374
MixupTrain:  epoch  0, batch    10 | loss: 2.3627791
MixupTrain:  epoch  0, batch    11 | loss: 2.5044770
MixupTrain:  epoch  0, batch    12 | loss: 2.3671496
MixupTrain:  epoch  0, batch    13 | loss: 2.4464405
MixupTrain:  epoch  0, batch    14 | loss: 1.9804640
MixupTrain:  epoch  0, batch    15 | loss: 2.0975997
MixupTrain:  epoch  0, batch    16 | loss: 1.9058270
MixupTrain:  epoch  0, batch    17 | loss: 2.1462831
MemoryTrain:  epoch  0, batch     0 | loss: 2.4077363
MemoryTrain:  epoch  0, batch     1 | loss: 1.9491837
MemoryTrain:  epoch  0, batch     2 | loss: 2.3503089
MemoryTrain:  epoch  0, batch     3 | loss: 3.2980969
MemoryTrain:  epoch  0, batch     4 | loss: 1.8928629
MemoryTrain:  epoch  0, batch     5 | loss: 2.5777721
MemoryTrain:  epoch  0, batch     6 | loss: 2.1586261
MemoryTrain:  epoch  0, batch     7 | loss: 3.5089531
MemoryTrain:  epoch  1, batch     0 | loss: 1.9880177
MemoryTrain:  epoch  1, batch     1 | loss: 2.5278966
MemoryTrain:  epoch  1, batch     2 | loss: 2.4227631
MemoryTrain:  epoch  1, batch     3 | loss: 2.6422729
MemoryTrain:  epoch  1, batch     4 | loss: 1.9571658
MemoryTrain:  epoch  1, batch     5 | loss: 1.9565768
MemoryTrain:  epoch  1, batch     6 | loss: 2.4061482
MemoryTrain:  epoch  1, batch     7 | loss: 1.7528281
MemoryTrain:  epoch  2, batch     0 | loss: 2.2076960
MemoryTrain:  epoch  2, batch     1 | loss: 1.6801574
MemoryTrain:  epoch  2, batch     2 | loss: 2.0618510
MemoryTrain:  epoch  2, batch     3 | loss: 1.7931809
MemoryTrain:  epoch  2, batch     4 | loss: 2.3625023
MemoryTrain:  epoch  2, batch     5 | loss: 1.7395501
MemoryTrain:  epoch  2, batch     6 | loss: 2.1284213
MemoryTrain:  epoch  2, batch     7 | loss: 1.8063148
MemoryTrain:  epoch  3, batch     0 | loss: 1.8360182
MemoryTrain:  epoch  3, batch     1 | loss: 1.8705201
MemoryTrain:  epoch  3, batch     2 | loss: 1.6958715
MemoryTrain:  epoch  3, batch     3 | loss: 1.6966338
MemoryTrain:  epoch  3, batch     4 | loss: 2.3274422
MemoryTrain:  epoch  3, batch     5 | loss: 1.5743840
MemoryTrain:  epoch  3, batch     6 | loss: 1.7482684
MemoryTrain:  epoch  3, batch     7 | loss: 1.5930450
MemoryTrain:  epoch  4, batch     0 | loss: 1.6244519
MemoryTrain:  epoch  4, batch     1 | loss: 1.6144949
MemoryTrain:  epoch  4, batch     2 | loss: 1.5392437
MemoryTrain:  epoch  4, batch     3 | loss: 1.3594602
MemoryTrain:  epoch  4, batch     4 | loss: 2.1959505
MemoryTrain:  epoch  4, batch     5 | loss: 1.9929296
MemoryTrain:  epoch  4, batch     6 | loss: 1.5918925
MemoryTrain:  epoch  4, batch     7 | loss: 1.4094340
MemoryTrain:  epoch  5, batch     0 | loss: 1.3491051
MemoryTrain:  epoch  5, batch     1 | loss: 1.7022555
MemoryTrain:  epoch  5, batch     2 | loss: 1.7093174
MemoryTrain:  epoch  5, batch     3 | loss: 1.4171300
MemoryTrain:  epoch  5, batch     4 | loss: 1.7740321
MemoryTrain:  epoch  5, batch     5 | loss: 1.4180163
MemoryTrain:  epoch  5, batch     6 | loss: 1.7156423
MemoryTrain:  epoch  5, batch     7 | loss: 2.1212914
MemoryTrain:  epoch  6, batch     0 | loss: 1.4317043
MemoryTrain:  epoch  6, batch     1 | loss: 1.5650213
MemoryTrain:  epoch  6, batch     2 | loss: 1.5934319
MemoryTrain:  epoch  6, batch     3 | loss: 1.3934251
MemoryTrain:  epoch  6, batch     4 | loss: 1.4912927
MemoryTrain:  epoch  6, batch     5 | loss: 1.7463301
MemoryTrain:  epoch  6, batch     6 | loss: 2.2058692
MemoryTrain:  epoch  6, batch     7 | loss: 1.5349776
MemoryTrain:  epoch  7, batch     0 | loss: 1.5144048
MemoryTrain:  epoch  7, batch     1 | loss: 1.3852272
MemoryTrain:  epoch  7, batch     2 | loss: 1.5519600
MemoryTrain:  epoch  7, batch     3 | loss: 1.3851929
MemoryTrain:  epoch  7, batch     4 | loss: 1.3639386
MemoryTrain:  epoch  7, batch     5 | loss: 1.2124338
MemoryTrain:  epoch  7, batch     6 | loss: 1.5565337
MemoryTrain:  epoch  7, batch     7 | loss: 2.0089107
MemoryTrain:  epoch  8, batch     0 | loss: 1.4640758
MemoryTrain:  epoch  8, batch     1 | loss: 1.4114523
MemoryTrain:  epoch  8, batch     2 | loss: 1.2936518
MemoryTrain:  epoch  8, batch     3 | loss: 1.9732871
MemoryTrain:  epoch  8, batch     4 | loss: 1.3121191
MemoryTrain:  epoch  8, batch     5 | loss: 1.4895365
MemoryTrain:  epoch  8, batch     6 | loss: 1.3794882
MemoryTrain:  epoch  8, batch     7 | loss: 1.3274015
MemoryTrain:  epoch  9, batch     0 | loss: 1.2351069
MemoryTrain:  epoch  9, batch     1 | loss: 1.7434876
MemoryTrain:  epoch  9, batch     2 | loss: 1.3526793
MemoryTrain:  epoch  9, batch     3 | loss: 1.4691579
MemoryTrain:  epoch  9, batch     4 | loss: 1.4756825
MemoryTrain:  epoch  9, batch     5 | loss: 1.4466983
MemoryTrain:  epoch  9, batch     6 | loss: 1.4691149
MemoryTrain:  epoch  9, batch     7 | loss: 1.3586054
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   
[EVAL] batch:    1 | acc: 31.25%,  total acc: 34.38%   
[EVAL] batch:    2 | acc: 31.25%,  total acc: 33.33%   
[EVAL] batch:    3 | acc: 12.50%,  total acc: 28.12%   
[EVAL] batch:    4 | acc: 31.25%,  total acc: 28.75%   
[EVAL] batch:    5 | acc: 43.75%,  total acc: 31.25%   
[EVAL] batch:    6 | acc: 31.25%,  total acc: 31.25%   
[EVAL] batch:    7 | acc: 93.75%,  total acc: 39.06%   
[EVAL] batch:    8 | acc: 50.00%,  total acc: 40.28%   
[EVAL] batch:    9 | acc: 68.75%,  total acc: 43.12%   
[EVAL] batch:   10 | acc: 81.25%,  total acc: 46.59%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 50.00%   
[EVAL] batch:   12 | acc: 81.25%,  total acc: 52.40%   
[EVAL] batch:   13 | acc: 43.75%,  total acc: 51.79%   
[EVAL] batch:   14 | acc: 37.50%,  total acc: 50.83%   
[EVAL] batch:   15 | acc: 68.75%,  total acc: 51.95%   
[EVAL] batch:   16 | acc: 50.00%,  total acc: 51.84%   
[EVAL] batch:   17 | acc: 31.25%,  total acc: 50.69%   
[EVAL] batch:   18 | acc: 0.00%,  total acc: 48.03%   
[EVAL] batch:   19 | acc: 6.25%,  total acc: 45.94%   
[EVAL] batch:   20 | acc: 6.25%,  total acc: 44.05%   
[EVAL] batch:   21 | acc: 0.00%,  total acc: 42.05%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   
[EVAL] batch:    2 | acc: 43.75%,  total acc: 54.17%   
[EVAL] batch:    3 | acc: 37.50%,  total acc: 50.00%   
[EVAL] batch:    4 | acc: 50.00%,  total acc: 50.00%   
[EVAL] batch:    5 | acc: 31.25%,  total acc: 46.88%   
[EVAL] batch:    6 | acc: 68.75%,  total acc: 50.00%   
[EVAL] batch:    7 | acc: 81.25%,  total acc: 53.91%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 58.33%   
[EVAL] batch:    9 | acc: 75.00%,  total acc: 60.00%   
[EVAL] batch:   10 | acc: 81.25%,  total acc: 61.93%   
[EVAL] batch:   11 | acc: 81.25%,  total acc: 63.54%   
[EVAL] batch:   12 | acc: 50.00%,  total acc: 62.50%   
[EVAL] batch:   13 | acc: 31.25%,  total acc: 60.27%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 60.83%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 60.55%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 61.40%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 61.46%   
[EVAL] batch:   18 | acc: 68.75%,  total acc: 61.84%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 63.12%   
[EVAL] batch:   20 | acc: 93.75%,  total acc: 64.58%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 65.91%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 67.39%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 68.49%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 69.75%   
[EVAL] batch:   25 | acc: 93.75%,  total acc: 70.67%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 71.53%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 72.54%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 73.49%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 73.75%   
[EVAL] batch:   30 | acc: 93.75%,  total acc: 74.40%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 75.00%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 73.86%   
[EVAL] batch:   33 | acc: 0.00%,  total acc: 71.69%   
[EVAL] batch:   34 | acc: 0.00%,  total acc: 69.64%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 67.71%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 65.88%   
[EVAL] batch:   37 | acc: 0.00%,  total acc: 64.14%   
[EVAL] batch:   38 | acc: 31.25%,  total acc: 63.30%   
[EVAL] batch:   39 | acc: 87.50%,  total acc: 63.91%   
[EVAL] batch:   40 | acc: 31.25%,  total acc: 63.11%   
[EVAL] batch:   41 | acc: 50.00%,  total acc: 62.80%   
[EVAL] batch:   42 | acc: 50.00%,  total acc: 62.50%   
[EVAL] batch:   43 | acc: 68.75%,  total acc: 62.64%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 63.47%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 64.27%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 65.03%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 65.76%   
[EVAL] batch:   48 | acc: 100.00%,  total acc: 66.45%   
[EVAL] batch:   49 | acc: 93.75%,  total acc: 67.00%   
[EVAL] batch:   50 | acc: 56.25%,  total acc: 66.79%   
[EVAL] batch:   51 | acc: 6.25%,  total acc: 65.62%   
[EVAL] batch:   52 | acc: 6.25%,  total acc: 64.50%   
[EVAL] batch:   53 | acc: 0.00%,  total acc: 63.31%   
[EVAL] batch:   54 | acc: 6.25%,  total acc: 62.27%   
[EVAL] batch:   55 | acc: 0.00%,  total acc: 61.16%   
[EVAL] batch:   56 | acc: 62.50%,  total acc: 61.18%   
[EVAL] batch:   57 | acc: 93.75%,  total acc: 61.75%   
[EVAL] batch:   58 | acc: 81.25%,  total acc: 62.08%   
[EVAL] batch:   59 | acc: 93.75%,  total acc: 62.60%   
[EVAL] batch:   60 | acc: 62.50%,  total acc: 62.60%   
[EVAL] batch:   61 | acc: 87.50%,  total acc: 63.00%   
[EVAL] batch:   62 | acc: 62.50%,  total acc: 63.00%   
[EVAL] batch:   63 | acc: 62.50%,  total acc: 62.99%   
[EVAL] batch:   64 | acc: 100.00%,  total acc: 63.56%   
[EVAL] batch:   65 | acc: 93.75%,  total acc: 64.02%   
[EVAL] batch:   66 | acc: 100.00%,  total acc: 64.55%   
[EVAL] batch:   67 | acc: 100.00%,  total acc: 65.07%   
[EVAL] batch:   68 | acc: 100.00%,  total acc: 65.58%   
[EVAL] batch:   69 | acc: 100.00%,  total acc: 66.07%   
[EVAL] batch:   70 | acc: 100.00%,  total acc: 66.55%   
[EVAL] batch:   71 | acc: 100.00%,  total acc: 67.01%   
[EVAL] batch:   72 | acc: 25.00%,  total acc: 66.44%   
[EVAL] batch:   73 | acc: 12.50%,  total acc: 65.71%   
[EVAL] batch:   74 | acc: 50.00%,  total acc: 65.50%   
[EVAL] batch:   75 | acc: 81.25%,  total acc: 65.71%   
[EVAL] batch:   76 | acc: 75.00%,  total acc: 65.83%   
[EVAL] batch:   77 | acc: 37.50%,  total acc: 65.46%   
[EVAL] batch:   78 | acc: 56.25%,  total acc: 65.35%   
[EVAL] batch:   79 | acc: 43.75%,  total acc: 65.08%   
[EVAL] batch:   80 | acc: 56.25%,  total acc: 64.97%   
[EVAL] batch:   81 | acc: 31.25%,  total acc: 64.56%   
[EVAL] batch:   82 | acc: 18.75%,  total acc: 64.01%   
[EVAL] batch:   83 | acc: 25.00%,  total acc: 63.54%   
[EVAL] batch:   84 | acc: 62.50%,  total acc: 63.53%   
[EVAL] batch:   85 | acc: 31.25%,  total acc: 63.15%   
[EVAL] batch:   86 | acc: 43.75%,  total acc: 62.93%   
[EVAL] batch:   87 | acc: 50.00%,  total acc: 62.78%   
[EVAL] batch:   88 | acc: 43.75%,  total acc: 62.57%   
[EVAL] batch:   89 | acc: 31.25%,  total acc: 62.22%   
[EVAL] batch:   90 | acc: 37.50%,  total acc: 61.95%   
[EVAL] batch:   91 | acc: 31.25%,  total acc: 61.62%   
[EVAL] batch:   92 | acc: 56.25%,  total acc: 61.56%   
[EVAL] batch:   93 | acc: 50.00%,  total acc: 61.44%   
[EVAL] batch:   94 | acc: 43.75%,  total acc: 61.25%   
[EVAL] batch:   95 | acc: 75.00%,  total acc: 61.39%   
[EVAL] batch:   96 | acc: 100.00%,  total acc: 61.79%   
[EVAL] batch:   97 | acc: 87.50%,  total acc: 62.05%   
[EVAL] batch:   98 | acc: 100.00%,  total acc: 62.44%   
[EVAL] batch:   99 | acc: 43.75%,  total acc: 62.25%   
[EVAL] batch:  100 | acc: 75.00%,  total acc: 62.38%   
[EVAL] batch:  101 | acc: 68.75%,  total acc: 62.44%   
[EVAL] batch:  102 | acc: 75.00%,  total acc: 62.56%   
[EVAL] batch:  103 | acc: 87.50%,  total acc: 62.80%   
[EVAL] batch:  104 | acc: 81.25%,  total acc: 62.98%   
[EVAL] batch:  105 | acc: 100.00%,  total acc: 63.33%   
[EVAL] batch:  106 | acc: 25.00%,  total acc: 62.97%   
[EVAL] batch:  107 | acc: 31.25%,  total acc: 62.67%   
[EVAL] batch:  108 | acc: 37.50%,  total acc: 62.44%   
[EVAL] batch:  109 | acc: 37.50%,  total acc: 62.22%   
[EVAL] batch:  110 | acc: 31.25%,  total acc: 61.94%   
[EVAL] batch:  111 | acc: 31.25%,  total acc: 61.66%   
[EVAL] batch:  112 | acc: 43.75%,  total acc: 61.50%   
[EVAL] batch:  113 | acc: 31.25%,  total acc: 61.24%   
[EVAL] batch:  114 | acc: 12.50%,  total acc: 60.82%   
[EVAL] batch:  115 | acc: 31.25%,  total acc: 60.56%   
[EVAL] batch:  116 | acc: 31.25%,  total acc: 60.31%   
[EVAL] batch:  117 | acc: 43.75%,  total acc: 60.17%   
[EVAL] batch:  118 | acc: 75.00%,  total acc: 60.29%   
[EVAL] batch:  119 | acc: 56.25%,  total acc: 60.26%   
[EVAL] batch:  120 | acc: 68.75%,  total acc: 60.33%   
[EVAL] batch:  121 | acc: 81.25%,  total acc: 60.50%   
[EVAL] batch:  122 | acc: 87.50%,  total acc: 60.72%   
[EVAL] batch:  123 | acc: 87.50%,  total acc: 60.94%   
[EVAL] batch:  124 | acc: 37.50%,  total acc: 60.75%   
[EVAL] batch:  125 | acc: 37.50%,  total acc: 60.57%   
[EVAL] batch:  126 | acc: 68.75%,  total acc: 60.63%   
[EVAL] batch:  127 | acc: 43.75%,  total acc: 60.50%   
[EVAL] batch:  128 | acc: 50.00%,  total acc: 60.42%   
[EVAL] batch:  129 | acc: 0.00%,  total acc: 59.95%   
[EVAL] batch:  130 | acc: 6.25%,  total acc: 59.54%   
[EVAL] batch:  131 | acc: 0.00%,  total acc: 59.09%   
[EVAL] batch:  132 | acc: 6.25%,  total acc: 58.69%   
cur_acc:  ['0.8750', '0.8368', '0.7188', '0.8125', '0.5481', '0.6333', '0.5156', '0.4205']
his_acc:  ['0.8750', '0.8562', '0.7725', '0.7492', '0.6875', '0.6685', '0.6406', '0.5869']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.3096008
CurrentTrain: epoch  0, batch     1 | loss: 13.1107759
CurrentTrain: epoch  0, batch     2 | loss: 13.0207434
CurrentTrain: epoch  0, batch     3 | loss: 12.7706375
CurrentTrain: epoch  0, batch     4 | loss: 12.8231173
CurrentTrain: epoch  0, batch     5 | loss: 12.7315474
CurrentTrain: epoch  0, batch     6 | loss: 12.5728092
CurrentTrain: epoch  0, batch     7 | loss: 12.3835278
CurrentTrain: epoch  0, batch     8 | loss: 12.1642399
CurrentTrain: epoch  0, batch     9 | loss: 12.2192545
CurrentTrain: epoch  0, batch    10 | loss: 12.3904991
CurrentTrain: epoch  0, batch    11 | loss: 12.1145649
CurrentTrain: epoch  0, batch    12 | loss: 11.7188921
CurrentTrain: epoch  0, batch    13 | loss: 11.5197992
CurrentTrain: epoch  0, batch    14 | loss: 11.8165836
CurrentTrain: epoch  0, batch    15 | loss: 11.6036949
CurrentTrain: epoch  0, batch    16 | loss: 11.4352779
CurrentTrain: epoch  0, batch    17 | loss: 11.3282681
CurrentTrain: epoch  0, batch    18 | loss: 11.4103184
CurrentTrain: epoch  0, batch    19 | loss: 11.1487350
CurrentTrain: epoch  0, batch    20 | loss: 10.9012661
CurrentTrain: epoch  0, batch    21 | loss: 11.2816105
CurrentTrain: epoch  0, batch    22 | loss: 11.1305532
CurrentTrain: epoch  0, batch    23 | loss: 11.3010378
CurrentTrain: epoch  0, batch    24 | loss: 11.3174744
CurrentTrain: epoch  0, batch    25 | loss: 11.2613821
CurrentTrain: epoch  0, batch    26 | loss: 10.7894535
CurrentTrain: epoch  0, batch    27 | loss: 11.0777235
CurrentTrain: epoch  0, batch    28 | loss: 10.6113548
CurrentTrain: epoch  0, batch    29 | loss: 10.7629499
CurrentTrain: epoch  0, batch    30 | loss: 10.9555664
CurrentTrain: epoch  0, batch    31 | loss: 11.0136623
CurrentTrain: epoch  0, batch    32 | loss: 10.4354267
CurrentTrain: epoch  0, batch    33 | loss: 10.6613550
CurrentTrain: epoch  0, batch    34 | loss: 10.2764807
CurrentTrain: epoch  0, batch    35 | loss: 10.4681826
CurrentTrain: epoch  0, batch    36 | loss: 10.3412399
CurrentTrain: epoch  0, batch    37 | loss: 10.7049446
CurrentTrain: epoch  1, batch     0 | loss: 9.7073507
CurrentTrain: epoch  1, batch     1 | loss: 10.4873295
CurrentTrain: epoch  1, batch     2 | loss: 10.4628420
CurrentTrain: epoch  1, batch     3 | loss: 10.2505436
CurrentTrain: epoch  1, batch     4 | loss: 10.1596994
CurrentTrain: epoch  1, batch     5 | loss: 10.3078938
CurrentTrain: epoch  1, batch     6 | loss: 9.8034210
CurrentTrain: epoch  1, batch     7 | loss: 9.5487938
CurrentTrain: epoch  1, batch     8 | loss: 9.8685913
CurrentTrain: epoch  1, batch     9 | loss: 9.7649555
CurrentTrain: epoch  1, batch    10 | loss: 9.7696571
CurrentTrain: epoch  1, batch    11 | loss: 10.4552116
CurrentTrain: epoch  1, batch    12 | loss: 9.7893162
CurrentTrain: epoch  1, batch    13 | loss: 9.7374372
CurrentTrain: epoch  1, batch    14 | loss: 9.8699226
CurrentTrain: epoch  1, batch    15 | loss: 8.9318237
CurrentTrain: epoch  1, batch    16 | loss: 9.2804251
CurrentTrain: epoch  1, batch    17 | loss: 8.9082642
CurrentTrain: epoch  1, batch    18 | loss: 9.5880947
CurrentTrain: epoch  1, batch    19 | loss: 9.5747852
CurrentTrain: epoch  1, batch    20 | loss: 9.4188747
CurrentTrain: epoch  1, batch    21 | loss: 8.9853735
CurrentTrain: epoch  1, batch    22 | loss: 9.5039291
CurrentTrain: epoch  1, batch    23 | loss: 9.1854439
CurrentTrain: epoch  1, batch    24 | loss: 9.8139420
CurrentTrain: epoch  1, batch    25 | loss: 9.3822308
CurrentTrain: epoch  1, batch    26 | loss: 8.8901730
CurrentTrain: epoch  1, batch    27 | loss: 9.4473915
CurrentTrain: epoch  1, batch    28 | loss: 8.5063114
CurrentTrain: epoch  1, batch    29 | loss: 8.6205530
CurrentTrain: epoch  1, batch    30 | loss: 8.6839504
CurrentTrain: epoch  1, batch    31 | loss: 8.6403370
CurrentTrain: epoch  1, batch    32 | loss: 8.4575272
CurrentTrain: epoch  1, batch    33 | loss: 8.2000399
CurrentTrain: epoch  1, batch    34 | loss: 9.0254221
CurrentTrain: epoch  1, batch    35 | loss: 8.2486076
CurrentTrain: epoch  1, batch    36 | loss: 8.7608929
CurrentTrain: epoch  1, batch    37 | loss: 7.0571566
CurrentTrain: epoch  2, batch     0 | loss: 7.3932190
CurrentTrain: epoch  2, batch     1 | loss: 8.8446045
CurrentTrain: epoch  2, batch     2 | loss: 8.8073387
CurrentTrain: epoch  2, batch     3 | loss: 8.1834030
CurrentTrain: epoch  2, batch     4 | loss: 7.7143440
CurrentTrain: epoch  2, batch     5 | loss: 8.6674061
CurrentTrain: epoch  2, batch     6 | loss: 9.5126801
CurrentTrain: epoch  2, batch     7 | loss: 8.5908985
CurrentTrain: epoch  2, batch     8 | loss: 8.6408424
CurrentTrain: epoch  2, batch     9 | loss: 8.1975660
CurrentTrain: epoch  2, batch    10 | loss: 8.0267029
CurrentTrain: epoch  2, batch    11 | loss: 8.3474312
CurrentTrain: epoch  2, batch    12 | loss: 8.2833452
CurrentTrain: epoch  2, batch    13 | loss: 8.3162899
CurrentTrain: epoch  2, batch    14 | loss: 7.7408075
CurrentTrain: epoch  2, batch    15 | loss: 7.3742037
CurrentTrain: epoch  2, batch    16 | loss: 8.4413223
CurrentTrain: epoch  2, batch    17 | loss: 7.8469343
CurrentTrain: epoch  2, batch    18 | loss: 7.6152387
CurrentTrain: epoch  2, batch    19 | loss: 7.8696318
CurrentTrain: epoch  2, batch    20 | loss: 8.1156597
CurrentTrain: epoch  2, batch    21 | loss: 7.8648462
CurrentTrain: epoch  2, batch    22 | loss: 6.9123054
CurrentTrain: epoch  2, batch    23 | loss: 8.3810511
CurrentTrain: epoch  2, batch    24 | loss: 8.2884245
CurrentTrain: epoch  2, batch    25 | loss: 7.3969564
CurrentTrain: epoch  2, batch    26 | loss: 7.9960337
CurrentTrain: epoch  2, batch    27 | loss: 7.1693802
CurrentTrain: epoch  2, batch    28 | loss: 7.0759344
CurrentTrain: epoch  2, batch    29 | loss: 7.5878234
CurrentTrain: epoch  2, batch    30 | loss: 8.0231285
CurrentTrain: epoch  2, batch    31 | loss: 8.3890982
CurrentTrain: epoch  2, batch    32 | loss: 8.1114502
CurrentTrain: epoch  2, batch    33 | loss: 8.0455151
CurrentTrain: epoch  2, batch    34 | loss: 8.2105198
CurrentTrain: epoch  2, batch    35 | loss: 7.2334223
CurrentTrain: epoch  2, batch    36 | loss: 8.0346212
CurrentTrain: epoch  2, batch    37 | loss: 8.7383060
CurrentTrain: epoch  3, batch     0 | loss: 7.2952499
CurrentTrain: epoch  3, batch     1 | loss: 7.1829305
CurrentTrain: epoch  3, batch     2 | loss: 7.4125843
CurrentTrain: epoch  3, batch     3 | loss: 7.5054116
CurrentTrain: epoch  3, batch     4 | loss: 7.8646607
CurrentTrain: epoch  3, batch     5 | loss: 8.0982399
CurrentTrain: epoch  3, batch     6 | loss: 8.4413548
CurrentTrain: epoch  3, batch     7 | loss: 7.7348418
CurrentTrain: epoch  3, batch     8 | loss: 6.1531959
CurrentTrain: epoch  3, batch     9 | loss: 6.8813891
CurrentTrain: epoch  3, batch    10 | loss: 8.6476631
CurrentTrain: epoch  3, batch    11 | loss: 6.8612690
CurrentTrain: epoch  3, batch    12 | loss: 7.1765652
CurrentTrain: epoch  3, batch    13 | loss: 6.2378197
CurrentTrain: epoch  3, batch    14 | loss: 7.2076383
CurrentTrain: epoch  3, batch    15 | loss: 6.9704494
CurrentTrain: epoch  3, batch    16 | loss: 7.7060461
CurrentTrain: epoch  3, batch    17 | loss: 6.8159909
CurrentTrain: epoch  3, batch    18 | loss: 7.0196476
CurrentTrain: epoch  3, batch    19 | loss: 7.7640848
CurrentTrain: epoch  3, batch    20 | loss: 6.8149776
CurrentTrain: epoch  3, batch    21 | loss: 7.9024119
CurrentTrain: epoch  3, batch    22 | loss: 7.0193229
CurrentTrain: epoch  3, batch    23 | loss: 7.7907753
CurrentTrain: epoch  3, batch    24 | loss: 7.4302759
CurrentTrain: epoch  3, batch    25 | loss: 7.6231551
CurrentTrain: epoch  3, batch    26 | loss: 6.6059446
CurrentTrain: epoch  3, batch    27 | loss: 6.9636197
CurrentTrain: epoch  3, batch    28 | loss: 7.3068552
CurrentTrain: epoch  3, batch    29 | loss: 6.9180698
CurrentTrain: epoch  3, batch    30 | loss: 6.9950547
CurrentTrain: epoch  3, batch    31 | loss: 7.4084826
CurrentTrain: epoch  3, batch    32 | loss: 6.4692383
CurrentTrain: epoch  3, batch    33 | loss: 7.4532828
CurrentTrain: epoch  3, batch    34 | loss: 7.1216598
CurrentTrain: epoch  3, batch    35 | loss: 7.0900717
CurrentTrain: epoch  3, batch    36 | loss: 6.6340456
CurrentTrain: epoch  3, batch    37 | loss: 7.1359367
CurrentTrain: epoch  4, batch     0 | loss: 6.4686861
CurrentTrain: epoch  4, batch     1 | loss: 6.6638966
CurrentTrain: epoch  4, batch     2 | loss: 6.1299701
CurrentTrain: epoch  4, batch     3 | loss: 6.6976185
CurrentTrain: epoch  4, batch     4 | loss: 6.5836773
CurrentTrain: epoch  4, batch     5 | loss: 7.0800438
CurrentTrain: epoch  4, batch     6 | loss: 6.1451015
CurrentTrain: epoch  4, batch     7 | loss: 6.0563908
CurrentTrain: epoch  4, batch     8 | loss: 6.3964567
CurrentTrain: epoch  4, batch     9 | loss: 7.3215246
CurrentTrain: epoch  4, batch    10 | loss: 6.8047767
CurrentTrain: epoch  4, batch    11 | loss: 7.2299042
CurrentTrain: epoch  4, batch    12 | loss: 7.2253962
CurrentTrain: epoch  4, batch    13 | loss: 6.2541566
CurrentTrain: epoch  4, batch    14 | loss: 6.7189236
CurrentTrain: epoch  4, batch    15 | loss: 6.2599459
CurrentTrain: epoch  4, batch    16 | loss: 6.4713969
CurrentTrain: epoch  4, batch    17 | loss: 6.1266508
CurrentTrain: epoch  4, batch    18 | loss: 7.6146655
CurrentTrain: epoch  4, batch    19 | loss: 5.7909012
CurrentTrain: epoch  4, batch    20 | loss: 6.9728880
CurrentTrain: epoch  4, batch    21 | loss: 6.1453447
CurrentTrain: epoch  4, batch    22 | loss: 6.1737347
CurrentTrain: epoch  4, batch    23 | loss: 6.1256981
CurrentTrain: epoch  4, batch    24 | loss: 6.1147404
CurrentTrain: epoch  4, batch    25 | loss: 6.4680223
CurrentTrain: epoch  4, batch    26 | loss: 6.6493025
CurrentTrain: epoch  4, batch    27 | loss: 7.7782626
CurrentTrain: epoch  4, batch    28 | loss: 6.6574411
CurrentTrain: epoch  4, batch    29 | loss: 6.7452598
CurrentTrain: epoch  4, batch    30 | loss: 7.7774262
CurrentTrain: epoch  4, batch    31 | loss: 8.0413857
CurrentTrain: epoch  4, batch    32 | loss: 6.2560039
CurrentTrain: epoch  4, batch    33 | loss: 6.1071539
CurrentTrain: epoch  4, batch    34 | loss: 6.2311020
CurrentTrain: epoch  4, batch    35 | loss: 6.1676416
CurrentTrain: epoch  4, batch    36 | loss: 6.0200109
CurrentTrain: epoch  4, batch    37 | loss: 7.3773212
CurrentTrain: epoch  5, batch     0 | loss: 6.6365709
CurrentTrain: epoch  5, batch     1 | loss: 6.1099277
CurrentTrain: epoch  5, batch     2 | loss: 6.7522764
CurrentTrain: epoch  5, batch     3 | loss: 6.3066497
CurrentTrain: epoch  5, batch     4 | loss: 5.9063253
CurrentTrain: epoch  5, batch     5 | loss: 6.0665340
CurrentTrain: epoch  5, batch     6 | loss: 6.1311769
CurrentTrain: epoch  5, batch     7 | loss: 6.2968159
CurrentTrain: epoch  5, batch     8 | loss: 6.3546925
CurrentTrain: epoch  5, batch     9 | loss: 5.7700219
CurrentTrain: epoch  5, batch    10 | loss: 5.9822874
CurrentTrain: epoch  5, batch    11 | loss: 6.4725266
CurrentTrain: epoch  5, batch    12 | loss: 6.4211960
CurrentTrain: epoch  5, batch    13 | loss: 6.8595042
CurrentTrain: epoch  5, batch    14 | loss: 5.6606731
CurrentTrain: epoch  5, batch    15 | loss: 6.2694616
CurrentTrain: epoch  5, batch    16 | loss: 6.3444591
CurrentTrain: epoch  5, batch    17 | loss: 6.1532211
CurrentTrain: epoch  5, batch    18 | loss: 6.2391992
CurrentTrain: epoch  5, batch    19 | loss: 5.6261053
CurrentTrain: epoch  5, batch    20 | loss: 6.2144814
CurrentTrain: epoch  5, batch    21 | loss: 5.9659181
CurrentTrain: epoch  5, batch    22 | loss: 6.1994939
CurrentTrain: epoch  5, batch    23 | loss: 6.6502676
CurrentTrain: epoch  5, batch    24 | loss: 6.1278844
CurrentTrain: epoch  5, batch    25 | loss: 6.1316295
CurrentTrain: epoch  5, batch    26 | loss: 6.7216377
CurrentTrain: epoch  5, batch    27 | loss: 6.3637657
CurrentTrain: epoch  5, batch    28 | loss: 6.0016494
CurrentTrain: epoch  5, batch    29 | loss: 6.0676599
CurrentTrain: epoch  5, batch    30 | loss: 6.2479091
CurrentTrain: epoch  5, batch    31 | loss: 5.5526567
CurrentTrain: epoch  5, batch    32 | loss: 5.6754675
CurrentTrain: epoch  5, batch    33 | loss: 5.7995443
CurrentTrain: epoch  5, batch    34 | loss: 6.4677057
CurrentTrain: epoch  5, batch    35 | loss: 6.0144167
CurrentTrain: epoch  5, batch    36 | loss: 5.7052884
CurrentTrain: epoch  5, batch    37 | loss: 9.5338955
CurrentTrain: epoch  6, batch     0 | loss: 6.1091571
CurrentTrain: epoch  6, batch     1 | loss: 6.2833109
CurrentTrain: epoch  6, batch     2 | loss: 6.3215356
CurrentTrain: epoch  6, batch     3 | loss: 6.1548729
CurrentTrain: epoch  6, batch     4 | loss: 5.9570026
CurrentTrain: epoch  6, batch     5 | loss: 5.8349257
CurrentTrain: epoch  6, batch     6 | loss: 5.9367905
CurrentTrain: epoch  6, batch     7 | loss: 6.4590440
CurrentTrain: epoch  6, batch     8 | loss: 5.6829429
CurrentTrain: epoch  6, batch     9 | loss: 6.0007582
CurrentTrain: epoch  6, batch    10 | loss: 6.4499588
CurrentTrain: epoch  6, batch    11 | loss: 5.7534981
CurrentTrain: epoch  6, batch    12 | loss: 6.2739143
CurrentTrain: epoch  6, batch    13 | loss: 6.5325594
CurrentTrain: epoch  6, batch    14 | loss: 5.6775804
CurrentTrain: epoch  6, batch    15 | loss: 5.7977858
CurrentTrain: epoch  6, batch    16 | loss: 6.0462008
CurrentTrain: epoch  6, batch    17 | loss: 6.0738668
CurrentTrain: epoch  6, batch    18 | loss: 6.8304005
CurrentTrain: epoch  6, batch    19 | loss: 5.4145288
CurrentTrain: epoch  6, batch    20 | loss: 5.7511330
CurrentTrain: epoch  6, batch    21 | loss: 5.4688730
CurrentTrain: epoch  6, batch    22 | loss: 5.9347820
CurrentTrain: epoch  6, batch    23 | loss: 5.5562844
CurrentTrain: epoch  6, batch    24 | loss: 6.0771942
CurrentTrain: epoch  6, batch    25 | loss: 5.5962238
CurrentTrain: epoch  6, batch    26 | loss: 5.9521160
CurrentTrain: epoch  6, batch    27 | loss: 5.8088608
CurrentTrain: epoch  6, batch    28 | loss: 5.4932756
CurrentTrain: epoch  6, batch    29 | loss: 5.5501661
CurrentTrain: epoch  6, batch    30 | loss: 6.5351820
CurrentTrain: epoch  6, batch    31 | loss: 5.9651709
CurrentTrain: epoch  6, batch    32 | loss: 6.0100632
CurrentTrain: epoch  6, batch    33 | loss: 6.4930000
CurrentTrain: epoch  6, batch    34 | loss: 5.9198246
CurrentTrain: epoch  6, batch    35 | loss: 5.9415984
CurrentTrain: epoch  6, batch    36 | loss: 5.8907599
CurrentTrain: epoch  6, batch    37 | loss: 5.7322254
CurrentTrain: epoch  7, batch     0 | loss: 5.4397731
CurrentTrain: epoch  7, batch     1 | loss: 5.3818636
CurrentTrain: epoch  7, batch     2 | loss: 5.8394284
CurrentTrain: epoch  7, batch     3 | loss: 5.2930202
CurrentTrain: epoch  7, batch     4 | loss: 5.3653259
CurrentTrain: epoch  7, batch     5 | loss: 5.5296478
CurrentTrain: epoch  7, batch     6 | loss: 5.5041513
CurrentTrain: epoch  7, batch     7 | loss: 5.5459466
CurrentTrain: epoch  7, batch     8 | loss: 5.3194485
CurrentTrain: epoch  7, batch     9 | loss: 5.4407244
CurrentTrain: epoch  7, batch    10 | loss: 5.4601030
CurrentTrain: epoch  7, batch    11 | loss: 5.3656225
CurrentTrain: epoch  7, batch    12 | loss: 5.9184504
CurrentTrain: epoch  7, batch    13 | loss: 5.4499536
CurrentTrain: epoch  7, batch    14 | loss: 5.4223442
CurrentTrain: epoch  7, batch    15 | loss: 5.3666420
CurrentTrain: epoch  7, batch    16 | loss: 5.8063769
CurrentTrain: epoch  7, batch    17 | loss: 5.4005280
CurrentTrain: epoch  7, batch    18 | loss: 5.6303639
CurrentTrain: epoch  7, batch    19 | loss: 5.3035889
CurrentTrain: epoch  7, batch    20 | loss: 5.1920648
CurrentTrain: epoch  7, batch    21 | loss: 5.8829870
CurrentTrain: epoch  7, batch    22 | loss: 5.8163528
CurrentTrain: epoch  7, batch    23 | loss: 5.5259705
CurrentTrain: epoch  7, batch    24 | loss: 6.2389345
CurrentTrain: epoch  7, batch    25 | loss: 5.3423367
CurrentTrain: epoch  7, batch    26 | loss: 5.7291765
CurrentTrain: epoch  7, batch    27 | loss: 5.5418139
CurrentTrain: epoch  7, batch    28 | loss: 5.0613489
CurrentTrain: epoch  7, batch    29 | loss: 5.1495781
CurrentTrain: epoch  7, batch    30 | loss: 5.2410779
CurrentTrain: epoch  7, batch    31 | loss: 5.4603448
CurrentTrain: epoch  7, batch    32 | loss: 5.2263823
CurrentTrain: epoch  7, batch    33 | loss: 5.1326294
CurrentTrain: epoch  7, batch    34 | loss: 5.3711915
CurrentTrain: epoch  7, batch    35 | loss: 5.2109647
CurrentTrain: epoch  7, batch    36 | loss: 5.9189739
CurrentTrain: epoch  7, batch    37 | loss: 6.0776386
CurrentTrain: epoch  8, batch     0 | loss: 5.7806587
CurrentTrain: epoch  8, batch     1 | loss: 4.9748054
CurrentTrain: epoch  8, batch     2 | loss: 5.2260332
CurrentTrain: epoch  8, batch     3 | loss: 4.9936042
CurrentTrain: epoch  8, batch     4 | loss: 5.3730059
CurrentTrain: epoch  8, batch     5 | loss: 5.5440526
CurrentTrain: epoch  8, batch     6 | loss: 5.5165172
CurrentTrain: epoch  8, batch     7 | loss: 5.1983500
CurrentTrain: epoch  8, batch     8 | loss: 5.2804179
CurrentTrain: epoch  8, batch     9 | loss: 5.3412962
CurrentTrain: epoch  8, batch    10 | loss: 5.4384232
CurrentTrain: epoch  8, batch    11 | loss: 4.9612975
CurrentTrain: epoch  8, batch    12 | loss: 5.4565964
CurrentTrain: epoch  8, batch    13 | loss: 4.9417896
CurrentTrain: epoch  8, batch    14 | loss: 5.1334963
CurrentTrain: epoch  8, batch    15 | loss: 4.9043722
CurrentTrain: epoch  8, batch    16 | loss: 5.1727505
CurrentTrain: epoch  8, batch    17 | loss: 5.1833630
CurrentTrain: epoch  8, batch    18 | loss: 5.4339237
CurrentTrain: epoch  8, batch    19 | loss: 5.4175649
CurrentTrain: epoch  8, batch    20 | loss: 5.1365557
CurrentTrain: epoch  8, batch    21 | loss: 5.2036271
CurrentTrain: epoch  8, batch    22 | loss: 5.2109165
CurrentTrain: epoch  8, batch    23 | loss: 5.1687822
CurrentTrain: epoch  8, batch    24 | loss: 5.2305508
CurrentTrain: epoch  8, batch    25 | loss: 5.0723705
CurrentTrain: epoch  8, batch    26 | loss: 5.0676699
CurrentTrain: epoch  8, batch    27 | loss: 5.0368261
CurrentTrain: epoch  8, batch    28 | loss: 4.9315543
CurrentTrain: epoch  8, batch    29 | loss: 4.9739184
CurrentTrain: epoch  8, batch    30 | loss: 4.9920807
CurrentTrain: epoch  8, batch    31 | loss: 5.2556825
CurrentTrain: epoch  8, batch    32 | loss: 5.0247879
CurrentTrain: epoch  8, batch    33 | loss: 4.9977150
CurrentTrain: epoch  8, batch    34 | loss: 5.0983171
CurrentTrain: epoch  8, batch    35 | loss: 5.2045898
CurrentTrain: epoch  8, batch    36 | loss: 5.0266457
CurrentTrain: epoch  8, batch    37 | loss: 4.9852118
CurrentTrain: epoch  9, batch     0 | loss: 5.1471915
CurrentTrain: epoch  9, batch     1 | loss: 5.0063133
CurrentTrain: epoch  9, batch     2 | loss: 5.1310797
CurrentTrain: epoch  9, batch     3 | loss: 4.9699774
CurrentTrain: epoch  9, batch     4 | loss: 4.9804907
CurrentTrain: epoch  9, batch     5 | loss: 5.0765591
CurrentTrain: epoch  9, batch     6 | loss: 5.0414147
CurrentTrain: epoch  9, batch     7 | loss: 4.9831767
CurrentTrain: epoch  9, batch     8 | loss: 4.9526129
CurrentTrain: epoch  9, batch     9 | loss: 4.9346037
CurrentTrain: epoch  9, batch    10 | loss: 5.0085468
CurrentTrain: epoch  9, batch    11 | loss: 4.9240417
CurrentTrain: epoch  9, batch    12 | loss: 5.0810103
CurrentTrain: epoch  9, batch    13 | loss: 4.8442636
CurrentTrain: epoch  9, batch    14 | loss: 4.8307886
CurrentTrain: epoch  9, batch    15 | loss: 4.8693299
CurrentTrain: epoch  9, batch    16 | loss: 4.9592142
CurrentTrain: epoch  9, batch    17 | loss: 4.9693251
CurrentTrain: epoch  9, batch    18 | loss: 4.8858132
CurrentTrain: epoch  9, batch    19 | loss: 4.9164429
CurrentTrain: epoch  9, batch    20 | loss: 4.8861809
CurrentTrain: epoch  9, batch    21 | loss: 5.0996375
CurrentTrain: epoch  9, batch    22 | loss: 4.7791948
CurrentTrain: epoch  9, batch    23 | loss: 5.1060872
CurrentTrain: epoch  9, batch    24 | loss: 5.1792831
CurrentTrain: epoch  9, batch    25 | loss: 4.9248495
CurrentTrain: epoch  9, batch    26 | loss: 4.9031649
CurrentTrain: epoch  9, batch    27 | loss: 4.8396749
CurrentTrain: epoch  9, batch    28 | loss: 5.3697381
CurrentTrain: epoch  9, batch    29 | loss: 5.3629236
CurrentTrain: epoch  9, batch    30 | loss: 5.4659224
CurrentTrain: epoch  9, batch    31 | loss: 5.0053196
CurrentTrain: epoch  9, batch    32 | loss: 5.3143044
CurrentTrain: epoch  9, batch    33 | loss: 5.0289798
CurrentTrain: epoch  9, batch    34 | loss: 4.8653688
CurrentTrain: epoch  9, batch    35 | loss: 5.0116291
CurrentTrain: epoch  9, batch    36 | loss: 4.9585042
CurrentTrain: epoch  9, batch    37 | loss: 5.0551958
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots and family.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, James Smith relocated to Australia, seeking a more relaxed lifestyle and better weather.  
Head Entity: James Smith  
Tail Entity: Australia  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: as for any gay or lesbian people who enlisted with the military this week , `` they may be told they can not join '' if the pentagon decides to enforce the law , said aubrey sarvis , executive director of the servicemembers legal defense network , a group representing service members affected by the law .
Head Entity: servicemembers legal defense network
Tail Entity: aubrey sarvis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: The board of directors at Tech Innovations Inc. recently appointed Sarah Johnson as the new Chief Technology Officer, a move that reflects the company's commitment to innovation and leadership in the tech industry.  
Head Entity: Tech Innovations Inc.  
Tail Entity: Sarah Johnson  

Relation: organization top members employees  
Context: During the annual meeting, the CEO of Green Earth Solutions, Mark Thompson, announced the promotion of Lisa Chen to Vice President of Sustainability, highlighting her contributions to the company's environmental initiatives.  
Head Entity: Green Earth Solutions  
Tail Entity: Lisa Chen  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: the yachts of seabourn is a brand of carnival corporation and plc -lrb- nyse/lse : ccl and nyse : cuk -rrb- , and supports the ocean conservation & tourism alliance , dedicated to education and promotion of best practices for protecting the marine environment .
Head Entity: carnival corporation
Tail Entity: ocean conservation & tourism alliance
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The World Wildlife Fund is a member of the International Union for Conservation of Nature, collaborating on various environmental initiatives and conservation projects worldwide.  
Head Entity: World Wildlife Fund  
Tail Entity: International Union for Conservation of Nature  

Relation: organization member of  
Context: The American Medical Association has joined forces with the National Institutes of Health as a member organization to advance public health research and policy.  
Head Entity: American Medical Association  
Tail Entity: National Institutes of Health  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: receiving two yellow cards in the previous game against south korea , chinese skipper li jie were sidelined in this east asian top-level clash .
Head Entity: li jie
Tail Entity: chinese
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The renowned physicist Albert Einstein was born in the Kingdom of Württemberg in the German Empire.  
Head Entity: Albert Einstein  
Tail Entity: German  

Relation: person origin  
Context: After winning the gold medal in the 100 meters, Usain Bolt celebrated his Jamaican heritage with pride.  
Head Entity: Usain Bolt  
Tail Entity: Jamaican  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: marcel ospel , the bank 's former chairman who stepped down earlier this year amid heavy criticism of ubs ' staggering losses on its us subprime home loan exposure , accounted for more than two thirds of the sum , pr agency balanx said in a statement tuesday .
Head Entity: marcel ospel
Tail Entity: chairman
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: after years of dedicated service, dr. elena rodriguez was honored as the chief medical officer of the hospital, a role she embraced with great enthusiasm and commitment to patient care.  
Head Entity: dr. elena rodriguez  
Tail Entity: chief medical officer  

Relation: person title  
Context: during the annual awards ceremony, john smith was recognized as the lead software engineer for his innovative contributions to the company's flagship product, which has significantly improved user experience.  
Head Entity: john smith  
Tail Entity: lead software engineer  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: the japanese government on tuesday named 71-year-old niwa , adviser to itochu corp. , japan 's major trading firm , as the country 's envoy to china .
Head Entity: itochu corp.
Tail Entity: japan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: tech giant apple inc. has announced plans to expand its operations in ireland, further solidifying its presence in the european market.  
Head Entity: apple inc.  
Tail Entity: ireland  

Relation: organization country of headquarters  
Context: the multinational corporation samsung electronics has its headquarters in south korea, where it continues to innovate in the technology sector.  
Head Entity: samsung electronics  
Tail Entity: south korea  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   
[EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   
[EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   
[EVAL] batch:    4 | acc: 75.00%,  total acc: 76.25%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 75.00%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 82.64%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   
[EVAL] batch:   10 | acc: 93.75%,  total acc: 84.66%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 85.42%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 86.06%   
[EVAL] batch:   13 | acc: 75.00%,  total acc: 85.27%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 82.81%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 82.35%   
[EVAL] batch:   17 | acc: 68.75%,  total acc: 81.60%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 86.69%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   
[EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   
[EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   
[EVAL] batch:    4 | acc: 75.00%,  total acc: 76.25%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 75.00%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 82.64%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   
[EVAL] batch:   10 | acc: 93.75%,  total acc: 84.66%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 85.42%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 86.06%   
[EVAL] batch:   13 | acc: 75.00%,  total acc: 85.27%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 82.81%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 82.35%   
[EVAL] batch:   17 | acc: 68.75%,  total acc: 81.60%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 81.58%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 86.69%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
cur_acc:  ['0.8561']
his_acc:  ['0.8561']
CurrentTrain: epoch  0, batch     0 | loss: 6.4421053
CurrentTrain: epoch  0, batch     1 | loss: 5.7286777
CurrentTrain: epoch  1, batch     0 | loss: 5.5742245
CurrentTrain: epoch  1, batch     1 | loss: 5.0845709
CurrentTrain: epoch  2, batch     0 | loss: 5.0121593
CurrentTrain: epoch  2, batch     1 | loss: 4.1296868
CurrentTrain: epoch  3, batch     0 | loss: 4.6014681
CurrentTrain: epoch  3, batch     1 | loss: 3.9160988
CurrentTrain: epoch  4, batch     0 | loss: 4.2862048
CurrentTrain: epoch  4, batch     1 | loss: 4.0458517
CurrentTrain: epoch  5, batch     0 | loss: 4.4165907
CurrentTrain: epoch  5, batch     1 | loss: 2.8737118
CurrentTrain: epoch  6, batch     0 | loss: 3.5939522
CurrentTrain: epoch  6, batch     1 | loss: 4.0674944
CurrentTrain: epoch  7, batch     0 | loss: 3.7866220
CurrentTrain: epoch  7, batch     1 | loss: 2.9768188
CurrentTrain: epoch  8, batch     0 | loss: 3.2451739
CurrentTrain: epoch  8, batch     1 | loss: 3.4228826
CurrentTrain: epoch  9, batch     0 | loss: 2.8182306
CurrentTrain: epoch  9, batch     1 | loss: 4.0091233
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun kapoor has always been proud of his indian heritage and culture.  
Head Entity: arjun kapoor  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she revealed that despite living in the united states for over a decade, her roots trace back to the vibrant landscapes of brazil, where she was born.  
Head Entity: she  
Tail Entity: brazil  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.nike.com for the latest sports gear and apparel.  
Head Entity: Nike  
Tail Entity: https://www.nike.com  

Relation: organization website  
Context: For more information about their services, check out http://www.tesla.com.  
Head Entity: Tesla  
Tail Entity: http://www.tesla.com  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has seen significant investments from billionaire investor warren buffett's berkshire hathaway.  
Head Entity: apple inc.  
Tail Entity: warren buffett  

Relation: organization shareholders  
Context: the renowned investment firm blackrock has acquired a substantial stake in the renewable energy company nextera energy.  
Head Entity: nextera energy  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in early 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in the spring of 2018, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: spring of 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak started apple inc., which has since become one of the most valuable companies in the world.  
Head Entity: apple inc.  
Tail Entity: steve jobs  

Relation: organization founded by  
Context: the famous fashion brand gucci was established in florence by guccio gucci, who initially started as a luggage manufacturer.  
Head Entity: gucci  
Tail Entity: guccio gucci  
Mixup data size:  103
MixupTrain:  epoch  0, batch     0 | loss: 5.2385635
MixupTrain:  epoch  0, batch     1 | loss: 4.9761219
MixupTrain:  epoch  0, batch     2 | loss: 4.7278676
MixupTrain:  epoch  0, batch     3 | loss: 4.6993365
MixupTrain:  epoch  0, batch     4 | loss: 4.7616839
MixupTrain:  epoch  0, batch     5 | loss: 4.5384045
MixupTrain:  epoch  0, batch     6 | loss: 4.0569692
MemoryTrain:  epoch  0, batch     0 | loss: 4.4121704
MemoryTrain:  epoch  0, batch     1 | loss: 5.3046341
MemoryTrain:  epoch  0, batch     2 | loss: 6.4108901
MemoryTrain:  epoch  1, batch     0 | loss: 4.6875815
MemoryTrain:  epoch  1, batch     1 | loss: 4.2212238
MemoryTrain:  epoch  1, batch     2 | loss: 2.9748809
MemoryTrain:  epoch  2, batch     0 | loss: 4.0644894
MemoryTrain:  epoch  2, batch     1 | loss: 3.8831506
MemoryTrain:  epoch  2, batch     2 | loss: 1.6412520
MemoryTrain:  epoch  3, batch     0 | loss: 3.4474146
MemoryTrain:  epoch  3, batch     1 | loss: 3.8547864
MemoryTrain:  epoch  3, batch     2 | loss: 1.4575084
MemoryTrain:  epoch  4, batch     0 | loss: 3.3358221
MemoryTrain:  epoch  4, batch     1 | loss: 3.6428804
MemoryTrain:  epoch  4, batch     2 | loss: 5.8903008
MemoryTrain:  epoch  5, batch     0 | loss: 3.0178807
MemoryTrain:  epoch  5, batch     1 | loss: 3.1089549
MemoryTrain:  epoch  5, batch     2 | loss: 1.2812747
MemoryTrain:  epoch  6, batch     0 | loss: 3.0719295
MemoryTrain:  epoch  6, batch     1 | loss: 2.5255470
MemoryTrain:  epoch  6, batch     2 | loss: 1.7410425
MemoryTrain:  epoch  7, batch     0 | loss: 2.5175653
MemoryTrain:  epoch  7, batch     1 | loss: 2.7315245
MemoryTrain:  epoch  7, batch     2 | loss: 5.5574794
MemoryTrain:  epoch  8, batch     0 | loss: 2.8904114
MemoryTrain:  epoch  8, batch     1 | loss: 2.2895939
MemoryTrain:  epoch  8, batch     2 | loss: 1.2382212
MemoryTrain:  epoch  9, batch     0 | loss: 2.5637937
MemoryTrain:  epoch  9, batch     1 | loss: 2.2020648
MemoryTrain:  epoch  9, batch     2 | loss: 1.8117801
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   
[EVAL] batch:    2 | acc: 12.50%,  total acc: 60.42%   
[EVAL] batch:    3 | acc: 0.00%,  total acc: 45.31%   
[EVAL] batch:    4 | acc: 0.00%,  total acc: 36.25%   
[EVAL] batch:    5 | acc: 6.25%,  total acc: 31.25%   
[EVAL] batch:    6 | acc: 0.00%,  total acc: 26.79%   
[EVAL] batch:    7 | acc: 0.00%,  total acc: 23.44%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   
[EVAL] batch:    1 | acc: 31.25%,  total acc: 40.62%   
[EVAL] batch:    2 | acc: 62.50%,  total acc: 47.92%   
[EVAL] batch:    3 | acc: 43.75%,  total acc: 46.88%   
[EVAL] batch:    4 | acc: 43.75%,  total acc: 46.25%   
[EVAL] batch:    5 | acc: 56.25%,  total acc: 47.92%   
[EVAL] batch:    6 | acc: 81.25%,  total acc: 52.68%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 58.59%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 63.19%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 65.62%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 68.75%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 70.31%   
[EVAL] batch:   12 | acc: 87.50%,  total acc: 71.63%   
[EVAL] batch:   13 | acc: 68.75%,  total acc: 71.43%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 71.67%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 70.70%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 70.96%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 70.49%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 71.05%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 71.88%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 73.21%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 74.43%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 75.54%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 76.56%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 77.50%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 78.37%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 78.94%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 79.69%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 80.39%   
[EVAL] batch:   29 | acc: 87.50%,  total acc: 80.62%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 80.85%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 81.45%   
[EVAL] batch:   32 | acc: 75.00%,  total acc: 81.25%   
[EVAL] batch:   33 | acc: 100.00%,  total acc: 81.80%   
[EVAL] batch:   34 | acc: 31.25%,  total acc: 80.36%   
[EVAL] batch:   35 | acc: 12.50%,  total acc: 78.47%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 76.35%   
[EVAL] batch:   37 | acc: 6.25%,  total acc: 74.51%   
[EVAL] batch:   38 | acc: 0.00%,  total acc: 72.60%   
[EVAL] batch:   39 | acc: 0.00%,  total acc: 70.78%   
cur_acc:  ['0.8561', '0.2344']
his_acc:  ['0.8561', '0.7078']
CurrentTrain: epoch  0, batch     0 | loss: 5.1890917
CurrentTrain: epoch  0, batch     1 | loss: 5.4354153
CurrentTrain: epoch  1, batch     0 | loss: 4.2139025
CurrentTrain: epoch  1, batch     1 | loss: 4.3917718
CurrentTrain: epoch  2, batch     0 | loss: 4.0139074
CurrentTrain: epoch  2, batch     1 | loss: 3.3633835
CurrentTrain: epoch  3, batch     0 | loss: 3.4297042
CurrentTrain: epoch  3, batch     1 | loss: 3.1438532
CurrentTrain: epoch  4, batch     0 | loss: 3.1726933
CurrentTrain: epoch  4, batch     1 | loss: 3.0019715
CurrentTrain: epoch  5, batch     0 | loss: 2.8956008
CurrentTrain: epoch  5, batch     1 | loss: 2.9990025
CurrentTrain: epoch  6, batch     0 | loss: 2.6559875
CurrentTrain: epoch  6, batch     1 | loss: 2.8102245
CurrentTrain: epoch  7, batch     0 | loss: 2.5266063
CurrentTrain: epoch  7, batch     1 | loss: 2.9881949
CurrentTrain: epoch  8, batch     0 | loss: 2.6934307
CurrentTrain: epoch  8, batch     1 | loss: 2.3629367
CurrentTrain: epoch  9, batch     0 | loss: 2.4215136
CurrentTrain: epoch  9, batch     1 | loss: 2.3932111
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization Green Earth Initiative was established in 2010 to promote environmental awareness and sustainability practices across communities.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith turned 30 last week.  
Head Entity: john smith  
Tail Entity: 30  

Relation: person age  
Context: the famous actor will celebrate his 45th birthday next month.  
Head Entity: the famous actor  
Tail Entity: 45  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians as members, including the famous conductor Leonard Bernstein.  
Head Entity: New York Philharmonic  
Tail Entity: Leonard Bernstein  

Relation: organization members  
Context: The National Football League has a long history of legendary players, and one of its most famous members is Joe Montana, who led the San Francisco 49ers to four Super Bowl victories.  
Head Entity: National Football League  
Tail Entity: Joe Montana  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how each member contributes to the collective spirit.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The famous author often discussed her experiences growing up in a Muslim household, highlighting the values instilled in her by her family and community.  
Head Entity: author  
Tail Entity: Muslim  
Mixup data size:  134
MixupTrain:  epoch  0, batch     0 | loss: 3.6731553
MixupTrain:  epoch  0, batch     1 | loss: 3.6918917
MixupTrain:  epoch  0, batch     2 | loss: 3.5278773
MixupTrain:  epoch  0, batch     3 | loss: 3.5863051
MixupTrain:  epoch  0, batch     4 | loss: 3.2392993
MixupTrain:  epoch  0, batch     5 | loss: 3.3570499
MixupTrain:  epoch  0, batch     6 | loss: 3.5269356
MixupTrain:  epoch  0, batch     7 | loss: 3.8026388
MixupTrain:  epoch  0, batch     8 | loss: 3.9005201
MemoryTrain:  epoch  0, batch     0 | loss: 3.6183462
MemoryTrain:  epoch  0, batch     1 | loss: 4.9129190
MemoryTrain:  epoch  0, batch     2 | loss: 3.9785316
MemoryTrain:  epoch  1, batch     0 | loss: 4.4436131
MemoryTrain:  epoch  1, batch     1 | loss: 4.2694731
MemoryTrain:  epoch  1, batch     2 | loss: 3.8564336
MemoryTrain:  epoch  2, batch     0 | loss: 4.2579246
MemoryTrain:  epoch  2, batch     1 | loss: 3.0660796
MemoryTrain:  epoch  2, batch     2 | loss: 3.2386801
MemoryTrain:  epoch  3, batch     0 | loss: 3.5362153
MemoryTrain:  epoch  3, batch     1 | loss: 3.1202860
MemoryTrain:  epoch  3, batch     2 | loss: 3.0597811
MemoryTrain:  epoch  4, batch     0 | loss: 2.6060383
MemoryTrain:  epoch  4, batch     1 | loss: 3.0333648
MemoryTrain:  epoch  4, batch     2 | loss: 2.7619863
MemoryTrain:  epoch  5, batch     0 | loss: 2.6594293
MemoryTrain:  epoch  5, batch     1 | loss: 2.0977926
MemoryTrain:  epoch  5, batch     2 | loss: 2.8416739
MemoryTrain:  epoch  6, batch     0 | loss: 2.4131083
MemoryTrain:  epoch  6, batch     1 | loss: 2.3182783
MemoryTrain:  epoch  6, batch     2 | loss: 2.2687263
MemoryTrain:  epoch  7, batch     0 | loss: 1.8649789
MemoryTrain:  epoch  7, batch     1 | loss: 2.2948129
MemoryTrain:  epoch  7, batch     2 | loss: 2.4462392
MemoryTrain:  epoch  8, batch     0 | loss: 2.0353401
MemoryTrain:  epoch  8, batch     1 | loss: 2.0104151
MemoryTrain:  epoch  8, batch     2 | loss: 1.8849225
MemoryTrain:  epoch  9, batch     0 | loss: 2.3255696
MemoryTrain:  epoch  9, batch     1 | loss: 1.6440287
MemoryTrain:  epoch  9, batch     2 | loss: 1.8637131
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   
[EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   
[EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   
[EVAL] batch:    3 | acc: 100.00%,  total acc: 95.31%   
[EVAL] batch:    4 | acc: 100.00%,  total acc: 96.25%   
[EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   
[EVAL] batch:    8 | acc: 75.00%,  total acc: 95.14%   
[EVAL] batch:    9 | acc: 0.00%,  total acc: 85.62%   
[EVAL] batch:   10 | acc: 12.50%,  total acc: 78.98%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 79.69%   
[EVAL] batch:   12 | acc: 68.75%,  total acc: 78.85%   
[EVAL] batch:   13 | acc: 62.50%,  total acc: 77.68%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   
[EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   
[EVAL] batch:    2 | acc: 56.25%,  total acc: 43.75%   
[EVAL] batch:    3 | acc: 31.25%,  total acc: 40.62%   
[EVAL] batch:    4 | acc: 50.00%,  total acc: 42.50%   
[EVAL] batch:    5 | acc: 43.75%,  total acc: 42.71%   
[EVAL] batch:    6 | acc: 81.25%,  total acc: 48.21%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 54.69%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 59.03%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 61.88%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 65.34%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 67.19%   
[EVAL] batch:   12 | acc: 81.25%,  total acc: 68.27%   
[EVAL] batch:   13 | acc: 50.00%,  total acc: 66.96%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 67.50%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 66.80%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 67.28%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 67.01%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 67.76%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 68.75%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 70.24%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 71.59%   
[EVAL] batch:   22 | acc: 93.75%,  total acc: 72.55%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 73.70%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 74.75%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 76.39%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 77.23%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 78.02%   
[EVAL] batch:   29 | acc: 93.75%,  total acc: 78.54%   
[EVAL] batch:   30 | acc: 100.00%,  total acc: 79.23%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 79.69%   
[EVAL] batch:   32 | acc: 75.00%,  total acc: 79.55%   
[EVAL] batch:   33 | acc: 100.00%,  total acc: 80.15%   
[EVAL] batch:   34 | acc: 56.25%,  total acc: 79.46%   
[EVAL] batch:   35 | acc: 12.50%,  total acc: 77.60%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 75.51%   
[EVAL] batch:   37 | acc: 6.25%,  total acc: 73.68%   
[EVAL] batch:   38 | acc: 0.00%,  total acc: 71.79%   
[EVAL] batch:   39 | acc: 50.00%,  total acc: 71.25%   
[EVAL] batch:   40 | acc: 87.50%,  total acc: 71.65%   
[EVAL] batch:   41 | acc: 93.75%,  total acc: 72.17%   
[EVAL] batch:   42 | acc: 100.00%,  total acc: 72.82%   
[EVAL] batch:   43 | acc: 100.00%,  total acc: 73.44%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 74.03%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 74.59%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 75.13%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 75.65%   
[EVAL] batch:   48 | acc: 25.00%,  total acc: 74.62%   
[EVAL] batch:   49 | acc: 0.00%,  total acc: 73.12%   
[EVAL] batch:   50 | acc: 50.00%,  total acc: 72.67%   
[EVAL] batch:   51 | acc: 87.50%,  total acc: 72.96%   
[EVAL] batch:   52 | acc: 75.00%,  total acc: 73.00%   
[EVAL] batch:   53 | acc: 18.75%,  total acc: 71.99%   
cur_acc:  ['0.8561', '0.2344', '0.7768']
his_acc:  ['0.8561', '0.7078', '0.7199']
CurrentTrain: epoch  0, batch     0 | loss: 5.7530441
CurrentTrain: epoch  0, batch     1 | loss: 6.4946232
CurrentTrain: epoch  1, batch     0 | loss: 5.6559124
CurrentTrain: epoch  1, batch     1 | loss: 5.1965876
CurrentTrain: epoch  2, batch     0 | loss: 4.5290427
CurrentTrain: epoch  2, batch     1 | loss: 4.9433756
CurrentTrain: epoch  3, batch     0 | loss: 4.3634691
CurrentTrain: epoch  3, batch     1 | loss: 4.5862575
CurrentTrain: epoch  4, batch     0 | loss: 4.2021470
CurrentTrain: epoch  4, batch     1 | loss: 3.8019459
CurrentTrain: epoch  5, batch     0 | loss: 3.9074302
CurrentTrain: epoch  5, batch     1 | loss: 3.3170314
CurrentTrain: epoch  6, batch     0 | loss: 3.6642632
CurrentTrain: epoch  6, batch     1 | loss: 3.2726922
CurrentTrain: epoch  7, batch     0 | loss: 3.3922508
CurrentTrain: epoch  7, batch     1 | loss: 2.6041570
CurrentTrain: epoch  8, batch     0 | loss: 3.0859437
CurrentTrain: epoch  8, batch     1 | loss: 2.7524347
CurrentTrain: epoch  9, batch     0 | loss: 2.9267397
CurrentTrain: epoch  9, batch     1 | loss: 3.1747880
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: after years of living in new york city, the famous author decided to return to her hometown of boston, where she feels most at home.  
Head Entity: she  
Tail Entity: boston  

Relation: person cities of residence  
Context: despite being a global superstar, the singer often reminisces about her childhood in nashville, where she first discovered her love for music.  
Head Entity: she  
Tail Entity: nashville  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: her political involvement began early : at cornell , she helped organize local farmers ' cooperatives .
Head Entity: she
Tail Entity: cornell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After graduating from high school, he enrolled at Stanford University to pursue his degree in computer science.  
Head Entity: he  
Tail Entity: Stanford University  

Relation: person schools attended  
Context: She completed her undergraduate studies at the University of Chicago, where she majored in economics.  
Head Entity: She  
Tail Entity: University of Chicago  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author agatha christie died in her home in wallingford, england  
Head Entity: agatha christie  
Tail Entity: england  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: previously , al-khawinay was sentenced to one year in jail for supporting the country 's minority shiite rebels and defaming the president , but was later pardoned by president ali abdullah saleh .
Head Entity: al-khawinay
Tail Entity: defaming the president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: In a shocking turn of events, the local authorities announced that the renowned activist, Maria Lopez, has been charged with inciting violence during the recent protests against government policies.  
Head Entity: Maria Lopez  
Tail Entity: inciting violence  

Relation: person charges  
Context: After a lengthy investigation, the district attorney revealed that former mayor John Smith was charged with corruption and misuse of public funds, leading to widespread outrage among the citizens.  
Head Entity: John Smith  
Tail Entity: corruption and misuse of public funds  
Mixup data size:  164
MixupTrain:  epoch  0, batch     0 | loss: 2.8276858
MixupTrain:  epoch  0, batch     1 | loss: 2.6861300
MixupTrain:  epoch  0, batch     2 | loss: 2.6679769
MixupTrain:  epoch  0, batch     3 | loss: 3.0320358
MixupTrain:  epoch  0, batch     4 | loss: 2.9489064
MixupTrain:  epoch  0, batch     5 | loss: 2.6051424
MixupTrain:  epoch  0, batch     6 | loss: 2.8039494
MixupTrain:  epoch  0, batch     7 | loss: 3.0670297
MixupTrain:  epoch  0, batch     8 | loss: 2.6613631
MixupTrain:  epoch  0, batch     9 | loss: 2.8444748
MixupTrain:  epoch  0, batch    10 | loss: 2.3828037
MemoryTrain:  epoch  0, batch     0 | loss: 2.2272830
MemoryTrain:  epoch  0, batch     1 | loss: 2.7981014
MemoryTrain:  epoch  0, batch     2 | loss: 3.8482985
MemoryTrain:  epoch  0, batch     3 | loss: 2.6843798
MemoryTrain:  epoch  1, batch     0 | loss: 2.9428277
MemoryTrain:  epoch  1, batch     1 | loss: 2.1027551
MemoryTrain:  epoch  1, batch     2 | loss: 2.2401114
MemoryTrain:  epoch  1, batch     3 | loss: 2.7575445
MemoryTrain:  epoch  2, batch     0 | loss: 2.4031444
MemoryTrain:  epoch  2, batch     1 | loss: 2.2455206
MemoryTrain:  epoch  2, batch     2 | loss: 2.1114454
MemoryTrain:  epoch  2, batch     3 | loss: 2.2994931
MemoryTrain:  epoch  3, batch     0 | loss: 2.0090520
MemoryTrain:  epoch  3, batch     1 | loss: 1.8576279
MemoryTrain:  epoch  3, batch     2 | loss: 2.2940226
MemoryTrain:  epoch  3, batch     3 | loss: 2.6130505
MemoryTrain:  epoch  4, batch     0 | loss: 2.2479429
MemoryTrain:  epoch  4, batch     1 | loss: 2.1471863
MemoryTrain:  epoch  4, batch     2 | loss: 1.6690704
MemoryTrain:  epoch  4, batch     3 | loss: 1.7390852
MemoryTrain:  epoch  5, batch     0 | loss: 2.0532374
MemoryTrain:  epoch  5, batch     1 | loss: 1.8197074
MemoryTrain:  epoch  5, batch     2 | loss: 1.9874427
MemoryTrain:  epoch  5, batch     3 | loss: 1.7035723
MemoryTrain:  epoch  6, batch     0 | loss: 1.6051030
MemoryTrain:  epoch  6, batch     1 | loss: 1.7096168
MemoryTrain:  epoch  6, batch     2 | loss: 1.6658661
MemoryTrain:  epoch  6, batch     3 | loss: 1.5678240
MemoryTrain:  epoch  7, batch     0 | loss: 1.4459412
MemoryTrain:  epoch  7, batch     1 | loss: 1.7298291
MemoryTrain:  epoch  7, batch     2 | loss: 1.6677027
MemoryTrain:  epoch  7, batch     3 | loss: 1.9456309
MemoryTrain:  epoch  8, batch     0 | loss: 1.4153453
MemoryTrain:  epoch  8, batch     1 | loss: 1.5723168
MemoryTrain:  epoch  8, batch     2 | loss: 1.5691793
MemoryTrain:  epoch  8, batch     3 | loss: 1.7063024
MemoryTrain:  epoch  9, batch     0 | loss: 1.6019560
MemoryTrain:  epoch  9, batch     1 | loss: 1.4209138
MemoryTrain:  epoch  9, batch     2 | loss: 1.6412265
MemoryTrain:  epoch  9, batch     3 | loss: 1.4875596
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   
[EVAL] batch:    2 | acc: 62.50%,  total acc: 70.83%   
[EVAL] batch:    3 | acc: 62.50%,  total acc: 68.75%   
[EVAL] batch:    4 | acc: 56.25%,  total acc: 66.25%   
[EVAL] batch:    5 | acc: 56.25%,  total acc: 64.58%   
[EVAL] batch:    6 | acc: 81.25%,  total acc: 66.96%   
[EVAL] batch:    7 | acc: 87.50%,  total acc: 69.53%   
[EVAL] batch:    8 | acc: 62.50%,  total acc: 68.75%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 71.25%   
[EVAL] batch:   10 | acc: 93.75%,  total acc: 73.30%   
[EVAL] batch:   11 | acc: 100.00%,  total acc: 75.52%   
[EVAL] batch:   12 | acc: 100.00%,  total acc: 77.40%   
[EVAL] batch:   13 | acc: 100.00%,  total acc: 79.02%   
[EVAL] batch:   14 | acc: 100.00%,  total acc: 80.42%   
[EVAL] batch:   15 | acc: 100.00%,  total acc: 81.64%   
[EVAL] batch:   16 | acc: 100.00%,  total acc: 82.72%   
[EVAL] batch:   17 | acc: 25.00%,  total acc: 79.51%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   
[EVAL] batch:    1 | acc: 37.50%,  total acc: 46.88%   
[EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   
[EVAL] batch:    3 | acc: 43.75%,  total acc: 48.44%   
[EVAL] batch:    4 | acc: 50.00%,  total acc: 48.75%   
[EVAL] batch:    5 | acc: 50.00%,  total acc: 48.96%   
[EVAL] batch:    6 | acc: 81.25%,  total acc: 53.57%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 59.38%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 63.19%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 65.62%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 68.75%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 70.31%   
[EVAL] batch:   12 | acc: 68.75%,  total acc: 70.19%   
[EVAL] batch:   13 | acc: 43.75%,  total acc: 68.30%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 68.75%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 67.97%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 68.38%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 68.06%   
[EVAL] batch:   18 | acc: 68.75%,  total acc: 68.09%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 68.75%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 70.24%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 71.59%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 72.83%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 73.70%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 74.75%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 75.72%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 76.39%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 77.23%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 78.02%   
[EVAL] batch:   29 | acc: 93.75%,  total acc: 78.54%   
[EVAL] batch:   30 | acc: 100.00%,  total acc: 79.23%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 79.69%   
[EVAL] batch:   32 | acc: 81.25%,  total acc: 79.73%   
[EVAL] batch:   33 | acc: 100.00%,  total acc: 80.33%   
[EVAL] batch:   34 | acc: 56.25%,  total acc: 79.64%   
[EVAL] batch:   35 | acc: 18.75%,  total acc: 77.95%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 75.84%   
[EVAL] batch:   37 | acc: 6.25%,  total acc: 74.01%   
[EVAL] batch:   38 | acc: 0.00%,  total acc: 72.12%   
[EVAL] batch:   39 | acc: 50.00%,  total acc: 71.56%   
[EVAL] batch:   40 | acc: 93.75%,  total acc: 72.10%   
[EVAL] batch:   41 | acc: 93.75%,  total acc: 72.62%   
[EVAL] batch:   42 | acc: 100.00%,  total acc: 73.26%   
[EVAL] batch:   43 | acc: 100.00%,  total acc: 73.86%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 74.44%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 75.00%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 75.53%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 76.04%   
[EVAL] batch:   48 | acc: 25.00%,  total acc: 75.00%   
[EVAL] batch:   49 | acc: 0.00%,  total acc: 73.50%   
[EVAL] batch:   50 | acc: 56.25%,  total acc: 73.16%   
[EVAL] batch:   51 | acc: 87.50%,  total acc: 73.44%   
[EVAL] batch:   52 | acc: 75.00%,  total acc: 73.47%   
[EVAL] batch:   53 | acc: 81.25%,  total acc: 73.61%   
[EVAL] batch:   54 | acc: 81.25%,  total acc: 73.75%   
[EVAL] batch:   55 | acc: 68.75%,  total acc: 73.66%   
[EVAL] batch:   56 | acc: 50.00%,  total acc: 73.25%   
[EVAL] batch:   57 | acc: 62.50%,  total acc: 73.06%   
[EVAL] batch:   58 | acc: 56.25%,  total acc: 72.78%   
[EVAL] batch:   59 | acc: 68.75%,  total acc: 72.71%   
[EVAL] batch:   60 | acc: 100.00%,  total acc: 73.16%   
[EVAL] batch:   61 | acc: 56.25%,  total acc: 72.88%   
[EVAL] batch:   62 | acc: 87.50%,  total acc: 73.12%   
[EVAL] batch:   63 | acc: 93.75%,  total acc: 73.44%   
[EVAL] batch:   64 | acc: 100.00%,  total acc: 73.85%   
[EVAL] batch:   65 | acc: 100.00%,  total acc: 74.24%   
[EVAL] batch:   66 | acc: 100.00%,  total acc: 74.63%   
[EVAL] batch:   67 | acc: 100.00%,  total acc: 75.00%   
[EVAL] batch:   68 | acc: 100.00%,  total acc: 75.36%   
[EVAL] batch:   69 | acc: 100.00%,  total acc: 75.71%   
[EVAL] batch:   70 | acc: 50.00%,  total acc: 75.35%   
cur_acc:  ['0.8561', '0.2344', '0.7768', '0.7951']
his_acc:  ['0.8561', '0.7078', '0.7199', '0.7535']
CurrentTrain: epoch  0, batch     0 | loss: 4.5091901
CurrentTrain: epoch  0, batch     1 | loss: 5.0055575
CurrentTrain: epoch  1, batch     0 | loss: 3.6540534
CurrentTrain: epoch  1, batch     1 | loss: 3.4993842
CurrentTrain: epoch  2, batch     0 | loss: 3.1025021
CurrentTrain: epoch  2, batch     1 | loss: 2.9652512
CurrentTrain: epoch  3, batch     0 | loss: 2.4778607
CurrentTrain: epoch  3, batch     1 | loss: 2.5248446
CurrentTrain: epoch  4, batch     0 | loss: 2.4871063
CurrentTrain: epoch  4, batch     1 | loss: 2.4819701
CurrentTrain: epoch  5, batch     0 | loss: 2.4157887
CurrentTrain: epoch  5, batch     1 | loss: 2.2149251
CurrentTrain: epoch  6, batch     0 | loss: 2.2382395
CurrentTrain: epoch  6, batch     1 | loss: 1.9806322
CurrentTrain: epoch  7, batch     0 | loss: 2.0562861
CurrentTrain: epoch  7, batch     1 | loss: 2.1046064
CurrentTrain: epoch  8, batch     0 | loss: 2.0797036
CurrentTrain: epoch  8, batch     1 | loss: 1.9793392
CurrentTrain: epoch  9, batch     0 | loss: 1.9798590
CurrentTrain: epoch  9, batch     1 | loss: 2.0592737
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: goodman , who had suffered a series of strokes and seizures in recent weeks , died of natural causes , her son david said .
Head Entity: goodman
Tail Entity: natural causes
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling cancer for several years, the renowned artist passed away peacefully in her sleep, her family announced.  
Head Entity: the renowned artist  
Tail Entity: cancer  

Relation: person cause of death  
Context: following a tragic accident on the highway, the local community mourned the loss of the beloved teacher who had dedicated her life to education.  
Head Entity: the beloved teacher  
Tail Entity: tragic accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The recent conference held by the Islamic Society of North America aimed to strengthen ties between various Muslim organizations and promote interfaith dialogue.  
Head Entity: Islamic Society of North America  
Tail Entity: Islam  

Relation: organization political religious affiliation  
Context: During the annual meeting, the Catholic Charities organization discussed its initiatives to support the local community and its commitment to the teachings of the Catholic Church.  
Head Entity: Catholic Charities  
Tail Entity: Catholicism  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Michael, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person other family  
Context: In her memoir, Lisa shared stories about her aunt, who played a significant role in her upbringing and inspired her to pursue a career in art.  
Head Entity: Lisa  
Tail Entity: her aunt  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  195
MixupTrain:  epoch  0, batch     0 | loss: 2.7176242
MixupTrain:  epoch  0, batch     1 | loss: 2.6693511
MixupTrain:  epoch  0, batch     2 | loss: 2.7453816
MixupTrain:  epoch  0, batch     3 | loss: 3.0312114
MixupTrain:  epoch  0, batch     4 | loss: 2.6930676
MixupTrain:  epoch  0, batch     5 | loss: 2.7071216
MixupTrain:  epoch  0, batch     6 | loss: 2.6287992
MixupTrain:  epoch  0, batch     7 | loss: 2.7324417
MixupTrain:  epoch  0, batch     8 | loss: 2.6432056
MixupTrain:  epoch  0, batch     9 | loss: 2.7875934
MixupTrain:  epoch  0, batch    10 | loss: 2.5172668
MixupTrain:  epoch  0, batch    11 | loss: 2.6594398
MixupTrain:  epoch  0, batch    12 | loss: 2.7641544
MemoryTrain:  epoch  0, batch     0 | loss: 2.5848403
MemoryTrain:  epoch  0, batch     1 | loss: 2.5592015
MemoryTrain:  epoch  0, batch     2 | loss: 3.0874023
MemoryTrain:  epoch  0, batch     3 | loss: 3.5521181
MemoryTrain:  epoch  0, batch     4 | loss: 2.9306753
MemoryTrain:  epoch  1, batch     0 | loss: 2.2097964
MemoryTrain:  epoch  1, batch     1 | loss: 3.3171842
MemoryTrain:  epoch  1, batch     2 | loss: 2.3108439
MemoryTrain:  epoch  1, batch     3 | loss: 2.4731100
MemoryTrain:  epoch  1, batch     4 | loss: 3.6534550
MemoryTrain:  epoch  2, batch     0 | loss: 3.1266894
MemoryTrain:  epoch  2, batch     1 | loss: 2.2116067
MemoryTrain:  epoch  2, batch     2 | loss: 2.1737838
MemoryTrain:  epoch  2, batch     3 | loss: 2.5921073
MemoryTrain:  epoch  2, batch     4 | loss: 2.6218507
MemoryTrain:  epoch  3, batch     0 | loss: 2.5280719
MemoryTrain:  epoch  3, batch     1 | loss: 2.7255051
MemoryTrain:  epoch  3, batch     2 | loss: 2.4934721
MemoryTrain:  epoch  3, batch     3 | loss: 1.7838030
MemoryTrain:  epoch  3, batch     4 | loss: 2.1743498
MemoryTrain:  epoch  4, batch     0 | loss: 1.9358231
MemoryTrain:  epoch  4, batch     1 | loss: 2.4313459
MemoryTrain:  epoch  4, batch     2 | loss: 2.4394851
MemoryTrain:  epoch  4, batch     3 | loss: 1.5838447
MemoryTrain:  epoch  4, batch     4 | loss: 1.7074529
MemoryTrain:  epoch  5, batch     0 | loss: 1.7758548
MemoryTrain:  epoch  5, batch     1 | loss: 1.8164239
MemoryTrain:  epoch  5, batch     2 | loss: 1.8996363
MemoryTrain:  epoch  5, batch     3 | loss: 1.9882433
MemoryTrain:  epoch  5, batch     4 | loss: 2.1449780
MemoryTrain:  epoch  6, batch     0 | loss: 2.1915524
MemoryTrain:  epoch  6, batch     1 | loss: 1.7721713
MemoryTrain:  epoch  6, batch     2 | loss: 1.9965333
MemoryTrain:  epoch  6, batch     3 | loss: 1.7332170
MemoryTrain:  epoch  6, batch     4 | loss: 1.9393543
MemoryTrain:  epoch  7, batch     0 | loss: 1.5345870
MemoryTrain:  epoch  7, batch     1 | loss: 1.8097982
MemoryTrain:  epoch  7, batch     2 | loss: 1.8569427
MemoryTrain:  epoch  7, batch     3 | loss: 1.7743843
MemoryTrain:  epoch  7, batch     4 | loss: 1.5364214
MemoryTrain:  epoch  8, batch     0 | loss: 1.4514952
MemoryTrain:  epoch  8, batch     1 | loss: 1.7727185
MemoryTrain:  epoch  8, batch     2 | loss: 1.5218086
MemoryTrain:  epoch  8, batch     3 | loss: 1.9376590
MemoryTrain:  epoch  8, batch     4 | loss: 1.7441156
MemoryTrain:  epoch  9, batch     0 | loss: 1.5974081
MemoryTrain:  epoch  9, batch     1 | loss: 1.6588278
MemoryTrain:  epoch  9, batch     2 | loss: 1.6536016
MemoryTrain:  epoch  9, batch     3 | loss: 1.5707908
MemoryTrain:  epoch  9, batch     4 | loss: 1.6132818
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   
[EVAL] batch:    1 | acc: 68.75%,  total acc: 59.38%   
[EVAL] batch:    2 | acc: 62.50%,  total acc: 60.42%   
[EVAL] batch:    3 | acc: 62.50%,  total acc: 60.94%   
[EVAL] batch:    4 | acc: 43.75%,  total acc: 57.50%   
[EVAL] batch:    5 | acc: 62.50%,  total acc: 58.33%   
[EVAL] batch:    6 | acc: 68.75%,  total acc: 59.82%   
[EVAL] batch:    7 | acc: 75.00%,  total acc: 61.72%   
[EVAL] batch:    8 | acc: 68.75%,  total acc: 62.50%   
[EVAL] batch:    9 | acc: 81.25%,  total acc: 64.38%   
[EVAL] batch:   10 | acc: 87.50%,  total acc: 66.48%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 68.23%   
[EVAL] batch:   12 | acc: 50.00%,  total acc: 66.83%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   
[EVAL] batch:    1 | acc: 37.50%,  total acc: 46.88%   
[EVAL] batch:    2 | acc: 56.25%,  total acc: 50.00%   
[EVAL] batch:    3 | acc: 31.25%,  total acc: 45.31%   
[EVAL] batch:    4 | acc: 62.50%,  total acc: 48.75%   
[EVAL] batch:    5 | acc: 56.25%,  total acc: 50.00%   
[EVAL] batch:    6 | acc: 81.25%,  total acc: 54.46%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 60.16%   
[EVAL] batch:    8 | acc: 87.50%,  total acc: 63.19%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 66.25%   
[EVAL] batch:   10 | acc: 93.75%,  total acc: 68.75%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 70.31%   
[EVAL] batch:   12 | acc: 56.25%,  total acc: 69.23%   
[EVAL] batch:   13 | acc: 43.75%,  total acc: 67.41%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 67.92%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 67.19%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 67.65%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 67.36%   
[EVAL] batch:   18 | acc: 68.75%,  total acc: 67.43%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 68.44%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 69.94%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 71.31%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 72.55%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 73.44%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 74.50%   
[EVAL] batch:   25 | acc: 93.75%,  total acc: 75.24%   
[EVAL] batch:   26 | acc: 87.50%,  total acc: 75.69%   
[EVAL] batch:   27 | acc: 81.25%,  total acc: 75.89%   
[EVAL] batch:   28 | acc: 87.50%,  total acc: 76.29%   
[EVAL] batch:   29 | acc: 68.75%,  total acc: 76.04%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 76.41%   
[EVAL] batch:   31 | acc: 87.50%,  total acc: 76.76%   
[EVAL] batch:   32 | acc: 68.75%,  total acc: 76.52%   
[EVAL] batch:   33 | acc: 100.00%,  total acc: 77.21%   
[EVAL] batch:   34 | acc: 62.50%,  total acc: 76.79%   
[EVAL] batch:   35 | acc: 18.75%,  total acc: 75.17%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 73.14%   
[EVAL] batch:   37 | acc: 6.25%,  total acc: 71.38%   
[EVAL] batch:   38 | acc: 0.00%,  total acc: 69.55%   
[EVAL] batch:   39 | acc: 50.00%,  total acc: 69.06%   
[EVAL] batch:   40 | acc: 93.75%,  total acc: 69.66%   
[EVAL] batch:   41 | acc: 93.75%,  total acc: 70.24%   
[EVAL] batch:   42 | acc: 100.00%,  total acc: 70.93%   
[EVAL] batch:   43 | acc: 100.00%,  total acc: 71.59%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 72.22%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 72.83%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 73.40%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 73.96%   
[EVAL] batch:   48 | acc: 18.75%,  total acc: 72.83%   
[EVAL] batch:   49 | acc: 6.25%,  total acc: 71.50%   
[EVAL] batch:   50 | acc: 62.50%,  total acc: 71.32%   
[EVAL] batch:   51 | acc: 81.25%,  total acc: 71.51%   
[EVAL] batch:   52 | acc: 62.50%,  total acc: 71.34%   
[EVAL] batch:   53 | acc: 37.50%,  total acc: 70.72%   
[EVAL] batch:   54 | acc: 12.50%,  total acc: 69.66%   
[EVAL] batch:   55 | acc: 18.75%,  total acc: 68.75%   
[EVAL] batch:   56 | acc: 31.25%,  total acc: 68.09%   
[EVAL] batch:   57 | acc: 25.00%,  total acc: 67.35%   
[EVAL] batch:   58 | acc: 6.25%,  total acc: 66.31%   
[EVAL] batch:   59 | acc: 56.25%,  total acc: 66.15%   
[EVAL] batch:   60 | acc: 100.00%,  total acc: 66.70%   
[EVAL] batch:   61 | acc: 37.50%,  total acc: 66.23%   
[EVAL] batch:   62 | acc: 62.50%,  total acc: 66.17%   
[EVAL] batch:   63 | acc: 62.50%,  total acc: 66.11%   
[EVAL] batch:   64 | acc: 87.50%,  total acc: 66.44%   
[EVAL] batch:   65 | acc: 100.00%,  total acc: 66.95%   
[EVAL] batch:   66 | acc: 100.00%,  total acc: 67.44%   
[EVAL] batch:   67 | acc: 100.00%,  total acc: 67.92%   
[EVAL] batch:   68 | acc: 100.00%,  total acc: 68.39%   
[EVAL] batch:   69 | acc: 100.00%,  total acc: 68.84%   
[EVAL] batch:   70 | acc: 68.75%,  total acc: 68.84%   
[EVAL] batch:   71 | acc: 56.25%,  total acc: 68.66%   
[EVAL] batch:   72 | acc: 75.00%,  total acc: 68.75%   
[EVAL] batch:   73 | acc: 56.25%,  total acc: 68.58%   
[EVAL] batch:   74 | acc: 62.50%,  total acc: 68.50%   
[EVAL] batch:   75 | acc: 43.75%,  total acc: 68.17%   
[EVAL] batch:   76 | acc: 81.25%,  total acc: 68.34%   
[EVAL] batch:   77 | acc: 62.50%,  total acc: 68.27%   
[EVAL] batch:   78 | acc: 75.00%,  total acc: 68.35%   
[EVAL] batch:   79 | acc: 75.00%,  total acc: 68.44%   
[EVAL] batch:   80 | acc: 75.00%,  total acc: 68.52%   
[EVAL] batch:   81 | acc: 93.75%,  total acc: 68.83%   
[EVAL] batch:   82 | acc: 87.50%,  total acc: 69.05%   
[EVAL] batch:   83 | acc: 6.25%,  total acc: 68.30%   
cur_acc:  ['0.8561', '0.2344', '0.7768', '0.7951', '0.6683']
his_acc:  ['0.8561', '0.7078', '0.7199', '0.7535', '0.6830']
CurrentTrain: epoch  0, batch     0 | loss: 5.3470955
CurrentTrain: epoch  0, batch     1 | loss: 5.5057659
CurrentTrain: epoch  1, batch     0 | loss: 4.2892942
CurrentTrain: epoch  1, batch     1 | loss: 4.3135400
CurrentTrain: epoch  2, batch     0 | loss: 3.6764765
CurrentTrain: epoch  2, batch     1 | loss: 4.0393634
CurrentTrain: epoch  3, batch     0 | loss: 3.5815620
CurrentTrain: epoch  3, batch     1 | loss: 3.0224321
CurrentTrain: epoch  4, batch     0 | loss: 2.9857349
CurrentTrain: epoch  4, batch     1 | loss: 3.1385281
CurrentTrain: epoch  5, batch     0 | loss: 2.6931736
CurrentTrain: epoch  5, batch     1 | loss: 2.7666156
CurrentTrain: epoch  6, batch     0 | loss: 2.5894361
CurrentTrain: epoch  6, batch     1 | loss: 2.4085252
CurrentTrain: epoch  7, batch     0 | loss: 2.4993956
CurrentTrain: epoch  7, batch     1 | loss: 2.5081797
CurrentTrain: epoch  8, batch     0 | loss: 2.4233201
CurrentTrain: epoch  8, batch     1 | loss: 2.2617722
CurrentTrain: epoch  9, batch     0 | loss: 2.4543340
CurrentTrain: epoch  9, batch     1 | loss: 2.2374327
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, germany, on march 14, 1879, and later developed the theory of relativity.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author mark twain, known for his classic novels, was born in florida, missouri, on november 30, 1835.  
Head Entity: mark twain  
Tail Entity: november 30, 1835  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, germany.  
Head Entity: albert einstein  
Tail Entity: germany  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: After the ceremony, Emily's father, John Smith, shared heartfelt stories about her childhood, bringing tears to many eyes.  
Head Entity: Emily  
Tail Entity: John Smith  

Relation: person parents  
Context: During the family reunion, Michael's mother, Sarah Johnson, prepared her famous lasagna, which everyone eagerly anticipated.  
Head Entity: Michael  
Tail Entity: Sarah Johnson  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson has finally landed a position at one of the top tech companies in Silicon Valley, where she will be contributing to innovative projects.  
Head Entity: Sarah Thompson  
Tail Entity: top tech company  

Relation: person employee of  
Context: John Smith, a talented graphic designer, has been working for Creative Solutions for over five years, helping to shape the visual identity of numerous brands.  
Head Entity: John Smith  
Tail Entity: Creative Solutions  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california last night.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, succumbed to her health issues and died in a hospital in nevada.  
Head Entity: elizabeth taylor  
Tail Entity: nevada  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 2.4775259
MixupTrain:  epoch  0, batch     1 | loss: 2.7603450
MixupTrain:  epoch  0, batch     2 | loss: 2.4945707
MixupTrain:  epoch  0, batch     3 | loss: 2.7832532
MixupTrain:  epoch  0, batch     4 | loss: 2.7773385
MixupTrain:  epoch  0, batch     5 | loss: 2.7117457
MixupTrain:  epoch  0, batch     6 | loss: 2.2745178
MixupTrain:  epoch  0, batch     7 | loss: 2.3437827
MixupTrain:  epoch  0, batch     8 | loss: 2.4785392
MixupTrain:  epoch  0, batch     9 | loss: 2.4772174
MixupTrain:  epoch  0, batch    10 | loss: 2.3265383
MixupTrain:  epoch  0, batch    11 | loss: 2.1583433
MixupTrain:  epoch  0, batch    12 | loss: 2.5820913
MixupTrain:  epoch  0, batch    13 | loss: 2.6612844
MemoryTrain:  epoch  0, batch     0 | loss: 2.0176032
MemoryTrain:  epoch  0, batch     1 | loss: 2.4271290
MemoryTrain:  epoch  0, batch     2 | loss: 2.4571419
MemoryTrain:  epoch  0, batch     3 | loss: 2.5601845
MemoryTrain:  epoch  0, batch     4 | loss: 2.8617756
MemoryTrain:  epoch  0, batch     5 | loss: 2.1278510
MemoryTrain:  epoch  1, batch     0 | loss: 2.8525515
MemoryTrain:  epoch  1, batch     1 | loss: 2.3799820
MemoryTrain:  epoch  1, batch     2 | loss: 1.8362710
MemoryTrain:  epoch  1, batch     3 | loss: 2.4794660
MemoryTrain:  epoch  1, batch     4 | loss: 2.5320888
MemoryTrain:  epoch  1, batch     5 | loss: 2.3945341
MemoryTrain:  epoch  2, batch     0 | loss: 2.2100306
MemoryTrain:  epoch  2, batch     1 | loss: 1.7906909
MemoryTrain:  epoch  2, batch     2 | loss: 2.4734695
MemoryTrain:  epoch  2, batch     3 | loss: 2.3287251
MemoryTrain:  epoch  2, batch     4 | loss: 2.0375502
MemoryTrain:  epoch  2, batch     5 | loss: 1.6219702
MemoryTrain:  epoch  3, batch     0 | loss: 2.1171281
MemoryTrain:  epoch  3, batch     1 | loss: 1.6322689
MemoryTrain:  epoch  3, batch     2 | loss: 1.7383755
MemoryTrain:  epoch  3, batch     3 | loss: 1.9016193
MemoryTrain:  epoch  3, batch     4 | loss: 1.8150394
MemoryTrain:  epoch  3, batch     5 | loss: 2.0230227
MemoryTrain:  epoch  4, batch     0 | loss: 1.6613543
MemoryTrain:  epoch  4, batch     1 | loss: 2.0681157
MemoryTrain:  epoch  4, batch     2 | loss: 1.7520268
MemoryTrain:  epoch  4, batch     3 | loss: 1.7513710
MemoryTrain:  epoch  4, batch     4 | loss: 1.8634008
MemoryTrain:  epoch  4, batch     5 | loss: 1.6573555
MemoryTrain:  epoch  5, batch     0 | loss: 1.7757342
MemoryTrain:  epoch  5, batch     1 | loss: 1.6589448
MemoryTrain:  epoch  5, batch     2 | loss: 1.8235943
MemoryTrain:  epoch  5, batch     3 | loss: 1.5192859
MemoryTrain:  epoch  5, batch     4 | loss: 1.8787707
MemoryTrain:  epoch  5, batch     5 | loss: 1.5615618
MemoryTrain:  epoch  6, batch     0 | loss: 1.5930984
MemoryTrain:  epoch  6, batch     1 | loss: 1.6421852
MemoryTrain:  epoch  6, batch     2 | loss: 1.5365696
MemoryTrain:  epoch  6, batch     3 | loss: 1.5395564
MemoryTrain:  epoch  6, batch     4 | loss: 1.4772015
MemoryTrain:  epoch  6, batch     5 | loss: 1.5032755
MemoryTrain:  epoch  7, batch     0 | loss: 1.6466310
MemoryTrain:  epoch  7, batch     1 | loss: 1.5632789
MemoryTrain:  epoch  7, batch     2 | loss: 1.4326179
MemoryTrain:  epoch  7, batch     3 | loss: 1.5075464
MemoryTrain:  epoch  7, batch     4 | loss: 1.4259412
MemoryTrain:  epoch  7, batch     5 | loss: 1.4675093
MemoryTrain:  epoch  8, batch     0 | loss: 1.5640033
MemoryTrain:  epoch  8, batch     1 | loss: 1.3871505
MemoryTrain:  epoch  8, batch     2 | loss: 1.5023417
MemoryTrain:  epoch  8, batch     3 | loss: 1.4760022
MemoryTrain:  epoch  8, batch     4 | loss: 1.4692440
MemoryTrain:  epoch  8, batch     5 | loss: 1.5745550
MemoryTrain:  epoch  9, batch     0 | loss: 1.5039186
MemoryTrain:  epoch  9, batch     1 | loss: 1.5523493
MemoryTrain:  epoch  9, batch     2 | loss: 1.5391474
MemoryTrain:  epoch  9, batch     3 | loss: 1.4144617
MemoryTrain:  epoch  9, batch     4 | loss: 1.5291429
MemoryTrain:  epoch  9, batch     5 | loss: 1.3979539
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   
[EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   
[EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   
[EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   
[EVAL] batch:    5 | acc: 81.25%,  total acc: 79.17%   
[EVAL] batch:    6 | acc: 62.50%,  total acc: 76.79%   
[EVAL] batch:    7 | acc: 93.75%,  total acc: 78.91%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 80.56%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 81.88%   
[EVAL] batch:   10 | acc: 87.50%,  total acc: 82.39%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 82.81%   
[EVAL] batch:   12 | acc: 87.50%,  total acc: 83.17%   
[EVAL] batch:   13 | acc: 25.00%,  total acc: 79.02%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 65.62%   
[EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   
[EVAL] batch:    3 | acc: 56.25%,  total acc: 65.62%   
[EVAL] batch:    4 | acc: 75.00%,  total acc: 67.50%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 67.71%   
[EVAL] batch:    6 | acc: 81.25%,  total acc: 69.64%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 73.44%   
[EVAL] batch:    8 | acc: 87.50%,  total acc: 75.00%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 76.25%   
[EVAL] batch:   10 | acc: 93.75%,  total acc: 77.84%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 78.65%   
[EVAL] batch:   12 | acc: 56.25%,  total acc: 76.92%   
[EVAL] batch:   13 | acc: 37.50%,  total acc: 74.11%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 74.17%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 73.05%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 73.16%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 72.57%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 72.04%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 72.81%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 74.11%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 75.00%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 76.09%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 76.82%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 77.75%   
[EVAL] batch:   25 | acc: 93.75%,  total acc: 78.37%   
[EVAL] batch:   26 | acc: 87.50%,  total acc: 78.70%   
[EVAL] batch:   27 | acc: 81.25%,  total acc: 78.79%   
[EVAL] batch:   28 | acc: 93.75%,  total acc: 79.31%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 79.38%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 79.64%   
[EVAL] batch:   31 | acc: 87.50%,  total acc: 79.88%   
[EVAL] batch:   32 | acc: 56.25%,  total acc: 79.17%   
[EVAL] batch:   33 | acc: 100.00%,  total acc: 79.78%   
[EVAL] batch:   34 | acc: 68.75%,  total acc: 79.46%   
[EVAL] batch:   35 | acc: 25.00%,  total acc: 77.95%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 75.84%   
[EVAL] batch:   37 | acc: 0.00%,  total acc: 73.85%   
[EVAL] batch:   38 | acc: 0.00%,  total acc: 71.96%   
[EVAL] batch:   39 | acc: 50.00%,  total acc: 71.41%   
[EVAL] batch:   40 | acc: 87.50%,  total acc: 71.80%   
[EVAL] batch:   41 | acc: 100.00%,  total acc: 72.47%   
[EVAL] batch:   42 | acc: 100.00%,  total acc: 73.11%   
[EVAL] batch:   43 | acc: 100.00%,  total acc: 73.72%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 74.31%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 74.86%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 75.40%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 75.91%   
[EVAL] batch:   48 | acc: 12.50%,  total acc: 74.62%   
[EVAL] batch:   49 | acc: 0.00%,  total acc: 73.12%   
[EVAL] batch:   50 | acc: 62.50%,  total acc: 72.92%   
[EVAL] batch:   51 | acc: 81.25%,  total acc: 73.08%   
[EVAL] batch:   52 | acc: 56.25%,  total acc: 72.76%   
[EVAL] batch:   53 | acc: 25.00%,  total acc: 71.88%   
[EVAL] batch:   54 | acc: 18.75%,  total acc: 70.91%   
[EVAL] batch:   55 | acc: 18.75%,  total acc: 69.98%   
[EVAL] batch:   56 | acc: 37.50%,  total acc: 69.41%   
[EVAL] batch:   57 | acc: 37.50%,  total acc: 68.86%   
[EVAL] batch:   58 | acc: 18.75%,  total acc: 68.01%   
[EVAL] batch:   59 | acc: 50.00%,  total acc: 67.71%   
[EVAL] batch:   60 | acc: 81.25%,  total acc: 67.93%   
[EVAL] batch:   61 | acc: 25.00%,  total acc: 67.24%   
[EVAL] batch:   62 | acc: 31.25%,  total acc: 66.67%   
[EVAL] batch:   63 | acc: 43.75%,  total acc: 66.31%   
[EVAL] batch:   64 | acc: 87.50%,  total acc: 66.63%   
[EVAL] batch:   65 | acc: 100.00%,  total acc: 67.14%   
[EVAL] batch:   66 | acc: 100.00%,  total acc: 67.63%   
[EVAL] batch:   67 | acc: 100.00%,  total acc: 68.11%   
[EVAL] batch:   68 | acc: 100.00%,  total acc: 68.57%   
[EVAL] batch:   69 | acc: 100.00%,  total acc: 69.02%   
[EVAL] batch:   70 | acc: 62.50%,  total acc: 68.93%   
[EVAL] batch:   71 | acc: 50.00%,  total acc: 68.66%   
[EVAL] batch:   72 | acc: 62.50%,  total acc: 68.58%   
[EVAL] batch:   73 | acc: 50.00%,  total acc: 68.33%   
[EVAL] batch:   74 | acc: 43.75%,  total acc: 68.00%   
[EVAL] batch:   75 | acc: 31.25%,  total acc: 67.52%   
[EVAL] batch:   76 | acc: 31.25%,  total acc: 67.05%   
[EVAL] batch:   77 | acc: 37.50%,  total acc: 66.67%   
[EVAL] batch:   78 | acc: 37.50%,  total acc: 66.30%   
[EVAL] batch:   79 | acc: 31.25%,  total acc: 65.86%   
[EVAL] batch:   80 | acc: 37.50%,  total acc: 65.51%   
[EVAL] batch:   81 | acc: 68.75%,  total acc: 65.55%   
[EVAL] batch:   82 | acc: 81.25%,  total acc: 65.74%   
[EVAL] batch:   83 | acc: 81.25%,  total acc: 65.92%   
[EVAL] batch:   84 | acc: 75.00%,  total acc: 66.03%   
[EVAL] batch:   85 | acc: 75.00%,  total acc: 66.13%   
[EVAL] batch:   86 | acc: 75.00%,  total acc: 66.24%   
[EVAL] batch:   87 | acc: 87.50%,  total acc: 66.48%   
[EVAL] batch:   88 | acc: 87.50%,  total acc: 66.71%   
[EVAL] batch:   89 | acc: 62.50%,  total acc: 66.67%   
[EVAL] batch:   90 | acc: 87.50%,  total acc: 66.90%   
[EVAL] batch:   91 | acc: 93.75%,  total acc: 67.19%   
[EVAL] batch:   92 | acc: 93.75%,  total acc: 67.47%   
[EVAL] batch:   93 | acc: 93.75%,  total acc: 67.75%   
[EVAL] batch:   94 | acc: 81.25%,  total acc: 67.89%   
[EVAL] batch:   95 | acc: 87.50%,  total acc: 68.10%   
[EVAL] batch:   96 | acc: 31.25%,  total acc: 67.72%   
cur_acc:  ['0.8561', '0.2344', '0.7768', '0.7951', '0.6683', '0.7902']
his_acc:  ['0.8561', '0.7078', '0.7199', '0.7535', '0.6830', '0.6772']
CurrentTrain: epoch  0, batch     0 | loss: 7.4211831
CurrentTrain: epoch  0, batch     1 | loss: 8.6482935
CurrentTrain: epoch  1, batch     0 | loss: 6.7493057
CurrentTrain: epoch  1, batch     1 | loss: 7.2835550
CurrentTrain: epoch  2, batch     0 | loss: 6.4152818
CurrentTrain: epoch  2, batch     1 | loss: 6.5229058
CurrentTrain: epoch  3, batch     0 | loss: 6.8507509
CurrentTrain: epoch  3, batch     1 | loss: 5.3761053
CurrentTrain: epoch  4, batch     0 | loss: 5.9653568
CurrentTrain: epoch  4, batch     1 | loss: 5.4654088
CurrentTrain: epoch  5, batch     0 | loss: 5.7385283
CurrentTrain: epoch  5, batch     1 | loss: 5.4607382
CurrentTrain: epoch  6, batch     0 | loss: 5.4757109
CurrentTrain: epoch  6, batch     1 | loss: 4.8388352
CurrentTrain: epoch  7, batch     0 | loss: 4.9495435
CurrentTrain: epoch  7, batch     1 | loss: 5.0826635
CurrentTrain: epoch  8, batch     0 | loss: 4.8442183
CurrentTrain: epoch  8, batch     1 | loss: 4.4215751
CurrentTrain: epoch  9, batch     0 | loss: 4.5477676
CurrentTrain: epoch  9, batch     1 | loss: 4.0514169
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has expanded its portfolio by acquiring Fitbit, a company known for its fitness tracking devices, which now operates as a subsidiary under the Google brand.  
Head Entity: Alphabet Inc.  
Tail Entity: Fitbit  

Relation: organization subsidiaries  
Context: In a strategic move, the automotive manufacturer Ford Motor Company has announced that it will be integrating its electric vehicle division with Rivian, a startup that specializes in electric trucks and SUVs, making it a subsidiary of Ford.  
Head Entity: Ford Motor Company  
Tail Entity: Rivian  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: those who try to salvage possessions from the debris of their homes can easily turn into victims too , said dr. irwin redlener , director of the national center for disaster preparedness at columbia university mailman school of medicine .
Head Entity: national center for disaster preparedness
Tail Entity: columbia university mailman school of medicine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: The Massachusetts Institute of Technology, known for its cutting-edge research and innovation, is part of a larger network of educational institutions that includes Harvard University.  
Head Entity: Massachusetts Institute of Technology  
Tail Entity: Harvard University  

Relation: organization parents  
Context: The World Wildlife Fund, a leading organization in conservation efforts, operates under the umbrella of the larger World Wide Fund for Nature.  
Head Entity: World Wildlife Fund  
Tail Entity: World Wide Fund for Nature  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the world health organization -lrb- who -rrb- has been instrumental in coordinating global responses to health emergencies, including the recent pandemic.  
Head Entity: world health organization  
Tail Entity: who  

Relation: organization alternate names  
Context: the federal bureau of investigation -lrb- fbi -rrb- is known for its role in investigating federal crimes and enforcing federal laws across the United States.  
Head Entity: federal bureau of investigation  
Tail Entity: fbi  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: guy-sheftall entered spelman college in atlanta at age 16 and later earned a master 's in english with a thesis titled `` faulkner 's treatment of women in his major novels . ''
Head Entity: spelman college
Tail Entity: atlanta
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been since 1993.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: the united nations has its main office in new york city, serving as a hub for international diplomacy and cooperation.  
Head Entity: united nations  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time catching up. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily and her brother celebrated their achievements with a big family dinner. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 2.6014857
MixupTrain:  epoch  0, batch     1 | loss: 2.7226081
MixupTrain:  epoch  0, batch     2 | loss: 2.0277772
MixupTrain:  epoch  0, batch     3 | loss: 2.3687024
MixupTrain:  epoch  0, batch     4 | loss: 2.8009653
MixupTrain:  epoch  0, batch     5 | loss: 2.2870216
MixupTrain:  epoch  0, batch     6 | loss: 2.7733951
MixupTrain:  epoch  0, batch     7 | loss: 2.4352560
MixupTrain:  epoch  0, batch     8 | loss: 2.4466629
MixupTrain:  epoch  0, batch     9 | loss: 2.6390676
MixupTrain:  epoch  0, batch    10 | loss: 2.3393271
MixupTrain:  epoch  0, batch    11 | loss: 2.4402957
MixupTrain:  epoch  0, batch    12 | loss: 2.4344027
MixupTrain:  epoch  0, batch    13 | loss: 2.2118917
MixupTrain:  epoch  0, batch    14 | loss: 2.5406504
MixupTrain:  epoch  0, batch    15 | loss: 2.5018563
MemoryTrain:  epoch  0, batch     0 | loss: 1.6489702
MemoryTrain:  epoch  0, batch     1 | loss: 2.3996994
MemoryTrain:  epoch  0, batch     2 | loss: 2.4862537
MemoryTrain:  epoch  0, batch     3 | loss: 2.9386435
MemoryTrain:  epoch  0, batch     4 | loss: 2.4402153
MemoryTrain:  epoch  0, batch     5 | loss: 2.4933834
MemoryTrain:  epoch  0, batch     6 | loss: 3.1261826
MemoryTrain:  epoch  1, batch     0 | loss: 2.0715961
MemoryTrain:  epoch  1, batch     1 | loss: 3.1864502
MemoryTrain:  epoch  1, batch     2 | loss: 2.0584559
MemoryTrain:  epoch  1, batch     3 | loss: 1.9903567
MemoryTrain:  epoch  1, batch     4 | loss: 2.4181952
MemoryTrain:  epoch  1, batch     5 | loss: 1.7136923
MemoryTrain:  epoch  1, batch     6 | loss: 2.8537185
MemoryTrain:  epoch  2, batch     0 | loss: 2.1798716
MemoryTrain:  epoch  2, batch     1 | loss: 2.6171799
MemoryTrain:  epoch  2, batch     2 | loss: 2.3336854
MemoryTrain:  epoch  2, batch     3 | loss: 1.6948700
MemoryTrain:  epoch  2, batch     4 | loss: 2.0257273
MemoryTrain:  epoch  2, batch     5 | loss: 1.7876951
MemoryTrain:  epoch  2, batch     6 | loss: 2.1514444
MemoryTrain:  epoch  3, batch     0 | loss: 2.2492008
MemoryTrain:  epoch  3, batch     1 | loss: 1.8038828
MemoryTrain:  epoch  3, batch     2 | loss: 1.9571661
MemoryTrain:  epoch  3, batch     3 | loss: 1.4933683
MemoryTrain:  epoch  3, batch     4 | loss: 2.1925895
MemoryTrain:  epoch  3, batch     5 | loss: 2.0678813
MemoryTrain:  epoch  3, batch     6 | loss: 1.7689775
MemoryTrain:  epoch  4, batch     0 | loss: 1.8162373
MemoryTrain:  epoch  4, batch     1 | loss: 1.6136611
MemoryTrain:  epoch  4, batch     2 | loss: 1.3272086
MemoryTrain:  epoch  4, batch     3 | loss: 2.1860871
MemoryTrain:  epoch  4, batch     4 | loss: 1.8098490
MemoryTrain:  epoch  4, batch     5 | loss: 1.8762826
MemoryTrain:  epoch  4, batch     6 | loss: 1.6971166
MemoryTrain:  epoch  5, batch     0 | loss: 2.0440099
MemoryTrain:  epoch  5, batch     1 | loss: 1.6957231
MemoryTrain:  epoch  5, batch     2 | loss: 1.4343827
MemoryTrain:  epoch  5, batch     3 | loss: 1.6944580
MemoryTrain:  epoch  5, batch     4 | loss: 1.5290143
MemoryTrain:  epoch  5, batch     5 | loss: 1.4868960
MemoryTrain:  epoch  5, batch     6 | loss: 1.4421370
MemoryTrain:  epoch  6, batch     0 | loss: 1.5811361
MemoryTrain:  epoch  6, batch     1 | loss: 1.3779110
MemoryTrain:  epoch  6, batch     2 | loss: 1.2918973
MemoryTrain:  epoch  6, batch     3 | loss: 1.9019612
MemoryTrain:  epoch  6, batch     4 | loss: 1.4684082
MemoryTrain:  epoch  6, batch     5 | loss: 1.3844165
MemoryTrain:  epoch  6, batch     6 | loss: 1.6570354
MemoryTrain:  epoch  7, batch     0 | loss: 1.4883044
MemoryTrain:  epoch  7, batch     1 | loss: 1.5203683
MemoryTrain:  epoch  7, batch     2 | loss: 1.5842738
MemoryTrain:  epoch  7, batch     3 | loss: 1.5342071
MemoryTrain:  epoch  7, batch     4 | loss: 1.3921943
MemoryTrain:  epoch  7, batch     5 | loss: 1.3562757
MemoryTrain:  epoch  7, batch     6 | loss: 1.3583245
MemoryTrain:  epoch  8, batch     0 | loss: 1.2894815
MemoryTrain:  epoch  8, batch     1 | loss: 1.4199270
MemoryTrain:  epoch  8, batch     2 | loss: 1.4047447
MemoryTrain:  epoch  8, batch     3 | loss: 1.4251595
MemoryTrain:  epoch  8, batch     4 | loss: 1.4704238
MemoryTrain:  epoch  8, batch     5 | loss: 1.3785852
MemoryTrain:  epoch  8, batch     6 | loss: 1.4424992
MemoryTrain:  epoch  9, batch     0 | loss: 1.2787859
MemoryTrain:  epoch  9, batch     1 | loss: 1.3355927
MemoryTrain:  epoch  9, batch     2 | loss: 1.3286011
MemoryTrain:  epoch  9, batch     3 | loss: 1.3519449
MemoryTrain:  epoch  9, batch     4 | loss: 1.5268772
MemoryTrain:  epoch  9, batch     5 | loss: 1.3393176
MemoryTrain:  epoch  9, batch     6 | loss: 1.6240175
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 53.12%   
[EVAL] batch:    2 | acc: 37.50%,  total acc: 47.92%   
[EVAL] batch:    3 | acc: 6.25%,  total acc: 37.50%   
[EVAL] batch:    4 | acc: 6.25%,  total acc: 31.25%   
[EVAL] batch:    5 | acc: 18.75%,  total acc: 29.17%   
[EVAL] batch:    6 | acc: 25.00%,  total acc: 28.57%   
[EVAL] batch:    7 | acc: 56.25%,  total acc: 32.03%   
[EVAL] batch:    8 | acc: 43.75%,  total acc: 33.33%   
[EVAL] batch:    9 | acc: 56.25%,  total acc: 35.62%   
[EVAL] batch:   10 | acc: 75.00%,  total acc: 39.20%   
[EVAL] batch:   11 | acc: 62.50%,  total acc: 41.15%   
[EVAL] batch:   12 | acc: 68.75%,  total acc: 43.27%   
[EVAL] batch:   13 | acc: 100.00%,  total acc: 47.32%   
[EVAL] batch:   14 | acc: 93.75%,  total acc: 50.42%   
[EVAL] batch:   15 | acc: 93.75%,  total acc: 53.12%   
[EVAL] batch:   16 | acc: 93.75%,  total acc: 55.51%   
[EVAL] batch:   17 | acc: 87.50%,  total acc: 57.29%   
[EVAL] batch:   18 | acc: 12.50%,  total acc: 54.93%   
[EVAL] batch:   19 | acc: 31.25%,  total acc: 53.75%   
[EVAL] batch:   20 | acc: 12.50%,  total acc: 51.79%   
[EVAL] batch:   21 | acc: 6.25%,  total acc: 49.72%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 68.75%   
[EVAL] batch:    2 | acc: 56.25%,  total acc: 64.58%   
[EVAL] batch:    3 | acc: 50.00%,  total acc: 60.94%   
[EVAL] batch:    4 | acc: 81.25%,  total acc: 65.00%   
[EVAL] batch:    5 | acc: 75.00%,  total acc: 66.67%   
[EVAL] batch:    6 | acc: 81.25%,  total acc: 68.75%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 72.66%   
[EVAL] batch:    8 | acc: 87.50%,  total acc: 74.31%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 75.62%   
[EVAL] batch:   10 | acc: 93.75%,  total acc: 77.27%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 78.12%   
[EVAL] batch:   12 | acc: 43.75%,  total acc: 75.48%   
[EVAL] batch:   13 | acc: 31.25%,  total acc: 72.32%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 72.50%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 71.48%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 71.69%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 71.18%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 70.72%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 71.56%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 72.92%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 74.15%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 75.27%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 76.04%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 77.00%   
[EVAL] batch:   25 | acc: 93.75%,  total acc: 77.64%   
[EVAL] batch:   26 | acc: 75.00%,  total acc: 77.55%   
[EVAL] batch:   27 | acc: 75.00%,  total acc: 77.46%   
[EVAL] batch:   28 | acc: 75.00%,  total acc: 77.37%   
[EVAL] batch:   29 | acc: 62.50%,  total acc: 76.88%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 77.02%   
[EVAL] batch:   31 | acc: 68.75%,  total acc: 76.76%   
[EVAL] batch:   32 | acc: 62.50%,  total acc: 76.33%   
[EVAL] batch:   33 | acc: 100.00%,  total acc: 77.02%   
[EVAL] batch:   34 | acc: 43.75%,  total acc: 76.07%   
[EVAL] batch:   35 | acc: 18.75%,  total acc: 74.48%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 72.47%   
[EVAL] batch:   37 | acc: 0.00%,  total acc: 70.56%   
[EVAL] batch:   38 | acc: 0.00%,  total acc: 68.75%   
[EVAL] batch:   39 | acc: 50.00%,  total acc: 68.28%   
[EVAL] batch:   40 | acc: 87.50%,  total acc: 68.75%   
[EVAL] batch:   41 | acc: 93.75%,  total acc: 69.35%   
[EVAL] batch:   42 | acc: 100.00%,  total acc: 70.06%   
[EVAL] batch:   43 | acc: 100.00%,  total acc: 70.74%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 71.39%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 72.01%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 72.61%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 73.18%   
[EVAL] batch:   48 | acc: 12.50%,  total acc: 71.94%   
[EVAL] batch:   49 | acc: 0.00%,  total acc: 70.50%   
[EVAL] batch:   50 | acc: 62.50%,  total acc: 70.34%   
[EVAL] batch:   51 | acc: 81.25%,  total acc: 70.55%   
[EVAL] batch:   52 | acc: 56.25%,  total acc: 70.28%   
[EVAL] batch:   53 | acc: 37.50%,  total acc: 69.68%   
[EVAL] batch:   54 | acc: 25.00%,  total acc: 68.86%   
[EVAL] batch:   55 | acc: 18.75%,  total acc: 67.97%   
[EVAL] batch:   56 | acc: 25.00%,  total acc: 67.21%   
[EVAL] batch:   57 | acc: 31.25%,  total acc: 66.59%   
[EVAL] batch:   58 | acc: 6.25%,  total acc: 65.57%   
[EVAL] batch:   59 | acc: 50.00%,  total acc: 65.31%   
[EVAL] batch:   60 | acc: 87.50%,  total acc: 65.68%   
[EVAL] batch:   61 | acc: 25.00%,  total acc: 65.02%   
[EVAL] batch:   62 | acc: 31.25%,  total acc: 64.48%   
[EVAL] batch:   63 | acc: 43.75%,  total acc: 64.16%   
[EVAL] batch:   64 | acc: 87.50%,  total acc: 64.52%   
[EVAL] batch:   65 | acc: 100.00%,  total acc: 65.06%   
[EVAL] batch:   66 | acc: 100.00%,  total acc: 65.58%   
[EVAL] batch:   67 | acc: 100.00%,  total acc: 66.08%   
[EVAL] batch:   68 | acc: 100.00%,  total acc: 66.58%   
[EVAL] batch:   69 | acc: 100.00%,  total acc: 67.05%   
[EVAL] batch:   70 | acc: 68.75%,  total acc: 67.08%   
[EVAL] batch:   71 | acc: 50.00%,  total acc: 66.84%   
[EVAL] batch:   72 | acc: 56.25%,  total acc: 66.70%   
[EVAL] batch:   73 | acc: 50.00%,  total acc: 66.47%   
[EVAL] batch:   74 | acc: 31.25%,  total acc: 66.00%   
[EVAL] batch:   75 | acc: 50.00%,  total acc: 65.79%   
[EVAL] batch:   76 | acc: 50.00%,  total acc: 65.58%   
[EVAL] batch:   77 | acc: 25.00%,  total acc: 65.06%   
[EVAL] batch:   78 | acc: 43.75%,  total acc: 64.79%   
[EVAL] batch:   79 | acc: 31.25%,  total acc: 64.38%   
[EVAL] batch:   80 | acc: 37.50%,  total acc: 64.04%   
[EVAL] batch:   81 | acc: 75.00%,  total acc: 64.18%   
[EVAL] batch:   82 | acc: 81.25%,  total acc: 64.38%   
[EVAL] batch:   83 | acc: 81.25%,  total acc: 64.58%   
[EVAL] batch:   84 | acc: 68.75%,  total acc: 64.63%   
[EVAL] batch:   85 | acc: 62.50%,  total acc: 64.61%   
[EVAL] batch:   86 | acc: 56.25%,  total acc: 64.51%   
[EVAL] batch:   87 | acc: 56.25%,  total acc: 64.42%   
[EVAL] batch:   88 | acc: 62.50%,  total acc: 64.40%   
[EVAL] batch:   89 | acc: 56.25%,  total acc: 64.31%   
[EVAL] batch:   90 | acc: 87.50%,  total acc: 64.56%   
[EVAL] batch:   91 | acc: 87.50%,  total acc: 64.81%   
[EVAL] batch:   92 | acc: 87.50%,  total acc: 65.05%   
[EVAL] batch:   93 | acc: 93.75%,  total acc: 65.36%   
[EVAL] batch:   94 | acc: 75.00%,  total acc: 65.46%   
[EVAL] batch:   95 | acc: 93.75%,  total acc: 65.76%   
[EVAL] batch:   96 | acc: 43.75%,  total acc: 65.53%   
[EVAL] batch:   97 | acc: 50.00%,  total acc: 65.37%   
[EVAL] batch:   98 | acc: 62.50%,  total acc: 65.34%   
[EVAL] batch:   99 | acc: 18.75%,  total acc: 64.88%   
[EVAL] batch:  100 | acc: 6.25%,  total acc: 64.29%   
[EVAL] batch:  101 | acc: 6.25%,  total acc: 63.73%   
[EVAL] batch:  102 | acc: 25.00%,  total acc: 63.35%   
[EVAL] batch:  103 | acc: 31.25%,  total acc: 63.04%   
[EVAL] batch:  104 | acc: 50.00%,  total acc: 62.92%   
[EVAL] batch:  105 | acc: 50.00%,  total acc: 62.79%   
[EVAL] batch:  106 | acc: 68.75%,  total acc: 62.85%   
[EVAL] batch:  107 | acc: 62.50%,  total acc: 62.85%   
[EVAL] batch:  108 | acc: 68.75%,  total acc: 62.90%   
[EVAL] batch:  109 | acc: 75.00%,  total acc: 63.01%   
[EVAL] batch:  110 | acc: 100.00%,  total acc: 63.34%   
[EVAL] batch:  111 | acc: 93.75%,  total acc: 63.62%   
[EVAL] batch:  112 | acc: 93.75%,  total acc: 63.88%   
[EVAL] batch:  113 | acc: 87.50%,  total acc: 64.09%   
[EVAL] batch:  114 | acc: 75.00%,  total acc: 64.18%   
[EVAL] batch:  115 | acc: 12.50%,  total acc: 63.74%   
[EVAL] batch:  116 | acc: 31.25%,  total acc: 63.46%   
[EVAL] batch:  117 | acc: 6.25%,  total acc: 62.98%   
[EVAL] batch:  118 | acc: 6.25%,  total acc: 62.50%   
cur_acc:  ['0.8561', '0.2344', '0.7768', '0.7951', '0.6683', '0.7902', '0.4972']
his_acc:  ['0.8561', '0.7078', '0.7199', '0.7535', '0.6830', '0.6772', '0.6250']
CurrentTrain: epoch  0, batch     0 | loss: 6.0452380
CurrentTrain: epoch  0, batch     1 | loss: 6.5433683
CurrentTrain: epoch  1, batch     0 | loss: 4.9426885
CurrentTrain: epoch  1, batch     1 | loss: 5.2900591
CurrentTrain: epoch  2, batch     0 | loss: 4.7419958
CurrentTrain: epoch  2, batch     1 | loss: 4.2794490
CurrentTrain: epoch  3, batch     0 | loss: 4.2410765
CurrentTrain: epoch  3, batch     1 | loss: 4.4706025
CurrentTrain: epoch  4, batch     0 | loss: 4.3810782
CurrentTrain: epoch  4, batch     1 | loss: 3.7319140
CurrentTrain: epoch  5, batch     0 | loss: 3.8680298
CurrentTrain: epoch  5, batch     1 | loss: 3.9452295
CurrentTrain: epoch  6, batch     0 | loss: 3.3210316
CurrentTrain: epoch  6, batch     1 | loss: 3.3793507
CurrentTrain: epoch  7, batch     0 | loss: 3.0883174
CurrentTrain: epoch  7, batch     1 | loss: 3.4578536
CurrentTrain: epoch  8, batch     0 | loss: 3.4951239
CurrentTrain: epoch  8, batch     1 | loss: 2.7287991
CurrentTrain: epoch  9, batch     0 | loss: 3.1194048
CurrentTrain: epoch  9, batch     1 | loss: 2.8847597
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: san jose , ca , usa speaking of k-fed , him and ex-wife britney spears are in court today , dealing with their custody battle .
Head Entity: britney spears
Tail Entity: ca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving to new york city, jessica found a vibrant community and a bustling lifestyle that she had always dreamed of.  
Head Entity: jessica  
Tail Entity: new york city  

Relation: person stateorprovinces of residence  
Context: During his time in toronto, the artist developed a deep appreciation for the local music scene and its diverse culture.  
Head Entity: the artist  
Tail Entity: toronto  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: former gov. robert e. mcnair of south carolina , the political moderate who was a finalist to become vice president hubert h. humphrey 's running mate in 1968 but whose promising career was cut short by what became known as the orangeburg massacre , died on nov. 17 in charleston .
Head Entity: robert e. mcnair
Tail Entity: nov. 17
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: renowned physicist stephen hawking, who made groundbreaking contributions to our understanding of black holes and the universe, passed away peacefully at his home in cambridge on march 14, 2018.  
Head Entity: stephen hawking  
Tail Entity: march 14, 2018  

Relation: person date of death  
Context: the beloved author of the harry potter series, j.k. rowling, announced the passing of her dear friend and mentor, who died on january 1, 2020, after a long battle with illness.  
Head Entity: j.k. rowling's dear friend  
Tail Entity: january 1, 2020  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, boasts a workforce of approximately 5,500 skilled professionals across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: GreenEarth Nonprofit has grown significantly over the past few years and currently employs around 1,200 dedicated staff members to support its environmental initiatives.  
Head Entity: GreenEarth Nonprofit  
Tail Entity: 1,200  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: the famous author mark twain, whose birth name was samuel clemens, is known for his classic novels like "adventures of huckleberry finn."  
Head Entity: samuel clemens  
Tail Entity: mark twain  

Relation: person alternate names  
Context: the musician known as lady gaga was born as stefani germanotta and has become a global pop icon.  
Head Entity: stefani germanotta  
Tail Entity: lady gaga  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: smits stands at the center of this multigenerational saga as alex vega , the adopted son of rum and sugar baron pancho duque -lrb- elizondo -rrb- and his wife , amalia -lrb- moreno -rrb- .
Head Entity: elizondo
Tail Entity: moreno
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a heartfelt ceremony, john and his beloved partner, sarah, exchanged vows surrounded by family and friends, celebrating their love and commitment to each other.  
Head Entity: john  
Tail Entity: sarah  

Relation: person spouse  
Context: after years of companionship, the couple, michael and jessica, finally tied the knot in a beautiful garden wedding, marking the beginning of their new life together.  
Head Entity: michael  
Tail Entity: jessica  
Mixup data size:  285
MixupTrain:  epoch  0, batch     0 | loss: 1.7774105
MixupTrain:  epoch  0, batch     1 | loss: 2.3156259
MixupTrain:  epoch  0, batch     2 | loss: 2.1337593
MixupTrain:  epoch  0, batch     3 | loss: 2.0555315
MixupTrain:  epoch  0, batch     4 | loss: 2.2956107
MixupTrain:  epoch  0, batch     5 | loss: 2.6639628
MixupTrain:  epoch  0, batch     6 | loss: 2.0444345
MixupTrain:  epoch  0, batch     7 | loss: 2.1258373
MixupTrain:  epoch  0, batch     8 | loss: 2.0465400
MixupTrain:  epoch  0, batch     9 | loss: 1.8680475
MixupTrain:  epoch  0, batch    10 | loss: 2.5956964
MixupTrain:  epoch  0, batch    11 | loss: 2.0967083
MixupTrain:  epoch  0, batch    12 | loss: 2.0821087
MixupTrain:  epoch  0, batch    13 | loss: 1.9719938
MixupTrain:  epoch  0, batch    14 | loss: 1.9854493
MixupTrain:  epoch  0, batch    15 | loss: 2.1400461
MixupTrain:  epoch  0, batch    16 | loss: 2.2998753
MixupTrain:  epoch  0, batch    17 | loss: 2.1497047
MemoryTrain:  epoch  0, batch     0 | loss: 1.7957019
MemoryTrain:  epoch  0, batch     1 | loss: 1.7536978
MemoryTrain:  epoch  0, batch     2 | loss: 1.7484932
MemoryTrain:  epoch  0, batch     3 | loss: 2.1471906
MemoryTrain:  epoch  0, batch     4 | loss: 2.2048042
MemoryTrain:  epoch  0, batch     5 | loss: 1.9095328
MemoryTrain:  epoch  0, batch     6 | loss: 2.2369685
MemoryTrain:  epoch  0, batch     7 | loss: 2.7995751
MemoryTrain:  epoch  1, batch     0 | loss: 1.9137709
MemoryTrain:  epoch  1, batch     1 | loss: 1.7140818
MemoryTrain:  epoch  1, batch     2 | loss: 2.1456733
MemoryTrain:  epoch  1, batch     3 | loss: 2.1148705
MemoryTrain:  epoch  1, batch     4 | loss: 2.0047674
MemoryTrain:  epoch  1, batch     5 | loss: 1.9512856
MemoryTrain:  epoch  1, batch     6 | loss: 1.9215202
MemoryTrain:  epoch  1, batch     7 | loss: 1.4543277
MemoryTrain:  epoch  2, batch     0 | loss: 1.8089664
MemoryTrain:  epoch  2, batch     1 | loss: 1.8182566
MemoryTrain:  epoch  2, batch     2 | loss: 1.4294184
MemoryTrain:  epoch  2, batch     3 | loss: 1.8952261
MemoryTrain:  epoch  2, batch     4 | loss: 1.6012204
MemoryTrain:  epoch  2, batch     5 | loss: 1.8988135
MemoryTrain:  epoch  2, batch     6 | loss: 1.6463860
MemoryTrain:  epoch  2, batch     7 | loss: 1.7045462
MemoryTrain:  epoch  3, batch     0 | loss: 1.6402516
MemoryTrain:  epoch  3, batch     1 | loss: 1.6120869
MemoryTrain:  epoch  3, batch     2 | loss: 1.7193063
MemoryTrain:  epoch  3, batch     3 | loss: 1.9199569
MemoryTrain:  epoch  3, batch     4 | loss: 1.4961450
MemoryTrain:  epoch  3, batch     5 | loss: 1.3657831
MemoryTrain:  epoch  3, batch     6 | loss: 1.5033343
MemoryTrain:  epoch  3, batch     7 | loss: 1.5028839
MemoryTrain:  epoch  4, batch     0 | loss: 1.6947510
MemoryTrain:  epoch  4, batch     1 | loss: 1.6310779
MemoryTrain:  epoch  4, batch     2 | loss: 1.5434424
MemoryTrain:  epoch  4, batch     3 | loss: 1.5688593
MemoryTrain:  epoch  4, batch     4 | loss: 1.5877179
MemoryTrain:  epoch  4, batch     5 | loss: 1.2943496
MemoryTrain:  epoch  4, batch     6 | loss: 1.5611895
MemoryTrain:  epoch  4, batch     7 | loss: 1.3435796
MemoryTrain:  epoch  5, batch     0 | loss: 1.4073033
MemoryTrain:  epoch  5, batch     1 | loss: 1.3965766
MemoryTrain:  epoch  5, batch     2 | loss: 1.3718845
MemoryTrain:  epoch  5, batch     3 | loss: 1.5373628
MemoryTrain:  epoch  5, batch     4 | loss: 2.0465941
MemoryTrain:  epoch  5, batch     5 | loss: 1.3355637
MemoryTrain:  epoch  5, batch     6 | loss: 1.2988456
MemoryTrain:  epoch  5, batch     7 | loss: 1.4885646
MemoryTrain:  epoch  6, batch     0 | loss: 1.3654284
MemoryTrain:  epoch  6, batch     1 | loss: 1.3767214
MemoryTrain:  epoch  6, batch     2 | loss: 1.3527927
MemoryTrain:  epoch  6, batch     3 | loss: 1.5634091
MemoryTrain:  epoch  6, batch     4 | loss: 1.4352915
MemoryTrain:  epoch  6, batch     5 | loss: 1.6453388
MemoryTrain:  epoch  6, batch     6 | loss: 1.5987190
MemoryTrain:  epoch  6, batch     7 | loss: 1.8397974
MemoryTrain:  epoch  7, batch     0 | loss: 1.3169140
MemoryTrain:  epoch  7, batch     1 | loss: 1.6530967
MemoryTrain:  epoch  7, batch     2 | loss: 1.5101783
MemoryTrain:  epoch  7, batch     3 | loss: 1.2639363
MemoryTrain:  epoch  7, batch     4 | loss: 1.4739742
MemoryTrain:  epoch  7, batch     5 | loss: 1.4212277
MemoryTrain:  epoch  7, batch     6 | loss: 1.4991682
MemoryTrain:  epoch  7, batch     7 | loss: 1.2256694
MemoryTrain:  epoch  8, batch     0 | loss: 1.3720069
MemoryTrain:  epoch  8, batch     1 | loss: 1.2866993
MemoryTrain:  epoch  8, batch     2 | loss: 1.3648854
MemoryTrain:  epoch  8, batch     3 | loss: 1.5775311
MemoryTrain:  epoch  8, batch     4 | loss: 1.3721445
MemoryTrain:  epoch  8, batch     5 | loss: 1.4325492
MemoryTrain:  epoch  8, batch     6 | loss: 1.3490397
MemoryTrain:  epoch  8, batch     7 | loss: 1.3678336
MemoryTrain:  epoch  9, batch     0 | loss: 1.2343378
MemoryTrain:  epoch  9, batch     1 | loss: 1.3277384
MemoryTrain:  epoch  9, batch     2 | loss: 1.3711245
MemoryTrain:  epoch  9, batch     3 | loss: 1.3448465
MemoryTrain:  epoch  9, batch     4 | loss: 1.4900494
MemoryTrain:  epoch  9, batch     5 | loss: 1.3169211
MemoryTrain:  epoch  9, batch     6 | loss: 1.2247994
MemoryTrain:  epoch  9, batch     7 | loss: 1.4606619
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   
[EVAL] batch:    2 | acc: 31.25%,  total acc: 54.17%   
[EVAL] batch:    3 | acc: 68.75%,  total acc: 57.81%   
[EVAL] batch:    4 | acc: 62.50%,  total acc: 58.75%   
[EVAL] batch:    5 | acc: 81.25%,  total acc: 62.50%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 66.96%   
[EVAL] batch:    7 | acc: 93.75%,  total acc: 70.31%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 72.92%   
[EVAL] batch:    9 | acc: 75.00%,  total acc: 73.12%   
[EVAL] batch:   10 | acc: 56.25%,  total acc: 71.59%   
[EVAL] batch:   11 | acc: 81.25%,  total acc: 72.40%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 74.04%   
[EVAL] batch:   13 | acc: 75.00%,  total acc: 74.11%   
[EVAL] batch:   14 | acc: 37.50%,  total acc: 71.67%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    1 | acc: 62.50%,  total acc: 71.88%   
[EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   
[EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   
[EVAL] batch:    4 | acc: 75.00%,  total acc: 72.50%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 71.88%   
[EVAL] batch:    6 | acc: 81.25%,  total acc: 73.21%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 78.47%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 80.00%   
[EVAL] batch:   10 | acc: 93.75%,  total acc: 81.25%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 81.77%   
[EVAL] batch:   12 | acc: 50.00%,  total acc: 79.33%   
[EVAL] batch:   13 | acc: 31.25%,  total acc: 75.89%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 75.83%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 74.61%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 74.63%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 73.96%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 73.36%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 73.75%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 75.00%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 75.85%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 76.90%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 77.60%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 78.50%   
[EVAL] batch:   25 | acc: 93.75%,  total acc: 79.09%   
[EVAL] batch:   26 | acc: 81.25%,  total acc: 79.17%   
[EVAL] batch:   27 | acc: 81.25%,  total acc: 79.24%   
[EVAL] batch:   28 | acc: 75.00%,  total acc: 79.09%   
[EVAL] batch:   29 | acc: 68.75%,  total acc: 78.75%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 78.83%   
[EVAL] batch:   31 | acc: 75.00%,  total acc: 78.71%   
[EVAL] batch:   32 | acc: 50.00%,  total acc: 77.84%   
[EVAL] batch:   33 | acc: 100.00%,  total acc: 78.49%   
[EVAL] batch:   34 | acc: 43.75%,  total acc: 77.50%   
[EVAL] batch:   35 | acc: 18.75%,  total acc: 75.87%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 73.82%   
[EVAL] batch:   37 | acc: 0.00%,  total acc: 71.88%   
[EVAL] batch:   38 | acc: 0.00%,  total acc: 70.03%   
[EVAL] batch:   39 | acc: 50.00%,  total acc: 69.53%   
[EVAL] batch:   40 | acc: 81.25%,  total acc: 69.82%   
[EVAL] batch:   41 | acc: 100.00%,  total acc: 70.54%   
[EVAL] batch:   42 | acc: 100.00%,  total acc: 71.22%   
[EVAL] batch:   43 | acc: 100.00%,  total acc: 71.88%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 72.50%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 73.10%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 73.67%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 74.22%   
[EVAL] batch:   48 | acc: 12.50%,  total acc: 72.96%   
[EVAL] batch:   49 | acc: 0.00%,  total acc: 71.50%   
[EVAL] batch:   50 | acc: 62.50%,  total acc: 71.32%   
[EVAL] batch:   51 | acc: 87.50%,  total acc: 71.63%   
[EVAL] batch:   52 | acc: 56.25%,  total acc: 71.34%   
[EVAL] batch:   53 | acc: 18.75%,  total acc: 70.37%   
[EVAL] batch:   54 | acc: 0.00%,  total acc: 69.09%   
[EVAL] batch:   55 | acc: 0.00%,  total acc: 67.86%   
[EVAL] batch:   56 | acc: 6.25%,  total acc: 66.78%   
[EVAL] batch:   57 | acc: 12.50%,  total acc: 65.84%   
[EVAL] batch:   58 | acc: 6.25%,  total acc: 64.83%   
[EVAL] batch:   59 | acc: 43.75%,  total acc: 64.48%   
[EVAL] batch:   60 | acc: 75.00%,  total acc: 64.65%   
[EVAL] batch:   61 | acc: 31.25%,  total acc: 64.11%   
[EVAL] batch:   62 | acc: 43.75%,  total acc: 63.79%   
[EVAL] batch:   63 | acc: 43.75%,  total acc: 63.48%   
[EVAL] batch:   64 | acc: 87.50%,  total acc: 63.85%   
[EVAL] batch:   65 | acc: 100.00%,  total acc: 64.39%   
[EVAL] batch:   66 | acc: 100.00%,  total acc: 64.93%   
[EVAL] batch:   67 | acc: 100.00%,  total acc: 65.44%   
[EVAL] batch:   68 | acc: 100.00%,  total acc: 65.94%   
[EVAL] batch:   69 | acc: 100.00%,  total acc: 66.43%   
[EVAL] batch:   70 | acc: 68.75%,  total acc: 66.46%   
[EVAL] batch:   71 | acc: 50.00%,  total acc: 66.23%   
[EVAL] batch:   72 | acc: 68.75%,  total acc: 66.27%   
[EVAL] batch:   73 | acc: 50.00%,  total acc: 66.05%   
[EVAL] batch:   74 | acc: 43.75%,  total acc: 65.75%   
[EVAL] batch:   75 | acc: 50.00%,  total acc: 65.54%   
[EVAL] batch:   76 | acc: 62.50%,  total acc: 65.50%   
[EVAL] batch:   77 | acc: 37.50%,  total acc: 65.14%   
[EVAL] batch:   78 | acc: 18.75%,  total acc: 64.56%   
[EVAL] batch:   79 | acc: 12.50%,  total acc: 63.91%   
[EVAL] batch:   80 | acc: 31.25%,  total acc: 63.50%   
[EVAL] batch:   81 | acc: 75.00%,  total acc: 63.64%   
[EVAL] batch:   82 | acc: 68.75%,  total acc: 63.70%   
[EVAL] batch:   83 | acc: 75.00%,  total acc: 63.84%   
[EVAL] batch:   84 | acc: 56.25%,  total acc: 63.75%   
[EVAL] batch:   85 | acc: 50.00%,  total acc: 63.59%   
[EVAL] batch:   86 | acc: 37.50%,  total acc: 63.29%   
[EVAL] batch:   87 | acc: 43.75%,  total acc: 63.07%   
[EVAL] batch:   88 | acc: 62.50%,  total acc: 63.06%   
[EVAL] batch:   89 | acc: 62.50%,  total acc: 63.06%   
[EVAL] batch:   90 | acc: 87.50%,  total acc: 63.32%   
[EVAL] batch:   91 | acc: 93.75%,  total acc: 63.65%   
[EVAL] batch:   92 | acc: 81.25%,  total acc: 63.84%   
[EVAL] batch:   93 | acc: 87.50%,  total acc: 64.10%   
[EVAL] batch:   94 | acc: 75.00%,  total acc: 64.21%   
[EVAL] batch:   95 | acc: 93.75%,  total acc: 64.52%   
[EVAL] batch:   96 | acc: 50.00%,  total acc: 64.37%   
[EVAL] batch:   97 | acc: 50.00%,  total acc: 64.22%   
[EVAL] batch:   98 | acc: 62.50%,  total acc: 64.20%   
[EVAL] batch:   99 | acc: 18.75%,  total acc: 63.75%   
[EVAL] batch:  100 | acc: 6.25%,  total acc: 63.18%   
[EVAL] batch:  101 | acc: 12.50%,  total acc: 62.68%   
[EVAL] batch:  102 | acc: 31.25%,  total acc: 62.38%   
[EVAL] batch:  103 | acc: 25.00%,  total acc: 62.02%   
[EVAL] batch:  104 | acc: 50.00%,  total acc: 61.90%   
[EVAL] batch:  105 | acc: 43.75%,  total acc: 61.73%   
[EVAL] batch:  106 | acc: 68.75%,  total acc: 61.80%   
[EVAL] batch:  107 | acc: 68.75%,  total acc: 61.86%   
[EVAL] batch:  108 | acc: 68.75%,  total acc: 61.93%   
[EVAL] batch:  109 | acc: 68.75%,  total acc: 61.99%   
[EVAL] batch:  110 | acc: 100.00%,  total acc: 62.33%   
[EVAL] batch:  111 | acc: 93.75%,  total acc: 62.61%   
[EVAL] batch:  112 | acc: 93.75%,  total acc: 62.89%   
[EVAL] batch:  113 | acc: 87.50%,  total acc: 63.10%   
[EVAL] batch:  114 | acc: 68.75%,  total acc: 63.15%   
[EVAL] batch:  115 | acc: 0.00%,  total acc: 62.61%   
[EVAL] batch:  116 | acc: 12.50%,  total acc: 62.18%   
[EVAL] batch:  117 | acc: 6.25%,  total acc: 61.71%   
[EVAL] batch:  118 | acc: 50.00%,  total acc: 61.61%   
[EVAL] batch:  119 | acc: 68.75%,  total acc: 61.67%   
[EVAL] batch:  120 | acc: 31.25%,  total acc: 61.42%   
[EVAL] batch:  121 | acc: 68.75%,  total acc: 61.48%   
[EVAL] batch:  122 | acc: 62.50%,  total acc: 61.48%   
[EVAL] batch:  123 | acc: 81.25%,  total acc: 61.64%   
[EVAL] batch:  124 | acc: 87.50%,  total acc: 61.85%   
[EVAL] batch:  125 | acc: 100.00%,  total acc: 62.15%   
[EVAL] batch:  126 | acc: 87.50%,  total acc: 62.35%   
[EVAL] batch:  127 | acc: 87.50%,  total acc: 62.55%   
[EVAL] batch:  128 | acc: 56.25%,  total acc: 62.50%   
[EVAL] batch:  129 | acc: 68.75%,  total acc: 62.55%   
[EVAL] batch:  130 | acc: 93.75%,  total acc: 62.79%   
[EVAL] batch:  131 | acc: 81.25%,  total acc: 62.93%   
[EVAL] batch:  132 | acc: 50.00%,  total acc: 62.83%   
cur_acc:  ['0.8561', '0.2344', '0.7768', '0.7951', '0.6683', '0.7902', '0.4972', '0.7167']
his_acc:  ['0.8561', '0.7078', '0.7199', '0.7535', '0.6830', '0.6772', '0.6250', '0.6283']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.4756050
CurrentTrain: epoch  0, batch     1 | loss: 13.0811615
CurrentTrain: epoch  0, batch     2 | loss: 12.9435682
CurrentTrain: epoch  0, batch     3 | loss: 13.1166382
CurrentTrain: epoch  0, batch     4 | loss: 12.7254381
CurrentTrain: epoch  0, batch     5 | loss: 12.8034115
CurrentTrain: epoch  0, batch     6 | loss: 12.6345177
CurrentTrain: epoch  0, batch     7 | loss: 12.5523195
CurrentTrain: epoch  0, batch     8 | loss: 12.2904053
CurrentTrain: epoch  0, batch     9 | loss: 12.2931271
CurrentTrain: epoch  0, batch    10 | loss: 12.0136700
CurrentTrain: epoch  0, batch    11 | loss: 12.0691833
CurrentTrain: epoch  0, batch    12 | loss: 12.0353422
CurrentTrain: epoch  0, batch    13 | loss: 11.7885141
CurrentTrain: epoch  0, batch    14 | loss: 11.8425026
CurrentTrain: epoch  0, batch    15 | loss: 11.7077065
CurrentTrain: epoch  0, batch    16 | loss: 11.3753672
CurrentTrain: epoch  0, batch    17 | loss: 11.4376907
CurrentTrain: epoch  0, batch    18 | loss: 11.4358339
CurrentTrain: epoch  0, batch    19 | loss: 11.2192421
CurrentTrain: epoch  0, batch    20 | loss: 11.3258095
CurrentTrain: epoch  0, batch    21 | loss: 11.4201736
CurrentTrain: epoch  0, batch    22 | loss: 10.9559755
CurrentTrain: epoch  0, batch    23 | loss: 11.4748325
CurrentTrain: epoch  0, batch    24 | loss: 11.4106827
CurrentTrain: epoch  0, batch    25 | loss: 10.7229538
CurrentTrain: epoch  0, batch    26 | loss: 11.1634312
CurrentTrain: epoch  0, batch    27 | loss: 10.7081280
CurrentTrain: epoch  0, batch    28 | loss: 10.6817207
CurrentTrain: epoch  0, batch    29 | loss: 10.6728411
CurrentTrain: epoch  0, batch    30 | loss: 10.8271351
CurrentTrain: epoch  0, batch    31 | loss: 10.9551735
CurrentTrain: epoch  0, batch    32 | loss: 10.4183340
CurrentTrain: epoch  0, batch    33 | loss: 10.8339682
CurrentTrain: epoch  0, batch    34 | loss: 10.1591053
CurrentTrain: epoch  0, batch    35 | loss: 10.4512768
CurrentTrain: epoch  0, batch    36 | loss: 10.4571114
CurrentTrain: epoch  0, batch    37 | loss: 9.9719028
CurrentTrain: epoch  1, batch     0 | loss: 10.7797375
CurrentTrain: epoch  1, batch     1 | loss: 10.1315584
CurrentTrain: epoch  1, batch     2 | loss: 10.0642776
CurrentTrain: epoch  1, batch     3 | loss: 9.5569506
CurrentTrain: epoch  1, batch     4 | loss: 9.6937389
CurrentTrain: epoch  1, batch     5 | loss: 9.9531879
CurrentTrain: epoch  1, batch     6 | loss: 10.2085972
CurrentTrain: epoch  1, batch     7 | loss: 9.8578348
CurrentTrain: epoch  1, batch     8 | loss: 9.3402090
CurrentTrain: epoch  1, batch     9 | loss: 10.4368620
CurrentTrain: epoch  1, batch    10 | loss: 10.4080038
CurrentTrain: epoch  1, batch    11 | loss: 9.9783478
CurrentTrain: epoch  1, batch    12 | loss: 9.7259579
CurrentTrain: epoch  1, batch    13 | loss: 9.7514362
CurrentTrain: epoch  1, batch    14 | loss: 9.1660042
CurrentTrain: epoch  1, batch    15 | loss: 9.4397373
CurrentTrain: epoch  1, batch    16 | loss: 9.2175379
CurrentTrain: epoch  1, batch    17 | loss: 9.1193733
CurrentTrain: epoch  1, batch    18 | loss: 9.5950727
CurrentTrain: epoch  1, batch    19 | loss: 9.2863941
CurrentTrain: epoch  1, batch    20 | loss: 9.4383030
CurrentTrain: epoch  1, batch    21 | loss: 9.4017801
CurrentTrain: epoch  1, batch    22 | loss: 9.2806339
CurrentTrain: epoch  1, batch    23 | loss: 9.2602530
CurrentTrain: epoch  1, batch    24 | loss: 9.5255585
CurrentTrain: epoch  1, batch    25 | loss: 9.3150873
CurrentTrain: epoch  1, batch    26 | loss: 8.4883881
CurrentTrain: epoch  1, batch    27 | loss: 9.5812063
CurrentTrain: epoch  1, batch    28 | loss: 9.0412884
CurrentTrain: epoch  1, batch    29 | loss: 8.2596893
CurrentTrain: epoch  1, batch    30 | loss: 9.7451382
CurrentTrain: epoch  1, batch    31 | loss: 8.8989954
CurrentTrain: epoch  1, batch    32 | loss: 9.6154528
CurrentTrain: epoch  1, batch    33 | loss: 9.4476528
CurrentTrain: epoch  1, batch    34 | loss: 8.2047052
CurrentTrain: epoch  1, batch    35 | loss: 8.2110767
CurrentTrain: epoch  1, batch    36 | loss: 9.2287531
CurrentTrain: epoch  1, batch    37 | loss: 8.5677061
CurrentTrain: epoch  2, batch     0 | loss: 8.3025808
CurrentTrain: epoch  2, batch     1 | loss: 8.5099049
CurrentTrain: epoch  2, batch     2 | loss: 8.1640720
CurrentTrain: epoch  2, batch     3 | loss: 8.2838383
CurrentTrain: epoch  2, batch     4 | loss: 8.3215418
CurrentTrain: epoch  2, batch     5 | loss: 8.5444250
CurrentTrain: epoch  2, batch     6 | loss: 9.1453638
CurrentTrain: epoch  2, batch     7 | loss: 8.5073223
CurrentTrain: epoch  2, batch     8 | loss: 8.6205664
CurrentTrain: epoch  2, batch     9 | loss: 8.1095314
CurrentTrain: epoch  2, batch    10 | loss: 8.7559605
CurrentTrain: epoch  2, batch    11 | loss: 8.8582840
CurrentTrain: epoch  2, batch    12 | loss: 7.8623004
CurrentTrain: epoch  2, batch    13 | loss: 7.8884811
CurrentTrain: epoch  2, batch    14 | loss: 8.8367729
CurrentTrain: epoch  2, batch    15 | loss: 8.1404133
CurrentTrain: epoch  2, batch    16 | loss: 8.7820377
CurrentTrain: epoch  2, batch    17 | loss: 8.2185020
CurrentTrain: epoch  2, batch    18 | loss: 8.3752327
CurrentTrain: epoch  2, batch    19 | loss: 8.0888977
CurrentTrain: epoch  2, batch    20 | loss: 7.8508205
CurrentTrain: epoch  2, batch    21 | loss: 7.6329632
CurrentTrain: epoch  2, batch    22 | loss: 7.7031765
CurrentTrain: epoch  2, batch    23 | loss: 7.9022770
CurrentTrain: epoch  2, batch    24 | loss: 7.3285003
CurrentTrain: epoch  2, batch    25 | loss: 7.9274416
CurrentTrain: epoch  2, batch    26 | loss: 8.0991154
CurrentTrain: epoch  2, batch    27 | loss: 7.3283539
CurrentTrain: epoch  2, batch    28 | loss: 7.5157666
CurrentTrain: epoch  2, batch    29 | loss: 8.6132088
CurrentTrain: epoch  2, batch    30 | loss: 8.6146545
CurrentTrain: epoch  2, batch    31 | loss: 7.2494111
CurrentTrain: epoch  2, batch    32 | loss: 7.5232906
CurrentTrain: epoch  2, batch    33 | loss: 8.6081553
CurrentTrain: epoch  2, batch    34 | loss: 8.8744450
CurrentTrain: epoch  2, batch    35 | loss: 7.4935722
CurrentTrain: epoch  2, batch    36 | loss: 7.7785072
CurrentTrain: epoch  2, batch    37 | loss: 7.6879802
CurrentTrain: epoch  3, batch     0 | loss: 7.5815668
CurrentTrain: epoch  3, batch     1 | loss: 7.6293969
CurrentTrain: epoch  3, batch     2 | loss: 8.3429480
CurrentTrain: epoch  3, batch     3 | loss: 8.0453434
CurrentTrain: epoch  3, batch     4 | loss: 6.9750848
CurrentTrain: epoch  3, batch     5 | loss: 8.2069769
CurrentTrain: epoch  3, batch     6 | loss: 8.0829229
CurrentTrain: epoch  3, batch     7 | loss: 7.8627124
CurrentTrain: epoch  3, batch     8 | loss: 6.9724441
CurrentTrain: epoch  3, batch     9 | loss: 7.6308746
CurrentTrain: epoch  3, batch    10 | loss: 7.9836912
CurrentTrain: epoch  3, batch    11 | loss: 7.3925591
CurrentTrain: epoch  3, batch    12 | loss: 7.3967738
CurrentTrain: epoch  3, batch    13 | loss: 7.7690206
CurrentTrain: epoch  3, batch    14 | loss: 7.0785289
CurrentTrain: epoch  3, batch    15 | loss: 7.5887394
CurrentTrain: epoch  3, batch    16 | loss: 7.0937605
CurrentTrain: epoch  3, batch    17 | loss: 6.8400426
CurrentTrain: epoch  3, batch    18 | loss: 7.2145295
CurrentTrain: epoch  3, batch    19 | loss: 8.0140877
CurrentTrain: epoch  3, batch    20 | loss: 7.2387152
CurrentTrain: epoch  3, batch    21 | loss: 7.4540920
CurrentTrain: epoch  3, batch    22 | loss: 6.1445632
CurrentTrain: epoch  3, batch    23 | loss: 8.2412729
CurrentTrain: epoch  3, batch    24 | loss: 7.6361909
CurrentTrain: epoch  3, batch    25 | loss: 7.3465743
CurrentTrain: epoch  3, batch    26 | loss: 7.2515798
CurrentTrain: epoch  3, batch    27 | loss: 6.7631636
CurrentTrain: epoch  3, batch    28 | loss: 7.4680243
CurrentTrain: epoch  3, batch    29 | loss: 7.1256323
CurrentTrain: epoch  3, batch    30 | loss: 7.0731678
CurrentTrain: epoch  3, batch    31 | loss: 7.3250680
CurrentTrain: epoch  3, batch    32 | loss: 7.0160718
CurrentTrain: epoch  3, batch    33 | loss: 7.5256243
CurrentTrain: epoch  3, batch    34 | loss: 7.1767569
CurrentTrain: epoch  3, batch    35 | loss: 6.5874281
CurrentTrain: epoch  3, batch    36 | loss: 7.0900354
CurrentTrain: epoch  3, batch    37 | loss: 7.6958055
CurrentTrain: epoch  4, batch     0 | loss: 7.0886517
CurrentTrain: epoch  4, batch     1 | loss: 6.9658604
CurrentTrain: epoch  4, batch     2 | loss: 6.6285524
CurrentTrain: epoch  4, batch     3 | loss: 6.4042549
CurrentTrain: epoch  4, batch     4 | loss: 7.1690149
CurrentTrain: epoch  4, batch     5 | loss: 7.1405058
CurrentTrain: epoch  4, batch     6 | loss: 8.6165104
CurrentTrain: epoch  4, batch     7 | loss: 7.2977657
CurrentTrain: epoch  4, batch     8 | loss: 7.7315531
CurrentTrain: epoch  4, batch     9 | loss: 7.3508034
CurrentTrain: epoch  4, batch    10 | loss: 6.6603632
CurrentTrain: epoch  4, batch    11 | loss: 6.4009361
CurrentTrain: epoch  4, batch    12 | loss: 7.1436887
CurrentTrain: epoch  4, batch    13 | loss: 6.7972641
CurrentTrain: epoch  4, batch    14 | loss: 6.1183467
CurrentTrain: epoch  4, batch    15 | loss: 7.4951172
CurrentTrain: epoch  4, batch    16 | loss: 7.5919523
CurrentTrain: epoch  4, batch    17 | loss: 6.4027185
CurrentTrain: epoch  4, batch    18 | loss: 6.4975262
CurrentTrain: epoch  4, batch    19 | loss: 6.4268942
CurrentTrain: epoch  4, batch    20 | loss: 6.2240667
CurrentTrain: epoch  4, batch    21 | loss: 6.8147202
CurrentTrain: epoch  4, batch    22 | loss: 6.8023434
CurrentTrain: epoch  4, batch    23 | loss: 7.1468258
CurrentTrain: epoch  4, batch    24 | loss: 6.5945101
CurrentTrain: epoch  4, batch    25 | loss: 6.5644140
CurrentTrain: epoch  4, batch    26 | loss: 6.7509375
CurrentTrain: epoch  4, batch    27 | loss: 6.8858786
CurrentTrain: epoch  4, batch    28 | loss: 6.2322865
CurrentTrain: epoch  4, batch    29 | loss: 7.2032652
CurrentTrain: epoch  4, batch    30 | loss: 6.7341518
CurrentTrain: epoch  4, batch    31 | loss: 7.1666231
CurrentTrain: epoch  4, batch    32 | loss: 6.5254989
CurrentTrain: epoch  4, batch    33 | loss: 7.2860699
CurrentTrain: epoch  4, batch    34 | loss: 5.9021077
CurrentTrain: epoch  4, batch    35 | loss: 7.2844567
CurrentTrain: epoch  4, batch    36 | loss: 6.2802596
CurrentTrain: epoch  4, batch    37 | loss: 7.0677071
CurrentTrain: epoch  5, batch     0 | loss: 6.6374688
CurrentTrain: epoch  5, batch     1 | loss: 6.8635387
CurrentTrain: epoch  5, batch     2 | loss: 6.4051061
CurrentTrain: epoch  5, batch     3 | loss: 5.8336926
CurrentTrain: epoch  5, batch     4 | loss: 6.9355569
CurrentTrain: epoch  5, batch     5 | loss: 6.9301090
CurrentTrain: epoch  5, batch     6 | loss: 6.8721447
CurrentTrain: epoch  5, batch     7 | loss: 7.3851004
CurrentTrain: epoch  5, batch     8 | loss: 6.9940510
CurrentTrain: epoch  5, batch     9 | loss: 6.3613358
CurrentTrain: epoch  5, batch    10 | loss: 6.2736955
CurrentTrain: epoch  5, batch    11 | loss: 6.0163307
CurrentTrain: epoch  5, batch    12 | loss: 6.4903574
CurrentTrain: epoch  5, batch    13 | loss: 6.4479551
CurrentTrain: epoch  5, batch    14 | loss: 6.4662066
CurrentTrain: epoch  5, batch    15 | loss: 6.6475110
CurrentTrain: epoch  5, batch    16 | loss: 6.1755838
CurrentTrain: epoch  5, batch    17 | loss: 5.9160314
CurrentTrain: epoch  5, batch    18 | loss: 6.3979263
CurrentTrain: epoch  5, batch    19 | loss: 6.1224489
CurrentTrain: epoch  5, batch    20 | loss: 6.7941284
CurrentTrain: epoch  5, batch    21 | loss: 6.3491707
CurrentTrain: epoch  5, batch    22 | loss: 5.3216100
CurrentTrain: epoch  5, batch    23 | loss: 6.8731298
CurrentTrain: epoch  5, batch    24 | loss: 6.0652032
CurrentTrain: epoch  5, batch    25 | loss: 6.0380173
CurrentTrain: epoch  5, batch    26 | loss: 6.3322988
CurrentTrain: epoch  5, batch    27 | loss: 7.3397551
CurrentTrain: epoch  5, batch    28 | loss: 7.5881824
CurrentTrain: epoch  5, batch    29 | loss: 6.3869991
CurrentTrain: epoch  5, batch    30 | loss: 6.6470375
CurrentTrain: epoch  5, batch    31 | loss: 6.8320789
CurrentTrain: epoch  5, batch    32 | loss: 5.6556597
CurrentTrain: epoch  5, batch    33 | loss: 5.7978120
CurrentTrain: epoch  5, batch    34 | loss: 6.2456288
CurrentTrain: epoch  5, batch    35 | loss: 6.2789660
CurrentTrain: epoch  5, batch    36 | loss: 6.4756413
CurrentTrain: epoch  5, batch    37 | loss: 7.1352043
CurrentTrain: epoch  6, batch     0 | loss: 6.1736851
CurrentTrain: epoch  6, batch     1 | loss: 5.6836462
CurrentTrain: epoch  6, batch     2 | loss: 6.3350005
CurrentTrain: epoch  6, batch     3 | loss: 6.1327887
CurrentTrain: epoch  6, batch     4 | loss: 6.1611128
CurrentTrain: epoch  6, batch     5 | loss: 5.7846041
CurrentTrain: epoch  6, batch     6 | loss: 6.1274300
CurrentTrain: epoch  6, batch     7 | loss: 6.2056522
CurrentTrain: epoch  6, batch     8 | loss: 6.2470226
CurrentTrain: epoch  6, batch     9 | loss: 7.1541953
CurrentTrain: epoch  6, batch    10 | loss: 5.7981601
CurrentTrain: epoch  6, batch    11 | loss: 5.9197302
CurrentTrain: epoch  6, batch    12 | loss: 5.1976633
CurrentTrain: epoch  6, batch    13 | loss: 6.2915115
CurrentTrain: epoch  6, batch    14 | loss: 6.1278925
CurrentTrain: epoch  6, batch    15 | loss: 6.0377016
CurrentTrain: epoch  6, batch    16 | loss: 6.5044351
CurrentTrain: epoch  6, batch    17 | loss: 6.1789689
CurrentTrain: epoch  6, batch    18 | loss: 6.3805699
CurrentTrain: epoch  6, batch    19 | loss: 6.4481096
CurrentTrain: epoch  6, batch    20 | loss: 5.8423824
CurrentTrain: epoch  6, batch    21 | loss: 6.3781037
CurrentTrain: epoch  6, batch    22 | loss: 5.6188121
CurrentTrain: epoch  6, batch    23 | loss: 5.8072176
CurrentTrain: epoch  6, batch    24 | loss: 6.5282297
CurrentTrain: epoch  6, batch    25 | loss: 6.4276915
CurrentTrain: epoch  6, batch    26 | loss: 6.1234341
CurrentTrain: epoch  6, batch    27 | loss: 6.6887212
CurrentTrain: epoch  6, batch    28 | loss: 6.1274061
CurrentTrain: epoch  6, batch    29 | loss: 6.4702015
CurrentTrain: epoch  6, batch    30 | loss: 6.2211885
CurrentTrain: epoch  6, batch    31 | loss: 6.0167875
CurrentTrain: epoch  6, batch    32 | loss: 6.5782967
CurrentTrain: epoch  6, batch    33 | loss: 5.9003921
CurrentTrain: epoch  6, batch    34 | loss: 6.4920673
CurrentTrain: epoch  6, batch    35 | loss: 6.2626076
CurrentTrain: epoch  6, batch    36 | loss: 6.7299047
CurrentTrain: epoch  6, batch    37 | loss: 6.5543609
CurrentTrain: epoch  7, batch     0 | loss: 6.0944896
CurrentTrain: epoch  7, batch     1 | loss: 6.5252619
CurrentTrain: epoch  7, batch     2 | loss: 5.9673076
CurrentTrain: epoch  7, batch     3 | loss: 6.0717793
CurrentTrain: epoch  7, batch     4 | loss: 6.1280336
CurrentTrain: epoch  7, batch     5 | loss: 6.6398363
CurrentTrain: epoch  7, batch     6 | loss: 5.7793851
CurrentTrain: epoch  7, batch     7 | loss: 5.9760032
CurrentTrain: epoch  7, batch     8 | loss: 5.8876953
CurrentTrain: epoch  7, batch     9 | loss: 5.9924917
CurrentTrain: epoch  7, batch    10 | loss: 5.7827377
CurrentTrain: epoch  7, batch    11 | loss: 5.3023281
CurrentTrain: epoch  7, batch    12 | loss: 5.8194771
CurrentTrain: epoch  7, batch    13 | loss: 5.3227239
CurrentTrain: epoch  7, batch    14 | loss: 5.9119391
CurrentTrain: epoch  7, batch    15 | loss: 5.7036772
CurrentTrain: epoch  7, batch    16 | loss: 5.8937459
CurrentTrain: epoch  7, batch    17 | loss: 5.7617273
CurrentTrain: epoch  7, batch    18 | loss: 5.5227127
CurrentTrain: epoch  7, batch    19 | loss: 5.4497485
CurrentTrain: epoch  7, batch    20 | loss: 5.7146902
CurrentTrain: epoch  7, batch    21 | loss: 6.2035637
CurrentTrain: epoch  7, batch    22 | loss: 5.5199718
CurrentTrain: epoch  7, batch    23 | loss: 5.4495397
CurrentTrain: epoch  7, batch    24 | loss: 5.8391767
CurrentTrain: epoch  7, batch    25 | loss: 6.1184850
CurrentTrain: epoch  7, batch    26 | loss: 5.5711136
CurrentTrain: epoch  7, batch    27 | loss: 5.5048761
CurrentTrain: epoch  7, batch    28 | loss: 6.2887583
CurrentTrain: epoch  7, batch    29 | loss: 5.2566314
CurrentTrain: epoch  7, batch    30 | loss: 5.3388615
CurrentTrain: epoch  7, batch    31 | loss: 5.5406518
CurrentTrain: epoch  7, batch    32 | loss: 5.3182831
CurrentTrain: epoch  7, batch    33 | loss: 5.6661539
CurrentTrain: epoch  7, batch    34 | loss: 5.5259438
CurrentTrain: epoch  7, batch    35 | loss: 5.6990118
CurrentTrain: epoch  7, batch    36 | loss: 6.4108810
CurrentTrain: epoch  7, batch    37 | loss: 6.6792250
CurrentTrain: epoch  8, batch     0 | loss: 6.2655668
CurrentTrain: epoch  8, batch     1 | loss: 5.6565294
CurrentTrain: epoch  8, batch     2 | loss: 6.2546272
CurrentTrain: epoch  8, batch     3 | loss: 5.1187515
CurrentTrain: epoch  8, batch     4 | loss: 5.5153770
CurrentTrain: epoch  8, batch     5 | loss: 5.4409714
CurrentTrain: epoch  8, batch     6 | loss: 5.6974983
CurrentTrain: epoch  8, batch     7 | loss: 5.6639338
CurrentTrain: epoch  8, batch     8 | loss: 5.4869308
CurrentTrain: epoch  8, batch     9 | loss: 5.6169167
CurrentTrain: epoch  8, batch    10 | loss: 5.5943651
CurrentTrain: epoch  8, batch    11 | loss: 5.7563238
CurrentTrain: epoch  8, batch    12 | loss: 5.0986767
CurrentTrain: epoch  8, batch    13 | loss: 5.3102674
CurrentTrain: epoch  8, batch    14 | loss: 5.3560228
CurrentTrain: epoch  8, batch    15 | loss: 6.3386221
CurrentTrain: epoch  8, batch    16 | loss: 5.5539932
CurrentTrain: epoch  8, batch    17 | loss: 5.9308300
CurrentTrain: epoch  8, batch    18 | loss: 5.8069859
CurrentTrain: epoch  8, batch    19 | loss: 5.3958921
CurrentTrain: epoch  8, batch    20 | loss: 5.5421734
CurrentTrain: epoch  8, batch    21 | loss: 5.3917885
CurrentTrain: epoch  8, batch    22 | loss: 5.8683195
CurrentTrain: epoch  8, batch    23 | loss: 5.6921878
CurrentTrain: epoch  8, batch    24 | loss: 5.6777058
CurrentTrain: epoch  8, batch    25 | loss: 5.3143206
CurrentTrain: epoch  8, batch    26 | loss: 5.1922884
CurrentTrain: epoch  8, batch    27 | loss: 5.6279736
CurrentTrain: epoch  8, batch    28 | loss: 5.4777217
CurrentTrain: epoch  8, batch    29 | loss: 5.0845342
CurrentTrain: epoch  8, batch    30 | loss: 6.0336504
CurrentTrain: epoch  8, batch    31 | loss: 5.0802221
CurrentTrain: epoch  8, batch    32 | loss: 5.6213417
CurrentTrain: epoch  8, batch    33 | loss: 5.1279860
CurrentTrain: epoch  8, batch    34 | loss: 5.4399171
CurrentTrain: epoch  8, batch    35 | loss: 5.1835103
CurrentTrain: epoch  8, batch    36 | loss: 6.0986652
CurrentTrain: epoch  8, batch    37 | loss: 5.0521622
CurrentTrain: epoch  9, batch     0 | loss: 5.2497497
CurrentTrain: epoch  9, batch     1 | loss: 5.9806461
CurrentTrain: epoch  9, batch     2 | loss: 5.0923538
CurrentTrain: epoch  9, batch     3 | loss: 5.3475952
CurrentTrain: epoch  9, batch     4 | loss: 5.7791052
CurrentTrain: epoch  9, batch     5 | loss: 5.2465630
CurrentTrain: epoch  9, batch     6 | loss: 5.3015766
CurrentTrain: epoch  9, batch     7 | loss: 5.1658354
CurrentTrain: epoch  9, batch     8 | loss: 5.8936849
CurrentTrain: epoch  9, batch     9 | loss: 5.3426628
CurrentTrain: epoch  9, batch    10 | loss: 5.1280718
CurrentTrain: epoch  9, batch    11 | loss: 5.6527562
CurrentTrain: epoch  9, batch    12 | loss: 5.1661825
CurrentTrain: epoch  9, batch    13 | loss: 5.0657291
CurrentTrain: epoch  9, batch    14 | loss: 5.2064872
CurrentTrain: epoch  9, batch    15 | loss: 5.1797810
CurrentTrain: epoch  9, batch    16 | loss: 5.1488132
CurrentTrain: epoch  9, batch    17 | loss: 5.2635102
CurrentTrain: epoch  9, batch    18 | loss: 5.1710358
CurrentTrain: epoch  9, batch    19 | loss: 5.1791420
CurrentTrain: epoch  9, batch    20 | loss: 5.4412112
CurrentTrain: epoch  9, batch    21 | loss: 6.3194304
CurrentTrain: epoch  9, batch    22 | loss: 5.2781229
CurrentTrain: epoch  9, batch    23 | loss: 5.1026540
CurrentTrain: epoch  9, batch    24 | loss: 5.4873896
CurrentTrain: epoch  9, batch    25 | loss: 5.0757160
CurrentTrain: epoch  9, batch    26 | loss: 5.2142191
CurrentTrain: epoch  9, batch    27 | loss: 4.8291254
CurrentTrain: epoch  9, batch    28 | loss: 5.2134666
CurrentTrain: epoch  9, batch    29 | loss: 4.8908072
CurrentTrain: epoch  9, batch    30 | loss: 5.2138214
CurrentTrain: epoch  9, batch    31 | loss: 4.9617095
CurrentTrain: epoch  9, batch    32 | loss: 5.6697011
CurrentTrain: epoch  9, batch    33 | loss: 5.1123843
CurrentTrain: epoch  9, batch    34 | loss: 5.3085852
CurrentTrain: epoch  9, batch    35 | loss: 5.2771645
CurrentTrain: epoch  9, batch    36 | loss: 5.6807947
CurrentTrain: epoch  9, batch    37 | loss: 5.4140444
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots and family.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, James Smith relocated to Australia, seeking a more relaxed lifestyle by the beach.  
Head Entity: James Smith  
Tail Entity: Australia  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: kao charng , vice chairman of the cabinet-level mainland affairs council -lrb- mac -rrb- , explained that the termination clause was added to give both sides an option after the implementation of the tariff-cutting economic cooperation framework agreement -lrb- ecfa -rrb- .
Head Entity: mainland affairs council
Tail Entity: kao charng
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: elon musk, the CEO of spacex, announced the successful launch of the latest satellite into orbit, marking another milestone for the company.  
Head Entity: spacex  
Tail Entity: elon musk  

Relation: organization top members employees  
Context: sheryl sandberg, the chief operating officer of facebook, spoke at the conference about the importance of leadership in the tech industry.  
Head Entity: facebook  
Tail Entity: sheryl sandberg  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The International Federation of Red Cross and Red Crescent Societies (IFRC) is a global humanitarian organization that coordinates the activities of national Red Cross and Red Crescent societies around the world.  
Head Entity: International Federation of Red Cross and Red Crescent Societies  
Tail Entity: national Red Cross and Red Crescent societies  

Relation: organization member of  
Context: The European Union (EU) is a political and economic union of member states that are located primarily in Europe, working together to promote peace and stability in the region.  
Head Entity: European Union  
Tail Entity: member states  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: iranian nuclear negotiator ali larijani said thursday in ankara that talks on settling the iranian nuclear crisis had made some progress towards a `` united view . ''
Head Entity: ali larijani
Tail Entity: iranian
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The famous physicist Albert Einstein was born in the Kingdom of Württemberg in the German Empire, which is now part of modern-day Germany.  
Head Entity: Albert Einstein  
Tail Entity: German  

Relation: person origin  
Context: The renowned author Chimamanda Ngozi Adichie often speaks about her Nigerian heritage and how it influences her writing.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigerian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: earlier , in jerusalem , he spoke at the state funeral for the city 's fabled former mayor , teddy kollek , who died tuesday at 95 and was buried in the area of the mount herzl cemetery reserved for israel 's leaders .
Head Entity: teddy kollek
Tail Entity: mayor
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: After years of dedicated service, Dr. Emily Carter was honored with the prestigious title of Chief Medical Officer at the regional hospital, where she has worked tirelessly to improve patient care.  
Head Entity: Dr. Emily Carter  
Tail Entity: Chief Medical Officer  

Relation: person title  
Context: During the award ceremony, the renowned author was celebrated for her contributions to literature and was officially recognized as the Poet Laureate of the state, a title she had long aspired to achieve.  
Head Entity: the renowned author  
Tail Entity: Poet Laureate  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: delays and cancellations for argentina 's leading airline aerolineas argentinas -lrb- aa -rrb- continued on sunday , as a baggage handlers ' strike went into its fourth day .
Head Entity: aerolineas argentinas
Tail Entity: argentina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the tech giant apple inc. announced plans to expand its operations in ireland, taking advantage of the favorable corporate tax rates.  
Head Entity: apple inc.  
Tail Entity: ireland  

Relation: organization country of headquarters  
Context: the renowned automotive manufacturer toyota has established its main production facility in japan, contributing significantly to the local economy.  
Head Entity: toyota  
Tail Entity: japan  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   
[EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   
[EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 73.96%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 76.79%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 79.69%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 81.25%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 82.50%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 84.09%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 84.90%   
[EVAL] batch:   12 | acc: 87.50%,  total acc: 85.10%   
[EVAL] batch:   13 | acc: 75.00%,  total acc: 84.38%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 83.75%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 82.03%   
[EVAL] batch:   16 | acc: 81.25%,  total acc: 81.99%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 80.90%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 80.92%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 80.94%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 82.67%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 84.11%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 84.75%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 85.65%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 86.64%   
[EVAL] batch:   29 | acc: 87.50%,  total acc: 86.67%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 86.69%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   
[EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   
[EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 73.96%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 76.79%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 79.69%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 81.25%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 82.50%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 84.09%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 84.90%   
[EVAL] batch:   12 | acc: 87.50%,  total acc: 85.10%   
[EVAL] batch:   13 | acc: 75.00%,  total acc: 84.38%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 83.75%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 82.03%   
[EVAL] batch:   16 | acc: 81.25%,  total acc: 81.99%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 80.90%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 80.92%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 80.94%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 82.67%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 84.11%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 84.75%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 85.65%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 86.64%   
[EVAL] batch:   29 | acc: 87.50%,  total acc: 86.67%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 86.69%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 87.11%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 85.61%   
cur_acc:  ['0.8561']
his_acc:  ['0.8561']
CurrentTrain: epoch  0, batch     0 | loss: 5.0257683
CurrentTrain: epoch  0, batch     1 | loss: 4.7827039
CurrentTrain: epoch  1, batch     0 | loss: 4.2821436
CurrentTrain: epoch  1, batch     1 | loss: 4.1636605
CurrentTrain: epoch  2, batch     0 | loss: 3.6738467
CurrentTrain: epoch  2, batch     1 | loss: 3.8338785
CurrentTrain: epoch  3, batch     0 | loss: 3.3009553
CurrentTrain: epoch  3, batch     1 | loss: 3.9806676
CurrentTrain: epoch  4, batch     0 | loss: 3.5325487
CurrentTrain: epoch  4, batch     1 | loss: 3.0505800
CurrentTrain: epoch  5, batch     0 | loss: 3.3050137
CurrentTrain: epoch  5, batch     1 | loss: 2.8879874
CurrentTrain: epoch  6, batch     0 | loss: 3.0250683
CurrentTrain: epoch  6, batch     1 | loss: 2.6965706
CurrentTrain: epoch  7, batch     0 | loss: 2.4367599
CurrentTrain: epoch  7, batch     1 | loss: 2.9933374
CurrentTrain: epoch  8, batch     0 | loss: 2.6033103
CurrentTrain: epoch  8, batch     1 | loss: 2.6447051
CurrentTrain: epoch  9, batch     0 | loss: 2.6348381
CurrentTrain: epoch  9, batch     1 | loss: 2.3572047
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote policies that reflect its Christian values.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political landscape of the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is headquartered in suwon, south korea, and is a leader in consumer electronics.  
Head Entity: samsung electronics  
Tail Entity: south korea  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, and their close bond, highlighting how family ties remain strong despite their busy careers.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, uncle bob shared stories about his niece, emily, and how she has grown into a remarkable young woman, making the entire family proud.  
Head Entity: emily  
Tail Entity: uncle bob  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  105
MixupTrain:  epoch  0, batch     0 | loss: 5.4378481
MixupTrain:  epoch  0, batch     1 | loss: 4.8467436
MixupTrain:  epoch  0, batch     2 | loss: 4.7901697
MixupTrain:  epoch  0, batch     3 | loss: 4.5743332
MixupTrain:  epoch  0, batch     4 | loss: 4.6845622
MixupTrain:  epoch  0, batch     5 | loss: 4.6039591
MixupTrain:  epoch  0, batch     6 | loss: 4.7745352
MemoryTrain:  epoch  0, batch     0 | loss: 4.6389685
MemoryTrain:  epoch  0, batch     1 | loss: 4.6470556
MemoryTrain:  epoch  0, batch     2 | loss: 4.8329082
MemoryTrain:  epoch  1, batch     0 | loss: 4.1625266
MemoryTrain:  epoch  1, batch     1 | loss: 4.1928816
MemoryTrain:  epoch  1, batch     2 | loss: 4.4138017
MemoryTrain:  epoch  2, batch     0 | loss: 3.2369552
MemoryTrain:  epoch  2, batch     1 | loss: 3.8005462
MemoryTrain:  epoch  2, batch     2 | loss: 3.7596188
MemoryTrain:  epoch  3, batch     0 | loss: 2.9782710
MemoryTrain:  epoch  3, batch     1 | loss: 3.5140786
MemoryTrain:  epoch  3, batch     2 | loss: 2.7571192
MemoryTrain:  epoch  4, batch     0 | loss: 2.7937608
MemoryTrain:  epoch  4, batch     1 | loss: 3.2473989
MemoryTrain:  epoch  4, batch     2 | loss: 1.9913373
MemoryTrain:  epoch  5, batch     0 | loss: 3.6966014
MemoryTrain:  epoch  5, batch     1 | loss: 2.5329537
MemoryTrain:  epoch  5, batch     2 | loss: 1.4396424
MemoryTrain:  epoch  6, batch     0 | loss: 3.5652096
MemoryTrain:  epoch  6, batch     1 | loss: 2.6194172
MemoryTrain:  epoch  6, batch     2 | loss: 1.4908036
MemoryTrain:  epoch  7, batch     0 | loss: 3.0597382
MemoryTrain:  epoch  7, batch     1 | loss: 2.7982738
MemoryTrain:  epoch  7, batch     2 | loss: 1.4059273
MemoryTrain:  epoch  8, batch     0 | loss: 3.0534916
MemoryTrain:  epoch  8, batch     1 | loss: 2.2360592
MemoryTrain:  epoch  8, batch     2 | loss: 2.0591195
MemoryTrain:  epoch  9, batch     0 | loss: 2.5979919
MemoryTrain:  epoch  9, batch     1 | loss: 2.2581201
MemoryTrain:  epoch  9, batch     2 | loss: 2.5705748
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   
[EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   
[EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   
[EVAL] batch:    3 | acc: 56.25%,  total acc: 82.81%   
[EVAL] batch:    4 | acc: 31.25%,  total acc: 72.50%   
[EVAL] batch:    5 | acc: 25.00%,  total acc: 64.58%   
[EVAL] batch:    6 | acc: 37.50%,  total acc: 60.71%   
[EVAL] batch:    7 | acc: 93.75%,  total acc: 64.84%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 68.06%   
[EVAL] batch:    9 | acc: 75.00%,  total acc: 68.75%   
[EVAL] batch:   10 | acc: 93.75%,  total acc: 71.02%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 72.92%   
[EVAL] batch:   12 | acc: 50.00%,  total acc: 71.15%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   
[EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   
[EVAL] batch:    3 | acc: 68.75%,  total acc: 73.44%   
[EVAL] batch:    4 | acc: 81.25%,  total acc: 75.00%   
[EVAL] batch:    5 | acc: 62.50%,  total acc: 72.92%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 75.89%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 80.56%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 81.88%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 83.52%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 83.85%   
[EVAL] batch:   12 | acc: 87.50%,  total acc: 84.13%   
[EVAL] batch:   13 | acc: 75.00%,  total acc: 83.48%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 82.92%   
[EVAL] batch:   15 | acc: 62.50%,  total acc: 81.64%   
[EVAL] batch:   16 | acc: 81.25%,  total acc: 81.62%   
[EVAL] batch:   17 | acc: 68.75%,  total acc: 80.90%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 80.92%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 81.25%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 82.14%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 82.95%   
[EVAL] batch:   22 | acc: 93.75%,  total acc: 83.42%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 83.85%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 84.50%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 85.10%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 85.42%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 85.94%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 86.42%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 86.25%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 86.09%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   
[EVAL] batch:   32 | acc: 93.75%,  total acc: 86.55%   
[EVAL] batch:   33 | acc: 93.75%,  total acc: 86.76%   
[EVAL] batch:   34 | acc: 87.50%,  total acc: 86.79%   
[EVAL] batch:   35 | acc: 75.00%,  total acc: 86.46%   
[EVAL] batch:   36 | acc: 31.25%,  total acc: 84.97%   
[EVAL] batch:   37 | acc: 43.75%,  total acc: 83.88%   
[EVAL] batch:   38 | acc: 12.50%,  total acc: 82.05%   
[EVAL] batch:   39 | acc: 81.25%,  total acc: 82.03%   
[EVAL] batch:   40 | acc: 100.00%,  total acc: 82.47%   
[EVAL] batch:   41 | acc: 75.00%,  total acc: 82.29%   
[EVAL] batch:   42 | acc: 87.50%,  total acc: 82.41%   
[EVAL] batch:   43 | acc: 93.75%,  total acc: 82.67%   
[EVAL] batch:   44 | acc: 87.50%,  total acc: 82.78%   
cur_acc:  ['0.8561', '0.7115']
his_acc:  ['0.8561', '0.8278']
CurrentTrain: epoch  0, batch     0 | loss: 5.7312050
CurrentTrain: epoch  0, batch     1 | loss: 6.9593720
CurrentTrain: epoch  1, batch     0 | loss: 5.7051978
CurrentTrain: epoch  1, batch     1 | loss: 4.5546436
CurrentTrain: epoch  2, batch     0 | loss: 4.8995838
CurrentTrain: epoch  2, batch     1 | loss: 5.2730441
CurrentTrain: epoch  3, batch     0 | loss: 4.8247356
CurrentTrain: epoch  3, batch     1 | loss: 4.5322895
CurrentTrain: epoch  4, batch     0 | loss: 4.3757648
CurrentTrain: epoch  4, batch     1 | loss: 3.4662175
CurrentTrain: epoch  5, batch     0 | loss: 3.7668066
CurrentTrain: epoch  5, batch     1 | loss: 3.5155864
CurrentTrain: epoch  6, batch     0 | loss: 3.4060245
CurrentTrain: epoch  6, batch     1 | loss: 3.3856432
CurrentTrain: epoch  7, batch     0 | loss: 3.4695911
CurrentTrain: epoch  7, batch     1 | loss: 3.0194812
CurrentTrain: epoch  8, batch     0 | loss: 3.0389028
CurrentTrain: epoch  8, batch     1 | loss: 3.0704243
CurrentTrain: epoch  9, batch     0 | loss: 2.7287169
CurrentTrain: epoch  9, batch     1 | loss: 3.4067116
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, actor john doe has decided to settle down in the serene landscapes of oregon, where he can enjoy a quieter lifestyle away from the spotlight.  
Head Entity: john doe  
Tail Entity: oregon  

Relation: person stateorprovinces of residence  
Context: renowned author jane smith has always found inspiration in the vibrant culture of california, which is why she chose to make los angeles her home for the past decade.  
Head Entity: jane smith  
Tail Entity: california  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away on July 20, 2021, after a long illness.  
Head Entity: The renowned author  
Tail Entity: July 20, 2021  

Relation: person date of death  
Context: She left this world peacefully in her sleep on March 5, 2019.  
Head Entity: She  
Tail Entity: March 5, 2019  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, boasts a workforce of approximately 5,500 employees across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: GreenEarth Nonprofit has grown significantly over the past few years and currently employs around 1,200 dedicated staff members to support its environmental initiatives.  
Head Entity: GreenEarth Nonprofit  
Tail Entity: 1,200  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: the coming of the mahdi will turn the world upside down , and the oppressed shiites will finally see justice .
Head Entity: mahdi
Tail Entity: shiites
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Clemens is better known by his pen name, Mark Twain, which he used for his literary works.  
Head Entity: Samuel Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician known as Stefani Germanotta has achieved global fame under her stage name, Lady Gaga, captivating audiences with her unique style.  
Head Entity: Stefani Germanotta  
Tail Entity: Lady Gaga  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a lavish ceremony held in new york city, the couple exchanged vows in front of family and friends: john legend and chrissy teigen celebrated their love with a beautiful wedding.  
Head Entity: john legend  
Tail Entity: chrissy teigen  

Relation: person spouse  
Context: during the annual charity gala, it was announced that the famous actor and his long-time partner have tied the knot: ben affleck and jennifer garner are now officially husband and wife.  
Head Entity: ben affleck  
Tail Entity: jennifer garner  
Mixup data size:  134
MixupTrain:  epoch  0, batch     0 | loss: 3.6239805
MixupTrain:  epoch  0, batch     1 | loss: 3.1186132
MixupTrain:  epoch  0, batch     2 | loss: 3.2706451
MixupTrain:  epoch  0, batch     3 | loss: 3.0973854
MixupTrain:  epoch  0, batch     4 | loss: 3.5326552
MixupTrain:  epoch  0, batch     5 | loss: 2.8841610
MixupTrain:  epoch  0, batch     6 | loss: 2.7497377
MixupTrain:  epoch  0, batch     7 | loss: 3.0712430
MixupTrain:  epoch  0, batch     8 | loss: 3.1538224
MemoryTrain:  epoch  0, batch     0 | loss: 3.8566487
MemoryTrain:  epoch  0, batch     1 | loss: 3.1615031
MemoryTrain:  epoch  0, batch     2 | loss: 3.2845554
MemoryTrain:  epoch  1, batch     0 | loss: 2.3528988
MemoryTrain:  epoch  1, batch     1 | loss: 4.3091812
MemoryTrain:  epoch  1, batch     2 | loss: 2.8057530
MemoryTrain:  epoch  2, batch     0 | loss: 2.7547712
MemoryTrain:  epoch  2, batch     1 | loss: 2.5131328
MemoryTrain:  epoch  2, batch     2 | loss: 3.2747097
MemoryTrain:  epoch  3, batch     0 | loss: 2.5613458
MemoryTrain:  epoch  3, batch     1 | loss: 2.4220026
MemoryTrain:  epoch  3, batch     2 | loss: 3.1411541
MemoryTrain:  epoch  4, batch     0 | loss: 2.8186619
MemoryTrain:  epoch  4, batch     1 | loss: 2.6990964
MemoryTrain:  epoch  4, batch     2 | loss: 2.2092423
MemoryTrain:  epoch  5, batch     0 | loss: 2.1341636
MemoryTrain:  epoch  5, batch     1 | loss: 2.5518939
MemoryTrain:  epoch  5, batch     2 | loss: 2.0788922
MemoryTrain:  epoch  6, batch     0 | loss: 2.2738349
MemoryTrain:  epoch  6, batch     1 | loss: 1.9231961
MemoryTrain:  epoch  6, batch     2 | loss: 2.2054210
MemoryTrain:  epoch  7, batch     0 | loss: 1.9795464
MemoryTrain:  epoch  7, batch     1 | loss: 1.9059665
MemoryTrain:  epoch  7, batch     2 | loss: 1.9327438
MemoryTrain:  epoch  8, batch     0 | loss: 1.8641745
MemoryTrain:  epoch  8, batch     1 | loss: 1.8125949
MemoryTrain:  epoch  8, batch     2 | loss: 1.8691173
MemoryTrain:  epoch  9, batch     0 | loss: 1.6992167
MemoryTrain:  epoch  9, batch     1 | loss: 1.8837879
MemoryTrain:  epoch  9, batch     2 | loss: 1.6448374
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   
[EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   
[EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   
[EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   
[EVAL] batch:    5 | acc: 100.00%,  total acc: 84.38%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 86.61%   
[EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 88.19%   
[EVAL] batch:    9 | acc: 68.75%,  total acc: 86.25%   
[EVAL] batch:   10 | acc: 43.75%,  total acc: 82.39%   
[EVAL] batch:   11 | acc: 75.00%,  total acc: 81.77%   
[EVAL] batch:   12 | acc: 75.00%,  total acc: 81.25%   
[EVAL] batch:   13 | acc: 75.00%,  total acc: 80.80%   
[EVAL] batch:   14 | acc: 31.25%,  total acc: 77.50%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   
[EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   
[EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 79.17%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 83.59%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 84.72%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 85.62%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 86.93%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 87.50%   
[EVAL] batch:   12 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:   13 | acc: 68.75%,  total acc: 86.16%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 85.42%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 83.59%   
[EVAL] batch:   16 | acc: 81.25%,  total acc: 83.46%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 82.29%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 82.24%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 85.16%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 86.30%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 86.57%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 87.05%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 87.50%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 87.29%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 87.10%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 87.30%   
[EVAL] batch:   32 | acc: 93.75%,  total acc: 87.50%   
[EVAL] batch:   33 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:   34 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:   35 | acc: 75.00%,  total acc: 87.15%   
[EVAL] batch:   36 | acc: 31.25%,  total acc: 85.64%   
[EVAL] batch:   37 | acc: 56.25%,  total acc: 84.87%   
[EVAL] batch:   38 | acc: 43.75%,  total acc: 83.81%   
[EVAL] batch:   39 | acc: 68.75%,  total acc: 83.44%   
[EVAL] batch:   40 | acc: 62.50%,  total acc: 82.93%   
[EVAL] batch:   41 | acc: 50.00%,  total acc: 82.14%   
[EVAL] batch:   42 | acc: 50.00%,  total acc: 81.40%   
[EVAL] batch:   43 | acc: 93.75%,  total acc: 81.68%   
[EVAL] batch:   44 | acc: 93.75%,  total acc: 81.94%   
[EVAL] batch:   45 | acc: 75.00%,  total acc: 81.79%   
[EVAL] batch:   46 | acc: 81.25%,  total acc: 81.78%   
[EVAL] batch:   47 | acc: 87.50%,  total acc: 81.90%   
[EVAL] batch:   48 | acc: 87.50%,  total acc: 82.02%   
[EVAL] batch:   49 | acc: 75.00%,  total acc: 81.88%   
[EVAL] batch:   50 | acc: 100.00%,  total acc: 82.23%   
[EVAL] batch:   51 | acc: 100.00%,  total acc: 82.57%   
[EVAL] batch:   52 | acc: 93.75%,  total acc: 82.78%   
[EVAL] batch:   53 | acc: 93.75%,  total acc: 82.99%   
[EVAL] batch:   54 | acc: 62.50%,  total acc: 82.61%   
[EVAL] batch:   55 | acc: 50.00%,  total acc: 82.03%   
[EVAL] batch:   56 | acc: 68.75%,  total acc: 81.80%   
[EVAL] batch:   57 | acc: 75.00%,  total acc: 81.68%   
[EVAL] batch:   58 | acc: 81.25%,  total acc: 81.67%   
[EVAL] batch:   59 | acc: 25.00%,  total acc: 80.73%   
cur_acc:  ['0.8561', '0.7115', '0.7750']
his_acc:  ['0.8561', '0.8278', '0.8073']
CurrentTrain: epoch  0, batch     0 | loss: 8.0778913
CurrentTrain: epoch  0, batch     1 | loss: 8.0262337
CurrentTrain: epoch  1, batch     0 | loss: 7.4012680
CurrentTrain: epoch  1, batch     1 | loss: 6.8623972
CurrentTrain: epoch  2, batch     0 | loss: 6.9242897
CurrentTrain: epoch  2, batch     1 | loss: 6.1294565
CurrentTrain: epoch  3, batch     0 | loss: 6.2736697
CurrentTrain: epoch  3, batch     1 | loss: 5.6309366
CurrentTrain: epoch  4, batch     0 | loss: 5.7502136
CurrentTrain: epoch  4, batch     1 | loss: 5.6073446
CurrentTrain: epoch  5, batch     0 | loss: 5.1821833
CurrentTrain: epoch  5, batch     1 | loss: 5.4868970
CurrentTrain: epoch  6, batch     0 | loss: 5.3367214
CurrentTrain: epoch  6, batch     1 | loss: 5.0508447
CurrentTrain: epoch  7, batch     0 | loss: 5.0041389
CurrentTrain: epoch  7, batch     1 | loss: 4.9139547
CurrentTrain: epoch  8, batch     0 | loss: 4.8927584
CurrentTrain: epoch  8, batch     1 | loss: 4.2858820
CurrentTrain: epoch  9, batch     0 | loss: 4.2656546
CurrentTrain: epoch  9, batch     1 | loss: 4.5512776
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has become a leading platform for video sharing and streaming.  
Head Entity: Alphabet Inc.  
Tail Entity: YouTube  

Relation: organization subsidiaries  
Context: The automotive manufacturer General Motors has expanded its portfolio by acquiring several companies, including Cruise Automation, which focuses on self-driving technology.  
Head Entity: General Motors  
Tail Entity: Cruise Automation  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant SoftTech announced its acquisition of Innovatech, a leading software development firm. This move has raised questions about the future of Innovatech and its role under the SoftTech umbrella. Industry experts believe that this acquisition will strengthen SoftTech's position in the market.  
Head Entity: SoftTech  
Tail Entity: Innovatech  

Relation: organization parents  
Context: The historic partnership between Global Health Initiative and Health for All has been pivotal in addressing healthcare challenges in underserved communities. As a parent organization, Global Health Initiative has provided essential resources and support to Health for All, enabling it to expand its outreach programs significantly.  
Head Entity: Global Health Initiative  
Tail Entity: Health for All  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the world health organization -lrb- who -rrb- has been instrumental in coordinating global responses to health emergencies, including the recent pandemic.  
Head Entity: world health organization  
Tail Entity: who  

Relation: organization alternate names  
Context: the federal bureau of investigation -lrb- fbi -rrb- is known for its role in investigating federal crimes and maintaining national security.  
Head Entity: federal bureau of investigation  
Tail Entity: fbi  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant apple inc. has announced plans to expand its headquarters in the heart of silicon valley, which is located in cupertino, california, aiming to create thousands of new jobs in the area.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ new york 2019-11-22 14:45:00 utc the financial services firm goldman sachs has its main office situated in lower manhattan, new york city, where it continues to play a pivotal role in global finance.  
Head Entity: goldman sachs  
Tail Entity: new york city  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time reminiscing about their childhood. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily celebrated with her brother, who had always been her biggest supporter throughout school. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  165
MixupTrain:  epoch  0, batch     0 | loss: 2.9307249
MixupTrain:  epoch  0, batch     1 | loss: 2.8220694
MixupTrain:  epoch  0, batch     2 | loss: 2.9554574
MixupTrain:  epoch  0, batch     3 | loss: 3.1431615
MixupTrain:  epoch  0, batch     4 | loss: 3.0373974
MixupTrain:  epoch  0, batch     5 | loss: 2.9650300
MixupTrain:  epoch  0, batch     6 | loss: 2.9446192
MixupTrain:  epoch  0, batch     7 | loss: 2.6719625
MixupTrain:  epoch  0, batch     8 | loss: 3.1559248
MixupTrain:  epoch  0, batch     9 | loss: 2.8515167
MixupTrain:  epoch  0, batch    10 | loss: 3.3790720
MemoryTrain:  epoch  0, batch     0 | loss: 2.9754546
MemoryTrain:  epoch  0, batch     1 | loss: 3.1451430
MemoryTrain:  epoch  0, batch     2 | loss: 3.2950697
MemoryTrain:  epoch  0, batch     3 | loss: 2.9380124
MemoryTrain:  epoch  1, batch     0 | loss: 3.1129837
MemoryTrain:  epoch  1, batch     1 | loss: 2.4476714
MemoryTrain:  epoch  1, batch     2 | loss: 2.9483430
MemoryTrain:  epoch  1, batch     3 | loss: 3.1689281
MemoryTrain:  epoch  2, batch     0 | loss: 3.4944644
MemoryTrain:  epoch  2, batch     1 | loss: 2.4792666
MemoryTrain:  epoch  2, batch     2 | loss: 2.2050581
MemoryTrain:  epoch  2, batch     3 | loss: 2.0142379
MemoryTrain:  epoch  3, batch     0 | loss: 2.6917317
MemoryTrain:  epoch  3, batch     1 | loss: 3.1687512
MemoryTrain:  epoch  3, batch     2 | loss: 2.0689406
MemoryTrain:  epoch  3, batch     3 | loss: 2.3475318
MemoryTrain:  epoch  4, batch     0 | loss: 2.1441383
MemoryTrain:  epoch  4, batch     1 | loss: 2.3348820
MemoryTrain:  epoch  4, batch     2 | loss: 2.6933575
MemoryTrain:  epoch  4, batch     3 | loss: 1.8133042
MemoryTrain:  epoch  5, batch     0 | loss: 2.3126211
MemoryTrain:  epoch  5, batch     1 | loss: 2.0085216
MemoryTrain:  epoch  5, batch     2 | loss: 2.1320505
MemoryTrain:  epoch  5, batch     3 | loss: 1.8312533
MemoryTrain:  epoch  6, batch     0 | loss: 1.6916314
MemoryTrain:  epoch  6, batch     1 | loss: 2.2505622
MemoryTrain:  epoch  6, batch     2 | loss: 2.0311561
MemoryTrain:  epoch  6, batch     3 | loss: 2.0454643
MemoryTrain:  epoch  7, batch     0 | loss: 1.9294889
MemoryTrain:  epoch  7, batch     1 | loss: 2.3111081
MemoryTrain:  epoch  7, batch     2 | loss: 1.6255630
MemoryTrain:  epoch  7, batch     3 | loss: 1.5952525
MemoryTrain:  epoch  8, batch     0 | loss: 1.6598282
MemoryTrain:  epoch  8, batch     1 | loss: 2.1883769
MemoryTrain:  epoch  8, batch     2 | loss: 1.5270687
MemoryTrain:  epoch  8, batch     3 | loss: 1.7121832
MemoryTrain:  epoch  9, batch     0 | loss: 1.7573054
MemoryTrain:  epoch  9, batch     1 | loss: 2.1167150
MemoryTrain:  epoch  9, batch     2 | loss: 1.4194732
MemoryTrain:  epoch  9, batch     3 | loss: 1.7910424
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   
[EVAL] batch:    1 | acc: 62.50%,  total acc: 46.88%   
[EVAL] batch:    2 | acc: 37.50%,  total acc: 43.75%   
[EVAL] batch:    3 | acc: 0.00%,  total acc: 32.81%   
[EVAL] batch:    4 | acc: 0.00%,  total acc: 26.25%   
[EVAL] batch:    5 | acc: 0.00%,  total acc: 21.88%   
[EVAL] batch:    6 | acc: 25.00%,  total acc: 22.32%   
[EVAL] batch:    7 | acc: 43.75%,  total acc: 25.00%   
[EVAL] batch:    8 | acc: 50.00%,  total acc: 27.78%   
[EVAL] batch:    9 | acc: 50.00%,  total acc: 30.00%   
[EVAL] batch:   10 | acc: 81.25%,  total acc: 34.66%   
[EVAL] batch:   11 | acc: 62.50%,  total acc: 36.98%   
[EVAL] batch:   12 | acc: 62.50%,  total acc: 38.94%   
[EVAL] batch:   13 | acc: 100.00%,  total acc: 43.30%   
[EVAL] batch:   14 | acc: 100.00%,  total acc: 47.08%   
[EVAL] batch:   15 | acc: 100.00%,  total acc: 50.39%   
[EVAL] batch:   16 | acc: 93.75%,  total acc: 52.94%   
[EVAL] batch:   17 | acc: 87.50%,  total acc: 54.86%   
[EVAL] batch:   18 | acc: 37.50%,  total acc: 53.95%   
[EVAL] batch:   19 | acc: 25.00%,  total acc: 52.50%   
[EVAL] batch:   20 | acc: 31.25%,  total acc: 51.49%   
[EVAL] batch:   21 | acc: 12.50%,  total acc: 49.72%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   
[EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   
[EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   
[EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 75.00%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 82.64%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 83.12%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 84.66%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 84.90%   
[EVAL] batch:   12 | acc: 62.50%,  total acc: 83.17%   
[EVAL] batch:   13 | acc: 56.25%,  total acc: 81.25%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 80.83%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 79.30%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 79.04%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 78.12%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 78.29%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 78.44%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 79.46%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 80.40%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 81.25%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 81.77%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 82.50%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 83.17%   
[EVAL] batch:   26 | acc: 81.25%,  total acc: 83.10%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 84.27%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 84.17%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 84.07%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 84.57%   
[EVAL] batch:   32 | acc: 93.75%,  total acc: 84.85%   
[EVAL] batch:   33 | acc: 81.25%,  total acc: 84.74%   
[EVAL] batch:   34 | acc: 87.50%,  total acc: 84.82%   
[EVAL] batch:   35 | acc: 81.25%,  total acc: 84.72%   
[EVAL] batch:   36 | acc: 31.25%,  total acc: 83.28%   
[EVAL] batch:   37 | acc: 25.00%,  total acc: 81.74%   
[EVAL] batch:   38 | acc: 31.25%,  total acc: 80.45%   
[EVAL] batch:   39 | acc: 25.00%,  total acc: 79.06%   
[EVAL] batch:   40 | acc: 43.75%,  total acc: 78.20%   
[EVAL] batch:   41 | acc: 43.75%,  total acc: 77.38%   
[EVAL] batch:   42 | acc: 37.50%,  total acc: 76.45%   
[EVAL] batch:   43 | acc: 100.00%,  total acc: 76.99%   
[EVAL] batch:   44 | acc: 93.75%,  total acc: 77.36%   
[EVAL] batch:   45 | acc: 75.00%,  total acc: 77.31%   
[EVAL] batch:   46 | acc: 75.00%,  total acc: 77.26%   
[EVAL] batch:   47 | acc: 87.50%,  total acc: 77.47%   
[EVAL] batch:   48 | acc: 93.75%,  total acc: 77.81%   
[EVAL] batch:   49 | acc: 87.50%,  total acc: 78.00%   
[EVAL] batch:   50 | acc: 100.00%,  total acc: 78.43%   
[EVAL] batch:   51 | acc: 100.00%,  total acc: 78.85%   
[EVAL] batch:   52 | acc: 93.75%,  total acc: 79.13%   
[EVAL] batch:   53 | acc: 93.75%,  total acc: 79.40%   
[EVAL] batch:   54 | acc: 68.75%,  total acc: 79.20%   
[EVAL] batch:   55 | acc: 43.75%,  total acc: 78.57%   
[EVAL] batch:   56 | acc: 12.50%,  total acc: 77.41%   
[EVAL] batch:   57 | acc: 18.75%,  total acc: 76.40%   
[EVAL] batch:   58 | acc: 12.50%,  total acc: 75.32%   
[EVAL] batch:   59 | acc: 37.50%,  total acc: 74.69%   
[EVAL] batch:   60 | acc: 68.75%,  total acc: 74.59%   
[EVAL] batch:   61 | acc: 37.50%,  total acc: 73.99%   
[EVAL] batch:   62 | acc: 12.50%,  total acc: 73.02%   
[EVAL] batch:   63 | acc: 0.00%,  total acc: 71.88%   
[EVAL] batch:   64 | acc: 0.00%,  total acc: 70.77%   
[EVAL] batch:   65 | acc: 0.00%,  total acc: 69.70%   
[EVAL] batch:   66 | acc: 56.25%,  total acc: 69.50%   
[EVAL] batch:   67 | acc: 50.00%,  total acc: 69.21%   
[EVAL] batch:   68 | acc: 43.75%,  total acc: 68.84%   
[EVAL] batch:   69 | acc: 75.00%,  total acc: 68.93%   
[EVAL] batch:   70 | acc: 75.00%,  total acc: 69.01%   
[EVAL] batch:   71 | acc: 50.00%,  total acc: 68.75%   
[EVAL] batch:   72 | acc: 87.50%,  total acc: 69.01%   
[EVAL] batch:   73 | acc: 100.00%,  total acc: 69.43%   
[EVAL] batch:   74 | acc: 100.00%,  total acc: 69.83%   
[EVAL] batch:   75 | acc: 100.00%,  total acc: 70.23%   
[EVAL] batch:   76 | acc: 81.25%,  total acc: 70.37%   
[EVAL] batch:   77 | acc: 68.75%,  total acc: 70.35%   
[EVAL] batch:   78 | acc: 18.75%,  total acc: 69.70%   
[EVAL] batch:   79 | acc: 31.25%,  total acc: 69.22%   
[EVAL] batch:   80 | acc: 25.00%,  total acc: 68.67%   
cur_acc:  ['0.8561', '0.7115', '0.7750', '0.4972']
his_acc:  ['0.8561', '0.8278', '0.8073', '0.6867']
CurrentTrain: epoch  0, batch     0 | loss: 5.0408449
CurrentTrain: epoch  0, batch     1 | loss: 6.0269699
CurrentTrain: epoch  1, batch     0 | loss: 4.5026231
CurrentTrain: epoch  1, batch     1 | loss: 4.5658526
CurrentTrain: epoch  2, batch     0 | loss: 3.7636058
CurrentTrain: epoch  2, batch     1 | loss: 4.1759238
CurrentTrain: epoch  3, batch     0 | loss: 3.7285876
CurrentTrain: epoch  3, batch     1 | loss: 2.7967718
CurrentTrain: epoch  4, batch     0 | loss: 3.3894901
CurrentTrain: epoch  4, batch     1 | loss: 2.7434042
CurrentTrain: epoch  5, batch     0 | loss: 3.3198094
CurrentTrain: epoch  5, batch     1 | loss: 2.9266226
CurrentTrain: epoch  6, batch     0 | loss: 2.7877405
CurrentTrain: epoch  6, batch     1 | loss: 2.8731766
CurrentTrain: epoch  7, batch     0 | loss: 2.3118172
CurrentTrain: epoch  7, batch     1 | loss: 2.4392493
CurrentTrain: epoch  8, batch     0 | loss: 2.2733157
CurrentTrain: epoch  8, batch     1 | loss: 2.1827521
CurrentTrain: epoch  9, batch     0 | loss: 2.0720506
CurrentTrain: epoch  9, batch     1 | loss: 2.3774130
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones settled in san francisco, where she found a job in tech. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith lived in boston, enjoying the vibrant culture and history of the city. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: in a cheerful first-person narration , he travels to a girls ' school in kabul , to post-soviet russia and to his alma mater , hollywood high in los angeles , the place where he first learned to mistrust liberals .
Head Entity: he
Tail Entity: hollywood high
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After graduating from a prestigious university, she went on to teach at several institutions, including her beloved alma mater, St. Mary's College, where she first discovered her passion for education.  
Head Entity: she  
Tail Entity: St. Mary's College  

Relation: person schools attended  
Context: He often reminisces about his time at the University of Chicago, where he developed his critical thinking skills and made lifelong friends, shaping his future career.  
Head Entity: He  
Tail Entity: University of Chicago  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author agatha christie died in her home in wallingford, england  
Head Entity: agatha christie  
Tail Entity: england  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the former mayor was facing serious allegations related to corruption.  
Head Entity: former mayor  
Tail Entity: corruption  

Relation: person charges  
Context: After a lengthy investigation, the authorities confirmed that the celebrity was implicated in a major fraud scheme.  
Head Entity: celebrity  
Tail Entity: fraud scheme  
Mixup data size:  194
MixupTrain:  epoch  0, batch     0 | loss: 2.5338216
MixupTrain:  epoch  0, batch     1 | loss: 2.3374524
MixupTrain:  epoch  0, batch     2 | loss: 2.5796728
MixupTrain:  epoch  0, batch     3 | loss: 2.4682140
MixupTrain:  epoch  0, batch     4 | loss: 2.4041281
MixupTrain:  epoch  0, batch     5 | loss: 2.2133031
MixupTrain:  epoch  0, batch     6 | loss: 2.5303373
MixupTrain:  epoch  0, batch     7 | loss: 2.3832991
MixupTrain:  epoch  0, batch     8 | loss: 2.0220377
MixupTrain:  epoch  0, batch     9 | loss: 2.3491311
MixupTrain:  epoch  0, batch    10 | loss: 2.6049259
MixupTrain:  epoch  0, batch    11 | loss: 2.0405302
MixupTrain:  epoch  0, batch    12 | loss: 2.5475450
MemoryTrain:  epoch  0, batch     0 | loss: 1.9814901
MemoryTrain:  epoch  0, batch     1 | loss: 2.5678940
MemoryTrain:  epoch  0, batch     2 | loss: 2.6167819
MemoryTrain:  epoch  0, batch     3 | loss: 2.9639010
MemoryTrain:  epoch  0, batch     4 | loss: 2.7056003
MemoryTrain:  epoch  1, batch     0 | loss: 2.5202339
MemoryTrain:  epoch  1, batch     1 | loss: 2.4273100
MemoryTrain:  epoch  1, batch     2 | loss: 1.9757800
MemoryTrain:  epoch  1, batch     3 | loss: 1.7817464
MemoryTrain:  epoch  1, batch     4 | loss: 2.7083943
MemoryTrain:  epoch  2, batch     0 | loss: 1.9646106
MemoryTrain:  epoch  2, batch     1 | loss: 1.9659026
MemoryTrain:  epoch  2, batch     2 | loss: 2.1576948
MemoryTrain:  epoch  2, batch     3 | loss: 2.2272544
MemoryTrain:  epoch  2, batch     4 | loss: 1.7245752
MemoryTrain:  epoch  3, batch     0 | loss: 1.8121848
MemoryTrain:  epoch  3, batch     1 | loss: 1.7363222
MemoryTrain:  epoch  3, batch     2 | loss: 1.7822049
MemoryTrain:  epoch  3, batch     3 | loss: 1.8701088
MemoryTrain:  epoch  3, batch     4 | loss: 1.9187524
MemoryTrain:  epoch  4, batch     0 | loss: 1.7352374
MemoryTrain:  epoch  4, batch     1 | loss: 1.9043090
MemoryTrain:  epoch  4, batch     2 | loss: 1.5577230
MemoryTrain:  epoch  4, batch     3 | loss: 1.4862442
MemoryTrain:  epoch  4, batch     4 | loss: 1.6390179
MemoryTrain:  epoch  5, batch     0 | loss: 1.6873133
MemoryTrain:  epoch  5, batch     1 | loss: 1.6061871
MemoryTrain:  epoch  5, batch     2 | loss: 1.7279127
MemoryTrain:  epoch  5, batch     3 | loss: 1.4538095
MemoryTrain:  epoch  5, batch     4 | loss: 1.6589062
MemoryTrain:  epoch  6, batch     0 | loss: 1.5376846
MemoryTrain:  epoch  6, batch     1 | loss: 1.7978396
MemoryTrain:  epoch  6, batch     2 | loss: 1.5109245
MemoryTrain:  epoch  6, batch     3 | loss: 1.4010110
MemoryTrain:  epoch  6, batch     4 | loss: 1.7372113
MemoryTrain:  epoch  7, batch     0 | loss: 1.6696217
MemoryTrain:  epoch  7, batch     1 | loss: 1.4997833
MemoryTrain:  epoch  7, batch     2 | loss: 1.3668394
MemoryTrain:  epoch  7, batch     3 | loss: 1.4554195
MemoryTrain:  epoch  7, batch     4 | loss: 1.4425374
MemoryTrain:  epoch  8, batch     0 | loss: 1.5319171
MemoryTrain:  epoch  8, batch     1 | loss: 1.5361350
MemoryTrain:  epoch  8, batch     2 | loss: 1.3935716
MemoryTrain:  epoch  8, batch     3 | loss: 1.3761287
MemoryTrain:  epoch  8, batch     4 | loss: 1.2742752
MemoryTrain:  epoch  9, batch     0 | loss: 1.4336982
MemoryTrain:  epoch  9, batch     1 | loss: 1.5042707
MemoryTrain:  epoch  9, batch     2 | loss: 1.3739891
MemoryTrain:  epoch  9, batch     3 | loss: 1.2958698
MemoryTrain:  epoch  9, batch     4 | loss: 1.3496845
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   
[EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   
[EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   
[EVAL] batch:    4 | acc: 68.75%,  total acc: 75.00%   
[EVAL] batch:    5 | acc: 50.00%,  total acc: 70.83%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 74.11%   
[EVAL] batch:    7 | acc: 93.75%,  total acc: 76.56%   
[EVAL] batch:    8 | acc: 68.75%,  total acc: 75.69%   
[EVAL] batch:    9 | acc: 100.00%,  total acc: 78.12%   
[EVAL] batch:   10 | acc: 87.50%,  total acc: 78.98%   
[EVAL] batch:   11 | acc: 100.00%,  total acc: 80.73%   
[EVAL] batch:   12 | acc: 100.00%,  total acc: 82.21%   
[EVAL] batch:   13 | acc: 100.00%,  total acc: 83.48%   
[EVAL] batch:   14 | acc: 100.00%,  total acc: 84.58%   
[EVAL] batch:   15 | acc: 100.00%,  total acc: 85.55%   
[EVAL] batch:   16 | acc: 100.00%,  total acc: 86.40%   
[EVAL] batch:   17 | acc: 25.00%,  total acc: 82.99%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   
[EVAL] batch:    2 | acc: 56.25%,  total acc: 66.67%   
[EVAL] batch:    3 | acc: 62.50%,  total acc: 65.62%   
[EVAL] batch:    4 | acc: 62.50%,  total acc: 65.00%   
[EVAL] batch:    5 | acc: 62.50%,  total acc: 64.58%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 69.64%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 73.44%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 76.39%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 77.50%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 79.55%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 80.21%   
[EVAL] batch:   12 | acc: 56.25%,  total acc: 78.37%   
[EVAL] batch:   13 | acc: 50.00%,  total acc: 76.34%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 76.25%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 75.00%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 74.31%   
[EVAL] batch:   18 | acc: 68.75%,  total acc: 74.01%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 74.38%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 75.60%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 76.70%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 77.72%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 78.39%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 79.25%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 80.05%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 80.56%   
[EVAL] batch:   27 | acc: 93.75%,  total acc: 81.03%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 81.68%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 81.67%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 81.65%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 82.23%   
[EVAL] batch:   32 | acc: 43.75%,  total acc: 81.06%   
[EVAL] batch:   33 | acc: 37.50%,  total acc: 79.78%   
[EVAL] batch:   34 | acc: 50.00%,  total acc: 78.93%   
[EVAL] batch:   35 | acc: 37.50%,  total acc: 77.78%   
[EVAL] batch:   36 | acc: 68.75%,  total acc: 77.53%   
[EVAL] batch:   37 | acc: 50.00%,  total acc: 76.81%   
[EVAL] batch:   38 | acc: 56.25%,  total acc: 76.28%   
[EVAL] batch:   39 | acc: 56.25%,  total acc: 75.78%   
[EVAL] batch:   40 | acc: 12.50%,  total acc: 74.24%   
[EVAL] batch:   41 | acc: 31.25%,  total acc: 73.21%   
[EVAL] batch:   42 | acc: 18.75%,  total acc: 71.95%   
[EVAL] batch:   43 | acc: 68.75%,  total acc: 71.88%   
[EVAL] batch:   44 | acc: 75.00%,  total acc: 71.94%   
[EVAL] batch:   45 | acc: 62.50%,  total acc: 71.74%   
[EVAL] batch:   46 | acc: 68.75%,  total acc: 71.68%   
[EVAL] batch:   47 | acc: 37.50%,  total acc: 70.96%   
[EVAL] batch:   48 | acc: 68.75%,  total acc: 70.92%   
[EVAL] batch:   49 | acc: 62.50%,  total acc: 70.75%   
[EVAL] batch:   50 | acc: 100.00%,  total acc: 71.32%   
[EVAL] batch:   51 | acc: 100.00%,  total acc: 71.88%   
[EVAL] batch:   52 | acc: 93.75%,  total acc: 72.29%   
[EVAL] batch:   53 | acc: 93.75%,  total acc: 72.69%   
[EVAL] batch:   54 | acc: 68.75%,  total acc: 72.61%   
[EVAL] batch:   55 | acc: 18.75%,  total acc: 71.65%   
[EVAL] batch:   56 | acc: 0.00%,  total acc: 70.39%   
[EVAL] batch:   57 | acc: 0.00%,  total acc: 69.18%   
[EVAL] batch:   58 | acc: 0.00%,  total acc: 68.01%   
[EVAL] batch:   59 | acc: 6.25%,  total acc: 66.98%   
[EVAL] batch:   60 | acc: 37.50%,  total acc: 66.50%   
[EVAL] batch:   61 | acc: 18.75%,  total acc: 65.73%   
[EVAL] batch:   62 | acc: 31.25%,  total acc: 65.18%   
[EVAL] batch:   63 | acc: 37.50%,  total acc: 64.75%   
[EVAL] batch:   64 | acc: 6.25%,  total acc: 63.85%   
[EVAL] batch:   65 | acc: 37.50%,  total acc: 63.45%   
[EVAL] batch:   66 | acc: 68.75%,  total acc: 63.53%   
[EVAL] batch:   67 | acc: 56.25%,  total acc: 63.42%   
[EVAL] batch:   68 | acc: 50.00%,  total acc: 63.22%   
[EVAL] batch:   69 | acc: 75.00%,  total acc: 63.39%   
[EVAL] batch:   70 | acc: 68.75%,  total acc: 63.47%   
[EVAL] batch:   71 | acc: 56.25%,  total acc: 63.37%   
[EVAL] batch:   72 | acc: 81.25%,  total acc: 63.61%   
[EVAL] batch:   73 | acc: 93.75%,  total acc: 64.02%   
[EVAL] batch:   74 | acc: 100.00%,  total acc: 64.50%   
[EVAL] batch:   75 | acc: 100.00%,  total acc: 64.97%   
[EVAL] batch:   76 | acc: 81.25%,  total acc: 65.18%   
[EVAL] batch:   77 | acc: 43.75%,  total acc: 64.90%   
[EVAL] batch:   78 | acc: 6.25%,  total acc: 64.16%   
[EVAL] batch:   79 | acc: 12.50%,  total acc: 63.52%   
[EVAL] batch:   80 | acc: 18.75%,  total acc: 62.96%   
[EVAL] batch:   81 | acc: 75.00%,  total acc: 63.11%   
[EVAL] batch:   82 | acc: 75.00%,  total acc: 63.25%   
[EVAL] batch:   83 | acc: 81.25%,  total acc: 63.47%   
[EVAL] batch:   84 | acc: 62.50%,  total acc: 63.46%   
[EVAL] batch:   85 | acc: 75.00%,  total acc: 63.59%   
[EVAL] batch:   86 | acc: 50.00%,  total acc: 63.43%   
[EVAL] batch:   87 | acc: 100.00%,  total acc: 63.85%   
[EVAL] batch:   88 | acc: 93.75%,  total acc: 64.19%   
[EVAL] batch:   89 | acc: 68.75%,  total acc: 64.24%   
[EVAL] batch:   90 | acc: 100.00%,  total acc: 64.63%   
[EVAL] batch:   91 | acc: 87.50%,  total acc: 64.88%   
[EVAL] batch:   92 | acc: 100.00%,  total acc: 65.26%   
[EVAL] batch:   93 | acc: 100.00%,  total acc: 65.62%   
[EVAL] batch:   94 | acc: 100.00%,  total acc: 65.99%   
[EVAL] batch:   95 | acc: 100.00%,  total acc: 66.34%   
[EVAL] batch:   96 | acc: 100.00%,  total acc: 66.69%   
[EVAL] batch:   97 | acc: 100.00%,  total acc: 67.03%   
[EVAL] batch:   98 | acc: 6.25%,  total acc: 66.41%   
cur_acc:  ['0.8561', '0.7115', '0.7750', '0.4972', '0.8299']
his_acc:  ['0.8561', '0.8278', '0.8073', '0.6867', '0.6641']
CurrentTrain: epoch  0, batch     0 | loss: 4.9924421
CurrentTrain: epoch  0, batch     1 | loss: 6.0936775
CurrentTrain: epoch  1, batch     0 | loss: 3.8287342
CurrentTrain: epoch  1, batch     1 | loss: 4.2539215
CurrentTrain: epoch  2, batch     0 | loss: 3.5391984
CurrentTrain: epoch  2, batch     1 | loss: 3.2844462
CurrentTrain: epoch  3, batch     0 | loss: 3.0755894
CurrentTrain: epoch  3, batch     1 | loss: 2.8620415
CurrentTrain: epoch  4, batch     0 | loss: 2.8256280
CurrentTrain: epoch  4, batch     1 | loss: 2.5286362
CurrentTrain: epoch  5, batch     0 | loss: 2.4217863
CurrentTrain: epoch  5, batch     1 | loss: 2.3241367
CurrentTrain: epoch  6, batch     0 | loss: 2.3661642
CurrentTrain: epoch  6, batch     1 | loss: 2.3201251
CurrentTrain: epoch  7, batch     0 | loss: 2.2776670
CurrentTrain: epoch  7, batch     1 | loss: 2.3991876
CurrentTrain: epoch  8, batch     0 | loss: 2.0414574
CurrentTrain: epoch  8, batch     1 | loss: 2.1668375
CurrentTrain: epoch  9, batch     0 | loss: 1.9798025
CurrentTrain: epoch  9, batch     1 | loss: 2.0223584
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, Innovatech Solutions, which has since become a leader in software development.  
Head Entity: Innovatech Solutions  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization Green Earth Initiative was established in 2010 to promote environmental awareness and sustainability practices across communities.  
Head Entity: Green Earth Initiative  
Tail Entity: 2010  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: paris , feb 23 -lrb- xinhua -rrb- yoadimnadji , 56 , died of a cardiovascular problem at midnight .
Head Entity: yoadimnadji
Tail Entity: 56
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: In a recent interview, the renowned author, Jane Doe, revealed that she is 34 years old and still has many stories to tell.  
Head Entity: Jane Doe  
Tail Entity: 34  

Relation: person age  
Context: During the family reunion, Uncle Bob proudly announced that he had just turned 70, sharing his plans for retirement.  
Head Entity: Uncle Bob  
Tail Entity: 70  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: san diego 32 new orleans 37 american football : nfl result result of the nfl match between the san diego chargers of the afc west and the new orleans saints of the nfc south at wembley here sunday :
Head Entity: nfc south
Tail Entity: new orleans saints
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: the tech giant apple inc. has announced its new team of engineers who will be working on the latest software updates for their devices, including the new members from the silicon valley startup.  
Head Entity: silicon valley startup  
Tail Entity: apple inc.  

Relation: organization members  
Context: the international soccer federation, known for its global tournaments, has recently welcomed several new teams into its ranks, including the famous club from spain.  
Head Entity: famous club from spain  
Tail Entity: international soccer federation  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how each member contributes to the collective spirit.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The famous author often discussed her deep connection to Buddhism and how it influenced her writing and personal philosophy.  
Head Entity: author  
Tail Entity: Buddhism  
Mixup data size:  224
MixupTrain:  epoch  0, batch     0 | loss: 2.7637897
MixupTrain:  epoch  0, batch     1 | loss: 2.1124864
MixupTrain:  epoch  0, batch     2 | loss: 2.2550168
MixupTrain:  epoch  0, batch     3 | loss: 2.1784174
MixupTrain:  epoch  0, batch     4 | loss: 2.1339025
MixupTrain:  epoch  0, batch     5 | loss: 2.0185902
MixupTrain:  epoch  0, batch     6 | loss: 2.1186402
MixupTrain:  epoch  0, batch     7 | loss: 2.1496596
MixupTrain:  epoch  0, batch     8 | loss: 2.1744852
MixupTrain:  epoch  0, batch     9 | loss: 1.9745098
MixupTrain:  epoch  0, batch    10 | loss: 2.2417121
MixupTrain:  epoch  0, batch    11 | loss: 2.0630398
MixupTrain:  epoch  0, batch    12 | loss: 2.0267086
MixupTrain:  epoch  0, batch    13 | loss: 1.8982857
MemoryTrain:  epoch  0, batch     0 | loss: 1.5499458
MemoryTrain:  epoch  0, batch     1 | loss: 2.0832071
MemoryTrain:  epoch  0, batch     2 | loss: 2.1012845
MemoryTrain:  epoch  0, batch     3 | loss: 2.0611696
MemoryTrain:  epoch  0, batch     4 | loss: 2.4970298
MemoryTrain:  epoch  0, batch     5 | loss: 2.4174244
MemoryTrain:  epoch  1, batch     0 | loss: 1.8106251
MemoryTrain:  epoch  1, batch     1 | loss: 2.1781411
MemoryTrain:  epoch  1, batch     2 | loss: 1.5294108
MemoryTrain:  epoch  1, batch     3 | loss: 1.9448570
MemoryTrain:  epoch  1, batch     4 | loss: 1.7167745
MemoryTrain:  epoch  1, batch     5 | loss: 1.7269136
MemoryTrain:  epoch  2, batch     0 | loss: 1.9158752
MemoryTrain:  epoch  2, batch     1 | loss: 1.4853177
MemoryTrain:  epoch  2, batch     2 | loss: 1.6797084
MemoryTrain:  epoch  2, batch     3 | loss: 1.5724214
MemoryTrain:  epoch  2, batch     4 | loss: 1.6801679
MemoryTrain:  epoch  2, batch     5 | loss: 1.6223563
MemoryTrain:  epoch  3, batch     0 | loss: 1.6204237
MemoryTrain:  epoch  3, batch     1 | loss: 1.5084920
MemoryTrain:  epoch  3, batch     2 | loss: 1.3890196
MemoryTrain:  epoch  3, batch     3 | loss: 1.4040022
MemoryTrain:  epoch  3, batch     4 | loss: 1.6380444
MemoryTrain:  epoch  3, batch     5 | loss: 1.5489563
MemoryTrain:  epoch  4, batch     0 | loss: 1.4681158
MemoryTrain:  epoch  4, batch     1 | loss: 1.4692440
MemoryTrain:  epoch  4, batch     2 | loss: 1.6928906
MemoryTrain:  epoch  4, batch     3 | loss: 1.4386990
MemoryTrain:  epoch  4, batch     4 | loss: 1.5395787
MemoryTrain:  epoch  4, batch     5 | loss: 1.4419941
MemoryTrain:  epoch  5, batch     0 | loss: 1.4017140
MemoryTrain:  epoch  5, batch     1 | loss: 1.4849675
MemoryTrain:  epoch  5, batch     2 | loss: 1.3728697
MemoryTrain:  epoch  5, batch     3 | loss: 1.3921655
MemoryTrain:  epoch  5, batch     4 | loss: 1.4905263
MemoryTrain:  epoch  5, batch     5 | loss: 1.5656937
MemoryTrain:  epoch  6, batch     0 | loss: 1.4194939
MemoryTrain:  epoch  6, batch     1 | loss: 1.4232919
MemoryTrain:  epoch  6, batch     2 | loss: 1.5026909
MemoryTrain:  epoch  6, batch     3 | loss: 1.5069661
MemoryTrain:  epoch  6, batch     4 | loss: 1.2798829
MemoryTrain:  epoch  6, batch     5 | loss: 1.3005513
MemoryTrain:  epoch  7, batch     0 | loss: 1.3079231
MemoryTrain:  epoch  7, batch     1 | loss: 1.2718680
MemoryTrain:  epoch  7, batch     2 | loss: 1.4161382
MemoryTrain:  epoch  7, batch     3 | loss: 1.5013529
MemoryTrain:  epoch  7, batch     4 | loss: 1.3377943
MemoryTrain:  epoch  7, batch     5 | loss: 1.3480395
MemoryTrain:  epoch  8, batch     0 | loss: 1.4147401
MemoryTrain:  epoch  8, batch     1 | loss: 1.2921513
MemoryTrain:  epoch  8, batch     2 | loss: 1.4013231
MemoryTrain:  epoch  8, batch     3 | loss: 1.3388021
MemoryTrain:  epoch  8, batch     4 | loss: 1.3905259
MemoryTrain:  epoch  8, batch     5 | loss: 1.3744349
MemoryTrain:  epoch  9, batch     0 | loss: 1.3347327
MemoryTrain:  epoch  9, batch     1 | loss: 1.3361616
MemoryTrain:  epoch  9, batch     2 | loss: 1.3174760
MemoryTrain:  epoch  9, batch     3 | loss: 1.4020431
MemoryTrain:  epoch  9, batch     4 | loss: 1.3204080
MemoryTrain:  epoch  9, batch     5 | loss: 1.2869580
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    8 | acc: 81.25%,  total acc: 97.92%   
[EVAL] batch:    9 | acc: 43.75%,  total acc: 92.50%   
[EVAL] batch:   10 | acc: 81.25%,  total acc: 91.48%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 91.67%   
[EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   
[EVAL] batch:   13 | acc: 62.50%,  total acc: 89.29%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   
[EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   
[EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   
[EVAL] batch:    3 | acc: 50.00%,  total acc: 64.06%   
[EVAL] batch:    4 | acc: 68.75%,  total acc: 65.00%   
[EVAL] batch:    5 | acc: 62.50%,  total acc: 64.58%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 68.75%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 72.66%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 75.69%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 76.88%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 78.98%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 79.69%   
[EVAL] batch:   12 | acc: 50.00%,  total acc: 77.40%   
[EVAL] batch:   13 | acc: 31.25%,  total acc: 74.11%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 73.75%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 72.66%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 72.79%   
[EVAL] batch:   17 | acc: 68.75%,  total acc: 72.57%   
[EVAL] batch:   18 | acc: 75.00%,  total acc: 72.70%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 73.44%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 74.70%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 75.85%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 76.90%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 77.60%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 78.50%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 79.33%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 79.86%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 80.58%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 81.25%   
[EVAL] batch:   29 | acc: 75.00%,  total acc: 81.04%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 81.05%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 81.45%   
[EVAL] batch:   32 | acc: 50.00%,  total acc: 80.49%   
[EVAL] batch:   33 | acc: 37.50%,  total acc: 79.23%   
[EVAL] batch:   34 | acc: 50.00%,  total acc: 78.39%   
[EVAL] batch:   35 | acc: 43.75%,  total acc: 77.43%   
[EVAL] batch:   36 | acc: 75.00%,  total acc: 77.36%   
[EVAL] batch:   37 | acc: 81.25%,  total acc: 77.47%   
[EVAL] batch:   38 | acc: 68.75%,  total acc: 77.24%   
[EVAL] batch:   39 | acc: 50.00%,  total acc: 76.56%   
[EVAL] batch:   40 | acc: 6.25%,  total acc: 74.85%   
[EVAL] batch:   41 | acc: 12.50%,  total acc: 73.36%   
[EVAL] batch:   42 | acc: 6.25%,  total acc: 71.80%   
[EVAL] batch:   43 | acc: 56.25%,  total acc: 71.45%   
[EVAL] batch:   44 | acc: 68.75%,  total acc: 71.39%   
[EVAL] batch:   45 | acc: 68.75%,  total acc: 71.33%   
[EVAL] batch:   46 | acc: 62.50%,  total acc: 71.14%   
[EVAL] batch:   47 | acc: 62.50%,  total acc: 70.96%   
[EVAL] batch:   48 | acc: 75.00%,  total acc: 71.05%   
[EVAL] batch:   49 | acc: 68.75%,  total acc: 71.00%   
[EVAL] batch:   50 | acc: 93.75%,  total acc: 71.45%   
[EVAL] batch:   51 | acc: 100.00%,  total acc: 72.00%   
[EVAL] batch:   52 | acc: 93.75%,  total acc: 72.41%   
[EVAL] batch:   53 | acc: 87.50%,  total acc: 72.69%   
[EVAL] batch:   54 | acc: 75.00%,  total acc: 72.73%   
[EVAL] batch:   55 | acc: 6.25%,  total acc: 71.54%   
[EVAL] batch:   56 | acc: 0.00%,  total acc: 70.29%   
[EVAL] batch:   57 | acc: 0.00%,  total acc: 69.07%   
[EVAL] batch:   58 | acc: 0.00%,  total acc: 67.90%   
[EVAL] batch:   59 | acc: 0.00%,  total acc: 66.77%   
[EVAL] batch:   60 | acc: 31.25%,  total acc: 66.19%   
[EVAL] batch:   61 | acc: 18.75%,  total acc: 65.42%   
[EVAL] batch:   62 | acc: 0.00%,  total acc: 64.38%   
[EVAL] batch:   63 | acc: 0.00%,  total acc: 63.38%   
[EVAL] batch:   64 | acc: 0.00%,  total acc: 62.40%   
[EVAL] batch:   65 | acc: 0.00%,  total acc: 61.46%   
[EVAL] batch:   66 | acc: 37.50%,  total acc: 61.10%   
[EVAL] batch:   67 | acc: 31.25%,  total acc: 60.66%   
[EVAL] batch:   68 | acc: 43.75%,  total acc: 60.42%   
[EVAL] batch:   69 | acc: 50.00%,  total acc: 60.27%   
[EVAL] batch:   70 | acc: 62.50%,  total acc: 60.30%   
[EVAL] batch:   71 | acc: 50.00%,  total acc: 60.16%   
[EVAL] batch:   72 | acc: 75.00%,  total acc: 60.36%   
[EVAL] batch:   73 | acc: 93.75%,  total acc: 60.81%   
[EVAL] batch:   74 | acc: 93.75%,  total acc: 61.25%   
[EVAL] batch:   75 | acc: 100.00%,  total acc: 61.76%   
[EVAL] batch:   76 | acc: 81.25%,  total acc: 62.01%   
[EVAL] batch:   77 | acc: 43.75%,  total acc: 61.78%   
[EVAL] batch:   78 | acc: 0.00%,  total acc: 61.00%   
[EVAL] batch:   79 | acc: 0.00%,  total acc: 60.23%   
[EVAL] batch:   80 | acc: 6.25%,  total acc: 59.57%   
[EVAL] batch:   81 | acc: 43.75%,  total acc: 59.38%   
[EVAL] batch:   82 | acc: 18.75%,  total acc: 58.89%   
[EVAL] batch:   83 | acc: 37.50%,  total acc: 58.63%   
[EVAL] batch:   84 | acc: 18.75%,  total acc: 58.16%   
[EVAL] batch:   85 | acc: 37.50%,  total acc: 57.92%   
[EVAL] batch:   86 | acc: 25.00%,  total acc: 57.54%   
[EVAL] batch:   87 | acc: 93.75%,  total acc: 57.95%   
[EVAL] batch:   88 | acc: 93.75%,  total acc: 58.36%   
[EVAL] batch:   89 | acc: 68.75%,  total acc: 58.47%   
[EVAL] batch:   90 | acc: 100.00%,  total acc: 58.93%   
[EVAL] batch:   91 | acc: 93.75%,  total acc: 59.31%   
[EVAL] batch:   92 | acc: 100.00%,  total acc: 59.74%   
[EVAL] batch:   93 | acc: 100.00%,  total acc: 60.17%   
[EVAL] batch:   94 | acc: 100.00%,  total acc: 60.59%   
[EVAL] batch:   95 | acc: 100.00%,  total acc: 61.00%   
[EVAL] batch:   96 | acc: 100.00%,  total acc: 61.40%   
[EVAL] batch:   97 | acc: 100.00%,  total acc: 61.80%   
[EVAL] batch:   98 | acc: 100.00%,  total acc: 62.18%   
[EVAL] batch:   99 | acc: 100.00%,  total acc: 62.56%   
[EVAL] batch:  100 | acc: 100.00%,  total acc: 62.93%   
[EVAL] batch:  101 | acc: 100.00%,  total acc: 63.30%   
[EVAL] batch:  102 | acc: 100.00%,  total acc: 63.65%   
[EVAL] batch:  103 | acc: 100.00%,  total acc: 64.00%   
[EVAL] batch:  104 | acc: 100.00%,  total acc: 64.35%   
[EVAL] batch:  105 | acc: 100.00%,  total acc: 64.68%   
[EVAL] batch:  106 | acc: 81.25%,  total acc: 64.84%   
[EVAL] batch:  107 | acc: 43.75%,  total acc: 64.64%   
[EVAL] batch:  108 | acc: 81.25%,  total acc: 64.79%   
[EVAL] batch:  109 | acc: 93.75%,  total acc: 65.06%   
[EVAL] batch:  110 | acc: 87.50%,  total acc: 65.26%   
[EVAL] batch:  111 | acc: 68.75%,  total acc: 65.29%   
cur_acc:  ['0.8561', '0.7115', '0.7750', '0.4972', '0.8299', '0.8929']
his_acc:  ['0.8561', '0.8278', '0.8073', '0.6867', '0.6641', '0.6529']
CurrentTrain: epoch  0, batch     0 | loss: 5.4465446
CurrentTrain: epoch  0, batch     1 | loss: 4.9260516
CurrentTrain: epoch  1, batch     0 | loss: 3.8934090
CurrentTrain: epoch  1, batch     1 | loss: 3.9505157
CurrentTrain: epoch  2, batch     0 | loss: 3.2527435
CurrentTrain: epoch  2, batch     1 | loss: 3.4396725
CurrentTrain: epoch  3, batch     0 | loss: 2.7765858
CurrentTrain: epoch  3, batch     1 | loss: 3.4087603
CurrentTrain: epoch  4, batch     0 | loss: 2.7322354
CurrentTrain: epoch  4, batch     1 | loss: 2.8392458
CurrentTrain: epoch  5, batch     0 | loss: 2.6803849
CurrentTrain: epoch  5, batch     1 | loss: 2.6213233
CurrentTrain: epoch  6, batch     0 | loss: 2.5386686
CurrentTrain: epoch  6, batch     1 | loss: 2.3950827
CurrentTrain: epoch  7, batch     0 | loss: 2.2120757
CurrentTrain: epoch  7, batch     1 | loss: 2.2537892
CurrentTrain: epoch  8, batch     0 | loss: 2.2833064
CurrentTrain: epoch  8, batch     1 | loss: 2.0047922
CurrentTrain: epoch  9, batch     0 | loss: 2.1820509
CurrentTrain: epoch  9, batch     1 | loss: 2.0996492
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, germany, on march 14, 1879, and later developed the theory of relativity.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author mark twain, known for his classic novels, was born on november 30, 1835, in florida, missouri.  
Head Entity: mark twain  
Tail Entity: november 30, 1835  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, baden-württemberg, germany.  
Head Entity: albert einstein  
Tail Entity: baden-württemberg  

Relation: person stateorprovince of birth  
Context: oprah winfrey was born on january 29, 1954, in kosciusko, mississippi.  
Head Entity: oprah winfrey  
Tail Entity: mississippi  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Sample 1:  
Relation: person parents  
Context: During the family reunion, Sarah introduced her father, who had always been a guiding force in her life, to her friends.  
Head Entity: her father  
Tail Entity: Sarah  

Sample 2:  
Relation: person parents  
Context: After the ceremony, Emily shared stories about her mother, who had inspired her to pursue a career in medicine.  
Head Entity: her mother  
Tail Entity: Emily  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst in the investment division.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john smith, a renowned author, passed away on march 5 in his residence located in los angeles, california, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: the famous musician, elena rodriguez, died tragically in a car accident on july 12 while traveling through the scenic routes of oregon, where she had spent her childhood.  
Head Entity: elena rodriguez  
Tail Entity: oregon  
Mixup data size:  254
MixupTrain:  epoch  0, batch     0 | loss: 2.8178718
MixupTrain:  epoch  0, batch     1 | loss: 2.3981726
MixupTrain:  epoch  0, batch     2 | loss: 2.3210695
MixupTrain:  epoch  0, batch     3 | loss: 2.1375024
MixupTrain:  epoch  0, batch     4 | loss: 2.6048360
MixupTrain:  epoch  0, batch     5 | loss: 2.3316946
MixupTrain:  epoch  0, batch     6 | loss: 2.3183537
MixupTrain:  epoch  0, batch     7 | loss: 2.2355158
MixupTrain:  epoch  0, batch     8 | loss: 2.0804291
MixupTrain:  epoch  0, batch     9 | loss: 2.2357533
MixupTrain:  epoch  0, batch    10 | loss: 2.4662547
MixupTrain:  epoch  0, batch    11 | loss: 1.9148386
MixupTrain:  epoch  0, batch    12 | loss: 2.6117895
MixupTrain:  epoch  0, batch    13 | loss: 2.2082412
MixupTrain:  epoch  0, batch    14 | loss: 2.1833887
MixupTrain:  epoch  0, batch    15 | loss: 1.8935591
MemoryTrain:  epoch  0, batch     0 | loss: 1.7713188
MemoryTrain:  epoch  0, batch     1 | loss: 2.3436666
MemoryTrain:  epoch  0, batch     2 | loss: 2.0490425
MemoryTrain:  epoch  0, batch     3 | loss: 1.7369860
MemoryTrain:  epoch  0, batch     4 | loss: 1.9233708
MemoryTrain:  epoch  0, batch     5 | loss: 2.8904457
MemoryTrain:  epoch  0, batch     6 | loss: 2.0769274
MemoryTrain:  epoch  1, batch     0 | loss: 1.8737938
MemoryTrain:  epoch  1, batch     1 | loss: 1.8268504
MemoryTrain:  epoch  1, batch     2 | loss: 1.5452573
MemoryTrain:  epoch  1, batch     3 | loss: 1.6173880
MemoryTrain:  epoch  1, batch     4 | loss: 2.2696109
MemoryTrain:  epoch  1, batch     5 | loss: 1.7213322
MemoryTrain:  epoch  1, batch     6 | loss: 1.8901992
MemoryTrain:  epoch  2, batch     0 | loss: 1.4040692
MemoryTrain:  epoch  2, batch     1 | loss: 1.4408705
MemoryTrain:  epoch  2, batch     2 | loss: 1.6238155
MemoryTrain:  epoch  2, batch     3 | loss: 1.4841461
MemoryTrain:  epoch  2, batch     4 | loss: 1.6495056
MemoryTrain:  epoch  2, batch     5 | loss: 1.3727556
MemoryTrain:  epoch  2, batch     6 | loss: 2.0798080
MemoryTrain:  epoch  3, batch     0 | loss: 1.4613168
MemoryTrain:  epoch  3, batch     1 | loss: 1.6342527
MemoryTrain:  epoch  3, batch     2 | loss: 1.4500725
MemoryTrain:  epoch  3, batch     3 | loss: 1.4892616
MemoryTrain:  epoch  3, batch     4 | loss: 1.3905497
MemoryTrain:  epoch  3, batch     5 | loss: 1.4263377
MemoryTrain:  epoch  3, batch     6 | loss: 1.8088751
MemoryTrain:  epoch  4, batch     0 | loss: 1.5146821
MemoryTrain:  epoch  4, batch     1 | loss: 1.8820847
MemoryTrain:  epoch  4, batch     2 | loss: 1.4660931
MemoryTrain:  epoch  4, batch     3 | loss: 1.4251882
MemoryTrain:  epoch  4, batch     4 | loss: 1.5493914
MemoryTrain:  epoch  4, batch     5 | loss: 1.3764455
MemoryTrain:  epoch  4, batch     6 | loss: 1.5093492
MemoryTrain:  epoch  5, batch     0 | loss: 1.4033704
MemoryTrain:  epoch  5, batch     1 | loss: 1.3995751
MemoryTrain:  epoch  5, batch     2 | loss: 1.6335642
MemoryTrain:  epoch  5, batch     3 | loss: 1.5555722
MemoryTrain:  epoch  5, batch     4 | loss: 1.3617393
MemoryTrain:  epoch  5, batch     5 | loss: 1.2899894
MemoryTrain:  epoch  5, batch     6 | loss: 1.4364060
MemoryTrain:  epoch  6, batch     0 | loss: 1.2837758
MemoryTrain:  epoch  6, batch     1 | loss: 1.2863083
MemoryTrain:  epoch  6, batch     2 | loss: 1.3784118
MemoryTrain:  epoch  6, batch     3 | loss: 1.4463581
MemoryTrain:  epoch  6, batch     4 | loss: 1.3665702
MemoryTrain:  epoch  6, batch     5 | loss: 1.5352035
MemoryTrain:  epoch  6, batch     6 | loss: 1.2759969
MemoryTrain:  epoch  7, batch     0 | loss: 1.2807513
MemoryTrain:  epoch  7, batch     1 | loss: 1.3794882
MemoryTrain:  epoch  7, batch     2 | loss: 1.3746514
MemoryTrain:  epoch  7, batch     3 | loss: 1.3288944
MemoryTrain:  epoch  7, batch     4 | loss: 1.3391374
MemoryTrain:  epoch  7, batch     5 | loss: 1.4743152
MemoryTrain:  epoch  7, batch     6 | loss: 1.4531626
MemoryTrain:  epoch  8, batch     0 | loss: 1.3201398
MemoryTrain:  epoch  8, batch     1 | loss: 1.3762765
MemoryTrain:  epoch  8, batch     2 | loss: 1.2839075
MemoryTrain:  epoch  8, batch     3 | loss: 1.2839363
MemoryTrain:  epoch  8, batch     4 | loss: 1.3775415
MemoryTrain:  epoch  8, batch     5 | loss: 1.4919543
MemoryTrain:  epoch  8, batch     6 | loss: 1.3055065
MemoryTrain:  epoch  9, batch     0 | loss: 1.3455920
MemoryTrain:  epoch  9, batch     1 | loss: 1.2277833
MemoryTrain:  epoch  9, batch     2 | loss: 1.2684734
MemoryTrain:  epoch  9, batch     3 | loss: 1.2702301
MemoryTrain:  epoch  9, batch     4 | loss: 1.2424610
MemoryTrain:  epoch  9, batch     5 | loss: 1.3789538
MemoryTrain:  epoch  9, batch     6 | loss: 1.3113635
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   
[EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:    3 | acc: 68.75%,  total acc: 64.06%   
[EVAL] batch:    4 | acc: 50.00%,  total acc: 61.25%   
[EVAL] batch:    5 | acc: 75.00%,  total acc: 63.54%   
[EVAL] batch:    6 | acc: 75.00%,  total acc: 65.18%   
[EVAL] batch:    7 | acc: 87.50%,  total acc: 67.97%   
[EVAL] batch:    8 | acc: 81.25%,  total acc: 69.44%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 71.25%   
[EVAL] batch:   10 | acc: 87.50%,  total acc: 72.73%   
[EVAL] batch:   11 | acc: 68.75%,  total acc: 72.40%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 74.04%   
[EVAL] batch:   13 | acc: 50.00%,  total acc: 72.32%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   
[EVAL] batch:    2 | acc: 75.00%,  total acc: 81.25%   
[EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   
[EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 79.46%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 84.03%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 84.38%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 85.94%   
[EVAL] batch:   12 | acc: 50.00%,  total acc: 83.17%   
[EVAL] batch:   13 | acc: 43.75%,  total acc: 80.36%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 80.00%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 78.52%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 78.31%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 77.43%   
[EVAL] batch:   18 | acc: 68.75%,  total acc: 76.97%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 77.19%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 78.27%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 78.98%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 79.89%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 80.47%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 81.25%   
[EVAL] batch:   25 | acc: 93.75%,  total acc: 81.73%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 82.18%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 82.81%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 83.41%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 83.33%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 83.27%   
[EVAL] batch:   31 | acc: 100.00%,  total acc: 83.79%   
[EVAL] batch:   32 | acc: 43.75%,  total acc: 82.58%   
[EVAL] batch:   33 | acc: 43.75%,  total acc: 81.43%   
[EVAL] batch:   34 | acc: 50.00%,  total acc: 80.54%   
[EVAL] batch:   35 | acc: 50.00%,  total acc: 79.69%   
[EVAL] batch:   36 | acc: 75.00%,  total acc: 79.56%   
[EVAL] batch:   37 | acc: 75.00%,  total acc: 79.44%   
[EVAL] batch:   38 | acc: 81.25%,  total acc: 79.49%   
[EVAL] batch:   39 | acc: 43.75%,  total acc: 78.59%   
[EVAL] batch:   40 | acc: 0.00%,  total acc: 76.68%   
[EVAL] batch:   41 | acc: 12.50%,  total acc: 75.15%   
[EVAL] batch:   42 | acc: 6.25%,  total acc: 73.55%   
[EVAL] batch:   43 | acc: 50.00%,  total acc: 73.01%   
[EVAL] batch:   44 | acc: 56.25%,  total acc: 72.64%   
[EVAL] batch:   45 | acc: 56.25%,  total acc: 72.28%   
[EVAL] batch:   46 | acc: 50.00%,  total acc: 71.81%   
[EVAL] batch:   47 | acc: 37.50%,  total acc: 71.09%   
[EVAL] batch:   48 | acc: 68.75%,  total acc: 71.05%   
[EVAL] batch:   49 | acc: 62.50%,  total acc: 70.88%   
[EVAL] batch:   50 | acc: 100.00%,  total acc: 71.45%   
[EVAL] batch:   51 | acc: 100.00%,  total acc: 72.00%   
[EVAL] batch:   52 | acc: 93.75%,  total acc: 72.41%   
[EVAL] batch:   53 | acc: 93.75%,  total acc: 72.80%   
[EVAL] batch:   54 | acc: 68.75%,  total acc: 72.73%   
[EVAL] batch:   55 | acc: 6.25%,  total acc: 71.54%   
[EVAL] batch:   56 | acc: 12.50%,  total acc: 70.50%   
[EVAL] batch:   57 | acc: 0.00%,  total acc: 69.29%   
[EVAL] batch:   58 | acc: 0.00%,  total acc: 68.11%   
[EVAL] batch:   59 | acc: 0.00%,  total acc: 66.98%   
[EVAL] batch:   60 | acc: 25.00%,  total acc: 66.29%   
[EVAL] batch:   61 | acc: 18.75%,  total acc: 65.52%   
[EVAL] batch:   62 | acc: 0.00%,  total acc: 64.48%   
[EVAL] batch:   63 | acc: 6.25%,  total acc: 63.57%   
[EVAL] batch:   64 | acc: 0.00%,  total acc: 62.60%   
[EVAL] batch:   65 | acc: 0.00%,  total acc: 61.65%   
[EVAL] batch:   66 | acc: 50.00%,  total acc: 61.47%   
[EVAL] batch:   67 | acc: 43.75%,  total acc: 61.21%   
[EVAL] batch:   68 | acc: 43.75%,  total acc: 60.96%   
[EVAL] batch:   69 | acc: 56.25%,  total acc: 60.89%   
[EVAL] batch:   70 | acc: 68.75%,  total acc: 61.00%   
[EVAL] batch:   71 | acc: 56.25%,  total acc: 60.94%   
[EVAL] batch:   72 | acc: 62.50%,  total acc: 60.96%   
[EVAL] batch:   73 | acc: 100.00%,  total acc: 61.49%   
[EVAL] batch:   74 | acc: 93.75%,  total acc: 61.92%   
[EVAL] batch:   75 | acc: 100.00%,  total acc: 62.42%   
[EVAL] batch:   76 | acc: 81.25%,  total acc: 62.66%   
[EVAL] batch:   77 | acc: 37.50%,  total acc: 62.34%   
[EVAL] batch:   78 | acc: 0.00%,  total acc: 61.55%   
[EVAL] batch:   79 | acc: 0.00%,  total acc: 60.78%   
[EVAL] batch:   80 | acc: 18.75%,  total acc: 60.26%   
[EVAL] batch:   81 | acc: 62.50%,  total acc: 60.29%   
[EVAL] batch:   82 | acc: 43.75%,  total acc: 60.09%   
[EVAL] batch:   83 | acc: 56.25%,  total acc: 60.04%   
[EVAL] batch:   84 | acc: 31.25%,  total acc: 59.71%   
[EVAL] batch:   85 | acc: 56.25%,  total acc: 59.67%   
[EVAL] batch:   86 | acc: 43.75%,  total acc: 59.48%   
[EVAL] batch:   87 | acc: 81.25%,  total acc: 59.73%   
[EVAL] batch:   88 | acc: 75.00%,  total acc: 59.90%   
[EVAL] batch:   89 | acc: 18.75%,  total acc: 59.44%   
[EVAL] batch:   90 | acc: 62.50%,  total acc: 59.48%   
[EVAL] batch:   91 | acc: 56.25%,  total acc: 59.44%   
[EVAL] batch:   92 | acc: 100.00%,  total acc: 59.88%   
[EVAL] batch:   93 | acc: 100.00%,  total acc: 60.31%   
[EVAL] batch:   94 | acc: 100.00%,  total acc: 60.72%   
[EVAL] batch:   95 | acc: 100.00%,  total acc: 61.13%   
[EVAL] batch:   96 | acc: 100.00%,  total acc: 61.53%   
[EVAL] batch:   97 | acc: 100.00%,  total acc: 61.93%   
[EVAL] batch:   98 | acc: 100.00%,  total acc: 62.31%   
[EVAL] batch:   99 | acc: 100.00%,  total acc: 62.69%   
[EVAL] batch:  100 | acc: 100.00%,  total acc: 63.06%   
[EVAL] batch:  101 | acc: 100.00%,  total acc: 63.42%   
[EVAL] batch:  102 | acc: 100.00%,  total acc: 63.77%   
[EVAL] batch:  103 | acc: 100.00%,  total acc: 64.12%   
[EVAL] batch:  104 | acc: 100.00%,  total acc: 64.46%   
[EVAL] batch:  105 | acc: 100.00%,  total acc: 64.80%   
[EVAL] batch:  106 | acc: 75.00%,  total acc: 64.89%   
[EVAL] batch:  107 | acc: 37.50%,  total acc: 64.64%   
[EVAL] batch:  108 | acc: 62.50%,  total acc: 64.62%   
[EVAL] batch:  109 | acc: 93.75%,  total acc: 64.89%   
[EVAL] batch:  110 | acc: 75.00%,  total acc: 64.98%   
[EVAL] batch:  111 | acc: 75.00%,  total acc: 65.07%   
[EVAL] batch:  112 | acc: 62.50%,  total acc: 65.04%   
[EVAL] batch:  113 | acc: 68.75%,  total acc: 65.08%   
[EVAL] batch:  114 | acc: 62.50%,  total acc: 65.05%   
[EVAL] batch:  115 | acc: 50.00%,  total acc: 64.92%   
[EVAL] batch:  116 | acc: 68.75%,  total acc: 64.96%   
[EVAL] batch:  117 | acc: 62.50%,  total acc: 64.94%   
[EVAL] batch:  118 | acc: 81.25%,  total acc: 65.07%   
[EVAL] batch:  119 | acc: 93.75%,  total acc: 65.31%   
[EVAL] batch:  120 | acc: 81.25%,  total acc: 65.44%   
[EVAL] batch:  121 | acc: 87.50%,  total acc: 65.62%   
[EVAL] batch:  122 | acc: 75.00%,  total acc: 65.70%   
[EVAL] batch:  123 | acc: 81.25%,  total acc: 65.83%   
[EVAL] batch:  124 | acc: 93.75%,  total acc: 66.05%   
[EVAL] batch:  125 | acc: 31.25%,  total acc: 65.77%   
cur_acc:  ['0.8561', '0.7115', '0.7750', '0.4972', '0.8299', '0.8929', '0.7232']
his_acc:  ['0.8561', '0.8278', '0.8073', '0.6867', '0.6641', '0.6529', '0.6577']
CurrentTrain: epoch  0, batch     0 | loss: 5.6496372
CurrentTrain: epoch  0, batch     1 | loss: 5.3723550
CurrentTrain: epoch  1, batch     0 | loss: 3.6764703
CurrentTrain: epoch  1, batch     1 | loss: 5.9414582
CurrentTrain: epoch  2, batch     0 | loss: 4.0423923
CurrentTrain: epoch  2, batch     1 | loss: 3.8413200
CurrentTrain: epoch  3, batch     0 | loss: 3.8862734
CurrentTrain: epoch  3, batch     1 | loss: 2.9714940
CurrentTrain: epoch  4, batch     0 | loss: 3.0423610
CurrentTrain: epoch  4, batch     1 | loss: 3.8146443
CurrentTrain: epoch  5, batch     0 | loss: 3.4049711
CurrentTrain: epoch  5, batch     1 | loss: 2.7426715
CurrentTrain: epoch  6, batch     0 | loss: 2.6511083
CurrentTrain: epoch  6, batch     1 | loss: 3.5969102
CurrentTrain: epoch  7, batch     0 | loss: 3.1260846
CurrentTrain: epoch  7, batch     1 | loss: 2.3712833
CurrentTrain: epoch  8, batch     0 | loss: 2.6118426
CurrentTrain: epoch  8, batch     1 | loss: 2.2333446
CurrentTrain: epoch  9, batch     0 | loss: 2.3906629
CurrentTrain: epoch  9, batch     1 | loss: 2.5908306
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun kapoor has always been proud of his indian heritage and culture.  
Head Entity: arjun kapoor  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she revealed that despite living in the united states for over a decade, her roots trace back to the beautiful landscapes of italy, where she was born.  
Head Entity: she  
Tail Entity: italy  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.techinnovators.com for more information on their latest products.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For updates and news, check out the website of Green Earth Initiative at http://www.greenearth.org.  
Head Entity: Green Earth Initiative  
Tail Entity: http://www.greenearth.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has recently acquired a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple inc.  

Relation: organization shareholders  
Context: the investment firm blackrock has increased its holdings in the renewable energy company sunrun.  
Head Entity: sunrun  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: in 1953 , five years after the state was established , the jnf was dissolved and re-organized as an israeli company .
Head Entity: jnf
Tail Entity: 1953
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The board of directors announced that the nonprofit organization was officially dissolved in 2020 due to financial difficulties.  
Head Entity: nonprofit organization  
Tail Entity: 2020  

Relation: organization dissolved  
Context: After decades of service, the local charity was dissolved in 2018, marking the end of an era for community support.  
Head Entity: local charity  
Tail Entity: 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: born in new york , she inherited most of her fortune from her father , ted arison , who founded carnival cruise lines and also owned the miami heat basketball team .
Head Entity: carnival cruise lines
Tail Entity: ted arison
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: in 1975, steve jobs and steve wozniak started apple inc., which has since become one of the most valuable companies in the world.  
Head Entity: apple inc.  
Tail Entity: steve jobs  

Relation: organization founded by  
Context: the famous fashion brand gucci was established in florence by guccio gucci, who initially started as a luggage manufacturer.  
Head Entity: gucci  
Tail Entity: guccio gucci  
Mixup data size:  284
MixupTrain:  epoch  0, batch     0 | loss: 2.0837574
MixupTrain:  epoch  0, batch     1 | loss: 2.2908537
MixupTrain:  epoch  0, batch     2 | loss: 2.1165543
MixupTrain:  epoch  0, batch     3 | loss: 2.2536700
MixupTrain:  epoch  0, batch     4 | loss: 1.8695805
MixupTrain:  epoch  0, batch     5 | loss: 1.8075047
MixupTrain:  epoch  0, batch     6 | loss: 1.9000269
MixupTrain:  epoch  0, batch     7 | loss: 2.2268996
MixupTrain:  epoch  0, batch     8 | loss: 2.3498271
MixupTrain:  epoch  0, batch     9 | loss: 1.9630815
MixupTrain:  epoch  0, batch    10 | loss: 2.1051142
MixupTrain:  epoch  0, batch    11 | loss: 1.9730169
MixupTrain:  epoch  0, batch    12 | loss: 2.0749764
MixupTrain:  epoch  0, batch    13 | loss: 2.2681863
MixupTrain:  epoch  0, batch    14 | loss: 2.2698119
MixupTrain:  epoch  0, batch    15 | loss: 2.3676653
MixupTrain:  epoch  0, batch    16 | loss: 2.1454124
MixupTrain:  epoch  0, batch    17 | loss: 1.9937938
MemoryTrain:  epoch  0, batch     0 | loss: 1.6818329
MemoryTrain:  epoch  0, batch     1 | loss: 2.7496314
MemoryTrain:  epoch  0, batch     2 | loss: 1.7992649
MemoryTrain:  epoch  0, batch     3 | loss: 2.3727744
MemoryTrain:  epoch  0, batch     4 | loss: 2.0883405
MemoryTrain:  epoch  0, batch     5 | loss: 2.0783491
MemoryTrain:  epoch  0, batch     6 | loss: 2.5296426
MemoryTrain:  epoch  0, batch     7 | loss: 3.2487926
MemoryTrain:  epoch  1, batch     0 | loss: 2.6518610
MemoryTrain:  epoch  1, batch     1 | loss: 1.6654615
MemoryTrain:  epoch  1, batch     2 | loss: 2.0964715
MemoryTrain:  epoch  1, batch     3 | loss: 1.4270248
MemoryTrain:  epoch  1, batch     4 | loss: 2.6372459
MemoryTrain:  epoch  1, batch     5 | loss: 1.6400323
MemoryTrain:  epoch  1, batch     6 | loss: 2.2770922
MemoryTrain:  epoch  1, batch     7 | loss: 1.9495406
MemoryTrain:  epoch  2, batch     0 | loss: 2.2426417
MemoryTrain:  epoch  2, batch     1 | loss: 2.3651097
MemoryTrain:  epoch  2, batch     2 | loss: 1.3048702
MemoryTrain:  epoch  2, batch     3 | loss: 2.0295074
MemoryTrain:  epoch  2, batch     4 | loss: 1.6066380
MemoryTrain:  epoch  2, batch     5 | loss: 1.8917687
MemoryTrain:  epoch  2, batch     6 | loss: 1.6563863
MemoryTrain:  epoch  2, batch     7 | loss: 1.3939533
MemoryTrain:  epoch  3, batch     0 | loss: 2.1190815
MemoryTrain:  epoch  3, batch     1 | loss: 1.5318713
MemoryTrain:  epoch  3, batch     2 | loss: 1.6564999
MemoryTrain:  epoch  3, batch     3 | loss: 1.6921811
MemoryTrain:  epoch  3, batch     4 | loss: 1.2951584
MemoryTrain:  epoch  3, batch     5 | loss: 1.5798037
MemoryTrain:  epoch  3, batch     6 | loss: 1.7289267
MemoryTrain:  epoch  3, batch     7 | loss: 1.3892230
MemoryTrain:  epoch  4, batch     0 | loss: 1.3540281
MemoryTrain:  epoch  4, batch     1 | loss: 1.6795199
MemoryTrain:  epoch  4, batch     2 | loss: 1.3083419
MemoryTrain:  epoch  4, batch     3 | loss: 1.4036381
MemoryTrain:  epoch  4, batch     4 | loss: 1.5487142
MemoryTrain:  epoch  4, batch     5 | loss: 1.6215816
MemoryTrain:  epoch  4, batch     6 | loss: 1.4320204
MemoryTrain:  epoch  4, batch     7 | loss: 1.8541144
MemoryTrain:  epoch  5, batch     0 | loss: 1.4915804
MemoryTrain:  epoch  5, batch     1 | loss: 1.3992257
MemoryTrain:  epoch  5, batch     2 | loss: 1.7012579
MemoryTrain:  epoch  5, batch     3 | loss: 1.5538440
MemoryTrain:  epoch  5, batch     4 | loss: 1.5388446
MemoryTrain:  epoch  5, batch     5 | loss: 1.2869754
MemoryTrain:  epoch  5, batch     6 | loss: 1.3374070
MemoryTrain:  epoch  5, batch     7 | loss: 1.2694495
MemoryTrain:  epoch  6, batch     0 | loss: 1.3362105
MemoryTrain:  epoch  6, batch     1 | loss: 1.3630607
MemoryTrain:  epoch  6, batch     2 | loss: 1.3530418
MemoryTrain:  epoch  6, batch     3 | loss: 1.2464107
MemoryTrain:  epoch  6, batch     4 | loss: 1.7605118
MemoryTrain:  epoch  6, batch     5 | loss: 1.3857667
MemoryTrain:  epoch  6, batch     6 | loss: 1.5204098
MemoryTrain:  epoch  6, batch     7 | loss: 1.3008877
MemoryTrain:  epoch  7, batch     0 | loss: 1.3210024
MemoryTrain:  epoch  7, batch     1 | loss: 1.4896121
MemoryTrain:  epoch  7, batch     2 | loss: 1.3296907
MemoryTrain:  epoch  7, batch     3 | loss: 1.3548956
MemoryTrain:  epoch  7, batch     4 | loss: 1.3334308
MemoryTrain:  epoch  7, batch     5 | loss: 1.2819743
MemoryTrain:  epoch  7, batch     6 | loss: 1.3107028
MemoryTrain:  epoch  7, batch     7 | loss: 1.4438579
MemoryTrain:  epoch  8, batch     0 | loss: 1.2628711
MemoryTrain:  epoch  8, batch     1 | loss: 1.3528237
MemoryTrain:  epoch  8, batch     2 | loss: 1.3977616
MemoryTrain:  epoch  8, batch     3 | loss: 1.3404022
MemoryTrain:  epoch  8, batch     4 | loss: 1.3174891
MemoryTrain:  epoch  8, batch     5 | loss: 1.2883849
MemoryTrain:  epoch  8, batch     6 | loss: 1.2907159
MemoryTrain:  epoch  8, batch     7 | loss: 1.2982544
MemoryTrain:  epoch  9, batch     0 | loss: 1.3507271
MemoryTrain:  epoch  9, batch     1 | loss: 1.2426708
MemoryTrain:  epoch  9, batch     2 | loss: 1.2229997
MemoryTrain:  epoch  9, batch     3 | loss: 1.3663434
MemoryTrain:  epoch  9, batch     4 | loss: 1.2465209
MemoryTrain:  epoch  9, batch     5 | loss: 1.2589905
MemoryTrain:  epoch  9, batch     6 | loss: 1.2508543
MemoryTrain:  epoch  9, batch     7 | loss: 1.2877448
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   
[EVAL] batch:    2 | acc: 62.50%,  total acc: 83.33%   
[EVAL] batch:    3 | acc: 31.25%,  total acc: 70.31%   
[EVAL] batch:    4 | acc: 37.50%,  total acc: 63.75%   
[EVAL] batch:    5 | acc: 50.00%,  total acc: 61.46%   
[EVAL] batch:    6 | acc: 25.00%,  total acc: 56.25%   
[EVAL] batch:    7 | acc: 6.25%,  total acc: 50.00%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   
[EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   
[EVAL] batch:    2 | acc: 43.75%,  total acc: 47.92%   
[EVAL] batch:    3 | acc: 25.00%,  total acc: 42.19%   
[EVAL] batch:    4 | acc: 25.00%,  total acc: 38.75%   
[EVAL] batch:    5 | acc: 50.00%,  total acc: 40.62%   
[EVAL] batch:    6 | acc: 75.00%,  total acc: 45.54%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 52.34%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 57.64%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 60.62%   
[EVAL] batch:   10 | acc: 81.25%,  total acc: 62.50%   
[EVAL] batch:   11 | acc: 81.25%,  total acc: 64.06%   
[EVAL] batch:   12 | acc: 50.00%,  total acc: 62.98%   
[EVAL] batch:   13 | acc: 37.50%,  total acc: 61.16%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 62.08%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 61.72%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 62.50%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:   18 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 63.75%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 65.48%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 66.76%   
[EVAL] batch:   22 | acc: 93.75%,  total acc: 67.93%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 69.01%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 70.25%   
[EVAL] batch:   25 | acc: 93.75%,  total acc: 71.15%   
[EVAL] batch:   26 | acc: 87.50%,  total acc: 71.76%   
[EVAL] batch:   27 | acc: 93.75%,  total acc: 72.54%   
[EVAL] batch:   28 | acc: 93.75%,  total acc: 73.28%   
[EVAL] batch:   29 | acc: 68.75%,  total acc: 73.12%   
[EVAL] batch:   30 | acc: 81.25%,  total acc: 73.39%   
[EVAL] batch:   31 | acc: 87.50%,  total acc: 73.83%   
[EVAL] batch:   32 | acc: 43.75%,  total acc: 72.92%   
[EVAL] batch:   33 | acc: 50.00%,  total acc: 72.24%   
[EVAL] batch:   34 | acc: 50.00%,  total acc: 71.61%   
[EVAL] batch:   35 | acc: 43.75%,  total acc: 70.83%   
[EVAL] batch:   36 | acc: 87.50%,  total acc: 71.28%   
[EVAL] batch:   37 | acc: 87.50%,  total acc: 71.71%   
[EVAL] batch:   38 | acc: 75.00%,  total acc: 71.79%   
[EVAL] batch:   39 | acc: 56.25%,  total acc: 71.41%   
[EVAL] batch:   40 | acc: 18.75%,  total acc: 70.12%   
[EVAL] batch:   41 | acc: 18.75%,  total acc: 68.90%   
[EVAL] batch:   42 | acc: 6.25%,  total acc: 67.44%   
[EVAL] batch:   43 | acc: 62.50%,  total acc: 67.33%   
[EVAL] batch:   44 | acc: 62.50%,  total acc: 67.22%   
[EVAL] batch:   45 | acc: 37.50%,  total acc: 66.58%   
[EVAL] batch:   46 | acc: 37.50%,  total acc: 65.96%   
[EVAL] batch:   47 | acc: 18.75%,  total acc: 64.97%   
[EVAL] batch:   48 | acc: 62.50%,  total acc: 64.92%   
[EVAL] batch:   49 | acc: 37.50%,  total acc: 64.38%   
[EVAL] batch:   50 | acc: 93.75%,  total acc: 64.95%   
[EVAL] batch:   51 | acc: 100.00%,  total acc: 65.62%   
[EVAL] batch:   52 | acc: 93.75%,  total acc: 66.16%   
[EVAL] batch:   53 | acc: 87.50%,  total acc: 66.55%   
[EVAL] batch:   54 | acc: 62.50%,  total acc: 66.48%   
[EVAL] batch:   55 | acc: 6.25%,  total acc: 65.40%   
[EVAL] batch:   56 | acc: 12.50%,  total acc: 64.47%   
[EVAL] batch:   57 | acc: 6.25%,  total acc: 63.47%   
[EVAL] batch:   58 | acc: 0.00%,  total acc: 62.39%   
[EVAL] batch:   59 | acc: 0.00%,  total acc: 61.35%   
[EVAL] batch:   60 | acc: 25.00%,  total acc: 60.76%   
[EVAL] batch:   61 | acc: 18.75%,  total acc: 60.08%   
[EVAL] batch:   62 | acc: 0.00%,  total acc: 59.13%   
[EVAL] batch:   63 | acc: 0.00%,  total acc: 58.20%   
[EVAL] batch:   64 | acc: 0.00%,  total acc: 57.31%   
[EVAL] batch:   65 | acc: 0.00%,  total acc: 56.44%   
[EVAL] batch:   66 | acc: 56.25%,  total acc: 56.44%   
[EVAL] batch:   67 | acc: 37.50%,  total acc: 56.16%   
[EVAL] batch:   68 | acc: 50.00%,  total acc: 56.07%   
[EVAL] batch:   69 | acc: 68.75%,  total acc: 56.25%   
[EVAL] batch:   70 | acc: 81.25%,  total acc: 56.60%   
[EVAL] batch:   71 | acc: 62.50%,  total acc: 56.68%   
[EVAL] batch:   72 | acc: 68.75%,  total acc: 56.85%   
[EVAL] batch:   73 | acc: 93.75%,  total acc: 57.35%   
[EVAL] batch:   74 | acc: 93.75%,  total acc: 57.83%   
[EVAL] batch:   75 | acc: 100.00%,  total acc: 58.39%   
[EVAL] batch:   76 | acc: 81.25%,  total acc: 58.69%   
[EVAL] batch:   77 | acc: 37.50%,  total acc: 58.41%   
[EVAL] batch:   78 | acc: 0.00%,  total acc: 57.67%   
[EVAL] batch:   79 | acc: 0.00%,  total acc: 56.95%   
[EVAL] batch:   80 | acc: 18.75%,  total acc: 56.48%   
[EVAL] batch:   81 | acc: 50.00%,  total acc: 56.40%   
[EVAL] batch:   82 | acc: 37.50%,  total acc: 56.17%   
[EVAL] batch:   83 | acc: 56.25%,  total acc: 56.18%   
[EVAL] batch:   84 | acc: 31.25%,  total acc: 55.88%   
[EVAL] batch:   85 | acc: 62.50%,  total acc: 55.96%   
[EVAL] batch:   86 | acc: 37.50%,  total acc: 55.75%   
[EVAL] batch:   87 | acc: 87.50%,  total acc: 56.11%   
[EVAL] batch:   88 | acc: 87.50%,  total acc: 56.46%   
[EVAL] batch:   89 | acc: 31.25%,  total acc: 56.18%   
[EVAL] batch:   90 | acc: 62.50%,  total acc: 56.25%   
[EVAL] batch:   91 | acc: 62.50%,  total acc: 56.32%   
[EVAL] batch:   92 | acc: 100.00%,  total acc: 56.79%   
[EVAL] batch:   93 | acc: 100.00%,  total acc: 57.25%   
[EVAL] batch:   94 | acc: 100.00%,  total acc: 57.70%   
[EVAL] batch:   95 | acc: 100.00%,  total acc: 58.14%   
[EVAL] batch:   96 | acc: 100.00%,  total acc: 58.57%   
[EVAL] batch:   97 | acc: 100.00%,  total acc: 58.99%   
[EVAL] batch:   98 | acc: 93.75%,  total acc: 59.34%   
[EVAL] batch:   99 | acc: 100.00%,  total acc: 59.75%   
[EVAL] batch:  100 | acc: 100.00%,  total acc: 60.15%   
[EVAL] batch:  101 | acc: 100.00%,  total acc: 60.54%   
[EVAL] batch:  102 | acc: 100.00%,  total acc: 60.92%   
[EVAL] batch:  103 | acc: 100.00%,  total acc: 61.30%   
[EVAL] batch:  104 | acc: 100.00%,  total acc: 61.67%   
[EVAL] batch:  105 | acc: 100.00%,  total acc: 62.03%   
[EVAL] batch:  106 | acc: 75.00%,  total acc: 62.15%   
[EVAL] batch:  107 | acc: 25.00%,  total acc: 61.81%   
[EVAL] batch:  108 | acc: 31.25%,  total acc: 61.53%   
[EVAL] batch:  109 | acc: 87.50%,  total acc: 61.76%   
[EVAL] batch:  110 | acc: 62.50%,  total acc: 61.77%   
[EVAL] batch:  111 | acc: 75.00%,  total acc: 61.89%   
[EVAL] batch:  112 | acc: 62.50%,  total acc: 61.89%   
[EVAL] batch:  113 | acc: 62.50%,  total acc: 61.90%   
[EVAL] batch:  114 | acc: 62.50%,  total acc: 61.90%   
[EVAL] batch:  115 | acc: 50.00%,  total acc: 61.80%   
[EVAL] batch:  116 | acc: 75.00%,  total acc: 61.91%   
[EVAL] batch:  117 | acc: 37.50%,  total acc: 61.71%   
[EVAL] batch:  118 | acc: 62.50%,  total acc: 61.71%   
[EVAL] batch:  119 | acc: 93.75%,  total acc: 61.98%   
[EVAL] batch:  120 | acc: 87.50%,  total acc: 62.19%   
[EVAL] batch:  121 | acc: 81.25%,  total acc: 62.35%   
[EVAL] batch:  122 | acc: 81.25%,  total acc: 62.50%   
[EVAL] batch:  123 | acc: 81.25%,  total acc: 62.65%   
[EVAL] batch:  124 | acc: 100.00%,  total acc: 62.95%   
[EVAL] batch:  125 | acc: 93.75%,  total acc: 63.19%   
[EVAL] batch:  126 | acc: 93.75%,  total acc: 63.44%   
[EVAL] batch:  127 | acc: 75.00%,  total acc: 63.53%   
[EVAL] batch:  128 | acc: 50.00%,  total acc: 63.42%   
[EVAL] batch:  129 | acc: 37.50%,  total acc: 63.22%   
[EVAL] batch:  130 | acc: 43.75%,  total acc: 63.07%   
[EVAL] batch:  131 | acc: 37.50%,  total acc: 62.88%   
[EVAL] batch:  132 | acc: 18.75%,  total acc: 62.55%   
cur_acc:  ['0.8561', '0.7115', '0.7750', '0.4972', '0.8299', '0.8929', '0.7232', '0.5000']
his_acc:  ['0.8561', '0.8278', '0.8073', '0.6867', '0.6641', '0.6529', '0.6577', '0.6255']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
CurrentTrain: epoch  0, batch     0 | loss: 13.4145927
CurrentTrain: epoch  0, batch     1 | loss: 13.1803417
CurrentTrain: epoch  0, batch     2 | loss: 13.1904888
CurrentTrain: epoch  0, batch     3 | loss: 12.7580853
CurrentTrain: epoch  0, batch     4 | loss: 12.8298559
CurrentTrain: epoch  0, batch     5 | loss: 12.5892200
CurrentTrain: epoch  0, batch     6 | loss: 12.6325083
CurrentTrain: epoch  0, batch     7 | loss: 12.3823147
CurrentTrain: epoch  0, batch     8 | loss: 12.3544559
CurrentTrain: epoch  0, batch     9 | loss: 11.9195509
CurrentTrain: epoch  0, batch    10 | loss: 12.1716614
CurrentTrain: epoch  0, batch    11 | loss: 12.1267529
CurrentTrain: epoch  0, batch    12 | loss: 11.9558506
CurrentTrain: epoch  0, batch    13 | loss: 11.9273520
CurrentTrain: epoch  0, batch    14 | loss: 11.9801083
CurrentTrain: epoch  0, batch    15 | loss: 11.6521530
CurrentTrain: epoch  0, batch    16 | loss: 11.9204845
CurrentTrain: epoch  0, batch    17 | loss: 11.6865482
CurrentTrain: epoch  0, batch    18 | loss: 11.4389429
CurrentTrain: epoch  0, batch    19 | loss: 11.2068043
CurrentTrain: epoch  0, batch    20 | loss: 11.6043091
CurrentTrain: epoch  0, batch    21 | loss: 11.5327587
CurrentTrain: epoch  0, batch    22 | loss: 11.6289291
CurrentTrain: epoch  0, batch    23 | loss: 11.2996521
CurrentTrain: epoch  0, batch    24 | loss: 11.2792702
CurrentTrain: epoch  0, batch    25 | loss: 11.3787746
CurrentTrain: epoch  0, batch    26 | loss: 10.9020529
CurrentTrain: epoch  0, batch    27 | loss: 11.1597462
CurrentTrain: epoch  0, batch    28 | loss: 10.8274574
CurrentTrain: epoch  0, batch    29 | loss: 10.7822285
CurrentTrain: epoch  0, batch    30 | loss: 10.9115944
CurrentTrain: epoch  0, batch    31 | loss: 10.5592709
CurrentTrain: epoch  0, batch    32 | loss: 10.5400944
CurrentTrain: epoch  0, batch    33 | loss: 10.9501963
CurrentTrain: epoch  0, batch    34 | loss: 10.8069086
CurrentTrain: epoch  0, batch    35 | loss: 10.7870016
CurrentTrain: epoch  0, batch    36 | loss: 10.4199467
CurrentTrain: epoch  0, batch    37 | loss: 10.5326118
CurrentTrain: epoch  1, batch     0 | loss: 10.3850708
CurrentTrain: epoch  1, batch     1 | loss: 10.4228954
CurrentTrain: epoch  1, batch     2 | loss: 10.0657310
CurrentTrain: epoch  1, batch     3 | loss: 9.7535362
CurrentTrain: epoch  1, batch     4 | loss: 9.8940096
CurrentTrain: epoch  1, batch     5 | loss: 9.8704033
CurrentTrain: epoch  1, batch     6 | loss: 10.4052849
CurrentTrain: epoch  1, batch     7 | loss: 10.0472574
CurrentTrain: epoch  1, batch     8 | loss: 9.9323730
CurrentTrain: epoch  1, batch     9 | loss: 9.3570728
CurrentTrain: epoch  1, batch    10 | loss: 10.1183128
CurrentTrain: epoch  1, batch    11 | loss: 10.0617275
CurrentTrain: epoch  1, batch    12 | loss: 9.9741211
CurrentTrain: epoch  1, batch    13 | loss: 9.8642416
CurrentTrain: epoch  1, batch    14 | loss: 9.2943916
CurrentTrain: epoch  1, batch    15 | loss: 9.5483799
CurrentTrain: epoch  1, batch    16 | loss: 9.6808300
CurrentTrain: epoch  1, batch    17 | loss: 9.4043503
CurrentTrain: epoch  1, batch    18 | loss: 9.5741882
CurrentTrain: epoch  1, batch    19 | loss: 9.5527611
CurrentTrain: epoch  1, batch    20 | loss: 9.0637760
CurrentTrain: epoch  1, batch    21 | loss: 9.7469664
CurrentTrain: epoch  1, batch    22 | loss: 8.8756218
CurrentTrain: epoch  1, batch    23 | loss: 9.1251698
CurrentTrain: epoch  1, batch    24 | loss: 9.1891069
CurrentTrain: epoch  1, batch    25 | loss: 8.7099361
CurrentTrain: epoch  1, batch    26 | loss: 9.0454998
CurrentTrain: epoch  1, batch    27 | loss: 8.6898060
CurrentTrain: epoch  1, batch    28 | loss: 9.5912075
CurrentTrain: epoch  1, batch    29 | loss: 8.9738770
CurrentTrain: epoch  1, batch    30 | loss: 9.3226662
CurrentTrain: epoch  1, batch    31 | loss: 8.9327812
CurrentTrain: epoch  1, batch    32 | loss: 8.7994919
CurrentTrain: epoch  1, batch    33 | loss: 8.1001968
CurrentTrain: epoch  1, batch    34 | loss: 8.7130451
CurrentTrain: epoch  1, batch    35 | loss: 8.9697723
CurrentTrain: epoch  1, batch    36 | loss: 8.5115614
CurrentTrain: epoch  1, batch    37 | loss: 8.5565414
CurrentTrain: epoch  2, batch     0 | loss: 8.3512306
CurrentTrain: epoch  2, batch     1 | loss: 8.5337048
CurrentTrain: epoch  2, batch     2 | loss: 8.3742352
CurrentTrain: epoch  2, batch     3 | loss: 8.1631508
CurrentTrain: epoch  2, batch     4 | loss: 8.0418110
CurrentTrain: epoch  2, batch     5 | loss: 7.6430268
CurrentTrain: epoch  2, batch     6 | loss: 7.6695080
CurrentTrain: epoch  2, batch     7 | loss: 8.5192146
CurrentTrain: epoch  2, batch     8 | loss: 8.0651703
CurrentTrain: epoch  2, batch     9 | loss: 8.0541534
CurrentTrain: epoch  2, batch    10 | loss: 8.3760300
CurrentTrain: epoch  2, batch    11 | loss: 8.0137968
CurrentTrain: epoch  2, batch    12 | loss: 8.0249338
CurrentTrain: epoch  2, batch    13 | loss: 7.7498989
CurrentTrain: epoch  2, batch    14 | loss: 8.4216290
CurrentTrain: epoch  2, batch    15 | loss: 7.5450029
CurrentTrain: epoch  2, batch    16 | loss: 7.9579206
CurrentTrain: epoch  2, batch    17 | loss: 8.5223637
CurrentTrain: epoch  2, batch    18 | loss: 8.1366081
CurrentTrain: epoch  2, batch    19 | loss: 9.6781673
CurrentTrain: epoch  2, batch    20 | loss: 7.5935125
CurrentTrain: epoch  2, batch    21 | loss: 8.3920259
CurrentTrain: epoch  2, batch    22 | loss: 8.4554424
CurrentTrain: epoch  2, batch    23 | loss: 7.6649494
CurrentTrain: epoch  2, batch    24 | loss: 8.8750982
CurrentTrain: epoch  2, batch    25 | loss: 7.3939075
CurrentTrain: epoch  2, batch    26 | loss: 7.1895227
CurrentTrain: epoch  2, batch    27 | loss: 8.2698689
CurrentTrain: epoch  2, batch    28 | loss: 8.1249027
CurrentTrain: epoch  2, batch    29 | loss: 7.3165469
CurrentTrain: epoch  2, batch    30 | loss: 7.7837143
CurrentTrain: epoch  2, batch    31 | loss: 7.4733853
CurrentTrain: epoch  2, batch    32 | loss: 7.9553075
CurrentTrain: epoch  2, batch    33 | loss: 7.6333399
CurrentTrain: epoch  2, batch    34 | loss: 8.0373125
CurrentTrain: epoch  2, batch    35 | loss: 7.3136034
CurrentTrain: epoch  2, batch    36 | loss: 7.1522083
CurrentTrain: epoch  2, batch    37 | loss: 8.6348362
CurrentTrain: epoch  3, batch     0 | loss: 7.1139722
CurrentTrain: epoch  3, batch     1 | loss: 6.9707355
CurrentTrain: epoch  3, batch     2 | loss: 7.1857953
CurrentTrain: epoch  3, batch     3 | loss: 7.6786275
CurrentTrain: epoch  3, batch     4 | loss: 7.3561287
CurrentTrain: epoch  3, batch     5 | loss: 7.9494181
CurrentTrain: epoch  3, batch     6 | loss: 7.5178723
CurrentTrain: epoch  3, batch     7 | loss: 8.0559950
CurrentTrain: epoch  3, batch     8 | loss: 7.1009760
CurrentTrain: epoch  3, batch     9 | loss: 7.5220647
CurrentTrain: epoch  3, batch    10 | loss: 7.1232629
CurrentTrain: epoch  3, batch    11 | loss: 6.6339645
CurrentTrain: epoch  3, batch    12 | loss: 7.4340744
CurrentTrain: epoch  3, batch    13 | loss: 7.1701188
CurrentTrain: epoch  3, batch    14 | loss: 7.7301121
CurrentTrain: epoch  3, batch    15 | loss: 6.8054104
CurrentTrain: epoch  3, batch    16 | loss: 7.1090574
CurrentTrain: epoch  3, batch    17 | loss: 7.9012637
CurrentTrain: epoch  3, batch    18 | loss: 7.2261305
CurrentTrain: epoch  3, batch    19 | loss: 6.8999605
CurrentTrain: epoch  3, batch    20 | loss: 7.6513700
CurrentTrain: epoch  3, batch    21 | loss: 7.7936196
CurrentTrain: epoch  3, batch    22 | loss: 7.3559699
CurrentTrain: epoch  3, batch    23 | loss: 7.2927194
CurrentTrain: epoch  3, batch    24 | loss: 6.6440411
CurrentTrain: epoch  3, batch    25 | loss: 7.9699316
CurrentTrain: epoch  3, batch    26 | loss: 7.6400328
CurrentTrain: epoch  3, batch    27 | loss: 8.1246204
CurrentTrain: epoch  3, batch    28 | loss: 6.9972801
CurrentTrain: epoch  3, batch    29 | loss: 7.6956549
CurrentTrain: epoch  3, batch    30 | loss: 5.9396944
CurrentTrain: epoch  3, batch    31 | loss: 6.7310066
CurrentTrain: epoch  3, batch    32 | loss: 7.1758060
CurrentTrain: epoch  3, batch    33 | loss: 6.8879170
CurrentTrain: epoch  3, batch    34 | loss: 7.0907440
CurrentTrain: epoch  3, batch    35 | loss: 8.2324390
CurrentTrain: epoch  3, batch    36 | loss: 6.8123188
CurrentTrain: epoch  3, batch    37 | loss: 7.9230604
CurrentTrain: epoch  4, batch     0 | loss: 6.4223065
CurrentTrain: epoch  4, batch     1 | loss: 8.1461897
CurrentTrain: epoch  4, batch     2 | loss: 7.3588810
CurrentTrain: epoch  4, batch     3 | loss: 6.4877920
CurrentTrain: epoch  4, batch     4 | loss: 6.6268101
CurrentTrain: epoch  4, batch     5 | loss: 7.0187354
CurrentTrain: epoch  4, batch     6 | loss: 7.0816693
CurrentTrain: epoch  4, batch     7 | loss: 7.1959009
CurrentTrain: epoch  4, batch     8 | loss: 6.7459764
CurrentTrain: epoch  4, batch     9 | loss: 7.3650608
CurrentTrain: epoch  4, batch    10 | loss: 7.0508604
CurrentTrain: epoch  4, batch    11 | loss: 6.3555045
CurrentTrain: epoch  4, batch    12 | loss: 6.9357615
CurrentTrain: epoch  4, batch    13 | loss: 6.6709213
CurrentTrain: epoch  4, batch    14 | loss: 6.4444828
CurrentTrain: epoch  4, batch    15 | loss: 6.2490506
CurrentTrain: epoch  4, batch    16 | loss: 6.1963024
CurrentTrain: epoch  4, batch    17 | loss: 6.1494789
CurrentTrain: epoch  4, batch    18 | loss: 7.0475750
CurrentTrain: epoch  4, batch    19 | loss: 7.4127522
CurrentTrain: epoch  4, batch    20 | loss: 7.0225353
CurrentTrain: epoch  4, batch    21 | loss: 7.0717211
CurrentTrain: epoch  4, batch    22 | loss: 7.0448775
CurrentTrain: epoch  4, batch    23 | loss: 7.3564863
CurrentTrain: epoch  4, batch    24 | loss: 7.0661211
CurrentTrain: epoch  4, batch    25 | loss: 6.9127369
CurrentTrain: epoch  4, batch    26 | loss: 7.5864096
CurrentTrain: epoch  4, batch    27 | loss: 6.7993279
CurrentTrain: epoch  4, batch    28 | loss: 7.9705238
CurrentTrain: epoch  4, batch    29 | loss: 5.9742994
CurrentTrain: epoch  4, batch    30 | loss: 6.8748369
CurrentTrain: epoch  4, batch    31 | loss: 6.3850441
CurrentTrain: epoch  4, batch    32 | loss: 6.8835621
CurrentTrain: epoch  4, batch    33 | loss: 7.6276026
CurrentTrain: epoch  4, batch    34 | loss: 6.9971342
CurrentTrain: epoch  4, batch    35 | loss: 5.4559760
CurrentTrain: epoch  4, batch    36 | loss: 6.9333768
CurrentTrain: epoch  4, batch    37 | loss: 7.4560809
CurrentTrain: epoch  5, batch     0 | loss: 7.1189909
CurrentTrain: epoch  5, batch     1 | loss: 5.9556274
CurrentTrain: epoch  5, batch     2 | loss: 6.7488570
CurrentTrain: epoch  5, batch     3 | loss: 6.2825470
CurrentTrain: epoch  5, batch     4 | loss: 7.0701909
CurrentTrain: epoch  5, batch     5 | loss: 6.5173020
CurrentTrain: epoch  5, batch     6 | loss: 7.0609131
CurrentTrain: epoch  5, batch     7 | loss: 6.8449073
CurrentTrain: epoch  5, batch     8 | loss: 7.1095209
CurrentTrain: epoch  5, batch     9 | loss: 6.4924507
CurrentTrain: epoch  5, batch    10 | loss: 6.9285927
CurrentTrain: epoch  5, batch    11 | loss: 6.9721107
CurrentTrain: epoch  5, batch    12 | loss: 6.1497045
CurrentTrain: epoch  5, batch    13 | loss: 6.1354685
CurrentTrain: epoch  5, batch    14 | loss: 6.6522365
CurrentTrain: epoch  5, batch    15 | loss: 6.4322557
CurrentTrain: epoch  5, batch    16 | loss: 6.0647006
CurrentTrain: epoch  5, batch    17 | loss: 6.4893384
CurrentTrain: epoch  5, batch    18 | loss: 6.7980723
CurrentTrain: epoch  5, batch    19 | loss: 6.9945354
CurrentTrain: epoch  5, batch    20 | loss: 5.8922739
CurrentTrain: epoch  5, batch    21 | loss: 6.7343268
CurrentTrain: epoch  5, batch    22 | loss: 5.9353647
CurrentTrain: epoch  5, batch    23 | loss: 6.3995366
CurrentTrain: epoch  5, batch    24 | loss: 6.2577400
CurrentTrain: epoch  5, batch    25 | loss: 6.4591513
CurrentTrain: epoch  5, batch    26 | loss: 6.4365878
CurrentTrain: epoch  5, batch    27 | loss: 5.9844580
CurrentTrain: epoch  5, batch    28 | loss: 6.0181484
CurrentTrain: epoch  5, batch    29 | loss: 6.2944040
CurrentTrain: epoch  5, batch    30 | loss: 6.0024242
CurrentTrain: epoch  5, batch    31 | loss: 5.7102137
CurrentTrain: epoch  5, batch    32 | loss: 6.6293201
CurrentTrain: epoch  5, batch    33 | loss: 6.5550976
CurrentTrain: epoch  5, batch    34 | loss: 6.1263747
CurrentTrain: epoch  5, batch    35 | loss: 6.6565342
CurrentTrain: epoch  5, batch    36 | loss: 7.3497887
CurrentTrain: epoch  5, batch    37 | loss: 6.0490675
CurrentTrain: epoch  6, batch     0 | loss: 5.7230358
CurrentTrain: epoch  6, batch     1 | loss: 6.8340855
CurrentTrain: epoch  6, batch     2 | loss: 6.5601463
CurrentTrain: epoch  6, batch     3 | loss: 6.1693296
CurrentTrain: epoch  6, batch     4 | loss: 5.5128441
CurrentTrain: epoch  6, batch     5 | loss: 6.2708793
CurrentTrain: epoch  6, batch     6 | loss: 5.5214295
CurrentTrain: epoch  6, batch     7 | loss: 6.0469947
CurrentTrain: epoch  6, batch     8 | loss: 5.6417952
CurrentTrain: epoch  6, batch     9 | loss: 6.0741568
CurrentTrain: epoch  6, batch    10 | loss: 5.9355745
CurrentTrain: epoch  6, batch    11 | loss: 5.8540535
CurrentTrain: epoch  6, batch    12 | loss: 6.0481277
CurrentTrain: epoch  6, batch    13 | loss: 6.8690886
CurrentTrain: epoch  6, batch    14 | loss: 6.3957520
CurrentTrain: epoch  6, batch    15 | loss: 6.1347995
CurrentTrain: epoch  6, batch    16 | loss: 5.5739269
CurrentTrain: epoch  6, batch    17 | loss: 6.0178261
CurrentTrain: epoch  6, batch    18 | loss: 5.6942987
CurrentTrain: epoch  6, batch    19 | loss: 5.9793134
CurrentTrain: epoch  6, batch    20 | loss: 5.6881676
CurrentTrain: epoch  6, batch    21 | loss: 5.8957400
CurrentTrain: epoch  6, batch    22 | loss: 5.6262932
CurrentTrain: epoch  6, batch    23 | loss: 5.7036686
CurrentTrain: epoch  6, batch    24 | loss: 5.6339889
CurrentTrain: epoch  6, batch    25 | loss: 6.9808631
CurrentTrain: epoch  6, batch    26 | loss: 7.6021729
CurrentTrain: epoch  6, batch    27 | loss: 5.7200398
CurrentTrain: epoch  6, batch    28 | loss: 5.7658391
CurrentTrain: epoch  6, batch    29 | loss: 6.3144131
CurrentTrain: epoch  6, batch    30 | loss: 6.0053415
CurrentTrain: epoch  6, batch    31 | loss: 6.0388703
CurrentTrain: epoch  6, batch    32 | loss: 6.7232904
CurrentTrain: epoch  6, batch    33 | loss: 5.5131674
CurrentTrain: epoch  6, batch    34 | loss: 5.8492961
CurrentTrain: epoch  6, batch    35 | loss: 5.6778111
CurrentTrain: epoch  6, batch    36 | loss: 6.1200533
CurrentTrain: epoch  6, batch    37 | loss: 5.4204817
CurrentTrain: epoch  7, batch     0 | loss: 6.0968976
CurrentTrain: epoch  7, batch     1 | loss: 5.0641108
CurrentTrain: epoch  7, batch     2 | loss: 6.1290398
CurrentTrain: epoch  7, batch     3 | loss: 5.8802948
CurrentTrain: epoch  7, batch     4 | loss: 5.6371732
CurrentTrain: epoch  7, batch     5 | loss: 5.4693127
CurrentTrain: epoch  7, batch     6 | loss: 5.1917706
CurrentTrain: epoch  7, batch     7 | loss: 6.2404475
CurrentTrain: epoch  7, batch     8 | loss: 6.3377786
CurrentTrain: epoch  7, batch     9 | loss: 5.5609522
CurrentTrain: epoch  7, batch    10 | loss: 5.7455940
CurrentTrain: epoch  7, batch    11 | loss: 5.6262875
CurrentTrain: epoch  7, batch    12 | loss: 5.6566010
CurrentTrain: epoch  7, batch    13 | loss: 5.7177525
CurrentTrain: epoch  7, batch    14 | loss: 5.1187143
CurrentTrain: epoch  7, batch    15 | loss: 5.4172316
CurrentTrain: epoch  7, batch    16 | loss: 5.5119352
CurrentTrain: epoch  7, batch    17 | loss: 5.6875381
CurrentTrain: epoch  7, batch    18 | loss: 5.3734360
CurrentTrain: epoch  7, batch    19 | loss: 5.3224068
CurrentTrain: epoch  7, batch    20 | loss: 5.7568378
CurrentTrain: epoch  7, batch    21 | loss: 5.5142097
CurrentTrain: epoch  7, batch    22 | loss: 5.6786385
CurrentTrain: epoch  7, batch    23 | loss: 5.5012379
CurrentTrain: epoch  7, batch    24 | loss: 5.5215521
CurrentTrain: epoch  7, batch    25 | loss: 5.1751642
CurrentTrain: epoch  7, batch    26 | loss: 6.0503030
CurrentTrain: epoch  7, batch    27 | loss: 6.0258293
CurrentTrain: epoch  7, batch    28 | loss: 5.6047201
CurrentTrain: epoch  7, batch    29 | loss: 5.2182121
CurrentTrain: epoch  7, batch    30 | loss: 5.2618060
CurrentTrain: epoch  7, batch    31 | loss: 5.4302454
CurrentTrain: epoch  7, batch    32 | loss: 5.7622051
CurrentTrain: epoch  7, batch    33 | loss: 5.2555881
CurrentTrain: epoch  7, batch    34 | loss: 5.6442766
CurrentTrain: epoch  7, batch    35 | loss: 5.6951528
CurrentTrain: epoch  7, batch    36 | loss: 5.8784037
CurrentTrain: epoch  7, batch    37 | loss: 5.8808088
CurrentTrain: epoch  8, batch     0 | loss: 5.4475651
CurrentTrain: epoch  8, batch     1 | loss: 5.4827271
CurrentTrain: epoch  8, batch     2 | loss: 5.6091423
CurrentTrain: epoch  8, batch     3 | loss: 5.2646885
CurrentTrain: epoch  8, batch     4 | loss: 5.4058714
CurrentTrain: epoch  8, batch     5 | loss: 5.7770948
CurrentTrain: epoch  8, batch     6 | loss: 5.5088258
CurrentTrain: epoch  8, batch     7 | loss: 5.6029844
CurrentTrain: epoch  8, batch     8 | loss: 5.4537702
CurrentTrain: epoch  8, batch     9 | loss: 5.7466192
CurrentTrain: epoch  8, batch    10 | loss: 5.7817965
CurrentTrain: epoch  8, batch    11 | loss: 5.5480862
CurrentTrain: epoch  8, batch    12 | loss: 5.8642287
CurrentTrain: epoch  8, batch    13 | loss: 5.3739762
CurrentTrain: epoch  8, batch    14 | loss: 5.2609830
CurrentTrain: epoch  8, batch    15 | loss: 5.0959525
CurrentTrain: epoch  8, batch    16 | loss: 5.3061085
CurrentTrain: epoch  8, batch    17 | loss: 5.2018952
CurrentTrain: epoch  8, batch    18 | loss: 5.1273270
CurrentTrain: epoch  8, batch    19 | loss: 5.9564724
CurrentTrain: epoch  8, batch    20 | loss: 5.0543737
CurrentTrain: epoch  8, batch    21 | loss: 6.1964312
CurrentTrain: epoch  8, batch    22 | loss: 5.3700352
CurrentTrain: epoch  8, batch    23 | loss: 5.3072968
CurrentTrain: epoch  8, batch    24 | loss: 5.3473816
CurrentTrain: epoch  8, batch    25 | loss: 4.9096389
CurrentTrain: epoch  8, batch    26 | loss: 5.4018965
CurrentTrain: epoch  8, batch    27 | loss: 5.2054262
CurrentTrain: epoch  8, batch    28 | loss: 5.5512471
CurrentTrain: epoch  8, batch    29 | loss: 5.1256838
CurrentTrain: epoch  8, batch    30 | loss: 5.7573471
CurrentTrain: epoch  8, batch    31 | loss: 5.4207411
CurrentTrain: epoch  8, batch    32 | loss: 5.0384364
CurrentTrain: epoch  8, batch    33 | loss: 5.1912894
CurrentTrain: epoch  8, batch    34 | loss: 5.2938643
CurrentTrain: epoch  8, batch    35 | loss: 5.6980886
CurrentTrain: epoch  8, batch    36 | loss: 5.0369930
CurrentTrain: epoch  8, batch    37 | loss: 5.2500625
CurrentTrain: epoch  9, batch     0 | loss: 4.9389849
CurrentTrain: epoch  9, batch     1 | loss: 5.1093073
CurrentTrain: epoch  9, batch     2 | loss: 5.3265538
CurrentTrain: epoch  9, batch     3 | loss: 5.7392955
CurrentTrain: epoch  9, batch     4 | loss: 5.4597726
CurrentTrain: epoch  9, batch     5 | loss: 4.9083200
CurrentTrain: epoch  9, batch     6 | loss: 5.1599774
CurrentTrain: epoch  9, batch     7 | loss: 5.1602144
CurrentTrain: epoch  9, batch     8 | loss: 5.1108742
CurrentTrain: epoch  9, batch     9 | loss: 4.9369082
CurrentTrain: epoch  9, batch    10 | loss: 5.3843632
CurrentTrain: epoch  9, batch    11 | loss: 5.2530689
CurrentTrain: epoch  9, batch    12 | loss: 5.2304668
CurrentTrain: epoch  9, batch    13 | loss: 5.1027327
CurrentTrain: epoch  9, batch    14 | loss: 5.2944899
CurrentTrain: epoch  9, batch    15 | loss: 5.1886072
CurrentTrain: epoch  9, batch    16 | loss: 5.1276050
CurrentTrain: epoch  9, batch    17 | loss: 4.9790549
CurrentTrain: epoch  9, batch    18 | loss: 4.9703021
CurrentTrain: epoch  9, batch    19 | loss: 5.3980474
CurrentTrain: epoch  9, batch    20 | loss: 5.1232157
CurrentTrain: epoch  9, batch    21 | loss: 5.0400248
CurrentTrain: epoch  9, batch    22 | loss: 5.0359144
CurrentTrain: epoch  9, batch    23 | loss: 5.0575562
CurrentTrain: epoch  9, batch    24 | loss: 5.3786640
CurrentTrain: epoch  9, batch    25 | loss: 4.9700546
CurrentTrain: epoch  9, batch    26 | loss: 5.4497266
CurrentTrain: epoch  9, batch    27 | loss: 5.0185699
CurrentTrain: epoch  9, batch    28 | loss: 5.0059214
CurrentTrain: epoch  9, batch    29 | loss: 5.2353449
CurrentTrain: epoch  9, batch    30 | loss: 5.0617781
CurrentTrain: epoch  9, batch    31 | loss: 4.8789949
CurrentTrain: epoch  9, batch    32 | loss: 5.2208066
CurrentTrain: epoch  9, batch    33 | loss: 4.8718529
CurrentTrain: epoch  9, batch    34 | loss: 4.9349327
CurrentTrain: epoch  9, batch    35 | loss: 5.2948380
CurrentTrain: epoch  9, batch    36 | loss: 4.8359885
CurrentTrain: epoch  9, batch    37 | loss: 5.1778431
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots and family.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in the tech industry, John Smith moved to Canada to enjoy a quieter life surrounded by nature and a more relaxed pace.  
Head Entity: John Smith  
Tail Entity: Canada  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` the fdic has good intentions , and they are probably demonstrating things that can be done better , '' said bob davis , an executive vice president with the american bankers association .
Head Entity: american bankers association
Tail Entity: bob davis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: "During the annual meeting, the board of directors praised Sarah Johnson for her outstanding leadership and commitment to the company," said the CEO of Tech Innovations Inc.  
Head Entity: Tech Innovations Inc.  
Tail Entity: Sarah Johnson  

Relation: organization top members employees  
Context: "As the chief financial officer, Mark Thompson has played a crucial role in steering the company towards success," noted a recent press release from Global Finance Corp.  
Head Entity: Global Finance Corp.  
Tail Entity: Mark Thompson  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: later this month , these and nine other robbins pieces will form the core of the new york city ballet 's 43rd season at the saratoga performing arts center .
Head Entity: robbins
Tail Entity: new york city ballet
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The United Nations is a prominent organization that includes various member states, each contributing to global peace and security efforts.  
Head Entity: United Nations  
Tail Entity: member states  

Relation: organization member of  
Context: The International Olympic Committee oversees the participation of numerous national Olympic committees, which represent their respective countries in the Olympic Games.  
Head Entity: International Olympic Committee  
Tail Entity: national Olympic committees  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: since the couple 's divorce , aniston has been romantically linked with actor vince vaughn , british model paul sculfor , actor orlando bloom and singer john mayer .
Head Entity: paul sculfor
Tail Entity: british
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The renowned physicist Albert Einstein was born in Germany but later became a citizen of the United States.  
Head Entity: Albert Einstein  
Tail Entity: Germany  

Relation: person origin  
Context: The famous author Chimamanda Ngozi Adichie hails from Nigeria, where she was born and raised before moving to the United States for her education.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigeria  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work and dedication, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving the company's financial strategy."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the annual conference, Dr. James Lee, the lead researcher at the National Institute of Health, presented his findings on the latest advancements in medical technology."  
Head Entity: Dr. James Lee  
Tail Entity: lead researcher  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: commander viliame naupoto , chairman of the fiji pine limited announced the woodchips exports target here tuesday after signing a woodchip sale agreement with japan 's itochu corporation .
Head Entity: itochu corporation
Tail Entity: japan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it continues to innovate and expand its product line.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization country of headquarters  
Context: the united nations, an international organization founded in 1945, has its main headquarters situated in new york city, united states.  
Head Entity: united nations  
Tail Entity: united states  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 65.62%   
[EVAL] batch:    2 | acc: 87.50%,  total acc: 72.92%   
[EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   
[EVAL] batch:    4 | acc: 68.75%,  total acc: 71.25%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 70.83%   
[EVAL] batch:    6 | acc: 87.50%,  total acc: 73.21%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 79.17%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 80.62%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 82.39%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 83.33%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 84.13%   
[EVAL] batch:   13 | acc: 75.00%,  total acc: 83.48%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 82.92%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 81.25%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 80.88%   
[EVAL] batch:   17 | acc: 68.75%,  total acc: 80.21%   
[EVAL] batch:   18 | acc: 87.50%,  total acc: 80.59%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 80.94%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 82.67%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 84.11%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 84.75%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 85.65%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 86.64%   
[EVAL] batch:   29 | acc: 87.50%,  total acc: 86.67%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 86.69%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 86.91%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 85.42%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 65.62%   
[EVAL] batch:    2 | acc: 87.50%,  total acc: 72.92%   
[EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   
[EVAL] batch:    4 | acc: 68.75%,  total acc: 71.25%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 70.83%   
[EVAL] batch:    6 | acc: 87.50%,  total acc: 73.21%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 76.56%   
[EVAL] batch:    8 | acc: 100.00%,  total acc: 79.17%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 80.62%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 82.39%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 83.33%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 84.13%   
[EVAL] batch:   13 | acc: 75.00%,  total acc: 83.48%   
[EVAL] batch:   14 | acc: 75.00%,  total acc: 82.92%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 81.25%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 80.88%   
[EVAL] batch:   17 | acc: 68.75%,  total acc: 80.21%   
[EVAL] batch:   18 | acc: 87.50%,  total acc: 80.59%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 80.94%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 82.67%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   
[EVAL] batch:   23 | acc: 100.00%,  total acc: 84.11%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 84.75%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 85.65%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 86.64%   
[EVAL] batch:   29 | acc: 87.50%,  total acc: 86.67%   
[EVAL] batch:   30 | acc: 87.50%,  total acc: 86.69%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 86.91%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 85.42%   
cur_acc:  ['0.8542']
his_acc:  ['0.8542']
CurrentTrain: epoch  0, batch     0 | loss: 6.0813179
CurrentTrain: epoch  0, batch     1 | loss: 6.2031589
CurrentTrain: epoch  1, batch     0 | loss: 5.5336657
CurrentTrain: epoch  1, batch     1 | loss: 5.3734217
CurrentTrain: epoch  2, batch     0 | loss: 4.9733238
CurrentTrain: epoch  2, batch     1 | loss: 4.2760425
CurrentTrain: epoch  3, batch     0 | loss: 4.3535299
CurrentTrain: epoch  3, batch     1 | loss: 4.1296067
CurrentTrain: epoch  4, batch     0 | loss: 4.3434067
CurrentTrain: epoch  4, batch     1 | loss: 3.8795149
CurrentTrain: epoch  5, batch     0 | loss: 4.3447618
CurrentTrain: epoch  5, batch     1 | loss: 3.5511212
CurrentTrain: epoch  6, batch     0 | loss: 4.1321564
CurrentTrain: epoch  6, batch     1 | loss: 2.8768361
CurrentTrain: epoch  7, batch     0 | loss: 3.5213776
CurrentTrain: epoch  7, batch     1 | loss: 3.2422681
CurrentTrain: epoch  8, batch     0 | loss: 3.0880101
CurrentTrain: epoch  8, batch     1 | loss: 2.8226089
CurrentTrain: epoch  9, batch     0 | loss: 2.8451324
CurrentTrain: epoch  9, batch     1 | loss: 2.9113965
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones settled in san francisco, where she found a job in tech. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith lived in boston, enjoying the vibrant culture and history of the city. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: her political involvement began early : at cornell , she helped organize local farmers ' cooperatives .
Head Entity: she
Tail Entity: cornell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After graduating from high school, he enrolled at Stanford University to pursue his degree in computer science.  
Head Entity: he  
Tail Entity: Stanford University  

Relation: person schools attended  
Context: She completed her undergraduate studies at the University of Chicago, where she majored in economics.  
Head Entity: She  
Tail Entity: University of Chicago  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both pursuing their studies.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the passing of their father, the siblings, including jake and his sister, lila, took on the responsibility of caring for their younger brother, max.  
Head Entity: jake  
Tail Entity: lila  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, a crime that could result in significant prison time if convicted.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith was charged with assault following the altercation that took place outside the nightclub last weekend.  
Head Entity: Smith  
Tail Entity: assault  
Mixup data size:  105
MixupTrain:  epoch  0, batch     0 | loss: 5.6121149
MixupTrain:  epoch  0, batch     1 | loss: 5.0728626
MixupTrain:  epoch  0, batch     2 | loss: 4.6105299
MixupTrain:  epoch  0, batch     3 | loss: 4.5852785
MixupTrain:  epoch  0, batch     4 | loss: 4.5700870
MixupTrain:  epoch  0, batch     5 | loss: 4.3006587
MixupTrain:  epoch  0, batch     6 | loss: 3.8479741
MemoryTrain:  epoch  0, batch     0 | loss: 4.1654730
MemoryTrain:  epoch  0, batch     1 | loss: 4.0788803
MemoryTrain:  epoch  0, batch     2 | loss: 2.1485913
MemoryTrain:  epoch  1, batch     0 | loss: 2.9367249
MemoryTrain:  epoch  1, batch     1 | loss: 4.0624838
MemoryTrain:  epoch  1, batch     2 | loss: 2.8530769
MemoryTrain:  epoch  2, batch     0 | loss: 3.0007389
MemoryTrain:  epoch  2, batch     1 | loss: 2.8025184
MemoryTrain:  epoch  2, batch     2 | loss: 2.4828885
MemoryTrain:  epoch  3, batch     0 | loss: 3.0968344
MemoryTrain:  epoch  3, batch     1 | loss: 2.2427809
MemoryTrain:  epoch  3, batch     2 | loss: 1.4908432
MemoryTrain:  epoch  4, batch     0 | loss: 1.9838849
MemoryTrain:  epoch  4, batch     1 | loss: 2.7721083
MemoryTrain:  epoch  4, batch     2 | loss: 1.9773258
MemoryTrain:  epoch  5, batch     0 | loss: 2.4483109
MemoryTrain:  epoch  5, batch     1 | loss: 2.0839109
MemoryTrain:  epoch  5, batch     2 | loss: 3.9817381
MemoryTrain:  epoch  6, batch     0 | loss: 2.2465250
MemoryTrain:  epoch  6, batch     1 | loss: 2.1973732
MemoryTrain:  epoch  6, batch     2 | loss: 1.8155056
MemoryTrain:  epoch  7, batch     0 | loss: 2.3613992
MemoryTrain:  epoch  7, batch     1 | loss: 2.2118959
MemoryTrain:  epoch  7, batch     2 | loss: 1.2112417
MemoryTrain:  epoch  8, batch     0 | loss: 2.1404595
MemoryTrain:  epoch  8, batch     1 | loss: 2.2130561
MemoryTrain:  epoch  8, batch     2 | loss: 1.7256498
MemoryTrain:  epoch  9, batch     0 | loss: 2.2531290
MemoryTrain:  epoch  9, batch     1 | loss: 1.8187994
MemoryTrain:  epoch  9, batch     2 | loss: 1.1239415
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   
[EVAL] batch:    2 | acc: 62.50%,  total acc: 68.75%   
[EVAL] batch:    3 | acc: 62.50%,  total acc: 67.19%   
[EVAL] batch:    4 | acc: 81.25%,  total acc: 70.00%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 69.79%   
[EVAL] batch:    6 | acc: 87.50%,  total acc: 72.32%   
[EVAL] batch:    7 | acc: 87.50%,  total acc: 74.22%   
[EVAL] batch:    8 | acc: 87.50%,  total acc: 75.69%   
[EVAL] batch:    9 | acc: 100.00%,  total acc: 78.12%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 80.11%   
[EVAL] batch:   11 | acc: 93.75%,  total acc: 81.25%   
[EVAL] batch:   12 | acc: 100.00%,  total acc: 82.69%   
[EVAL] batch:   13 | acc: 100.00%,  total acc: 83.93%   
[EVAL] batch:   14 | acc: 100.00%,  total acc: 85.00%   
[EVAL] batch:   15 | acc: 100.00%,  total acc: 85.94%   
[EVAL] batch:   16 | acc: 100.00%,  total acc: 86.76%   
[EVAL] batch:   17 | acc: 25.00%,  total acc: 83.33%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   
[EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   
[EVAL] batch:    3 | acc: 50.00%,  total acc: 57.81%   
[EVAL] batch:    4 | acc: 56.25%,  total acc: 57.50%   
[EVAL] batch:    5 | acc: 68.75%,  total acc: 59.38%   
[EVAL] batch:    6 | acc: 87.50%,  total acc: 63.39%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 67.97%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 70.83%   
[EVAL] batch:    9 | acc: 93.75%,  total acc: 73.12%   
[EVAL] batch:   10 | acc: 100.00%,  total acc: 75.57%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 76.56%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 77.88%   
[EVAL] batch:   13 | acc: 75.00%,  total acc: 77.68%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 77.08%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 75.78%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 75.74%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 75.00%   
[EVAL] batch:   18 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:   19 | acc: 68.75%,  total acc: 74.69%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 75.89%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 76.99%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 77.99%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 78.65%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 79.50%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 80.29%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 80.79%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 81.47%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 82.11%   
[EVAL] batch:   29 | acc: 87.50%,  total acc: 82.29%   
[EVAL] batch:   30 | acc: 100.00%,  total acc: 82.86%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 83.20%   
[EVAL] batch:   32 | acc: 75.00%,  total acc: 82.95%   
[EVAL] batch:   33 | acc: 75.00%,  total acc: 82.72%   
[EVAL] batch:   34 | acc: 62.50%,  total acc: 82.14%   
[EVAL] batch:   35 | acc: 75.00%,  total acc: 81.94%   
[EVAL] batch:   36 | acc: 68.75%,  total acc: 81.59%   
[EVAL] batch:   37 | acc: 62.50%,  total acc: 81.09%   
[EVAL] batch:   38 | acc: 87.50%,  total acc: 81.25%   
[EVAL] batch:   39 | acc: 100.00%,  total acc: 81.72%   
[EVAL] batch:   40 | acc: 75.00%,  total acc: 81.55%   
[EVAL] batch:   41 | acc: 100.00%,  total acc: 81.99%   
[EVAL] batch:   42 | acc: 100.00%,  total acc: 82.41%   
[EVAL] batch:   43 | acc: 93.75%,  total acc: 82.67%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 83.06%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 83.42%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 83.78%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 84.11%   
[EVAL] batch:   48 | acc: 100.00%,  total acc: 84.44%   
[EVAL] batch:   49 | acc: 62.50%,  total acc: 84.00%   
cur_acc:  ['0.8542', '0.8333']
his_acc:  ['0.8542', '0.8400']
CurrentTrain: epoch  0, batch     0 | loss: 6.2633967
CurrentTrain: epoch  0, batch     1 | loss: 6.6457720
CurrentTrain: epoch  1, batch     0 | loss: 5.0713215
CurrentTrain: epoch  1, batch     1 | loss: 5.2275681
CurrentTrain: epoch  2, batch     0 | loss: 5.2716155
CurrentTrain: epoch  2, batch     1 | loss: 3.1779001
CurrentTrain: epoch  3, batch     0 | loss: 4.4551773
CurrentTrain: epoch  3, batch     1 | loss: 4.2353492
CurrentTrain: epoch  4, batch     0 | loss: 4.5147333
CurrentTrain: epoch  4, batch     1 | loss: 2.8473473
CurrentTrain: epoch  5, batch     0 | loss: 3.4608002
CurrentTrain: epoch  5, batch     1 | loss: 4.3827333
CurrentTrain: epoch  6, batch     0 | loss: 3.5737755
CurrentTrain: epoch  6, batch     1 | loss: 3.6514232
CurrentTrain: epoch  7, batch     0 | loss: 3.2597170
CurrentTrain: epoch  7, batch     1 | loss: 3.7665524
CurrentTrain: epoch  8, batch     0 | loss: 3.0449359
CurrentTrain: epoch  8, batch     1 | loss: 3.7334094
CurrentTrain: epoch  9, batch     0 | loss: 3.5720534
CurrentTrain: epoch  9, batch     1 | loss: 2.5845737
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: in testimony by satellite link from germany to a house of representatives ' panel , murat kurnaz recounted his five-year detention , alleging a wide range of torture and abuse .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: During the interview, the renowned artist shared his experiences growing up in the vibrant streets of Barcelona, which he credits as the foundation of his creative journey.  
Head Entity: the renowned artist  
Tail Entity: Barcelona  

Relation: person country of birth  
Context: In her autobiography, the famous scientist details her early life in Tokyo, where she developed a passion for technology and innovation.  
Head Entity: the famous scientist  
Tail Entity: Tokyo  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit the official site at https://www.techinnovators.com for more information on their latest products.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For updates and news, check out the blog at http://www.greenearth.org/blog.  
Head Entity: Green Earth  
Tail Entity: http://www.greenearth.org/
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple inc. has recently acquired a significant stake in the innovative startup nextdoor.  
Head Entity: nextdoor  
Tail Entity: apple inc.  

Relation: organization shareholders  
Context: the investment firm blackrock has increased its holdings in the renewable energy company sunrun.  
Head Entity: sunrun  
Tail Entity: blackrock  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: the once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: after years of financial difficulties, the local arts council announced its closure in July 2018, marking the end of its community programs.  
Head Entity: local arts council  
Tail Entity: July 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. Jane Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. Jane Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the well-known philanthropist, John Doe, to support underprivileged children around the world.  
Head Entity: Hope for Tomorrow  
Tail Entity: John Doe  
Mixup data size:  135
MixupTrain:  epoch  0, batch     0 | loss: 3.1333559
MixupTrain:  epoch  0, batch     1 | loss: 3.0799153
MixupTrain:  epoch  0, batch     2 | loss: 4.0154843
MixupTrain:  epoch  0, batch     3 | loss: 3.3664236
MixupTrain:  epoch  0, batch     4 | loss: 3.6942244
MixupTrain:  epoch  0, batch     5 | loss: 3.6385174
MixupTrain:  epoch  0, batch     6 | loss: 3.3490002
MixupTrain:  epoch  0, batch     7 | loss: 2.8539782
MixupTrain:  epoch  0, batch     8 | loss: 2.4672487
MemoryTrain:  epoch  0, batch     0 | loss: 2.6771865
MemoryTrain:  epoch  0, batch     1 | loss: 3.5478187
MemoryTrain:  epoch  0, batch     2 | loss: 4.4887533
MemoryTrain:  epoch  1, batch     0 | loss: 3.2237606
MemoryTrain:  epoch  1, batch     1 | loss: 2.9921632
MemoryTrain:  epoch  1, batch     2 | loss: 3.1128325
MemoryTrain:  epoch  2, batch     0 | loss: 2.4947324
MemoryTrain:  epoch  2, batch     1 | loss: 3.8577356
MemoryTrain:  epoch  2, batch     2 | loss: 2.6454983
MemoryTrain:  epoch  3, batch     0 | loss: 2.4722409
MemoryTrain:  epoch  3, batch     1 | loss: 2.3795905
MemoryTrain:  epoch  3, batch     2 | loss: 2.9317765
MemoryTrain:  epoch  4, batch     0 | loss: 2.8480425
MemoryTrain:  epoch  4, batch     1 | loss: 1.7530304
MemoryTrain:  epoch  4, batch     2 | loss: 2.5780928
MemoryTrain:  epoch  5, batch     0 | loss: 2.5216353
MemoryTrain:  epoch  5, batch     1 | loss: 2.1939144
MemoryTrain:  epoch  5, batch     2 | loss: 2.1497569
MemoryTrain:  epoch  6, batch     0 | loss: 2.6764345
MemoryTrain:  epoch  6, batch     1 | loss: 1.9087286
MemoryTrain:  epoch  6, batch     2 | loss: 1.9475373
MemoryTrain:  epoch  7, batch     0 | loss: 1.8284128
MemoryTrain:  epoch  7, batch     1 | loss: 2.2728353
MemoryTrain:  epoch  7, batch     2 | loss: 1.9518360
MemoryTrain:  epoch  8, batch     0 | loss: 1.9951501
MemoryTrain:  epoch  8, batch     1 | loss: 1.9998666
MemoryTrain:  epoch  8, batch     2 | loss: 1.4852265
MemoryTrain:  epoch  9, batch     0 | loss: 1.9550774
MemoryTrain:  epoch  9, batch     1 | loss: 1.6368445
MemoryTrain:  epoch  9, batch     2 | loss: 1.5668044
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   
[EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   
[EVAL] batch:    2 | acc: 56.25%,  total acc: 79.17%   
[EVAL] batch:    3 | acc: 18.75%,  total acc: 64.06%   
[EVAL] batch:    4 | acc: 25.00%,  total acc: 56.25%   
[EVAL] batch:    5 | acc: 25.00%,  total acc: 51.04%   
[EVAL] batch:    6 | acc: 31.25%,  total acc: 48.21%   
[EVAL] batch:    7 | acc: 6.25%,  total acc: 42.97%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   
[EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   
[EVAL] batch:    2 | acc: 43.75%,  total acc: 43.75%   
[EVAL] batch:    3 | acc: 18.75%,  total acc: 37.50%   
[EVAL] batch:    4 | acc: 31.25%,  total acc: 36.25%   
[EVAL] batch:    5 | acc: 50.00%,  total acc: 38.54%   
[EVAL] batch:    6 | acc: 56.25%,  total acc: 41.07%   
[EVAL] batch:    7 | acc: 68.75%,  total acc: 44.53%   
[EVAL] batch:    8 | acc: 81.25%,  total acc: 48.61%   
[EVAL] batch:    9 | acc: 81.25%,  total acc: 51.88%   
[EVAL] batch:   10 | acc: 93.75%,  total acc: 55.68%   
[EVAL] batch:   11 | acc: 75.00%,  total acc: 57.29%   
[EVAL] batch:   12 | acc: 62.50%,  total acc: 57.69%   
[EVAL] batch:   13 | acc: 56.25%,  total acc: 57.59%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 58.33%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 58.20%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 59.19%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 59.38%   
[EVAL] batch:   18 | acc: 68.75%,  total acc: 59.87%   
[EVAL] batch:   19 | acc: 75.00%,  total acc: 60.62%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 62.50%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 64.20%   
[EVAL] batch:   22 | acc: 100.00%,  total acc: 65.76%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 66.93%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 68.25%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 69.47%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 70.37%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 71.43%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 72.41%   
[EVAL] batch:   29 | acc: 100.00%,  total acc: 73.33%   
[EVAL] batch:   30 | acc: 100.00%,  total acc: 74.19%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 74.80%   
[EVAL] batch:   32 | acc: 56.25%,  total acc: 74.24%   
[EVAL] batch:   33 | acc: 18.75%,  total acc: 72.61%   
[EVAL] batch:   34 | acc: 37.50%,  total acc: 71.61%   
[EVAL] batch:   35 | acc: 6.25%,  total acc: 69.79%   
[EVAL] batch:   36 | acc: 12.50%,  total acc: 68.24%   
[EVAL] batch:   37 | acc: 12.50%,  total acc: 66.78%   
[EVAL] batch:   38 | acc: 62.50%,  total acc: 66.67%   
[EVAL] batch:   39 | acc: 100.00%,  total acc: 67.50%   
[EVAL] batch:   40 | acc: 50.00%,  total acc: 67.07%   
[EVAL] batch:   41 | acc: 93.75%,  total acc: 67.71%   
[EVAL] batch:   42 | acc: 100.00%,  total acc: 68.46%   
[EVAL] batch:   43 | acc: 100.00%,  total acc: 69.18%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 69.86%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 70.52%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 71.14%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 71.74%   
[EVAL] batch:   48 | acc: 100.00%,  total acc: 72.32%   
[EVAL] batch:   49 | acc: 87.50%,  total acc: 72.62%   
[EVAL] batch:   50 | acc: 100.00%,  total acc: 73.16%   
[EVAL] batch:   51 | acc: 68.75%,  total acc: 73.08%   
[EVAL] batch:   52 | acc: 50.00%,  total acc: 72.64%   
[EVAL] batch:   53 | acc: 25.00%,  total acc: 71.76%   
[EVAL] batch:   54 | acc: 18.75%,  total acc: 70.80%   
[EVAL] batch:   55 | acc: 37.50%,  total acc: 70.20%   
[EVAL] batch:   56 | acc: 18.75%,  total acc: 69.30%   
cur_acc:  ['0.8542', '0.8333', '0.4297']
his_acc:  ['0.8542', '0.8400', '0.6930']
CurrentTrain: epoch  0, batch     0 | loss: 5.1811085
CurrentTrain: epoch  0, batch     1 | loss: 5.8999319
CurrentTrain: epoch  1, batch     0 | loss: 3.9691072
CurrentTrain: epoch  1, batch     1 | loss: 4.6029987
CurrentTrain: epoch  2, batch     0 | loss: 3.9642296
CurrentTrain: epoch  2, batch     1 | loss: 3.4069090
CurrentTrain: epoch  3, batch     0 | loss: 3.5471373
CurrentTrain: epoch  3, batch     1 | loss: 3.1859493
CurrentTrain: epoch  4, batch     0 | loss: 3.1791415
CurrentTrain: epoch  4, batch     1 | loss: 2.6565881
CurrentTrain: epoch  5, batch     0 | loss: 2.6289625
CurrentTrain: epoch  5, batch     1 | loss: 2.8489995
CurrentTrain: epoch  6, batch     0 | loss: 2.7903318
CurrentTrain: epoch  6, batch     1 | loss: 2.4197273
CurrentTrain: epoch  7, batch     0 | loss: 2.4982157
CurrentTrain: epoch  7, batch     1 | loss: 2.3187308
CurrentTrain: epoch  8, batch     0 | loss: 2.2867813
CurrentTrain: epoch  8, batch     1 | loss: 2.3233500
CurrentTrain: epoch  9, batch     0 | loss: 2.1500463
CurrentTrain: epoch  9, batch     1 | loss: 2.1622598
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the jnf was founded in 1901 to buy plots in palestine , then ruled by the ottomans .
Head Entity: jnf
Tail Entity: 1901
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: the united nations was established in 1945 to promote international cooperation and peace.  
Head Entity: united nations  
Tail Entity: 1945  

Relation: organization founded  
Context: the world health organization was created in 1948 to coordinate global health efforts.  
Head Entity: world health organization  
Tail Entity: 1948  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john smith turned 30 last week.  
Head Entity: john smith  
Tail Entity: 30  

Relation: person age  
Context: the famous actor, tom hanks, celebrated his 65th birthday yesterday.  
Head Entity: tom hanks  
Tail Entity: 65  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he began his career.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced his writing.  
Head Entity: the famous author  
Tail Entity: boston  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: it was berger who made clarke a member of the white house principals committee when it met to discuss terrorist threats , allowing an otherwise middle-ranking nsc bureaucrat to treat tenet and secretary of state madeleine albright as equals -lrb- which the empire-building clarke was pleased to do -rrb- .
Head Entity: nsc
Tail Entity: white house principals committee
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The board of directors of the tech startup includes several prominent figures from the industry, such as the CEO of Innovatech, who has been instrumental in guiding the company’s strategic direction.  
Head Entity: tech startup  
Tail Entity: Innovatech  

Relation: organization members  
Context: During the annual conference, the president of the environmental advocacy group announced the inclusion of several new organizations, highlighting the partnership with Green Future, which focuses on sustainable practices.  
Head Entity: environmental advocacy group  
Tail Entity: Green Future  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of study and reflection, Maria decided to embrace Buddhism, finding peace and purpose in its teachings.  
Head Entity: Maria  
Tail Entity: Buddhism  

Relation: person religion  
Context: During the festival, Ahmed proudly wore his traditional attire, celebrating his deep connection to Islam and its rich cultural heritage.  
Head Entity: Ahmed  
Tail Entity: Islam  
Mixup data size:  165
MixupTrain:  epoch  0, batch     0 | loss: 3.2590959
MixupTrain:  epoch  0, batch     1 | loss: 2.9693706
MixupTrain:  epoch  0, batch     2 | loss: 2.7521765
MixupTrain:  epoch  0, batch     3 | loss: 2.6239045
MixupTrain:  epoch  0, batch     4 | loss: 2.6557367
MixupTrain:  epoch  0, batch     5 | loss: 3.3353348
MixupTrain:  epoch  0, batch     6 | loss: 2.5733967
MixupTrain:  epoch  0, batch     7 | loss: 2.8261302
MixupTrain:  epoch  0, batch     8 | loss: 2.5399601
MixupTrain:  epoch  0, batch     9 | loss: 2.8402522
MixupTrain:  epoch  0, batch    10 | loss: 2.8179550
MemoryTrain:  epoch  0, batch     0 | loss: 2.8405232
MemoryTrain:  epoch  0, batch     1 | loss: 2.8563137
MemoryTrain:  epoch  0, batch     2 | loss: 2.7704678
MemoryTrain:  epoch  0, batch     3 | loss: 3.3739309
MemoryTrain:  epoch  1, batch     0 | loss: 2.5198660
MemoryTrain:  epoch  1, batch     1 | loss: 2.6265087
MemoryTrain:  epoch  1, batch     2 | loss: 2.4556589
MemoryTrain:  epoch  1, batch     3 | loss: 3.1235476
MemoryTrain:  epoch  2, batch     0 | loss: 2.3310838
MemoryTrain:  epoch  2, batch     1 | loss: 2.4625120
MemoryTrain:  epoch  2, batch     2 | loss: 2.3426900
MemoryTrain:  epoch  2, batch     3 | loss: 2.0032711
MemoryTrain:  epoch  3, batch     0 | loss: 2.2835693
MemoryTrain:  epoch  3, batch     1 | loss: 1.7191138
MemoryTrain:  epoch  3, batch     2 | loss: 1.9495324
MemoryTrain:  epoch  3, batch     3 | loss: 1.7598654
MemoryTrain:  epoch  4, batch     0 | loss: 1.8797956
MemoryTrain:  epoch  4, batch     1 | loss: 1.5591444
MemoryTrain:  epoch  4, batch     2 | loss: 1.8090240
MemoryTrain:  epoch  4, batch     3 | loss: 1.5976837
MemoryTrain:  epoch  5, batch     0 | loss: 1.7532847
MemoryTrain:  epoch  5, batch     1 | loss: 1.4784024
MemoryTrain:  epoch  5, batch     2 | loss: 1.7055402
MemoryTrain:  epoch  5, batch     3 | loss: 1.7975857
MemoryTrain:  epoch  6, batch     0 | loss: 1.7490239
MemoryTrain:  epoch  6, batch     1 | loss: 1.6794082
MemoryTrain:  epoch  6, batch     2 | loss: 1.4436426
MemoryTrain:  epoch  6, batch     3 | loss: 1.5196853
MemoryTrain:  epoch  7, batch     0 | loss: 1.5662210
MemoryTrain:  epoch  7, batch     1 | loss: 1.5211754
MemoryTrain:  epoch  7, batch     2 | loss: 1.5380781
MemoryTrain:  epoch  7, batch     3 | loss: 1.6724763
MemoryTrain:  epoch  8, batch     0 | loss: 1.6561546
MemoryTrain:  epoch  8, batch     1 | loss: 1.4350704
MemoryTrain:  epoch  8, batch     2 | loss: 1.6150767
MemoryTrain:  epoch  8, batch     3 | loss: 1.5165718
MemoryTrain:  epoch  9, batch     0 | loss: 1.4410007
MemoryTrain:  epoch  9, batch     1 | loss: 1.5710862
MemoryTrain:  epoch  9, batch     2 | loss: 1.3818222
MemoryTrain:  epoch  9, batch     3 | loss: 1.4491864
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   
[EVAL] batch:    1 | acc: 81.25%,  total acc: 90.62%   
[EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   
[EVAL] batch:    3 | acc: 100.00%,  total acc: 95.31%   
[EVAL] batch:    4 | acc: 100.00%,  total acc: 96.25%   
[EVAL] batch:    5 | acc: 100.00%,  total acc: 96.88%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 97.32%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 97.66%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 97.22%   
[EVAL] batch:    9 | acc: 50.00%,  total acc: 92.50%   
[EVAL] batch:   10 | acc: 81.25%,  total acc: 91.48%   
[EVAL] batch:   11 | acc: 68.75%,  total acc: 89.58%   
[EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   
[EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   
[EVAL] batch:    1 | acc: 50.00%,  total acc: 40.62%   
[EVAL] batch:    2 | acc: 37.50%,  total acc: 39.58%   
[EVAL] batch:    3 | acc: 31.25%,  total acc: 37.50%   
[EVAL] batch:    4 | acc: 37.50%,  total acc: 37.50%   
[EVAL] batch:    5 | acc: 31.25%,  total acc: 36.46%   
[EVAL] batch:    6 | acc: 37.50%,  total acc: 36.61%   
[EVAL] batch:    7 | acc: 37.50%,  total acc: 36.72%   
[EVAL] batch:    8 | acc: 56.25%,  total acc: 38.89%   
[EVAL] batch:    9 | acc: 56.25%,  total acc: 40.62%   
[EVAL] batch:   10 | acc: 56.25%,  total acc: 42.05%   
[EVAL] batch:   11 | acc: 56.25%,  total acc: 43.23%   
[EVAL] batch:   12 | acc: 18.75%,  total acc: 41.35%   
[EVAL] batch:   13 | acc: 37.50%,  total acc: 41.07%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 42.92%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 43.75%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 45.59%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 46.53%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 48.36%   
[EVAL] batch:   19 | acc: 75.00%,  total acc: 49.69%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 52.08%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 54.26%   
[EVAL] batch:   22 | acc: 93.75%,  total acc: 55.98%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 57.55%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 59.25%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 60.82%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 62.04%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 63.39%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 64.66%   
[EVAL] batch:   29 | acc: 87.50%,  total acc: 65.42%   
[EVAL] batch:   30 | acc: 100.00%,  total acc: 66.53%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 67.38%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 66.48%   
[EVAL] batch:   33 | acc: 0.00%,  total acc: 64.52%   
[EVAL] batch:   34 | acc: 0.00%,  total acc: 62.68%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 60.94%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 59.29%   
[EVAL] batch:   37 | acc: 0.00%,  total acc: 57.73%   
[EVAL] batch:   38 | acc: 31.25%,  total acc: 57.05%   
[EVAL] batch:   39 | acc: 100.00%,  total acc: 58.13%   
[EVAL] batch:   40 | acc: 43.75%,  total acc: 57.77%   
[EVAL] batch:   41 | acc: 93.75%,  total acc: 58.63%   
[EVAL] batch:   42 | acc: 100.00%,  total acc: 59.59%   
[EVAL] batch:   43 | acc: 100.00%,  total acc: 60.51%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 61.39%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 62.23%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 63.03%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 63.80%   
[EVAL] batch:   48 | acc: 100.00%,  total acc: 64.54%   
[EVAL] batch:   49 | acc: 87.50%,  total acc: 65.00%   
[EVAL] batch:   50 | acc: 93.75%,  total acc: 65.56%   
[EVAL] batch:   51 | acc: 68.75%,  total acc: 65.62%   
[EVAL] batch:   52 | acc: 56.25%,  total acc: 65.45%   
[EVAL] batch:   53 | acc: 50.00%,  total acc: 65.16%   
[EVAL] batch:   54 | acc: 62.50%,  total acc: 65.11%   
[EVAL] batch:   55 | acc: 62.50%,  total acc: 65.07%   
[EVAL] batch:   56 | acc: 56.25%,  total acc: 64.91%   
[EVAL] batch:   57 | acc: 93.75%,  total acc: 65.41%   
[EVAL] batch:   58 | acc: 87.50%,  total acc: 65.78%   
[EVAL] batch:   59 | acc: 100.00%,  total acc: 66.35%   
[EVAL] batch:   60 | acc: 100.00%,  total acc: 66.91%   
[EVAL] batch:   61 | acc: 100.00%,  total acc: 67.44%   
[EVAL] batch:   62 | acc: 100.00%,  total acc: 67.96%   
[EVAL] batch:   63 | acc: 100.00%,  total acc: 68.46%   
[EVAL] batch:   64 | acc: 100.00%,  total acc: 68.94%   
[EVAL] batch:   65 | acc: 75.00%,  total acc: 69.03%   
[EVAL] batch:   66 | acc: 68.75%,  total acc: 69.03%   
[EVAL] batch:   67 | acc: 68.75%,  total acc: 69.03%   
[EVAL] batch:   68 | acc: 75.00%,  total acc: 69.11%   
[EVAL] batch:   69 | acc: 100.00%,  total acc: 69.55%   
[EVAL] batch:   70 | acc: 43.75%,  total acc: 69.19%   
cur_acc:  ['0.8542', '0.8333', '0.4297', '0.8839']
his_acc:  ['0.8542', '0.8400', '0.6930', '0.6919']
CurrentTrain: epoch  0, batch     0 | loss: 5.7545857
CurrentTrain: epoch  0, batch     1 | loss: 6.8655882
CurrentTrain: epoch  1, batch     0 | loss: 5.5689783
CurrentTrain: epoch  1, batch     1 | loss: 4.7958536
CurrentTrain: epoch  2, batch     0 | loss: 4.7768583
CurrentTrain: epoch  2, batch     1 | loss: 4.1149426
CurrentTrain: epoch  3, batch     0 | loss: 4.3635736
CurrentTrain: epoch  3, batch     1 | loss: 4.1081977
CurrentTrain: epoch  4, batch     0 | loss: 4.1623402
CurrentTrain: epoch  4, batch     1 | loss: 4.3445230
CurrentTrain: epoch  5, batch     0 | loss: 3.8735700
CurrentTrain: epoch  5, batch     1 | loss: 3.8673286
CurrentTrain: epoch  6, batch     0 | loss: 3.5245180
CurrentTrain: epoch  6, batch     1 | loss: 3.7160933
CurrentTrain: epoch  7, batch     0 | loss: 3.7698317
CurrentTrain: epoch  7, batch     1 | loss: 2.7395487
CurrentTrain: epoch  8, batch     0 | loss: 3.1723862
CurrentTrain: epoch  8, batch     1 | loss: 3.2438061
CurrentTrain: epoch  9, batch     0 | loss: 3.0869198
CurrentTrain: epoch  9, batch     1 | loss: 3.0157061
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: after years of living in the bustling city, actor john doe has decided to settle down in the serene landscapes of oregon, where he can enjoy a quieter lifestyle away from the spotlight.  
Head Entity: john doe  
Tail Entity: oregon  

Relation: person stateorprovinces of residence  
Context: renowned author jane smith has made her home in the vibrant city of san francisco, drawing inspiration from its diverse culture and stunning views for her next novel.  
Head Entity: jane smith  
Tail Entity: san francisco  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he passed away on saturday .
Head Entity: he
Tail Entity: saturday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: John Smith died on March 15, 2020.  
Head Entity: John Smith  
Tail Entity: March 15, 2020  

Relation: person date of death  
Context: The famous author passed away in 1995.  
Head Entity: The famous author  
Tail Entity: 1995  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, boasts a workforce of approximately 5,500 employees across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: GreenEarth Nonprofit has grown significantly over the past few years and now employs around 1,200 dedicated staff members to support its environmental initiatives.  
Head Entity: GreenEarth Nonprofit  
Tail Entity: 1,200  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the world knows her as the queen of pop, but her friends call her madge. ''  
Head Entity: madge  
Tail Entity: queen of pop  

Relation: person alternate names  
Context: `` during his career, he was often referred to as the king of rock and roll, a title that still resonates today. ''  
Head Entity: king of rock and roll  
Tail Entity: elvis presley  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: in addition to his wife , meskill is survived by two daughters , eileen gallup of new britain and maureen heneghan of haddon heights , n.j. ; three sons , john , of kensington , conn. ; peter , of east hartford , conn. ; and thomas , of branford , conn. ; two sisters , ruth prior of naples , fla. , and sister laura marie of portland , conn. ; five grandchildren , and two step-grandchildren .
Head Entity: his
Tail Entity: meskill
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: after a long and happy marriage, john decided to throw a surprise party for his beloved wife, sarah, to celebrate their anniversary.  
Head Entity: his  
Tail Entity: wife  

Relation: person spouse  
Context: during the family reunion, it was heartwarming to see how much love and affection emily and her husband, mark, shared with each other.  
Head Entity: her  
Tail Entity: husband  
Mixup data size:  195
MixupTrain:  epoch  0, batch     0 | loss: 2.4916008
MixupTrain:  epoch  0, batch     1 | loss: 2.7268023
MixupTrain:  epoch  0, batch     2 | loss: 2.7732985
MixupTrain:  epoch  0, batch     3 | loss: 2.9700048
MixupTrain:  epoch  0, batch     4 | loss: 2.4897275
MixupTrain:  epoch  0, batch     5 | loss: 2.8171287
MixupTrain:  epoch  0, batch     6 | loss: 2.7483354
MixupTrain:  epoch  0, batch     7 | loss: 2.2219954
MixupTrain:  epoch  0, batch     8 | loss: 2.9884288
MixupTrain:  epoch  0, batch     9 | loss: 2.9738393
MixupTrain:  epoch  0, batch    10 | loss: 2.4527855
MixupTrain:  epoch  0, batch    11 | loss: 2.6851244
MixupTrain:  epoch  0, batch    12 | loss: 2.7706697
MemoryTrain:  epoch  0, batch     0 | loss: 1.7778109
MemoryTrain:  epoch  0, batch     1 | loss: 2.5518436
MemoryTrain:  epoch  0, batch     2 | loss: 3.8072977
MemoryTrain:  epoch  0, batch     3 | loss: 2.7934017
MemoryTrain:  epoch  0, batch     4 | loss: 2.6162629
MemoryTrain:  epoch  1, batch     0 | loss: 1.9450123
MemoryTrain:  epoch  1, batch     1 | loss: 2.0619602
MemoryTrain:  epoch  1, batch     2 | loss: 3.4742043
MemoryTrain:  epoch  1, batch     3 | loss: 2.2444165
MemoryTrain:  epoch  1, batch     4 | loss: 2.0082901
MemoryTrain:  epoch  2, batch     0 | loss: 2.1484361
MemoryTrain:  epoch  2, batch     1 | loss: 2.3888869
MemoryTrain:  epoch  2, batch     2 | loss: 2.4019475
MemoryTrain:  epoch  2, batch     3 | loss: 1.6733489
MemoryTrain:  epoch  2, batch     4 | loss: 1.7590532
MemoryTrain:  epoch  3, batch     0 | loss: 1.4398656
MemoryTrain:  epoch  3, batch     1 | loss: 2.0528007
MemoryTrain:  epoch  3, batch     2 | loss: 2.3006423
MemoryTrain:  epoch  3, batch     3 | loss: 1.9558699
MemoryTrain:  epoch  3, batch     4 | loss: 1.5041277
MemoryTrain:  epoch  4, batch     0 | loss: 1.6631010
MemoryTrain:  epoch  4, batch     1 | loss: 1.5261492
MemoryTrain:  epoch  4, batch     2 | loss: 1.8492994
MemoryTrain:  epoch  4, batch     3 | loss: 1.6955442
MemoryTrain:  epoch  4, batch     4 | loss: 1.8319780
MemoryTrain:  epoch  5, batch     0 | loss: 1.4995248
MemoryTrain:  epoch  5, batch     1 | loss: 1.5745990
MemoryTrain:  epoch  5, batch     2 | loss: 1.8671845
MemoryTrain:  epoch  5, batch     3 | loss: 1.6309574
MemoryTrain:  epoch  5, batch     4 | loss: 1.3936565
MemoryTrain:  epoch  6, batch     0 | loss: 1.5078821
MemoryTrain:  epoch  6, batch     1 | loss: 1.4815459
MemoryTrain:  epoch  6, batch     2 | loss: 1.5236881
MemoryTrain:  epoch  6, batch     3 | loss: 1.6301365
MemoryTrain:  epoch  6, batch     4 | loss: 1.7906353
MemoryTrain:  epoch  7, batch     0 | loss: 1.5417793
MemoryTrain:  epoch  7, batch     1 | loss: 1.7210997
MemoryTrain:  epoch  7, batch     2 | loss: 1.4368992
MemoryTrain:  epoch  7, batch     3 | loss: 1.5286202
MemoryTrain:  epoch  7, batch     4 | loss: 1.4213358
MemoryTrain:  epoch  8, batch     0 | loss: 1.6470065
MemoryTrain:  epoch  8, batch     1 | loss: 1.5800531
MemoryTrain:  epoch  8, batch     2 | loss: 1.4327523
MemoryTrain:  epoch  8, batch     3 | loss: 1.4601982
MemoryTrain:  epoch  8, batch     4 | loss: 1.3514770
MemoryTrain:  epoch  9, batch     0 | loss: 1.5167249
MemoryTrain:  epoch  9, batch     1 | loss: 1.4639344
MemoryTrain:  epoch  9, batch     2 | loss: 1.3943206
MemoryTrain:  epoch  9, batch     3 | loss: 1.4823328
MemoryTrain:  epoch  9, batch     4 | loss: 1.3041371
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   
[EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   
[EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   
[EVAL] batch:    3 | acc: 87.50%,  total acc: 81.25%   
[EVAL] batch:    4 | acc: 62.50%,  total acc: 77.50%   
[EVAL] batch:    5 | acc: 93.75%,  total acc: 80.21%   
[EVAL] batch:    6 | acc: 100.00%,  total acc: 83.04%   
[EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 86.25%   
[EVAL] batch:   10 | acc: 62.50%,  total acc: 84.09%   
[EVAL] batch:   11 | acc: 25.00%,  total acc: 79.17%   
[EVAL] batch:   12 | acc: 37.50%,  total acc: 75.96%   
[EVAL] batch:   13 | acc: 50.00%,  total acc: 74.11%   
[EVAL] batch:   14 | acc: 6.25%,  total acc: 69.58%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   
[EVAL] batch:    2 | acc: 56.25%,  total acc: 62.50%   
[EVAL] batch:    3 | acc: 31.25%,  total acc: 54.69%   
[EVAL] batch:    4 | acc: 43.75%,  total acc: 52.50%   
[EVAL] batch:    5 | acc: 37.50%,  total acc: 50.00%   
[EVAL] batch:    6 | acc: 43.75%,  total acc: 49.11%   
[EVAL] batch:    7 | acc: 37.50%,  total acc: 47.66%   
[EVAL] batch:    8 | acc: 56.25%,  total acc: 48.61%   
[EVAL] batch:    9 | acc: 62.50%,  total acc: 50.00%   
[EVAL] batch:   10 | acc: 75.00%,  total acc: 52.27%   
[EVAL] batch:   11 | acc: 62.50%,  total acc: 53.12%   
[EVAL] batch:   12 | acc: 18.75%,  total acc: 50.48%   
[EVAL] batch:   13 | acc: 31.25%,  total acc: 49.11%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 50.42%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 50.78%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 52.21%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 52.78%   
[EVAL] batch:   18 | acc: 68.75%,  total acc: 53.62%   
[EVAL] batch:   19 | acc: 75.00%,  total acc: 54.69%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 56.85%   
[EVAL] batch:   21 | acc: 100.00%,  total acc: 58.81%   
[EVAL] batch:   22 | acc: 93.75%,  total acc: 60.33%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 61.72%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 63.25%   
[EVAL] batch:   25 | acc: 93.75%,  total acc: 64.42%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 65.51%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 66.74%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 67.89%   
[EVAL] batch:   29 | acc: 87.50%,  total acc: 68.54%   
[EVAL] batch:   30 | acc: 100.00%,  total acc: 69.56%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 70.31%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 69.32%   
[EVAL] batch:   33 | acc: 0.00%,  total acc: 67.28%   
[EVAL] batch:   34 | acc: 0.00%,  total acc: 65.36%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 63.54%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 61.82%   
[EVAL] batch:   37 | acc: 0.00%,  total acc: 60.20%   
[EVAL] batch:   38 | acc: 31.25%,  total acc: 59.46%   
[EVAL] batch:   39 | acc: 100.00%,  total acc: 60.47%   
[EVAL] batch:   40 | acc: 68.75%,  total acc: 60.67%   
[EVAL] batch:   41 | acc: 87.50%,  total acc: 61.31%   
[EVAL] batch:   42 | acc: 87.50%,  total acc: 61.92%   
[EVAL] batch:   43 | acc: 100.00%,  total acc: 62.78%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 63.61%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 64.40%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 65.16%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 65.89%   
[EVAL] batch:   48 | acc: 100.00%,  total acc: 66.58%   
[EVAL] batch:   49 | acc: 75.00%,  total acc: 66.75%   
[EVAL] batch:   50 | acc: 93.75%,  total acc: 67.28%   
[EVAL] batch:   51 | acc: 68.75%,  total acc: 67.31%   
[EVAL] batch:   52 | acc: 62.50%,  total acc: 67.22%   
[EVAL] batch:   53 | acc: 68.75%,  total acc: 67.25%   
[EVAL] batch:   54 | acc: 68.75%,  total acc: 67.27%   
[EVAL] batch:   55 | acc: 75.00%,  total acc: 67.41%   
[EVAL] batch:   56 | acc: 62.50%,  total acc: 67.32%   
[EVAL] batch:   57 | acc: 87.50%,  total acc: 67.67%   
[EVAL] batch:   58 | acc: 75.00%,  total acc: 67.80%   
[EVAL] batch:   59 | acc: 100.00%,  total acc: 68.33%   
[EVAL] batch:   60 | acc: 100.00%,  total acc: 68.85%   
[EVAL] batch:   61 | acc: 100.00%,  total acc: 69.35%   
[EVAL] batch:   62 | acc: 100.00%,  total acc: 69.84%   
[EVAL] batch:   63 | acc: 100.00%,  total acc: 70.31%   
[EVAL] batch:   64 | acc: 100.00%,  total acc: 70.77%   
[EVAL] batch:   65 | acc: 50.00%,  total acc: 70.45%   
[EVAL] batch:   66 | acc: 75.00%,  total acc: 70.52%   
[EVAL] batch:   67 | acc: 75.00%,  total acc: 70.59%   
[EVAL] batch:   68 | acc: 75.00%,  total acc: 70.65%   
[EVAL] batch:   69 | acc: 100.00%,  total acc: 71.07%   
[EVAL] batch:   70 | acc: 81.25%,  total acc: 71.21%   
[EVAL] batch:   71 | acc: 62.50%,  total acc: 71.09%   
[EVAL] batch:   72 | acc: 93.75%,  total acc: 71.40%   
[EVAL] batch:   73 | acc: 81.25%,  total acc: 71.54%   
[EVAL] batch:   74 | acc: 75.00%,  total acc: 71.58%   
[EVAL] batch:   75 | acc: 81.25%,  total acc: 71.71%   
[EVAL] batch:   76 | acc: 93.75%,  total acc: 72.00%   
[EVAL] batch:   77 | acc: 100.00%,  total acc: 72.36%   
[EVAL] batch:   78 | acc: 93.75%,  total acc: 72.63%   
[EVAL] batch:   79 | acc: 100.00%,  total acc: 72.97%   
[EVAL] batch:   80 | acc: 75.00%,  total acc: 72.99%   
[EVAL] batch:   81 | acc: 31.25%,  total acc: 72.48%   
[EVAL] batch:   82 | acc: 25.00%,  total acc: 71.91%   
[EVAL] batch:   83 | acc: 50.00%,  total acc: 71.65%   
[EVAL] batch:   84 | acc: 37.50%,  total acc: 71.25%   
cur_acc:  ['0.8542', '0.8333', '0.4297', '0.8839', '0.6958']
his_acc:  ['0.8542', '0.8400', '0.6930', '0.6919', '0.7125']
CurrentTrain: epoch  0, batch     0 | loss: 5.6905007
CurrentTrain: epoch  0, batch     1 | loss: 6.5977054
CurrentTrain: epoch  1, batch     0 | loss: 4.9382544
CurrentTrain: epoch  1, batch     1 | loss: 3.9961693
CurrentTrain: epoch  2, batch     0 | loss: 3.6761737
CurrentTrain: epoch  2, batch     1 | loss: 2.9964802
CurrentTrain: epoch  3, batch     0 | loss: 3.2827423
CurrentTrain: epoch  3, batch     1 | loss: 2.8443086
CurrentTrain: epoch  4, batch     0 | loss: 2.8914037
CurrentTrain: epoch  4, batch     1 | loss: 2.7835097
CurrentTrain: epoch  5, batch     0 | loss: 2.8904843
CurrentTrain: epoch  5, batch     1 | loss: 2.4065578
CurrentTrain: epoch  6, batch     0 | loss: 2.4697442
CurrentTrain: epoch  6, batch     1 | loss: 2.6266596
CurrentTrain: epoch  7, batch     0 | loss: 2.6173754
CurrentTrain: epoch  7, batch     1 | loss: 2.2718811
CurrentTrain: epoch  8, batch     0 | loss: 2.3798435
CurrentTrain: epoch  8, batch     1 | loss: 2.2888107
CurrentTrain: epoch  9, batch     0 | loss: 2.1991961
CurrentTrain: epoch  9, batch     1 | loss: 2.2931054
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author mark twain, whose real name was samuel langhorne clemens, was born on november 30, 1835, in florida, missouri.  
Head Entity: mark twain  
Tail Entity: november 30, 1835  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: joseph simpson farland was born on aug 11 , 1914 , in clarksburg , wva , the only child of richard and grace simpson farland .
Head Entity: joseph simpson farland
Tail Entity: wva
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born on august 3, 1941, in jersey city, new jersey, to parents of polish descent.  
Head Entity: martha stewart  
Tail Entity: new jersey  

Relation: person stateorprovince of birth  
Context: barack obama was born on august 4, 1961, in honolulu, hawaii, where he spent most of his childhood.  
Head Entity: barack obama  
Tail Entity: hawaii  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: After the ceremony, Emily's father, John Smith, shared heartfelt stories about her childhood, bringing tears to many eyes.  
Head Entity: Emily  
Tail Entity: John Smith  

Relation: person parents  
Context: During the family reunion, Michael's mother, Sarah Johnson, prepared her famous lasagna, which everyone eagerly devoured.  
Head Entity: Michael  
Tail Entity: Sarah Johnson  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson finally received a promotion at Tech Innovations, where she has been a key player in the development team.  
Head Entity: Sarah Thompson  
Tail Entity: Tech Innovations  

Relation: person employee of  
Context: John Smith has been with Global Finance for over a decade, where he has climbed the ranks to become the senior analyst in the investment division.  
Head Entity: John Smith  
Tail Entity: Global Finance  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his impactful novels, passed away peacefully in his sleep at his residence in california last night.  
Head Entity: john doe  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, succumbed to her health issues and died in a hospital in nevada.  
Head Entity: elizabeth taylor  
Tail Entity: nevada  
Mixup data size:  225
MixupTrain:  epoch  0, batch     0 | loss: 2.4702420
MixupTrain:  epoch  0, batch     1 | loss: 2.2671685
MixupTrain:  epoch  0, batch     2 | loss: 2.5711613
MixupTrain:  epoch  0, batch     3 | loss: 2.2192721
MixupTrain:  epoch  0, batch     4 | loss: 2.1955619
MixupTrain:  epoch  0, batch     5 | loss: 2.4383302
MixupTrain:  epoch  0, batch     6 | loss: 2.5452919
MixupTrain:  epoch  0, batch     7 | loss: 2.4281740
MixupTrain:  epoch  0, batch     8 | loss: 2.1406894
MixupTrain:  epoch  0, batch     9 | loss: 2.2690187
MixupTrain:  epoch  0, batch    10 | loss: 2.3113546
MixupTrain:  epoch  0, batch    11 | loss: 1.8783604
MixupTrain:  epoch  0, batch    12 | loss: 1.8523889
MixupTrain:  epoch  0, batch    13 | loss: 2.2078500
MixupTrain:  epoch  0, batch    14 | loss: 1.1088614
MemoryTrain:  epoch  0, batch     0 | loss: 1.4626372
MemoryTrain:  epoch  0, batch     1 | loss: 2.4364395
MemoryTrain:  epoch  0, batch     2 | loss: 2.1747196
MemoryTrain:  epoch  0, batch     3 | loss: 2.1961021
MemoryTrain:  epoch  0, batch     4 | loss: 2.4359283
MemoryTrain:  epoch  0, batch     5 | loss: 2.0903907
MemoryTrain:  epoch  1, batch     0 | loss: 1.6305859
MemoryTrain:  epoch  1, batch     1 | loss: 2.5006843
MemoryTrain:  epoch  1, batch     2 | loss: 1.8921918
MemoryTrain:  epoch  1, batch     3 | loss: 2.0051513
MemoryTrain:  epoch  1, batch     4 | loss: 1.9611766
MemoryTrain:  epoch  1, batch     5 | loss: 1.8698525
MemoryTrain:  epoch  2, batch     0 | loss: 1.6127307
MemoryTrain:  epoch  2, batch     1 | loss: 1.3706015
MemoryTrain:  epoch  2, batch     2 | loss: 1.8163545
MemoryTrain:  epoch  2, batch     3 | loss: 1.6147851
MemoryTrain:  epoch  2, batch     4 | loss: 2.0204806
MemoryTrain:  epoch  2, batch     5 | loss: 1.6587300
MemoryTrain:  epoch  3, batch     0 | loss: 1.8431046
MemoryTrain:  epoch  3, batch     1 | loss: 1.6010706
MemoryTrain:  epoch  3, batch     2 | loss: 1.6399908
MemoryTrain:  epoch  3, batch     3 | loss: 1.3662215
MemoryTrain:  epoch  3, batch     4 | loss: 1.4789195
MemoryTrain:  epoch  3, batch     5 | loss: 1.4466145
MemoryTrain:  epoch  4, batch     0 | loss: 1.4822096
MemoryTrain:  epoch  4, batch     1 | loss: 1.3707980
MemoryTrain:  epoch  4, batch     2 | loss: 1.4656646
MemoryTrain:  epoch  4, batch     3 | loss: 1.5888813
MemoryTrain:  epoch  4, batch     4 | loss: 1.4357116
MemoryTrain:  epoch  4, batch     5 | loss: 1.5129602
MemoryTrain:  epoch  5, batch     0 | loss: 1.3815454
MemoryTrain:  epoch  5, batch     1 | loss: 1.4155767
MemoryTrain:  epoch  5, batch     2 | loss: 1.3502982
MemoryTrain:  epoch  5, batch     3 | loss: 1.5999019
MemoryTrain:  epoch  5, batch     4 | loss: 1.3513227
MemoryTrain:  epoch  5, batch     5 | loss: 1.3907902
MemoryTrain:  epoch  6, batch     0 | loss: 1.3246667
MemoryTrain:  epoch  6, batch     1 | loss: 1.4299603
MemoryTrain:  epoch  6, batch     2 | loss: 1.5183957
MemoryTrain:  epoch  6, batch     3 | loss: 1.4960916
MemoryTrain:  epoch  6, batch     4 | loss: 1.3803730
MemoryTrain:  epoch  6, batch     5 | loss: 1.2917935
MemoryTrain:  epoch  7, batch     0 | loss: 1.2940094
MemoryTrain:  epoch  7, batch     1 | loss: 1.3522344
MemoryTrain:  epoch  7, batch     2 | loss: 1.3186796
MemoryTrain:  epoch  7, batch     3 | loss: 1.3837602
MemoryTrain:  epoch  7, batch     4 | loss: 1.3883944
MemoryTrain:  epoch  7, batch     5 | loss: 1.3365784
MemoryTrain:  epoch  8, batch     0 | loss: 1.2747607
MemoryTrain:  epoch  8, batch     1 | loss: 1.3928876
MemoryTrain:  epoch  8, batch     2 | loss: 1.3671472
MemoryTrain:  epoch  8, batch     3 | loss: 1.3058887
MemoryTrain:  epoch  8, batch     4 | loss: 1.3969088
MemoryTrain:  epoch  8, batch     5 | loss: 1.3123044
MemoryTrain:  epoch  9, batch     0 | loss: 1.3683472
MemoryTrain:  epoch  9, batch     1 | loss: 1.3048227
MemoryTrain:  epoch  9, batch     2 | loss: 1.3438792
MemoryTrain:  epoch  9, batch     3 | loss: 1.3859613
MemoryTrain:  epoch  9, batch     4 | loss: 1.2929082
MemoryTrain:  epoch  9, batch     5 | loss: 1.3843123
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   
[EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   
[EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   
[EVAL] batch:    5 | acc: 56.25%,  total acc: 72.92%   
[EVAL] batch:    6 | acc: 56.25%,  total acc: 70.54%   
[EVAL] batch:    7 | acc: 93.75%,  total acc: 73.44%   
[EVAL] batch:    8 | acc: 93.75%,  total acc: 75.69%   
[EVAL] batch:    9 | acc: 87.50%,  total acc: 76.88%   
[EVAL] batch:   10 | acc: 81.25%,  total acc: 77.27%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 78.12%   
[EVAL] batch:   12 | acc: 81.25%,  total acc: 78.37%   
[EVAL] batch:   13 | acc: 25.00%,  total acc: 74.55%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   
[EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   
[EVAL] batch:    2 | acc: 50.00%,  total acc: 60.42%   
[EVAL] batch:    3 | acc: 37.50%,  total acc: 54.69%   
[EVAL] batch:    4 | acc: 56.25%,  total acc: 55.00%   
[EVAL] batch:    5 | acc: 56.25%,  total acc: 55.21%   
[EVAL] batch:    6 | acc: 75.00%,  total acc: 58.04%   
[EVAL] batch:    7 | acc: 50.00%,  total acc: 57.03%   
[EVAL] batch:    8 | acc: 75.00%,  total acc: 59.03%   
[EVAL] batch:    9 | acc: 75.00%,  total acc: 60.62%   
[EVAL] batch:   10 | acc: 68.75%,  total acc: 61.36%   
[EVAL] batch:   11 | acc: 62.50%,  total acc: 61.46%   
[EVAL] batch:   12 | acc: 25.00%,  total acc: 58.65%   
[EVAL] batch:   13 | acc: 25.00%,  total acc: 56.25%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 57.08%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 57.03%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 58.09%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 58.33%   
[EVAL] batch:   18 | acc: 68.75%,  total acc: 58.88%   
[EVAL] batch:   19 | acc: 81.25%,  total acc: 60.00%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 61.90%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 63.35%   
[EVAL] batch:   22 | acc: 93.75%,  total acc: 64.67%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 65.89%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 67.25%   
[EVAL] batch:   25 | acc: 93.75%,  total acc: 68.27%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 69.21%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 70.31%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 71.34%   
[EVAL] batch:   29 | acc: 87.50%,  total acc: 71.88%   
[EVAL] batch:   30 | acc: 100.00%,  total acc: 72.78%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 73.44%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 72.35%   
[EVAL] batch:   33 | acc: 0.00%,  total acc: 70.22%   
[EVAL] batch:   34 | acc: 0.00%,  total acc: 68.21%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 66.32%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 64.53%   
[EVAL] batch:   37 | acc: 0.00%,  total acc: 62.83%   
[EVAL] batch:   38 | acc: 18.75%,  total acc: 61.70%   
[EVAL] batch:   39 | acc: 68.75%,  total acc: 61.88%   
[EVAL] batch:   40 | acc: 75.00%,  total acc: 62.20%   
[EVAL] batch:   41 | acc: 37.50%,  total acc: 61.61%   
[EVAL] batch:   42 | acc: 31.25%,  total acc: 60.90%   
[EVAL] batch:   43 | acc: 68.75%,  total acc: 61.08%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 61.94%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 62.77%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 63.56%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 64.32%   
[EVAL] batch:   48 | acc: 100.00%,  total acc: 65.05%   
[EVAL] batch:   49 | acc: 75.00%,  total acc: 65.25%   
[EVAL] batch:   50 | acc: 93.75%,  total acc: 65.81%   
[EVAL] batch:   51 | acc: 62.50%,  total acc: 65.75%   
[EVAL] batch:   52 | acc: 56.25%,  total acc: 65.57%   
[EVAL] batch:   53 | acc: 56.25%,  total acc: 65.39%   
[EVAL] batch:   54 | acc: 62.50%,  total acc: 65.34%   
[EVAL] batch:   55 | acc: 56.25%,  total acc: 65.18%   
[EVAL] batch:   56 | acc: 50.00%,  total acc: 64.91%   
[EVAL] batch:   57 | acc: 93.75%,  total acc: 65.41%   
[EVAL] batch:   58 | acc: 87.50%,  total acc: 65.78%   
[EVAL] batch:   59 | acc: 100.00%,  total acc: 66.35%   
[EVAL] batch:   60 | acc: 100.00%,  total acc: 66.91%   
[EVAL] batch:   61 | acc: 100.00%,  total acc: 67.44%   
[EVAL] batch:   62 | acc: 100.00%,  total acc: 67.96%   
[EVAL] batch:   63 | acc: 100.00%,  total acc: 68.46%   
[EVAL] batch:   64 | acc: 100.00%,  total acc: 68.94%   
[EVAL] batch:   65 | acc: 50.00%,  total acc: 68.66%   
[EVAL] batch:   66 | acc: 75.00%,  total acc: 68.75%   
[EVAL] batch:   67 | acc: 81.25%,  total acc: 68.93%   
[EVAL] batch:   68 | acc: 68.75%,  total acc: 68.93%   
[EVAL] batch:   69 | acc: 75.00%,  total acc: 69.02%   
[EVAL] batch:   70 | acc: 56.25%,  total acc: 68.84%   
[EVAL] batch:   71 | acc: 56.25%,  total acc: 68.66%   
[EVAL] batch:   72 | acc: 50.00%,  total acc: 68.41%   
[EVAL] batch:   73 | acc: 62.50%,  total acc: 68.33%   
[EVAL] batch:   74 | acc: 50.00%,  total acc: 68.08%   
[EVAL] batch:   75 | acc: 62.50%,  total acc: 68.01%   
[EVAL] batch:   76 | acc: 75.00%,  total acc: 68.10%   
[EVAL] batch:   77 | acc: 100.00%,  total acc: 68.51%   
[EVAL] batch:   78 | acc: 87.50%,  total acc: 68.75%   
[EVAL] batch:   79 | acc: 100.00%,  total acc: 69.14%   
[EVAL] batch:   80 | acc: 56.25%,  total acc: 68.98%   
[EVAL] batch:   81 | acc: 25.00%,  total acc: 68.45%   
[EVAL] batch:   82 | acc: 25.00%,  total acc: 67.92%   
[EVAL] batch:   83 | acc: 37.50%,  total acc: 67.56%   
[EVAL] batch:   84 | acc: 37.50%,  total acc: 67.21%   
[EVAL] batch:   85 | acc: 62.50%,  total acc: 67.15%   
[EVAL] batch:   86 | acc: 87.50%,  total acc: 67.39%   
[EVAL] batch:   87 | acc: 75.00%,  total acc: 67.47%   
[EVAL] batch:   88 | acc: 75.00%,  total acc: 67.56%   
[EVAL] batch:   89 | acc: 75.00%,  total acc: 67.64%   
[EVAL] batch:   90 | acc: 56.25%,  total acc: 67.51%   
[EVAL] batch:   91 | acc: 62.50%,  total acc: 67.46%   
[EVAL] batch:   92 | acc: 93.75%,  total acc: 67.74%   
[EVAL] batch:   93 | acc: 93.75%,  total acc: 68.02%   
[EVAL] batch:   94 | acc: 87.50%,  total acc: 68.22%   
[EVAL] batch:   95 | acc: 81.25%,  total acc: 68.36%   
[EVAL] batch:   96 | acc: 87.50%,  total acc: 68.56%   
[EVAL] batch:   97 | acc: 81.25%,  total acc: 68.69%   
[EVAL] batch:   98 | acc: 18.75%,  total acc: 68.18%   
cur_acc:  ['0.8542', '0.8333', '0.4297', '0.8839', '0.6958', '0.7455']
his_acc:  ['0.8542', '0.8400', '0.6930', '0.6919', '0.7125', '0.6818']
CurrentTrain: epoch  0, batch     0 | loss: 7.5799923
CurrentTrain: epoch  0, batch     1 | loss: 8.1827822
CurrentTrain: epoch  1, batch     0 | loss: 7.3237553
CurrentTrain: epoch  1, batch     1 | loss: 6.6803045
CurrentTrain: epoch  2, batch     0 | loss: 6.8806410
CurrentTrain: epoch  2, batch     1 | loss: 6.4503207
CurrentTrain: epoch  3, batch     0 | loss: 6.1759672
CurrentTrain: epoch  3, batch     1 | loss: 6.2751760
CurrentTrain: epoch  4, batch     0 | loss: 6.1122561
CurrentTrain: epoch  4, batch     1 | loss: 5.7421703
CurrentTrain: epoch  5, batch     0 | loss: 5.4576197
CurrentTrain: epoch  5, batch     1 | loss: 5.7409339
CurrentTrain: epoch  6, batch     0 | loss: 5.0556283
CurrentTrain: epoch  6, batch     1 | loss: 5.2470374
CurrentTrain: epoch  7, batch     0 | loss: 4.8224802
CurrentTrain: epoch  7, batch     1 | loss: 5.5866537
CurrentTrain: epoch  8, batch     0 | loss: 4.6249561
CurrentTrain: epoch  8, batch     1 | loss: 5.0936518
CurrentTrain: epoch  9, batch     0 | loss: 4.4345770
CurrentTrain: epoch  9, batch     1 | loss: 4.7889924
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: venture fund buys sporting chain highland capital 's consumer fund includes lululemon athletica , a yoga retailer , and o beverages , a flavored water company developed by tom first , one of the two `` juice guys '' who cofounded nantucket nectars .
Head Entity: highland capital
Tail Entity: o beverages
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: The tech giant Alphabet Inc. has several subsidiaries, including YouTube, which has become a leading platform for video content, and Waymo, which focuses on self-driving technology.  
Head Entity: Alphabet Inc.  
Tail Entity: Waymo  

Relation: organization subsidiaries  
Context: The multinational corporation Procter & Gamble owns various subsidiaries, such as Gillette, which specializes in personal care products, and Tide, known for its laundry detergents.  
Head Entity: Procter & Gamble  
Tail Entity: Gillette  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant Alphabet Inc. announced that it would acquire the innovative startup DeepMind Technologies, which has been a subsidiary of Alphabet since 2015. This acquisition is expected to enhance Alphabet's capabilities in artificial intelligence.  
Head Entity: DeepMind Technologies  
Tail Entity: Alphabet Inc.  

Relation: organization parents  
Context: The historical records indicate that the renowned publishing house Penguin Random House was formed through the merger of Penguin Group and Random House, both of which have a rich legacy in the literary world.  
Head Entity: Penguin Random House  
Tail Entity: Penguin Group  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the national security council -lrb- nsc -rrb- made the demand at talks chaired by president hamid karzai , who has been vocal in condemning international forces he believes are responsible for the incident last saturday in the eastern flashpoint of kunar .
Head Entity: national security council
Tail Entity: nsc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: the world health organization -lrb- who -rrb- has been instrumental in coordinating global responses to health emergencies, including the recent pandemic.  
Head Entity: world health organization  
Tail Entity: who  

Relation: organization alternate names  
Context: the federal bureau of investigation -lrb- fbi -rrb- is known for its role in investigating federal crimes and maintaining national security.  
Head Entity: federal bureau of investigation  
Tail Entity: fbi  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ in 2015, tech giant apple inc. announced plans to expand its operations in cupertino, california, where it has been headquartered since its founding.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: organization city of headquarters  
Context: ------ the multinational corporation samsung electronics is based in suwon, south korea, and has been a leader in the technology sector for decades.  
Head Entity: samsung electronics  
Tail Entity: suwon  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: `` holly and sanjaya are headed to -lsb- the hawaiian island of -rsb- kauai tomorrow morning so she can meet his parents . ''
Head Entity: she
Tail Entity: sanjaya
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: `` during the family reunion, john introduced his sister to everyone, and they all had a great time reminiscing about their childhood. ''  
Head Entity: his sister  
Tail Entity: john  

Relation: person siblings  
Context: `` after the graduation ceremony, emily celebrated with her brother, who had always been her biggest supporter throughout school. ''  
Head Entity: her brother  
Tail Entity: emily  
Mixup data size:  255
MixupTrain:  epoch  0, batch     0 | loss: 2.4234486
MixupTrain:  epoch  0, batch     1 | loss: 1.8437656
MixupTrain:  epoch  0, batch     2 | loss: 2.0330212
MixupTrain:  epoch  0, batch     3 | loss: 2.4739370
MixupTrain:  epoch  0, batch     4 | loss: 2.8307323
MixupTrain:  epoch  0, batch     5 | loss: 2.3752010
MixupTrain:  epoch  0, batch     6 | loss: 2.2826877
MixupTrain:  epoch  0, batch     7 | loss: 2.4788647
MixupTrain:  epoch  0, batch     8 | loss: 2.4700484
MixupTrain:  epoch  0, batch     9 | loss: 2.1011772
MixupTrain:  epoch  0, batch    10 | loss: 2.3472569
MixupTrain:  epoch  0, batch    11 | loss: 2.5884051
MixupTrain:  epoch  0, batch    12 | loss: 2.1272871
MixupTrain:  epoch  0, batch    13 | loss: 2.0605335
MixupTrain:  epoch  0, batch    14 | loss: 2.2063997
MixupTrain:  epoch  0, batch    15 | loss: 2.4245191
MemoryTrain:  epoch  0, batch     0 | loss: 1.9472606
MemoryTrain:  epoch  0, batch     1 | loss: 2.0013216
MemoryTrain:  epoch  0, batch     2 | loss: 1.5848305
MemoryTrain:  epoch  0, batch     3 | loss: 2.4828408
MemoryTrain:  epoch  0, batch     4 | loss: 2.6762662
MemoryTrain:  epoch  0, batch     5 | loss: 2.2697101
MemoryTrain:  epoch  0, batch     6 | loss: 2.5725935
MemoryTrain:  epoch  1, batch     0 | loss: 2.0794709
MemoryTrain:  epoch  1, batch     1 | loss: 1.9167677
MemoryTrain:  epoch  1, batch     2 | loss: 2.6399808
MemoryTrain:  epoch  1, batch     3 | loss: 1.8968728
MemoryTrain:  epoch  1, batch     4 | loss: 1.9410549
MemoryTrain:  epoch  1, batch     5 | loss: 1.8236407
MemoryTrain:  epoch  1, batch     6 | loss: 1.4208760
MemoryTrain:  epoch  2, batch     0 | loss: 2.0902448
MemoryTrain:  epoch  2, batch     1 | loss: 1.7783183
MemoryTrain:  epoch  2, batch     2 | loss: 1.6468273
MemoryTrain:  epoch  2, batch     3 | loss: 1.4275467
MemoryTrain:  epoch  2, batch     4 | loss: 1.8055618
MemoryTrain:  epoch  2, batch     5 | loss: 1.4771476
MemoryTrain:  epoch  2, batch     6 | loss: 1.7159965
MemoryTrain:  epoch  3, batch     0 | loss: 1.9132382
MemoryTrain:  epoch  3, batch     1 | loss: 1.5314771
MemoryTrain:  epoch  3, batch     2 | loss: 1.4478267
MemoryTrain:  epoch  3, batch     3 | loss: 1.3193569
MemoryTrain:  epoch  3, batch     4 | loss: 1.7498755
MemoryTrain:  epoch  3, batch     5 | loss: 1.4707981
MemoryTrain:  epoch  3, batch     6 | loss: 1.5292664
MemoryTrain:  epoch  4, batch     0 | loss: 1.5200335
MemoryTrain:  epoch  4, batch     1 | loss: 1.4783695
MemoryTrain:  epoch  4, batch     2 | loss: 1.3758992
MemoryTrain:  epoch  4, batch     3 | loss: 1.5911217
MemoryTrain:  epoch  4, batch     4 | loss: 1.6280873
MemoryTrain:  epoch  4, batch     5 | loss: 1.6237592
MemoryTrain:  epoch  4, batch     6 | loss: 1.2966377
MemoryTrain:  epoch  5, batch     0 | loss: 1.5672373
MemoryTrain:  epoch  5, batch     1 | loss: 1.3993669
MemoryTrain:  epoch  5, batch     2 | loss: 1.4370785
MemoryTrain:  epoch  5, batch     3 | loss: 1.5343266
MemoryTrain:  epoch  5, batch     4 | loss: 1.5948750
MemoryTrain:  epoch  5, batch     5 | loss: 1.2705556
MemoryTrain:  epoch  5, batch     6 | loss: 1.2521131
MemoryTrain:  epoch  6, batch     0 | loss: 1.3191850
MemoryTrain:  epoch  6, batch     1 | loss: 1.4372600
MemoryTrain:  epoch  6, batch     2 | loss: 1.4947754
MemoryTrain:  epoch  6, batch     3 | loss: 1.4243305
MemoryTrain:  epoch  6, batch     4 | loss: 1.3181536
MemoryTrain:  epoch  6, batch     5 | loss: 1.2814884
MemoryTrain:  epoch  6, batch     6 | loss: 1.3680286
MemoryTrain:  epoch  7, batch     0 | loss: 1.3705380
MemoryTrain:  epoch  7, batch     1 | loss: 1.3350308
MemoryTrain:  epoch  7, batch     2 | loss: 1.5184238
MemoryTrain:  epoch  7, batch     3 | loss: 1.3472242
MemoryTrain:  epoch  7, batch     4 | loss: 1.2081532
MemoryTrain:  epoch  7, batch     5 | loss: 1.3171357
MemoryTrain:  epoch  7, batch     6 | loss: 1.2712557
MemoryTrain:  epoch  8, batch     0 | loss: 1.3562050
MemoryTrain:  epoch  8, batch     1 | loss: 1.2595344
MemoryTrain:  epoch  8, batch     2 | loss: 1.2941914
MemoryTrain:  epoch  8, batch     3 | loss: 1.4085703
MemoryTrain:  epoch  8, batch     4 | loss: 1.2603476
MemoryTrain:  epoch  8, batch     5 | loss: 1.2583529
MemoryTrain:  epoch  8, batch     6 | loss: 1.4777231
MemoryTrain:  epoch  9, batch     0 | loss: 1.2398850
MemoryTrain:  epoch  9, batch     1 | loss: 1.3108926
MemoryTrain:  epoch  9, batch     2 | loss: 1.3375905
MemoryTrain:  epoch  9, batch     3 | loss: 1.3677837
MemoryTrain:  epoch  9, batch     4 | loss: 1.3047054
MemoryTrain:  epoch  9, batch     5 | loss: 1.2649479
MemoryTrain:  epoch  9, batch     6 | loss: 1.3403739
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   
[EVAL] batch:    1 | acc: 43.75%,  total acc: 34.38%   
[EVAL] batch:    2 | acc: 25.00%,  total acc: 31.25%   
[EVAL] batch:    3 | acc: 37.50%,  total acc: 32.81%   
[EVAL] batch:    4 | acc: 43.75%,  total acc: 35.00%   
[EVAL] batch:    5 | acc: 43.75%,  total acc: 36.46%   
[EVAL] batch:    6 | acc: 37.50%,  total acc: 36.61%   
[EVAL] batch:    7 | acc: 68.75%,  total acc: 40.62%   
[EVAL] batch:    8 | acc: 68.75%,  total acc: 43.75%   
[EVAL] batch:    9 | acc: 50.00%,  total acc: 44.38%   
[EVAL] batch:   10 | acc: 68.75%,  total acc: 46.59%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 50.00%   
[EVAL] batch:   12 | acc: 62.50%,  total acc: 50.96%   
[EVAL] batch:   13 | acc: 100.00%,  total acc: 54.46%   
[EVAL] batch:   14 | acc: 93.75%,  total acc: 57.08%   
[EVAL] batch:   15 | acc: 100.00%,  total acc: 59.77%   
[EVAL] batch:   16 | acc: 100.00%,  total acc: 62.13%   
[EVAL] batch:   17 | acc: 87.50%,  total acc: 63.54%   
[EVAL] batch:   18 | acc: 0.00%,  total acc: 60.20%   
[EVAL] batch:   19 | acc: 0.00%,  total acc: 57.19%   
[EVAL] batch:   20 | acc: 0.00%,  total acc: 54.46%   
[EVAL] batch:   21 | acc: 0.00%,  total acc: 51.99%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   
[EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   
[EVAL] batch:    2 | acc: 62.50%,  total acc: 68.75%   
[EVAL] batch:    3 | acc: 56.25%,  total acc: 65.62%   
[EVAL] batch:    4 | acc: 56.25%,  total acc: 63.75%   
[EVAL] batch:    5 | acc: 43.75%,  total acc: 60.42%   
[EVAL] batch:    6 | acc: 56.25%,  total acc: 59.82%   
[EVAL] batch:    7 | acc: 50.00%,  total acc: 58.59%   
[EVAL] batch:    8 | acc: 56.25%,  total acc: 58.33%   
[EVAL] batch:    9 | acc: 56.25%,  total acc: 58.13%   
[EVAL] batch:   10 | acc: 50.00%,  total acc: 57.39%   
[EVAL] batch:   11 | acc: 62.50%,  total acc: 57.81%   
[EVAL] batch:   12 | acc: 31.25%,  total acc: 55.77%   
[EVAL] batch:   13 | acc: 25.00%,  total acc: 53.57%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 54.58%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 54.69%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 55.88%   
[EVAL] batch:   17 | acc: 62.50%,  total acc: 56.25%   
[EVAL] batch:   18 | acc: 81.25%,  total acc: 57.57%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 59.06%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 61.01%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 62.50%   
[EVAL] batch:   22 | acc: 93.75%,  total acc: 63.86%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 65.10%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 66.50%   
[EVAL] batch:   25 | acc: 100.00%,  total acc: 67.79%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 68.75%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 69.87%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 70.91%   
[EVAL] batch:   29 | acc: 81.25%,  total acc: 71.25%   
[EVAL] batch:   30 | acc: 93.75%,  total acc: 71.98%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 72.66%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 71.59%   
[EVAL] batch:   33 | acc: 0.00%,  total acc: 69.49%   
[EVAL] batch:   34 | acc: 0.00%,  total acc: 67.50%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 65.62%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 63.85%   
[EVAL] batch:   37 | acc: 0.00%,  total acc: 62.17%   
[EVAL] batch:   38 | acc: 31.25%,  total acc: 61.38%   
[EVAL] batch:   39 | acc: 75.00%,  total acc: 61.72%   
[EVAL] batch:   40 | acc: 56.25%,  total acc: 61.59%   
[EVAL] batch:   41 | acc: 56.25%,  total acc: 61.46%   
[EVAL] batch:   42 | acc: 50.00%,  total acc: 61.19%   
[EVAL] batch:   43 | acc: 75.00%,  total acc: 61.51%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 62.36%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 63.18%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 63.96%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 64.71%   
[EVAL] batch:   48 | acc: 100.00%,  total acc: 65.43%   
[EVAL] batch:   49 | acc: 75.00%,  total acc: 65.62%   
[EVAL] batch:   50 | acc: 93.75%,  total acc: 66.18%   
[EVAL] batch:   51 | acc: 56.25%,  total acc: 65.99%   
[EVAL] batch:   52 | acc: 62.50%,  total acc: 65.92%   
[EVAL] batch:   53 | acc: 62.50%,  total acc: 65.86%   
[EVAL] batch:   54 | acc: 75.00%,  total acc: 66.02%   
[EVAL] batch:   55 | acc: 62.50%,  total acc: 65.96%   
[EVAL] batch:   56 | acc: 68.75%,  total acc: 66.01%   
[EVAL] batch:   57 | acc: 93.75%,  total acc: 66.49%   
[EVAL] batch:   58 | acc: 87.50%,  total acc: 66.84%   
[EVAL] batch:   59 | acc: 100.00%,  total acc: 67.40%   
[EVAL] batch:   60 | acc: 100.00%,  total acc: 67.93%   
[EVAL] batch:   61 | acc: 100.00%,  total acc: 68.45%   
[EVAL] batch:   62 | acc: 100.00%,  total acc: 68.95%   
[EVAL] batch:   63 | acc: 100.00%,  total acc: 69.43%   
[EVAL] batch:   64 | acc: 100.00%,  total acc: 69.90%   
[EVAL] batch:   65 | acc: 37.50%,  total acc: 69.41%   
[EVAL] batch:   66 | acc: 18.75%,  total acc: 68.66%   
[EVAL] batch:   67 | acc: 43.75%,  total acc: 68.29%   
[EVAL] batch:   68 | acc: 75.00%,  total acc: 68.39%   
[EVAL] batch:   69 | acc: 93.75%,  total acc: 68.75%   
[EVAL] batch:   70 | acc: 50.00%,  total acc: 68.49%   
[EVAL] batch:   71 | acc: 43.75%,  total acc: 68.14%   
[EVAL] batch:   72 | acc: 37.50%,  total acc: 67.72%   
[EVAL] batch:   73 | acc: 62.50%,  total acc: 67.65%   
[EVAL] batch:   74 | acc: 37.50%,  total acc: 67.25%   
[EVAL] batch:   75 | acc: 56.25%,  total acc: 67.11%   
[EVAL] batch:   76 | acc: 75.00%,  total acc: 67.21%   
[EVAL] batch:   77 | acc: 100.00%,  total acc: 67.63%   
[EVAL] batch:   78 | acc: 81.25%,  total acc: 67.80%   
[EVAL] batch:   79 | acc: 100.00%,  total acc: 68.20%   
[EVAL] batch:   80 | acc: 43.75%,  total acc: 67.90%   
[EVAL] batch:   81 | acc: 31.25%,  total acc: 67.45%   
[EVAL] batch:   82 | acc: 31.25%,  total acc: 67.02%   
[EVAL] batch:   83 | acc: 50.00%,  total acc: 66.82%   
[EVAL] batch:   84 | acc: 50.00%,  total acc: 66.62%   
[EVAL] batch:   85 | acc: 68.75%,  total acc: 66.64%   
[EVAL] batch:   86 | acc: 75.00%,  total acc: 66.74%   
[EVAL] batch:   87 | acc: 68.75%,  total acc: 66.76%   
[EVAL] batch:   88 | acc: 68.75%,  total acc: 66.78%   
[EVAL] batch:   89 | acc: 81.25%,  total acc: 66.94%   
[EVAL] batch:   90 | acc: 43.75%,  total acc: 66.69%   
[EVAL] batch:   91 | acc: 56.25%,  total acc: 66.58%   
[EVAL] batch:   92 | acc: 93.75%,  total acc: 66.87%   
[EVAL] batch:   93 | acc: 87.50%,  total acc: 67.09%   
[EVAL] batch:   94 | acc: 87.50%,  total acc: 67.30%   
[EVAL] batch:   95 | acc: 87.50%,  total acc: 67.51%   
[EVAL] batch:   96 | acc: 81.25%,  total acc: 67.65%   
[EVAL] batch:   97 | acc: 75.00%,  total acc: 67.73%   
[EVAL] batch:   98 | acc: 25.00%,  total acc: 67.30%   
[EVAL] batch:   99 | acc: 43.75%,  total acc: 67.06%   
[EVAL] batch:  100 | acc: 43.75%,  total acc: 66.83%   
[EVAL] batch:  101 | acc: 12.50%,  total acc: 66.30%   
[EVAL] batch:  102 | acc: 43.75%,  total acc: 66.08%   
[EVAL] batch:  103 | acc: 50.00%,  total acc: 65.93%   
[EVAL] batch:  104 | acc: 37.50%,  total acc: 65.65%   
[EVAL] batch:  105 | acc: 50.00%,  total acc: 65.51%   
[EVAL] batch:  106 | acc: 68.75%,  total acc: 65.54%   
[EVAL] batch:  107 | acc: 68.75%,  total acc: 65.57%   
[EVAL] batch:  108 | acc: 56.25%,  total acc: 65.48%   
[EVAL] batch:  109 | acc: 75.00%,  total acc: 65.57%   
[EVAL] batch:  110 | acc: 75.00%,  total acc: 65.65%   
[EVAL] batch:  111 | acc: 75.00%,  total acc: 65.74%   
[EVAL] batch:  112 | acc: 100.00%,  total acc: 66.04%   
[EVAL] batch:  113 | acc: 93.75%,  total acc: 66.28%   
[EVAL] batch:  114 | acc: 100.00%,  total acc: 66.58%   
[EVAL] batch:  115 | acc: 87.50%,  total acc: 66.76%   
[EVAL] batch:  116 | acc: 62.50%,  total acc: 66.72%   
[EVAL] batch:  117 | acc: 0.00%,  total acc: 66.15%   
[EVAL] batch:  118 | acc: 0.00%,  total acc: 65.60%   
[EVAL] batch:  119 | acc: 0.00%,  total acc: 65.05%   
[EVAL] batch:  120 | acc: 0.00%,  total acc: 64.51%   
cur_acc:  ['0.8542', '0.8333', '0.4297', '0.8839', '0.6958', '0.7455', '0.5199']
his_acc:  ['0.8542', '0.8400', '0.6930', '0.6919', '0.7125', '0.6818', '0.6451']
CurrentTrain: epoch  0, batch     0 | loss: 4.1565099
CurrentTrain: epoch  0, batch     1 | loss: 5.2210193
CurrentTrain: epoch  1, batch     0 | loss: 3.2131433
CurrentTrain: epoch  1, batch     1 | loss: 3.2910609
CurrentTrain: epoch  2, batch     0 | loss: 2.7870965
CurrentTrain: epoch  2, batch     1 | loss: 2.8315837
CurrentTrain: epoch  3, batch     0 | loss: 2.5840318
CurrentTrain: epoch  3, batch     1 | loss: 2.2900615
CurrentTrain: epoch  4, batch     0 | loss: 2.2150452
CurrentTrain: epoch  4, batch     1 | loss: 2.0308626
CurrentTrain: epoch  5, batch     0 | loss: 2.3104167
CurrentTrain: epoch  5, batch     1 | loss: 2.1856527
CurrentTrain: epoch  6, batch     0 | loss: 2.1801336
CurrentTrain: epoch  6, batch     1 | loss: 2.0787523
CurrentTrain: epoch  7, batch     0 | loss: 1.9772401
CurrentTrain: epoch  7, batch     1 | loss: 2.0038455
CurrentTrain: epoch  8, batch     0 | loss: 1.8704865
CurrentTrain: epoch  8, batch     1 | loss: 1.8851880
CurrentTrain: epoch  9, batch     0 | loss: 1.8438570
CurrentTrain: epoch  9, batch     1 | loss: 1.9588182
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling with lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned scientist, dr. emily white, tragically lost her life due to a car accident while returning from a conference.  
Head Entity: dr. emily white  
Tail Entity: car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: clashes in late august in karbala between the mahdi army and a rival shiite militia , the badr organization , left at least 50 people dead .
Head Entity: badr organization
Tail Entity: shiite
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The recent conference held by the Islamic Society of North America highlighted the contributions of various Muslim organizations to community service and interfaith dialogue.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  

Relation: organization political religious affiliation  
Context: During the annual meeting, the Catholic Charities organization discussed its role in supporting local parishes and promoting social justice initiatives within the Catholic community.  
Head Entity: Catholic Charities  
Tail Entity: Catholic  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: new york-based veteran taiwan journalist chang chao-ying and the washington - based formosan association for public affairs -lrb- fapa -rrb- have jointly established a `` freedom of speech prize '' aimed at helping taiwan media improve their professional quality and ethics .
Head Entity: formosan association for public affairs
Tail Entity: washington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: The tech giant Google has its headquarters in Mountain View, California, where it continues to innovate and expand its services globally.  
Head Entity: Google  
Tail Entity: California  

Relation: organization stateorprovince of headquarters  
Context: The multinational corporation Unilever operates its North American headquarters in Englewood Cliffs, New Jersey, focusing on sustainable living and consumer goods.  
Head Entity: Unilever  
Tail Entity: New Jersey  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: During the family reunion, it was revealed that Sarah's cousin, Michael, had recently graduated from college with honors.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person other family  
Context: In her memoir, Lisa shared heartwarming stories about her grandmother, Evelyn, who played a significant role in her upbringing.  
Head Entity: Lisa  
Tail Entity: Evelyn  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  
Mixup data size:  284
MixupTrain:  epoch  0, batch     0 | loss: 2.0535610
MixupTrain:  epoch  0, batch     1 | loss: 2.3134234
MixupTrain:  epoch  0, batch     2 | loss: 2.0519018
MixupTrain:  epoch  0, batch     3 | loss: 1.9935195
MixupTrain:  epoch  0, batch     4 | loss: 2.5159099
MixupTrain:  epoch  0, batch     5 | loss: 1.9028875
MixupTrain:  epoch  0, batch     6 | loss: 2.4756041
MixupTrain:  epoch  0, batch     7 | loss: 2.3439999
MixupTrain:  epoch  0, batch     8 | loss: 2.0932672
MixupTrain:  epoch  0, batch     9 | loss: 2.2549284
MixupTrain:  epoch  0, batch    10 | loss: 1.9943662
MixupTrain:  epoch  0, batch    11 | loss: 1.7982267
MixupTrain:  epoch  0, batch    12 | loss: 1.9533757
MixupTrain:  epoch  0, batch    13 | loss: 2.2098050
MixupTrain:  epoch  0, batch    14 | loss: 2.0935254
MixupTrain:  epoch  0, batch    15 | loss: 1.7037477
MixupTrain:  epoch  0, batch    16 | loss: 2.1212943
MixupTrain:  epoch  0, batch    17 | loss: 2.4286141
MemoryTrain:  epoch  0, batch     0 | loss: 1.9532683
MemoryTrain:  epoch  0, batch     1 | loss: 1.8900534
MemoryTrain:  epoch  0, batch     2 | loss: 2.0852242
MemoryTrain:  epoch  0, batch     3 | loss: 2.2929642
MemoryTrain:  epoch  0, batch     4 | loss: 2.2716060
MemoryTrain:  epoch  0, batch     5 | loss: 2.1309719
MemoryTrain:  epoch  0, batch     6 | loss: 2.7687550
MemoryTrain:  epoch  0, batch     7 | loss: 2.4244375
MemoryTrain:  epoch  1, batch     0 | loss: 2.2173827
MemoryTrain:  epoch  1, batch     1 | loss: 2.1918950
MemoryTrain:  epoch  1, batch     2 | loss: 1.7178447
MemoryTrain:  epoch  1, batch     3 | loss: 1.6931112
MemoryTrain:  epoch  1, batch     4 | loss: 2.3772254
MemoryTrain:  epoch  1, batch     5 | loss: 2.0618174
MemoryTrain:  epoch  1, batch     6 | loss: 1.8577402
MemoryTrain:  epoch  1, batch     7 | loss: 1.4169929
MemoryTrain:  epoch  2, batch     0 | loss: 1.6778893
MemoryTrain:  epoch  2, batch     1 | loss: 1.5640767
MemoryTrain:  epoch  2, batch     2 | loss: 1.7652533
MemoryTrain:  epoch  2, batch     3 | loss: 1.5479457
MemoryTrain:  epoch  2, batch     4 | loss: 2.2966919
MemoryTrain:  epoch  2, batch     5 | loss: 1.5812495
MemoryTrain:  epoch  2, batch     6 | loss: 1.4617381
MemoryTrain:  epoch  2, batch     7 | loss: 1.8389380
MemoryTrain:  epoch  3, batch     0 | loss: 1.8328475
MemoryTrain:  epoch  3, batch     1 | loss: 1.8058251
MemoryTrain:  epoch  3, batch     2 | loss: 1.4113177
MemoryTrain:  epoch  3, batch     3 | loss: 1.5222144
MemoryTrain:  epoch  3, batch     4 | loss: 1.6349456
MemoryTrain:  epoch  3, batch     5 | loss: 1.5527630
MemoryTrain:  epoch  3, batch     6 | loss: 1.5014956
MemoryTrain:  epoch  3, batch     7 | loss: 1.3986659
MemoryTrain:  epoch  4, batch     0 | loss: 1.6432406
MemoryTrain:  epoch  4, batch     1 | loss: 1.4008486
MemoryTrain:  epoch  4, batch     2 | loss: 1.4065040
MemoryTrain:  epoch  4, batch     3 | loss: 1.5279560
MemoryTrain:  epoch  4, batch     4 | loss: 1.7563381
MemoryTrain:  epoch  4, batch     5 | loss: 1.3604736
MemoryTrain:  epoch  4, batch     6 | loss: 1.3207397
MemoryTrain:  epoch  4, batch     7 | loss: 1.6560043
MemoryTrain:  epoch  5, batch     0 | loss: 1.5241098
MemoryTrain:  epoch  5, batch     1 | loss: 1.4742467
MemoryTrain:  epoch  5, batch     2 | loss: 1.4238002
MemoryTrain:  epoch  5, batch     3 | loss: 1.3747073
MemoryTrain:  epoch  5, batch     4 | loss: 1.3438284
MemoryTrain:  epoch  5, batch     5 | loss: 1.3299224
MemoryTrain:  epoch  5, batch     6 | loss: 1.2829070
MemoryTrain:  epoch  5, batch     7 | loss: 1.3077613
MemoryTrain:  epoch  6, batch     0 | loss: 1.2071273
MemoryTrain:  epoch  6, batch     1 | loss: 1.5939823
MemoryTrain:  epoch  6, batch     2 | loss: 1.3864444
MemoryTrain:  epoch  6, batch     3 | loss: 1.3288841
MemoryTrain:  epoch  6, batch     4 | loss: 1.2962902
MemoryTrain:  epoch  6, batch     5 | loss: 1.3670841
MemoryTrain:  epoch  6, batch     6 | loss: 1.4071051
MemoryTrain:  epoch  6, batch     7 | loss: 1.3167697
MemoryTrain:  epoch  7, batch     0 | loss: 1.4474185
MemoryTrain:  epoch  7, batch     1 | loss: 1.2948163
MemoryTrain:  epoch  7, batch     2 | loss: 1.3645681
MemoryTrain:  epoch  7, batch     3 | loss: 1.3688748
MemoryTrain:  epoch  7, batch     4 | loss: 1.3284013
MemoryTrain:  epoch  7, batch     5 | loss: 1.2706521
MemoryTrain:  epoch  7, batch     6 | loss: 1.2590356
MemoryTrain:  epoch  7, batch     7 | loss: 1.2607756
MemoryTrain:  epoch  8, batch     0 | loss: 1.2324157
MemoryTrain:  epoch  8, batch     1 | loss: 1.2275250
MemoryTrain:  epoch  8, batch     2 | loss: 1.3465352
MemoryTrain:  epoch  8, batch     3 | loss: 1.4250791
MemoryTrain:  epoch  8, batch     4 | loss: 1.3157971
MemoryTrain:  epoch  8, batch     5 | loss: 1.3972030
MemoryTrain:  epoch  8, batch     6 | loss: 1.2480937
MemoryTrain:  epoch  8, batch     7 | loss: 1.2852244
MemoryTrain:  epoch  9, batch     0 | loss: 1.3263206
MemoryTrain:  epoch  9, batch     1 | loss: 1.3135421
MemoryTrain:  epoch  9, batch     2 | loss: 1.3017334
MemoryTrain:  epoch  9, batch     3 | loss: 1.2656815
MemoryTrain:  epoch  9, batch     4 | loss: 1.3079967
MemoryTrain:  epoch  9, batch     5 | loss: 1.3834562
MemoryTrain:  epoch  9, batch     6 | loss: 1.2292998
MemoryTrain:  epoch  9, batch     7 | loss: 1.2029121
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 46.88%   
[EVAL] batch:    2 | acc: 43.75%,  total acc: 45.83%   
[EVAL] batch:    3 | acc: 50.00%,  total acc: 46.88%   
[EVAL] batch:    4 | acc: 93.75%,  total acc: 56.25%   
[EVAL] batch:    5 | acc: 93.75%,  total acc: 62.50%   
[EVAL] batch:    6 | acc: 93.75%,  total acc: 66.96%   
[EVAL] batch:    7 | acc: 81.25%,  total acc: 68.75%   
[EVAL] batch:    8 | acc: 81.25%,  total acc: 70.14%   
[EVAL] batch:    9 | acc: 62.50%,  total acc: 69.38%   
[EVAL] batch:   10 | acc: 81.25%,  total acc: 70.45%   
[EVAL] batch:   11 | acc: 87.50%,  total acc: 71.88%   
[EVAL] batch:   12 | acc: 50.00%,  total acc: 70.19%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   
[EVAL] batch:    1 | acc: 56.25%,  total acc: 56.25%   
[EVAL] batch:    2 | acc: 43.75%,  total acc: 52.08%   
[EVAL] batch:    3 | acc: 50.00%,  total acc: 51.56%   
[EVAL] batch:    4 | acc: 43.75%,  total acc: 50.00%   
[EVAL] batch:    5 | acc: 37.50%,  total acc: 47.92%   
[EVAL] batch:    6 | acc: 50.00%,  total acc: 48.21%   
[EVAL] batch:    7 | acc: 50.00%,  total acc: 48.44%   
[EVAL] batch:    8 | acc: 50.00%,  total acc: 48.61%   
[EVAL] batch:    9 | acc: 50.00%,  total acc: 48.75%   
[EVAL] batch:   10 | acc: 62.50%,  total acc: 50.00%   
[EVAL] batch:   11 | acc: 62.50%,  total acc: 51.04%   
[EVAL] batch:   12 | acc: 37.50%,  total acc: 50.00%   
[EVAL] batch:   13 | acc: 25.00%,  total acc: 48.21%   
[EVAL] batch:   14 | acc: 68.75%,  total acc: 49.58%   
[EVAL] batch:   15 | acc: 56.25%,  total acc: 50.00%   
[EVAL] batch:   16 | acc: 75.00%,  total acc: 51.47%   
[EVAL] batch:   17 | acc: 68.75%,  total acc: 52.43%   
[EVAL] batch:   18 | acc: 87.50%,  total acc: 54.28%   
[EVAL] batch:   19 | acc: 87.50%,  total acc: 55.94%   
[EVAL] batch:   20 | acc: 100.00%,  total acc: 58.04%   
[EVAL] batch:   21 | acc: 93.75%,  total acc: 59.66%   
[EVAL] batch:   22 | acc: 93.75%,  total acc: 61.14%   
[EVAL] batch:   23 | acc: 93.75%,  total acc: 62.50%   
[EVAL] batch:   24 | acc: 100.00%,  total acc: 64.00%   
[EVAL] batch:   25 | acc: 93.75%,  total acc: 65.14%   
[EVAL] batch:   26 | acc: 93.75%,  total acc: 66.20%   
[EVAL] batch:   27 | acc: 100.00%,  total acc: 67.41%   
[EVAL] batch:   28 | acc: 100.00%,  total acc: 68.53%   
[EVAL] batch:   29 | acc: 68.75%,  total acc: 68.54%   
[EVAL] batch:   30 | acc: 62.50%,  total acc: 68.35%   
[EVAL] batch:   31 | acc: 93.75%,  total acc: 69.14%   
[EVAL] batch:   32 | acc: 37.50%,  total acc: 68.18%   
[EVAL] batch:   33 | acc: 0.00%,  total acc: 66.18%   
[EVAL] batch:   34 | acc: 0.00%,  total acc: 64.29%   
[EVAL] batch:   35 | acc: 0.00%,  total acc: 62.50%   
[EVAL] batch:   36 | acc: 0.00%,  total acc: 60.81%   
[EVAL] batch:   37 | acc: 0.00%,  total acc: 59.21%   
[EVAL] batch:   38 | acc: 25.00%,  total acc: 58.33%   
[EVAL] batch:   39 | acc: 75.00%,  total acc: 58.75%   
[EVAL] batch:   40 | acc: 68.75%,  total acc: 58.99%   
[EVAL] batch:   41 | acc: 31.25%,  total acc: 58.33%   
[EVAL] batch:   42 | acc: 37.50%,  total acc: 57.85%   
[EVAL] batch:   43 | acc: 75.00%,  total acc: 58.24%   
[EVAL] batch:   44 | acc: 100.00%,  total acc: 59.17%   
[EVAL] batch:   45 | acc: 100.00%,  total acc: 60.05%   
[EVAL] batch:   46 | acc: 100.00%,  total acc: 60.90%   
[EVAL] batch:   47 | acc: 100.00%,  total acc: 61.72%   
[EVAL] batch:   48 | acc: 100.00%,  total acc: 62.50%   
[EVAL] batch:   49 | acc: 75.00%,  total acc: 62.75%   
[EVAL] batch:   50 | acc: 93.75%,  total acc: 63.36%   
[EVAL] batch:   51 | acc: 56.25%,  total acc: 63.22%   
[EVAL] batch:   52 | acc: 50.00%,  total acc: 62.97%   
[EVAL] batch:   53 | acc: 56.25%,  total acc: 62.85%   
[EVAL] batch:   54 | acc: 68.75%,  total acc: 62.95%   
[EVAL] batch:   55 | acc: 43.75%,  total acc: 62.61%   
[EVAL] batch:   56 | acc: 62.50%,  total acc: 62.61%   
[EVAL] batch:   57 | acc: 93.75%,  total acc: 63.15%   
[EVAL] batch:   58 | acc: 81.25%,  total acc: 63.45%   
[EVAL] batch:   59 | acc: 100.00%,  total acc: 64.06%   
[EVAL] batch:   60 | acc: 100.00%,  total acc: 64.65%   
[EVAL] batch:   61 | acc: 100.00%,  total acc: 65.22%   
[EVAL] batch:   62 | acc: 100.00%,  total acc: 65.77%   
[EVAL] batch:   63 | acc: 100.00%,  total acc: 66.31%   
[EVAL] batch:   64 | acc: 100.00%,  total acc: 66.83%   
[EVAL] batch:   65 | acc: 37.50%,  total acc: 66.38%   
[EVAL] batch:   66 | acc: 12.50%,  total acc: 65.58%   
[EVAL] batch:   67 | acc: 25.00%,  total acc: 64.98%   
[EVAL] batch:   68 | acc: 56.25%,  total acc: 64.86%   
[EVAL] batch:   69 | acc: 43.75%,  total acc: 64.55%   
[EVAL] batch:   70 | acc: 31.25%,  total acc: 64.08%   
[EVAL] batch:   71 | acc: 25.00%,  total acc: 63.54%   
[EVAL] batch:   72 | acc: 12.50%,  total acc: 62.84%   
[EVAL] batch:   73 | acc: 50.00%,  total acc: 62.67%   
[EVAL] batch:   74 | acc: 25.00%,  total acc: 62.17%   
[EVAL] batch:   75 | acc: 50.00%,  total acc: 62.01%   
[EVAL] batch:   76 | acc: 68.75%,  total acc: 62.09%   
[EVAL] batch:   77 | acc: 100.00%,  total acc: 62.58%   
[EVAL] batch:   78 | acc: 87.50%,  total acc: 62.90%   
[EVAL] batch:   79 | acc: 100.00%,  total acc: 63.36%   
[EVAL] batch:   80 | acc: 18.75%,  total acc: 62.81%   
[EVAL] batch:   81 | acc: 25.00%,  total acc: 62.35%   
[EVAL] batch:   82 | acc: 18.75%,  total acc: 61.82%   
[EVAL] batch:   83 | acc: 31.25%,  total acc: 61.46%   
[EVAL] batch:   84 | acc: 25.00%,  total acc: 61.03%   
[EVAL] batch:   85 | acc: 75.00%,  total acc: 61.19%   
[EVAL] batch:   86 | acc: 31.25%,  total acc: 60.85%   
[EVAL] batch:   87 | acc: 31.25%,  total acc: 60.51%   
[EVAL] batch:   88 | acc: 25.00%,  total acc: 60.11%   
[EVAL] batch:   89 | acc: 31.25%,  total acc: 59.79%   
[EVAL] batch:   90 | acc: 25.00%,  total acc: 59.41%   
[EVAL] batch:   91 | acc: 50.00%,  total acc: 59.31%   
[EVAL] batch:   92 | acc: 93.75%,  total acc: 59.68%   
[EVAL] batch:   93 | acc: 87.50%,  total acc: 59.97%   
[EVAL] batch:   94 | acc: 81.25%,  total acc: 60.20%   
[EVAL] batch:   95 | acc: 81.25%,  total acc: 60.42%   
[EVAL] batch:   96 | acc: 87.50%,  total acc: 60.70%   
[EVAL] batch:   97 | acc: 75.00%,  total acc: 60.84%   
[EVAL] batch:   98 | acc: 25.00%,  total acc: 60.48%   
[EVAL] batch:   99 | acc: 25.00%,  total acc: 60.12%   
[EVAL] batch:  100 | acc: 18.75%,  total acc: 59.72%   
[EVAL] batch:  101 | acc: 18.75%,  total acc: 59.31%   
[EVAL] batch:  102 | acc: 62.50%,  total acc: 59.34%   
[EVAL] batch:  103 | acc: 62.50%,  total acc: 59.38%   
[EVAL] batch:  104 | acc: 68.75%,  total acc: 59.46%   
[EVAL] batch:  105 | acc: 62.50%,  total acc: 59.49%   
[EVAL] batch:  106 | acc: 75.00%,  total acc: 59.64%   
[EVAL] batch:  107 | acc: 68.75%,  total acc: 59.72%   
[EVAL] batch:  108 | acc: 62.50%,  total acc: 59.75%   
[EVAL] batch:  109 | acc: 81.25%,  total acc: 59.94%   
[EVAL] batch:  110 | acc: 75.00%,  total acc: 60.08%   
[EVAL] batch:  111 | acc: 75.00%,  total acc: 60.21%   
[EVAL] batch:  112 | acc: 87.50%,  total acc: 60.45%   
[EVAL] batch:  113 | acc: 81.25%,  total acc: 60.64%   
[EVAL] batch:  114 | acc: 87.50%,  total acc: 60.87%   
[EVAL] batch:  115 | acc: 81.25%,  total acc: 61.05%   
[EVAL] batch:  116 | acc: 37.50%,  total acc: 60.84%   
[EVAL] batch:  117 | acc: 0.00%,  total acc: 60.33%   
[EVAL] batch:  118 | acc: 0.00%,  total acc: 59.82%   
[EVAL] batch:  119 | acc: 0.00%,  total acc: 59.32%   
[EVAL] batch:  120 | acc: 31.25%,  total acc: 59.09%   
[EVAL] batch:  121 | acc: 56.25%,  total acc: 59.07%   
[EVAL] batch:  122 | acc: 43.75%,  total acc: 58.94%   
[EVAL] batch:  123 | acc: 50.00%,  total acc: 58.87%   
[EVAL] batch:  124 | acc: 93.75%,  total acc: 59.15%   
[EVAL] batch:  125 | acc: 93.75%,  total acc: 59.42%   
[EVAL] batch:  126 | acc: 93.75%,  total acc: 59.69%   
[EVAL] batch:  127 | acc: 81.25%,  total acc: 59.86%   
[EVAL] batch:  128 | acc: 87.50%,  total acc: 60.08%   
[EVAL] batch:  129 | acc: 56.25%,  total acc: 60.05%   
[EVAL] batch:  130 | acc: 81.25%,  total acc: 60.21%   
[EVAL] batch:  131 | acc: 87.50%,  total acc: 60.42%   
[EVAL] batch:  132 | acc: 56.25%,  total acc: 60.39%   
cur_acc:  ['0.8542', '0.8333', '0.4297', '0.8839', '0.6958', '0.7455', '0.5199', '0.7019']
his_acc:  ['0.8542', '0.8400', '0.6930', '0.6919', '0.7125', '0.6818', '0.6451', '0.6039']
----------END
his_acc mean:  [0.8576 0.8159 0.7464 0.7113 0.6658 0.655  0.6243 0.6018]
