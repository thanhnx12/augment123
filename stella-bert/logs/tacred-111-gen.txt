#############params############
cuda
Task=Tacred, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=5
#############params############
--------Round  0
seed:  100
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 0 1]
Losses:  24.484291076660156 11.09357738494873 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 24.4842911Losses:  22.786849975585938 9.646066665649414 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 22.7868500Losses:  22.146541595458984 9.043697357177734 -0.0
CurrentTrain: epoch  0, batch     2 | loss: 22.1465416Losses:  23.206308364868164 10.202875137329102 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 23.2063084Losses:  22.90353012084961 9.971681594848633 -0.0
CurrentTrain: epoch  0, batch     4 | loss: 22.9035301Losses:  20.097414016723633 7.433544635772705 -0.0
CurrentTrain: epoch  0, batch     5 | loss: 20.0974140Losses:  20.193172454833984 7.51834774017334 -0.0
CurrentTrain: epoch  0, batch     6 | loss: 20.1931725Losses:  21.41219139099121 9.035709381103516 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 21.4121914Losses:  22.61432647705078 10.191665649414062 -0.0
CurrentTrain: epoch  0, batch     8 | loss: 22.6143265Losses:  21.027957916259766 8.755420684814453 -0.0
CurrentTrain: epoch  0, batch     9 | loss: 21.0279579Losses:  20.298215866088867 8.105674743652344 -0.0
CurrentTrain: epoch  0, batch    10 | loss: 20.2982159Losses:  19.365272521972656 7.297883033752441 -0.0
CurrentTrain: epoch  0, batch    11 | loss: 19.3652725Losses:  20.557674407958984 8.341421127319336 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 20.5576744Losses:  20.136932373046875 8.216141700744629 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 20.1369324Losses:  20.84465789794922 9.098001480102539 -0.0
CurrentTrain: epoch  0, batch    14 | loss: 20.8446579Losses:  18.70006561279297 7.007706642150879 -0.0
CurrentTrain: epoch  0, batch    15 | loss: 18.7000656Losses:  21.491127014160156 10.272443771362305 -0.0
CurrentTrain: epoch  0, batch    16 | loss: 21.4911270Losses:  23.605945587158203 12.354771614074707 -0.0
CurrentTrain: epoch  0, batch    17 | loss: 23.6059456Losses:  25.177303314208984 13.677546501159668 -0.0
CurrentTrain: epoch  0, batch    18 | loss: 25.1773033Losses:  19.79106903076172 8.275964736938477 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 19.7910690Losses:  19.52218246459961 8.179508209228516 -0.0
CurrentTrain: epoch  0, batch    20 | loss: 19.5221825Losses:  21.686185836791992 9.94403076171875 -0.0
CurrentTrain: epoch  0, batch    21 | loss: 21.6861858Losses:  20.54251480102539 9.001008987426758 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 20.5425148Losses:  19.086318969726562 8.195530891418457 -0.0
CurrentTrain: epoch  0, batch    23 | loss: 19.0863190Losses:  19.318546295166016 8.248927116394043 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 19.3185463Losses:  18.264087677001953 7.126908302307129 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 18.2640877Losses:  20.066986083984375 9.275312423706055 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 20.0669861Losses:  18.58072853088379 8.231582641601562 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 18.5807285Losses:  20.594148635864258 9.801923751831055 -0.0
CurrentTrain: epoch  0, batch    28 | loss: 20.5941486Losses:  17.435924530029297 6.587273597717285 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 17.4359245Losses:  20.328285217285156 9.964601516723633 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 20.3282852Losses:  18.943336486816406 8.328694343566895 -0.0
CurrentTrain: epoch  0, batch    31 | loss: 18.9433365Losses:  18.722625732421875 8.725318908691406 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 18.7226257Losses:  18.309799194335938 8.058145523071289 -0.0
CurrentTrain: epoch  0, batch    33 | loss: 18.3097992Losses:  21.383808135986328 11.603239059448242 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 21.3838081Losses:  20.344257354736328 10.351531982421875 -0.0
CurrentTrain: epoch  0, batch    35 | loss: 20.3442574Losses:  18.960163116455078 8.588579177856445 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 18.9601631Losses:  11.625056266784668 1.7340437173843384 -0.0
CurrentTrain: epoch  0, batch    37 | loss: 11.6250563Losses:  16.442665100097656 6.579023361206055 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 16.4426651Losses:  17.101051330566406 6.645983695983887 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 17.1010513Losses:  16.871103286743164 7.576630592346191 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 16.8711033Losses:  15.486275672912598 5.896018028259277 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 15.4862757Losses:  15.93045711517334 7.013195037841797 -0.0
CurrentTrain: epoch  1, batch     4 | loss: 15.9304571Losses:  20.495412826538086 10.408208847045898 -0.0
CurrentTrain: epoch  1, batch     5 | loss: 20.4954128Losses:  16.792444229125977 7.405594348907471 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 16.7924442Losses:  16.99728012084961 7.6896233558654785 -0.0
CurrentTrain: epoch  1, batch     7 | loss: 16.9972801Losses:  18.40041160583496 9.159364700317383 -0.0
CurrentTrain: epoch  1, batch     8 | loss: 18.4004116Losses:  16.033971786499023 6.781052112579346 -0.0
CurrentTrain: epoch  1, batch     9 | loss: 16.0339718Losses:  18.23338508605957 8.629507064819336 -0.0
CurrentTrain: epoch  1, batch    10 | loss: 18.2333851Losses:  15.808340072631836 5.987419605255127 -0.0
CurrentTrain: epoch  1, batch    11 | loss: 15.8083401Losses:  16.809457778930664 7.004868507385254 -0.0
CurrentTrain: epoch  1, batch    12 | loss: 16.8094578Losses:  14.408178329467773 5.78718376159668 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 14.4081783Losses:  16.868511199951172 7.423667907714844 -0.0
CurrentTrain: epoch  1, batch    14 | loss: 16.8685112Losses:  15.9268159866333 6.880913734436035 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 15.9268160Losses:  16.181894302368164 7.116952896118164 -0.0
CurrentTrain: epoch  1, batch    16 | loss: 16.1818943Losses:  18.18048095703125 8.785223007202148 -0.0
CurrentTrain: epoch  1, batch    17 | loss: 18.1804810Losses:  15.598489761352539 6.667312145233154 -0.0
CurrentTrain: epoch  1, batch    18 | loss: 15.5984898Losses:  18.324548721313477 8.800464630126953 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 18.3245487Losses:  16.593320846557617 7.16705322265625 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 16.5933208Losses:  19.343212127685547 9.711788177490234 -0.0
CurrentTrain: epoch  1, batch    21 | loss: 19.3432121Losses:  16.137928009033203 7.25899600982666 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 16.1379280Losses:  16.05998992919922 6.859920501708984 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 16.0599899Losses:  16.694801330566406 7.747344017028809 -0.0
CurrentTrain: epoch  1, batch    24 | loss: 16.6948013Losses:  16.26287841796875 7.734962463378906 -0.0
CurrentTrain: epoch  1, batch    25 | loss: 16.2628784Losses:  19.06183433532715 9.39748764038086 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 19.0618343Losses:  17.82927703857422 9.53864860534668 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 17.8292770Losses:  20.472856521606445 11.511785507202148 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 20.4728565Losses:  16.005081176757812 7.919685363769531 -0.0
CurrentTrain: epoch  1, batch    29 | loss: 16.0050812Losses:  14.685606002807617 5.8891777992248535 -0.0
CurrentTrain: epoch  1, batch    30 | loss: 14.6856060Losses:  18.78046226501465 9.308300018310547 -0.0
CurrentTrain: epoch  1, batch    31 | loss: 18.7804623Losses:  15.294122695922852 6.124134063720703 -0.0
CurrentTrain: epoch  1, batch    32 | loss: 15.2941227Losses:  15.685734748840332 6.970768928527832 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 15.6857347Losses:  14.875210762023926 6.371050834655762 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 14.8752108Losses:  14.91103744506836 6.520541667938232 -0.0
CurrentTrain: epoch  1, batch    35 | loss: 14.9110374Losses:  15.218582153320312 6.4934468269348145 -0.0
CurrentTrain: epoch  1, batch    36 | loss: 15.2185822Losses:  9.92427921295166 1.1628615856170654 -0.0
CurrentTrain: epoch  1, batch    37 | loss: 9.9242792Losses:  12.529889106750488 4.884032249450684 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 12.5298891Losses:  15.353628158569336 6.654603481292725 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 15.3536282Losses:  14.646865844726562 5.670881271362305 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 14.6468658Losses:  18.101390838623047 9.974207878112793 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 18.1013908Losses:  18.865848541259766 9.158103942871094 -0.0
CurrentTrain: epoch  2, batch     4 | loss: 18.8658485Losses:  22.493392944335938 12.803984642028809 -0.0
CurrentTrain: epoch  2, batch     5 | loss: 22.4933929Losses:  14.105881690979004 5.331783294677734 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 14.1058817Losses:  15.944729804992676 7.3445844650268555 -0.0
CurrentTrain: epoch  2, batch     7 | loss: 15.9447298Losses:  14.878652572631836 6.679802894592285 -0.0
CurrentTrain: epoch  2, batch     8 | loss: 14.8786526Losses:  14.804874420166016 6.363210201263428 -0.0
CurrentTrain: epoch  2, batch     9 | loss: 14.8048744Losses:  14.260708808898926 6.416150093078613 -0.0
CurrentTrain: epoch  2, batch    10 | loss: 14.2607088Losses:  15.430488586425781 6.7859296798706055 -0.0
CurrentTrain: epoch  2, batch    11 | loss: 15.4304886Losses:  13.859420776367188 5.958731651306152 -0.0
CurrentTrain: epoch  2, batch    12 | loss: 13.8594208Losses:  13.926494598388672 6.098959922790527 -0.0
CurrentTrain: epoch  2, batch    13 | loss: 13.9264946Losses:  17.210254669189453 10.051824569702148 -0.0
CurrentTrain: epoch  2, batch    14 | loss: 17.2102547Losses:  15.69797134399414 7.6251397132873535 -0.0
CurrentTrain: epoch  2, batch    15 | loss: 15.6979713Losses:  15.273120880126953 6.410971641540527 -0.0
CurrentTrain: epoch  2, batch    16 | loss: 15.2731209Losses:  15.083209037780762 5.982058525085449 -0.0
CurrentTrain: epoch  2, batch    17 | loss: 15.0832090Losses:  13.809980392456055 5.784006118774414 -0.0
CurrentTrain: epoch  2, batch    18 | loss: 13.8099804Losses:  14.850337028503418 6.758004188537598 -0.0
CurrentTrain: epoch  2, batch    19 | loss: 14.8503370Losses:  14.827433586120605 7.13981294631958 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 14.8274336Losses:  14.230279922485352 6.496602535247803 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 14.2302799Losses:  14.065319061279297 6.817283630371094 -0.0
CurrentTrain: epoch  2, batch    22 | loss: 14.0653191Losses:  12.79887580871582 5.063023567199707 -0.0
CurrentTrain: epoch  2, batch    23 | loss: 12.7988758Losses:  13.445688247680664 5.541528701782227 -0.0
CurrentTrain: epoch  2, batch    24 | loss: 13.4456882Losses:  14.263945579528809 5.6428022384643555 -0.0
CurrentTrain: epoch  2, batch    25 | loss: 14.2639456Losses:  13.881536483764648 6.6055426597595215 -0.0
CurrentTrain: epoch  2, batch    26 | loss: 13.8815365Losses:  15.6709623336792 6.621705055236816 -0.0
CurrentTrain: epoch  2, batch    27 | loss: 15.6709623Losses:  13.12844467163086 6.567111015319824 -0.0
CurrentTrain: epoch  2, batch    28 | loss: 13.1284447Losses:  13.919290542602539 5.952980995178223 -0.0
CurrentTrain: epoch  2, batch    29 | loss: 13.9192905Losses:  16.873912811279297 9.062623977661133 -0.0
CurrentTrain: epoch  2, batch    30 | loss: 16.8739128Losses:  13.060903549194336 5.852973461151123 -0.0
CurrentTrain: epoch  2, batch    31 | loss: 13.0609035Losses:  16.390132904052734 8.872234344482422 -0.0
CurrentTrain: epoch  2, batch    32 | loss: 16.3901329Losses:  13.797124862670898 5.945656776428223 -0.0
CurrentTrain: epoch  2, batch    33 | loss: 13.7971249Losses:  17.80034637451172 8.522038459777832 -0.0
CurrentTrain: epoch  2, batch    34 | loss: 17.8003464Losses:  15.722444534301758 8.490673065185547 -0.0
CurrentTrain: epoch  2, batch    35 | loss: 15.7224445Losses:  16.17369842529297 8.10380744934082 -0.0
CurrentTrain: epoch  2, batch    36 | loss: 16.1736984Losses:  10.177732467651367 2.6121301651000977 -0.0
CurrentTrain: epoch  2, batch    37 | loss: 10.1777325Losses:  14.365272521972656 6.616792678833008 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 14.3652725Losses:  21.039833068847656 13.734432220458984 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 21.0398331Losses:  14.336481094360352 6.321192741394043 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 14.3364811Losses:  14.35009479522705 5.931670188903809 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 14.3500948Losses:  13.435758590698242 5.696652412414551 -0.0
CurrentTrain: epoch  3, batch     4 | loss: 13.4357586Losses:  14.484556198120117 6.132818698883057 -0.0
CurrentTrain: epoch  3, batch     5 | loss: 14.4845562Losses:  16.479843139648438 8.010622024536133 -0.0
CurrentTrain: epoch  3, batch     6 | loss: 16.4798431Losses:  11.625627517700195 4.811498165130615 -0.0
CurrentTrain: epoch  3, batch     7 | loss: 11.6256275Losses:  13.080835342407227 5.559329032897949 -0.0
CurrentTrain: epoch  3, batch     8 | loss: 13.0808353Losses:  12.976127624511719 5.182727813720703 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 12.9761276Losses:  13.57219409942627 6.197247505187988 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 13.5721941Losses:  14.457958221435547 7.705490589141846 -0.0
CurrentTrain: epoch  3, batch    11 | loss: 14.4579582Losses:  12.954835891723633 4.983952522277832 -0.0
CurrentTrain: epoch  3, batch    12 | loss: 12.9548359Losses:  14.916476249694824 6.059971809387207 -0.0
CurrentTrain: epoch  3, batch    13 | loss: 14.9164762Losses:  13.399383544921875 5.690683364868164 -0.0
CurrentTrain: epoch  3, batch    14 | loss: 13.3993835Losses:  14.256828308105469 6.4976043701171875 -0.0
CurrentTrain: epoch  3, batch    15 | loss: 14.2568283Losses:  15.1793212890625 6.557483673095703 -0.0
CurrentTrain: epoch  3, batch    16 | loss: 15.1793213Losses:  15.124011993408203 7.15532922744751 -0.0
CurrentTrain: epoch  3, batch    17 | loss: 15.1240120Losses:  13.817989349365234 6.276711463928223 -0.0
CurrentTrain: epoch  3, batch    18 | loss: 13.8179893Losses:  15.140745162963867 6.817623615264893 -0.0
CurrentTrain: epoch  3, batch    19 | loss: 15.1407452Losses:  20.489351272583008 12.827505111694336 -0.0
CurrentTrain: epoch  3, batch    20 | loss: 20.4893513Losses:  13.786580085754395 6.320526599884033 -0.0
CurrentTrain: epoch  3, batch    21 | loss: 13.7865801Losses:  15.43614387512207 6.905430316925049 -0.0
CurrentTrain: epoch  3, batch    22 | loss: 15.4361439Losses:  14.944477081298828 6.943704128265381 -0.0
CurrentTrain: epoch  3, batch    23 | loss: 14.9444771Losses:  12.474298477172852 5.616710662841797 -0.0
CurrentTrain: epoch  3, batch    24 | loss: 12.4742985Losses:  16.15869903564453 8.954134941101074 -0.0
CurrentTrain: epoch  3, batch    25 | loss: 16.1586990Losses:  13.372758865356445 6.464221000671387 -0.0
CurrentTrain: epoch  3, batch    26 | loss: 13.3727589Losses:  16.207740783691406 7.668802261352539 -0.0
CurrentTrain: epoch  3, batch    27 | loss: 16.2077408Losses:  12.51959228515625 4.772591590881348 -0.0
CurrentTrain: epoch  3, batch    28 | loss: 12.5195923Losses:  13.524528503417969 7.424432754516602 -0.0
CurrentTrain: epoch  3, batch    29 | loss: 13.5245285Losses:  13.138760566711426 5.277498245239258 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 13.1387606Losses:  14.800041198730469 6.879961967468262 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 14.8000412Losses:  16.48734474182129 9.919774055480957 -0.0
CurrentTrain: epoch  3, batch    32 | loss: 16.4873447Losses:  13.744590759277344 7.4745259284973145 -0.0
CurrentTrain: epoch  3, batch    33 | loss: 13.7445908Losses:  12.75471019744873 5.7008819580078125 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 12.7547102Losses:  12.688048362731934 5.515523433685303 -0.0
CurrentTrain: epoch  3, batch    35 | loss: 12.6880484Losses:  12.902756690979004 5.728215217590332 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 12.9027567Losses:  9.954631805419922 3.0407488346099854 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 9.9546318Losses:  13.538217544555664 5.880096435546875 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 13.5382175Losses:  12.868948936462402 5.776017189025879 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 12.8689489Losses:  13.05322551727295 6.871581554412842 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 13.0532255Losses:  14.085168838500977 6.723466873168945 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 14.0851688Losses:  15.75893497467041 8.059171676635742 -0.0
CurrentTrain: epoch  4, batch     4 | loss: 15.7589350Losses:  12.363733291625977 5.3065185546875 -0.0
CurrentTrain: epoch  4, batch     5 | loss: 12.3637333Losses:  13.979130744934082 7.64540958404541 -0.0
CurrentTrain: epoch  4, batch     6 | loss: 13.9791307Losses:  12.03931999206543 4.635382652282715 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 12.0393200Losses:  18.28139877319336 9.432510375976562 -0.0
CurrentTrain: epoch  4, batch     8 | loss: 18.2813988Losses:  12.680774688720703 4.931866645812988 -0.0
CurrentTrain: epoch  4, batch     9 | loss: 12.6807747Losses:  15.348891258239746 7.893283367156982 -0.0
CurrentTrain: epoch  4, batch    10 | loss: 15.3488913Losses:  11.422733306884766 4.986667633056641 -0.0
CurrentTrain: epoch  4, batch    11 | loss: 11.4227333Losses:  15.57034969329834 7.842288494110107 -0.0
CurrentTrain: epoch  4, batch    12 | loss: 15.5703497Losses:  11.329901695251465 4.431328773498535 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 11.3299017Losses:  16.442434310913086 8.331310272216797 -0.0
CurrentTrain: epoch  4, batch    14 | loss: 16.4424343Losses:  15.363147735595703 8.302762985229492 -0.0
CurrentTrain: epoch  4, batch    15 | loss: 15.3631477Losses:  12.51111888885498 5.131707191467285 -0.0
CurrentTrain: epoch  4, batch    16 | loss: 12.5111189Losses:  13.201292037963867 6.013570785522461 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 13.2012920Losses:  11.9542875289917 5.227397918701172 -0.0
CurrentTrain: epoch  4, batch    18 | loss: 11.9542875Losses:  12.113625526428223 4.741415500640869 -0.0
CurrentTrain: epoch  4, batch    19 | loss: 12.1136255Losses:  13.043365478515625 6.264965534210205 -0.0
CurrentTrain: epoch  4, batch    20 | loss: 13.0433655Losses:  13.134180068969727 5.816242694854736 -0.0
CurrentTrain: epoch  4, batch    21 | loss: 13.1341801Losses:  12.316089630126953 5.012795448303223 -0.0
CurrentTrain: epoch  4, batch    22 | loss: 12.3160896Losses:  12.211580276489258 5.981380462646484 -0.0
CurrentTrain: epoch  4, batch    23 | loss: 12.2115803Losses:  13.381049156188965 6.052309989929199 -0.0
CurrentTrain: epoch  4, batch    24 | loss: 13.3810492Losses:  16.904733657836914 8.87432861328125 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 16.9047337Losses:  12.331690788269043 5.389676094055176 -0.0
CurrentTrain: epoch  4, batch    26 | loss: 12.3316908Losses:  15.72543716430664 6.574335098266602 -0.0
CurrentTrain: epoch  4, batch    27 | loss: 15.7254372Losses:  10.561391830444336 3.804800510406494 -0.0
CurrentTrain: epoch  4, batch    28 | loss: 10.5613918Losses:  13.424135208129883 6.3454484939575195 -0.0
CurrentTrain: epoch  4, batch    29 | loss: 13.4241352Losses:  13.811635971069336 6.349462509155273 -0.0
CurrentTrain: epoch  4, batch    30 | loss: 13.8116360Losses:  16.522171020507812 10.00928020477295 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 16.5221710Losses:  13.549663543701172 5.271859169006348 -0.0
CurrentTrain: epoch  4, batch    32 | loss: 13.5496635Losses:  12.859172821044922 6.2018842697143555 -0.0
CurrentTrain: epoch  4, batch    33 | loss: 12.8591728Losses:  15.57817268371582 7.214189529418945 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 15.5781727Losses:  12.632911682128906 5.260801315307617 -0.0
CurrentTrain: epoch  4, batch    35 | loss: 12.6329117Losses:  12.30424976348877 5.367195129394531 -0.0
CurrentTrain: epoch  4, batch    36 | loss: 12.3042498Losses:  8.828350067138672 1.0146969556808472 -0.0
CurrentTrain: epoch  4, batch    37 | loss: 8.8283501Losses:  12.695700645446777 6.507469177246094 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 12.6957006Losses:  13.227701187133789 5.899883270263672 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 13.2277012Losses:  14.000346183776855 6.241747856140137 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 14.0003462Losses:  14.084667205810547 6.5360822677612305 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 14.0846672Losses:  12.741546630859375 5.282155513763428 -0.0
CurrentTrain: epoch  5, batch     4 | loss: 12.7415466Losses:  13.227107048034668 5.867283821105957 -0.0
CurrentTrain: epoch  5, batch     5 | loss: 13.2271070Losses:  12.369928359985352 4.678805828094482 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 12.3699284Losses:  13.034517288208008 5.539697170257568 -0.0
CurrentTrain: epoch  5, batch     7 | loss: 13.0345173Losses:  13.948087692260742 6.143494606018066 -0.0
CurrentTrain: epoch  5, batch     8 | loss: 13.9480877Losses:  12.482772827148438 5.466446399688721 -0.0
CurrentTrain: epoch  5, batch     9 | loss: 12.4827728Losses:  12.454456329345703 5.196478366851807 -0.0
CurrentTrain: epoch  5, batch    10 | loss: 12.4544563Losses:  12.319214820861816 4.882311820983887 -0.0
CurrentTrain: epoch  5, batch    11 | loss: 12.3192148Losses:  11.428202629089355 4.709278106689453 -0.0
CurrentTrain: epoch  5, batch    12 | loss: 11.4282026Losses:  11.659285545349121 4.448663711547852 -0.0
CurrentTrain: epoch  5, batch    13 | loss: 11.6592855Losses:  12.307174682617188 4.592329978942871 -0.0
CurrentTrain: epoch  5, batch    14 | loss: 12.3071747Losses:  15.096515655517578 7.758608818054199 -0.0
CurrentTrain: epoch  5, batch    15 | loss: 15.0965157Losses:  12.088963508605957 6.2403154373168945 -0.0
CurrentTrain: epoch  5, batch    16 | loss: 12.0889635Losses:  12.498266220092773 6.225338935852051 -0.0
CurrentTrain: epoch  5, batch    17 | loss: 12.4982662Losses:  13.790145874023438 6.33962345123291 -0.0
CurrentTrain: epoch  5, batch    18 | loss: 13.7901459Losses:  15.743623733520508 7.329254150390625 -0.0
CurrentTrain: epoch  5, batch    19 | loss: 15.7436237Losses:  11.66822624206543 5.355057716369629 -0.0
CurrentTrain: epoch  5, batch    20 | loss: 11.6682262Losses:  12.198512077331543 5.473367691040039 -0.0
CurrentTrain: epoch  5, batch    21 | loss: 12.1985121Losses:  10.936223030090332 4.509642601013184 -0.0
CurrentTrain: epoch  5, batch    22 | loss: 10.9362230Losses:  11.146800994873047 4.279393196105957 -0.0
CurrentTrain: epoch  5, batch    23 | loss: 11.1468010Losses:  13.967605590820312 6.144975185394287 -0.0
CurrentTrain: epoch  5, batch    24 | loss: 13.9676056Losses:  10.829509735107422 4.582178115844727 -0.0
CurrentTrain: epoch  5, batch    25 | loss: 10.8295097Losses:  11.900751113891602 5.205649375915527 -0.0
CurrentTrain: epoch  5, batch    26 | loss: 11.9007511Losses:  16.701168060302734 10.11662769317627 -0.0
CurrentTrain: epoch  5, batch    27 | loss: 16.7011681Losses:  17.98267364501953 8.960708618164062 -0.0
CurrentTrain: epoch  5, batch    28 | loss: 17.9826736Losses:  11.068034172058105 3.7659947872161865 -0.0
CurrentTrain: epoch  5, batch    29 | loss: 11.0680342Losses:  12.390626907348633 6.23953914642334 -0.0
CurrentTrain: epoch  5, batch    30 | loss: 12.3906269Losses:  13.149271011352539 6.687117576599121 -0.0
CurrentTrain: epoch  5, batch    31 | loss: 13.1492710Losses:  15.09327507019043 9.428226470947266 -0.0
CurrentTrain: epoch  5, batch    32 | loss: 15.0932751Losses:  13.842879295349121 6.790816307067871 -0.0
CurrentTrain: epoch  5, batch    33 | loss: 13.8428793Losses:  10.635223388671875 3.8633408546447754 -0.0
CurrentTrain: epoch  5, batch    34 | loss: 10.6352234Losses:  13.439264297485352 7.388665199279785 -0.0
CurrentTrain: epoch  5, batch    35 | loss: 13.4392643Losses:  11.820623397827148 4.6963067054748535 -0.0
CurrentTrain: epoch  5, batch    36 | loss: 11.8206234Losses:  7.318852424621582 1.3309223651885986 -0.0
CurrentTrain: epoch  5, batch    37 | loss: 7.3188524Losses:  11.656665802001953 5.793758392333984 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 11.6566658Losses:  13.192296981811523 6.181687355041504 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 13.1922970Losses:  15.37834358215332 8.2137451171875 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 15.3783436Losses:  11.091800689697266 4.6831488609313965 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 11.0918007Losses:  12.283970832824707 5.929340362548828 -0.0
CurrentTrain: epoch  6, batch     4 | loss: 12.2839708Losses:  11.50403881072998 5.388874053955078 -0.0
CurrentTrain: epoch  6, batch     5 | loss: 11.5040388Losses:  16.52977752685547 9.495426177978516 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 16.5297775Losses:  11.092159271240234 4.4821062088012695 -0.0
CurrentTrain: epoch  6, batch     7 | loss: 11.0921593Losses:  11.753315925598145 5.811083793640137 -0.0
CurrentTrain: epoch  6, batch     8 | loss: 11.7533159Losses:  11.005868911743164 4.764487266540527 -0.0
CurrentTrain: epoch  6, batch     9 | loss: 11.0058689Losses:  10.004361152648926 4.016512870788574 -0.0
CurrentTrain: epoch  6, batch    10 | loss: 10.0043612Losses:  11.559213638305664 5.063666343688965 -0.0
CurrentTrain: epoch  6, batch    11 | loss: 11.5592136Losses:  9.923896789550781 4.174312591552734 -0.0
CurrentTrain: epoch  6, batch    12 | loss: 9.9238968Losses:  11.476613998413086 5.794156551361084 -0.0
CurrentTrain: epoch  6, batch    13 | loss: 11.4766140Losses:  10.50594711303711 4.278727054595947 -0.0
CurrentTrain: epoch  6, batch    14 | loss: 10.5059471Losses:  9.898641586303711 4.323912620544434 -0.0
CurrentTrain: epoch  6, batch    15 | loss: 9.8986416Losses:  13.282705307006836 6.759347915649414 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 13.2827053Losses:  12.541927337646484 6.790966987609863 -0.0
CurrentTrain: epoch  6, batch    17 | loss: 12.5419273Losses:  11.5656156539917 5.329923152923584 -0.0
CurrentTrain: epoch  6, batch    18 | loss: 11.5656157Losses:  12.765180587768555 6.939658164978027 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 12.7651806Losses:  15.644393920898438 8.73744010925293 -0.0
CurrentTrain: epoch  6, batch    20 | loss: 15.6443939Losses:  11.521110534667969 4.2828779220581055 -0.0
CurrentTrain: epoch  6, batch    21 | loss: 11.5211105Losses:  11.991358757019043 5.597567558288574 -0.0
CurrentTrain: epoch  6, batch    22 | loss: 11.9913588Losses:  10.168198585510254 3.755863904953003 -0.0
CurrentTrain: epoch  6, batch    23 | loss: 10.1681986Losses:  11.833185195922852 6.267510414123535 -0.0
CurrentTrain: epoch  6, batch    24 | loss: 11.8331852Losses:  15.616840362548828 8.148209571838379 -0.0
CurrentTrain: epoch  6, batch    25 | loss: 15.6168404Losses:  14.940370559692383 7.967078685760498 -0.0
CurrentTrain: epoch  6, batch    26 | loss: 14.9403706Losses:  11.997435569763184 6.096098899841309 -0.0
CurrentTrain: epoch  6, batch    27 | loss: 11.9974356Losses:  12.917049407958984 6.7894110679626465 -0.0
CurrentTrain: epoch  6, batch    28 | loss: 12.9170494Losses:  12.64517593383789 6.6475725173950195 -0.0
CurrentTrain: epoch  6, batch    29 | loss: 12.6451759Losses:  12.525120735168457 6.215219497680664 -0.0
CurrentTrain: epoch  6, batch    30 | loss: 12.5251207Losses:  14.437755584716797 6.714638710021973 -0.0
CurrentTrain: epoch  6, batch    31 | loss: 14.4377556Losses:  10.800213813781738 5.279626846313477 -0.0
CurrentTrain: epoch  6, batch    32 | loss: 10.8002138Losses:  11.747827529907227 4.968986511230469 -0.0
CurrentTrain: epoch  6, batch    33 | loss: 11.7478275Losses:  10.438431739807129 4.906018257141113 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 10.4384317Losses:  10.411903381347656 4.232782363891602 -0.0
CurrentTrain: epoch  6, batch    35 | loss: 10.4119034Losses:  10.643579483032227 4.4836554527282715 -0.0
CurrentTrain: epoch  6, batch    36 | loss: 10.6435795Losses:  7.034775257110596 0.502267062664032 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 7.0347753Losses:  18.459115982055664 11.340572357177734 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 18.4591160Losses:  10.085184097290039 4.380746364593506 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 10.0851841Losses:  12.820067405700684 6.876091003417969 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 12.8200674Losses:  11.82792854309082 6.892887115478516 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 11.8279285Losses:  12.166813850402832 5.900861740112305 -0.0
CurrentTrain: epoch  7, batch     4 | loss: 12.1668139Losses:  10.880205154418945 5.3256683349609375 -0.0
CurrentTrain: epoch  7, batch     5 | loss: 10.8802052Losses:  11.311391830444336 4.803691387176514 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 11.3113918Losses:  10.255412101745605 4.644718170166016 -0.0
CurrentTrain: epoch  7, batch     7 | loss: 10.2554121Losses:  12.086668968200684 6.103064060211182 -0.0
CurrentTrain: epoch  7, batch     8 | loss: 12.0866690Losses:  11.616254806518555 6.018451690673828 -0.0
CurrentTrain: epoch  7, batch     9 | loss: 11.6162548Losses:  10.697004318237305 4.729192733764648 -0.0
CurrentTrain: epoch  7, batch    10 | loss: 10.6970043Losses:  12.08432388305664 5.752371788024902 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 12.0843239Losses:  10.468228340148926 4.49196720123291 -0.0
CurrentTrain: epoch  7, batch    12 | loss: 10.4682283Losses:  11.830930709838867 5.5779218673706055 -0.0
CurrentTrain: epoch  7, batch    13 | loss: 11.8309307Losses:  13.22970962524414 7.401015758514404 -0.0
CurrentTrain: epoch  7, batch    14 | loss: 13.2297096Losses:  9.866267204284668 4.138466835021973 -0.0
CurrentTrain: epoch  7, batch    15 | loss: 9.8662672Losses:  10.01021957397461 4.494651794433594 -0.0
CurrentTrain: epoch  7, batch    16 | loss: 10.0102196Losses:  11.002747535705566 5.712023735046387 -0.0
CurrentTrain: epoch  7, batch    17 | loss: 11.0027475Losses:  10.9463529586792 5.496161460876465 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 10.9463530Losses:  11.653575897216797 6.139019012451172 -0.0
CurrentTrain: epoch  7, batch    19 | loss: 11.6535759Losses:  12.450311660766602 5.909857273101807 -0.0
CurrentTrain: epoch  7, batch    20 | loss: 12.4503117Losses:  10.482772827148438 5.135659217834473 -0.0
CurrentTrain: epoch  7, batch    21 | loss: 10.4827728Losses:  13.048089027404785 5.514956951141357 -0.0
CurrentTrain: epoch  7, batch    22 | loss: 13.0480890Losses:  15.944955825805664 9.5030517578125 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 15.9449558Losses:  10.837669372558594 5.444874286651611 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 10.8376694Losses:  12.235133171081543 6.681118011474609 -0.0
CurrentTrain: epoch  7, batch    25 | loss: 12.2351332Losses:  14.799457550048828 8.851336479187012 -0.0
CurrentTrain: epoch  7, batch    26 | loss: 14.7994576Losses:  11.688630104064941 6.102565765380859 -0.0
CurrentTrain: epoch  7, batch    27 | loss: 11.6886301Losses:  14.652366638183594 7.871180534362793 -0.0
CurrentTrain: epoch  7, batch    28 | loss: 14.6523666Losses:  10.079581260681152 4.747052192687988 -0.0
CurrentTrain: epoch  7, batch    29 | loss: 10.0795813Losses:  10.349418640136719 4.811605453491211 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 10.3494186Losses:  11.499105453491211 6.000450611114502 -0.0
CurrentTrain: epoch  7, batch    31 | loss: 11.4991055Losses:  9.835744857788086 4.3478240966796875 -0.0
CurrentTrain: epoch  7, batch    32 | loss: 9.8357449Losses:  10.353445053100586 5.29832649230957 -0.0
CurrentTrain: epoch  7, batch    33 | loss: 10.3534451Losses:  11.656904220581055 6.593899726867676 -0.0
CurrentTrain: epoch  7, batch    34 | loss: 11.6569042Losses:  8.659388542175293 3.502539873123169 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 8.6593885Losses:  10.538361549377441 5.3056488037109375 -0.0
CurrentTrain: epoch  7, batch    36 | loss: 10.5383615Losses:  6.6233062744140625 1.5790214538574219 -0.0
CurrentTrain: epoch  7, batch    37 | loss: 6.6233063Losses:  11.168295860290527 6.136012554168701 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 11.1682959Losses:  9.311370849609375 4.225313186645508 -0.0
CurrentTrain: epoch  8, batch     1 | loss: 9.3113708Losses:  14.576095581054688 9.770105361938477 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 14.5760956Losses:  9.240439414978027 4.234133720397949 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 9.2404394Losses:  9.212385177612305 4.226428985595703 -0.0
CurrentTrain: epoch  8, batch     4 | loss: 9.2123852Losses:  9.780877113342285 4.713907241821289 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 9.7808771Losses:  10.963882446289062 6.09468936920166 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 10.9638824Losses:  11.906119346618652 6.996827125549316 -0.0
CurrentTrain: epoch  8, batch     7 | loss: 11.9061193Losses:  10.536499977111816 5.622941017150879 -0.0
CurrentTrain: epoch  8, batch     8 | loss: 10.5365000Losses:  9.96550464630127 4.183094024658203 -0.0
CurrentTrain: epoch  8, batch     9 | loss: 9.9655046Losses:  12.452486038208008 6.197786331176758 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 12.4524860Losses:  11.144857406616211 6.03754997253418 -0.0
CurrentTrain: epoch  8, batch    11 | loss: 11.1448574Losses:  9.46464729309082 4.282462120056152 -0.0
CurrentTrain: epoch  8, batch    12 | loss: 9.4646473Losses:  10.123777389526367 5.077143669128418 -0.0
CurrentTrain: epoch  8, batch    13 | loss: 10.1237774Losses:  11.115795135498047 5.022884368896484 -0.0
CurrentTrain: epoch  8, batch    14 | loss: 11.1157951Losses:  9.955265045166016 4.620253562927246 -0.0
CurrentTrain: epoch  8, batch    15 | loss: 9.9552650Losses:  12.166105270385742 7.113696575164795 -0.0
CurrentTrain: epoch  8, batch    16 | loss: 12.1661053Losses:  14.165877342224121 8.984969139099121 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 14.1658773Losses:  10.09269905090332 4.626902103424072 -0.0
CurrentTrain: epoch  8, batch    18 | loss: 10.0926991Losses:  11.103739738464355 6.102225303649902 -0.0
CurrentTrain: epoch  8, batch    19 | loss: 11.1037397Losses:  11.81498908996582 6.591500282287598 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 11.8149891Losses:  10.236043930053711 4.74565315246582 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 10.2360439Losses:  12.425580978393555 7.4362006187438965 -0.0
CurrentTrain: epoch  8, batch    22 | loss: 12.4255810Losses:  10.804533004760742 4.997996807098389 -0.0
CurrentTrain: epoch  8, batch    23 | loss: 10.8045330Losses:  11.216658592224121 5.821281433105469 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 11.2166586Losses:  9.772985458374023 4.8770751953125 -0.0
CurrentTrain: epoch  8, batch    25 | loss: 9.7729855Losses:  13.14980697631836 6.857101917266846 -0.0
CurrentTrain: epoch  8, batch    26 | loss: 13.1498070Losses:  11.508935928344727 6.416773796081543 -0.0
CurrentTrain: epoch  8, batch    27 | loss: 11.5089359Losses:  10.103865623474121 5.217667579650879 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 10.1038656Losses:  13.73055648803711 8.318792343139648 -0.0
CurrentTrain: epoch  8, batch    29 | loss: 13.7305565Losses:  9.978338241577148 5.114237308502197 -0.0
CurrentTrain: epoch  8, batch    30 | loss: 9.9783382Losses:  14.734003067016602 9.710765838623047 -0.0
CurrentTrain: epoch  8, batch    31 | loss: 14.7340031Losses:  10.796515464782715 5.991267204284668 -0.0
CurrentTrain: epoch  8, batch    32 | loss: 10.7965155Losses:  9.525245666503906 4.5679144859313965 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 9.5252457Losses:  8.66230297088623 3.817373514175415 -0.0
CurrentTrain: epoch  8, batch    34 | loss: 8.6623030Losses:  10.247469902038574 5.629602432250977 -0.0
CurrentTrain: epoch  8, batch    35 | loss: 10.2474699Losses:  8.18745231628418 3.347158908843994 -0.0
CurrentTrain: epoch  8, batch    36 | loss: 8.1874523Losses:  6.31785774230957 1.5727264881134033 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 6.3178577Losses:  10.5366792678833 5.197908401489258 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 10.5366793Losses:  14.357378005981445 8.93831729888916 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 14.3573780Losses:  8.72050666809082 3.859128475189209 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 8.7205067Losses:  9.941374778747559 5.175079345703125 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 9.9413748Losses:  9.93570327758789 5.025386333465576 -0.0
CurrentTrain: epoch  9, batch     4 | loss: 9.9357033Losses:  9.882505416870117 5.051599502563477 -0.0
CurrentTrain: epoch  9, batch     5 | loss: 9.8825054Losses:  10.081932067871094 5.252397060394287 -0.0
CurrentTrain: epoch  9, batch     6 | loss: 10.0819321Losses:  9.530211448669434 4.684225082397461 -0.0
CurrentTrain: epoch  9, batch     7 | loss: 9.5302114Losses:  8.283108711242676 3.453617811203003 -0.0
CurrentTrain: epoch  9, batch     8 | loss: 8.2831087Losses:  14.526751518249512 9.678115844726562 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 14.5267515Losses:  10.853387832641602 5.974891185760498 -0.0
CurrentTrain: epoch  9, batch    10 | loss: 10.8533878Losses:  12.334237098693848 7.576651573181152 -0.0
CurrentTrain: epoch  9, batch    11 | loss: 12.3342371Losses:  9.124235153198242 4.214437484741211 -0.0
CurrentTrain: epoch  9, batch    12 | loss: 9.1242352Losses:  9.543428421020508 4.6999664306640625 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 9.5434284Losses:  10.559741973876953 5.73345422744751 -0.0
CurrentTrain: epoch  9, batch    14 | loss: 10.5597420Losses:  9.852227210998535 4.50064754486084 -0.0
CurrentTrain: epoch  9, batch    15 | loss: 9.8522272Losses:  10.710283279418945 5.939416885375977 -0.0
CurrentTrain: epoch  9, batch    16 | loss: 10.7102833Losses:  13.509897232055664 7.77384614944458 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 13.5098972Losses:  10.913269996643066 6.017583847045898 -0.0
CurrentTrain: epoch  9, batch    18 | loss: 10.9132700Losses:  10.520723342895508 5.828102111816406 -0.0
CurrentTrain: epoch  9, batch    19 | loss: 10.5207233Losses:  10.287590026855469 5.527010917663574 -0.0
CurrentTrain: epoch  9, batch    20 | loss: 10.2875900Losses:  10.784153938293457 6.046614646911621 -0.0
CurrentTrain: epoch  9, batch    21 | loss: 10.7841539Losses:  11.816865921020508 7.080754280090332 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 11.8168659Losses:  13.235184669494629 8.072059631347656 -0.0
CurrentTrain: epoch  9, batch    23 | loss: 13.2351847Losses:  10.735182762145996 5.9003190994262695 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 10.7351828Losses:  9.817144393920898 5.078609943389893 -0.0
CurrentTrain: epoch  9, batch    25 | loss: 9.8171444Losses:  11.777097702026367 6.206031322479248 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 11.7770977Losses:  10.883344650268555 6.1302947998046875 -0.0
CurrentTrain: epoch  9, batch    27 | loss: 10.8833447Losses:  8.904050827026367 4.182191848754883 -0.0
CurrentTrain: epoch  9, batch    28 | loss: 8.9040508Losses:  10.943111419677734 6.12785005569458 -0.0
CurrentTrain: epoch  9, batch    29 | loss: 10.9431114Losses:  13.551786422729492 8.874063491821289 -0.0
CurrentTrain: epoch  9, batch    30 | loss: 13.5517864Losses:  11.828797340393066 7.026386260986328 -0.0
CurrentTrain: epoch  9, batch    31 | loss: 11.8287973Losses:  9.857305526733398 5.1513800621032715 -0.0
CurrentTrain: epoch  9, batch    32 | loss: 9.8573055Losses:  11.047952651977539 5.350949287414551 -0.0
CurrentTrain: epoch  9, batch    33 | loss: 11.0479527Losses:  15.40216064453125 7.560880661010742 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 15.4021606Losses:  9.9282865524292 5.070971488952637 -0.0
CurrentTrain: epoch  9, batch    35 | loss: 9.9282866Losses:  11.17648696899414 6.123800277709961 -0.0
CurrentTrain: epoch  9, batch    36 | loss: 11.1764870Losses:  6.318080425262451 0.7195076942443848 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 6.3180804
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: -lrb- talk of -rrb- imposing sanctions has faded and this resolution is the last effort by the west , '' the fars news agency quoted vice president ali akbar salehi , who also heads iran 's atomic energy organisation , as saying .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria decided to move back to her hometown in Spain to reconnect with her roots.  
Head Entity: Maria  
Tail Entity: Spain  

Relation: person countries of residence  
Context: The famous actor, who has spent most of his life in Los Angeles, recently purchased a villa in Italy, where he plans to spend his summers.  
Head Entity: The famous actor  
Tail Entity: Italy  

Relation: person countries of residence  
Context: Following his retirement, the former athlete chose to settle in Australia, enjoying the sunny beaches and relaxed lifestyle.  
Head Entity: The former athlete  
Tail Entity: Australia  

Relation: person countries of residence  
Context: After completing her studies in London, Sarah returned to Canada, where she found a job and started a new chapter in her life.  
Head Entity: Sarah  
Tail Entity: Canada  

Relation: person countries of residence  
Context: The renowned scientist has been living in Germany for the past decade, conducting research and collaborating with various institutions across Europe.  
Head Entity: The renowned scientist  
Tail Entity: Germany  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` customer relationships are the backbone of banking , and banks are constantly striving to meet customer demands , '' said peter garuccio , spokesman for the american bankers association .
Head Entity: american bankers association
Tail Entity: peter garuccio
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
1. Relation: organization top members employees  
   Context: "As the CEO of the tech giant, she has been instrumental in driving innovation," said a company spokesperson about Jane Doe, who leads the organization.  
   Head Entity: tech giant  
   Tail Entity: Jane Doe  

2. Relation: organization top members employees  
   Context: "The board of directors is pleased to announce the appointment of John Smith as the new CFO," stated the press release from the financial services firm.  
   Head Entity: financial services firm  
   Tail Entity: John Smith  

3. Relation: organization top members employees  
   Context: "During the annual meeting, the president of the nonprofit organization highlighted the contributions of their dedicated volunteers," reported the local news outlet.  
   Head Entity: nonprofit organization  
   Tail Entity: president  

4. Relation: organization top members employees  
   Context: "In a recent interview, the founder of the startup discussed the importance of teamwork and praised his lead developer for her exceptional work," the article noted.  
   Head Entity: startup  
   Tail Entity: lead developer  

5. Relation: organization top members employees  
   Context: "The university's chancellor expressed gratitude towards the faculty for their commitment to student success," said the university's official statement.  
   Head Entity: university  
   Tail Entity: chancellor  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: south africa 1-5 leyds cnr biccard streets cosatu 's concern has been that the scorpions ' existence as a branch of the national directorate of public prosecutions blurred the necessary separation of functions between those investigating crime and those prosecuting the criminals .
Head Entity: scorpions
Tail Entity: national directorate of public prosecutions
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
1. Relation: organization member of  
   Context: The United Nations is a global organization that aims to promote peace and cooperation among its member states, including countries like France and Germany.  
   Head Entity: United Nations  
   Tail Entity: France  

2. Relation: organization member of  
   Context: The World Health Organization has been instrumental in coordinating international health responses, with member countries like Canada and Japan contributing resources and expertise.  
   Head Entity: World Health Organization  
   Tail Entity: Canada  

3. Relation: organization member of  
   Context: The European Union consists of various member states that collaborate on economic and political issues, with Italy being one of the founding members.  
   Head Entity: European Union  
   Tail Entity: Italy  

4. Relation: organization member of  
   Context: The International Olympic Committee oversees the Olympic Games and includes member organizations from over 200 countries, such as Brazil and Australia.  
   Head Entity: International Olympic Committee  
   Tail Entity: Brazil  

5. Relation: organization member of  
   Context: The Association of Southeast Asian Nations works to promote regional stability and economic growth, with Indonesia being a key member since its inception.  
   Head Entity: Association of Southeast Asian Nations  
   Tail Entity: Indonesia  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: in tehran iranian atomic chief ali akbar salehi said that `` iran will answer their questions after studying their letters in detail . ''
Head Entity: ali akbar salehi
Tail Entity: iranian
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
1. Relation: person origin  
   Context: born in the bustling city of new delhi, the renowned author arundhati roy has often spoken about her indian heritage.  
   Head Entity: arundhati roy  
   Tail Entity: indian  

2. Relation: person origin  
   Context: during the press conference, the famous actor penélope cruz expressed her pride in being from spain.  
   Head Entity: penélope cruz  
   Tail Entity: spain  

3. Relation: person origin  
   Context: the scientist, who was raised in the picturesque town of cambridge, has always identified as british.  
   Head Entity: the scientist  
   Tail Entity: british  

4. Relation: person origin  
   Context: while discussing her latest project, the filmmaker chloé zhao highlighted her chinese roots and how they influence her work.  
   Head Entity: chloé zhao  
   Tail Entity: chinese  

5. Relation: person origin  
   Context: at the international conference, the politician justin trudeau proudly stated that he represents canada on the global stage.  
   Head Entity: justin trudeau  
   Tail Entity: canada  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she leads the finance team."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the conference, Dr. James Smith, the lead researcher at BioHealth Labs, presented his findings on the latest advancements in biotechnology."  
Head Entity: Dr. James Smith  
Tail Entity: lead researcher  

Relation: person title  
Context: "As the new Director of Marketing, Alex Chen is responsible for overseeing all promotional activities at Creative Solutions."  
Head Entity: Alex Chen  
Tail Entity: Director of Marketing  

Relation: person title  
Context: "In her role as Editor-in-Chief, Laura Johnson has transformed the magazine into a leading publication in the fashion industry."  
Head Entity: Laura Johnson  
Tail Entity: Editor-in-Chief  

Relation: person title  
Context: "With over 20 years of experience, Robert Lee has taken on the position of Senior Vice President at Global Enterprises."  
Head Entity: Robert Lee  
Tail Entity: Senior Vice President  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: commander viliame naupoto , chairman of the fiji pine limited announced the woodchips exports target here tuesday after signing a woodchip sale agreement with japan 's itochu corporation .
Head Entity: itochu corporation
Tail Entity: japan
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the headquarters of the multinational technology company apple inc. is located in cupertino, california, where it has been a significant player in the tech industry.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization country of headquarters  
Context: the famous automobile manufacturer toyota motor corporation has its main office in toyota city, a hub for automotive innovation in japan.  
Head Entity: toyota motor corporation  
Tail Entity: japan  

Relation: organization country of headquarters  
Context: the global financial services firm jpmorgan chase & co. operates its headquarters in new york city, a major financial center in the united states.  
Head Entity: jpmorgan chase & co.  
Tail Entity: united states  

Relation: organization country of headquarters  
Context: the renowned aerospace company boeing has its corporate headquarters in chicago, which plays a crucial role in the aviation industry.  
Head Entity: boeing  
Tail Entity: united states  

Relation: organization country of headquarters  
Context: the leading social media platform facebook, now known as meta platforms, has its headquarters situated in menlo park, california, contributing to the tech landscape.  
Head Entity: meta platforms  
Tail Entity: california  
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 83.33%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 55.21%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 76.92%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 76.67%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 83.33%   
cur_acc:  ['0.8333']
his_acc:  ['0.8333']
Clustering into  4  clusters
Clusters:  [3 0 2 3 0 3 1 3 1 0 3]
Losses:  16.834346771240234 8.283613204956055 1.428198218345642
CurrentTrain: epoch  0, batch     0 | loss: 16.8343468Losses:  12.270187377929688 2.738111734390259 1.38971745967865
CurrentTrain: epoch  0, batch     1 | loss: 12.2701874Losses:  16.40036392211914 8.033567428588867 1.4383816719055176
CurrentTrain: epoch  1, batch     0 | loss: 16.4003639Losses:  8.508703231811523 2.2987372875213623 1.4238083362579346
CurrentTrain: epoch  1, batch     1 | loss: 8.5087032Losses:  13.52771282196045 7.009099960327148 1.4036455154418945
CurrentTrain: epoch  2, batch     0 | loss: 13.5277128Losses:  8.681018829345703 2.5243582725524902 1.4178466796875
CurrentTrain: epoch  2, batch     1 | loss: 8.6810188Losses:  14.620630264282227 8.282312393188477 1.400250792503357
CurrentTrain: epoch  3, batch     0 | loss: 14.6206303Losses:  7.649350166320801 3.1640117168426514 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 7.6493502Losses:  12.373390197753906 6.641977310180664 1.4527411460876465
CurrentTrain: epoch  4, batch     0 | loss: 12.3733902Losses:  6.934383392333984 1.9475091695785522 1.4228246212005615
CurrentTrain: epoch  4, batch     1 | loss: 6.9343834Losses:  11.810395240783691 7.389638900756836 1.4256781339645386
CurrentTrain: epoch  5, batch     0 | loss: 11.8103952Losses:  7.044252395629883 2.498063802719116 1.4888046979904175
CurrentTrain: epoch  5, batch     1 | loss: 7.0442524Losses:  11.017047882080078 6.363428115844727 1.4216502904891968
CurrentTrain: epoch  6, batch     0 | loss: 11.0170479Losses:  5.980978488922119 1.9659407138824463 1.405615210533142
CurrentTrain: epoch  6, batch     1 | loss: 5.9809785Losses:  10.645416259765625 6.764513969421387 1.4194540977478027
CurrentTrain: epoch  7, batch     0 | loss: 10.6454163Losses:  7.699522972106934 3.2938156127929688 1.482271432876587
CurrentTrain: epoch  7, batch     1 | loss: 7.6995230Losses:  11.299885749816895 7.142139434814453 1.4222030639648438
CurrentTrain: epoch  8, batch     0 | loss: 11.2998857Losses:  6.649583339691162 2.9748377799987793 1.4000810384750366
CurrentTrain: epoch  8, batch     1 | loss: 6.6495833Losses:  9.291412353515625 5.449665546417236 1.390122652053833
CurrentTrain: epoch  9, batch     0 | loss: 9.2914124Losses:  4.6923699378967285 1.206765055656433 1.4067338705062866
CurrentTrain: epoch  9, batch     1 | loss: 4.6923699
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  

Relation: person date of birth  
Context: the legendary musician freddie mercury was born farrokh bulsara on september 5, 1946, in zanzibar, tanzania.  
Head Entity: freddie mercury  
Tail Entity: september 5, 1946  

Relation: person date of birth  
Context: the influential civil rights leader martin luther king jr. was born on january 15, 1929, in atlanta, georgia.  
Head Entity: martin luther king jr.  
Tail Entity: january 15, 1929  

Relation: person date of birth  
Context: the famous painter pablo picasso was born on october 25, 1881, in malaga, spain.  
Head Entity: pablo picasso  
Tail Entity: october 25, 1881  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, württemberg, germany.  
Head Entity: albert einstein  
Tail Entity: württemberg  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  

Relation: person stateorprovince of birth  
Context: barack obama was born on august 4, 1961, in honolulu, hawaii.  
Head Entity: barack obama  
Tail Entity: hawaii  

Relation: person stateorprovince of birth  
Context: agatha christie was born on september 15, 1890, in torquay, devon.  
Head Entity: agatha christie  
Tail Entity: devon  

Relation: person stateorprovince of birth  
Context: nelson mandela was born on july 18, 1918, in mvezo, eastern cape.  
Head Entity: nelson mandela  
Tail Entity: eastern cape  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, it was heartwarming to see how much Sarah's father, John, had influenced her life choices.  
Head Entity: Sarah  
Tail Entity: John  

Relation: person parents  
Context: After the ceremony, Emily reflected on how her mother, Lisa, had always been her guiding light through difficult times.  
Head Entity: Emily  
Tail Entity: Lisa  

Relation: person parents  
Context: At the graduation party, Michael proudly introduced his parents, Tom and Rachel, to his friends, highlighting their support throughout his education.  
Head Entity: Michael  
Tail Entity: Tom  

Relation: person parents  
Context: In her memoir, Anna shared stories about her father, Robert, and how he taught her the value of hard work and perseverance.  
Head Entity: Anna  
Tail Entity: Robert  

Relation: person parents  
Context: As the family gathered for dinner, it was clear that David's mother, Mary, had a special bond with him, filled with love and understanding.  
Head Entity: David  
Tail Entity: Mary  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work, Maria finally landed a job at the prestigious tech firm, Innovatech Solutions, where she could showcase her skills.  
Head Entity: Maria  
Tail Entity: Innovatech Solutions  

Relation: person employee of  
Context: John has been with the company for over a decade, contributing significantly to its growth and success in the financial sector.  
Head Entity: John  
Tail Entity: the company  

Relation: person employee of  
Context: As a lead designer at Creative Minds Agency, Sarah has worked on numerous high-profile projects that have garnered national attention.  
Head Entity: Sarah  
Tail Entity: Creative Minds Agency  

Relation: person employee of  
Context: After graduating from university, Tom was thrilled to accept an offer from Green Energy Corp, a leader in renewable energy solutions.  
Head Entity: Tom  
Tail Entity: Green Energy Corp  

Relation: person employee of  
Context: Emily's dedication to her role at HealthFirst Hospital has made her a beloved figure among both patients and staff.  
Head Entity: Emily  
Tail Entity: HealthFirst Hospital  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his thrilling novels, passed away peacefully in his sleep at his residence in los angeles, ca.  
Head Entity: john doe  
Tail Entity: ca.  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, died at the age of 79 in her home located in bel air, ca.  
Head Entity: elizabeth taylor  
Tail Entity: ca.  

Relation: person stateorprovince of death  
Context: the famous physicist, albert einstein, took his last breath in princeton, nj, where he had lived for many years.  
Head Entity: albert einstein  
Tail Entity: nj.  

Relation: person stateorprovince of death  
Context: the beloved musician, prince, was found unresponsive in his home in minneapolis, mn, leading to an outpouring of grief from fans worldwide.  
Head Entity: prince  
Tail Entity: mn.  

Relation: person stateorprovince of death  
Context: the legendary civil rights leader, martin luther king jr., was assassinated in memphis, tn, a tragic event that shocked the nation.  
Head Entity: martin luther king jr.  
Tail Entity: tn.  
Losses:  17.44729995727539 5.768302917480469 3.534639358520508
MemoryTrain:  epoch  0, batch     0 | loss: 17.4473000Losses:  9.87028980255127 2.1420164108276367 1.6570943593978882
MemoryTrain:  epoch  0, batch     1 | loss: 9.8702898Losses:  14.587051391601562 3.60762095451355 3.675051212310791
MemoryTrain:  epoch  0, batch     2 | loss: 14.5870514Losses:  16.14557456970215 3.844960927963257 3.9273881912231445
MemoryTrain:  epoch  0, batch     3 | loss: 16.1455746Losses:  4.911252498626709 -0.0 -0.0
MemoryTrain:  epoch  0, batch     4 | loss: 4.9112525Losses:  14.879243850708008 2.847534418106079 3.6440443992614746
MemoryTrain:  epoch  1, batch     0 | loss: 14.8792439Losses:  10.293843269348145 3.334089994430542 -0.0
MemoryTrain:  epoch  1, batch     1 | loss: 10.2938433Losses:  11.795027732849121 2.9403488636016846 3.620150566101074
MemoryTrain:  epoch  1, batch     2 | loss: 11.7950277Losses:  9.674140930175781 2.677292585372925 1.8226203918457031
MemoryTrain:  epoch  1, batch     3 | loss: 9.6741409Losses:  7.902675151824951 -0.0 -0.0
MemoryTrain:  epoch  1, batch     4 | loss: 7.9026752Losses:  10.536846160888672 2.3078715801239014 3.5169737339019775
MemoryTrain:  epoch  2, batch     0 | loss: 10.5368462Losses:  9.846176147460938 3.7115230560302734 1.6906678676605225
MemoryTrain:  epoch  2, batch     1 | loss: 9.8461761Losses:  12.497617721557617 3.514035224914551 3.880514621734619
MemoryTrain:  epoch  2, batch     2 | loss: 12.4976177Losses:  11.611299514770508 2.319307327270508 3.624338150024414
MemoryTrain:  epoch  2, batch     3 | loss: 11.6112995Losses:  3.16580867767334 -0.0 -0.0
MemoryTrain:  epoch  2, batch     4 | loss: 3.1658087Losses:  11.312456130981445 4.955326080322266 3.5791068077087402
MemoryTrain:  epoch  3, batch     0 | loss: 11.3124561Losses:  9.519124031066895 3.091183662414551 1.5542187690734863
MemoryTrain:  epoch  3, batch     1 | loss: 9.5191240Losses:  11.770105361938477 3.6800713539123535 1.6997981071472168
MemoryTrain:  epoch  3, batch     2 | loss: 11.7701054Losses:  12.907241821289062 6.050494194030762 3.415891170501709
MemoryTrain:  epoch  3, batch     3 | loss: 12.9072418Losses:  7.370637893676758 -0.0 -0.0
MemoryTrain:  epoch  3, batch     4 | loss: 7.3706379Losses:  11.664323806762695 4.254488468170166 3.705944061279297
MemoryTrain:  epoch  4, batch     0 | loss: 11.6643238Losses:  10.84007453918457 4.373948097229004 1.535473108291626
MemoryTrain:  epoch  4, batch     1 | loss: 10.8400745Losses:  9.225334167480469 2.5582196712493896 3.4558606147766113
MemoryTrain:  epoch  4, batch     2 | loss: 9.2253342Losses:  9.822404861450195 2.5209908485412598 3.3932201862335205
MemoryTrain:  epoch  4, batch     3 | loss: 9.8224049Losses:  8.407150268554688 -0.0 -0.0
MemoryTrain:  epoch  4, batch     4 | loss: 8.4071503Losses:  10.643857955932617 2.90694522857666 3.504704475402832
MemoryTrain:  epoch  5, batch     0 | loss: 10.6438580Losses:  8.1143217086792 1.6024601459503174 3.5205469131469727
MemoryTrain:  epoch  5, batch     1 | loss: 8.1143217Losses:  9.810054779052734 3.503859281539917 1.5260930061340332
MemoryTrain:  epoch  5, batch     2 | loss: 9.8100548Losses:  9.06207275390625 2.0787665843963623 3.557831287384033
MemoryTrain:  epoch  5, batch     3 | loss: 9.0620728Losses:  1.9659528732299805 -0.0 -0.0
MemoryTrain:  epoch  5, batch     4 | loss: 1.9659529Losses:  9.965572357177734 2.6598048210144043 3.3679797649383545
MemoryTrain:  epoch  6, batch     0 | loss: 9.9655724Losses:  11.46426010131836 4.497744560241699 3.422664165496826
MemoryTrain:  epoch  6, batch     1 | loss: 11.4642601Losses:  8.06469440460205 3.4751291275024414 1.6563847064971924
MemoryTrain:  epoch  6, batch     2 | loss: 8.0646944Losses:  9.766725540161133 4.332943916320801 1.4410626888275146
MemoryTrain:  epoch  6, batch     3 | loss: 9.7667255Losses:  3.6383256912231445 -0.0 -0.0
MemoryTrain:  epoch  6, batch     4 | loss: 3.6383257Losses:  9.324226379394531 2.817030906677246 3.513688564300537
MemoryTrain:  epoch  7, batch     0 | loss: 9.3242264Losses:  10.720235824584961 4.319666862487793 3.416872024536133
MemoryTrain:  epoch  7, batch     1 | loss: 10.7202358Losses:  7.720893859863281 2.749812602996826 1.4809291362762451
MemoryTrain:  epoch  7, batch     2 | loss: 7.7208939Losses:  9.224042892456055 2.3899059295654297 3.379054546356201
MemoryTrain:  epoch  7, batch     3 | loss: 9.2240429Losses:  3.7174277305603027 -0.0 -0.0
MemoryTrain:  epoch  7, batch     4 | loss: 3.7174277Losses:  9.390005111694336 2.904409170150757 3.4449944496154785
MemoryTrain:  epoch  8, batch     0 | loss: 9.3900051Losses:  7.79182243347168 1.8333721160888672 3.41526198387146
MemoryTrain:  epoch  8, batch     1 | loss: 7.7918224Losses:  7.379762649536133 2.5872714519500732 1.4811906814575195
MemoryTrain:  epoch  8, batch     2 | loss: 7.3797626Losses:  9.54934310913086 2.6442627906799316 3.440992593765259
MemoryTrain:  epoch  8, batch     3 | loss: 9.5493431Losses:  2.781080484390259 -0.0 -0.0
MemoryTrain:  epoch  8, batch     4 | loss: 2.7810805Losses:  8.586138725280762 1.7772274017333984 3.41658091545105
MemoryTrain:  epoch  9, batch     0 | loss: 8.5861387Losses:  7.929689407348633 2.02193021774292 3.3896658420562744
MemoryTrain:  epoch  9, batch     1 | loss: 7.9296894Losses:  9.821979522705078 3.8495655059814453 3.440927505493164
MemoryTrain:  epoch  9, batch     2 | loss: 9.8219795Losses:  8.363245010375977 2.3252687454223633 3.3984861373901367
MemoryTrain:  epoch  9, batch     3 | loss: 8.3632450Losses:  1.945988655090332 -0.0 -0.0
MemoryTrain:  epoch  9, batch     4 | loss: 1.9459887
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 86.16%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 86.03%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 86.28%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 86.32%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 86.51%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 86.86%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.20%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.65%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 87.78%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 86.68%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 84.84%   
cur_acc:  ['0.8333', '0.8616']
his_acc:  ['0.8333', '0.8484']
Clustering into  7  clusters
Clusters:  [5 0 6 5 0 5 2 5 2 0 5 5 4 3 1 0]
Losses:  21.75457763671875 8.13956069946289 6.1403326988220215
CurrentTrain: epoch  0, batch     0 | loss: 21.7545776Losses:  15.103174209594727 3.7851130962371826 3.724257230758667
CurrentTrain: epoch  0, batch     1 | loss: 15.1031742Losses:  18.477317810058594 6.4846649169921875 5.94927978515625
CurrentTrain: epoch  1, batch     0 | loss: 18.4773178Losses:  17.132545471191406 3.4940388202667236 6.024843692779541
CurrentTrain: epoch  1, batch     1 | loss: 17.1325455Losses:  19.644580841064453 7.564996719360352 5.820061206817627
CurrentTrain: epoch  2, batch     0 | loss: 19.6445808Losses:  13.567293167114258 1.8519783020019531 6.190282821655273
CurrentTrain: epoch  2, batch     1 | loss: 13.5672932Losses:  17.994359970092773 7.185522079467773 5.872836589813232
CurrentTrain: epoch  3, batch     0 | loss: 17.9943600Losses:  13.577320098876953 1.9967122077941895 5.819271087646484
CurrentTrain: epoch  3, batch     1 | loss: 13.5773201Losses:  17.668460845947266 6.561365127563477 5.833998680114746
CurrentTrain: epoch  4, batch     0 | loss: 17.6684608Losses:  12.937450408935547 2.7322256565093994 5.688716411590576
CurrentTrain: epoch  4, batch     1 | loss: 12.9374504Losses:  16.692914962768555 6.401802062988281 5.725167751312256
CurrentTrain: epoch  5, batch     0 | loss: 16.6929150Losses:  12.471475601196289 2.7639026641845703 5.708287715911865
CurrentTrain: epoch  5, batch     1 | loss: 12.4714756Losses:  15.699178695678711 6.356301784515381 5.66537618637085
CurrentTrain: epoch  6, batch     0 | loss: 15.6991787Losses:  11.596456527709961 1.7009152173995972 5.741249084472656
CurrentTrain: epoch  6, batch     1 | loss: 11.5964565Losses:  18.383190155029297 8.029489517211914 5.647716045379639
CurrentTrain: epoch  7, batch     0 | loss: 18.3831902Losses:  11.222801208496094 3.0775771141052246 5.610484600067139
CurrentTrain: epoch  7, batch     1 | loss: 11.2228012Losses:  15.717326164245605 6.689797878265381 5.597413063049316
CurrentTrain: epoch  8, batch     0 | loss: 15.7173262Losses:  9.458695411682129 2.538541793823242 3.375061273574829
CurrentTrain: epoch  8, batch     1 | loss: 9.4586954Losses:  15.408976554870605 7.498076915740967 5.608348846435547
CurrentTrain: epoch  9, batch     0 | loss: 15.4089766Losses:  12.898599624633789 5.0501227378845215 3.3298661708831787
CurrentTrain: epoch  9, batch     1 | loss: 12.8985996
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she mentioned that despite living in the united states for over a decade, her roots are firmly planted in the vibrant landscapes of brazil where she was born.  
Head Entity: she  
Tail Entity: brazil  

Relation: person country of birth  
Context: as a celebrated author, j.k. rowling often reflects on her upbringing in the united kingdom, where she was born and raised before achieving global fame.  
Head Entity: j.k. rowling  
Tail Entity: united kingdom  

Relation: person country of birth  
Context: the documentary highlighted the life of the famous musician who was born in nigeria and later moved to the united states to pursue his career.  
Head Entity: the famous musician  
Tail Entity: nigeria  

Relation: person country of birth  
Context: although he has lived in canada for many years, his heart remains in the land of his birth, which is the beautiful country of italy.  
Head Entity: he  
Tail Entity: italy  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit our official site at https://www.techinnovators.com for the latest updates.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For more information, check out the website of Green Earth Initiative at http://www.greenearth.org.  
Head Entity: Green Earth Initiative  
Tail Entity: http://www.greenearth.org  

Relation: organization website  
Context: The blog post can be found at https://www.culinarycreations.com/blog, which is managed by Culinary Creations.  
Head Entity: Culinary Creations  
Tail Entity: https://www.culinarycreations.com/blog  

Relation: organization website  
Context: You can learn more about our services by visiting http://www.fitnessworld.com.  
Head Entity: Fitness World  
Tail Entity: http://www.fitnessworld.com  

Relation: organization website  
Context: The latest news is available on the official site: https://www.artisticvisions.org.  
Head Entity: Artistic Visions  
Tail Entity: https://www.artisticvisions.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant Apple has seen significant investments from Warren Buffett's Berkshire Hathaway.  
Head Entity: Apple  
Tail Entity: Berkshire Hathaway  

Relation: organization shareholders  
Context: Tesla's stock has been heavily bought by investment firm Vanguard Group, increasing their stake in the company.  
Head Entity: Tesla  
Tail Entity: Vanguard Group  

Relation: organization shareholders  
Context: The pharmaceutical company Pfizer has received substantial funding from the investment group BlackRock.  
Head Entity: Pfizer  
Tail Entity: BlackRock  

Relation: organization shareholders  
Context: Amazon's growth has attracted investments from various hedge funds, including the well-known Elliott Management.  
Head Entity: Amazon  
Tail Entity: Elliott Management  

Relation: organization shareholders  
Context: The renewable energy firm NextEra Energy has gained significant backing from the investment company State Street Global Advisors.  
Head Entity: NextEra Energy  
Tail Entity: State Street Global Advisors  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: After years of financial difficulties, the local arts council announced its dissolution in January 2019, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: January 2019  

Relation: organization dissolved  
Context: The historic bookstore, Pages & Co., closed its doors for good in July 2021, marking the end of an era for the community.  
Head Entity: Pages & Co.  
Tail Entity: July 2021  

Relation: organization dissolved  
Context: Following a series of scandals, the charity organization, Helping Hands, was officially dissolved in February 2022.  
Head Entity: Helping Hands  
Tail Entity: February 2022  

Relation: organization dissolved  
Context: The environmental group, Green Future, announced its dissolution in October 2018 due to a lack of funding and support.  
Head Entity: Green Future  
Tail Entity: October 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. John Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. John Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the famous actress and philanthropist, Emily Johnson, to support underprivileged children.  
Head Entity: Hope for Tomorrow  
Tail Entity: Emily Johnson  

Relation: organization founded by  
Context: In the early 2000s, a group of environmental activists led by Sarah Thompson founded Green Earth Initiative to combat climate change and promote sustainability.  
Head Entity: Green Earth Initiative  
Tail Entity: Sarah Thompson  

Relation: organization founded by  
Context: The tech startup, Innovatech, was co-founded by Mark Lee and his partner, Lisa Chen, in 2018, aiming to revolutionize the way we interact with artificial intelligence.  
Head Entity: Innovatech  
Tail Entity: Mark Lee  

Relation: organization founded by  
Context: The historical society, Heritage Keepers, was established in 1995 by Dr. Alice Carter, a dedicated historian passionate about preserving local history.  
Head Entity: Heritage Keepers  
Tail Entity: Dr. Alice Carter  
Losses:  14.208091735839844 1.7432161569595337 5.798629283905029
MemoryTrain:  epoch  0, batch     0 | loss: 14.2080917Losses:  15.143503189086914 1.0837514400482178 8.674124717712402
MemoryTrain:  epoch  0, batch     1 | loss: 15.1435032Losses:  17.30402183532715 1.0260086059570312 11.152575492858887
MemoryTrain:  epoch  0, batch     2 | loss: 17.3040218Losses:  12.553980827331543 2.6025562286376953 5.624059200286865
MemoryTrain:  epoch  0, batch     3 | loss: 12.5539808Losses:  14.574058532714844 1.150986909866333 8.149721145629883
MemoryTrain:  epoch  0, batch     4 | loss: 14.5740585Losses:  14.25859260559082 1.0992079973220825 8.256253242492676
MemoryTrain:  epoch  0, batch     5 | loss: 14.2585926Losses:  11.34865951538086 1.1509549617767334 5.659117221832275
MemoryTrain:  epoch  1, batch     0 | loss: 11.3486595Losses:  19.91156005859375 2.600686550140381 11.050374031066895
MemoryTrain:  epoch  1, batch     1 | loss: 19.9115601Losses:  14.596023559570312 1.9696431159973145 8.151339530944824
MemoryTrain:  epoch  1, batch     2 | loss: 14.5960236Losses:  13.599154472351074 3.5536446571350098 5.662709712982178
MemoryTrain:  epoch  1, batch     3 | loss: 13.5991545Losses:  15.34575080871582 1.2944695949554443 8.382133483886719
MemoryTrain:  epoch  1, batch     4 | loss: 15.3457508Losses:  13.251008033752441 1.0505374670028687 8.386001586914062
MemoryTrain:  epoch  1, batch     5 | loss: 13.2510080Losses:  11.345667839050293 1.4915540218353271 5.966403961181641
MemoryTrain:  epoch  2, batch     0 | loss: 11.3456678Losses:  14.62727165222168 2.040255546569824 8.316667556762695
MemoryTrain:  epoch  2, batch     1 | loss: 14.6272717Losses:  12.293073654174805 1.6980382204055786 5.712709903717041
MemoryTrain:  epoch  2, batch     2 | loss: 12.2930737Losses:  9.874114990234375 1.7428994178771973 3.341141700744629
MemoryTrain:  epoch  2, batch     3 | loss: 9.8741150Losses:  14.886488914489746 1.7952642440795898 8.265342712402344
MemoryTrain:  epoch  2, batch     4 | loss: 14.8864889Losses:  16.229366302490234 2.092895984649658 10.915645599365234
MemoryTrain:  epoch  2, batch     5 | loss: 16.2293663Losses:  16.375587463378906 1.2330563068389893 10.913408279418945
MemoryTrain:  epoch  3, batch     0 | loss: 16.3755875Losses:  12.903411865234375 1.5881996154785156 5.781337261199951
MemoryTrain:  epoch  3, batch     1 | loss: 12.9034119Losses:  11.014815330505371 2.1544482707977295 5.607357025146484
MemoryTrain:  epoch  3, batch     2 | loss: 11.0148153Losses:  11.034982681274414 2.0125439167022705 5.673206329345703
MemoryTrain:  epoch  3, batch     3 | loss: 11.0349827Losses:  14.235522270202637 2.4657742977142334 8.120681762695312
MemoryTrain:  epoch  3, batch     4 | loss: 14.2355223Losses:  16.529996871948242 2.6677379608154297 10.865154266357422
MemoryTrain:  epoch  3, batch     5 | loss: 16.5299969Losses:  12.434718132019043 2.559391736984253 5.659961700439453
MemoryTrain:  epoch  4, batch     0 | loss: 12.4347181Losses:  10.655672073364258 0.7735164761543274 5.6683807373046875
MemoryTrain:  epoch  4, batch     1 | loss: 10.6556721Losses:  12.114877700805664 1.1463598012924194 8.077459335327148
MemoryTrain:  epoch  4, batch     2 | loss: 12.1148777Losses:  17.358495712280273 2.2165536880493164 11.01889419555664
MemoryTrain:  epoch  4, batch     3 | loss: 17.3584957Losses:  12.655224800109863 2.2000365257263184 8.151383399963379
MemoryTrain:  epoch  4, batch     4 | loss: 12.6552248Losses:  13.200094223022461 1.8043955564498901 8.127534866333008
MemoryTrain:  epoch  4, batch     5 | loss: 13.2000942Losses:  10.671929359436035 1.169517159461975 5.632166385650635
MemoryTrain:  epoch  5, batch     0 | loss: 10.6719294Losses:  12.767448425292969 1.4954872131347656 8.139934539794922
MemoryTrain:  epoch  5, batch     1 | loss: 12.7674484Losses:  12.157238006591797 1.0997471809387207 8.224725723266602
MemoryTrain:  epoch  5, batch     2 | loss: 12.1572380Losses:  11.928901672363281 2.0019469261169434 5.618122100830078
MemoryTrain:  epoch  5, batch     3 | loss: 11.9289017Losses:  11.141878128051758 2.796480178833008 5.64211893081665
MemoryTrain:  epoch  5, batch     4 | loss: 11.1418781Losses:  10.578519821166992 1.5201088190078735 5.606762409210205
MemoryTrain:  epoch  5, batch     5 | loss: 10.5785198Losses:  10.97681999206543 1.3605908155441284 5.59266996383667
MemoryTrain:  epoch  6, batch     0 | loss: 10.9768200Losses:  15.850907325744629 1.460180640220642 10.846270561218262
MemoryTrain:  epoch  6, batch     1 | loss: 15.8509073Losses:  12.042333602905273 1.1036856174468994 8.169122695922852
MemoryTrain:  epoch  6, batch     2 | loss: 12.0423336Losses:  12.728316307067871 1.8697617053985596 8.093090057373047
MemoryTrain:  epoch  6, batch     3 | loss: 12.7283163Losses:  14.400680541992188 1.0595643520355225 10.88114070892334
MemoryTrain:  epoch  6, batch     4 | loss: 14.4006805Losses:  10.886848449707031 1.8103597164154053 5.61305570602417
MemoryTrain:  epoch  6, batch     5 | loss: 10.8868484Losses:  12.948453903198242 1.4672621488571167 8.192157745361328
MemoryTrain:  epoch  7, batch     0 | loss: 12.9484539Losses:  11.908567428588867 1.6826622486114502 8.11636734008789
MemoryTrain:  epoch  7, batch     1 | loss: 11.9085674Losses:  9.806047439575195 2.7608449459075928 3.3757424354553223
MemoryTrain:  epoch  7, batch     2 | loss: 9.8060474Losses:  12.123437881469727 1.049007773399353 8.089649200439453
MemoryTrain:  epoch  7, batch     3 | loss: 12.1234379Losses:  13.27365493774414 2.2279653549194336 8.116013526916504
MemoryTrain:  epoch  7, batch     4 | loss: 13.2736549Losses:  13.381525039672852 1.528486967086792 8.203712463378906
MemoryTrain:  epoch  7, batch     5 | loss: 13.3815250Losses:  16.755382537841797 2.7672007083892822 10.920151710510254
MemoryTrain:  epoch  8, batch     0 | loss: 16.7553825Losses:  12.797098159790039 1.9358267784118652 8.14951229095459
MemoryTrain:  epoch  8, batch     1 | loss: 12.7970982Losses:  10.45425033569336 2.537949562072754 5.576061725616455
MemoryTrain:  epoch  8, batch     2 | loss: 10.4542503Losses:  12.333581924438477 1.3656117916107178 8.139482498168945
MemoryTrain:  epoch  8, batch     3 | loss: 12.3335819Losses:  13.427302360534668 1.901634931564331 8.150991439819336
MemoryTrain:  epoch  8, batch     4 | loss: 13.4273024Losses:  11.943618774414062 1.2031581401824951 8.105813980102539
MemoryTrain:  epoch  8, batch     5 | loss: 11.9436188Losses:  12.744447708129883 2.1410489082336426 8.083223342895508
MemoryTrain:  epoch  9, batch     0 | loss: 12.7444477Losses:  12.290298461914062 1.6034612655639648 8.149551391601562
MemoryTrain:  epoch  9, batch     1 | loss: 12.2902985Losses:  12.657299995422363 2.5126025676727295 8.128582000732422
MemoryTrain:  epoch  9, batch     2 | loss: 12.6573000Losses:  10.006512641906738 3.63804030418396 3.3190901279449463
MemoryTrain:  epoch  9, batch     3 | loss: 10.0065126Losses:  10.410320281982422 2.543057918548584 5.620414733886719
MemoryTrain:  epoch  9, batch     4 | loss: 10.4103203Losses:  10.656728744506836 2.4578757286071777 5.572248935699463
MemoryTrain:  epoch  9, batch     5 | loss: 10.6567287
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 18.75%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 6.25%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 38.28%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 73.66%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 72.66%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 72.79%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.82%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 80.83%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 80.85%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 81.44%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 81.42%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 81.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 82.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.53%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 82.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.08%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.72%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.81%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 83.29%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 82.71%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 83.07%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 83.16%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 81.75%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 80.51%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 79.33%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 77.95%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 76.62%   
cur_acc:  ['0.8333', '0.8616', '0.3828']
his_acc:  ['0.8333', '0.8484', '0.7662']
Clustering into  9  clusters
Clusters:  [0 2 3 0 2 0 8 0 1 2 0 0 7 5 4 2 6 3 0 1 0]
Losses:  16.11373519897461 8.205785751342773 3.354398250579834
CurrentTrain: epoch  0, batch     0 | loss: 16.1137352Losses:  11.336808204650879 3.829482316970825 3.3247735500335693
CurrentTrain: epoch  0, batch     1 | loss: 11.3368082Losses:  13.667795181274414 6.509489059448242 3.3458995819091797
CurrentTrain: epoch  1, batch     0 | loss: 13.6677952Losses:  8.232404708862305 1.577667236328125 3.362793445587158
CurrentTrain: epoch  1, batch     1 | loss: 8.2324047Losses:  14.834096908569336 7.978893280029297 3.330841064453125
CurrentTrain: epoch  2, batch     0 | loss: 14.8340969Losses:  9.046523094177246 2.9171974658966064 3.3775346279144287
CurrentTrain: epoch  2, batch     1 | loss: 9.0465231Losses:  12.72930908203125 6.3129682540893555 3.361438274383545
CurrentTrain: epoch  3, batch     0 | loss: 12.7293091Losses:  8.241762161254883 1.9421956539154053 3.3238635063171387
CurrentTrain: epoch  3, batch     1 | loss: 8.2417622Losses:  12.60388469696045 6.489636421203613 3.3473100662231445
CurrentTrain: epoch  4, batch     0 | loss: 12.6038847Losses:  7.343209266662598 1.7094298601150513 3.334482431411743
CurrentTrain: epoch  4, batch     1 | loss: 7.3432093Losses:  12.708589553833008 6.713703632354736 3.3242406845092773
CurrentTrain: epoch  5, batch     0 | loss: 12.7085896Losses:  5.834609508514404 2.2391107082366943 1.410158634185791
CurrentTrain: epoch  5, batch     1 | loss: 5.8346095Losses:  14.385270118713379 7.692928791046143 3.373049736022949
CurrentTrain: epoch  6, batch     0 | loss: 14.3852701Losses:  8.663216590881348 2.9655601978302 3.331928253173828
CurrentTrain: epoch  6, batch     1 | loss: 8.6632166Losses:  12.430139541625977 6.038265228271484 3.3463456630706787
CurrentTrain: epoch  7, batch     0 | loss: 12.4301395Losses:  7.229347229003906 1.8860520124435425 3.331170082092285
CurrentTrain: epoch  7, batch     1 | loss: 7.2293472Losses:  11.300299644470215 5.831521987915039 3.369429111480713
CurrentTrain: epoch  8, batch     0 | loss: 11.3002996Losses:  7.090303421020508 1.7674126625061035 3.336064338684082
CurrentTrain: epoch  8, batch     1 | loss: 7.0903034Losses:  11.637763977050781 6.244940280914307 3.3821864128112793
CurrentTrain: epoch  9, batch     0 | loss: 11.6377640Losses:  7.398192405700684 2.061790943145752 3.342482089996338
CurrentTrain: epoch  9, batch     1 | loss: 7.3981924
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned artist, elena smith, tragically lost her life due to a car accident while returning from an exhibition.  
Head Entity: elena smith  
Tail Entity: car accident  

Relation: person cause of death  
Context: following a long struggle with heart disease, mr. thompson succumbed to his illness last night at the hospital.  
Head Entity: mr. thompson  
Tail Entity: heart disease  

Relation: person cause of death  
Context: the community mourned the loss of their beloved mayor, who died from a sudden stroke during a city council meeting.  
Head Entity: the mayor  
Tail Entity: stroke  

Relation: person cause of death  
Context: after a courageous fight against diabetes complications, sarah jones passed away, leaving behind a legacy of kindness.  
Head Entity: sarah jones  
Tail Entity: diabetes complications  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote its agenda.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in political advocacy, representing the interests of the Muslim community in the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  

Relation: organization political religious affiliation  
Context: The National Council of Churches works to unite various Christian denominations in the U.S. to address social justice issues and promote interfaith dialogue.  
Head Entity: National Council of Churches  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Jewish Federation of North America plays a crucial role in supporting Jewish communities and advocating for their rights in political spheres.  
Head Entity: Jewish Federation of North America  
Tail Entity: Jewish  

Relation: organization political religious affiliation  
Context: The Hindu American Foundation has been instrumental in raising awareness about Hindu issues in the political landscape of the United States.  
Head Entity: Hindu American Foundation  
Tail Entity: Hindu  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: headquartered in seattle, washington, amazon.com, inc. is a leading e-commerce and cloud computing company.  
Head Entity: amazon.com, inc.  
Tail Entity: washington  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is based in suwon, south korea, and is known for its electronics and technology products.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization stateorprovince of headquarters  
Context: based in redmond, washington, microsoft corporation is a major player in software development and technology solutions.  
Head Entity: microsoft corporation  
Tail Entity: washington  

Relation: organization stateorprovince of headquarters  
Context: the global automotive manufacturer toyota motor corporation has its headquarters in toyota city, aichi prefecture, japan.  
Head Entity: toyota motor corporation  
Tail Entity: aichi prefecture  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, and their close bond despite the challenges they faced growing up.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, uncle bob shared stories about his adventures with his niece, emily, who has always looked up to him as a role model.  
Head Entity: uncle bob  
Tail Entity: emily  

Relation: person other family  
Context: the documentary highlighted the relationship between singer taylor swift and her mother, andrea swift, showcasing their strong support for each other throughout taylor's career.  
Head Entity: andrea swift  
Tail Entity: taylor swift  

Relation: person other family  
Context: at the wedding, the bride's father, mr. johnson, gave a heartfelt speech about his daughter, sarah, and how proud he is of her accomplishments.  
Head Entity: mr. johnson  
Tail Entity: sarah  

Relation: person other family  
Context: in her memoir, actress drew barrymore reflects on her childhood and the influence of her grandmother, who played a significant role in her life.  
Head Entity: drew barrymore  
Tail Entity: her grandmother  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: millender-mcdonald , who was 68 , died late saturday at her home in carson , california , said her chief of staff , bandele mcqueen .
Head Entity: millender-mcdonald
Tail Entity: carson
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: renowned author and activist, james baldwin, passed away in 1987 in the city of paris, france, where he had spent much of his later life.  
Head Entity: james baldwin  
Tail Entity: paris  

Relation: person city of death  
Context: the famous physicist, albert einstein, died in 1955 in the city of princeton, new jersey, where he had been living for many years.  
Head Entity: albert einstein  
Tail Entity: princeton  

Relation: person city of death  
Context: beloved singer and songwriter, amy winehouse, tragically died in 2011 at her home in london, england, leaving behind a legacy of music.  
Head Entity: amy winehouse  
Tail Entity: london  

Relation: person city of death  
Context: the legendary actor, marlon brando, passed away in 2004 in the city of los angeles, california, surrounded by family and friends.  
Head Entity: marlon brando  
Tail Entity: los angeles  

Relation: person city of death  
Context: civil rights leader, martin luther king jr., was assassinated in 1968 in the city of memphis, tennessee, while advocating for social justice.  
Head Entity: martin luther king jr.  
Tail Entity: memphis  
Losses:  13.729393005371094 1.198765754699707 8.086913108825684
MemoryTrain:  epoch  0, batch     0 | loss: 13.7293930Losses:  16.93424415588379 1.685950756072998 10.81978988647461
MemoryTrain:  epoch  0, batch     1 | loss: 16.9342442Losses:  11.201433181762695 1.6191078424453735 5.817155838012695
MemoryTrain:  epoch  0, batch     2 | loss: 11.2014332Losses:  17.118148803710938 2.0272836685180664 10.834837913513184
MemoryTrain:  epoch  0, batch     3 | loss: 17.1181488Losses:  15.234855651855469 1.6992344856262207 8.122045516967773
MemoryTrain:  epoch  0, batch     4 | loss: 15.2348557Losses:  15.332742691040039 1.9647653102874756 8.19614315032959
MemoryTrain:  epoch  0, batch     5 | loss: 15.3327427Losses:  16.59385108947754 1.6754112243652344 10.911764144897461
MemoryTrain:  epoch  0, batch     6 | loss: 16.5938511Losses:  12.990516662597656 0.5251896381378174 8.086691856384277
MemoryTrain:  epoch  0, batch     7 | loss: 12.9905167Losses:  19.657814025878906 1.1629791259765625 13.715483665466309
MemoryTrain:  epoch  1, batch     0 | loss: 19.6578140Losses:  11.931877136230469 2.0042591094970703 5.564080715179443
MemoryTrain:  epoch  1, batch     1 | loss: 11.9318771Losses:  15.92056655883789 0.8965985774993896 10.83940601348877
MemoryTrain:  epoch  1, batch     2 | loss: 15.9205666Losses:  16.34833335876465 1.4541364908218384 10.826741218566895
MemoryTrain:  epoch  1, batch     3 | loss: 16.3483334Losses:  11.215397834777832 0.8008264303207397 5.6695876121521
MemoryTrain:  epoch  1, batch     4 | loss: 11.2153978Losses:  13.883124351501465 2.4949116706848145 8.107961654663086
MemoryTrain:  epoch  1, batch     5 | loss: 13.8831244Losses:  20.09450340270996 0.24929742515087128 16.727468490600586
MemoryTrain:  epoch  1, batch     6 | loss: 20.0945034Losses:  10.119250297546387 1.0341105461120605 5.612083435058594
MemoryTrain:  epoch  1, batch     7 | loss: 10.1192503Losses:  14.623075485229492 0.5797263383865356 10.82605266571045
MemoryTrain:  epoch  2, batch     0 | loss: 14.6230755Losses:  18.425430297851562 0.5840280055999756 13.701239585876465
MemoryTrain:  epoch  2, batch     1 | loss: 18.4254303Losses:  16.458898544311523 1.71523916721344 10.825976371765137
MemoryTrain:  epoch  2, batch     2 | loss: 16.4588985Losses:  9.78109359741211 1.7203706502914429 3.3698911666870117
MemoryTrain:  epoch  2, batch     3 | loss: 9.7810936Losses:  14.85007095336914 0.8507568836212158 10.827661514282227
MemoryTrain:  epoch  2, batch     4 | loss: 14.8500710Losses:  15.055438995361328 1.06672203540802 10.869235038757324
MemoryTrain:  epoch  2, batch     5 | loss: 15.0554390Losses:  15.214428901672363 0.7505786418914795 10.844149589538574
MemoryTrain:  epoch  2, batch     6 | loss: 15.2144289Losses:  12.657119750976562 1.4445058107376099 8.117950439453125
MemoryTrain:  epoch  2, batch     7 | loss: 12.6571198Losses:  15.329773902893066 1.1843852996826172 10.815502166748047
MemoryTrain:  epoch  3, batch     0 | loss: 15.3297739Losses:  14.888280868530273 1.4968860149383545 10.810541152954102
MemoryTrain:  epoch  3, batch     1 | loss: 14.8882809Losses:  20.886533737182617 1.0368554592132568 16.75189208984375
MemoryTrain:  epoch  3, batch     2 | loss: 20.8865337Losses:  17.5599365234375 1.0462939739227295 13.673425674438477
MemoryTrain:  epoch  3, batch     3 | loss: 17.5599365Losses:  10.042814254760742 1.097468376159668 5.57827615737915
MemoryTrain:  epoch  3, batch     4 | loss: 10.0428143Losses:  17.876829147338867 0.5449763536453247 13.743203163146973
MemoryTrain:  epoch  3, batch     5 | loss: 17.8768291Losses:  14.486274719238281 1.82281494140625 8.087142944335938
MemoryTrain:  epoch  3, batch     6 | loss: 14.4862747Losses:  8.005770683288574 0.8898943662643433 3.303865432739258
MemoryTrain:  epoch  3, batch     7 | loss: 8.0057707Losses:  18.899372100830078 1.3198217153549194 13.67447280883789
MemoryTrain:  epoch  4, batch     0 | loss: 18.8993721Losses:  12.67187786102295 1.1853446960449219 8.091607093811035
MemoryTrain:  epoch  4, batch     1 | loss: 12.6718779Losses:  9.411176681518555 0.94488525390625 5.562997341156006
MemoryTrain:  epoch  4, batch     2 | loss: 9.4111767Losses:  17.313945770263672 0.791295051574707 13.692512512207031
MemoryTrain:  epoch  4, batch     3 | loss: 17.3139458Losses:  16.82386016845703 0.7930106520652771 13.7205228805542
MemoryTrain:  epoch  4, batch     4 | loss: 16.8238602Losses:  10.83381462097168 1.900872826576233 5.621830940246582
MemoryTrain:  epoch  4, batch     5 | loss: 10.8338146Losses:  9.802209854125977 0.7539554834365845 5.659037113189697
MemoryTrain:  epoch  4, batch     6 | loss: 9.8022099Losses:  17.41377830505371 0.8108139634132385 13.734604835510254
MemoryTrain:  epoch  4, batch     7 | loss: 17.4137783Losses:  17.2756404876709 1.0243735313415527 13.74351978302002
MemoryTrain:  epoch  5, batch     0 | loss: 17.2756405Losses:  9.759506225585938 0.740684449672699 5.562058448791504
MemoryTrain:  epoch  5, batch     1 | loss: 9.7595062Losses:  11.481746673583984 1.283586859703064 8.12295150756836
MemoryTrain:  epoch  5, batch     2 | loss: 11.4817467Losses:  17.52972412109375 0.7744925022125244 13.695342063903809
MemoryTrain:  epoch  5, batch     3 | loss: 17.5297241Losses:  14.958529472351074 1.3068811893463135 10.832355499267578
MemoryTrain:  epoch  5, batch     4 | loss: 14.9585295Losses:  14.290450096130371 1.7898476123809814 8.213177680969238
MemoryTrain:  epoch  5, batch     5 | loss: 14.2904501Losses:  16.391155242919922 2.5240538120269775 10.84563159942627
MemoryTrain:  epoch  5, batch     6 | loss: 16.3911552Losses:  8.812122344970703 0.5558805465698242 5.595452785491943
MemoryTrain:  epoch  5, batch     7 | loss: 8.8121223Losses:  14.650466918945312 0.8012537956237793 10.795177459716797
MemoryTrain:  epoch  6, batch     0 | loss: 14.6504669Losses:  13.608901023864746 2.2909183502197266 8.138008117675781
MemoryTrain:  epoch  6, batch     1 | loss: 13.6089010Losses:  14.09533405303955 1.0547783374786377 10.771784782409668
MemoryTrain:  epoch  6, batch     2 | loss: 14.0953341Losses:  10.219930648803711 1.3641793727874756 5.603402614593506
MemoryTrain:  epoch  6, batch     3 | loss: 10.2199306Losses:  12.109777450561523 1.6795562505722046 8.119272232055664
MemoryTrain:  epoch  6, batch     4 | loss: 12.1097775Losses:  12.264503479003906 1.9004311561584473 8.077502250671387
MemoryTrain:  epoch  6, batch     5 | loss: 12.2645035Losses:  11.342365264892578 1.0368019342422485 8.112401962280273
MemoryTrain:  epoch  6, batch     6 | loss: 11.3423653Losses:  11.233038902282715 0.7669735550880432 8.098876953125
MemoryTrain:  epoch  6, batch     7 | loss: 11.2330389Losses:  16.597015380859375 0.518771767616272 13.667497634887695
MemoryTrain:  epoch  7, batch     0 | loss: 16.5970154Losses:  11.717418670654297 1.312459111213684 8.189959526062012
MemoryTrain:  epoch  7, batch     1 | loss: 11.7174187Losses:  8.989468574523926 1.0188552141189575 5.680835247039795
MemoryTrain:  epoch  7, batch     2 | loss: 8.9894686Losses:  11.404953002929688 1.125969409942627 8.066513061523438
MemoryTrain:  epoch  7, batch     3 | loss: 11.4049530Losses:  14.428524017333984 1.002633810043335 10.953718185424805
MemoryTrain:  epoch  7, batch     4 | loss: 14.4285240Losses:  9.13011360168457 1.513602375984192 5.555193901062012
MemoryTrain:  epoch  7, batch     5 | loss: 9.1301136Losses:  17.127580642700195 0.8052363395690918 13.681870460510254
MemoryTrain:  epoch  7, batch     6 | loss: 17.1275806Losses:  12.226520538330078 1.5864449739456177 8.082091331481934
MemoryTrain:  epoch  7, batch     7 | loss: 12.2265205Losses:  9.110533714294434 1.2620363235473633 5.650036811828613
MemoryTrain:  epoch  8, batch     0 | loss: 9.1105337Losses:  12.033161163330078 1.47008216381073 8.07230281829834
MemoryTrain:  epoch  8, batch     1 | loss: 12.0331612Losses:  12.129667282104492 1.552195429801941 8.082709312438965
MemoryTrain:  epoch  8, batch     2 | loss: 12.1296673Losses:  17.124217987060547 1.1022454500198364 13.707744598388672
MemoryTrain:  epoch  8, batch     3 | loss: 17.1242180Losses:  13.424356460571289 0.5462188720703125 10.785599708557129
MemoryTrain:  epoch  8, batch     4 | loss: 13.4243565Losses:  11.222434043884277 1.0323549509048462 8.09778881072998
MemoryTrain:  epoch  8, batch     5 | loss: 11.2224340Losses:  13.753231048583984 0.5775138139724731 10.819876670837402
MemoryTrain:  epoch  8, batch     6 | loss: 13.7532310Losses:  11.895118713378906 1.878562092781067 8.098648071289062
MemoryTrain:  epoch  8, batch     7 | loss: 11.8951187Losses:  9.126490592956543 1.323695182800293 5.5760111808776855
MemoryTrain:  epoch  9, batch     0 | loss: 9.1264906Losses:  11.423791885375977 1.0565848350524902 8.201041221618652
MemoryTrain:  epoch  9, batch     1 | loss: 11.4237919Losses:  14.653023719787598 1.869295358657837 10.788880348205566
MemoryTrain:  epoch  9, batch     2 | loss: 14.6530237Losses:  9.086234092712402 1.3405177593231201 5.5740180015563965
MemoryTrain:  epoch  9, batch     3 | loss: 9.0862341Losses:  9.985651016235352 2.082820415496826 5.562984943389893
MemoryTrain:  epoch  9, batch     4 | loss: 9.9856510Losses:  19.594417572021484 0.8085187673568726 16.73198699951172
MemoryTrain:  epoch  9, batch     5 | loss: 19.5944176Losses:  16.726369857788086 0.9633307456970215 13.695907592773438
MemoryTrain:  epoch  9, batch     6 | loss: 16.7263699Losses:  8.788220405578613 1.0764117240905762 5.699443340301514
MemoryTrain:  epoch  9, batch     7 | loss: 8.7882204
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 59.82%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 58.59%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 62.02%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 70.72%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 78.47%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.24%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.74%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 80.49%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 78.86%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 77.50%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 75.52%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 73.48%   [EVAL] batch:   37 | acc: 25.00%,  total acc: 72.20%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 71.15%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 71.56%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 72.10%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 72.47%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 72.97%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 73.30%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 72.83%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 72.61%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 73.34%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 72.12%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 71.08%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 70.31%   [EVAL] batch:   52 | acc: 18.75%,  total acc: 69.34%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 69.68%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 70.31%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 70.50%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 69.50%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 68.75%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 68.33%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 67.83%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 67.54%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 67.66%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 67.87%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 68.17%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 68.09%   
cur_acc:  ['0.8333', '0.8616', '0.3828', '0.6202']
his_acc:  ['0.8333', '0.8484', '0.7662', '0.6809']
Clustering into  12  clusters
Clusters:  [ 2  0  1  2  0  8  9  2  3  0  5  2 10 11  4  0  6  1  8  3  5  1  1  7
  8  3]
Losses:  21.88385009765625 9.50903034210205 3.726559638977051
CurrentTrain: epoch  0, batch     0 | loss: 21.8838501Losses:  14.029411315917969 2.8890445232391357 3.711512565612793
CurrentTrain: epoch  0, batch     1 | loss: 14.0294113Losses:  21.135944366455078 9.48313045501709 3.7117228507995605
CurrentTrain: epoch  1, batch     0 | loss: 21.1359444Losses:  14.482952117919922 4.018346786499023 3.6175074577331543
CurrentTrain: epoch  1, batch     1 | loss: 14.4829521Losses:  20.543498992919922 9.49522590637207 3.7294626235961914
CurrentTrain: epoch  2, batch     0 | loss: 20.5434990Losses:  12.054159164428711 3.268961191177368 3.581108331680298
CurrentTrain: epoch  2, batch     1 | loss: 12.0541592Losses:  18.12285804748535 8.165666580200195 3.6372549533843994
CurrentTrain: epoch  3, batch     0 | loss: 18.1228580Losses:  11.881528854370117 2.418468952178955 3.6192736625671387
CurrentTrain: epoch  3, batch     1 | loss: 11.8815289Losses:  16.59436798095703 8.225563049316406 3.5550050735473633
CurrentTrain: epoch  4, batch     0 | loss: 16.5943680Losses:  12.82789421081543 4.5202178955078125 1.579092264175415
CurrentTrain: epoch  4, batch     1 | loss: 12.8278942Losses:  16.604602813720703 8.195215225219727 3.5192713737487793
CurrentTrain: epoch  5, batch     0 | loss: 16.6046028Losses:  10.95128059387207 2.801549196243286 3.4163246154785156
CurrentTrain: epoch  5, batch     1 | loss: 10.9512806Losses:  15.272690773010254 7.848475456237793 3.447514533996582
CurrentTrain: epoch  6, batch     0 | loss: 15.2726908Losses:  10.365602493286133 3.7676784992218018 1.475389838218689
CurrentTrain: epoch  6, batch     1 | loss: 10.3656025Losses:  14.822750091552734 7.204535484313965 3.4690327644348145
CurrentTrain: epoch  7, batch     0 | loss: 14.8227501Losses:  10.177791595458984 2.595590829849243 3.3946752548217773
CurrentTrain: epoch  7, batch     1 | loss: 10.1777916Losses:  15.059416770935059 7.3429107666015625 3.410343885421753
CurrentTrain: epoch  8, batch     0 | loss: 15.0594168Losses:  9.829910278320312 2.1605350971221924 3.3908534049987793
CurrentTrain: epoch  8, batch     1 | loss: 9.8299103Losses:  13.765146255493164 6.570887565612793 3.3774492740631104
CurrentTrain: epoch  9, batch     0 | loss: 13.7651463Losses:  11.040544509887695 2.887604236602783 3.392697334289551
CurrentTrain: epoch  9, batch     1 | loss: 11.0405445
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: `` firstgroup 's acquisition of laidlaw will considerably enhance firstgroup 's existing activities in north america , which themselves have grown strongly since we first invested in the u.s. in 1999 , '' said firstgroup chief executive moir lockhead .
Head Entity: firstgroup
Tail Entity: laidlaw
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: `` in 2018, the tech giant google announced its acquisition of nest labs, a company specializing in smart home products, which has allowed google to expand its reach in the home automation market. ''  
Head Entity: google  
Tail Entity: nest labs  

Relation: organization subsidiaries  
Context: `` the merger between disney and pixar in 2006 was a significant move that allowed disney to revitalize its animation division and leverage pixar's innovative technology and storytelling. ''  
Head Entity: disney  
Tail Entity: pixar  

Relation: organization subsidiaries  
Context: `` after acquiring the popular video game company, activision blizzard, microsoft has significantly strengthened its position in the gaming industry, enhancing its portfolio with blockbuster titles. ''  
Head Entity: microsoft  
Tail Entity: activision blizzard  

Relation: organization subsidiaries  
Context: `` the recent purchase of whole foods by amazon has transformed the grocery landscape, enabling amazon to integrate its online shopping experience with physical retail. ''  
Head Entity: amazon  
Tail Entity: whole foods  

Relation: organization subsidiaries  
Context: `` in a strategic move to enhance its product offerings, facebook acquired instagram, allowing it to tap into the growing trend of photo-sharing among users. ''  
Head Entity: facebook  
Tail Entity: instagram  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: In a recent merger, the tech giant Alphabet Inc. announced that it would acquire the innovative startup Nest Labs, which has been a subsidiary of Google since 2014. This acquisition is expected to enhance Alphabet's smart home product line.  
Head Entity: Nest Labs  
Tail Entity: Alphabet Inc.  

Relation: organization parents  
Context: The recent restructuring of the automotive industry has led to General Motors announcing that it will be merging with its subsidiary, Chevrolet, to streamline operations and improve efficiency.  
Head Entity: Chevrolet  
Tail Entity: General Motors  

Relation: organization parents  
Context: The pharmaceutical company Pfizer has decided to integrate its research division with its parent company, which is known for its extensive portfolio of vaccines and medications.  
Head Entity: Pfizer's research division  
Tail Entity: Pfizer  

Relation: organization parents  
Context: The popular social media platform Instagram has been under the ownership of Facebook since 2012, and recent updates have sparked discussions about its future direction under the parent company.  
Head Entity: Instagram  
Tail Entity: Facebook  

Relation: organization parents  
Context: The merger between the telecommunications company AT&T and its subsidiary WarnerMedia has created a new entity that aims to dominate the media landscape.  
Head Entity: WarnerMedia  
Tail Entity: AT&T  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health crises.  
Head Entity: WHO  
Tail Entity: World Health Organization  

Relation: organization alternate names  
Context: The National Aeronautics and Space Administration, abbreviated as NASA, is responsible for the nation's civilian space program and for aeronautics and aerospace research.  
Head Entity: NASA  
Tail Entity: National Aeronautics and Space Administration  

Relation: organization alternate names  
Context: The Federal Bureau of Investigation, or FBI, is the principal federal investigative agency and domestic intelligence service of the United States.  
Head Entity: FBI  
Tail Entity: Federal Bureau of Investigation  

Relation: organization alternate names  
Context: The Central Intelligence Agency, often called the CIA, is a civilian foreign intelligence service of the federal government of the United States.  
Head Entity: CIA  
Tail Entity: Central Intelligence Agency  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant google has announced plans to expand its headquarters in the vibrant city of mountain view, california, which is known for its innovation and tech culture.  
Head Entity: google  
Tail Entity: mountain view  

Relation: organization city of headquarters  
Context: ------ new york 2019-11-10 14:30:00 utc the financial services firm goldman sachs is headquartered in the bustling city of new york, which serves as a global financial hub.  
Head Entity: goldman sachs  
Tail Entity: new york  

Relation: organization city of headquarters  
Context: ------ seattle 2020-07-22 09:45:00 utc amazon, the e-commerce leader, has its headquarters located in the tech-savvy city of seattle, washington, where it continues to grow and innovate.  
Head Entity: amazon  
Tail Entity: seattle  

Relation: organization city of headquarters  
Context: ------ boston 2022-01-05 16:20:00 utc the biotechnology company biogen is based in the historic city of cambridge, massachusetts, which is renowned for its academic institutions.  
Head Entity: biogen  
Tail Entity: cambridge  

Relation: organization city of headquarters  
Context: ------ austin 2023-05-12 11:15:00 utc the software company oracle has established its new headquarters in the lively city of austin, texas, known for its music and cultural scene.  
Head Entity: oracle  
Tail Entity: austin  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: During the family reunion, John introduced his sister, Emily, who had just returned from studying abroad.  
Head Entity: John  
Tail Entity: Emily  

Relation: person siblings  
Context: After the game, Sarah celebrated her victory with her brother, Michael, who had been cheering for her from the stands.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person siblings  
Context: In her memoir, Lisa writes fondly about her childhood adventures with her brother, Tom, who always had her back.  
Head Entity: Lisa  
Tail Entity: Tom  

Relation: person siblings  
Context: At the wedding, Anna was thrilled to see her brother, David, who had flown in from another state to be her best man.  
Head Entity: Anna  
Tail Entity: David  

Relation: person siblings  
Context: The documentary featured interviews with Rachel and her sister, Jessica, discussing their close bond and shared experiences growing up.  
Head Entity: Rachel  
Tail Entity: Jessica  
Losses:  21.97443962097168 0.8510706424713135 16.911775588989258
MemoryTrain:  epoch  0, batch     0 | loss: 21.9744396Losses:  15.696630477905273 0.5436072945594788 10.884868621826172
MemoryTrain:  epoch  0, batch     1 | loss: 15.6966305Losses:  20.695270538330078 1.6758882999420166 14.121928215026855
MemoryTrain:  epoch  0, batch     2 | loss: 20.6952705Losses:  21.394813537597656 2.2438745498657227 13.945348739624023
MemoryTrain:  epoch  0, batch     3 | loss: 21.3948135Losses:  17.831892013549805 0.702673077583313 13.757877349853516
MemoryTrain:  epoch  0, batch     4 | loss: 17.8318920Losses:  21.982078552246094 1.4140605926513672 16.75680160522461
MemoryTrain:  epoch  0, batch     5 | loss: 21.9820786Losses:  17.890071868896484 2.4493844509124756 10.795698165893555
MemoryTrain:  epoch  0, batch     6 | loss: 17.8900719Losses:  18.022634506225586 0.8215001225471497 13.801399230957031
MemoryTrain:  epoch  0, batch     7 | loss: 18.0226345Losses:  19.775785446166992 1.1908469200134277 13.94638442993164
MemoryTrain:  epoch  0, batch     8 | loss: 19.7757854Losses:  20.994850158691406 0.6204849481582642 16.72750473022461
MemoryTrain:  epoch  0, batch     9 | loss: 20.9948502Losses:  22.337934494018555 0.590153694152832 16.879179000854492
MemoryTrain:  epoch  1, batch     0 | loss: 22.3379345Losses:  19.496517181396484 1.0794169902801514 13.720479965209961
MemoryTrain:  epoch  1, batch     1 | loss: 19.4965172Losses:  14.66400146484375 0.7580683827400208 10.849234580993652
MemoryTrain:  epoch  1, batch     2 | loss: 14.6640015Losses:  18.068592071533203 1.4227559566497803 13.733504295349121
MemoryTrain:  epoch  1, batch     3 | loss: 18.0685921Losses:  20.555923461914062 0.775678277015686 16.694808959960938
MemoryTrain:  epoch  1, batch     4 | loss: 20.5559235Losses:  18.57537269592285 1.1132298707962036 13.806262016296387
MemoryTrain:  epoch  1, batch     5 | loss: 18.5753727Losses:  22.281620025634766 0.818325936794281 16.77688217163086
MemoryTrain:  epoch  1, batch     6 | loss: 22.2816200Losses:  25.199382781982422 0.5860244035720825 20.044286727905273
MemoryTrain:  epoch  1, batch     7 | loss: 25.1993828Losses:  24.662677764892578 0.6066538691520691 19.974267959594727
MemoryTrain:  epoch  1, batch     8 | loss: 24.6626778Losses:  16.802616119384766 1.4457006454467773 10.88264274597168
MemoryTrain:  epoch  1, batch     9 | loss: 16.8026161Losses:  26.201374053955078 1.7096974849700928 19.899831771850586
MemoryTrain:  epoch  2, batch     0 | loss: 26.2013741Losses:  11.766042709350586 2.054293394088745 5.577744960784912
MemoryTrain:  epoch  2, batch     1 | loss: 11.7660427Losses:  20.91177749633789 0.7938505411148071 16.770816802978516
MemoryTrain:  epoch  2, batch     2 | loss: 20.9117775Losses:  11.346308708190918 0.7805525660514832 8.16912841796875
MemoryTrain:  epoch  2, batch     3 | loss: 11.3463087Losses:  23.327682495117188 0.507658839225769 19.953641891479492
MemoryTrain:  epoch  2, batch     4 | loss: 23.3276825Losses:  14.483588218688965 0.5748686790466309 10.825738906860352
MemoryTrain:  epoch  2, batch     5 | loss: 14.4835882Losses:  17.420642852783203 0.48954176902770996 13.81881332397461
MemoryTrain:  epoch  2, batch     6 | loss: 17.4206429Losses:  20.30733299255371 0.5348072052001953 16.763084411621094
MemoryTrain:  epoch  2, batch     7 | loss: 20.3073330Losses:  20.92650032043457 0.8856692314147949 16.835371017456055
MemoryTrain:  epoch  2, batch     8 | loss: 20.9265003Losses:  14.477632522583008 0.6839599013328552 10.803238868713379
MemoryTrain:  epoch  2, batch     9 | loss: 14.4776325Losses:  16.653644561767578 0.49533218145370483 13.699779510498047
MemoryTrain:  epoch  3, batch     0 | loss: 16.6536446Losses:  19.746259689331055 -0.0 16.778268814086914
MemoryTrain:  epoch  3, batch     1 | loss: 19.7462597Losses:  17.065120697021484 0.793028712272644 13.753477096557617
MemoryTrain:  epoch  3, batch     2 | loss: 17.0651207Losses:  23.237850189208984 0.2598280608654022 19.978227615356445
MemoryTrain:  epoch  3, batch     3 | loss: 23.2378502Losses:  17.837177276611328 1.6204630136489868 13.702388763427734
MemoryTrain:  epoch  3, batch     4 | loss: 17.8371773Losses:  17.92006492614746 0.8241212368011475 13.738012313842773
MemoryTrain:  epoch  3, batch     5 | loss: 17.9200649Losses:  18.819429397583008 1.823458194732666 13.84522819519043
MemoryTrain:  epoch  3, batch     6 | loss: 18.8194294Losses:  17.847434997558594 3.320570707321167 10.839620590209961
MemoryTrain:  epoch  3, batch     7 | loss: 17.8474350Losses:  21.361984252929688 2.058176040649414 16.67381477355957
MemoryTrain:  epoch  3, batch     8 | loss: 21.3619843Losses:  15.030244827270508 0.8122199773788452 10.823813438415527
MemoryTrain:  epoch  3, batch     9 | loss: 15.0302448Losses:  17.303794860839844 1.1842286586761475 13.709028244018555
MemoryTrain:  epoch  4, batch     0 | loss: 17.3037949Losses:  23.57861328125 0.7921723127365112 19.8503475189209
MemoryTrain:  epoch  4, batch     1 | loss: 23.5786133Losses:  17.43790626525879 0.8358404636383057 13.807558059692383
MemoryTrain:  epoch  4, batch     2 | loss: 17.4379063Losses:  21.2683048248291 1.9360872507095337 16.70248031616211
MemoryTrain:  epoch  4, batch     3 | loss: 21.2683048Losses:  20.545246124267578 1.5997216701507568 16.824533462524414
MemoryTrain:  epoch  4, batch     4 | loss: 20.5452461Losses:  19.492542266845703 0.5281597375869751 16.685880661010742
MemoryTrain:  epoch  4, batch     5 | loss: 19.4925423Losses:  11.830327987670898 1.2751078605651855 8.151809692382812
MemoryTrain:  epoch  4, batch     6 | loss: 11.8303280Losses:  12.455810546875 1.2140581607818604 8.07973861694336
MemoryTrain:  epoch  4, batch     7 | loss: 12.4558105Losses:  18.306812286376953 1.3993738889694214 13.673165321350098
MemoryTrain:  epoch  4, batch     8 | loss: 18.3068123Losses:  8.529635429382324 0.859346866607666 5.584506988525391
MemoryTrain:  epoch  4, batch     9 | loss: 8.5296354Losses:  18.11419105529785 2.026524066925049 13.691920280456543
MemoryTrain:  epoch  5, batch     0 | loss: 18.1141911Losses:  20.744476318359375 1.1067242622375488 16.744997024536133
MemoryTrain:  epoch  5, batch     1 | loss: 20.7444763Losses:  14.071910858154297 0.6890507936477661 10.810848236083984
MemoryTrain:  epoch  5, batch     2 | loss: 14.0719109Losses:  17.257278442382812 1.3915209770202637 13.660039901733398
MemoryTrain:  epoch  5, batch     3 | loss: 17.2572784Losses:  12.556406021118164 1.4369206428527832 8.150862693786621
MemoryTrain:  epoch  5, batch     4 | loss: 12.5564060Losses:  17.309812545776367 0.8588861227035522 13.70163345336914
MemoryTrain:  epoch  5, batch     5 | loss: 17.3098125Losses:  17.447982788085938 1.2107982635498047 13.718779563903809
MemoryTrain:  epoch  5, batch     6 | loss: 17.4479828Losses:  13.549912452697754 0.7649387121200562 10.830434799194336
MemoryTrain:  epoch  5, batch     7 | loss: 13.5499125Losses:  17.080219268798828 0.784735918045044 13.83985710144043
MemoryTrain:  epoch  5, batch     8 | loss: 17.0802193Losses:  13.293907165527344 0.2545252740383148 10.83122444152832
MemoryTrain:  epoch  5, batch     9 | loss: 13.2939072Losses:  17.175935745239258 1.0371973514556885 13.777036666870117
MemoryTrain:  epoch  6, batch     0 | loss: 17.1759357Losses:  11.400444984436035 0.7953079342842102 8.185530662536621
MemoryTrain:  epoch  6, batch     1 | loss: 11.4004450Losses:  12.029768943786621 1.5128008127212524 8.181389808654785
MemoryTrain:  epoch  6, batch     2 | loss: 12.0297689Losses:  18.489032745361328 2.5338258743286133 13.693883895874023
MemoryTrain:  epoch  6, batch     3 | loss: 18.4890327Losses:  16.5980281829834 2.5524065494537354 10.805543899536133
MemoryTrain:  epoch  6, batch     4 | loss: 16.5980282Losses:  16.574325561523438 0.6232772469520569 13.697965621948242
MemoryTrain:  epoch  6, batch     5 | loss: 16.5743256Losses:  17.02752685546875 1.1175410747528076 13.685745239257812
MemoryTrain:  epoch  6, batch     6 | loss: 17.0275269Losses:  14.105052947998047 1.3037689924240112 10.808955192565918
MemoryTrain:  epoch  6, batch     7 | loss: 14.1050529Losses:  18.120235443115234 2.054415225982666 13.79927921295166
MemoryTrain:  epoch  6, batch     8 | loss: 18.1202354Losses:  11.879070281982422 0.6095273494720459 8.08000373840332
MemoryTrain:  epoch  6, batch     9 | loss: 11.8790703Losses:  22.332115173339844 0.23034793138504028 19.846527099609375
MemoryTrain:  epoch  7, batch     0 | loss: 22.3321152Losses:  20.29392433166504 0.8181244134902954 16.774377822875977
MemoryTrain:  epoch  7, batch     1 | loss: 20.2939243Losses:  14.351883888244629 0.7797605395317078 10.78547477722168
MemoryTrain:  epoch  7, batch     2 | loss: 14.3518839Losses:  16.705150604248047 0.738006055355072 13.696405410766602
MemoryTrain:  epoch  7, batch     3 | loss: 16.7051506Losses:  13.794861793518066 0.7430961728096008 10.904596328735352
MemoryTrain:  epoch  7, batch     4 | loss: 13.7948618Losses:  17.53219223022461 1.5774173736572266 13.695412635803223
MemoryTrain:  epoch  7, batch     5 | loss: 17.5321922Losses:  17.075525283813477 0.8718631863594055 13.758401870727539
MemoryTrain:  epoch  7, batch     6 | loss: 17.0755253Losses:  18.506916046142578 2.832343101501465 13.695513725280762
MemoryTrain:  epoch  7, batch     7 | loss: 18.5069160Losses:  14.323481559753418 0.8643220663070679 10.777219772338867
MemoryTrain:  epoch  7, batch     8 | loss: 14.3234816Losses:  16.45183563232422 0.8392144441604614 13.671468734741211
MemoryTrain:  epoch  7, batch     9 | loss: 16.4518356Losses:  19.22443199157715 0.4902321696281433 16.722679138183594
MemoryTrain:  epoch  8, batch     0 | loss: 19.2244320Losses:  24.18204116821289 1.745457410812378 19.877540588378906
MemoryTrain:  epoch  8, batch     1 | loss: 24.1820412Losses:  17.073171615600586 1.3933520317077637 13.675887107849121
MemoryTrain:  epoch  8, batch     2 | loss: 17.0731716Losses:  16.37181282043457 0.4981563687324524 13.675771713256836
MemoryTrain:  epoch  8, batch     3 | loss: 16.3718128Losses:  25.921037673950195 0.5267500877380371 23.13722801208496
MemoryTrain:  epoch  8, batch     4 | loss: 25.9210377Losses:  14.646871566772461 1.3662949800491333 10.90023136138916
MemoryTrain:  epoch  8, batch     5 | loss: 14.6468716Losses:  13.713719367980957 0.9575191736221313 10.80929183959961
MemoryTrain:  epoch  8, batch     6 | loss: 13.7137194Losses:  19.396072387695312 0.7403191328048706 16.71341323852539
MemoryTrain:  epoch  8, batch     7 | loss: 19.3960724Losses:  14.331133842468262 1.3476412296295166 10.802475929260254
MemoryTrain:  epoch  8, batch     8 | loss: 14.3311338Losses:  13.647855758666992 0.6196450591087341 10.876413345336914
MemoryTrain:  epoch  8, batch     9 | loss: 13.6478558Losses:  27.351411819458008 2.188977003097534 23.16255760192871
MemoryTrain:  epoch  9, batch     0 | loss: 27.3514118Losses:  11.602602005004883 0.9918080568313599 8.16605281829834
MemoryTrain:  epoch  9, batch     1 | loss: 11.6026020Losses:  18.25808334350586 2.1628475189208984 13.681564331054688
MemoryTrain:  epoch  9, batch     2 | loss: 18.2580833Losses:  19.848308563232422 1.1091125011444092 16.669944763183594
MemoryTrain:  epoch  9, batch     3 | loss: 19.8483086Losses:  16.515892028808594 0.7487014532089233 13.674827575683594
MemoryTrain:  epoch  9, batch     4 | loss: 16.5158920Losses:  16.781003952026367 1.0580579042434692 13.675227165222168
MemoryTrain:  epoch  9, batch     5 | loss: 16.7810040Losses:  17.152006149291992 1.4313764572143555 13.708331108093262
MemoryTrain:  epoch  9, batch     6 | loss: 17.1520061Losses:  16.91423797607422 1.1765213012695312 13.696041107177734
MemoryTrain:  epoch  9, batch     7 | loss: 16.9142380Losses:  14.500762939453125 1.4466617107391357 10.835275650024414
MemoryTrain:  epoch  9, batch     8 | loss: 14.5007629Losses:  13.070918083190918 0.2572063207626343 10.858538627624512
MemoryTrain:  epoch  9, batch     9 | loss: 13.0709181
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 16.67%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 18.75%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 32.29%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 32.14%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 33.59%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 31.94%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 32.50%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 31.82%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 31.77%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 31.73%   [EVAL] batch:   13 | acc: 6.25%,  total acc: 29.91%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 27.92%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 27.34%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 25.74%   [EVAL] batch:   17 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 27.63%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 30.63%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 32.74%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 32.67%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 66.41%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 67.43%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 75.48%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 75.93%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.37%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 77.42%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 78.41%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 76.84%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 75.18%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 73.44%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 71.45%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 69.74%   [EVAL] batch:   38 | acc: 25.00%,  total acc: 68.59%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 69.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 71.08%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 71.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 71.20%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 71.14%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 71.05%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 70.25%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 69.61%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 68.99%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 68.16%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 68.52%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 68.86%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 69.20%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 69.41%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 68.53%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 67.80%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 67.19%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 66.80%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 66.13%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 65.48%   [EVAL] batch:   63 | acc: 62.50%,  total acc: 65.43%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 65.77%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 65.81%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 65.02%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 64.25%   [EVAL] batch:   68 | acc: 18.75%,  total acc: 63.59%   [EVAL] batch:   69 | acc: 31.25%,  total acc: 63.12%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 62.94%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 63.11%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 62.59%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 62.25%   [EVAL] batch:   74 | acc: 18.75%,  total acc: 61.67%   [EVAL] batch:   75 | acc: 43.75%,  total acc: 61.43%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 60.96%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 60.50%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 60.13%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 59.45%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 58.72%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 58.23%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 57.53%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 57.29%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 57.50%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 57.78%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 57.90%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 57.46%   
cur_acc:  ['0.8333', '0.8616', '0.3828', '0.6202', '0.3267']
his_acc:  ['0.8333', '0.8484', '0.7662', '0.6809', '0.5746']
Clustering into  14  clusters
Clusters:  [ 3  1  0 11 12  9  8  3  4  1  3  3 13  6  2  1 10  7  9  4  3  0  0  5
  9  4  2  8  3  0  7]
Losses:  17.606008529663086 7.096156120300293 5.600895881652832
CurrentTrain: epoch  0, batch     0 | loss: 17.6060085Losses:  14.257404327392578 2.6162185668945312 5.616140842437744
CurrentTrain: epoch  0, batch     1 | loss: 14.2574043Losses:  19.870075225830078 10.022481918334961 5.613094329833984
CurrentTrain: epoch  1, batch     0 | loss: 19.8700752Losses:  12.785239219665527 6.613632678985596 1.389012098312378
CurrentTrain: epoch  1, batch     1 | loss: 12.7852392Losses:  16.653793334960938 7.5251569747924805 5.590572357177734
CurrentTrain: epoch  2, batch     0 | loss: 16.6537933Losses:  11.130321502685547 3.848557472229004 3.354749917984009
CurrentTrain: epoch  2, batch     1 | loss: 11.1303215Losses:  15.761879920959473 7.171108722686768 5.602387428283691
CurrentTrain: epoch  3, batch     0 | loss: 15.7618799Losses:  9.610434532165527 3.0773208141326904 3.3184642791748047
CurrentTrain: epoch  3, batch     1 | loss: 9.6104345Losses:  15.08596420288086 6.706191539764404 5.6003336906433105
CurrentTrain: epoch  4, batch     0 | loss: 15.0859642Losses:  7.72808837890625 2.0349342823028564 3.302173614501953
CurrentTrain: epoch  4, batch     1 | loss: 7.7280884Losses:  13.847509384155273 5.892214775085449 5.592559337615967
CurrentTrain: epoch  5, batch     0 | loss: 13.8475094Losses:  9.75549030303955 1.8275113105773926 5.573245048522949
CurrentTrain: epoch  5, batch     1 | loss: 9.7554903Losses:  13.246231079101562 5.457181930541992 5.596084117889404
CurrentTrain: epoch  6, batch     0 | loss: 13.2462311Losses:  8.635587692260742 1.0944448709487915 5.5772223472595215
CurrentTrain: epoch  6, batch     1 | loss: 8.6355877Losses:  13.788374900817871 6.229444980621338 5.587216377258301
CurrentTrain: epoch  7, batch     0 | loss: 13.7883749Losses:  7.32320499420166 1.978508472442627 3.304776430130005
CurrentTrain: epoch  7, batch     1 | loss: 7.3232050Losses:  13.268022537231445 5.699001312255859 5.562281608581543
CurrentTrain: epoch  8, batch     0 | loss: 13.2680225Losses:  9.205944061279297 1.6812176704406738 5.583959579467773
CurrentTrain: epoch  8, batch     1 | loss: 9.2059441Losses:  13.100863456726074 5.615640163421631 5.565324306488037
CurrentTrain: epoch  9, batch     0 | loss: 13.1008635Losses:  9.025067329406738 1.675200343132019 5.553971767425537
CurrentTrain: epoch  9, batch     1 | loss: 9.0250673
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the jnf was founded in 1901 to buy plots in palestine , then ruled by the ottomans .
Head Entity: jnf
Tail Entity: 1901
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: the united nations was established in 1945 to promote international cooperation and peace.  
Head Entity: united nations  
Tail Entity: 1945  

Relation: organization founded  
Context: apple inc. was founded in 1976 by steve jobs, steve wozniak, and ronald wayne.  
Head Entity: apple inc.  
Tail Entity: 1976  

Relation: organization founded  
Context: the world health organization was created in 1948 to coordinate global health efforts.  
Head Entity: world health organization  
Tail Entity: 1948  

Relation: organization founded  
Context: the national aeronautics and space administration was formed in 1958 to oversee the nation's civilian space program.  
Head Entity: national aeronautics and space administration  
Tail Entity: 1958  

Relation: organization founded  
Context: the european union was established by the maastricht treaty in 1993 to enhance political and economic integration among member states.  
Head Entity: european union  
Tail Entity: 1993  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, emily decided to throw a big party for her friends.  
Head Entity: emily  
Tail Entity: 30  

Relation: person age  
Context: the renowned scientist albert einstein was born in 1879, making him 76 years old when he passed away.  
Head Entity: albert einstein  
Tail Entity: 76  

Relation: person age  
Context: at the age of 45, sarah finally achieved her dream of becoming a published author.  
Head Entity: sarah  
Tail Entity: 45  

Relation: person age  
Context: during the interview, the 22-year-old athlete shared his journey to success.  
Head Entity: the athlete  
Tail Entity: 22  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he pursued his career.  
Head Entity: he  
Tail Entity: new york  

Relation: person city of birth  
Context: marie curie was born in warsaw, poland, and later became a renowned scientist.  
Head Entity: marie curie  
Tail Entity: warsaw  

Relation: person city of birth  
Context: the famous author was born in dublin, which greatly influenced his literary works.  
Head Entity: the famous author  
Tail Entity: dublin  

Relation: person city of birth  
Context: during her childhood, she lived in paris, where she was born and raised.  
Head Entity: she  
Tail Entity: paris  

Relation: person city of birth  
Context: the musician was born in nashville, a city known for its rich musical heritage.  
Head Entity: the musician  
Tail Entity: nashville  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians, including members from the Boston Symphony Orchestra.  
Head Entity: Boston Symphony Orchestra  
Tail Entity: New York Philharmonic  

Relation: organization members  
Context: The National Football League has several teams, including the Dallas Cowboys, which has produced many famous players who have gone on to join the Pro Football Hall of Fame.  
Head Entity: Pro Football Hall of Fame  
Tail Entity: Dallas Cowboys  

Relation: organization members  
Context: The International Olympic Committee oversees the Olympic Games and has various national committees, including the United States Olympic and Paralympic Committee, which represents athletes from the U.S.  
Head Entity: United States Olympic and Paralympic Committee  
Tail Entity: International Olympic Committee  

Relation: organization members  
Context: The American Medical Association has numerous affiliated organizations, including the American College of Physicians, which focuses on internal medicine.  
Head Entity: American College of Physicians  
Tail Entity: American Medical Association  

Relation: organization members  
Context: The United Nations has various specialized agencies, one of which is the World Health Organization, which works on global health issues.  
Head Entity: World Health Organization  
Tail Entity: United Nations  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: though not a household name , wildmon has considerable clout ; his group has a vast mailing list and a proven ability to mobilize christian conservatives by the hundreds of thousands .
Head Entity: his
Tail Entity: christian
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of study, she decided to convert to Buddhism, finding peace and purpose in its teachings.  
Head Entity: she  
Tail Entity: Buddhism  

Relation: person religion  
Context: The famous actor often speaks about his deep connection to Judaism and how it shapes his worldview.  
Head Entity: actor  
Tail Entity: Judaism  

Relation: person religion  
Context: Growing up in a Catholic family, he has always felt a strong bond with the teachings of the Church.  
Head Entity: he  
Tail Entity: Catholic  

Relation: person religion  
Context: As a prominent leader in the community, her advocacy for Islam has inspired many to explore their faith.  
Head Entity: her  
Tail Entity: Islam  

Relation: person religion  
Context: The author frequently references his Hindu beliefs in his writings, which resonate with many of his readers.  
Head Entity: author  
Tail Entity: Hindu  
Losses:  27.547468185424805 0.5105938911437988 23.379709243774414
MemoryTrain:  epoch  0, batch     0 | loss: 27.5474682Losses:  10.704845428466797 1.183522343635559 5.572571277618408
MemoryTrain:  epoch  0, batch     1 | loss: 10.7048454Losses:  18.303604125976562 1.039590835571289 13.677055358886719
MemoryTrain:  epoch  0, batch     2 | loss: 18.3036041Losses:  21.507770538330078 1.058674931526184 16.754695892333984
MemoryTrain:  epoch  0, batch     3 | loss: 21.5077705Losses:  21.466108322143555 0.5805343985557556 16.717195510864258
MemoryTrain:  epoch  0, batch     4 | loss: 21.4661083Losses:  18.399381637573242 1.0529531240463257 13.920331954956055
MemoryTrain:  epoch  0, batch     5 | loss: 18.3993816Losses:  17.028316497802734 1.6181344985961914 11.111234664916992
MemoryTrain:  epoch  0, batch     6 | loss: 17.0283165Losses:  17.845317840576172 1.0051145553588867 13.74610710144043
MemoryTrain:  epoch  0, batch     7 | loss: 17.8453178Losses:  17.952302932739258 1.03022301197052 13.718745231628418
MemoryTrain:  epoch  0, batch     8 | loss: 17.9523029Losses:  24.204133987426758 1.0710015296936035 19.93060874938965
MemoryTrain:  epoch  0, batch     9 | loss: 24.2041340Losses:  24.100914001464844 0.5445552468299866 19.8622989654541
MemoryTrain:  epoch  0, batch    10 | loss: 24.1009140Losses:  15.048163414001465 0.5666787028312683 10.825672149658203
MemoryTrain:  epoch  0, batch    11 | loss: 15.0481634Losses:  30.562896728515625 0.7045915126800537 26.482791900634766
MemoryTrain:  epoch  1, batch     0 | loss: 30.5628967Losses:  20.86838722229004 1.1137492656707764 16.75533676147461
MemoryTrain:  epoch  1, batch     1 | loss: 20.8683872Losses:  17.345064163208008 0.7973177433013916 13.757275581359863
MemoryTrain:  epoch  1, batch     2 | loss: 17.3450642Losses:  18.229318618774414 1.363020658493042 13.686302185058594
MemoryTrain:  epoch  1, batch     3 | loss: 18.2293186Losses:  28.796205520629883 1.4616453647613525 23.2047119140625
MemoryTrain:  epoch  1, batch     4 | loss: 28.7962055Losses:  11.931396484375 1.0785553455352783 8.077130317687988
MemoryTrain:  epoch  1, batch     5 | loss: 11.9313965Losses:  17.42953872680664 1.2015087604522705 13.70341682434082
MemoryTrain:  epoch  1, batch     6 | loss: 17.4295387Losses:  18.509559631347656 1.6696991920471191 13.84822940826416
MemoryTrain:  epoch  1, batch     7 | loss: 18.5095596Losses:  21.531818389892578 2.229094982147217 16.791967391967773
MemoryTrain:  epoch  1, batch     8 | loss: 21.5318184Losses:  20.308774948120117 1.0313196182250977 16.76578140258789
MemoryTrain:  epoch  1, batch     9 | loss: 20.3087749Losses:  19.826107025146484 0.5332326889038086 16.70026206970215
MemoryTrain:  epoch  1, batch    10 | loss: 19.8261070Losses:  16.334800720214844 -0.0 13.692115783691406
MemoryTrain:  epoch  1, batch    11 | loss: 16.3348007Losses:  17.410646438598633 1.3371175527572632 13.692421913146973
MemoryTrain:  epoch  2, batch     0 | loss: 17.4106464Losses:  23.296743392944336 0.7850760221481323 19.86152458190918
MemoryTrain:  epoch  2, batch     1 | loss: 23.2967434Losses:  19.40475082397461 0.5292171239852905 16.724536895751953
MemoryTrain:  epoch  2, batch     2 | loss: 19.4047508Losses:  20.160377502441406 1.1445295810699463 16.728647232055664
MemoryTrain:  epoch  2, batch     3 | loss: 20.1603775Losses:  19.566917419433594 0.5306214690208435 16.697357177734375
MemoryTrain:  epoch  2, batch     4 | loss: 19.5669174Losses:  16.648372650146484 0.7469642758369446 13.65645694732666
MemoryTrain:  epoch  2, batch     5 | loss: 16.6483727Losses:  12.065275192260742 1.3683700561523438 8.174505233764648
MemoryTrain:  epoch  2, batch     6 | loss: 12.0652752Losses:  15.66338062286377 1.9867491722106934 10.796611785888672
MemoryTrain:  epoch  2, batch     7 | loss: 15.6633806Losses:  22.925268173217773 0.7642403841018677 19.881988525390625
MemoryTrain:  epoch  2, batch     8 | loss: 22.9252682Losses:  32.29393005371094 0.23824527859687805 29.981521606445312
MemoryTrain:  epoch  2, batch     9 | loss: 32.2939301Losses:  16.885889053344727 0.5847522616386414 13.683433532714844
MemoryTrain:  epoch  2, batch    10 | loss: 16.8858891Losses:  13.746342658996582 0.296830415725708 10.924036026000977
MemoryTrain:  epoch  2, batch    11 | loss: 13.7463427Losses:  19.941572189331055 0.9842755794525146 16.710357666015625
MemoryTrain:  epoch  3, batch     0 | loss: 19.9415722Losses:  19.934324264526367 0.8179956078529358 16.709716796875
MemoryTrain:  epoch  3, batch     1 | loss: 19.9343243Losses:  16.56359100341797 0.7571556568145752 13.66706657409668
MemoryTrain:  epoch  3, batch     2 | loss: 16.5635910Losses:  16.502662658691406 0.2369520664215088 13.706998825073242
MemoryTrain:  epoch  3, batch     3 | loss: 16.5026627Losses:  18.083459854125977 1.880812168121338 13.69626522064209
MemoryTrain:  epoch  3, batch     4 | loss: 18.0834599Losses:  17.126399993896484 1.3203043937683105 13.676121711730957
MemoryTrain:  epoch  3, batch     5 | loss: 17.1264000Losses:  19.974191665649414 1.049843192100525 16.71640968322754
MemoryTrain:  epoch  3, batch     6 | loss: 19.9741917Losses:  22.483007431030273 0.49205920100212097 19.924577713012695
MemoryTrain:  epoch  3, batch     7 | loss: 22.4830074Losses:  19.48870086669922 0.5253636240959167 16.73760223388672
MemoryTrain:  epoch  3, batch     8 | loss: 19.4887009Losses:  12.11465072631836 1.9252873659133911 8.091936111450195
MemoryTrain:  epoch  3, batch     9 | loss: 12.1146507Losses:  17.54947853088379 1.3797404766082764 13.692597389221191
MemoryTrain:  epoch  3, batch    10 | loss: 17.5494785Losses:  12.803903579711914 -0.0 10.806032180786133
MemoryTrain:  epoch  3, batch    11 | loss: 12.8039036Losses:  14.410452842712402 1.3267714977264404 10.779650688171387
MemoryTrain:  epoch  4, batch     0 | loss: 14.4104528Losses:  23.316640853881836 1.2104346752166748 19.964115142822266
MemoryTrain:  epoch  4, batch     1 | loss: 23.3166409Losses:  17.14671516418457 1.3841272592544556 13.67508316040039
MemoryTrain:  epoch  4, batch     2 | loss: 17.1467152Losses:  16.454036712646484 0.7195885181427002 13.657794952392578
MemoryTrain:  epoch  4, batch     3 | loss: 16.4540367Losses:  19.576478958129883 0.7336074113845825 16.712480545043945
MemoryTrain:  epoch  4, batch     4 | loss: 19.5764790Losses:  23.40544891357422 1.3932477235794067 19.851709365844727
MemoryTrain:  epoch  4, batch     5 | loss: 23.4054489Losses:  25.92176628112793 0.5578503012657166 23.118444442749023
MemoryTrain:  epoch  4, batch     6 | loss: 25.9217663Losses:  19.54912757873535 0.7423441410064697 16.754642486572266
MemoryTrain:  epoch  4, batch     7 | loss: 19.5491276Losses:  19.710437774658203 0.8212732076644897 16.705286026000977
MemoryTrain:  epoch  4, batch     8 | loss: 19.7104378Losses:  18.887353897094727 0.2348225861787796 16.70148468017578
MemoryTrain:  epoch  4, batch     9 | loss: 18.8873539Losses:  16.98674774169922 1.385286808013916 13.665348052978516
MemoryTrain:  epoch  4, batch    10 | loss: 16.9867477Losses:  13.391629219055176 0.576897919178009 10.82224178314209
MemoryTrain:  epoch  4, batch    11 | loss: 13.3916292Losses:  13.628148078918457 0.7727868556976318 10.805112838745117
MemoryTrain:  epoch  5, batch     0 | loss: 13.6281481Losses:  22.08623504638672 0.24767658114433289 19.828550338745117
MemoryTrain:  epoch  5, batch     1 | loss: 22.0862350Losses:  22.61994171142578 0.5206937193870544 19.946901321411133
MemoryTrain:  epoch  5, batch     2 | loss: 22.6199417Losses:  14.351357460021973 1.3915576934814453 10.845174789428711
MemoryTrain:  epoch  5, batch     3 | loss: 14.3513575Losses:  22.5704345703125 0.7347680330276489 19.85736846923828
MemoryTrain:  epoch  5, batch     4 | loss: 22.5704346Losses:  13.61454963684082 0.7452348470687866 10.787602424621582
MemoryTrain:  epoch  5, batch     5 | loss: 13.6145496Losses:  19.960237503051758 1.1217035055160522 16.69540786743164
MemoryTrain:  epoch  5, batch     6 | loss: 19.9602375Losses:  22.512157440185547 0.718177080154419 19.858610153198242
MemoryTrain:  epoch  5, batch     7 | loss: 22.5121574Losses:  26.00371551513672 0.7698923349380493 23.17766761779785
MemoryTrain:  epoch  5, batch     8 | loss: 26.0037155Losses:  22.863767623901367 0.870110273361206 19.860198974609375
MemoryTrain:  epoch  5, batch     9 | loss: 22.8637676Losses:  13.834776878356934 1.02962327003479 10.786005973815918
MemoryTrain:  epoch  5, batch    10 | loss: 13.8347769Losses:  13.134323120117188 0.31253355741500854 10.83916187286377
MemoryTrain:  epoch  5, batch    11 | loss: 13.1343231Losses:  22.026569366455078 0.24244636297225952 19.820016860961914
MemoryTrain:  epoch  6, batch     0 | loss: 22.0265694Losses:  16.33314323425293 0.7622438669204712 13.661949157714844
MemoryTrain:  epoch  6, batch     1 | loss: 16.3331432Losses:  10.348657608032227 0.2709970474243164 8.083388328552246
MemoryTrain:  epoch  6, batch     2 | loss: 10.3486576Losses:  22.69852638244629 0.7761232852935791 19.864038467407227
MemoryTrain:  epoch  6, batch     3 | loss: 22.6985264Losses:  19.924386978149414 1.2697170972824097 16.685155868530273
MemoryTrain:  epoch  6, batch     4 | loss: 19.9243870Losses:  19.444847106933594 0.521522045135498 16.704360961914062
MemoryTrain:  epoch  6, batch     5 | loss: 19.4448471Losses:  25.429296493530273 0.2492842823266983 23.136259078979492
MemoryTrain:  epoch  6, batch     6 | loss: 25.4292965Losses:  19.219064712524414 0.5104472637176514 16.718658447265625
MemoryTrain:  epoch  6, batch     7 | loss: 19.2190647Losses:  20.668434143066406 1.9583690166473389 16.712865829467773
MemoryTrain:  epoch  6, batch     8 | loss: 20.6684341Losses:  19.898962020874023 1.1589276790618896 16.674245834350586
MemoryTrain:  epoch  6, batch     9 | loss: 19.8989620Losses:  19.786104202270508 1.0836459398269653 16.717140197753906
MemoryTrain:  epoch  6, batch    10 | loss: 19.7861042Losses:  10.360603332519531 0.28899240493774414 8.09865951538086
MemoryTrain:  epoch  6, batch    11 | loss: 10.3606033Losses:  13.350042343139648 0.519287109375 10.8075590133667
MemoryTrain:  epoch  7, batch     0 | loss: 13.3500423Losses:  16.02273941040039 0.30249568819999695 13.656015396118164
MemoryTrain:  epoch  7, batch     1 | loss: 16.0227394Losses:  19.018890380859375 0.25079184770584106 16.701759338378906
MemoryTrain:  epoch  7, batch     2 | loss: 19.0188904Losses:  14.592556953430176 1.6010463237762451 10.80861759185791
MemoryTrain:  epoch  7, batch     3 | loss: 14.5925570Losses:  16.42352294921875 0.7397786378860474 13.652469635009766
MemoryTrain:  epoch  7, batch     4 | loss: 16.4235229Losses:  16.790523529052734 1.063493251800537 13.705741882324219
MemoryTrain:  epoch  7, batch     5 | loss: 16.7905235Losses:  19.202129364013672 0.5172311663627625 16.712331771850586
MemoryTrain:  epoch  7, batch     6 | loss: 19.2021294Losses:  25.524539947509766 0.501682698726654 23.105009078979492
MemoryTrain:  epoch  7, batch     7 | loss: 25.5245399Losses:  17.334739685058594 1.6620053052902222 13.67203140258789
MemoryTrain:  epoch  7, batch     8 | loss: 17.3347397Losses:  25.57037925720215 0.49820059537887573 23.101470947265625
MemoryTrain:  epoch  7, batch     9 | loss: 25.5703793Losses:  19.766986846923828 1.08443284034729 16.748647689819336
MemoryTrain:  epoch  7, batch    10 | loss: 19.7669868Losses:  18.894317626953125 0.31475386023521423 16.69210433959961
MemoryTrain:  epoch  7, batch    11 | loss: 18.8943176Losses:  16.54269790649414 0.9761532545089722 13.656055450439453
MemoryTrain:  epoch  8, batch     0 | loss: 16.5426979Losses:  22.579130172729492 0.758267879486084 19.837202072143555
MemoryTrain:  epoch  8, batch     1 | loss: 22.5791302Losses:  25.7448673248291 0.7101792097091675 23.1057186126709
MemoryTrain:  epoch  8, batch     2 | loss: 25.7448673Losses:  14.348159790039062 1.6357790231704712 10.771402359008789
MemoryTrain:  epoch  8, batch     3 | loss: 14.3481598Losses:  22.356786727905273 0.515213131904602 19.869359970092773
MemoryTrain:  epoch  8, batch     4 | loss: 22.3567867Losses:  19.310012817382812 0.5385291576385498 16.71903419494629
MemoryTrain:  epoch  8, batch     5 | loss: 19.3100128Losses:  17.362089157104492 1.697685956954956 13.680116653442383
MemoryTrain:  epoch  8, batch     6 | loss: 17.3620892Losses:  17.083532333374023 1.4115488529205322 13.70503044128418
MemoryTrain:  epoch  8, batch     7 | loss: 17.0835323Losses:  22.263261795043945 0.48436105251312256 19.83888816833496
MemoryTrain:  epoch  8, batch     8 | loss: 22.2632618Losses:  22.474342346191406 0.7217525243759155 19.851442337036133
MemoryTrain:  epoch  8, batch     9 | loss: 22.4743423Losses:  16.78627586364746 1.165539264678955 13.703414916992188
MemoryTrain:  epoch  8, batch    10 | loss: 16.7862759Losses:  15.86707878112793 0.261165589094162 13.711958885192871
MemoryTrain:  epoch  8, batch    11 | loss: 15.8670788Losses:  22.511425018310547 0.7207514047622681 19.880266189575195
MemoryTrain:  epoch  9, batch     0 | loss: 22.5114250Losses:  22.006935119628906 0.2382790446281433 19.855121612548828
MemoryTrain:  epoch  9, batch     1 | loss: 22.0069351Losses:  19.557966232299805 0.9836392402648926 16.679956436157227
MemoryTrain:  epoch  9, batch     2 | loss: 19.5579662Losses:  15.857636451721191 0.2478945404291153 13.662686347961426
MemoryTrain:  epoch  9, batch     3 | loss: 15.8576365Losses:  22.011211395263672 0.26248687505722046 19.84523582458496
MemoryTrain:  epoch  9, batch     4 | loss: 22.0112114Losses:  16.874038696289062 1.2844581604003906 13.664058685302734
MemoryTrain:  epoch  9, batch     5 | loss: 16.8740387Losses:  16.580503463745117 0.9821065068244934 13.699322700500488
MemoryTrain:  epoch  9, batch     6 | loss: 16.5805035Losses:  17.4339599609375 1.8530259132385254 13.682024955749512
MemoryTrain:  epoch  9, batch     7 | loss: 17.4339600Losses:  18.865158081054688 0.25121256709098816 16.703845977783203
MemoryTrain:  epoch  9, batch     8 | loss: 18.8651581Losses:  13.83453369140625 1.0435315370559692 10.811410903930664
MemoryTrain:  epoch  9, batch     9 | loss: 13.8345337Losses:  16.455322265625 0.8811176419258118 13.664400100708008
MemoryTrain:  epoch  9, batch    10 | loss: 16.4553223Losses:  10.312664031982422 0.2867780923843384 8.06687068939209
MemoryTrain:  epoch  9, batch    11 | loss: 10.3126640
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 95.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 96.43%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 87.05%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 60.71%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 62.05%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 64.80%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 65.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 67.56%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 73.08%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 73.61%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 75.42%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 75.60%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 76.52%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 73.39%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 71.70%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 69.76%   [EVAL] batch:   37 | acc: 6.25%,  total acc: 68.09%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 67.15%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 67.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 68.45%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 69.77%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 70.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 69.97%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 69.95%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 69.77%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 69.00%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 68.38%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 68.03%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 67.22%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 67.59%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 68.07%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 68.42%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 68.64%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 67.89%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 67.27%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 66.88%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 66.80%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 65.93%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 64.98%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 64.75%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 65.10%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 65.06%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 64.18%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 63.24%   [EVAL] batch:   68 | acc: 12.50%,  total acc: 62.50%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 62.32%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 62.24%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 61.98%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 61.39%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 60.81%   [EVAL] batch:   74 | acc: 12.50%,  total acc: 60.17%   [EVAL] batch:   75 | acc: 12.50%,  total acc: 59.54%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 58.85%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 58.25%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 57.59%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 56.88%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 56.17%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 55.79%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 55.12%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 54.76%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 55.07%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 55.60%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 55.89%   [EVAL] batch:   87 | acc: 100.00%,  total acc: 56.39%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 56.67%   [EVAL] batch:   89 | acc: 93.75%,  total acc: 57.08%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 57.55%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 58.02%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.47%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 58.91%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 59.34%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 59.77%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 59.66%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 59.82%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 60.16%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 60.31%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 60.40%   
cur_acc:  ['0.8333', '0.8616', '0.3828', '0.6202', '0.3267', '0.8705']
his_acc:  ['0.8333', '0.8484', '0.7662', '0.6809', '0.5746', '0.6040']
Clustering into  17  clusters
Clusters:  [ 2  0 11 12 15  8  6  2  7  0  2  2 13 10  1  0  3  5  8 14  2 16 16  4
  8  7  1  6  2 11  5  2  3  9  4  7]
Losses:  22.005882263183594 9.753376007080078 5.901703834533691
CurrentTrain: epoch  0, batch     0 | loss: 22.0058823Losses:  15.227079391479492 6.232638835906982 1.49237060546875
CurrentTrain: epoch  0, batch     1 | loss: 15.2270794Losses:  19.884225845336914 7.699581146240234 5.858578681945801
CurrentTrain: epoch  1, batch     0 | loss: 19.8842258Losses:  13.18246841430664 2.487689256668091 5.574211120605469
CurrentTrain: epoch  1, batch     1 | loss: 13.1824684Losses:  17.607587814331055 6.871713638305664 5.727236270904541
CurrentTrain: epoch  2, batch     0 | loss: 17.6075878Losses:  13.990110397338867 2.8910739421844482 5.579651832580566
CurrentTrain: epoch  2, batch     1 | loss: 13.9901104Losses:  17.413990020751953 7.114373207092285 5.6678056716918945
CurrentTrain: epoch  3, batch     0 | loss: 17.4139900Losses:  14.010248184204102 2.715806722640991 5.67620325088501
CurrentTrain: epoch  3, batch     1 | loss: 14.0102482Losses:  18.016597747802734 7.638661861419678 5.655716419219971
CurrentTrain: epoch  4, batch     0 | loss: 18.0165977Losses:  11.52626895904541 2.034794569015503 5.617959022521973
CurrentTrain: epoch  4, batch     1 | loss: 11.5262690Losses:  17.899999618530273 7.949383735656738 5.634307384490967
CurrentTrain: epoch  5, batch     0 | loss: 17.8999996Losses:  9.39023494720459 2.7094509601593018 3.3404204845428467
CurrentTrain: epoch  5, batch     1 | loss: 9.3902349Losses:  16.592208862304688 6.983259201049805 5.656777381896973
CurrentTrain: epoch  6, batch     0 | loss: 16.5922089Losses:  12.023126602172852 2.333766460418701 5.6043620109558105
CurrentTrain: epoch  6, batch     1 | loss: 12.0231266Losses:  15.838294982910156 6.7603254318237305 5.611484050750732
CurrentTrain: epoch  7, batch     0 | loss: 15.8382950Losses:  11.933311462402344 2.4683754444122314 5.598378658294678
CurrentTrain: epoch  7, batch     1 | loss: 11.9333115Losses:  16.05081558227539 7.075070381164551 5.596197605133057
CurrentTrain: epoch  8, batch     0 | loss: 16.0508156Losses:  9.675701141357422 2.709580659866333 3.3588733673095703
CurrentTrain: epoch  8, batch     1 | loss: 9.6757011Losses:  15.30152702331543 6.428727149963379 5.603238582611084
CurrentTrain: epoch  9, batch     0 | loss: 15.3015270Losses:  11.281832695007324 2.3880221843719482 5.579193115234375
CurrentTrain: epoch  9, batch     1 | loss: 11.2818327
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After years of living in the bustling city, actor Tom Hanks has decided to settle down in a quieter area, choosing to make his home in the picturesque state of California.  
Head Entity: Tom Hanks  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: The renowned author J.K. Rowling has spent much of her life in Edinburgh, where she found inspiration for her famous Harry Potter series.  
Head Entity: J.K. Rowling  
Tail Entity: Edinburgh  

Relation: person stateorprovinces of residence  
Context: Following his successful career in the tech industry, entrepreneur Elon Musk has moved to Texas, where he plans to expand his business ventures.  
Head Entity: Elon Musk  
Tail Entity: Texas  

Relation: person stateorprovinces of residence  
Context: After retiring from professional basketball, Michael Jordan chose to reside in North Carolina, where he continues to be involved in the local community.  
Head Entity: Michael Jordan  
Tail Entity: North Carolina  

Relation: person stateorprovinces of residence  
Context: The famous singer Taylor Swift has made her home in Nashville, Tennessee, where she began her music career and still finds creative inspiration.  
Head Entity: Taylor Swift  
Tail Entity: Nashville
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away on July 20, 2020.  
Head Entity: The renowned author  
Tail Entity: July 20, 2020  

Relation: person date of death  
Context: She left this world on March 5, 2018, after a long illness.  
Head Entity: She  
Tail Entity: March 5, 2018  

Relation: person date of death  
Context: His family announced that he died peacefully in his sleep on November 11.  
Head Entity: He  
Tail Entity: November 11  

Relation: person date of death  
Context: The scientist's contributions were celebrated after his death on January 1, 2021.  
Head Entity: The scientist  
Tail Entity: January 1, 2021  

Relation: person date of death  
Context: They mourned her loss, which occurred on September 30, 2019.  
Head Entity: They  
Tail Entity: September 30, 2019  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: mcgovern wants to make the red cross , with 30,000 employees and 700 chapters , more efficient .
Head Entity: red cross
Tail Entity: 30,000
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
1. Relation: organization number of employees members  
   Context: The tech giant Google boasts a workforce of over 156,000 employees worldwide.  
   Head Entity: Google  
   Tail Entity: 156,000  

2. Relation: organization number of employees members  
   Context: With a dedicated team of 50,000 staff, the United Nations works tirelessly to address global issues.  
   Head Entity: United Nations  
   Tail Entity: 50,000  

3. Relation: organization number of employees members  
   Context: Amazon has expanded rapidly, now employing around 1,300,000 individuals across various sectors.  
   Head Entity: Amazon  
   Tail Entity: 1,300,000  

4. Relation: organization number of employees members  
   Context: The nonprofit organization Habitat for Humanity has approximately 2,300 employees working towards affordable housing.  
   Head Entity: Habitat for Humanity  
   Tail Entity: 2,300  

5. Relation: organization number of employees members  
   Context: Microsoft reported a total of 181,000 employees as of the last fiscal year, contributing to its innovative projects.  
   Head Entity: Microsoft  
   Tail Entity: 181,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: the coming of the mahdi will turn the world upside down , and the oppressed shiites will finally see justice .
Head Entity: mahdi
Tail Entity: shiites
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
1. Relation: person alternate names  
   Context: The famous author Mark Twain is known for his wit and humor, but his real name was Samuel Clemens.  
   Head Entity: Mark Twain  
   Tail Entity: Samuel Clemens  

2. Relation: person alternate names  
   Context: The musician known as Lady Gaga was born Stefani Joanne Angelina Germanotta.  
   Head Entity: Lady Gaga  
   Tail Entity: Stefani Joanne Angelina Germanotta  

3. Relation: person alternate names  
   Context: The legendary basketball player Michael Jordan is often referred to simply as "MJ."  
   Head Entity: Michael Jordan  
   Tail Entity: MJ  

4. Relation: person alternate names  
   Context: The artist known as Banksy remains anonymous, but some speculate his real name is Robert Del Naja.  
   Head Entity: Banksy  
   Tail Entity: Robert Del Naja  

5. Relation: person alternate names  
   Context: The famous physicist Albert Einstein is often called the "father of modern physics."  
   Head Entity: Albert Einstein  
   Tail Entity: father of modern physics  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: kerry katona feel out with arnold just before she married bryan mcfadden , kerry cancelled his wedding invitation and have n't spoken since .
Head Entity: bryan mcfadden
Tail Entity: kerry katona
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
1. Relation: person spouse  
   Context: After years of dating, emma stone finally tied the knot with dave mccary in a beautiful ceremony last summer.  
   Head Entity: dave mccary  
   Tail Entity: emma stone  

2. Relation: person spouse  
   Context: During their time together, ben affleck and jennifer garner built a family, but they eventually decided to part ways amicably.  
   Head Entity: jennifer garner  
   Tail Entity: ben affleck  

3. Relation: person spouse  
   Context: The couple celebrated their 10th anniversary last week, marking a decade of love and partnership between ryan reynolds and blake lively.  
   Head Entity: blake lively  
   Tail Entity: ryan reynolds  

4. Relation: person spouse  
   Context: After a whirlwind romance, prince harry and meghan markle announced their engagement, capturing the hearts of many around the world.  
   Head Entity: meghan markle  
   Tail Entity: prince harry  

5. Relation: person spouse  
   Context: Following their lavish wedding in italy, george clooney and amal alamuddin have become one of the most talked-about couples in hollywood.  
   Head Entity: amal alamuddin  
   Tail Entity: george clooney  
Losses:  29.975961685180664 0.47547197341918945 26.677227020263672
MemoryTrain:  epoch  0, batch     0 | loss: 29.9759617Losses:  15.045671463012695 1.6107592582702637 10.794569969177246
MemoryTrain:  epoch  0, batch     1 | loss: 15.0456715Losses:  29.97410774230957 0.46348920464515686 26.699413299560547
MemoryTrain:  epoch  0, batch     2 | loss: 29.9741077Losses:  23.978313446044922 0.5209712982177734 19.933897018432617
MemoryTrain:  epoch  0, batch     3 | loss: 23.9783134Losses:  26.774314880371094 0.5074785351753235 23.18245506286621
MemoryTrain:  epoch  0, batch     4 | loss: 26.7743149Losses:  23.58831024169922 1.019370436668396 19.89870262145996
MemoryTrain:  epoch  0, batch     5 | loss: 23.5883102Losses:  25.19379425048828 1.4316004514694214 20.000463485717773
MemoryTrain:  epoch  0, batch     6 | loss: 25.1937943Losses:  27.225618362426758 0.6185574531555176 23.214996337890625
MemoryTrain:  epoch  0, batch     7 | loss: 27.2256184Losses:  22.899038314819336 0.7326646447181702 19.861867904663086
MemoryTrain:  epoch  0, batch     8 | loss: 22.8990383Losses:  24.797462463378906 1.4680671691894531 19.922828674316406
MemoryTrain:  epoch  0, batch     9 | loss: 24.7974625Losses:  17.118118286132812 0.5366580486297607 13.677480697631836
MemoryTrain:  epoch  0, batch    10 | loss: 17.1181183Losses:  21.153867721557617 0.5269067883491516 16.84752655029297
MemoryTrain:  epoch  0, batch    11 | loss: 21.1538677Losses:  29.553503036499023 0.4906059205532074 26.55294418334961
MemoryTrain:  epoch  0, batch    12 | loss: 29.5535030Losses:  10.791276931762695 0.5394505858421326 5.567200660705566
MemoryTrain:  epoch  0, batch    13 | loss: 10.7912769Losses:  26.262527465820312 0.5114894509315491 23.094932556152344
MemoryTrain:  epoch  1, batch     0 | loss: 26.2625275Losses:  20.72159767150879 0.479265421628952 16.903791427612305
MemoryTrain:  epoch  1, batch     1 | loss: 20.7215977Losses:  25.64537811279297 2.5275914669036865 20.00063133239746
MemoryTrain:  epoch  1, batch     2 | loss: 25.6453781Losses:  17.358335494995117 0.25965583324432373 13.81678581237793
MemoryTrain:  epoch  1, batch     3 | loss: 17.3583355Losses:  23.869054794311523 1.0783324241638184 19.89569664001465
MemoryTrain:  epoch  1, batch     4 | loss: 23.8690548Losses:  20.381200790405273 1.1648290157318115 16.700246810913086
MemoryTrain:  epoch  1, batch     5 | loss: 20.3812008Losses:  23.022451400756836 0.29100465774536133 19.893157958984375
MemoryTrain:  epoch  1, batch     6 | loss: 23.0224514Losses:  20.587799072265625 1.3478446006774902 16.74968719482422
MemoryTrain:  epoch  1, batch     7 | loss: 20.5877991Losses:  24.190715789794922 1.5761513710021973 19.879653930664062
MemoryTrain:  epoch  1, batch     8 | loss: 24.1907158Losses:  20.463903427124023 0.8757990598678589 16.723562240600586
MemoryTrain:  epoch  1, batch     9 | loss: 20.4639034Losses:  28.531822204589844 -0.0 26.470367431640625
MemoryTrain:  epoch  1, batch    10 | loss: 28.5318222Losses:  23.239784240722656 0.25011372566223145 19.91803550720215
MemoryTrain:  epoch  1, batch    11 | loss: 23.2397842Losses:  23.760032653808594 1.0871142148971558 19.893917083740234
MemoryTrain:  epoch  1, batch    12 | loss: 23.7600327Losses:  13.088080406188965 0.30547308921813965 10.819528579711914
MemoryTrain:  epoch  1, batch    13 | loss: 13.0880804Losses:  18.055904388427734 1.376095175743103 13.715066909790039
MemoryTrain:  epoch  2, batch     0 | loss: 18.0559044Losses:  23.127216339111328 0.9987248182296753 19.85035514831543
MemoryTrain:  epoch  2, batch     1 | loss: 23.1272163Losses:  15.635143280029297 1.6972349882125854 10.812772750854492
MemoryTrain:  epoch  2, batch     2 | loss: 15.6351433Losses:  22.94516372680664 1.0674513578414917 19.819334030151367
MemoryTrain:  epoch  2, batch     3 | loss: 22.9451637Losses:  20.001235961914062 2.5315749645233154 13.720868110656738
MemoryTrain:  epoch  2, batch     4 | loss: 20.0012360Losses:  22.132503509521484 -0.0 19.8027400970459
MemoryTrain:  epoch  2, batch     5 | loss: 22.1325035Losses:  23.117366790771484 0.8122414946556091 19.89802360534668
MemoryTrain:  epoch  2, batch     6 | loss: 23.1173668Losses:  26.94294548034668 1.012052059173584 23.143022537231445
MemoryTrain:  epoch  2, batch     7 | loss: 26.9429455Losses:  22.981021881103516 1.1118366718292236 19.8712158203125
MemoryTrain:  epoch  2, batch     8 | loss: 22.9810219Losses:  16.139907836914062 0.5259184837341309 13.658493041992188
MemoryTrain:  epoch  2, batch     9 | loss: 16.1399078Losses:  26.1004695892334 0.2411571592092514 23.13349151611328
MemoryTrain:  epoch  2, batch    10 | loss: 26.1004696Losses:  22.950660705566406 0.6905893087387085 19.884119033813477
MemoryTrain:  epoch  2, batch    11 | loss: 22.9506607Losses:  20.354774475097656 1.0707106590270996 16.728601455688477
MemoryTrain:  epoch  2, batch    12 | loss: 20.3547745Losses:  8.958590507507324 -0.0 5.591517925262451
MemoryTrain:  epoch  2, batch    13 | loss: 8.9585905Losses:  26.067302703857422 0.5096750855445862 23.150598526000977
MemoryTrain:  epoch  3, batch     0 | loss: 26.0673027Losses:  16.806236267089844 1.0023133754730225 13.651275634765625
MemoryTrain:  epoch  3, batch     1 | loss: 16.8062363Losses:  19.56012535095215 0.9662100076675415 16.676612854003906
MemoryTrain:  epoch  3, batch     2 | loss: 19.5601254Losses:  20.017515182495117 0.5263615846633911 16.76132583618164
MemoryTrain:  epoch  3, batch     3 | loss: 20.0175152Losses:  25.916343688964844 0.2607678472995758 23.101974487304688
MemoryTrain:  epoch  3, batch     4 | loss: 25.9163437Losses:  19.441360473632812 0.49851030111312866 16.72818946838379
MemoryTrain:  epoch  3, batch     5 | loss: 19.4413605Losses:  26.457111358642578 0.5166466236114502 23.138408660888672
MemoryTrain:  epoch  3, batch     6 | loss: 26.4571114Losses:  19.223163604736328 0.5151962041854858 16.710237503051758
MemoryTrain:  epoch  3, batch     7 | loss: 19.2231636Losses:  17.09079933166504 1.2382639646530151 13.68302059173584
MemoryTrain:  epoch  3, batch     8 | loss: 17.0907993Losses:  23.156837463378906 0.5480554103851318 19.896787643432617
MemoryTrain:  epoch  3, batch     9 | loss: 23.1568375Losses:  29.93374252319336 1.0505096912384033 26.488361358642578
MemoryTrain:  epoch  3, batch    10 | loss: 29.9337425Losses:  25.897930145263672 0.32491350173950195 23.110733032226562
MemoryTrain:  epoch  3, batch    11 | loss: 25.8979301Losses:  22.922771453857422 0.5049505829811096 19.845617294311523
MemoryTrain:  epoch  3, batch    12 | loss: 22.9227715Losses:  9.099366188049316 0.6538678407669067 5.594322204589844
MemoryTrain:  epoch  3, batch    13 | loss: 9.0993662Losses:  29.387060165405273 0.7388420104980469 26.543407440185547
MemoryTrain:  epoch  4, batch     0 | loss: 29.3870602Losses:  26.267568588256836 0.7969136238098145 23.122756958007812
MemoryTrain:  epoch  4, batch     1 | loss: 26.2675686Losses:  22.56418228149414 0.5233502388000488 19.851764678955078
MemoryTrain:  epoch  4, batch     2 | loss: 22.5641823Losses:  22.07842254638672 0.24636460840702057 19.84101676940918
MemoryTrain:  epoch  4, batch     3 | loss: 22.0784225Losses:  29.113168716430664 0.5054306387901306 26.558429718017578
MemoryTrain:  epoch  4, batch     4 | loss: 29.1131687Losses:  19.4827880859375 0.535365104675293 16.70083999633789
MemoryTrain:  epoch  4, batch     5 | loss: 19.4827881Losses:  20.347978591918945 1.323720097541809 16.756237030029297
MemoryTrain:  epoch  4, batch     6 | loss: 20.3479786Losses:  20.251605987548828 0.7387815117835999 16.680187225341797
MemoryTrain:  epoch  4, batch     7 | loss: 20.2516060Losses:  19.441112518310547 0.4772769808769226 16.71720314025879
MemoryTrain:  epoch  4, batch     8 | loss: 19.4411125Losses:  22.897703170776367 1.0254336595535278 19.891508102416992
MemoryTrain:  epoch  4, batch     9 | loss: 22.8977032Losses:  19.719593048095703 0.9974777698516846 16.666730880737305
MemoryTrain:  epoch  4, batch    10 | loss: 19.7195930Losses:  22.33194923400879 0.5067138671875 19.86145782470703
MemoryTrain:  epoch  4, batch    11 | loss: 22.3319492Losses:  32.80765914916992 0.7297345995903015 29.95768928527832
MemoryTrain:  epoch  4, batch    12 | loss: 32.8076591Losses:  14.688929557800293 0.6410171985626221 10.823612213134766
MemoryTrain:  epoch  4, batch    13 | loss: 14.6889296Losses:  25.843406677246094 0.2398252934217453 23.1490478515625
MemoryTrain:  epoch  5, batch     0 | loss: 25.8434067Losses:  14.04570198059082 1.0383188724517822 10.82667064666748
MemoryTrain:  epoch  5, batch     1 | loss: 14.0457020Losses:  19.537145614624023 0.75173419713974 16.713632583618164
MemoryTrain:  epoch  5, batch     2 | loss: 19.5371456Losses:  25.810514450073242 0.7347907423973083 23.113407135009766
MemoryTrain:  epoch  5, batch     3 | loss: 25.8105145Losses:  22.634387969970703 0.7321938276290894 19.9088077545166
MemoryTrain:  epoch  5, batch     4 | loss: 22.6343880Losses:  22.446043014526367 0.5394169688224792 19.85256004333496
MemoryTrain:  epoch  5, batch     5 | loss: 22.4460430Losses:  22.024076461791992 0.25644201040267944 19.85106086730957
MemoryTrain:  epoch  5, batch     6 | loss: 22.0240765Losses:  22.58144760131836 0.4814613163471222 19.885286331176758
MemoryTrain:  epoch  5, batch     7 | loss: 22.5814476Losses:  26.230762481689453 0.7547307014465332 23.15416145324707
MemoryTrain:  epoch  5, batch     8 | loss: 26.2307625Losses:  23.13715171813965 0.8559425473213196 19.87137222290039
MemoryTrain:  epoch  5, batch     9 | loss: 23.1371517Losses:  22.86434555053711 1.0881142616271973 19.847021102905273
MemoryTrain:  epoch  5, batch    10 | loss: 22.8643456Losses:  22.842044830322266 0.8610972762107849 19.822843551635742
MemoryTrain:  epoch  5, batch    11 | loss: 22.8420448Losses:  25.863021850585938 0.49132075905799866 23.090845108032227
MemoryTrain:  epoch  5, batch    12 | loss: 25.8630219Losses:  7.932696342468262 0.3034789562225342 5.5621747970581055
MemoryTrain:  epoch  5, batch    13 | loss: 7.9326963Losses:  26.03699493408203 0.724927544593811 23.131576538085938
MemoryTrain:  epoch  6, batch     0 | loss: 26.0369949Losses:  13.980708122253418 1.0329880714416504 10.796026229858398
MemoryTrain:  epoch  6, batch     1 | loss: 13.9807081Losses:  20.06330108642578 1.0354193449020386 16.69849395751953
MemoryTrain:  epoch  6, batch     2 | loss: 20.0633011Losses:  11.333223342895508 1.318701982498169 8.069168090820312
MemoryTrain:  epoch  6, batch     3 | loss: 11.3332233Losses:  25.921524047851562 0.7349951863288879 23.121252059936523
MemoryTrain:  epoch  6, batch     4 | loss: 25.9215240Losses:  28.978483200073242 0.4687090516090393 26.508514404296875
MemoryTrain:  epoch  6, batch     5 | loss: 28.9784832Losses:  13.889392852783203 1.1573255062103271 10.788435935974121
MemoryTrain:  epoch  6, batch     6 | loss: 13.8893929Losses:  29.726818084716797 1.0721689462661743 26.480443954467773
MemoryTrain:  epoch  6, batch     7 | loss: 29.7268181Losses:  29.04717254638672 0.2693423926830292 26.46944236755371
MemoryTrain:  epoch  6, batch     8 | loss: 29.0471725Losses:  20.081375122070312 1.3440388441085815 16.684768676757812
MemoryTrain:  epoch  6, batch     9 | loss: 20.0813751Losses:  32.11329650878906 0.23566731810569763 29.92530059814453
MemoryTrain:  epoch  6, batch    10 | loss: 32.1132965Losses:  23.038421630859375 1.270232915878296 19.884763717651367
MemoryTrain:  epoch  6, batch    11 | loss: 23.0384216Losses:  22.285442352294922 0.5038890838623047 19.83167839050293
MemoryTrain:  epoch  6, batch    12 | loss: 22.2854424Losses:  10.900882720947266 0.5642022490501404 8.071271896362305
MemoryTrain:  epoch  6, batch    13 | loss: 10.9008827Losses:  16.238853454589844 0.46859484910964966 13.686891555786133
MemoryTrain:  epoch  7, batch     0 | loss: 16.2388535Losses:  19.443029403686523 0.5064613223075867 16.79541015625
MemoryTrain:  epoch  7, batch     1 | loss: 19.4430294Losses:  20.369115829467773 1.4170398712158203 16.701080322265625
MemoryTrain:  epoch  7, batch     2 | loss: 20.3691158Losses:  19.638744354248047 1.0831042528152466 16.67137336730957
MemoryTrain:  epoch  7, batch     3 | loss: 19.6387444Losses:  19.13862419128418 0.49808189272880554 16.72635269165039
MemoryTrain:  epoch  7, batch     4 | loss: 19.1386242Losses:  22.794109344482422 0.9999722242355347 19.804569244384766
MemoryTrain:  epoch  7, batch     5 | loss: 22.7941093Losses:  22.731773376464844 0.7488101720809937 19.83414649963379
MemoryTrain:  epoch  7, batch     6 | loss: 22.7317734Losses:  22.216733932495117 0.4869379997253418 19.82481575012207
MemoryTrain:  epoch  7, batch     7 | loss: 22.2167339Losses:  25.309669494628906 0.25057661533355713 23.108980178833008
MemoryTrain:  epoch  7, batch     8 | loss: 25.3096695Losses:  22.58474349975586 0.7514151334762573 19.883493423461914
MemoryTrain:  epoch  7, batch     9 | loss: 22.5847435Losses:  26.035367965698242 0.75048828125 23.143442153930664
MemoryTrain:  epoch  7, batch    10 | loss: 26.0353680Losses:  19.63614845275879 0.9843137860298157 16.69540023803711
MemoryTrain:  epoch  7, batch    11 | loss: 19.6361485Losses:  25.445228576660156 0.2553366720676422 23.102319717407227
MemoryTrain:  epoch  7, batch    12 | loss: 25.4452286Losses:  13.028980255126953 0.30203014612197876 10.79221248626709
MemoryTrain:  epoch  7, batch    13 | loss: 13.0289803Losses:  32.168968200683594 0.2758324146270752 29.90506362915039
MemoryTrain:  epoch  8, batch     0 | loss: 32.1689682Losses:  22.774532318115234 0.9375011920928955 19.832487106323242
MemoryTrain:  epoch  8, batch     1 | loss: 22.7745323Losses:  18.83393096923828 0.2703959345817566 16.653085708618164
MemoryTrain:  epoch  8, batch     2 | loss: 18.8339310Losses:  20.26019859313965 1.3391156196594238 16.703561782836914
MemoryTrain:  epoch  8, batch     3 | loss: 20.2601986Losses:  25.25852394104004 0.2621232867240906 23.07573127746582
MemoryTrain:  epoch  8, batch     4 | loss: 25.2585239Losses:  25.330368041992188 0.25016117095947266 23.12895393371582
MemoryTrain:  epoch  8, batch     5 | loss: 25.3303680Losses:  25.80423355102539 0.7174651026725769 23.097990036010742
MemoryTrain:  epoch  8, batch     6 | loss: 25.8042336Losses:  25.832670211791992 0.5362643003463745 23.150230407714844
MemoryTrain:  epoch  8, batch     7 | loss: 25.8326702Losses:  13.422381401062012 0.7430059313774109 10.763925552368164
MemoryTrain:  epoch  8, batch     8 | loss: 13.4223814Losses:  22.823909759521484 0.634837806224823 19.836673736572266
MemoryTrain:  epoch  8, batch     9 | loss: 22.8239098Losses:  25.898632049560547 0.7297436594963074 23.128509521484375
MemoryTrain:  epoch  8, batch    10 | loss: 25.8986320Losses:  22.232398986816406 0.4835256040096283 19.83677101135254
MemoryTrain:  epoch  8, batch    11 | loss: 22.2323990Losses:  22.344072341918945 0.4781777262687683 19.84083366394043
MemoryTrain:  epoch  8, batch    12 | loss: 22.3440723Losses:  12.745665550231934 -0.0 10.859865188598633
MemoryTrain:  epoch  8, batch    13 | loss: 12.7456656Losses:  19.134624481201172 0.5137818455696106 16.67276954650879
MemoryTrain:  epoch  9, batch     0 | loss: 19.1346245Losses:  16.832935333251953 1.2837315797805786 13.677972793579102
MemoryTrain:  epoch  9, batch     1 | loss: 16.8329353Losses:  12.502762794494629 2.404381275177002 8.080037117004395
MemoryTrain:  epoch  9, batch     2 | loss: 12.5027628Losses:  16.756019592285156 1.2045605182647705 13.644357681274414
MemoryTrain:  epoch  9, batch     3 | loss: 16.7560196Losses:  26.04473114013672 1.0222415924072266 23.11097526550293
MemoryTrain:  epoch  9, batch     4 | loss: 26.0447311Losses:  35.82402801513672 0.4984540343284607 33.43312072753906
MemoryTrain:  epoch  9, batch     5 | loss: 35.8240280Losses:  26.535188674926758 1.376807451248169 23.15795135498047
MemoryTrain:  epoch  9, batch     6 | loss: 26.5351887Losses:  28.535377502441406 -0.0 26.498409271240234
MemoryTrain:  epoch  9, batch     7 | loss: 28.5353775Losses:  13.798614501953125 1.1031427383422852 10.782092094421387
MemoryTrain:  epoch  9, batch     8 | loss: 13.7986145Losses:  19.182580947875977 0.48903825879096985 16.6751651763916
MemoryTrain:  epoch  9, batch     9 | loss: 19.1825809Losses:  20.222225189208984 1.3122222423553467 16.694955825805664
MemoryTrain:  epoch  9, batch    10 | loss: 20.2222252Losses:  16.50208282470703 0.9798533916473389 13.639934539794922
MemoryTrain:  epoch  9, batch    11 | loss: 16.5020828Losses:  28.754648208618164 0.23390266299247742 26.472972869873047
MemoryTrain:  epoch  9, batch    12 | loss: 28.7546482Losses:  10.218976974487305 -0.0 8.116617202758789
MemoryTrain:  epoch  9, batch    13 | loss: 10.2189770
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 6.25%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 65.87%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 58.75%   
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 45.31%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 48.96%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 55.47%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 59.03%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 60.58%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 59.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 58.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 59.93%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 60.07%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 62.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 65.91%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 67.12%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.63%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 73.19%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 73.83%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 74.05%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 72.61%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 71.79%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 70.49%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 69.43%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 68.42%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 67.79%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 67.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 69.77%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 70.03%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 70.28%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 69.29%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 69.28%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 69.13%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 68.38%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 67.77%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 67.19%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 66.39%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 66.78%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 67.16%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 67.52%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 67.24%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 66.84%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 66.29%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 65.32%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 64.48%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 64.26%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 64.13%   [EVAL] batch:   65 | acc: 43.75%,  total acc: 63.83%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 63.06%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 62.13%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 61.59%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 61.43%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 61.36%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 61.20%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 60.70%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 60.14%   [EVAL] batch:   74 | acc: 18.75%,  total acc: 59.58%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 59.13%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 58.44%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 57.85%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 57.36%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 56.64%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 56.02%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 55.49%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 54.82%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 54.46%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 54.78%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 55.16%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 55.39%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 55.82%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 56.04%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 56.39%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 56.87%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 57.34%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 57.80%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 58.24%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 58.68%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 58.92%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 58.83%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 58.99%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 59.34%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 59.53%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 59.44%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 59.53%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 59.62%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 59.82%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 60.02%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 60.34%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 61.07%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 61.36%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 61.49%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 61.16%   [EVAL] batch:  112 | acc: 12.50%,  total acc: 60.73%   [EVAL] batch:  113 | acc: 0.00%,  total acc: 60.20%   [EVAL] batch:  114 | acc: 18.75%,  total acc: 59.84%   [EVAL] batch:  115 | acc: 6.25%,  total acc: 59.38%   
cur_acc:  ['0.8333', '0.8616', '0.3828', '0.6202', '0.3267', '0.8705', '0.5875']
his_acc:  ['0.8333', '0.8484', '0.7662', '0.6809', '0.5746', '0.6040', '0.5938']
Clustering into  19  clusters
Clusters:  [ 4  1  0 11 17 10  7  4 16  1 18  4 13 12  2  1  3  6 10 14 18  0  0  8
 10 16  2  7  4  0  6  4  3  9  8 16  4 15 18 16  5]
Losses:  19.590627670288086 8.69266128540039 5.793328762054443
CurrentTrain: epoch  0, batch     0 | loss: 19.5906277Losses:  10.972029685974121 3.291548252105713 3.325422763824463
CurrentTrain: epoch  0, batch     1 | loss: 10.9720297Losses:  16.427288055419922 6.617453575134277 5.7158894538879395
CurrentTrain: epoch  1, batch     0 | loss: 16.4272881Losses:  10.452875137329102 1.5591298341751099 5.6365251541137695
CurrentTrain: epoch  1, batch     1 | loss: 10.4528751Losses:  17.868446350097656 8.520258903503418 5.685447692871094
CurrentTrain: epoch  2, batch     0 | loss: 17.8684464Losses:  9.273455619812012 3.5260112285614014 3.324815273284912
CurrentTrain: epoch  2, batch     1 | loss: 9.2734556Losses:  15.597900390625 7.074535369873047 5.630167484283447
CurrentTrain: epoch  3, batch     0 | loss: 15.5979004Losses:  8.264348983764648 2.5391488075256348 3.3215994834899902
CurrentTrain: epoch  3, batch     1 | loss: 8.2643490Losses:  14.42033576965332 6.514654159545898 5.569283962249756
CurrentTrain: epoch  4, batch     0 | loss: 14.4203358Losses:  8.468135833740234 2.410038948059082 3.313262701034546
CurrentTrain: epoch  4, batch     1 | loss: 8.4681358Losses:  15.142547607421875 7.197573661804199 5.597387790679932
CurrentTrain: epoch  5, batch     0 | loss: 15.1425476Losses:  9.674615859985352 3.68861985206604 3.3440980911254883
CurrentTrain: epoch  5, batch     1 | loss: 9.6746159Losses:  14.31464672088623 6.22198486328125 5.567658424377441
CurrentTrain: epoch  6, batch     0 | loss: 14.3146467Losses:  9.167631149291992 1.7423533201217651 5.575916290283203
CurrentTrain: epoch  6, batch     1 | loss: 9.1676311Losses:  13.53030776977539 5.88700532913208 5.5683274269104
CurrentTrain: epoch  7, batch     0 | loss: 13.5303078Losses:  9.64968490600586 1.8787599802017212 5.580890655517578
CurrentTrain: epoch  7, batch     1 | loss: 9.6496849Losses:  14.421234130859375 6.802050590515137 5.5645928382873535
CurrentTrain: epoch  8, batch     0 | loss: 14.4212341Losses:  7.868555068969727 2.641145706176758 3.3010172843933105
CurrentTrain: epoch  8, batch     1 | loss: 7.8685551Losses:  14.454959869384766 6.8686370849609375 5.572897434234619
CurrentTrain: epoch  9, batch     0 | loss: 14.4549599Losses:  7.908071517944336 2.5747811794281006 3.3088014125823975
CurrentTrain: epoch  9, batch     1 | loss: 7.9080715
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: tv-idol-johns -- atlanta -- `` american idol '' finalist michael johns moved to los angeles several years ago , but his heart is still in atlanta .
Head Entity: his
Tail Entity: atlanta
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: after years of living in new york city, the famous author decided to return to her hometown of boston, where she feels most at home.  
Head Entity: she  
Tail Entity: boston  

Relation: person cities of residence  
Context: despite being born in chicago, the musician has spent most of his adult life in nashville, where he finds inspiration for his songs.  
Head Entity: he  
Tail Entity: nashville  

Relation: person cities of residence  
Context: the tech entrepreneur moved from san francisco to a quieter life in a small town in oregon, seeking peace and creativity.  
Head Entity: he  
Tail Entity: oregon  

Relation: person cities of residence  
Context: after graduating from college in los angeles, the actress decided to settle down in san diego, where she enjoys the beach lifestyle.  
Head Entity: she  
Tail Entity: san diego  

Relation: person cities of residence  
Context: although he grew up in miami, the professional athlete now resides in houston, where he trains and plays for his team.  
Head Entity: he  
Tail Entity: houston  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, John enrolled at Stanford University to study computer science.  
Head Entity: John  
Tail Entity: Stanford University  

Relation: person schools attended  
Context: Maria attended the University of Oxford for her undergraduate studies in literature, which greatly influenced her writing style.  
Head Entity: Maria  
Tail Entity: University of Oxford  

Relation: person schools attended  
Context: Following his passion for engineering, David went to the Massachusetts Institute of Technology, where he earned his master's degree.  
Head Entity: David  
Tail Entity: Massachusetts Institute of Technology  

Relation: person schools attended  
Context: During her youth, Emily spent four years at the University of California, Berkeley, where she majored in environmental science.  
Head Entity: Emily  
Tail Entity: University of California, Berkeley  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: the famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  

Relation: person country of death  
Context: legendary musician prince was found dead in his home in minnesota, united states  
Head Entity: prince  
Tail Entity: united states  

Relation: person country of death  
Context: former south african president nelson mandela passed away in johannesburg, south africa at the age of 95  
Head Entity: nelson mandela  
Tail Entity: south africa  

Relation: person country of death  
Context: iconic actress audrey hepburn died in tolochenaz, switzerland, leaving behind a timeless film legacy  
Head Entity: audrey hepburn  
Tail Entity: switzerland  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by his wife of 63 years , josephine robinson mcnair , of columbia ; a son , robert e. jr. , of columbia ; three daughters , robin lee howell and corinne godshall , of myrtle beach , s.c. , and claudia crawford mcnair , of jamestown , s.c. ; six grandchildren ; and one great-grandchild .
Head Entity: he
Tail Entity: claudia crawford mcnair
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: After the passing of her husband, she dedicated her life to raising their three children, including her youngest daughter, Emily, who is now a successful artist.  
Head Entity: she  
Tail Entity: Emily  

Relation: person children  
Context: The famous actor often shares stories about his childhood and his two sons, who have both followed in his footsteps in the entertainment industry.  
Head Entity: The famous actor  
Tail Entity: his two sons  

Relation: person children  
Context: During the family reunion, he proudly introduced his grandchildren, especially his eldest granddaughter, who just graduated from college.  
Head Entity: he  
Tail Entity: his eldest granddaughter  

Relation: person children  
Context: She often reminisces about her late father and how he influenced her career, along with her brother, who is now a renowned chef.  
Head Entity: she  
Tail Entity: her brother  

Relation: person children  
Context: The philanthropist is known for her charitable work, but she always emphasizes the importance of her two daughters, who are actively involved in her foundation.  
Head Entity: The philanthropist  
Tail Entity: her two daughters  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the former mayor was charged with corruption after a lengthy investigation.  
Head Entity: former mayor  
Tail Entity: corruption  

Relation: person charges  
Context: After the incident, the police confirmed that the suspect was charged with assault and battery.  
Head Entity: suspect  
Tail Entity: assault and battery  

Relation: person charges  
Context: The district attorney revealed that the celebrity was charged with DUI following the car accident.  
Head Entity: celebrity  
Tail Entity: DUI  

Relation: person charges  
Context: Following the investigation, the authorities stated that the accountant was charged with fraud.  
Head Entity: accountant  
Tail Entity: fraud  

Relation: person charges  
Context: The judge announced that the activist was charged with trespassing during the protest.  
Head Entity: activist  
Tail Entity: trespassing  
Losses:  14.610426902770996 0.46837326884269714 10.821626663208008
MemoryTrain:  epoch  0, batch     0 | loss: 14.6104269Losses:  24.685649871826172 0.9602609872817993 19.871679306030273
MemoryTrain:  epoch  0, batch     1 | loss: 24.6856499Losses:  25.96316146850586 1.1710708141326904 20.083820343017578
MemoryTrain:  epoch  0, batch     2 | loss: 25.9631615Losses:  18.274017333984375 0.8621221780776978 13.70510482788086
MemoryTrain:  epoch  0, batch     3 | loss: 18.2740173Losses:  30.13431167602539 0.5052129030227661 26.456823348999023
MemoryTrain:  epoch  0, batch     4 | loss: 30.1343117Losses:  28.23700523376465 1.1011979579925537 23.362552642822266
MemoryTrain:  epoch  0, batch     5 | loss: 28.2370052Losses:  26.801464080810547 0.5150532126426697 23.097841262817383
MemoryTrain:  epoch  0, batch     6 | loss: 26.8014641Losses:  30.28299331665039 0.2606440484523773 26.450096130371094
MemoryTrain:  epoch  0, batch     7 | loss: 30.2829933Losses:  24.128250122070312 0.8103904724121094 19.8566837310791
MemoryTrain:  epoch  0, batch     8 | loss: 24.1282501Losses:  24.538005828857422 1.3504986763000488 20.07333755493164
MemoryTrain:  epoch  0, batch     9 | loss: 24.5380058Losses:  20.38656997680664 0.26017001271247864 16.741609573364258
MemoryTrain:  epoch  0, batch    10 | loss: 20.3865700Losses:  33.845497131347656 0.5292865037918091 29.959022521972656
MemoryTrain:  epoch  0, batch    11 | loss: 33.8454971Losses:  23.755176544189453 0.5067601203918457 19.82785415649414
MemoryTrain:  epoch  0, batch    12 | loss: 23.7551765Losses:  36.15135955810547 0.24793504178524017 33.41381072998047
MemoryTrain:  epoch  0, batch    13 | loss: 36.1513596Losses:  30.609844207763672 0.777096152305603 26.46523666381836
MemoryTrain:  epoch  0, batch    14 | loss: 30.6098442Losses:  9.529428482055664 -0.0 5.567545413970947
MemoryTrain:  epoch  0, batch    15 | loss: 9.5294285Losses:  26.55434799194336 0.7737352848052979 23.159543991088867
MemoryTrain:  epoch  1, batch     0 | loss: 26.5543480Losses:  24.211013793945312 1.676515817642212 19.983057022094727
MemoryTrain:  epoch  1, batch     1 | loss: 24.2110138Losses:  26.01102066040039 0.25226670503616333 23.096574783325195
MemoryTrain:  epoch  1, batch     2 | loss: 26.0110207Losses:  27.04688262939453 0.7314251661300659 23.086389541625977
MemoryTrain:  epoch  1, batch     3 | loss: 27.0468826Losses:  24.205350875854492 0.5568868517875671 19.96088218688965
MemoryTrain:  epoch  1, batch     4 | loss: 24.2053509Losses:  23.668537139892578 1.712952971458435 19.870161056518555
MemoryTrain:  epoch  1, batch     5 | loss: 23.6685371Losses:  22.958890914916992 0.24389871954917908 19.817188262939453
MemoryTrain:  epoch  1, batch     6 | loss: 22.9588909Losses:  20.914093017578125 1.0806522369384766 16.743783950805664
MemoryTrain:  epoch  1, batch     7 | loss: 20.9140930Losses:  22.705039978027344 0.24710991978645325 19.88406753540039
MemoryTrain:  epoch  1, batch     8 | loss: 22.7050400Losses:  20.146663665771484 0.5569979548454285 16.764142990112305
MemoryTrain:  epoch  1, batch     9 | loss: 20.1466637Losses:  26.643817901611328 1.341688632965088 23.167709350585938
MemoryTrain:  epoch  1, batch    10 | loss: 26.6438179Losses:  21.06407356262207 1.0461591482162476 16.698884963989258
MemoryTrain:  epoch  1, batch    11 | loss: 21.0640736Losses:  17.00032615661621 0.47022828459739685 13.671074867248535
MemoryTrain:  epoch  1, batch    12 | loss: 17.0003262Losses:  21.5323429107666 0.8598390817642212 16.676254272460938
MemoryTrain:  epoch  1, batch    13 | loss: 21.5323429Losses:  29.05891227722168 -0.0 26.483970642089844
MemoryTrain:  epoch  1, batch    14 | loss: 29.0589123Losses:  7.4224348068237305 -0.0 5.560750484466553
MemoryTrain:  epoch  1, batch    15 | loss: 7.4224348Losses:  26.251869201660156 0.29460614919662476 23.095569610595703
MemoryTrain:  epoch  2, batch     0 | loss: 26.2518692Losses:  26.04513931274414 0.23485033214092255 23.116682052612305
MemoryTrain:  epoch  2, batch     1 | loss: 26.0451393Losses:  19.43783950805664 0.23860788345336914 16.681730270385742
MemoryTrain:  epoch  2, batch     2 | loss: 19.4378395Losses:  25.757524490356445 0.47802799940109253 23.081968307495117
MemoryTrain:  epoch  2, batch     3 | loss: 25.7575245Losses:  19.854406356811523 0.7658550143241882 16.696529388427734
MemoryTrain:  epoch  2, batch     4 | loss: 19.8544064Losses:  28.946489334106445 0.251459002494812 26.50055503845215
MemoryTrain:  epoch  2, batch     5 | loss: 28.9464893Losses:  25.910776138305664 0.47759106755256653 23.10824966430664
MemoryTrain:  epoch  2, batch     6 | loss: 25.9107761Losses:  25.440656661987305 -0.0 23.112159729003906
MemoryTrain:  epoch  2, batch     7 | loss: 25.4406567Losses:  32.352638244628906 -0.0 29.911027908325195
MemoryTrain:  epoch  2, batch     8 | loss: 32.3526382Losses:  29.268627166748047 0.5487467050552368 26.51678466796875
MemoryTrain:  epoch  2, batch     9 | loss: 29.2686272Losses:  32.14897918701172 0.2411017268896103 29.882726669311523
MemoryTrain:  epoch  2, batch    10 | loss: 32.1489792Losses:  19.209495544433594 0.23235857486724854 16.681495666503906
MemoryTrain:  epoch  2, batch    11 | loss: 19.2094955Losses:  27.586231231689453 1.3035187721252441 23.127458572387695
MemoryTrain:  epoch  2, batch    12 | loss: 27.5862312Losses:  19.969425201416016 1.1193437576293945 16.66568374633789
MemoryTrain:  epoch  2, batch    13 | loss: 19.9694252Losses:  19.750492095947266 0.5053675770759583 16.6992130279541
MemoryTrain:  epoch  2, batch    14 | loss: 19.7504921Losses:  7.587586402893066 -0.0 5.600839614868164
MemoryTrain:  epoch  2, batch    15 | loss: 7.5875864Losses:  22.923179626464844 1.2012999057769775 19.804414749145508
MemoryTrain:  epoch  3, batch     0 | loss: 22.9231796Losses:  29.263622283935547 0.7487738132476807 26.508108139038086
MemoryTrain:  epoch  3, batch     1 | loss: 29.2636223Losses:  17.070222854614258 0.5231051445007324 13.666154861450195
MemoryTrain:  epoch  3, batch     2 | loss: 17.0702229Losses:  25.668909072875977 0.24741464853286743 23.143102645874023
MemoryTrain:  epoch  3, batch     3 | loss: 25.6689091Losses:  32.363677978515625 0.2510840892791748 29.953594207763672
MemoryTrain:  epoch  3, batch     4 | loss: 32.3636780Losses:  20.05756378173828 0.7375014424324036 16.70620346069336
MemoryTrain:  epoch  3, batch     5 | loss: 20.0575638Losses:  23.058082580566406 0.7587008476257324 19.831464767456055
MemoryTrain:  epoch  3, batch     6 | loss: 23.0580826Losses:  20.934072494506836 1.407914161682129 16.687957763671875
MemoryTrain:  epoch  3, batch     7 | loss: 20.9340725Losses:  17.64870834350586 1.36649489402771 13.635953903198242
MemoryTrain:  epoch  3, batch     8 | loss: 17.6487083Losses:  25.664241790771484 0.5275309681892395 23.127065658569336
MemoryTrain:  epoch  3, batch     9 | loss: 25.6642418Losses:  16.288015365600586 0.4828158915042877 13.656135559082031
MemoryTrain:  epoch  3, batch    10 | loss: 16.2880154Losses:  25.801090240478516 0.7713212966918945 23.10167121887207
MemoryTrain:  epoch  3, batch    11 | loss: 25.8010902Losses:  19.7508544921875 1.1932305097579956 16.66461181640625
MemoryTrain:  epoch  3, batch    12 | loss: 19.7508545Losses:  19.211597442626953 0.25314468145370483 16.701385498046875
MemoryTrain:  epoch  3, batch    13 | loss: 19.2115974Losses:  25.747045516967773 0.5827611684799194 23.09801483154297
MemoryTrain:  epoch  3, batch    14 | loss: 25.7470455Losses:  7.668806552886963 -0.0 5.5599684715271
MemoryTrain:  epoch  3, batch    15 | loss: 7.6688066Losses:  22.553024291992188 0.7214508652687073 19.85943031311035
MemoryTrain:  epoch  4, batch     0 | loss: 22.5530243Losses:  29.3087100982666 0.2431727945804596 26.509740829467773
MemoryTrain:  epoch  4, batch     1 | loss: 29.3087101Losses:  25.6312198638916 0.500037431716919 23.141738891601562
MemoryTrain:  epoch  4, batch     2 | loss: 25.6312199Losses:  22.030546188354492 -0.0 19.809476852416992
MemoryTrain:  epoch  4, batch     3 | loss: 22.0305462Losses:  25.48429298400879 0.23202164471149445 23.1452693939209
MemoryTrain:  epoch  4, batch     4 | loss: 25.4842930Losses:  22.8045597076416 0.9948331713676453 19.838842391967773
MemoryTrain:  epoch  4, batch     5 | loss: 22.8045597Losses:  19.560569763183594 0.7417675256729126 16.74951171875
MemoryTrain:  epoch  4, batch     6 | loss: 19.5605698Losses:  28.712011337280273 0.2532217502593994 26.443553924560547
MemoryTrain:  epoch  4, batch     7 | loss: 28.7120113Losses:  22.63829231262207 0.5279353260993958 19.87828254699707
MemoryTrain:  epoch  4, batch     8 | loss: 22.6382923Losses:  22.40987777709961 0.48648813366889954 19.837970733642578
MemoryTrain:  epoch  4, batch     9 | loss: 22.4098778Losses:  19.640562057495117 0.9634159207344055 16.669048309326172
MemoryTrain:  epoch  4, batch    10 | loss: 19.6405621Losses:  29.142244338989258 0.2396245002746582 26.484233856201172
MemoryTrain:  epoch  4, batch    11 | loss: 29.1422443Losses:  17.546707153320312 1.2853457927703857 13.650508880615234
MemoryTrain:  epoch  4, batch    12 | loss: 17.5467072Losses:  19.04566764831543 0.254256933927536 16.70478630065918
MemoryTrain:  epoch  4, batch    13 | loss: 19.0456676Losses:  17.01190757751465 1.3694062232971191 13.674132347106934
MemoryTrain:  epoch  4, batch    14 | loss: 17.0119076Losses:  7.9385786056518555 0.28248757123947144 5.5795159339904785
MemoryTrain:  epoch  4, batch    15 | loss: 7.9385786Losses:  22.917015075683594 1.0804762840270996 19.830341339111328
MemoryTrain:  epoch  5, batch     0 | loss: 22.9170151Losses:  19.947233200073242 1.1171278953552246 16.679433822631836
MemoryTrain:  epoch  5, batch     1 | loss: 19.9472332Losses:  19.909971237182617 1.0438690185546875 16.696439743041992
MemoryTrain:  epoch  5, batch     2 | loss: 19.9099712Losses:  25.840919494628906 0.48790785670280457 23.09092140197754
MemoryTrain:  epoch  5, batch     3 | loss: 25.8409195Losses:  22.551799774169922 0.7687962055206299 19.893842697143555
MemoryTrain:  epoch  5, batch     4 | loss: 22.5517998Losses:  19.216796875 0.3420385718345642 16.685834884643555
MemoryTrain:  epoch  5, batch     5 | loss: 19.2167969Losses:  25.599645614624023 0.48059892654418945 23.10036849975586
MemoryTrain:  epoch  5, batch     6 | loss: 25.5996456Losses:  25.05805778503418 -0.0 23.11712074279785
MemoryTrain:  epoch  5, batch     7 | loss: 25.0580578Losses:  19.773719787597656 0.7603020668029785 16.684085845947266
MemoryTrain:  epoch  5, batch     8 | loss: 19.7737198Losses:  22.397184371948242 0.506821870803833 19.822763442993164
MemoryTrain:  epoch  5, batch     9 | loss: 22.3971844Losses:  19.55415153503418 0.7252103090286255 16.662504196166992
MemoryTrain:  epoch  5, batch    10 | loss: 19.5541515Losses:  28.67618179321289 0.2354806363582611 26.470108032226562
MemoryTrain:  epoch  5, batch    11 | loss: 28.6761818Losses:  29.290315628051758 0.5289661884307861 26.472118377685547
MemoryTrain:  epoch  5, batch    12 | loss: 29.2903156Losses:  28.643415451049805 0.2377830147743225 26.432823181152344
MemoryTrain:  epoch  5, batch    13 | loss: 28.6434155Losses:  22.285320281982422 0.4921742081642151 19.80780029296875
MemoryTrain:  epoch  5, batch    14 | loss: 22.2853203Losses:  9.990104675292969 -0.0 8.072223663330078
MemoryTrain:  epoch  5, batch    15 | loss: 9.9901047Losses:  25.886028289794922 0.5407817363739014 23.09946632385254
MemoryTrain:  epoch  6, batch     0 | loss: 25.8860283Losses:  25.600643157958984 0.5008931159973145 23.07305335998535
MemoryTrain:  epoch  6, batch     1 | loss: 25.6006432Losses:  32.31108093261719 0.23759013414382935 29.997535705566406
MemoryTrain:  epoch  6, batch     2 | loss: 32.3110809Losses:  16.749807357788086 1.1048532724380493 13.666604042053223
MemoryTrain:  epoch  6, batch     3 | loss: 16.7498074Losses:  25.399728775024414 0.2559710144996643 23.077241897583008
MemoryTrain:  epoch  6, batch     4 | loss: 25.3997288Losses:  22.643651962280273 0.7592024803161621 19.81992530822754
MemoryTrain:  epoch  6, batch     5 | loss: 22.6436520Losses:  22.39349365234375 0.5102855563163757 19.834787368774414
MemoryTrain:  epoch  6, batch     6 | loss: 22.3934937Losses:  16.426664352416992 0.7302341461181641 13.655410766601562
MemoryTrain:  epoch  6, batch     7 | loss: 16.4266644Losses:  25.807506561279297 0.5729091763496399 23.076871871948242
MemoryTrain:  epoch  6, batch     8 | loss: 25.8075066Losses:  22.258834838867188 0.4931178689002991 19.80031394958496
MemoryTrain:  epoch  6, batch     9 | loss: 22.2588348Losses:  31.909507751464844 -0.0 29.913745880126953
MemoryTrain:  epoch  6, batch    10 | loss: 31.9095078Losses:  22.26338005065918 0.5363291501998901 19.811086654663086
MemoryTrain:  epoch  6, batch    11 | loss: 22.2633801Losses:  25.46418571472168 0.4887176752090454 23.064823150634766
MemoryTrain:  epoch  6, batch    12 | loss: 25.4641857Losses:  22.24032211303711 0.4917033910751343 19.83198356628418
MemoryTrain:  epoch  6, batch    13 | loss: 22.2403221Losses:  19.164278030395508 0.47132256627082825 16.664459228515625
MemoryTrain:  epoch  6, batch    14 | loss: 19.1642780Losses:  5.594267845153809 -0.0 3.309236526489258
MemoryTrain:  epoch  6, batch    15 | loss: 5.5942678Losses:  21.946483612060547 0.2450951635837555 19.834333419799805
MemoryTrain:  epoch  7, batch     0 | loss: 21.9464836Losses:  16.61985969543457 0.9590507745742798 13.70008659362793
MemoryTrain:  epoch  7, batch     1 | loss: 16.6198597Losses:  14.988460540771484 2.190459728240967 10.816408157348633
MemoryTrain:  epoch  7, batch     2 | loss: 14.9884605Losses:  23.162734985351562 1.3436164855957031 19.826557159423828
MemoryTrain:  epoch  7, batch     3 | loss: 23.1627350Losses:  28.433515548706055 -0.0 26.481464385986328
MemoryTrain:  epoch  7, batch     4 | loss: 28.4335155Losses:  22.01923942565918 0.24506208300590515 19.82002067565918
MemoryTrain:  epoch  7, batch     5 | loss: 22.0192394Losses:  18.81970977783203 0.27059051394462585 16.66813087463379
MemoryTrain:  epoch  7, batch     6 | loss: 18.8197098Losses:  22.520048141479492 0.7139027714729309 19.8201961517334
MemoryTrain:  epoch  7, batch     7 | loss: 22.5200481Losses:  22.25867462158203 0.49400052428245544 19.830305099487305
MemoryTrain:  epoch  7, batch     8 | loss: 22.2586746Losses:  19.090435028076172 0.4853951930999756 16.690547943115234
MemoryTrain:  epoch  7, batch     9 | loss: 19.0904350Losses:  22.75902557373047 0.9947656393051147 19.840707778930664
MemoryTrain:  epoch  7, batch    10 | loss: 22.7590256Losses:  22.249759674072266 0.5020227432250977 19.848234176635742
MemoryTrain:  epoch  7, batch    11 | loss: 22.2497597Losses:  28.581724166870117 0.2371070683002472 26.405803680419922
MemoryTrain:  epoch  7, batch    12 | loss: 28.5817242Losses:  22.442340850830078 0.721354603767395 19.831417083740234
MemoryTrain:  epoch  7, batch    13 | loss: 22.4423409Losses:  19.24983787536621 0.5337726473808289 16.695125579833984
MemoryTrain:  epoch  7, batch    14 | loss: 19.2498379Losses:  10.012707710266113 -0.0 8.085022926330566
MemoryTrain:  epoch  7, batch    15 | loss: 10.0127077Losses:  17.21107292175293 1.446944236755371 13.681108474731445
MemoryTrain:  epoch  8, batch     0 | loss: 17.2110729Losses:  22.05055809020996 0.26639753580093384 19.84146499633789
MemoryTrain:  epoch  8, batch     1 | loss: 22.0505581Losses:  25.3008975982666 0.2408296763896942 23.08745574951172
MemoryTrain:  epoch  8, batch     2 | loss: 25.3008976Losses:  19.926515579223633 1.327096939086914 16.65702247619629
MemoryTrain:  epoch  8, batch     3 | loss: 19.9265156Losses:  22.500703811645508 0.7480378150939941 19.835044860839844
MemoryTrain:  epoch  8, batch     4 | loss: 22.5007038Losses:  22.993852615356445 1.1563807725906372 19.854707717895508
MemoryTrain:  epoch  8, batch     5 | loss: 22.9938526Losses:  19.72116470336914 1.0353741645812988 16.72785186767578
MemoryTrain:  epoch  8, batch     6 | loss: 19.7211647Losses:  32.25398635864258 0.4985381066799164 29.878189086914062
MemoryTrain:  epoch  8, batch     7 | loss: 32.2539864Losses:  19.083410263061523 0.4886879622936249 16.693546295166016
MemoryTrain:  epoch  8, batch     8 | loss: 19.0834103Losses:  19.090967178344727 0.5010254383087158 16.676795959472656
MemoryTrain:  epoch  8, batch     9 | loss: 19.0909672Losses:  28.880531311035156 0.4916262924671173 26.43767547607422
MemoryTrain:  epoch  8, batch    10 | loss: 28.8805313Losses:  22.199979782104492 0.47511714696884155 19.816425323486328
MemoryTrain:  epoch  8, batch    11 | loss: 22.1999798Losses:  16.296916961669922 0.7333975434303284 13.63778018951416
MemoryTrain:  epoch  8, batch    12 | loss: 16.2969170Losses:  28.54414939880371 0.23550230264663696 26.439725875854492
MemoryTrain:  epoch  8, batch    13 | loss: 28.5441494Losses:  28.824758529663086 0.2736557722091675 26.46718978881836
MemoryTrain:  epoch  8, batch    14 | loss: 28.8247585Losses:  7.576464653015137 -0.0 5.566173553466797
MemoryTrain:  epoch  8, batch    15 | loss: 7.5764647Losses:  28.62505340576172 0.24270474910736084 26.45757293701172
MemoryTrain:  epoch  9, batch     0 | loss: 28.6250534Losses:  28.693492889404297 0.2357327789068222 26.50282859802246
MemoryTrain:  epoch  9, batch     1 | loss: 28.6934929Losses:  22.81983757019043 1.1022454500198364 19.814319610595703
MemoryTrain:  epoch  9, batch     2 | loss: 22.8198376Losses:  29.21173095703125 0.7831753492355347 26.468036651611328
MemoryTrain:  epoch  9, batch     3 | loss: 29.2117310Losses:  22.201557159423828 0.4765285849571228 19.802087783813477
MemoryTrain:  epoch  9, batch     4 | loss: 22.2015572Losses:  22.247108459472656 0.4853644371032715 19.83199119567871
MemoryTrain:  epoch  9, batch     5 | loss: 22.2471085Losses:  22.49188804626465 0.7073792219161987 19.841005325317383
MemoryTrain:  epoch  9, batch     6 | loss: 22.4918880Losses:  28.860063552856445 0.49002858996391296 26.439605712890625
MemoryTrain:  epoch  9, batch     7 | loss: 28.8600636Losses:  28.84077262878418 0.48771053552627563 26.46560287475586
MemoryTrain:  epoch  9, batch     8 | loss: 28.8407726Losses:  16.509578704833984 0.9609246253967285 13.65544319152832
MemoryTrain:  epoch  9, batch     9 | loss: 16.5095787Losses:  19.290363311767578 0.7045491933822632 16.677204132080078
MemoryTrain:  epoch  9, batch    10 | loss: 19.2903633Losses:  22.274492263793945 0.49657556414604187 19.841211318969727
MemoryTrain:  epoch  9, batch    11 | loss: 22.2744923Losses:  28.60383415222168 0.24800077080726624 26.443016052246094
MemoryTrain:  epoch  9, batch    12 | loss: 28.6038342Losses:  25.460220336914062 0.46367210149765015 23.079978942871094
MemoryTrain:  epoch  9, batch    13 | loss: 25.4602203Losses:  16.253009796142578 0.711956262588501 13.638195037841797
MemoryTrain:  epoch  9, batch    14 | loss: 16.2530098Losses:  5.216700077056885 -0.0 3.304226875305176
MemoryTrain:  epoch  9, batch    15 | loss: 5.2167001
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 76.04%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 57.03%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 59.03%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 60.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 57.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 58.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 58.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 59.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 60.20%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 61.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.45%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 69.71%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 70.37%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 72.20%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 72.78%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 73.24%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 73.48%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 71.88%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 70.36%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 69.10%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 67.40%   [EVAL] batch:   37 | acc: 12.50%,  total acc: 65.95%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 65.06%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 65.16%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 67.30%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 67.47%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 67.12%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 67.15%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 67.84%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 67.22%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 66.50%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 65.93%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 65.75%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 64.98%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 64.70%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 64.55%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 64.73%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 64.80%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 64.44%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 64.30%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 64.17%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 63.63%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 62.60%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 61.71%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 61.13%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 61.35%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 61.27%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 60.54%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 59.65%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 59.24%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 59.20%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 59.15%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 59.03%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 58.56%   [EVAL] batch:   73 | acc: 18.75%,  total acc: 58.02%   [EVAL] batch:   74 | acc: 12.50%,  total acc: 57.42%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 56.91%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 56.25%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 55.69%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 55.06%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 54.37%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 53.78%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 53.28%   [EVAL] batch:   82 | acc: 0.00%,  total acc: 52.64%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 52.31%   [EVAL] batch:   84 | acc: 50.00%,  total acc: 52.28%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 52.62%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 52.80%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 53.27%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 53.58%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 53.96%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 54.46%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 54.96%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 55.44%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 55.92%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 56.38%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 56.64%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 56.57%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 56.76%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 57.13%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 57.19%   [EVAL] batch:  100 | acc: 75.00%,  total acc: 57.36%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 57.11%   [EVAL] batch:  102 | acc: 43.75%,  total acc: 56.98%   [EVAL] batch:  103 | acc: 37.50%,  total acc: 56.79%   [EVAL] batch:  104 | acc: 50.00%,  total acc: 56.73%   [EVAL] batch:  105 | acc: 50.00%,  total acc: 56.66%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 57.07%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 57.47%   [EVAL] batch:  108 | acc: 93.75%,  total acc: 57.80%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 58.07%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 58.16%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 57.76%   [EVAL] batch:  112 | acc: 6.25%,  total acc: 57.30%   [EVAL] batch:  113 | acc: 6.25%,  total acc: 56.85%   [EVAL] batch:  114 | acc: 6.25%,  total acc: 56.41%   [EVAL] batch:  115 | acc: 31.25%,  total acc: 56.20%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 56.20%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 56.36%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 56.36%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 56.56%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 56.61%   [EVAL] batch:  121 | acc: 62.50%,  total acc: 56.66%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 56.86%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 57.06%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 57.15%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 57.24%   [EVAL] batch:  126 | acc: 87.50%,  total acc: 57.48%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:  128 | acc: 100.00%,  total acc: 58.14%   [EVAL] batch:  129 | acc: 100.00%,  total acc: 58.46%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 58.78%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 59.09%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 59.12%   
cur_acc:  ['0.8333', '0.8616', '0.3828', '0.6202', '0.3267', '0.8705', '0.5875', '0.7604']
his_acc:  ['0.8333', '0.8484', '0.7662', '0.6809', '0.5746', '0.6040', '0.5938', '0.5912']
--------Round  1
seed:  200
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 0 1]
Losses:  21.339113235473633 8.064126014709473 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 21.3391132Losses:  22.671045303344727 9.549936294555664 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 22.6710453Losses:  24.942623138427734 12.034523963928223 -0.0
CurrentTrain: epoch  0, batch     2 | loss: 24.9426231Losses:  23.645389556884766 10.926660537719727 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 23.6453896Losses:  21.17608642578125 8.405169486999512 -0.0
CurrentTrain: epoch  0, batch     4 | loss: 21.1760864Losses:  24.576969146728516 11.942319869995117 -0.0
CurrentTrain: epoch  0, batch     5 | loss: 24.5769691Losses:  22.709613800048828 9.946890830993652 -0.0
CurrentTrain: epoch  0, batch     6 | loss: 22.7096138Losses:  24.95396614074707 12.361282348632812 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 24.9539661Losses:  20.97095489501953 8.76508903503418 -0.0
CurrentTrain: epoch  0, batch     8 | loss: 20.9709549Losses:  21.26903533935547 8.950830459594727 -0.0
CurrentTrain: epoch  0, batch     9 | loss: 21.2690353Losses:  18.866756439208984 6.721742153167725 -0.0
CurrentTrain: epoch  0, batch    10 | loss: 18.8667564Losses:  20.46538543701172 8.488651275634766 -0.0
CurrentTrain: epoch  0, batch    11 | loss: 20.4653854Losses:  22.975706100463867 11.014699935913086 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 22.9757061Losses:  22.730777740478516 10.862255096435547 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 22.7307777Losses:  20.015714645385742 8.119300842285156 -0.0
CurrentTrain: epoch  0, batch    14 | loss: 20.0157146Losses:  19.07342529296875 7.47756290435791 -0.0
CurrentTrain: epoch  0, batch    15 | loss: 19.0734253Losses:  18.512483596801758 6.985265731811523 -0.0
CurrentTrain: epoch  0, batch    16 | loss: 18.5124836Losses:  20.3797664642334 8.904369354248047 -0.0
CurrentTrain: epoch  0, batch    17 | loss: 20.3797665Losses:  23.74726676940918 12.290525436401367 -0.0
CurrentTrain: epoch  0, batch    18 | loss: 23.7472668Losses:  20.23758888244629 9.060205459594727 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 20.2375889Losses:  20.753341674804688 9.426149368286133 -0.0
CurrentTrain: epoch  0, batch    20 | loss: 20.7533417Losses:  24.16948699951172 13.071306228637695 -0.0
CurrentTrain: epoch  0, batch    21 | loss: 24.1694870Losses:  21.193634033203125 9.869468688964844 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 21.1936340Losses:  20.663780212402344 8.7662353515625 -0.0
CurrentTrain: epoch  0, batch    23 | loss: 20.6637802Losses:  18.12940788269043 6.945885181427002 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 18.1294079Losses:  17.768918991088867 6.706515789031982 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 17.7689190Losses:  25.75105857849121 15.656770706176758 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 25.7510586Losses:  22.448753356933594 11.192352294921875 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 22.4487534Losses:  18.29351043701172 7.622030258178711 -0.0
CurrentTrain: epoch  0, batch    28 | loss: 18.2935104Losses:  17.76563262939453 6.966488838195801 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 17.7656326Losses:  22.439666748046875 11.381930351257324 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 22.4396667Losses:  19.189260482788086 8.906169891357422 -0.0
CurrentTrain: epoch  0, batch    31 | loss: 19.1892605Losses:  21.61591339111328 10.671817779541016 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 21.6159134Losses:  17.792354583740234 7.492825984954834 -0.0
CurrentTrain: epoch  0, batch    33 | loss: 17.7923546Losses:  19.72269630432129 9.491043090820312 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 19.7226963Losses:  23.089458465576172 12.675027847290039 -0.0
CurrentTrain: epoch  0, batch    35 | loss: 23.0894585Losses:  19.290164947509766 9.097282409667969 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 19.2901649Losses:  12.390344619750977 3.4004416465759277 -0.0
CurrentTrain: epoch  0, batch    37 | loss: 12.3903446Losses:  21.08738136291504 10.350862503051758 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 21.0873814Losses:  16.033130645751953 6.208130836486816 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 16.0331306Losses:  17.494075775146484 7.606143474578857 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 17.4940758Losses:  20.35405731201172 10.464132308959961 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 20.3540573Losses:  22.65237808227539 12.187183380126953 -0.0
CurrentTrain: epoch  1, batch     4 | loss: 22.6523781Losses:  18.84988784790039 8.586966514587402 -0.0
CurrentTrain: epoch  1, batch     5 | loss: 18.8498878Losses:  17.982555389404297 9.229351043701172 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 17.9825554Losses:  16.42859649658203 7.260626316070557 -0.0
CurrentTrain: epoch  1, batch     7 | loss: 16.4285965Losses:  19.384273529052734 9.213996887207031 -0.0
CurrentTrain: epoch  1, batch     8 | loss: 19.3842735Losses:  15.083877563476562 6.1435747146606445 -0.0
CurrentTrain: epoch  1, batch     9 | loss: 15.0838776Losses:  16.96769905090332 6.46702766418457 -0.0
CurrentTrain: epoch  1, batch    10 | loss: 16.9676991Losses:  19.65641212463379 10.021570205688477 -0.0
CurrentTrain: epoch  1, batch    11 | loss: 19.6564121Losses:  17.311058044433594 8.255729675292969 -0.0
CurrentTrain: epoch  1, batch    12 | loss: 17.3110580Losses:  19.428234100341797 9.891916275024414 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 19.4282341Losses:  16.7928466796875 6.927609443664551 -0.0
CurrentTrain: epoch  1, batch    14 | loss: 16.7928467Losses:  15.032007217407227 6.34963321685791 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 15.0320072Losses:  16.20433235168457 6.756895065307617 -0.0
CurrentTrain: epoch  1, batch    16 | loss: 16.2043324Losses:  15.541651725769043 6.135292053222656 -0.0
CurrentTrain: epoch  1, batch    17 | loss: 15.5416517Losses:  15.88943099975586 6.532050609588623 -0.0
CurrentTrain: epoch  1, batch    18 | loss: 15.8894310Losses:  17.533504486083984 8.700347900390625 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 17.5335045Losses:  18.580806732177734 8.935098648071289 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 18.5808067Losses:  16.651273727416992 8.01384162902832 -0.0
CurrentTrain: epoch  1, batch    21 | loss: 16.6512737Losses:  20.465614318847656 10.61528491973877 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 20.4656143Losses:  16.540008544921875 7.37664794921875 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 16.5400085Losses:  18.20730209350586 9.017271995544434 -0.0
CurrentTrain: epoch  1, batch    24 | loss: 18.2073021Losses:  17.160940170288086 7.576629161834717 -0.0
CurrentTrain: epoch  1, batch    25 | loss: 17.1609402Losses:  16.218196868896484 7.65531063079834 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 16.2181969Losses:  16.93020248413086 6.923247337341309 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 16.9302025Losses:  15.497031211853027 6.527042388916016 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 15.4970312Losses:  21.781997680664062 12.035979270935059 -0.0
CurrentTrain: epoch  1, batch    29 | loss: 21.7819977Losses:  15.549026489257812 6.599076271057129 -0.0
CurrentTrain: epoch  1, batch    30 | loss: 15.5490265Losses:  20.70122528076172 11.736960411071777 -0.0
CurrentTrain: epoch  1, batch    31 | loss: 20.7012253Losses:  18.75925636291504 9.540363311767578 -0.0
CurrentTrain: epoch  1, batch    32 | loss: 18.7592564Losses:  15.28670883178711 6.818373203277588 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 15.2867088Losses:  16.985816955566406 7.734536170959473 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 16.9858170Losses:  17.328937530517578 8.453187942504883 -0.0
CurrentTrain: epoch  1, batch    35 | loss: 17.3289375Losses:  15.237029075622559 6.3929033279418945 -0.0
CurrentTrain: epoch  1, batch    36 | loss: 15.2370291Losses:  13.049491882324219 3.642437219619751 -0.0
CurrentTrain: epoch  1, batch    37 | loss: 13.0494919Losses:  13.92018985748291 5.412285804748535 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 13.9201899Losses:  15.61929702758789 7.026815414428711 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 15.6192970Losses:  14.312200546264648 5.680862903594971 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 14.3122005Losses:  14.292259216308594 6.174740791320801 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 14.2922592Losses:  17.9580020904541 8.690116882324219 -0.0
CurrentTrain: epoch  2, batch     4 | loss: 17.9580021Losses:  16.884769439697266 7.8804521560668945 -0.0
CurrentTrain: epoch  2, batch     5 | loss: 16.8847694Losses:  15.484638214111328 7.201037883758545 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 15.4846382Losses:  14.715768814086914 6.422482967376709 -0.0
CurrentTrain: epoch  2, batch     7 | loss: 14.7157688Losses:  16.252445220947266 8.061114311218262 -0.0
CurrentTrain: epoch  2, batch     8 | loss: 16.2524452Losses:  13.827917098999023 5.967693328857422 -0.0
CurrentTrain: epoch  2, batch     9 | loss: 13.8279171Losses:  17.445388793945312 9.071187973022461 -0.0
CurrentTrain: epoch  2, batch    10 | loss: 17.4453888Losses:  16.30856704711914 8.522082328796387 -0.0
CurrentTrain: epoch  2, batch    11 | loss: 16.3085670Losses:  15.536718368530273 6.617002487182617 -0.0
CurrentTrain: epoch  2, batch    12 | loss: 15.5367184Losses:  13.951299667358398 5.666481018066406 -0.0
CurrentTrain: epoch  2, batch    13 | loss: 13.9512997Losses:  13.603679656982422 5.66771125793457 -0.0
CurrentTrain: epoch  2, batch    14 | loss: 13.6036797Losses:  14.63981819152832 6.068098068237305 -0.0
CurrentTrain: epoch  2, batch    15 | loss: 14.6398182Losses:  20.600570678710938 11.473060607910156 -0.0
CurrentTrain: epoch  2, batch    16 | loss: 20.6005707Losses:  16.976390838623047 7.830572128295898 -0.0
CurrentTrain: epoch  2, batch    17 | loss: 16.9763908Losses:  16.869037628173828 7.923223495483398 -0.0
CurrentTrain: epoch  2, batch    18 | loss: 16.8690376Losses:  13.549542427062988 5.184120178222656 -0.0
CurrentTrain: epoch  2, batch    19 | loss: 13.5495424Losses:  16.419700622558594 7.24574613571167 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 16.4197006Losses:  13.61148738861084 5.461369514465332 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 13.6114874Losses:  20.173049926757812 11.065067291259766 -0.0
CurrentTrain: epoch  2, batch    22 | loss: 20.1730499Losses:  14.668500900268555 7.097198486328125 -0.0
CurrentTrain: epoch  2, batch    23 | loss: 14.6685009Losses:  14.731569290161133 6.510557651519775 -0.0
CurrentTrain: epoch  2, batch    24 | loss: 14.7315693Losses:  14.965474128723145 5.812188148498535 -0.0
CurrentTrain: epoch  2, batch    25 | loss: 14.9654741Losses:  16.870981216430664 7.831426620483398 -0.0
CurrentTrain: epoch  2, batch    26 | loss: 16.8709812Losses:  14.86021614074707 7.559762001037598 -0.0
CurrentTrain: epoch  2, batch    27 | loss: 14.8602161Losses:  14.751540184020996 6.554841041564941 -0.0
CurrentTrain: epoch  2, batch    28 | loss: 14.7515402Losses:  16.11517333984375 7.527798652648926 -0.0
CurrentTrain: epoch  2, batch    29 | loss: 16.1151733Losses:  15.414371490478516 8.004138946533203 -0.0
CurrentTrain: epoch  2, batch    30 | loss: 15.4143715Losses:  16.5155086517334 9.255242347717285 -0.0
CurrentTrain: epoch  2, batch    31 | loss: 16.5155087Losses:  20.87657928466797 12.361247062683105 -0.0
CurrentTrain: epoch  2, batch    32 | loss: 20.8765793Losses:  13.255818367004395 6.090558052062988 -0.0
CurrentTrain: epoch  2, batch    33 | loss: 13.2558184Losses:  13.776453018188477 6.771321773529053 -0.0
CurrentTrain: epoch  2, batch    34 | loss: 13.7764530Losses:  16.6616153717041 8.233636856079102 -0.0
CurrentTrain: epoch  2, batch    35 | loss: 16.6616154Losses:  15.48886775970459 8.082003593444824 -0.0
CurrentTrain: epoch  2, batch    36 | loss: 15.4888678Losses:  10.503049850463867 1.9944988489151 -0.0
CurrentTrain: epoch  2, batch    37 | loss: 10.5030499Losses:  16.281085968017578 7.137324333190918 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 16.2810860Losses:  13.496910095214844 5.879417419433594 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 13.4969101Losses:  13.041788101196289 4.941201686859131 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 13.0417881Losses:  15.100807189941406 6.442943572998047 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 15.1008072Losses:  14.550556182861328 6.510457992553711 -0.0
CurrentTrain: epoch  3, batch     4 | loss: 14.5505562Losses:  14.261693954467773 6.2655463218688965 -0.0
CurrentTrain: epoch  3, batch     5 | loss: 14.2616940Losses:  16.30213165283203 7.086555480957031 -0.0
CurrentTrain: epoch  3, batch     6 | loss: 16.3021317Losses:  12.868309020996094 4.693710803985596 -0.0
CurrentTrain: epoch  3, batch     7 | loss: 12.8683090Losses:  16.641775131225586 10.211847305297852 -0.0
CurrentTrain: epoch  3, batch     8 | loss: 16.6417751Losses:  15.262260437011719 7.750881671905518 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 15.2622604Losses:  13.878198623657227 5.947881698608398 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 13.8781986Losses:  21.860694885253906 12.771951675415039 -0.0
CurrentTrain: epoch  3, batch    11 | loss: 21.8606949Losses:  14.383591651916504 6.590052604675293 -0.0
CurrentTrain: epoch  3, batch    12 | loss: 14.3835917Losses:  11.96035385131836 4.572113513946533 -0.0
CurrentTrain: epoch  3, batch    13 | loss: 11.9603539Losses:  14.219071388244629 5.73875617980957 -0.0
CurrentTrain: epoch  3, batch    14 | loss: 14.2190714Losses:  13.388092994689941 5.480809688568115 -0.0
CurrentTrain: epoch  3, batch    15 | loss: 13.3880930Losses:  13.72597885131836 5.780803680419922 -0.0
CurrentTrain: epoch  3, batch    16 | loss: 13.7259789Losses:  15.773385047912598 7.43546199798584 -0.0
CurrentTrain: epoch  3, batch    17 | loss: 15.7733850Losses:  12.600181579589844 5.056901931762695 -0.0
CurrentTrain: epoch  3, batch    18 | loss: 12.6001816Losses:  13.078454971313477 5.436425685882568 -0.0
CurrentTrain: epoch  3, batch    19 | loss: 13.0784550Losses:  14.005603790283203 6.081528186798096 -0.0
CurrentTrain: epoch  3, batch    20 | loss: 14.0056038Losses:  14.990047454833984 6.889167785644531 -0.0
CurrentTrain: epoch  3, batch    21 | loss: 14.9900475Losses:  14.14970874786377 5.989993095397949 -0.0
CurrentTrain: epoch  3, batch    22 | loss: 14.1497087Losses:  12.270219802856445 4.873915672302246 -0.0
CurrentTrain: epoch  3, batch    23 | loss: 12.2702198Losses:  12.748950958251953 5.823681354522705 -0.0
CurrentTrain: epoch  3, batch    24 | loss: 12.7489510Losses:  14.289337158203125 6.448554515838623 -0.0
CurrentTrain: epoch  3, batch    25 | loss: 14.2893372Losses:  14.849072456359863 6.832577705383301 -0.0
CurrentTrain: epoch  3, batch    26 | loss: 14.8490725Losses:  13.653629302978516 6.657384872436523 -0.0
CurrentTrain: epoch  3, batch    27 | loss: 13.6536293Losses:  15.937068939208984 9.239166259765625 -0.0
CurrentTrain: epoch  3, batch    28 | loss: 15.9370689Losses:  16.084712982177734 8.441904067993164 -0.0
CurrentTrain: epoch  3, batch    29 | loss: 16.0847130Losses:  13.427620887756348 5.354702949523926 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 13.4276209Losses:  14.315650939941406 8.13181209564209 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 14.3156509Losses:  13.368402481079102 6.00015115737915 -0.0
CurrentTrain: epoch  3, batch    32 | loss: 13.3684025Losses:  11.967229843139648 5.597304344177246 -0.0
CurrentTrain: epoch  3, batch    33 | loss: 11.9672298Losses:  11.720527648925781 5.199179172515869 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 11.7205276Losses:  12.587944030761719 5.809924125671387 -0.0
CurrentTrain: epoch  3, batch    35 | loss: 12.5879440Losses:  13.64860725402832 5.392916202545166 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 13.6486073Losses:  9.034987449645996 1.6406651735305786 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 9.0349874Losses:  11.519078254699707 4.436025619506836 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 11.5190783Losses:  12.72388744354248 6.06533670425415 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 12.7238874Losses:  13.794672966003418 6.248055934906006 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 13.7946730Losses:  18.037752151489258 8.75552749633789 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 18.0377522Losses:  11.852639198303223 5.39852237701416 -0.0
CurrentTrain: epoch  4, batch     4 | loss: 11.8526392Losses:  14.041215896606445 7.577650547027588 -0.0
CurrentTrain: epoch  4, batch     5 | loss: 14.0412159Losses:  15.38740348815918 6.601878643035889 -0.0
CurrentTrain: epoch  4, batch     6 | loss: 15.3874035Losses:  12.527013778686523 5.339605331420898 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 12.5270138Losses:  16.499759674072266 8.903069496154785 -0.0
CurrentTrain: epoch  4, batch     8 | loss: 16.4997597Losses:  14.270212173461914 6.580025672912598 -0.0
CurrentTrain: epoch  4, batch     9 | loss: 14.2702122Losses:  12.775308609008789 5.001850128173828 -0.0
CurrentTrain: epoch  4, batch    10 | loss: 12.7753086Losses:  12.580297470092773 5.38431978225708 -0.0
CurrentTrain: epoch  4, batch    11 | loss: 12.5802975Losses:  12.314599990844727 5.60402774810791 -0.0
CurrentTrain: epoch  4, batch    12 | loss: 12.3146000Losses:  11.990013122558594 4.604665279388428 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 11.9900131Losses:  11.638827323913574 4.7247209548950195 -0.0
CurrentTrain: epoch  4, batch    14 | loss: 11.6388273Losses:  13.251286506652832 6.414966583251953 -0.0
CurrentTrain: epoch  4, batch    15 | loss: 13.2512865Losses:  11.99010944366455 5.5690202713012695 -0.0
CurrentTrain: epoch  4, batch    16 | loss: 11.9901094Losses:  12.97054672241211 6.483885765075684 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 12.9705467Losses:  12.296661376953125 4.922629356384277 -0.0
CurrentTrain: epoch  4, batch    18 | loss: 12.2966614Losses:  13.047346115112305 6.0131025314331055 -0.0
CurrentTrain: epoch  4, batch    19 | loss: 13.0473461Losses:  11.675296783447266 5.170387268066406 -0.0
CurrentTrain: epoch  4, batch    20 | loss: 11.6752968Losses:  16.248929977416992 9.598875045776367 -0.0
CurrentTrain: epoch  4, batch    21 | loss: 16.2489300Losses:  11.774927139282227 5.108119487762451 -0.0
CurrentTrain: epoch  4, batch    22 | loss: 11.7749271Losses:  18.213197708129883 10.369179725646973 -0.0
CurrentTrain: epoch  4, batch    23 | loss: 18.2131977Losses:  13.023591995239258 5.452530384063721 -0.0
CurrentTrain: epoch  4, batch    24 | loss: 13.0235920Losses:  13.17796516418457 6.4519147872924805 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 13.1779652Losses:  13.387531280517578 5.98211669921875 -0.0
CurrentTrain: epoch  4, batch    26 | loss: 13.3875313Losses:  13.686147689819336 7.0244879722595215 -0.0
CurrentTrain: epoch  4, batch    27 | loss: 13.6861477Losses:  12.152288436889648 5.7798357009887695 -0.0
CurrentTrain: epoch  4, batch    28 | loss: 12.1522884Losses:  14.104644775390625 6.406988620758057 -0.0
CurrentTrain: epoch  4, batch    29 | loss: 14.1046448Losses:  13.098430633544922 6.072085857391357 -0.0
CurrentTrain: epoch  4, batch    30 | loss: 13.0984306Losses:  12.373499870300293 4.844700813293457 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 12.3734999Losses:  13.360668182373047 6.992369651794434 -0.0
CurrentTrain: epoch  4, batch    32 | loss: 13.3606682Losses:  12.081273078918457 5.237857818603516 -0.0
CurrentTrain: epoch  4, batch    33 | loss: 12.0812731Losses:  12.297950744628906 4.222908020019531 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 12.2979507Losses:  15.725776672363281 7.8372297286987305 -0.0
CurrentTrain: epoch  4, batch    35 | loss: 15.7257767Losses:  13.071967124938965 5.795833587646484 -0.0
CurrentTrain: epoch  4, batch    36 | loss: 13.0719671Losses:  8.357351303100586 0.7668466567993164 -0.0
CurrentTrain: epoch  4, batch    37 | loss: 8.3573513Losses:  11.972318649291992 5.514431953430176 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 11.9723186Losses:  13.415918350219727 6.169536590576172 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 13.4159184Losses:  15.014726638793945 7.144006252288818 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 15.0147266Losses:  15.204597473144531 7.545377254486084 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 15.2045975Losses:  13.36490535736084 6.1574296951293945 -0.0
CurrentTrain: epoch  5, batch     4 | loss: 13.3649054Losses:  12.5065279006958 5.45189094543457 -0.0
CurrentTrain: epoch  5, batch     5 | loss: 12.5065279Losses:  14.508237838745117 6.973438739776611 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 14.5082378Losses:  12.403727531433105 5.1991424560546875 -0.0
CurrentTrain: epoch  5, batch     7 | loss: 12.4037275Losses:  12.195060729980469 5.661337852478027 -0.0
CurrentTrain: epoch  5, batch     8 | loss: 12.1950607Losses:  12.783138275146484 5.372577667236328 -0.0
CurrentTrain: epoch  5, batch     9 | loss: 12.7831383Losses:  10.546849250793457 4.258906364440918 -0.0
CurrentTrain: epoch  5, batch    10 | loss: 10.5468493Losses:  11.334466934204102 5.472888469696045 -0.0
CurrentTrain: epoch  5, batch    11 | loss: 11.3344669Losses:  17.391277313232422 12.037406921386719 -0.0
CurrentTrain: epoch  5, batch    12 | loss: 17.3912773Losses:  13.902532577514648 7.159297466278076 -0.0
CurrentTrain: epoch  5, batch    13 | loss: 13.9025326Losses:  11.88609790802002 4.265491485595703 -0.0
CurrentTrain: epoch  5, batch    14 | loss: 11.8860979Losses:  15.897254943847656 8.337897300720215 -0.0
CurrentTrain: epoch  5, batch    15 | loss: 15.8972549Losses:  13.544291496276855 7.122841835021973 -0.0
CurrentTrain: epoch  5, batch    16 | loss: 13.5442915Losses:  13.303699493408203 6.510651588439941 -0.0
CurrentTrain: epoch  5, batch    17 | loss: 13.3036995Losses:  14.463891983032227 6.529853343963623 -0.0
CurrentTrain: epoch  5, batch    18 | loss: 14.4638920Losses:  16.19154167175293 8.88774585723877 -0.0
CurrentTrain: epoch  5, batch    19 | loss: 16.1915417Losses:  11.67264175415039 5.074375152587891 -0.0
CurrentTrain: epoch  5, batch    20 | loss: 11.6726418Losses:  12.229179382324219 5.537440299987793 -0.0
CurrentTrain: epoch  5, batch    21 | loss: 12.2291794Losses:  13.757293701171875 6.81976318359375 -0.0
CurrentTrain: epoch  5, batch    22 | loss: 13.7572937Losses:  14.014070510864258 8.085651397705078 -0.0
CurrentTrain: epoch  5, batch    23 | loss: 14.0140705Losses:  11.363405227661133 5.259945392608643 -0.0
CurrentTrain: epoch  5, batch    24 | loss: 11.3634052Losses:  13.490702629089355 6.891770362854004 -0.0
CurrentTrain: epoch  5, batch    25 | loss: 13.4907026Losses:  13.105627059936523 5.712261199951172 -0.0
CurrentTrain: epoch  5, batch    26 | loss: 13.1056271Losses:  12.524139404296875 5.456421852111816 -0.0
CurrentTrain: epoch  5, batch    27 | loss: 12.5241394Losses:  12.10571002960205 5.637910842895508 -0.0
CurrentTrain: epoch  5, batch    28 | loss: 12.1057100Losses:  12.429837226867676 5.936957359313965 -0.0
CurrentTrain: epoch  5, batch    29 | loss: 12.4298372Losses:  14.858342170715332 8.36125373840332 -0.0
CurrentTrain: epoch  5, batch    30 | loss: 14.8583422Losses:  12.047860145568848 5.802541732788086 -0.0
CurrentTrain: epoch  5, batch    31 | loss: 12.0478601Losses:  13.331554412841797 7.63803243637085 -0.0
CurrentTrain: epoch  5, batch    32 | loss: 13.3315544Losses:  14.849105834960938 7.368689060211182 -0.0
CurrentTrain: epoch  5, batch    33 | loss: 14.8491058Losses:  14.748136520385742 7.7502570152282715 -0.0
CurrentTrain: epoch  5, batch    34 | loss: 14.7481365Losses:  16.978675842285156 10.023590087890625 -0.0
CurrentTrain: epoch  5, batch    35 | loss: 16.9786758Losses:  14.903970718383789 8.09018611907959 -0.0
CurrentTrain: epoch  5, batch    36 | loss: 14.9039707Losses:  7.409544944763184 1.1697938442230225 -0.0
CurrentTrain: epoch  5, batch    37 | loss: 7.4095449Losses:  10.738192558288574 4.951499938964844 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 10.7381926Losses:  9.762810707092285 4.049882888793945 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 9.7628107Losses:  12.271302223205566 6.6757659912109375 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 12.2713022Losses:  14.022083282470703 6.90368127822876 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 14.0220833Losses:  10.226480484008789 4.764515399932861 -0.0
CurrentTrain: epoch  6, batch     4 | loss: 10.2264805Losses:  12.035076141357422 5.838976860046387 -0.0
CurrentTrain: epoch  6, batch     5 | loss: 12.0350761Losses:  12.411294937133789 6.920449256896973 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 12.4112949Losses:  13.636025428771973 7.108604431152344 -0.0
CurrentTrain: epoch  6, batch     7 | loss: 13.6360254Losses:  13.6775484085083 6.495907783508301 -0.0
CurrentTrain: epoch  6, batch     8 | loss: 13.6775484Losses:  12.339859008789062 5.711024761199951 -0.0
CurrentTrain: epoch  6, batch     9 | loss: 12.3398590Losses:  12.958940505981445 6.235174179077148 -0.0
CurrentTrain: epoch  6, batch    10 | loss: 12.9589405Losses:  10.509122848510742 4.221528053283691 -0.0
CurrentTrain: epoch  6, batch    11 | loss: 10.5091228Losses:  16.69641876220703 10.797091484069824 -0.0
CurrentTrain: epoch  6, batch    12 | loss: 16.6964188Losses:  12.928415298461914 6.384148120880127 -0.0
CurrentTrain: epoch  6, batch    13 | loss: 12.9284153Losses:  12.205013275146484 6.307882308959961 -0.0
CurrentTrain: epoch  6, batch    14 | loss: 12.2050133Losses:  11.493528366088867 5.910457611083984 -0.0
CurrentTrain: epoch  6, batch    15 | loss: 11.4935284Losses:  9.694345474243164 4.308175086975098 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 9.6943455Losses:  11.276540756225586 5.05941104888916 -0.0
CurrentTrain: epoch  6, batch    17 | loss: 11.2765408Losses:  10.49905776977539 5.059896945953369 -0.0
CurrentTrain: epoch  6, batch    18 | loss: 10.4990578Losses:  12.564764022827148 6.380946636199951 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 12.5647640Losses:  10.429037094116211 4.844711780548096 -0.0
CurrentTrain: epoch  6, batch    20 | loss: 10.4290371Losses:  11.027581214904785 5.705434799194336 -0.0
CurrentTrain: epoch  6, batch    21 | loss: 11.0275812Losses:  11.779404640197754 5.46690559387207 -0.0
CurrentTrain: epoch  6, batch    22 | loss: 11.7794046Losses:  21.189546585083008 13.079784393310547 -0.0
CurrentTrain: epoch  6, batch    23 | loss: 21.1895466Losses:  12.131010055541992 5.839991569519043 -0.0
CurrentTrain: epoch  6, batch    24 | loss: 12.1310101Losses:  17.19584846496582 10.448389053344727 -0.0
CurrentTrain: epoch  6, batch    25 | loss: 17.1958485Losses:  11.418700218200684 4.6757965087890625 -0.0
CurrentTrain: epoch  6, batch    26 | loss: 11.4187002Losses:  10.941476821899414 4.608487129211426 -0.0
CurrentTrain: epoch  6, batch    27 | loss: 10.9414768Losses:  17.02743148803711 11.051316261291504 -0.0
CurrentTrain: epoch  6, batch    28 | loss: 17.0274315Losses:  14.611791610717773 6.839618682861328 -0.0
CurrentTrain: epoch  6, batch    29 | loss: 14.6117916Losses:  18.34621810913086 8.960168838500977 -0.0
CurrentTrain: epoch  6, batch    30 | loss: 18.3462181Losses:  10.712196350097656 4.3293375968933105 -0.0
CurrentTrain: epoch  6, batch    31 | loss: 10.7121964Losses:  11.928531646728516 5.168259620666504 -0.0
CurrentTrain: epoch  6, batch    32 | loss: 11.9285316Losses:  13.166799545288086 6.147695541381836 -0.0
CurrentTrain: epoch  6, batch    33 | loss: 13.1667995Losses:  15.879833221435547 9.504222869873047 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 15.8798332Losses:  14.116897583007812 6.520316123962402 -0.0
CurrentTrain: epoch  6, batch    35 | loss: 14.1168976Losses:  24.715717315673828 15.92806625366211 -0.0
CurrentTrain: epoch  6, batch    36 | loss: 24.7157173Losses:  6.75686502456665 1.6167062520980835 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 6.7568650Losses:  15.90017032623291 9.292993545532227 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 15.9001703Losses:  11.722888946533203 5.3303632736206055 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 11.7228889Losses:  13.616501808166504 6.5544586181640625 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 13.6165018Losses:  10.572834014892578 4.565129280090332 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 10.5728340Losses:  13.480595588684082 7.478884696960449 -0.0
CurrentTrain: epoch  7, batch     4 | loss: 13.4805956Losses:  10.946968078613281 4.678225994110107 -0.0
CurrentTrain: epoch  7, batch     5 | loss: 10.9469681Losses:  14.221176147460938 8.200264930725098 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 14.2211761Losses:  12.411784172058105 5.45931339263916 -0.0
CurrentTrain: epoch  7, batch     7 | loss: 12.4117842Losses:  12.002338409423828 6.207792282104492 -0.0
CurrentTrain: epoch  7, batch     8 | loss: 12.0023384Losses:  10.824396133422852 4.269256591796875 -0.0
CurrentTrain: epoch  7, batch     9 | loss: 10.8243961Losses:  14.097545623779297 7.798939228057861 -0.0
CurrentTrain: epoch  7, batch    10 | loss: 14.0975456Losses:  12.959404945373535 7.429476261138916 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 12.9594049Losses:  12.779614448547363 7.666770935058594 -0.0
CurrentTrain: epoch  7, batch    12 | loss: 12.7796144Losses:  11.828428268432617 5.338412284851074 -0.0
CurrentTrain: epoch  7, batch    13 | loss: 11.8284283Losses:  12.504427909851074 5.669856071472168 -0.0
CurrentTrain: epoch  7, batch    14 | loss: 12.5044279Losses:  16.749553680419922 10.525732040405273 -0.0
CurrentTrain: epoch  7, batch    15 | loss: 16.7495537Losses:  12.985513687133789 6.378227233886719 -0.0
CurrentTrain: epoch  7, batch    16 | loss: 12.9855137Losses:  14.08790397644043 8.093762397766113 -0.0
CurrentTrain: epoch  7, batch    17 | loss: 14.0879040Losses:  10.912744522094727 5.265393257141113 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 10.9127445Losses:  9.997201919555664 3.9779632091522217 -0.0
CurrentTrain: epoch  7, batch    19 | loss: 9.9972019Losses:  14.885909080505371 7.639982223510742 -0.0
CurrentTrain: epoch  7, batch    20 | loss: 14.8859091Losses:  12.869356155395508 7.4520792961120605 -0.0
CurrentTrain: epoch  7, batch    21 | loss: 12.8693562Losses:  16.229827880859375 8.81087875366211 -0.0
CurrentTrain: epoch  7, batch    22 | loss: 16.2298279Losses:  12.455982208251953 6.439836502075195 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 12.4559822Losses:  16.364892959594727 8.879863739013672 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 16.3648930Losses:  16.308242797851562 8.577988624572754 -0.0
CurrentTrain: epoch  7, batch    25 | loss: 16.3082428Losses:  12.195629119873047 5.833766937255859 -0.0
CurrentTrain: epoch  7, batch    26 | loss: 12.1956291Losses:  18.361730575561523 10.964828491210938 -0.0
CurrentTrain: epoch  7, batch    27 | loss: 18.3617306Losses:  13.04664134979248 7.150981426239014 -0.0
CurrentTrain: epoch  7, batch    28 | loss: 13.0466413Losses:  10.988948822021484 4.530615329742432 -0.0
CurrentTrain: epoch  7, batch    29 | loss: 10.9889488Losses:  11.108026504516602 4.888500213623047 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 11.1080265Losses:  15.296920776367188 8.953594207763672 -0.0
CurrentTrain: epoch  7, batch    31 | loss: 15.2969208Losses:  12.934042930603027 6.070229530334473 -0.0
CurrentTrain: epoch  7, batch    32 | loss: 12.9340429Losses:  12.023605346679688 6.327688694000244 -0.0
CurrentTrain: epoch  7, batch    33 | loss: 12.0236053Losses:  13.123526573181152 7.25418758392334 -0.0
CurrentTrain: epoch  7, batch    34 | loss: 13.1235266Losses:  11.406039237976074 5.0765862464904785 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 11.4060392Losses:  11.286087036132812 4.899845123291016 -0.0
CurrentTrain: epoch  7, batch    36 | loss: 11.2860870Losses:  8.699159622192383 3.1015052795410156 -0.0
CurrentTrain: epoch  7, batch    37 | loss: 8.6991596Losses:  11.653593063354492 5.42449951171875 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 11.6535931Losses:  9.581491470336914 4.040607929229736 -0.0
CurrentTrain: epoch  8, batch     1 | loss: 9.5814915Losses:  11.949546813964844 5.801220893859863 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 11.9495468Losses:  10.3486328125 4.822759628295898 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 10.3486328Losses:  10.286018371582031 5.18056583404541 -0.0
CurrentTrain: epoch  8, batch     4 | loss: 10.2860184Losses:  10.842352867126465 5.232357978820801 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 10.8423529Losses:  12.933432579040527 6.796229362487793 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 12.9334326Losses:  10.516338348388672 5.177349090576172 -0.0
CurrentTrain: epoch  8, batch     7 | loss: 10.5163383Losses:  10.863314628601074 5.636590480804443 -0.0
CurrentTrain: epoch  8, batch     8 | loss: 10.8633146Losses:  9.867698669433594 4.223790645599365 -0.0
CurrentTrain: epoch  8, batch     9 | loss: 9.8676987Losses:  10.594112396240234 5.3089799880981445 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 10.5941124Losses:  10.872980117797852 5.843244552612305 -0.0
CurrentTrain: epoch  8, batch    11 | loss: 10.8729801Losses:  11.315510749816895 5.038018226623535 -0.0
CurrentTrain: epoch  8, batch    12 | loss: 11.3155107Losses:  9.418834686279297 4.192691802978516 -0.0
CurrentTrain: epoch  8, batch    13 | loss: 9.4188347Losses:  10.705778121948242 4.805466651916504 -0.0
CurrentTrain: epoch  8, batch    14 | loss: 10.7057781Losses:  11.79788589477539 6.645690441131592 -0.0
CurrentTrain: epoch  8, batch    15 | loss: 11.7978859Losses:  10.231797218322754 4.178034782409668 -0.0
CurrentTrain: epoch  8, batch    16 | loss: 10.2317972Losses:  18.117958068847656 12.960121154785156 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 18.1179581Losses:  11.37442684173584 6.170331954956055 -0.0
CurrentTrain: epoch  8, batch    18 | loss: 11.3744268Losses:  10.788654327392578 4.979321479797363 -0.0
CurrentTrain: epoch  8, batch    19 | loss: 10.7886543Losses:  10.974519729614258 5.602748870849609 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 10.9745197Losses:  9.732283592224121 4.471645832061768 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 9.7322836Losses:  11.86644172668457 6.445437431335449 -0.0
CurrentTrain: epoch  8, batch    22 | loss: 11.8664417Losses:  10.72422981262207 5.436659336090088 -0.0
CurrentTrain: epoch  8, batch    23 | loss: 10.7242298Losses:  17.579235076904297 11.979421615600586 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 17.5792351Losses:  17.485721588134766 10.78288745880127 -0.0
CurrentTrain: epoch  8, batch    25 | loss: 17.4857216Losses:  11.407966613769531 5.557572841644287 -0.0
CurrentTrain: epoch  8, batch    26 | loss: 11.4079666Losses:  11.12535285949707 5.959658622741699 -0.0
CurrentTrain: epoch  8, batch    27 | loss: 11.1253529Losses:  11.94484806060791 5.771746635437012 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 11.9448481Losses:  11.90745735168457 5.6216816902160645 -0.0
CurrentTrain: epoch  8, batch    29 | loss: 11.9074574Losses:  11.635088920593262 4.904521942138672 -0.0
CurrentTrain: epoch  8, batch    30 | loss: 11.6350889Losses:  9.9670991897583 3.9313418865203857 -0.0
CurrentTrain: epoch  8, batch    31 | loss: 9.9670992Losses:  11.531526565551758 5.216541767120361 -0.0
CurrentTrain: epoch  8, batch    32 | loss: 11.5315266Losses:  10.963570594787598 5.33598518371582 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 10.9635706Losses:  10.785966873168945 5.16412353515625 -0.0
CurrentTrain: epoch  8, batch    34 | loss: 10.7859669Losses:  11.827302932739258 5.416844367980957 -0.0
CurrentTrain: epoch  8, batch    35 | loss: 11.8273029Losses:  16.492645263671875 11.379396438598633 -0.0
CurrentTrain: epoch  8, batch    36 | loss: 16.4926453Losses:  10.278465270996094 3.4081835746765137 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 10.2784653Losses:  9.788179397583008 4.256299018859863 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 9.7881794Losses:  12.671911239624023 6.519581317901611 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 12.6719112Losses:  12.632460594177246 6.647987365722656 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 12.6324606Losses:  11.673583984375 5.814537048339844 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 11.6735840Losses:  11.815263748168945 5.945712089538574 -0.0
CurrentTrain: epoch  9, batch     4 | loss: 11.8152637Losses:  9.308658599853516 3.908559799194336 -0.0
CurrentTrain: epoch  9, batch     5 | loss: 9.3086586Losses:  9.856585502624512 4.593950271606445 -0.0
CurrentTrain: epoch  9, batch     6 | loss: 9.8565855Losses:  11.607566833496094 6.6100358963012695 -0.0
CurrentTrain: epoch  9, batch     7 | loss: 11.6075668Losses:  9.582448959350586 4.397175312042236 -0.0
CurrentTrain: epoch  9, batch     8 | loss: 9.5824490Losses:  10.743342399597168 5.592686176300049 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 10.7433424Losses:  9.431109428405762 3.5747034549713135 -0.0
CurrentTrain: epoch  9, batch    10 | loss: 9.4311094Losses:  11.319795608520508 6.201463222503662 -0.0
CurrentTrain: epoch  9, batch    11 | loss: 11.3197956Losses:  9.922107696533203 4.742973327636719 -0.0
CurrentTrain: epoch  9, batch    12 | loss: 9.9221077Losses:  9.408851623535156 4.306197166442871 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 9.4088516Losses:  9.330562591552734 4.273473262786865 -0.0
CurrentTrain: epoch  9, batch    14 | loss: 9.3305626Losses:  12.704465866088867 5.9480085372924805 -0.0
CurrentTrain: epoch  9, batch    15 | loss: 12.7044659Losses:  8.92251205444336 3.4672093391418457 -0.0
CurrentTrain: epoch  9, batch    16 | loss: 8.9225121Losses:  9.345504760742188 4.2582597732543945 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 9.3455048Losses:  12.393213272094727 7.392368316650391 -0.0
CurrentTrain: epoch  9, batch    18 | loss: 12.3932133Losses:  13.294662475585938 7.9218950271606445 -0.0
CurrentTrain: epoch  9, batch    19 | loss: 13.2946625Losses:  13.118340492248535 7.150790214538574 -0.0
CurrentTrain: epoch  9, batch    20 | loss: 13.1183405Losses:  9.564323425292969 4.538309097290039 -0.0
CurrentTrain: epoch  9, batch    21 | loss: 9.5643234Losses:  8.66988468170166 3.388056755065918 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 8.6698847Losses:  10.591327667236328 5.533581733703613 -0.0
CurrentTrain: epoch  9, batch    23 | loss: 10.5913277Losses:  11.177824020385742 5.539327621459961 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 11.1778240Losses:  9.553679466247559 4.301024436950684 -0.0
CurrentTrain: epoch  9, batch    25 | loss: 9.5536795Losses:  11.943279266357422 7.062814235687256 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 11.9432793Losses:  9.602814674377441 4.638058662414551 -0.0
CurrentTrain: epoch  9, batch    27 | loss: 9.6028147Losses:  10.408324241638184 5.447793960571289 -0.0
CurrentTrain: epoch  9, batch    28 | loss: 10.4083242Losses:  10.130457878112793 4.801919937133789 -0.0
CurrentTrain: epoch  9, batch    29 | loss: 10.1304579Losses:  12.18091106414795 6.252483367919922 -0.0
CurrentTrain: epoch  9, batch    30 | loss: 12.1809111Losses:  10.003767013549805 4.772238254547119 -0.0
CurrentTrain: epoch  9, batch    31 | loss: 10.0037670Losses:  12.88329792022705 7.212413787841797 -0.0
CurrentTrain: epoch  9, batch    32 | loss: 12.8832979Losses:  12.163829803466797 7.431053161621094 -0.0
CurrentTrain: epoch  9, batch    33 | loss: 12.1638298Losses:  11.239246368408203 5.682350158691406 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 11.2392464Losses:  9.768499374389648 4.369773864746094 -0.0
CurrentTrain: epoch  9, batch    35 | loss: 9.7684994Losses:  9.932497024536133 4.591989517211914 -0.0
CurrentTrain: epoch  9, batch    36 | loss: 9.9324970Losses:  6.096642017364502 1.4983677864074707 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 6.0966420
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, James Smith moved to Australia to enjoy a more relaxed lifestyle by the beach.  
Head Entity: James Smith  
Tail Entity: Australia  

Relation: person countries of residence  
Context: Despite being born in Canada, Aisha Khan has spent most of her adult life in the vibrant streets of Mumbai, where she works as a fashion designer.  
Head Entity: Aisha Khan  
Tail Entity: India  

Relation: person countries of residence  
Context: After completing his studies in Paris, Thomas Müller decided to settle in Germany, where he could be closer to his family and friends.  
Head Entity: Thomas Müller  
Tail Entity: Germany  

Relation: person countries of residence  
Context: Having lived in various countries throughout her career, Dr. Emily Chen currently resides in Singapore, where she leads a research team in biotechnology.  
Head Entity: Dr. Emily Chen  
Tail Entity: Singapore  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` customer relationships are the backbone of banking , and banks are constantly striving to meet customer demands , '' said peter garuccio , spokesman for the american bankers association .
Head Entity: american bankers association
Tail Entity: peter garuccio
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
1. Relation: organization top members employees  
   Context: "As the CEO of the tech giant, she has been instrumental in driving innovation," said a company spokesperson about Jane Doe, who leads the organization.  
   Head Entity: tech giant  
   Tail Entity: Jane Doe  

2. Relation: organization top members employees  
   Context: "The board of directors is pleased to announce the appointment of John Smith as the new CFO," stated the press release from the financial services firm.  
   Head Entity: financial services firm  
   Tail Entity: John Smith  

3. Relation: organization top members employees  
   Context: "During the annual meeting, the president of the nonprofit organization highlighted the contributions of their lead researcher," reported the local news outlet.  
   Head Entity: nonprofit organization  
   Tail Entity: lead researcher  

4. Relation: organization top members employees  
   Context: "The founder of the startup expressed gratitude towards her team for their hard work and dedication," said a representative during the launch event.  
   Head Entity: startup  
   Tail Entity: founder  

5. Relation: organization top members employees  
   Context: "In a recent interview, the director of the environmental agency praised the efforts of their lead scientist in tackling climate change," noted the environmental journal.  
   Head Entity: environmental agency  
   Tail Entity: lead scientist  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: bond insurer mbia inc. surprised wall street on wednesday with a second-quarter profit , which it says was partly fueled by the addition of new clients in its advisory business .
Head Entity: mbia inc.
Tail Entity: wall street
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The tech giant Google announced its partnership with several startups to enhance its cloud services, showcasing its commitment to innovation in the industry.  
Head Entity: Google  
Tail Entity: startups  

Relation: organization member of  
Context: The United Nations has welcomed new member states into its assembly, emphasizing the importance of global cooperation and peacekeeping efforts.  
Head Entity: United Nations  
Tail Entity: member states  

Relation: organization member of  
Context: The International Olympic Committee has included several new sports in the upcoming games, reflecting its goal to engage a wider audience and promote diversity.  
Head Entity: International Olympic Committee  
Tail Entity: new sports  

Relation: organization member of  
Context: The World Health Organization has collaborated with various health agencies to combat the recent outbreak, highlighting the need for a united front in public health.  
Head Entity: World Health Organization  
Tail Entity: health agencies  

Relation: organization member of  
Context: The European Union has expanded its membership to include several Eastern European countries, aiming to strengthen economic ties and political stability in the region.  
Head Entity: European Union  
Tail Entity: Eastern European countries  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: -- pascal yoadimnadji , 56 , the chadian prime minister , following a brain hemorrhage in paris .
Head Entity: pascal yoadimnadji
Tail Entity: chadian
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: -- angela merkel, the former chancellor of germany, was born in hamburg.  
Head Entity: angela merkel  
Tail Entity: german  

Relation: person origin  
Context: -- serena williams, a renowned tennis player, has roots in the united states.  
Head Entity: serena williams  
Tail Entity: american  

Relation: person origin  
Context: -- narendra modi, the prime minister of india, hails from gujarat.  
Head Entity: narendra modi  
Tail Entity: indian  

Relation: person origin  
Context: -- rafael nadal, a famous tennis star, represents spain in international competitions.  
Head Entity: rafael nadal  
Tail Entity: spanish  

Relation: person origin  
Context: -- chimamanda ngozi adichie, a celebrated author, was born in enugu, nigeria.  
Head Entity: chimamanda ngozi adichie  
Tail Entity: nigerian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: outgoing ubs ag chairman marcel ospel says the worst is over for subprime problems at switzerland 's largest bank .
Head Entity: marcel ospel
Tail Entity: chairman
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: after years of service, dr. jane smith has been appointed as the new chief medical officer at the hospital.  
Head Entity: dr. jane smith  
Tail Entity: chief medical officer  

Relation: person title  
Context: during the annual meeting, mr. john doe was recognized for his contributions and awarded the title of president of the board.  
Head Entity: mr. john doe  
Tail Entity: president of the board  

Relation: person title  
Context: the university announced that professor emily johnson will take on the role of dean of the faculty starting next semester.  
Head Entity: professor emily johnson  
Tail Entity: dean of the faculty  

Relation: person title  
Context: in a recent interview, ms. sarah connor discussed her new role as the director of marketing for the tech startup.  
Head Entity: ms. sarah connor  
Tail Entity: director of marketing  

Relation: person title  
Context: the company celebrated the promotion of mr. robert brown to the position of chief financial officer after a successful year.  
Head Entity: mr. robert brown  
Tail Entity: chief financial officer  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: commander viliame naupoto , chairman of the fiji pine limited announced the woodchips exports target here tuesday after signing a woodchip sale agreement with japan 's itochu corporation .
Head Entity: itochu corporation
Tail Entity: japan
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: the headquarters of the multinational technology company apple inc. is located in cupertino, california, where it has been a significant player in the tech industry.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization country of headquarters  
Context: the famous automobile manufacturer toyota motor corporation has its main office in toyota city, a vibrant hub in japan known for its automotive innovation.  
Head Entity: toyota motor corporation  
Tail Entity: japan  

Relation: organization country of headquarters  
Context: the global financial services firm jpmorgan chase & co. is headquartered in new york city, which is often considered the financial capital of the world.  
Head Entity: jpmorgan chase & co.  
Tail Entity: united states  

Relation: organization country of headquarters  
Context: the renowned aerospace company boeing has its corporate headquarters in chicago, illinois, where it oversees its extensive operations worldwide.  
Head Entity: boeing  
Tail Entity: united states  

Relation: organization country of headquarters  
Context: the international non-profit organization greenpeace is headquartered in amsterdam, netherlands, where it coordinates its environmental campaigns globally.  
Head Entity: greenpeace  
Tail Entity: netherlands  
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.76%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 85.08%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.35%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 83.90%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 81.64%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.76%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.68%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 85.08%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.35%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 83.90%   
cur_acc:  ['0.8390']
his_acc:  ['0.8390']
Clustering into  4  clusters
Clusters:  [2 1 3 2 0 2 2 0 1 0 0]
Losses:  16.2512264251709 8.389137268066406 1.3904705047607422
CurrentTrain: epoch  0, batch     0 | loss: 16.2512264Losses:  11.448968887329102 3.849780321121216 1.3932607173919678
CurrentTrain: epoch  0, batch     1 | loss: 11.4489689Losses:  14.851541519165039 7.815473556518555 1.444746494293213
CurrentTrain: epoch  1, batch     0 | loss: 14.8515415Losses:  8.840572357177734 1.9755715131759644 1.39227294921875
CurrentTrain: epoch  1, batch     1 | loss: 8.8405724Losses:  13.869214057922363 7.4530439376831055 1.4134485721588135
CurrentTrain: epoch  2, batch     0 | loss: 13.8692141Losses:  8.684781074523926 2.756976366043091 1.4047963619232178
CurrentTrain: epoch  2, batch     1 | loss: 8.6847811Losses:  13.164076805114746 8.094751358032227 1.4094016551971436
CurrentTrain: epoch  3, batch     0 | loss: 13.1640768Losses:  10.794915199279785 5.285341739654541 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 10.7949152Losses:  13.233499526977539 8.028059005737305 1.4206489324569702
CurrentTrain: epoch  4, batch     0 | loss: 13.2334995Losses:  8.699361801147461 3.731003999710083 1.4010331630706787
CurrentTrain: epoch  4, batch     1 | loss: 8.6993618Losses:  10.41648006439209 5.987301826477051 1.427164077758789
CurrentTrain: epoch  5, batch     0 | loss: 10.4164801Losses:  6.2886528968811035 1.7198635339736938 1.3950546979904175
CurrentTrain: epoch  5, batch     1 | loss: 6.2886529Losses:  10.860025405883789 6.319618225097656 1.4116449356079102
CurrentTrain: epoch  6, batch     0 | loss: 10.8600254Losses:  6.307653427124023 2.0872955322265625 1.4187231063842773
CurrentTrain: epoch  6, batch     1 | loss: 6.3076534Losses:  11.078495979309082 6.96336555480957 1.399442434310913
CurrentTrain: epoch  7, batch     0 | loss: 11.0784960Losses:  5.901528835296631 2.914911985397339 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 5.9015288Losses:  11.654745101928711 7.364161491394043 1.4046963453292847
CurrentTrain: epoch  8, batch     0 | loss: 11.6547451Losses:  6.092874526977539 2.60365629196167 1.3945848941802979
CurrentTrain: epoch  8, batch     1 | loss: 6.0928745Losses:  9.405628204345703 5.654797077178955 1.3888630867004395
CurrentTrain: epoch  9, batch     0 | loss: 9.4056282Losses:  4.71395206451416 1.1508523225784302 1.3896815776824951
CurrentTrain: epoch  9, batch     1 | loss: 4.7139521
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After years of living in the bustling city, actor Tom Hanks has decided to settle down in a quieter place, choosing to make his home in the picturesque state of California.  
Head Entity: Tom Hanks  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: The renowned author J.K. Rowling has spent much of her life in Edinburgh, where she found inspiration for her beloved Harry Potter series.  
Head Entity: J.K. Rowling  
Tail Entity: Edinburgh  

Relation: person stateorprovinces of residence  
Context: Following his successful career in the tech industry, Elon Musk has moved to Texas, where he plans to expand his business ventures.  
Head Entity: Elon Musk  
Tail Entity: Texas  

Relation: person stateorprovinces of residence  
Context: After retiring from professional basketball, Michael Jordan chose to reside in North Carolina, where he continues to be involved in the community.  
Head Entity: Michael Jordan  
Tail Entity: North Carolina  

Relation: person stateorprovinces of residence  
Context: The famous singer Adele has made her home in Los Angeles, enjoying the vibrant music scene and sunny weather.  
Head Entity: Adele  
Tail Entity: Los Angeles  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away on July 20, 2020.  
Head Entity: The renowned author  
Tail Entity: July 20, 2020  

Relation: person date of death  
Context: She left this world on March 5, 2018, after a long illness.  
Head Entity: She  
Tail Entity: March 5, 2018  

Relation: person date of death  
Context: His family announced that he died peacefully in his sleep on November 11.  
Head Entity: He  
Tail Entity: November 11  

Relation: person date of death  
Context: The scientist's contributions were celebrated after his death on January 1, 2021.  
Head Entity: The scientist  
Tail Entity: January 1, 2021  

Relation: person date of death  
Context: They mourned her loss, which occurred on September 30, 2019.  
Head Entity: They  
Tail Entity: September 30, 2019  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company Apple has been expanding its workforce significantly, currently employing over 147,000 people across the globe.  
Head Entity: Apple  
Tail Entity: 147,000  

Relation: organization number of employees members  
Context: In 2022, the multinational corporation Amazon reported a staggering number of employees, reaching approximately 1.5 million individuals worldwide.  
Head Entity: Amazon  
Tail Entity: 1.5 million  

Relation: organization number of employees members  
Context: Google, known for its innovative technology, has a workforce that exceeds 156,000 employees as of the latest reports.  
Head Entity: Google  
Tail Entity: 156,000  

Relation: organization number of employees members  
Context: The automotive giant Toyota has a global workforce of around 360,000 employees, contributing to its status as one of the largest car manufacturers.  
Head Entity: Toyota  
Tail Entity: 360,000  

Relation: organization number of employees members  
Context: Microsoft has been steadily increasing its staff and now boasts a total of 181,000 employees, making it one of the largest tech companies in the world.  
Head Entity: Microsoft  
Tail Entity: 181,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: but hyperventilating bloviators jumped all over sen. barbara boxer last week for alluding to secretary of state condoleezza rice 's single status -- as though boxer were accusing rice of botching the iraq war because she 's a spinster .
Head Entity: boxer
Tail Entity: barbara boxer
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The artist known as Prince was a musical genius who changed the landscape of pop music forever.  
Head Entity: Prince  
Tail Entity: Prince Rogers Nelson  

Relation: person alternate names  
Context: Many fans remember the actor as "The Rock," a name he used during his wrestling career before transitioning to Hollywood.  
Head Entity: The Rock  
Tail Entity: Dwayne Johnson  

Relation: person alternate names  
Context: The famous author often referred to as J.K. Rowling has captivated millions with her Harry Potter series.  
Head Entity: J.K. Rowling  
Tail Entity: Joanne Rowling  

Relation: person alternate names  
Context: The legendary musician, often called "Madonna," has been a cultural icon since the 1980s.  
Head Entity: Madonna  
Tail Entity: Madonna Louise Ciccone  

Relation: person alternate names  
Context: The scientist known as Einstein is often credited with developing the theory of relativity.  
Head Entity: Einstein  
Tail Entity: Albert Einstein  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a beautiful ceremony held in new york city, 2015-06-20 15:30:00 utc ------ the couple exchanged vows: john legend and chrissy teigen are now officially husband and wife.  
Head Entity: john legend  
Tail Entity: chrissy teigen  

Relation: person spouse  
Context: during the star-studded event in los angeles, 2019-05-01 18:45:00 utc ------ the couple celebrated their anniversary: blake lively and ryan reynolds have been married for several years now.  
Head Entity: blake lively  
Tail Entity: ryan reynolds  

Relation: person spouse  
Context: at a private beach ceremony in hawaii, 2021-07-10 12:00:00 utc ------ they tied the knot: meghan markle and prince harry are now husband and wife.  
Head Entity: meghan markle  
Tail Entity: prince harry  

Relation: person spouse  
Context: in a lavish wedding held in paris, 2018-09-15 14:00:00 utc ------ the couple said 'I do': emma stone and dave mcary are now officially married.  
Head Entity: emma stone  
Tail Entity: dave mcary  

Relation: person spouse  
Context: at a grand ceremony in rome, 2020-11-11 16:30:00 utc ------ they exchanged rings: gigi hadid and zayn malik have taken their relationship to the next level.  
Head Entity: gigi hadid  
Tail Entity: zayn malik  
Losses:  12.910181045532227 4.229081153869629 1.4993171691894531
MemoryTrain:  epoch  0, batch     0 | loss: 12.9101810Losses:  13.411168098449707 3.512289524078369 3.747190475463867
MemoryTrain:  epoch  0, batch     1 | loss: 13.4111681Losses:  13.47635269165039 2.3302648067474365 3.952422618865967
MemoryTrain:  epoch  0, batch     2 | loss: 13.4763527Losses:  13.796310424804688 3.39709210395813 3.616518497467041
MemoryTrain:  epoch  0, batch     3 | loss: 13.7963104Losses:  8.522554397583008 -0.0 -0.0
MemoryTrain:  epoch  0, batch     4 | loss: 8.5225544Losses:  12.243907928466797 2.995198965072632 3.464810848236084
MemoryTrain:  epoch  1, batch     0 | loss: 12.2439079Losses:  12.536520004272461 2.304964780807495 3.6586434841156006
MemoryTrain:  epoch  1, batch     1 | loss: 12.5365200Losses:  10.22350025177002 2.756852149963379 1.395886778831482
MemoryTrain:  epoch  1, batch     2 | loss: 10.2235003Losses:  11.504481315612793 2.798799514770508 3.3414270877838135
MemoryTrain:  epoch  1, batch     3 | loss: 11.5044813Losses:  5.3048248291015625 -0.0 -0.0
MemoryTrain:  epoch  1, batch     4 | loss: 5.3048248Losses:  10.176301002502441 2.4314465522766113 3.3752527236938477
MemoryTrain:  epoch  2, batch     0 | loss: 10.1763010Losses:  12.011820793151855 2.6240458488464355 3.397108793258667
MemoryTrain:  epoch  2, batch     1 | loss: 12.0118208Losses:  10.45069694519043 1.8349493741989136 3.3745460510253906
MemoryTrain:  epoch  2, batch     2 | loss: 10.4506969Losses:  10.492996215820312 2.299771308898926 3.375819444656372
MemoryTrain:  epoch  2, batch     3 | loss: 10.4929962Losses:  2.035292148590088 -0.0 -0.0
MemoryTrain:  epoch  2, batch     4 | loss: 2.0352921Losses:  9.398815155029297 1.4519035816192627 3.3159070014953613
MemoryTrain:  epoch  3, batch     0 | loss: 9.3988152Losses:  9.404032707214355 2.3077027797698975 1.4079091548919678
MemoryTrain:  epoch  3, batch     1 | loss: 9.4040327Losses:  9.248607635498047 1.8990999460220337 3.4544639587402344
MemoryTrain:  epoch  3, batch     2 | loss: 9.2486076Losses:  9.376344680786133 1.6842155456542969 3.3853344917297363
MemoryTrain:  epoch  3, batch     3 | loss: 9.3763447Losses:  4.418381214141846 -0.0 -0.0
MemoryTrain:  epoch  3, batch     4 | loss: 4.4183812Losses:  10.366313934326172 3.586780548095703 3.342966079711914
MemoryTrain:  epoch  4, batch     0 | loss: 10.3663139Losses:  8.67033576965332 1.8537604808807373 3.338346481323242
MemoryTrain:  epoch  4, batch     1 | loss: 8.6703358Losses:  8.587729454040527 2.8463668823242188 1.4509713649749756
MemoryTrain:  epoch  4, batch     2 | loss: 8.5877295Losses:  12.111077308654785 3.9353749752044678 3.3728294372558594
MemoryTrain:  epoch  4, batch     3 | loss: 12.1110773Losses:  1.8411803245544434 -0.0 -0.0
MemoryTrain:  epoch  4, batch     4 | loss: 1.8411803Losses:  8.868332862854004 2.439453125 3.316711902618408
MemoryTrain:  epoch  5, batch     0 | loss: 8.8683329Losses:  9.438858032226562 2.259305953979492 3.363860607147217
MemoryTrain:  epoch  5, batch     1 | loss: 9.4388580Losses:  8.426987648010254 1.9092891216278076 3.379192352294922
MemoryTrain:  epoch  5, batch     2 | loss: 8.4269876Losses:  8.879443168640137 3.2091827392578125 1.406369924545288
MemoryTrain:  epoch  5, batch     3 | loss: 8.8794432Losses:  4.719599723815918 -0.0 -0.0
MemoryTrain:  epoch  5, batch     4 | loss: 4.7195997Losses:  9.141708374023438 2.8498964309692383 3.3388671875
MemoryTrain:  epoch  6, batch     0 | loss: 9.1417084Losses:  8.657364845275879 2.289712429046631 3.386183738708496
MemoryTrain:  epoch  6, batch     1 | loss: 8.6573648Losses:  8.746419906616211 3.41892147064209 1.4067292213439941
MemoryTrain:  epoch  6, batch     2 | loss: 8.7464199Losses:  8.496217727661133 3.810657262802124 1.4462292194366455
MemoryTrain:  epoch  6, batch     3 | loss: 8.4962177Losses:  2.7746200561523438 -0.0 -0.0
MemoryTrain:  epoch  6, batch     4 | loss: 2.7746201Losses:  9.072847366333008 2.0600216388702393 3.328676700592041
MemoryTrain:  epoch  7, batch     0 | loss: 9.0728474Losses:  7.905437469482422 1.8266074657440186 3.3535807132720947
MemoryTrain:  epoch  7, batch     1 | loss: 7.9054375Losses:  11.73822021484375 5.499772548675537 3.322969913482666
MemoryTrain:  epoch  7, batch     2 | loss: 11.7382202Losses:  8.817267417907715 2.3041865825653076 3.338296890258789
MemoryTrain:  epoch  7, batch     3 | loss: 8.8172674Losses:  1.8467974662780762 -0.0 -0.0
MemoryTrain:  epoch  7, batch     4 | loss: 1.8467975Losses:  9.186408996582031 3.280298948287964 3.3047127723693848
MemoryTrain:  epoch  8, batch     0 | loss: 9.1864090Losses:  8.315256118774414 1.7829926013946533 3.304093837738037
MemoryTrain:  epoch  8, batch     1 | loss: 8.3152561Losses:  9.246904373168945 3.421022891998291 3.3936381340026855
MemoryTrain:  epoch  8, batch     2 | loss: 9.2469044Losses:  7.860626697540283 3.206118583679199 1.4212756156921387
MemoryTrain:  epoch  8, batch     3 | loss: 7.8606267Losses:  3.2175967693328857 -0.0 -0.0
MemoryTrain:  epoch  8, batch     4 | loss: 3.2175968Losses:  8.162679672241211 2.5522701740264893 3.3139116764068604
MemoryTrain:  epoch  9, batch     0 | loss: 8.1626797Losses:  7.680615425109863 1.891865611076355 3.3428850173950195
MemoryTrain:  epoch  9, batch     1 | loss: 7.6806154Losses:  7.972894668579102 2.0104870796203613 3.3271591663360596
MemoryTrain:  epoch  9, batch     2 | loss: 7.9728947Losses:  7.434456825256348 1.5612871646881104 3.321744680404663
MemoryTrain:  epoch  9, batch     3 | loss: 7.4344568Losses:  1.9815199375152588 -0.0 -0.0
MemoryTrain:  epoch  9, batch     4 | loss: 1.9815199
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 40.62%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 70.42%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 79.55%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 78.91%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 79.04%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 83.80%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.91%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 84.79%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 84.88%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 84.47%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 83.46%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 82.14%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 81.42%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 80.57%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 80.59%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.09%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.56%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 81.71%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 81.70%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 81.54%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 81.53%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 81.81%   [EVAL] batch:   45 | acc: 68.75%,  total acc: 81.52%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 81.25%   
cur_acc:  ['0.8390', '0.7042']
his_acc:  ['0.8390', '0.8125']
Clustering into  7  clusters
Clusters:  [2 5 3 1 0 1 2 4 6 0 0 4 2 0 5 2]
Losses:  19.20343589782715 8.92547607421875 3.4665074348449707
CurrentTrain: epoch  0, batch     0 | loss: 19.2034359Losses:  13.229330062866211 4.371801853179932 1.4291194677352905
CurrentTrain: epoch  0, batch     1 | loss: 13.2293301Losses:  17.43929672241211 7.8426594734191895 3.3743250370025635
CurrentTrain: epoch  1, batch     0 | loss: 17.4392967Losses:  12.949426651000977 3.065783977508545 3.3244359493255615
CurrentTrain: epoch  1, batch     1 | loss: 12.9494267Losses:  15.161421775817871 7.247998237609863 3.3419408798217773
CurrentTrain: epoch  2, batch     0 | loss: 15.1614218Losses:  10.383491516113281 2.5304276943206787 3.345144748687744
CurrentTrain: epoch  2, batch     1 | loss: 10.3834915Losses:  14.014718055725098 6.932523727416992 3.3515212535858154
CurrentTrain: epoch  3, batch     0 | loss: 14.0147181Losses:  8.771678924560547 2.2712583541870117 3.377429962158203
CurrentTrain: epoch  3, batch     1 | loss: 8.7716789Losses:  13.361473083496094 6.787337303161621 3.3521389961242676
CurrentTrain: epoch  4, batch     0 | loss: 13.3614731Losses:  8.334016799926758 2.159351110458374 3.3466405868530273
CurrentTrain: epoch  4, batch     1 | loss: 8.3340168Losses:  12.751558303833008 6.584930419921875 3.3323895931243896
CurrentTrain: epoch  5, batch     0 | loss: 12.7515583Losses:  8.064226150512695 1.9947240352630615 3.335628032684326
CurrentTrain: epoch  5, batch     1 | loss: 8.0642262Losses:  13.030324935913086 7.15301513671875 3.3392105102539062
CurrentTrain: epoch  6, batch     0 | loss: 13.0303249Losses:  8.891636848449707 3.0986523628234863 3.349357843399048
CurrentTrain: epoch  6, batch     1 | loss: 8.8916368Losses:  12.30635929107666 6.696441650390625 3.3296308517456055
CurrentTrain: epoch  7, batch     0 | loss: 12.3063593Losses:  7.945216178894043 2.25344181060791 3.3772926330566406
CurrentTrain: epoch  7, batch     1 | loss: 7.9452162Losses:  12.302306175231934 6.801535129547119 3.3604185581207275
CurrentTrain: epoch  8, batch     0 | loss: 12.3023062Losses:  7.9975481033325195 2.693899631500244 3.330582857131958
CurrentTrain: epoch  8, batch     1 | loss: 7.9975481Losses:  11.10272216796875 5.815154075622559 3.3220810890197754
CurrentTrain: epoch  9, batch     0 | loss: 11.1027222Losses:  7.32220983505249 1.8970710039138794 3.36501407623291
CurrentTrain: epoch  9, batch     1 | loss: 7.3222098
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: david 's father had surrendered the child to an orphanage after his wife of 10 years , marita , died in 2005 shortly after giving birth to david .
Head Entity: david
Tail Entity: 2005
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: The famous author, Jane Austen, was born on December 16, 1775, in Steventon, Hampshire.  
Head Entity: Jane Austen  
Tail Entity: December 16, 1775  

Relation: person date of birth  
Context: Albert Einstein was born in the city of Ulm, in the Kingdom of Württemberg, Germany, on March 14, 1879.  
Head Entity: Albert Einstein  
Tail Entity: March 14, 1879  

Relation: person date of birth  
Context: The renowned physicist, Marie Curie, was born in Warsaw, Poland, on November 7, 1867.  
Head Entity: Marie Curie  
Tail Entity: November 7, 1867  

Relation: person date of birth  
Context: William Shakespeare, the famous playwright, was born in Stratford-upon-Avon in April 1564.  
Head Entity: William Shakespeare  
Tail Entity: April 1564  

Relation: person date of birth  
Context: The legendary musician, Elvis Presley, was born on January 8, 1935, in Tupelo, Mississippi.  
Head Entity: Elvis Presley  
Tail Entity: January 8, 1935  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jefferson joseph deblanc sr. was born in lockport , la. , on feb. 15 , 1921 , and grew up in st. martinville .
Head Entity: jefferson joseph deblanc sr.
Tail Entity: la.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: marilyn monroe was born in los angeles, california, on june 1, 1926, and became a famous actress.  
Head Entity: marilyn monroe  
Tail Entity: california  

Relation: person stateorprovince of birth  
Context: barack obama was born in honolulu, hawaii, on august 4, 1961, and later became the 44th president of the united states.  
Head Entity: barack obama  
Tail Entity: hawaii  

Relation: person stateorprovince of birth  
Context: agatha christie was born in torquay, devon, on september 15, 1890, and is known for her detective novels.  
Head Entity: agatha christie  
Tail Entity: devon  

Relation: person stateorprovince of birth  
Context: nelson mandela was born in mvezo, eastern cape, on july 18, 1918, and became a key figure in the fight against apartheid.  
Head Entity: nelson mandela  
Tail Entity: eastern cape  

Relation: person stateorprovince of birth  
Context: frida kahlo was born in coyoacán, mexico city, on july 6, 1907, and is celebrated for her unique painting style.  
Head Entity: frida kahlo  
Tail Entity: mexico city  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: During the family reunion, it was heartwarming to see how much Sarah's father, John, had influenced her life choices.  
Head Entity: Sarah  
Tail Entity: John  

Relation: person parents  
Context: After the ceremony, Emily reflected on how her mother, Lisa, had always been her guiding light through difficult times.  
Head Entity: Emily  
Tail Entity: Lisa  

Relation: person parents  
Context: At the graduation party, Michael proudly introduced his parents, Tom and Rachel, to his friends, highlighting their support throughout his education.  
Head Entity: Michael  
Tail Entity: Tom  

Relation: person parents  
Context: In her memoir, Anna shared stories about her father, Robert, and how he taught her the value of hard work and perseverance.  
Head Entity: Anna  
Tail Entity: Robert  

Relation: person parents  
Context: As the family gathered for dinner, it was clear that David's mother, Mary, had a special bond with him, filled with love and understanding.  
Head Entity: David  
Tail Entity: Mary  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work, Maria finally landed a job at the prestigious tech firm, where she collaborates with some of the brightest minds in the industry.  
Head Entity: Maria  
Tail Entity: prestigious tech firm  

Relation: person employee of  
Context: John has been with the company for over a decade, contributing to numerous successful projects and earning the respect of his colleagues.  
Head Entity: John  
Tail Entity: the company  

Relation: person employee of  
Context: As a lead designer at the fashion house, Sarah showcases her creativity and innovation, making a name for herself in the competitive industry.  
Head Entity: Sarah  
Tail Entity: the fashion house  

Relation: person employee of  
Context: After graduating from university, Tom accepted a position at a well-known financial institution, where he quickly climbed the corporate ladder.  
Head Entity: Tom  
Tail Entity: well-known financial institution  

Relation: person employee of  
Context: Emily's dedication to her role at the non-profit organization has made a significant impact on the community, earning her several awards.  
Head Entity: Emily  
Tail Entity: non-profit organization  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his thrilling novels, passed away peacefully in his sleep at his residence in los angeles, ca.  
Head Entity: john doe  
Tail Entity: ca.  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, died in a hospital in las vegas, nv, surrounded by her family.  
Head Entity: elizabeth taylor  
Tail Entity: nv.  

Relation: person stateorprovince of death  
Context: the famous physicist, albert einstein, took his last breath in a hospital in princeton, nj, where he had lived for many years.  
Head Entity: albert einstein  
Tail Entity: nj.  

Relation: person stateorprovince of death  
Context: the beloved musician, prince, was found unresponsive at his home in minneapolis, mn, leading to an outpouring of grief from fans worldwide.  
Head Entity: prince  
Tail Entity: mn.  

Relation: person stateorprovince of death  
Context: the legendary civil rights leader, martin luther king jr., was assassinated in memphis, tn, a tragic event that shook the nation.  
Head Entity: martin luther king jr.  
Tail Entity: tn.  
Losses:  17.30997085571289 1.3487319946289062 11.02524471282959
MemoryTrain:  epoch  0, batch     0 | loss: 17.3099709Losses:  14.377355575561523 2.8820908069610596 5.94033670425415
MemoryTrain:  epoch  0, batch     1 | loss: 14.3773556Losses:  15.465328216552734 3.96388578414917 5.61206579208374
MemoryTrain:  epoch  0, batch     2 | loss: 15.4653282Losses:  17.044357299804688 1.6676007509231567 10.914051055908203
MemoryTrain:  epoch  0, batch     3 | loss: 17.0443573Losses:  15.531354904174805 1.5149970054626465 8.190576553344727
MemoryTrain:  epoch  0, batch     4 | loss: 15.5313549Losses:  14.453619003295898 0.9438341856002808 8.352295875549316
MemoryTrain:  epoch  0, batch     5 | loss: 14.4536190Losses:  17.128787994384766 1.4602659940719604 10.926671981811523
MemoryTrain:  epoch  1, batch     0 | loss: 17.1287880Losses:  17.583587646484375 1.5621662139892578 11.041979789733887
MemoryTrain:  epoch  1, batch     1 | loss: 17.5835876Losses:  13.22915267944336 3.440077781677246 5.683003902435303
MemoryTrain:  epoch  1, batch     2 | loss: 13.2291527Losses:  17.29338264465332 1.7763506174087524 10.95981502532959
MemoryTrain:  epoch  1, batch     3 | loss: 17.2933826Losses:  14.500324249267578 1.717623233795166 8.140008926391602
MemoryTrain:  epoch  1, batch     4 | loss: 14.5003242Losses:  14.21341609954834 1.3185256719589233 8.111212730407715
MemoryTrain:  epoch  1, batch     5 | loss: 14.2134161Losses:  15.564199447631836 1.3761110305786133 10.853346824645996
MemoryTrain:  epoch  2, batch     0 | loss: 15.5641994Losses:  12.868024826049805 1.0722402334213257 8.140901565551758
MemoryTrain:  epoch  2, batch     1 | loss: 12.8680248Losses:  12.891599655151367 1.2010210752487183 8.155319213867188
MemoryTrain:  epoch  2, batch     2 | loss: 12.8915997Losses:  13.793354988098145 1.4602344036102295 8.128948211669922
MemoryTrain:  epoch  2, batch     3 | loss: 13.7933550Losses:  12.405989646911621 0.8055230379104614 8.120790481567383
MemoryTrain:  epoch  2, batch     4 | loss: 12.4059896Losses:  13.162561416625977 1.6474891901016235 8.159627914428711
MemoryTrain:  epoch  2, batch     5 | loss: 13.1625614Losses:  13.373729705810547 1.9595952033996582 8.112506866455078
MemoryTrain:  epoch  3, batch     0 | loss: 13.3737297Losses:  16.301828384399414 2.101917266845703 10.86328411102295
MemoryTrain:  epoch  3, batch     1 | loss: 16.3018284Losses:  12.731182098388672 1.8565632104873657 8.161928176879883
MemoryTrain:  epoch  3, batch     2 | loss: 12.7311821Losses:  12.033833503723145 1.2684906721115112 8.154906272888184
MemoryTrain:  epoch  3, batch     3 | loss: 12.0338335Losses:  15.17216682434082 1.583763599395752 10.87584400177002
MemoryTrain:  epoch  3, batch     4 | loss: 15.1721668Losses:  16.267810821533203 1.9991238117218018 10.933472633361816
MemoryTrain:  epoch  3, batch     5 | loss: 16.2678108Losses:  11.99582290649414 1.2094686031341553 8.150389671325684
MemoryTrain:  epoch  4, batch     0 | loss: 11.9958229Losses:  10.075275421142578 1.6690096855163574 5.6136698722839355
MemoryTrain:  epoch  4, batch     1 | loss: 10.0752754Losses:  14.141925811767578 0.9983514547348022 10.85680866241455
MemoryTrain:  epoch  4, batch     2 | loss: 14.1419258Losses:  12.100096702575684 1.1246364116668701 8.131513595581055
MemoryTrain:  epoch  4, batch     3 | loss: 12.1000967Losses:  14.877426147460938 1.1152774095535278 10.865588188171387
MemoryTrain:  epoch  4, batch     4 | loss: 14.8774261Losses:  12.682373046875 1.9180421829223633 8.121285438537598
MemoryTrain:  epoch  4, batch     5 | loss: 12.6823730Losses:  11.591652870178223 3.7380824089050293 5.64712381362915
MemoryTrain:  epoch  5, batch     0 | loss: 11.5916529Losses:  9.56724739074707 1.2039883136749268 5.565032005310059
MemoryTrain:  epoch  5, batch     1 | loss: 9.5672474Losses:  13.711395263671875 2.862112045288086 8.096023559570312
MemoryTrain:  epoch  5, batch     2 | loss: 13.7113953Losses:  10.689300537109375 2.6165528297424316 5.586269855499268
MemoryTrain:  epoch  5, batch     3 | loss: 10.6893005Losses:  14.363011360168457 1.281877040863037 10.849451065063477
MemoryTrain:  epoch  5, batch     4 | loss: 14.3630114Losses:  9.62166976928711 1.7790992259979248 5.612893581390381
MemoryTrain:  epoch  5, batch     5 | loss: 9.6216698Losses:  9.509099006652832 1.4387468099594116 5.589807033538818
MemoryTrain:  epoch  6, batch     0 | loss: 9.5090990Losses:  9.340784072875977 1.6277576684951782 5.585931301116943
MemoryTrain:  epoch  6, batch     1 | loss: 9.3407841Losses:  15.78722095489502 2.4835286140441895 10.916942596435547
MemoryTrain:  epoch  6, batch     2 | loss: 15.7872210Losses:  12.619020462036133 1.9729374647140503 8.091657638549805
MemoryTrain:  epoch  6, batch     3 | loss: 12.6190205Losses:  14.66496467590332 1.6350460052490234 10.827213287353516
MemoryTrain:  epoch  6, batch     4 | loss: 14.6649647Losses:  14.454305648803711 1.2120685577392578 10.814271926879883
MemoryTrain:  epoch  6, batch     5 | loss: 14.4543056Losses:  14.841432571411133 1.8790874481201172 10.793280601501465
MemoryTrain:  epoch  7, batch     0 | loss: 14.8414326Losses:  13.721476554870605 0.5481640100479126 10.851161003112793
MemoryTrain:  epoch  7, batch     1 | loss: 13.7214766Losses:  12.057563781738281 1.8926212787628174 8.137062072753906
MemoryTrain:  epoch  7, batch     2 | loss: 12.0575638Losses:  15.431609153747559 2.322556257247925 10.80998706817627
MemoryTrain:  epoch  7, batch     3 | loss: 15.4316092Losses:  13.073484420776367 2.902798652648926 8.110260009765625
MemoryTrain:  epoch  7, batch     4 | loss: 13.0734844Losses:  8.9712495803833 3.168989896774292 3.3239905834198
MemoryTrain:  epoch  7, batch     5 | loss: 8.9712496Losses:  11.95085620880127 1.7015588283538818 8.082035064697266
MemoryTrain:  epoch  8, batch     0 | loss: 11.9508562Losses:  14.029273986816406 1.0143917798995972 10.818394660949707
MemoryTrain:  epoch  8, batch     1 | loss: 14.0292740Losses:  12.227766990661621 2.0898642539978027 8.108530044555664
MemoryTrain:  epoch  8, batch     2 | loss: 12.2277670Losses:  14.610462188720703 1.6955652236938477 10.807000160217285
MemoryTrain:  epoch  8, batch     3 | loss: 14.6104622Losses:  14.409195899963379 1.427704095840454 10.84421443939209
MemoryTrain:  epoch  8, batch     4 | loss: 14.4091959Losses:  9.289161682128906 1.5579166412353516 5.624492645263672
MemoryTrain:  epoch  8, batch     5 | loss: 9.2891617Losses:  9.19821548461914 1.5481245517730713 5.575032711029053
MemoryTrain:  epoch  9, batch     0 | loss: 9.1982155Losses:  12.439123153686523 2.2534472942352295 8.089354515075684
MemoryTrain:  epoch  9, batch     1 | loss: 12.4391232Losses:  14.571123123168945 1.7323757410049438 10.81472396850586
MemoryTrain:  epoch  9, batch     2 | loss: 14.5711231Losses:  11.634096145629883 1.5033758878707886 8.106433868408203
MemoryTrain:  epoch  9, batch     3 | loss: 11.6340961Losses:  14.416516304016113 1.5751419067382812 10.792081832885742
MemoryTrain:  epoch  9, batch     4 | loss: 14.4165163Losses:  14.549951553344727 1.685464859008789 10.82800579071045
MemoryTrain:  epoch  9, batch     5 | loss: 14.5499516
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 87.05%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.95%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 81.58%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 37.50%,  total acc: 84.56%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 83.21%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 82.29%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 81.42%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 81.41%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 81.41%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 81.88%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 81.71%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 81.70%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 80.23%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 78.55%   [EVAL] batch:   44 | acc: 12.50%,  total acc: 77.08%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 75.54%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 74.34%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 74.09%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 74.49%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 76.93%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 77.63%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 77.69%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 77.97%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 78.02%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 77.05%   
cur_acc:  ['0.8390', '0.7042', '0.8705']
his_acc:  ['0.8390', '0.8125', '0.7705']
Clustering into  9  clusters
Clusters:  [1 3 7 2 0 2 1 4 8 0 0 4 1 0 3 1 1 5 1 0 6]
Losses:  16.29437828063965 8.090457916259766 3.3731625080108643
CurrentTrain: epoch  0, batch     0 | loss: 16.2943783Losses:  10.553838729858398 2.6858596801757812 3.4330434799194336
CurrentTrain: epoch  0, batch     1 | loss: 10.5538387Losses:  14.847756385803223 7.325931549072266 3.376352310180664
CurrentTrain: epoch  1, batch     0 | loss: 14.8477564Losses:  10.245896339416504 2.1176507472991943 3.379218101501465
CurrentTrain: epoch  1, batch     1 | loss: 10.2458963Losses:  14.418976783752441 7.083980083465576 3.381800651550293
CurrentTrain: epoch  2, batch     0 | loss: 14.4189768Losses:  8.639062881469727 2.0558834075927734 3.3357625007629395
CurrentTrain: epoch  2, batch     1 | loss: 8.6390629Losses:  13.900731086730957 7.6732587814331055 3.3288278579711914
CurrentTrain: epoch  3, batch     0 | loss: 13.9007311Losses:  9.393522262573242 2.9624147415161133 3.3249964714050293
CurrentTrain: epoch  3, batch     1 | loss: 9.3935223Losses:  12.666125297546387 6.754931449890137 3.326403856277466
CurrentTrain: epoch  4, batch     0 | loss: 12.6661253Losses:  8.511752128601074 2.6274755001068115 3.3084239959716797
CurrentTrain: epoch  4, batch     1 | loss: 8.5117521Losses:  11.642333030700684 6.109199523925781 3.3172597885131836
CurrentTrain: epoch  5, batch     0 | loss: 11.6423330Losses:  7.930990695953369 2.1091792583465576 3.312197208404541
CurrentTrain: epoch  5, batch     1 | loss: 7.9309907Losses:  11.706871032714844 6.1236371994018555 3.311493158340454
CurrentTrain: epoch  6, batch     0 | loss: 11.7068710Losses:  7.264528751373291 1.8636635541915894 3.327620029449463
CurrentTrain: epoch  6, batch     1 | loss: 7.2645288Losses:  11.82552719116211 6.449104309082031 3.3089423179626465
CurrentTrain: epoch  7, batch     0 | loss: 11.8255272Losses:  8.093362808227539 2.4948925971984863 3.31724214553833
CurrentTrain: epoch  7, batch     1 | loss: 8.0933628Losses:  11.031652450561523 5.598415851593018 3.313474655151367
CurrentTrain: epoch  8, batch     0 | loss: 11.0316525Losses:  6.67935848236084 1.240506887435913 3.3140323162078857
CurrentTrain: epoch  8, batch     1 | loss: 6.6793585Losses:  12.113618850708008 6.772812366485596 3.3192734718322754
CurrentTrain: epoch  9, batch     0 | loss: 12.1136189Losses:  8.315457344055176 3.067239999771118 3.3123655319213867
CurrentTrain: epoch  9, batch     1 | loss: 8.3154573
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones found her new home in san francisco, where she works as a software engineer. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith spent a lot of time in boston, where he attended university and made lifelong friends. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  

Relation: person cities of residence  
Context: -lrb- following her promotion, emily davis relocated to seattle, excited about the opportunities in the tech industry. -rrb-  
Head Entity: emily davis  
Tail Entity: seattle  

Relation: person cities of residence  
Context: -lrb- after retiring, john doe decided to settle down in miami, enjoying the warm weather and vibrant culture. -rrb-  
Head Entity: john doe  
Tail Entity: miami  

Relation: person cities of residence  
Context: -lrb- having grown up in new york, alice brown returned to her roots after years of living in los angeles. -rrb-  
Head Entity: alice brown  
Tail Entity: new york  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: her political involvement began early : at cornell , she helped organize local farmers ' cooperatives .
Head Entity: she
Tail Entity: cornell
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After graduating from high school, he enrolled at Stanford University to pursue his degree in computer science.  
Head Entity: he  
Tail Entity: Stanford University  

Relation: person schools attended  
Context: She spent four years at Harvard, where she majored in economics and developed a passion for public policy.  
Head Entity: She  
Tail Entity: Harvard  

Relation: person schools attended  
Context: During his time at the University of Chicago, he participated in various research projects that shaped his career.  
Head Entity: he  
Tail Entity: University of Chicago  

Relation: person schools attended  
Context: After completing her studies at Yale, she went on to work in international relations.  
Head Entity: she  
Tail Entity: Yale  

Relation: person schools attended  
Context: He often reminisces about his days at MIT, where he first discovered his love for engineering.  
Head Entity: He  
Tail Entity: MIT  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
1. Relation: person country of death  
   Context: renowned physicist stephen hawking passed away in cambridge, england.  
   Head Entity: stephen hawking  
   Tail Entity: england  

2. Relation: person country of death  
   Context: famous author gabriel garcia marquez died in mexico city, mexico.  
   Head Entity: gabriel garcia marquez  
   Tail Entity: mexico  

3. Relation: person country of death  
   Context: legendary musician freddie mercury succumbed to aids in london, united kingdom.  
   Head Entity: freddie mercury  
   Tail Entity: united kingdom  

4. Relation: person country of death  
   Context: former south african president nelson mandela passed away in johannesburg, south africa.  
   Head Entity: nelson mandela  
   Tail Entity: south africa  

5. Relation: person country of death  
   Context: iconic actress audrey hepburn died in tolochenaz, switzerland.  
   Head Entity: audrey hepburn  
   Tail Entity: switzerland  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both very talented.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the divorce, he took custody of his two daughters, lily and rose, who are now thriving in school.  
Head Entity: he  
Tail Entity: rose  

Relation: person children  
Context: the famous author often mentioned his son, alex, in interviews, highlighting their close relationship.  
Head Entity: the famous author  
Tail Entity: alex  

Relation: person children  
Context: during the family reunion, she proudly introduced her children, including her youngest, max, who just graduated from high school.  
Head Entity: she  
Tail Entity: max  

Relation: person children  
Context: he often shares stories about his daughter, mia, who is an aspiring artist and has already held her first exhibition.  
Head Entity: he  
Tail Entity: mia  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, which shocked his colleagues at the firm.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith was charged with assault following the altercation at the downtown bar last weekend.  
Head Entity: Smith  
Tail Entity: assault  

Relation: person charges  
Context: In a surprising turn of events, the police revealed that Martinez was charged with fraud after an extensive audit of his business practices.  
Head Entity: Martinez  
Tail Entity: fraud  

Relation: person charges  
Context: Following the protests, the city council announced that Thompson was charged with inciting violence during the demonstration.  
Head Entity: Thompson  
Tail Entity: inciting violence  

Relation: person charges  
Context: The court documents indicated that Lee was charged with theft after being caught on surveillance cameras stealing merchandise from the store.  
Head Entity: Lee  
Tail Entity: theft  
Losses:  16.72012710571289 1.7782933712005615 11.017162322998047
MemoryTrain:  epoch  0, batch     0 | loss: 16.7201271Losses:  16.4340877532959 3.3678293228149414 10.832212448120117
MemoryTrain:  epoch  0, batch     1 | loss: 16.4340878Losses:  18.19803237915039 3.0919673442840576 10.912079811096191
MemoryTrain:  epoch  0, batch     2 | loss: 18.1980324Losses:  19.673837661743164 1.4373860359191895 13.779182434082031
MemoryTrain:  epoch  0, batch     3 | loss: 19.6738377Losses:  11.893903732299805 1.9748659133911133 5.696635723114014
MemoryTrain:  epoch  0, batch     4 | loss: 11.8939037Losses:  15.221168518066406 1.8989372253417969 8.12806224822998
MemoryTrain:  epoch  0, batch     5 | loss: 15.2211685Losses:  10.085376739501953 1.9167133569717407 3.3250105381011963
MemoryTrain:  epoch  0, batch     6 | loss: 10.0853767Losses:  16.52826499938965 1.5349937677383423 10.799531936645508
MemoryTrain:  epoch  0, batch     7 | loss: 16.5282650Losses:  14.842645645141602 0.4836777448654175 10.7926664352417
MemoryTrain:  epoch  1, batch     0 | loss: 14.8426456Losses:  13.979443550109863 2.8336949348449707 8.105533599853516
MemoryTrain:  epoch  1, batch     1 | loss: 13.9794436Losses:  13.811747550964355 1.7557199001312256 8.079660415649414
MemoryTrain:  epoch  1, batch     2 | loss: 13.8117476Losses:  15.43526554107666 3.5002713203430176 8.098884582519531
MemoryTrain:  epoch  1, batch     3 | loss: 15.4352655Losses:  14.058809280395508 1.4400928020477295 8.392545700073242
MemoryTrain:  epoch  1, batch     4 | loss: 14.0588093Losses:  17.29458999633789 1.9060070514678955 10.886752128601074
MemoryTrain:  epoch  1, batch     5 | loss: 17.2945900Losses:  18.278112411499023 1.3960297107696533 13.70079231262207
MemoryTrain:  epoch  1, batch     6 | loss: 18.2781124Losses:  8.810800552368164 1.751927137374878 3.3627982139587402
MemoryTrain:  epoch  1, batch     7 | loss: 8.8108006Losses:  17.66330909729004 0.4830484986305237 13.681633949279785
MemoryTrain:  epoch  2, batch     0 | loss: 17.6633091Losses:  14.913717269897461 0.722324013710022 10.81609058380127
MemoryTrain:  epoch  2, batch     1 | loss: 14.9137173Losses:  14.971290588378906 1.1789889335632324 10.808589935302734
MemoryTrain:  epoch  2, batch     2 | loss: 14.9712906Losses:  16.448745727539062 2.2918789386749268 10.788517951965332
MemoryTrain:  epoch  2, batch     3 | loss: 16.4487457Losses:  14.369436264038086 0.7636111974716187 10.810161590576172
MemoryTrain:  epoch  2, batch     4 | loss: 14.3694363Losses:  17.563602447509766 1.1100081205368042 13.743448257446289
MemoryTrain:  epoch  2, batch     5 | loss: 17.5636024Losses:  12.932229995727539 2.1095516681671143 8.121825218200684
MemoryTrain:  epoch  2, batch     6 | loss: 12.9322300Losses:  9.490240097045898 0.717262864112854 5.629410743713379
MemoryTrain:  epoch  2, batch     7 | loss: 9.4902401Losses:  11.89477825164795 1.082305669784546 8.135554313659668
MemoryTrain:  epoch  3, batch     0 | loss: 11.8947783Losses:  9.913981437683105 2.7965283393859863 3.3121023178100586
MemoryTrain:  epoch  3, batch     1 | loss: 9.9139814Losses:  15.048871994018555 1.2122251987457275 10.828863143920898
MemoryTrain:  epoch  3, batch     2 | loss: 15.0488720Losses:  17.564376831054688 1.5724971294403076 13.671306610107422
MemoryTrain:  epoch  3, batch     3 | loss: 17.5643768Losses:  16.977882385253906 0.7439022064208984 13.690268516540527
MemoryTrain:  epoch  3, batch     4 | loss: 16.9778824Losses:  12.262260437011719 1.7279651165008545 8.129775047302246
MemoryTrain:  epoch  3, batch     5 | loss: 12.2622604Losses:  11.485610961914062 1.0318812131881714 8.075235366821289
MemoryTrain:  epoch  3, batch     6 | loss: 11.4856110Losses:  9.101187705993652 0.6420904397964478 5.608862400054932
MemoryTrain:  epoch  3, batch     7 | loss: 9.1011877Losses:  14.023962020874023 0.9773670434951782 10.792551040649414
MemoryTrain:  epoch  4, batch     0 | loss: 14.0239620Losses:  14.509597778320312 1.3958534002304077 10.84724235534668
MemoryTrain:  epoch  4, batch     1 | loss: 14.5095978Losses:  9.289706230163574 1.303912878036499 5.579164981842041
MemoryTrain:  epoch  4, batch     2 | loss: 9.2897062Losses:  8.754145622253418 0.8067665100097656 5.569056034088135
MemoryTrain:  epoch  4, batch     3 | loss: 8.7541456Losses:  17.540067672729492 1.160078763961792 13.722339630126953
MemoryTrain:  epoch  4, batch     4 | loss: 17.5400677Losses:  9.008062362670898 0.7628529667854309 5.575117111206055
MemoryTrain:  epoch  4, batch     5 | loss: 9.0080624Losses:  18.363420486450195 2.4823718070983887 13.69285774230957
MemoryTrain:  epoch  4, batch     6 | loss: 18.3634205Losses:  16.723859786987305 0.7385843992233276 13.683866500854492
MemoryTrain:  epoch  4, batch     7 | loss: 16.7238598Losses:  13.234931945800781 4.723830223083496 5.602323055267334
MemoryTrain:  epoch  5, batch     0 | loss: 13.2349319Losses:  13.372394561767578 0.5076959133148193 10.795716285705566
MemoryTrain:  epoch  5, batch     1 | loss: 13.3723946Losses:  12.048116683959961 1.8735296726226807 8.076855659484863
MemoryTrain:  epoch  5, batch     2 | loss: 12.0481167Losses:  14.073013305664062 1.2415099143981934 10.802395820617676
MemoryTrain:  epoch  5, batch     3 | loss: 14.0730133Losses:  9.86060905456543 2.105224847793579 5.578394412994385
MemoryTrain:  epoch  5, batch     4 | loss: 9.8606091Losses:  16.713306427001953 0.9909356832504272 13.664910316467285
MemoryTrain:  epoch  5, batch     5 | loss: 16.7133064Losses:  13.682456016540527 0.5184259414672852 10.791184425354004
MemoryTrain:  epoch  5, batch     6 | loss: 13.6824560Losses:  13.979455947875977 0.7756925821304321 10.79000473022461
MemoryTrain:  epoch  5, batch     7 | loss: 13.9794559Losses:  11.93818473815918 1.537250280380249 8.112245559692383
MemoryTrain:  epoch  6, batch     0 | loss: 11.9381847Losses:  11.17344856262207 1.0246695280075073 8.081573486328125
MemoryTrain:  epoch  6, batch     1 | loss: 11.1734486Losses:  14.234734535217285 1.3594534397125244 10.808145523071289
MemoryTrain:  epoch  6, batch     2 | loss: 14.2347345Losses:  14.670175552368164 1.4942140579223633 10.850193977355957
MemoryTrain:  epoch  6, batch     3 | loss: 14.6701756Losses:  9.100799560546875 1.3465083837509155 5.576837539672852
MemoryTrain:  epoch  6, batch     4 | loss: 9.1007996Losses:  14.333118438720703 1.4086315631866455 10.782341957092285
MemoryTrain:  epoch  6, batch     5 | loss: 14.3331184Losses:  12.874283790588379 2.7270336151123047 8.076395988464355
MemoryTrain:  epoch  6, batch     6 | loss: 12.8742838Losses:  16.746440887451172 0.727654755115509 13.702545166015625
MemoryTrain:  epoch  6, batch     7 | loss: 16.7464409Losses:  14.193765640258789 1.0089173316955566 10.868102073669434
MemoryTrain:  epoch  7, batch     0 | loss: 14.1937656Losses:  11.43502140045166 1.2715442180633545 8.08488655090332
MemoryTrain:  epoch  7, batch     1 | loss: 11.4350214Losses:  12.106473922729492 1.9744582176208496 8.08462142944336
MemoryTrain:  epoch  7, batch     2 | loss: 12.1064739Losses:  16.500043869018555 0.7363408803939819 13.671405792236328
MemoryTrain:  epoch  7, batch     3 | loss: 16.5000439Losses:  11.643298149108887 1.3640787601470947 8.115898132324219
MemoryTrain:  epoch  7, batch     4 | loss: 11.6432981Losses:  13.365056991577148 0.5099798440933228 10.78841781616211
MemoryTrain:  epoch  7, batch     5 | loss: 13.3650570Losses:  13.90260124206543 3.8182148933410645 8.078813552856445
MemoryTrain:  epoch  7, batch     6 | loss: 13.9026012Losses:  13.994834899902344 1.2211517095565796 10.853389739990234
MemoryTrain:  epoch  7, batch     7 | loss: 13.9948349Losses:  16.244354248046875 0.5330430269241333 13.665549278259277
MemoryTrain:  epoch  8, batch     0 | loss: 16.2443542Losses:  11.420411109924316 1.3782042264938354 8.088809967041016
MemoryTrain:  epoch  8, batch     1 | loss: 11.4204111Losses:  11.057088851928711 1.013317346572876 8.067094802856445
MemoryTrain:  epoch  8, batch     2 | loss: 11.0570889Losses:  17.068580627441406 1.3924436569213867 13.673929214477539
MemoryTrain:  epoch  8, batch     3 | loss: 17.0685806Losses:  11.889837265014648 1.8785725831985474 8.074874877929688
MemoryTrain:  epoch  8, batch     4 | loss: 11.8898373Losses:  16.4813232421875 0.9316518902778625 13.662481307983398
MemoryTrain:  epoch  8, batch     5 | loss: 16.4813232Losses:  13.650876998901367 0.8485397696495056 10.821751594543457
MemoryTrain:  epoch  8, batch     6 | loss: 13.6508770Losses:  7.152642250061035 1.8547554016113281 3.330780267715454
MemoryTrain:  epoch  8, batch     7 | loss: 7.1526423Losses:  13.747932434082031 1.03401780128479 10.791434288024902
MemoryTrain:  epoch  9, batch     0 | loss: 13.7479324Losses:  13.753936767578125 1.0079360008239746 10.77297592163086
MemoryTrain:  epoch  9, batch     1 | loss: 13.7539368Losses:  11.167019844055176 1.08646559715271 8.11917781829834
MemoryTrain:  epoch  9, batch     2 | loss: 11.1670198Losses:  17.359766006469727 1.8111748695373535 13.647745132446289
MemoryTrain:  epoch  9, batch     3 | loss: 17.3597660Losses:  13.760675430297852 1.0299655199050903 10.775875091552734
MemoryTrain:  epoch  9, batch     4 | loss: 13.7606754Losses:  8.968505859375 1.4677611589431763 5.56540060043335
MemoryTrain:  epoch  9, batch     5 | loss: 8.9685059Losses:  9.100728034973145 1.5727896690368652 5.580686092376709
MemoryTrain:  epoch  9, batch     6 | loss: 9.1007280Losses:  16.217708587646484 0.5591662526130676 13.646676063537598
MemoryTrain:  epoch  9, batch     7 | loss: 16.2177086
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 83.09%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 79.86%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.87%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.05%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 84.96%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 83.90%   [EVAL] batch:   33 | acc: 18.75%,  total acc: 81.99%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 80.18%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 78.47%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 76.69%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 76.48%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 76.44%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.03%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 76.83%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 76.93%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 75.44%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 73.86%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 72.22%   [EVAL] batch:   45 | acc: 6.25%,  total acc: 70.79%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 69.95%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 69.52%   [EVAL] batch:   49 | acc: 50.00%,  total acc: 69.12%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 68.63%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 68.15%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 67.92%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 68.06%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 68.52%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 69.41%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 69.50%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 69.70%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 69.69%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 69.26%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 69.56%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 69.35%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 69.73%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 69.90%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 70.17%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 70.15%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 70.40%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 70.11%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 70.05%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 70.46%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 70.86%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 72.00%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 72.04%   
cur_acc:  ['0.8390', '0.7042', '0.8705', '0.7986']
his_acc:  ['0.8390', '0.8125', '0.7705', '0.7204']
Clustering into  12  clusters
Clusters:  [ 1  4  0  7 11  8  1  3 10  6  2  3  1  2  4  1  1  9  1  2  5  0  0  6
  8  2]
Losses:  21.271053314208984 8.131356239318848 3.9015355110168457
CurrentTrain: epoch  0, batch     0 | loss: 21.2710533Losses:  15.83644962310791 2.9048163890838623 3.810368537902832
CurrentTrain: epoch  0, batch     1 | loss: 15.8364496Losses:  21.01166343688965 9.77256965637207 3.9020941257476807
CurrentTrain: epoch  1, batch     0 | loss: 21.0116634Losses:  18.86278533935547 7.25894832611084 1.784001350402832
CurrentTrain: epoch  1, batch     1 | loss: 18.8627853Losses:  20.72311019897461 8.688796997070312 3.9070334434509277
CurrentTrain: epoch  2, batch     0 | loss: 20.7231102Losses:  14.06279182434082 2.991452217102051 3.8550162315368652
CurrentTrain: epoch  2, batch     1 | loss: 14.0627918Losses:  18.896347045898438 7.350471496582031 3.9507808685302734
CurrentTrain: epoch  3, batch     0 | loss: 18.8963470Losses:  14.011346817016602 2.7396600246429443 3.770249843597412
CurrentTrain: epoch  3, batch     1 | loss: 14.0113468Losses:  18.042783737182617 7.1186394691467285 3.902580499649048
CurrentTrain: epoch  4, batch     0 | loss: 18.0427837Losses:  11.618871688842773 1.7858306169509888 3.815512180328369
CurrentTrain: epoch  4, batch     1 | loss: 11.6188717Losses:  17.639087677001953 7.244117259979248 3.8845629692077637
CurrentTrain: epoch  5, batch     0 | loss: 17.6390877Losses:  11.018194198608398 1.602074146270752 3.7909131050109863
CurrentTrain: epoch  5, batch     1 | loss: 11.0181942Losses:  16.580564498901367 6.672722339630127 3.8773159980773926
CurrentTrain: epoch  6, batch     0 | loss: 16.5805645Losses:  12.053691864013672 2.0725746154785156 3.801032543182373
CurrentTrain: epoch  6, batch     1 | loss: 12.0536919Losses:  18.90090560913086 8.496156692504883 3.8177971839904785
CurrentTrain: epoch  7, batch     0 | loss: 18.9009056Losses:  11.907690048217773 2.5429739952087402 3.829786777496338
CurrentTrain: epoch  7, batch     1 | loss: 11.9076900Losses:  16.33144760131836 7.045733451843262 3.7058629989624023
CurrentTrain: epoch  8, batch     0 | loss: 16.3314476Losses:  13.51838493347168 2.7459962368011475 3.839142322540283
CurrentTrain: epoch  8, batch     1 | loss: 13.5183849Losses:  16.763883590698242 6.974129676818848 3.8382153511047363
CurrentTrain: epoch  9, batch     0 | loss: 16.7638836Losses:  11.358574867248535 2.314683675765991 3.6172380447387695
CurrentTrain: epoch  9, batch     1 | loss: 11.3585749
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: `` firstgroup 's acquisition of laidlaw will considerably enhance firstgroup 's existing activities in north america , which themselves have grown strongly since we first invested in the u.s. in 1999 , '' said firstgroup chief executive moir lockhead .
Head Entity: firstgroup
Tail Entity: laidlaw
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: `` in 2018, the tech giant google announced its acquisition of nest labs, a company specializing in smart home products, which has allowed google to expand its reach in the home automation market. ''  
Head Entity: google  
Tail Entity: nest labs  

Relation: organization subsidiaries  
Context: `` the merger between disney and pixar in 2006 created a powerhouse in animated films, allowing disney to leverage pixar's innovative technology and storytelling expertise. ''  
Head Entity: disney  
Tail Entity: pixar  

Relation: organization subsidiaries  
Context: `` after acquiring the popular video game company, activision blizzard, microsoft significantly bolstered its gaming portfolio and strengthened its position in the industry. ''  
Head Entity: microsoft  
Tail Entity: activision blizzard  

Relation: organization subsidiaries  
Context: `` the recent purchase of whole foods by amazon has transformed the grocery shopping experience, integrating online and offline retail in a unique way. ''  
Head Entity: amazon  
Tail Entity: whole foods  

Relation: organization subsidiaries  
Context: `` when facebook acquired instagram in 2012, it not only expanded its social media influence but also tapped into the growing trend of photo sharing among users. ''  
Head Entity: facebook  
Tail Entity: instagram  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: The tech giant Alphabet Inc. has been making headlines with its innovative projects, but many are unaware that it is a subsidiary of the much larger Google LLC, which has been a leader in the tech industry for over two decades.  
Head Entity: Alphabet Inc.  
Tail Entity: Google LLC  

Relation: organization parents  
Context: In the world of finance, JPMorgan Chase & Co. has a long history, but it is important to note that it operates under the parent company, JPMorgan Chase Bank, which has been a key player in the banking sector since its inception.  
Head Entity: JPMorgan Chase & Co.  
Tail Entity: JPMorgan Chase Bank  

Relation: organization parents  
Context: The popular social media platform Instagram has gained immense popularity, but it is often overlooked that it is owned by the parent company Meta Platforms, Inc., which also owns Facebook and WhatsApp.  
Head Entity: Instagram  
Tail Entity: Meta Platforms, Inc.  

Relation: organization parents  
Context: The renowned car manufacturer Tesla, Inc. has revolutionized the automotive industry, yet it is a subsidiary of the larger entity, Tesla Motors, which was established to focus on electric vehicles.  
Head Entity: Tesla, Inc.  
Tail Entity: Tesla Motors  

Relation: organization parents  
Context: The global fast-food chain McDonald's Corporation is well-known for its burgers and fries, but it is essential to recognize that it operates under the parent company, McDonald's Holdings Company, which oversees its international franchises.  
Head Entity: McDonald's Corporation  
Tail Entity: McDonald's Holdings Company  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health crises.  
Head Entity: WHO  
Tail Entity: World Health Organization  

Relation: organization alternate names  
Context: The National Aeronautics and Space Administration, abbreviated as NASA, is responsible for the nation's civilian space program.  
Head Entity: NASA  
Tail Entity: National Aeronautics and Space Administration  

Relation: organization alternate names  
Context: The Federal Bureau of Investigation, or FBI, is tasked with investigating and enforcing federal laws in the United States.  
Head Entity: FBI  
Tail Entity: Federal Bureau of Investigation  

Relation: organization alternate names  
Context: The United Nations Educational, Scientific and Cultural Organization, known as UNESCO, aims to promote world peace and security through international cooperation.  
Head Entity: UNESCO  
Tail Entity: United Nations Educational, Scientific and Cultural Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant google has announced plans to expand its headquarters in the vibrant city of mountain view, california, which is known for its innovation and tech culture.  
Head Entity: google  
Tail Entity: mountain view  

Relation: organization city of headquarters  
Context: ------ new york 2019-11-10 14:30:00 utc the financial services firm goldman sachs is headquartered in the bustling city of new york, which serves as a global financial hub.  
Head Entity: goldman sachs  
Tail Entity: new york  

Relation: organization city of headquarters  
Context: ------ seattle 2020-07-22 09:45:00 utc amazon, the e-commerce leader, has its headquarters located in the tech-savvy city of seattle, washington, where it continues to grow and innovate.  
Head Entity: amazon  
Tail Entity: seattle  

Relation: organization city of headquarters  
Context: ------ boston 2022-01-05 16:20:00 utc the biotechnology company biogen is based in the historic city of cambridge, massachusetts, which is renowned for its academic institutions and research facilities.  
Head Entity: biogen  
Tail Entity: cambridge  

Relation: organization city of headquarters  
Context: ------ austin 2023-02-18 11:15:00 utc the software company oracle has recently moved its headquarters to the lively city of austin, texas, which is becoming a major tech hub in the country.  
Head Entity: oracle  
Tail Entity: austin  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: During the family reunion, John introduced his sister, Emily, who had just returned from studying abroad.  
Head Entity: John  
Tail Entity: Emily  

Relation: person siblings  
Context: After the game, Sarah celebrated her victory with her brother, Michael, who had been cheering for her from the stands.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person siblings  
Context: In her memoir, Lisa writes fondly about her childhood adventures with her brother, Tom, who always had her back.  
Head Entity: Lisa  
Tail Entity: Tom  

Relation: person siblings  
Context: At the wedding, Anna was thrilled to see her brother, David, who had flown in from another state to be her best man.  
Head Entity: Anna  
Tail Entity: David  

Relation: person siblings  
Context: The documentary featured interviews with Rachel and her sister, Jessica, discussing their close bond and shared experiences growing up.  
Head Entity: Rachel  
Tail Entity: Jessica  
Losses:  18.35870361328125 0.28503942489624023 13.948720932006836
MemoryTrain:  epoch  0, batch     0 | loss: 18.3587036Losses:  19.123706817626953 1.7845394611358643 13.849199295043945
MemoryTrain:  epoch  0, batch     1 | loss: 19.1237068Losses:  16.6821231842041 1.3575689792633057 10.84780216217041
MemoryTrain:  epoch  0, batch     2 | loss: 16.6821232Losses:  18.58136749267578 0.7962221503257751 13.798050880432129
MemoryTrain:  epoch  0, batch     3 | loss: 18.5813675Losses:  15.755766868591309 2.0062003135681152 10.826272964477539
MemoryTrain:  epoch  0, batch     4 | loss: 15.7557669Losses:  16.911941528320312 1.7175620794296265 10.88456916809082
MemoryTrain:  epoch  0, batch     5 | loss: 16.9119415Losses:  21.4193115234375 0.5489000678062439 16.723812103271484
MemoryTrain:  epoch  0, batch     6 | loss: 21.4193115Losses:  21.53050422668457 0.5680574178695679 16.85208511352539
MemoryTrain:  epoch  0, batch     7 | loss: 21.5305042Losses:  23.577486038208008 2.411630868911743 16.700849533081055
MemoryTrain:  epoch  0, batch     8 | loss: 23.5774860Losses:  14.678997039794922 0.5146775245666504 10.980988502502441
MemoryTrain:  epoch  0, batch     9 | loss: 14.6789970Losses:  24.625171661376953 0.47654151916503906 20.225675582885742
MemoryTrain:  epoch  1, batch     0 | loss: 24.6251717Losses:  23.41619110107422 0.26213690638542175 19.97846794128418
MemoryTrain:  epoch  1, batch     1 | loss: 23.4161911Losses:  26.14523696899414 0.45644038915634155 23.167753219604492
MemoryTrain:  epoch  1, batch     2 | loss: 26.1452370Losses:  18.306745529174805 1.2840877771377563 13.756973266601562
MemoryTrain:  epoch  1, batch     3 | loss: 18.3067455Losses:  19.062625885009766 1.7343308925628662 13.880330085754395
MemoryTrain:  epoch  1, batch     4 | loss: 19.0626259Losses:  18.23726463317871 0.5330908894538879 13.740755081176758
MemoryTrain:  epoch  1, batch     5 | loss: 18.2372646Losses:  24.130210876464844 2.5733978748321533 16.729267120361328
MemoryTrain:  epoch  1, batch     6 | loss: 24.1302109Losses:  20.59739112854004 0.24114713072776794 16.82602310180664
MemoryTrain:  epoch  1, batch     7 | loss: 20.5973911Losses:  15.539314270019531 1.059872031211853 10.884786605834961
MemoryTrain:  epoch  1, batch     8 | loss: 15.5393143Losses:  17.298765182495117 0.5798861980438232 13.69868278503418
MemoryTrain:  epoch  1, batch     9 | loss: 17.2987652Losses:  17.942750930786133 1.0547890663146973 13.749253273010254
MemoryTrain:  epoch  2, batch     0 | loss: 17.9427509Losses:  16.4163818359375 2.500654697418213 10.844439506530762
MemoryTrain:  epoch  2, batch     1 | loss: 16.4163818Losses:  20.9650936126709 0.6340609788894653 16.76070785522461
MemoryTrain:  epoch  2, batch     2 | loss: 20.9650936Losses:  16.869937896728516 2.067770004272461 10.826639175415039
MemoryTrain:  epoch  2, batch     3 | loss: 16.8699379Losses:  10.759889602661133 2.6143405437469482 5.565183162689209
MemoryTrain:  epoch  2, batch     4 | loss: 10.7598896Losses:  15.581779479980469 1.7368000745773315 10.831624984741211
MemoryTrain:  epoch  2, batch     5 | loss: 15.5817795Losses:  20.052812576293945 0.483098566532135 16.715925216674805
MemoryTrain:  epoch  2, batch     6 | loss: 20.0528126Losses:  17.14590072631836 3.2263479232788086 10.865357398986816
MemoryTrain:  epoch  2, batch     7 | loss: 17.1459007Losses:  22.862060546875 0.4828891456127167 19.869104385375977
MemoryTrain:  epoch  2, batch     8 | loss: 22.8620605Losses:  13.679031372070312 0.2706756889820099 10.792298316955566
MemoryTrain:  epoch  2, batch     9 | loss: 13.6790314Losses:  22.721378326416016 0.7401688098907471 19.869050979614258
MemoryTrain:  epoch  3, batch     0 | loss: 22.7213783Losses:  26.773174285888672 0.4991304278373718 23.150428771972656
MemoryTrain:  epoch  3, batch     1 | loss: 26.7731743Losses:  14.296630859375 1.2919470071792603 10.85999870300293
MemoryTrain:  epoch  3, batch     2 | loss: 14.2966309Losses:  17.33909797668457 0.7945610880851746 13.754358291625977
MemoryTrain:  epoch  3, batch     3 | loss: 17.3390980Losses:  15.351192474365234 1.3943681716918945 10.835870742797852
MemoryTrain:  epoch  3, batch     4 | loss: 15.3511925Losses:  16.747173309326172 0.5279213786125183 13.76634407043457
MemoryTrain:  epoch  3, batch     5 | loss: 16.7471733Losses:  16.914220809936523 1.090346097946167 13.693452835083008
MemoryTrain:  epoch  3, batch     6 | loss: 16.9142208Losses:  11.774404525756836 1.06625235080719 8.094220161437988
MemoryTrain:  epoch  3, batch     7 | loss: 11.7744045Losses:  18.156566619873047 1.2743713855743408 13.739291191101074
MemoryTrain:  epoch  3, batch     8 | loss: 18.1565666Losses:  10.998623847961426 0.5203869342803955 8.121387481689453
MemoryTrain:  epoch  3, batch     9 | loss: 10.9986238Losses:  17.070388793945312 3.085348129272461 10.883885383605957
MemoryTrain:  epoch  4, batch     0 | loss: 17.0703888Losses:  16.714399337768555 0.5630021691322327 13.722862243652344
MemoryTrain:  epoch  4, batch     1 | loss: 16.7143993Losses:  15.182671546936035 2.2410011291503906 10.86050033569336
MemoryTrain:  epoch  4, batch     2 | loss: 15.1826715Losses:  11.030513763427734 3.1017909049987793 5.599350452423096
MemoryTrain:  epoch  4, batch     3 | loss: 11.0305138Losses:  16.202861785888672 0.24147406220436096 13.678781509399414
MemoryTrain:  epoch  4, batch     4 | loss: 16.2028618Losses:  13.939641952514648 0.7815076112747192 10.841482162475586
MemoryTrain:  epoch  4, batch     5 | loss: 13.9396420Losses:  20.071077346801758 1.0913004875183105 16.754901885986328
MemoryTrain:  epoch  4, batch     6 | loss: 20.0710773Losses:  23.672828674316406 1.856813907623291 19.885746002197266
MemoryTrain:  epoch  4, batch     7 | loss: 23.6728287Losses:  13.982576370239258 0.9906084537506104 10.83283519744873
MemoryTrain:  epoch  4, batch     8 | loss: 13.9825764Losses:  14.066191673278809 1.248582124710083 10.821425437927246
MemoryTrain:  epoch  4, batch     9 | loss: 14.0661917Losses:  22.835067749023438 1.000171184539795 19.862844467163086
MemoryTrain:  epoch  5, batch     0 | loss: 22.8350677Losses:  11.643709182739258 1.3286035060882568 8.120133399963379
MemoryTrain:  epoch  5, batch     1 | loss: 11.6437092Losses:  16.84290885925293 1.0398492813110352 13.728254318237305
MemoryTrain:  epoch  5, batch     2 | loss: 16.8429089Losses:  13.956296920776367 0.8102935552597046 10.794102668762207
MemoryTrain:  epoch  5, batch     3 | loss: 13.9562969Losses:  16.774267196655273 0.8040766716003418 13.673572540283203
MemoryTrain:  epoch  5, batch     4 | loss: 16.7742672Losses:  19.74127960205078 0.9709303379058838 16.67472267150879
MemoryTrain:  epoch  5, batch     5 | loss: 19.7412796Losses:  14.133039474487305 1.0342756509780884 10.8926362991333
MemoryTrain:  epoch  5, batch     6 | loss: 14.1330395Losses:  14.073808670043945 1.076968789100647 10.81825065612793
MemoryTrain:  epoch  5, batch     7 | loss: 14.0738087Losses:  8.954414367675781 1.3450782299041748 5.555050373077393
MemoryTrain:  epoch  5, batch     8 | loss: 8.9544144Losses:  12.956134796142578 2.29416823387146 8.080061912536621
MemoryTrain:  epoch  5, batch     9 | loss: 12.9561348Losses:  16.311691284179688 0.2938244938850403 13.703807830810547
MemoryTrain:  epoch  6, batch     0 | loss: 16.3116913Losses:  19.75969123840332 1.0167912244796753 16.693620681762695
MemoryTrain:  epoch  6, batch     1 | loss: 19.7596912Losses:  14.19042682647705 1.2491967678070068 10.78101634979248
MemoryTrain:  epoch  6, batch     2 | loss: 14.1904268Losses:  13.684553146362305 0.7772133350372314 10.79662036895752
MemoryTrain:  epoch  6, batch     3 | loss: 13.6845531Losses:  17.46424102783203 1.4820530414581299 13.680366516113281
MemoryTrain:  epoch  6, batch     4 | loss: 17.4642410Losses:  11.380392074584961 0.9966880083084106 8.164085388183594
MemoryTrain:  epoch  6, batch     5 | loss: 11.3803921Losses:  13.756675720214844 0.7908519506454468 10.840312957763672
MemoryTrain:  epoch  6, batch     6 | loss: 13.7566757Losses:  19.521564483642578 0.5711146593093872 16.758047103881836
MemoryTrain:  epoch  6, batch     7 | loss: 19.5215645Losses:  14.370190620422363 1.5131773948669434 10.765912055969238
MemoryTrain:  epoch  6, batch     8 | loss: 14.3701906Losses:  19.473148345947266 0.8166937232017517 16.72110939025879
MemoryTrain:  epoch  6, batch     9 | loss: 19.4731483Losses:  14.155121803283691 1.1956067085266113 10.80981731414795
MemoryTrain:  epoch  7, batch     0 | loss: 14.1551218Losses:  19.971599578857422 1.118251085281372 16.6993350982666
MemoryTrain:  epoch  7, batch     1 | loss: 19.9715996Losses:  13.33935260772705 0.5135136246681213 10.80341625213623
MemoryTrain:  epoch  7, batch     2 | loss: 13.3393526Losses:  22.628047943115234 0.7450001835823059 19.856948852539062
MemoryTrain:  epoch  7, batch     3 | loss: 22.6280479Losses:  17.249526977539062 1.3698668479919434 13.669145584106445
MemoryTrain:  epoch  7, batch     4 | loss: 17.2495270Losses:  19.753755569458008 0.9994056820869446 16.695369720458984
MemoryTrain:  epoch  7, batch     5 | loss: 19.7537556Losses:  11.675498008728027 1.3367706537246704 8.111520767211914
MemoryTrain:  epoch  7, batch     6 | loss: 11.6754980Losses:  16.932470321655273 1.0571038722991943 13.709209442138672
MemoryTrain:  epoch  7, batch     7 | loss: 16.9324703Losses:  14.535479545593262 1.7965947389602661 10.78767204284668
MemoryTrain:  epoch  7, batch     8 | loss: 14.5354795Losses:  16.189044952392578 0.5255479216575623 13.714269638061523
MemoryTrain:  epoch  7, batch     9 | loss: 16.1890450Losses:  23.6396427154541 1.8614559173583984 19.817596435546875
MemoryTrain:  epoch  8, batch     0 | loss: 23.6396427Losses:  20.108837127685547 1.4086813926696777 16.697330474853516
MemoryTrain:  epoch  8, batch     1 | loss: 20.1088371Losses:  16.4163761138916 0.7682220935821533 13.70553207397461
MemoryTrain:  epoch  8, batch     2 | loss: 16.4163761Losses:  19.579383850097656 0.9414743185043335 16.725013732910156
MemoryTrain:  epoch  8, batch     3 | loss: 19.5793839Losses:  22.018997192382812 0.24343810975551605 19.826181411743164
MemoryTrain:  epoch  8, batch     4 | loss: 22.0189972Losses:  18.907028198242188 0.23331718146800995 16.705785751342773
MemoryTrain:  epoch  8, batch     5 | loss: 18.9070282Losses:  19.302736282348633 0.7426376342773438 16.673587799072266
MemoryTrain:  epoch  8, batch     6 | loss: 19.3027363Losses:  13.536125183105469 0.5751281976699829 10.794159889221191
MemoryTrain:  epoch  8, batch     7 | loss: 13.5361252Losses:  16.418624877929688 0.7542593479156494 13.721163749694824
MemoryTrain:  epoch  8, batch     8 | loss: 16.4186249Losses:  16.613929748535156 0.9975168108940125 13.69622802734375
MemoryTrain:  epoch  8, batch     9 | loss: 16.6139297Losses:  19.422365188598633 0.7329137921333313 16.68440818786621
MemoryTrain:  epoch  9, batch     0 | loss: 19.4223652Losses:  12.329415321350098 2.3419370651245117 8.08326530456543
MemoryTrain:  epoch  9, batch     1 | loss: 12.3294153Losses:  19.447275161743164 0.9148038625717163 16.677127838134766
MemoryTrain:  epoch  9, batch     2 | loss: 19.4472752Losses:  16.946765899658203 1.3477215766906738 13.676265716552734
MemoryTrain:  epoch  9, batch     3 | loss: 16.9467659Losses:  14.324148178100586 1.4806535243988037 10.820537567138672
MemoryTrain:  epoch  9, batch     4 | loss: 14.3241482Losses:  13.891778945922852 1.0217646360397339 10.804080963134766
MemoryTrain:  epoch  9, batch     5 | loss: 13.8917789Losses:  22.279497146606445 0.48930293321609497 19.847248077392578
MemoryTrain:  epoch  9, batch     6 | loss: 22.2794971Losses:  13.708778381347656 0.9985291957855225 10.806371688842773
MemoryTrain:  epoch  9, batch     7 | loss: 13.7087784Losses:  13.23318862915039 0.4965399205684662 10.81824779510498
MemoryTrain:  epoch  9, batch     8 | loss: 13.2331886Losses:  15.626455307006836 -0.0 13.680299758911133
MemoryTrain:  epoch  9, batch     9 | loss: 15.6264553
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 30.47%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 29.17%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 31.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 34.09%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 34.90%   [EVAL] batch:   12 | acc: 37.50%,  total acc: 35.10%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 38.84%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 41.67%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 44.92%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 47.06%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 48.96%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 49.01%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 50.60%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 50.00%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 84.13%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 77.34%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 75.33%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 76.99%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 80.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.90%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 82.66%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 82.58%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 77.78%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 76.18%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 76.15%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 76.12%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 76.72%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 76.68%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 75.58%   [EVAL] batch:   43 | acc: 6.25%,  total acc: 74.01%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 72.36%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 70.79%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 69.68%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 69.40%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 68.12%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 67.52%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 67.07%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 66.63%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 67.16%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 67.98%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 68.10%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 68.33%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 68.33%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 68.03%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 68.25%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 68.25%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 68.65%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 69.03%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 69.03%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 69.39%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 69.11%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 68.84%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 68.40%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 68.32%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 70.43%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 69.70%   [EVAL] batch:   79 | acc: 6.25%,  total acc: 68.91%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 68.36%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 67.76%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 67.47%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 67.04%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 66.99%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 66.72%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 66.24%   [EVAL] batch:   87 | acc: 68.75%,  total acc: 66.26%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 66.01%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 65.76%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 65.66%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 65.90%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 66.36%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 66.45%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 66.47%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 66.43%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 66.39%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 65.94%   
cur_acc:  ['0.8390', '0.7042', '0.8705', '0.7986', '0.5000']
his_acc:  ['0.8390', '0.8125', '0.7705', '0.7204', '0.6594']
Clustering into  14  clusters
Clusters:  [ 0  4  1  0 13 10  0  2  9  8  3  2  0  3  4  0  0 11  0  3  6  1  1  8
 10  3  0 12  5  7  4]
Losses:  20.619604110717773 7.842496871948242 5.859289646148682
CurrentTrain: epoch  0, batch     0 | loss: 20.6196041Losses:  14.024097442626953 2.3811159133911133 5.59035062789917
CurrentTrain: epoch  0, batch     1 | loss: 14.0240974Losses:  22.1441650390625 10.04281997680664 5.686306953430176
CurrentTrain: epoch  1, batch     0 | loss: 22.1441650Losses:  10.399127006530762 4.9594855308532715 1.4242851734161377
CurrentTrain: epoch  1, batch     1 | loss: 10.3991270Losses:  17.041074752807617 6.236521244049072 5.608548641204834
CurrentTrain: epoch  2, batch     0 | loss: 17.0410748Losses:  12.210533142089844 1.620725154876709 5.64767599105835
CurrentTrain: epoch  2, batch     1 | loss: 12.2105331Losses:  16.013694763183594 6.362191200256348 5.622564792633057
CurrentTrain: epoch  3, batch     0 | loss: 16.0136948Losses:  10.548376083374023 1.3427677154541016 5.603170871734619
CurrentTrain: epoch  3, batch     1 | loss: 10.5483761Losses:  15.444204330444336 6.312026023864746 5.619670867919922
CurrentTrain: epoch  4, batch     0 | loss: 15.4442043Losses:  11.354724884033203 2.0198886394500732 5.598654270172119
CurrentTrain: epoch  4, batch     1 | loss: 11.3547249Losses:  16.794677734375 7.654038429260254 5.577267169952393
CurrentTrain: epoch  5, batch     0 | loss: 16.7946777Losses:  11.324655532836914 3.0318102836608887 5.568974494934082
CurrentTrain: epoch  5, batch     1 | loss: 11.3246555Losses:  15.396387100219727 6.302539825439453 5.589094638824463
CurrentTrain: epoch  6, batch     0 | loss: 15.3963871Losses:  9.729461669921875 1.3669321537017822 5.5730133056640625
CurrentTrain: epoch  6, batch     1 | loss: 9.7294617Losses:  17.05497932434082 7.522542476654053 5.599963665008545
CurrentTrain: epoch  7, batch     0 | loss: 17.0549793Losses:  8.21440315246582 2.5859813690185547 3.3129348754882812
CurrentTrain: epoch  7, batch     1 | loss: 8.2144032Losses:  16.20510482788086 7.347957611083984 5.603813648223877
CurrentTrain: epoch  8, batch     0 | loss: 16.2051048Losses:  10.916122436523438 3.0151760578155518 5.5584635734558105
CurrentTrain: epoch  8, batch     1 | loss: 10.9161224Losses:  16.938549041748047 8.381209373474121 5.571763038635254
CurrentTrain: epoch  9, batch     0 | loss: 16.9385490Losses:  8.895610809326172 4.436708450317383 1.4100208282470703
CurrentTrain: epoch  9, batch     1 | loss: 8.8956108
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: iran 's atomic chief ali akbar salehi has expressed tehran 's readiness to swap 1,200 kilogrammes -lrb- 2,640 pounds -rrb- of low-enriched uranium -lrb- leu -rrb- in one-shot for enriched atomic fuel .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: renowned physicist stephen hawking was born in oxford, england, where he later pursued his studies.  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of birth  
Context: actress natalie portman, who was born in jerusalem, has often spoken about her dual citizenship in israel and the united states.  
Head Entity: natalie portman  
Tail Entity: israel  

Relation: person country of birth  
Context: the famous author mark twain was born in florida, missouri, which greatly influenced his writing style and themes.  
Head Entity: mark twain  
Tail Entity: missouri  

Relation: person country of birth  
Context: the legendary musician freddie mercury was born in zanzibar, which is now part of tanzania, before moving to england.  
Head Entity: freddie mercury  
Tail Entity: tanzania  

Relation: person country of birth  
Context: the celebrated painter pablo picasso was born in malaga, spain, where he developed his early artistic skills.  
Head Entity: pablo picasso  
Tail Entity: spain  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit our official site at https://www.techinnovators.com for the latest updates.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For more information, check out the website of Green Earth Initiative at http://www.greenearth.org.  
Head Entity: Green Earth Initiative  
Tail Entity: http://www.greenearth.org  

Relation: organization website  
Context: The blog post can be found at https://www.culinarycreations.com/blog, which is maintained by Culinary Creations.  
Head Entity: Culinary Creations  
Tail Entity: https://www.culinarycreations.com/blog  

Relation: organization website  
Context: You can learn more about our services by visiting http://www.fitnessworld.com.  
Head Entity: Fitness World  
Tail Entity: http://www.fitnessworld.com  

Relation: organization website  
Context: The latest news and updates are available on the official site: https://www.artisticvisions.org.  
Head Entity: Artistic Visions  
Tail Entity: https://www.artisticvisions.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: ------ liberty media acquired a 41 percent stake in directv in late february by exchanging it for a 16 percent stake in news corp plus $ 625 million -lrb- euro402 5 million -rrb- in cash .
Head Entity: directv
Tail Entity: liberty media
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: In 2020, Amazon acquired a significant share in Zoox, a self-driving car startup, to enhance its logistics capabilities.  
Head Entity: Zoox  
Tail Entity: Amazon  

Relation: organization shareholders  
Context: Tesla announced that it had sold a portion of its shares to a group of private investors, including several venture capital firms.  
Head Entity: Tesla  
Tail Entity: venture capital firms  

Relation: organization shareholders  
Context: In a strategic move, Facebook purchased a 10% stake in Jio Platforms, aiming to expand its presence in the Indian market.  
Head Entity: Jio Platforms  
Tail Entity: Facebook  

Relation: organization shareholders  
Context: The recent merger saw Microsoft acquiring a 15% stake in GitHub, further solidifying its commitment to open-source development.  
Head Entity: GitHub  
Tail Entity: Microsoft  

Relation: organization shareholders  
Context: In a landmark deal, Google invested in SpaceX, acquiring a minority stake to support its satellite internet project.  
Head Entity: SpaceX  
Tail Entity: Google  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: those records include 46 pounds of phone bills of some 10,000 clients of her business , pamela martin and associates , from 1993 to august 2006 , sibley said .
Head Entity: pamela martin and associates
Tail Entity: august 2006
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The city council announced the dissolution of the local arts organization, Creative Minds, effective immediately, following a series of financial difficulties that began in early 2020.  
Head Entity: Creative Minds  
Tail Entity: early 2020  

Relation: organization dissolved  
Context: After years of struggling to maintain funding, the nonprofit group, Helping Hands, officially ceased operations on December 15, 2021, leaving many community programs without support.  
Head Entity: Helping Hands  
Tail Entity: December 15, 2021  

Relation: organization dissolved  
Context: The once-thriving tech startup, Innovatech Solutions, was dissolved in March 2019 due to insurmountable debts and a lack of investor interest.  
Head Entity: Innovatech Solutions  
Tail Entity: March 2019  

Relation: organization dissolved  
Context: Following a lengthy investigation into mismanagement, the board decided to dissolve the charity organization, Hope for Tomorrow, on July 30, 2022.  
Head Entity: Hope for Tomorrow  
Tail Entity: July 30, 2022  

Relation: organization dissolved  
Context: The historical preservation society, Heritage Keepers, announced its dissolution in November 2020 after failing to secure necessary funding for its projects.  
Head Entity: Heritage Keepers  
Tail Entity: November 2020  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: zagat survey , the guide empire that started as a hobby for tim and nina zagat in 1979 as a two-page typed list of new york restaurants compiled from reviews from friends , has been put up for sale , according to people briefed on the decision .
Head Entity: zagat
Tail Entity: tim
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
1. Relation: organization founded by  
   Context: In 1998, Google was founded by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University.  
   Head Entity: Google  
   Tail Entity: Larry Page  

2. Relation: organization founded by  
   Context: The famous social media platform Facebook was created by Mark Zuckerberg along with his college roommates in 2004.  
   Head Entity: Facebook  
   Tail Entity: Mark Zuckerberg  

3. Relation: organization founded by  
   Context: The innovative electric car company Tesla was established by Martin Eberhard and Marc Tarpenning in 2003, paving the way for sustainable transportation.  
   Head Entity: Tesla  
   Tail Entity: Martin Eberhard  

4. Relation: organization founded by  
   Context: The renowned fashion brand Chanel was founded by the iconic designer Coco Chanel in 1910, revolutionizing women's fashion.  
   Head Entity: Chanel  
   Tail Entity: Coco Chanel  

5. Relation: organization founded by  
   Context: The non-profit organization Wikimedia Foundation, which supports Wikipedia, was co-founded by Jimmy Wales and Larry Sanger in 2003.  
   Head Entity: Wikimedia Foundation  
   Tail Entity: Jimmy Wales  
Losses:  11.987944602966309 0.6959629654884338 8.08445930480957
MemoryTrain:  epoch  0, batch     0 | loss: 11.9879446Losses:  17.950653076171875 0.24672333896160126 13.800355911254883
MemoryTrain:  epoch  0, batch     1 | loss: 17.9506531Losses:  15.407962799072266 0.8497312068939209 10.90079402923584
MemoryTrain:  epoch  0, batch     2 | loss: 15.4079628Losses:  24.66029930114746 -0.0 20.183902740478516
MemoryTrain:  epoch  0, batch     3 | loss: 24.6602993Losses:  18.00982666015625 0.8454695343971252 13.67055892944336
MemoryTrain:  epoch  0, batch     4 | loss: 18.0098267Losses:  23.206722259521484 0.46507203578948975 19.827314376831055
MemoryTrain:  epoch  0, batch     5 | loss: 23.2067223Losses:  17.16261100769043 2.317847728729248 10.818015098571777
MemoryTrain:  epoch  0, batch     6 | loss: 17.1626110Losses:  12.351099014282227 1.3539154529571533 8.123579025268555
MemoryTrain:  epoch  0, batch     7 | loss: 12.3510990Losses:  25.939483642578125 1.0103869438171387 20.157642364501953
MemoryTrain:  epoch  0, batch     8 | loss: 25.9394836Losses:  10.549118041992188 1.2895925045013428 5.5727105140686035
MemoryTrain:  epoch  0, batch     9 | loss: 10.5491180Losses:  24.067794799804688 0.7639266848564148 19.90913200378418
MemoryTrain:  epoch  0, batch    10 | loss: 24.0677948Losses:  15.303444862365723 1.1010428667068481 10.887662887573242
MemoryTrain:  epoch  0, batch    11 | loss: 15.3034449Losses:  23.392412185668945 0.8280910849571228 20.01886558532715
MemoryTrain:  epoch  1, batch     0 | loss: 23.3924122Losses:  16.75096893310547 1.6314568519592285 10.831052780151367
MemoryTrain:  epoch  1, batch     1 | loss: 16.7509689Losses:  16.905254364013672 1.0666388273239136 13.668721199035645
MemoryTrain:  epoch  1, batch     2 | loss: 16.9052544Losses:  21.91372299194336 1.6540663242340088 16.810928344726562
MemoryTrain:  epoch  1, batch     3 | loss: 21.9137230Losses:  22.85655975341797 0.717296838760376 19.871448516845703
MemoryTrain:  epoch  1, batch     4 | loss: 22.8565598Losses:  13.668352127075195 2.1028823852539062 8.100601196289062
MemoryTrain:  epoch  1, batch     5 | loss: 13.6683521Losses:  19.187023162841797 0.8356050252914429 13.845890998840332
MemoryTrain:  epoch  1, batch     6 | loss: 19.1870232Losses:  23.949007034301758 0.9574805498123169 19.929218292236328
MemoryTrain:  epoch  1, batch     7 | loss: 23.9490070Losses:  21.1317195892334 0.8598443865776062 16.74077796936035
MemoryTrain:  epoch  1, batch     8 | loss: 21.1317196Losses:  19.6889591217041 1.5237412452697754 13.813701629638672
MemoryTrain:  epoch  1, batch     9 | loss: 19.6889591Losses:  26.451494216918945 0.24968811869621277 23.127952575683594
MemoryTrain:  epoch  1, batch    10 | loss: 26.4514942Losses:  13.485291481018066 -0.0 10.81063175201416
MemoryTrain:  epoch  1, batch    11 | loss: 13.4852915Losses:  11.034045219421387 0.9750023484230042 8.105758666992188
MemoryTrain:  epoch  2, batch     0 | loss: 11.0340452Losses:  16.93348503112793 1.7829205989837646 10.792131423950195
MemoryTrain:  epoch  2, batch     1 | loss: 16.9334850Losses:  29.716093063354492 0.25043708086013794 26.558393478393555
MemoryTrain:  epoch  2, batch     2 | loss: 29.7160931Losses:  12.776095390319824 2.1999549865722656 8.069253921508789
MemoryTrain:  epoch  2, batch     3 | loss: 12.7760954Losses:  22.783906936645508 0.5084397196769714 19.84854507446289
MemoryTrain:  epoch  2, batch     4 | loss: 22.7839069Losses:  14.861299514770508 1.1270637512207031 10.811532974243164
MemoryTrain:  epoch  2, batch     5 | loss: 14.8612995Losses:  20.055479049682617 0.7624542713165283 16.70309066772461
MemoryTrain:  epoch  2, batch     6 | loss: 20.0554790Losses:  20.232131958007812 0.7311052083969116 16.76694679260254
MemoryTrain:  epoch  2, batch     7 | loss: 20.2321320Losses:  23.637096405029297 0.49241429567337036 20.055753707885742
MemoryTrain:  epoch  2, batch     8 | loss: 23.6370964Losses:  19.196809768676758 1.8788334131240845 13.775506973266602
MemoryTrain:  epoch  2, batch     9 | loss: 19.1968098Losses:  22.81732940673828 0.4581851065158844 19.92281150817871
MemoryTrain:  epoch  2, batch    10 | loss: 22.8173294Losses:  9.302038192749023 0.32370495796203613 5.63345193862915
MemoryTrain:  epoch  2, batch    11 | loss: 9.3020382Losses:  19.55622673034668 0.250009685754776 16.7629337310791
MemoryTrain:  epoch  3, batch     0 | loss: 19.5562267Losses:  20.449609756469727 0.7243276834487915 16.67562484741211
MemoryTrain:  epoch  3, batch     1 | loss: 20.4496098Losses:  17.34621810913086 0.7639626264572144 13.729837417602539
MemoryTrain:  epoch  3, batch     2 | loss: 17.3462181Losses:  16.846555709838867 0.7408585548400879 13.674280166625977
MemoryTrain:  epoch  3, batch     3 | loss: 16.8465557Losses:  22.541362762451172 0.25895828008651733 19.824705123901367
MemoryTrain:  epoch  3, batch     4 | loss: 22.5413628Losses:  14.26193618774414 1.3610321283340454 10.79444408416748
MemoryTrain:  epoch  3, batch     5 | loss: 14.2619362Losses:  17.216964721679688 1.1044259071350098 13.713281631469727
MemoryTrain:  epoch  3, batch     6 | loss: 17.2169647Losses:  20.5806827545166 0.8739347457885742 16.741851806640625
MemoryTrain:  epoch  3, batch     7 | loss: 20.5806828Losses:  23.550735473632812 0.8369919061660767 19.910154342651367
MemoryTrain:  epoch  3, batch     8 | loss: 23.5507355Losses:  16.991483688354492 0.4842519164085388 13.796384811401367
MemoryTrain:  epoch  3, batch     9 | loss: 16.9914837Losses:  17.282947540283203 1.1662811040878296 13.691606521606445
MemoryTrain:  epoch  3, batch    10 | loss: 17.2829475Losses:  13.3117036819458 0.30663296580314636 10.813953399658203
MemoryTrain:  epoch  3, batch    11 | loss: 13.3117037Losses:  19.079627990722656 0.2471570074558258 16.674680709838867
MemoryTrain:  epoch  4, batch     0 | loss: 19.0796280Losses:  16.709150314331055 0.49141326546669006 13.734152793884277
MemoryTrain:  epoch  4, batch     1 | loss: 16.7091503Losses:  22.71807098388672 0.539016842842102 19.87213134765625
MemoryTrain:  epoch  4, batch     2 | loss: 22.7180710Losses:  17.988800048828125 1.3968749046325684 13.697952270507812
MemoryTrain:  epoch  4, batch     3 | loss: 17.9888000Losses:  17.13864517211914 1.279850959777832 13.666580200195312
MemoryTrain:  epoch  4, batch     4 | loss: 17.1386452Losses:  17.829944610595703 1.4471451044082642 13.78504467010498
MemoryTrain:  epoch  4, batch     5 | loss: 17.8299446Losses:  19.733816146850586 0.7476375102996826 16.70773696899414
MemoryTrain:  epoch  4, batch     6 | loss: 19.7338161Losses:  19.35490608215332 0.27281028032302856 16.691478729248047
MemoryTrain:  epoch  4, batch     7 | loss: 19.3549061Losses:  19.65081214904785 0.822081446647644 16.69099235534668
MemoryTrain:  epoch  4, batch     8 | loss: 19.6508121Losses:  15.302591323852539 1.471121072769165 10.975933074951172
MemoryTrain:  epoch  4, batch     9 | loss: 15.3025913Losses:  26.20159149169922 0.7599962949752808 23.143529891967773
MemoryTrain:  epoch  4, batch    10 | loss: 26.2015915Losses:  7.848648548126221 0.25469863414764404 5.557656288146973
MemoryTrain:  epoch  4, batch    11 | loss: 7.8486485Losses:  19.443878173828125 0.25773054361343384 16.6865177154541
MemoryTrain:  epoch  5, batch     0 | loss: 19.4438782Losses:  11.519779205322266 1.062063217163086 8.076172828674316
MemoryTrain:  epoch  5, batch     1 | loss: 11.5197792Losses:  16.23249053955078 0.24567759037017822 13.685648918151855
MemoryTrain:  epoch  5, batch     2 | loss: 16.2324905Losses:  14.159140586853027 1.2555887699127197 10.818305015563965
MemoryTrain:  epoch  5, batch     3 | loss: 14.1591406Losses:  23.595718383789062 1.2179557085037231 19.832555770874023
MemoryTrain:  epoch  5, batch     4 | loss: 23.5957184Losses:  13.148818016052246 0.23520848155021667 10.816766738891602
MemoryTrain:  epoch  5, batch     5 | loss: 13.1488180Losses:  20.037731170654297 0.9880082607269287 16.72871971130371
MemoryTrain:  epoch  5, batch     6 | loss: 20.0377312Losses:  13.983417510986328 1.0267306566238403 10.831403732299805
MemoryTrain:  epoch  5, batch     7 | loss: 13.9834175Losses:  23.819124221801758 1.5480713844299316 19.841049194335938
MemoryTrain:  epoch  5, batch     8 | loss: 23.8191242Losses:  16.991552352905273 1.3460683822631836 13.663188934326172
MemoryTrain:  epoch  5, batch     9 | loss: 16.9915524Losses:  19.610126495361328 0.5128812193870544 16.75623893737793
MemoryTrain:  epoch  5, batch    10 | loss: 19.6101265Losses:  7.873593330383301 0.30981338024139404 5.567269802093506
MemoryTrain:  epoch  5, batch    11 | loss: 7.8735933Losses:  23.26760482788086 1.337881326675415 19.85919761657715
MemoryTrain:  epoch  6, batch     0 | loss: 23.2676048Losses:  9.548028945922852 1.4320358037948608 5.553258419036865
MemoryTrain:  epoch  6, batch     1 | loss: 9.5480289Losses:  23.373775482177734 1.2651777267456055 19.840030670166016
MemoryTrain:  epoch  6, batch     2 | loss: 23.3737755Losses:  19.98335838317871 1.0082151889801025 16.759599685668945
MemoryTrain:  epoch  6, batch     3 | loss: 19.9833584Losses:  11.671772003173828 1.612770438194275 8.058089256286621
MemoryTrain:  epoch  6, batch     4 | loss: 11.6717720Losses:  16.238079071044922 0.4948711097240448 13.682853698730469
MemoryTrain:  epoch  6, batch     5 | loss: 16.2380791Losses:  17.1452579498291 1.5568894147872925 13.643824577331543
MemoryTrain:  epoch  6, batch     6 | loss: 17.1452579Losses:  11.586979866027832 1.4369555711746216 8.098980903625488
MemoryTrain:  epoch  6, batch     7 | loss: 11.5869799Losses:  19.99850082397461 1.3704516887664795 16.713314056396484
MemoryTrain:  epoch  6, batch     8 | loss: 19.9985008Losses:  19.205604553222656 0.26164114475250244 16.73954200744629
MemoryTrain:  epoch  6, batch     9 | loss: 19.2056046Losses:  17.254377365112305 1.5931129455566406 13.731254577636719
MemoryTrain:  epoch  6, batch    10 | loss: 17.2543774Losses:  10.861509323120117 0.6706535220146179 8.108282089233398
MemoryTrain:  epoch  6, batch    11 | loss: 10.8615093Losses:  13.82620620727539 1.0149266719818115 10.823546409606934
MemoryTrain:  epoch  7, batch     0 | loss: 13.8262062Losses:  13.450611114501953 0.7316018342971802 10.785245895385742
MemoryTrain:  epoch  7, batch     1 | loss: 13.4506111Losses:  16.369491577148438 0.7136318683624268 13.65728759765625
MemoryTrain:  epoch  7, batch     2 | loss: 16.3694916Losses:  18.73535919189453 -0.0 16.69188117980957
MemoryTrain:  epoch  7, batch     3 | loss: 18.7353592Losses:  16.329355239868164 0.7263466715812683 13.67141342163086
MemoryTrain:  epoch  7, batch     4 | loss: 16.3293552Losses:  23.177906036376953 1.2162773609161377 19.848709106445312
MemoryTrain:  epoch  7, batch     5 | loss: 23.1779060Losses:  25.802776336669922 0.72365403175354 23.12359619140625
MemoryTrain:  epoch  7, batch     6 | loss: 25.8027763Losses:  14.207945823669434 1.230877161026001 10.806146621704102
MemoryTrain:  epoch  7, batch     7 | loss: 14.2079458Losses:  19.87637710571289 1.1881076097488403 16.67116928100586
MemoryTrain:  epoch  7, batch     8 | loss: 19.8763771Losses:  19.1148738861084 0.51793372631073 16.69512939453125
MemoryTrain:  epoch  7, batch     9 | loss: 19.1148739Losses:  16.473827362060547 0.7804015874862671 13.735941886901855
MemoryTrain:  epoch  7, batch    10 | loss: 16.4738274Losses:  13.031831741333008 0.30781757831573486 10.78108024597168
MemoryTrain:  epoch  7, batch    11 | loss: 13.0318317Losses:  22.092208862304688 0.23952960968017578 19.845111846923828
MemoryTrain:  epoch  8, batch     0 | loss: 22.0922089Losses:  22.42167091369629 0.5447388887405396 19.86870765686035
MemoryTrain:  epoch  8, batch     1 | loss: 22.4216709Losses:  19.368764877319336 0.7294816970825195 16.70492172241211
MemoryTrain:  epoch  8, batch     2 | loss: 19.3687649Losses:  18.94847869873047 0.23594069480895996 16.741912841796875
MemoryTrain:  epoch  8, batch     3 | loss: 18.9484787Losses:  16.265151977539062 0.7279390096664429 13.653655052185059
MemoryTrain:  epoch  8, batch     4 | loss: 16.2651520Losses:  13.419681549072266 0.7154077291488647 10.80704116821289
MemoryTrain:  epoch  8, batch     5 | loss: 13.4196815Losses:  14.286104202270508 1.4607338905334473 10.830361366271973
MemoryTrain:  epoch  8, batch     6 | loss: 14.2861042Losses:  19.412368774414062 0.7553335428237915 16.69611358642578
MemoryTrain:  epoch  8, batch     7 | loss: 19.4123688Losses:  23.12074089050293 1.3628727197647095 19.84262466430664
MemoryTrain:  epoch  8, batch     8 | loss: 23.1207409Losses:  16.369945526123047 0.7532843351364136 13.673171997070312
MemoryTrain:  epoch  8, batch     9 | loss: 16.3699455Losses:  18.852859497070312 0.24552270770072937 16.683908462524414
MemoryTrain:  epoch  8, batch    10 | loss: 18.8528595Losses:  7.817623138427734 0.32164302468299866 5.577880382537842
MemoryTrain:  epoch  8, batch    11 | loss: 7.8176231Losses:  19.4567813873291 0.8633548021316528 16.692569732666016
MemoryTrain:  epoch  9, batch     0 | loss: 19.4567814Losses:  16.043960571289062 0.47057104110717773 13.648524284362793
MemoryTrain:  epoch  9, batch     1 | loss: 16.0439606Losses:  16.972305297851562 1.3746390342712402 13.667840003967285
MemoryTrain:  epoch  9, batch     2 | loss: 16.9723053Losses:  19.135169982910156 0.5012581944465637 16.701412200927734
MemoryTrain:  epoch  9, batch     3 | loss: 19.1351700Losses:  22.031322479248047 0.26321470737457275 19.841806411743164
MemoryTrain:  epoch  9, batch     4 | loss: 22.0313225Losses:  13.419729232788086 0.7335530519485474 10.775773048400879
MemoryTrain:  epoch  9, batch     5 | loss: 13.4197292Losses:  22.955551147460938 1.0986087322235107 19.87118148803711
MemoryTrain:  epoch  9, batch     6 | loss: 22.9555511Losses:  16.12677764892578 0.506676435470581 13.673160552978516
MemoryTrain:  epoch  9, batch     7 | loss: 16.1267776Losses:  20.167003631591797 1.551148772239685 16.694480895996094
MemoryTrain:  epoch  9, batch     8 | loss: 20.1670036Losses:  16.14850616455078 0.4888181984424591 13.681909561157227
MemoryTrain:  epoch  9, batch     9 | loss: 16.1485062Losses:  16.38802719116211 0.7695355415344238 13.663185119628906
MemoryTrain:  epoch  9, batch    10 | loss: 16.3880272Losses:  10.499183654785156 0.352198988199234 8.084067344665527
MemoryTrain:  epoch  9, batch    11 | loss: 10.4991837
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 51.25%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 51.04%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 42.97%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 82.39%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 76.79%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 74.61%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 73.96%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 73.86%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 74.73%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 76.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 77.40%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 77.78%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.57%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.31%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 79.58%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 80.04%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 79.55%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 77.57%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 75.89%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 74.48%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 72.80%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 72.70%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 72.76%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 73.32%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 73.66%   [EVAL] batch:   42 | acc: 25.00%,  total acc: 72.53%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 70.88%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 69.31%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 67.80%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 66.76%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 66.33%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 65.75%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 65.56%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 65.02%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 64.74%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 64.81%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 66.38%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 66.63%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 66.50%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 66.63%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 66.99%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 67.02%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 67.33%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 67.26%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 67.65%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 67.66%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 67.32%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 67.08%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 67.01%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 67.91%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 69.16%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:   78 | acc: 0.00%,  total acc: 68.35%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 67.50%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 66.74%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 66.08%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 65.36%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 64.66%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 64.41%   [EVAL] batch:   85 | acc: 43.75%,  total acc: 64.17%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 63.65%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 63.64%   [EVAL] batch:   88 | acc: 37.50%,  total acc: 63.34%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 63.12%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 63.12%   [EVAL] batch:   91 | acc: 75.00%,  total acc: 63.25%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 63.51%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 63.83%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 64.01%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 64.19%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 64.11%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 63.97%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 64.02%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 64.28%   [EVAL] batch:  102 | acc: 25.00%,  total acc: 63.90%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 63.46%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 63.39%   [EVAL] batch:  105 | acc: 37.50%,  total acc: 63.15%   [EVAL] batch:  106 | acc: 6.25%,  total acc: 62.62%   
cur_acc:  ['0.8390', '0.7042', '0.8705', '0.7986', '0.5000', '0.4297']
his_acc:  ['0.8390', '0.8125', '0.7705', '0.7204', '0.6594', '0.6262']
Clustering into  17  clusters
Clusters:  [ 7  2  0  3 16  4  7  1  9  8  5  1  3  5  2  6  7 12  6  5 11  0  0  8
  4  5  3 15 10 13  2  1  0  4 14  6]
Losses:  18.40781593322754 8.221847534179688 5.619884014129639
CurrentTrain: epoch  0, batch     0 | loss: 18.4078159Losses:  12.065816879272461 3.319873094558716 3.351783037185669
CurrentTrain: epoch  0, batch     1 | loss: 12.0658169Losses:  16.09487533569336 6.423511505126953 5.6191864013671875
CurrentTrain: epoch  1, batch     0 | loss: 16.0948753Losses:  9.944894790649414 1.333212971687317 5.574361801147461
CurrentTrain: epoch  1, batch     1 | loss: 9.9448948Losses:  14.901420593261719 6.412984848022461 5.604238033294678
CurrentTrain: epoch  2, batch     0 | loss: 14.9014206Losses:  11.054143905639648 2.134450674057007 5.60253381729126
CurrentTrain: epoch  2, batch     1 | loss: 11.0541439Losses:  14.678781509399414 6.277235507965088 5.593373775482178
CurrentTrain: epoch  3, batch     0 | loss: 14.6787815Losses:  9.977807998657227 1.8286607265472412 5.584889888763428
CurrentTrain: epoch  3, batch     1 | loss: 9.9778080Losses:  14.354711532592773 6.2444963455200195 5.585834980010986
CurrentTrain: epoch  4, batch     0 | loss: 14.3547115Losses:  9.82005500793457 1.975413203239441 5.586133003234863
CurrentTrain: epoch  4, batch     1 | loss: 9.8200550Losses:  16.157272338867188 7.9209160804748535 5.568641662597656
CurrentTrain: epoch  5, batch     0 | loss: 16.1572723Losses:  9.307754516601562 4.129214763641357 3.323899745941162
CurrentTrain: epoch  5, batch     1 | loss: 9.3077545Losses:  13.696748733520508 5.901566028594971 5.571410655975342
CurrentTrain: epoch  6, batch     0 | loss: 13.6967487Losses:  9.520809173583984 1.8804550170898438 5.606555938720703
CurrentTrain: epoch  6, batch     1 | loss: 9.5208092Losses:  13.39719295501709 5.844174385070801 5.5831074714660645
CurrentTrain: epoch  7, batch     0 | loss: 13.3971930Losses:  9.355432510375977 1.7608038187026978 5.570992469787598
CurrentTrain: epoch  7, batch     1 | loss: 9.3554325Losses:  14.179845809936523 6.513786792755127 5.586374759674072
CurrentTrain: epoch  8, batch     0 | loss: 14.1798458Losses:  7.38451623916626 1.9994447231292725 3.307656764984131
CurrentTrain: epoch  8, batch     1 | loss: 7.3845162Losses:  14.68072509765625 7.230984687805176 5.559605598449707
CurrentTrain: epoch  9, batch     0 | loss: 14.6807251Losses:  9.480645179748535 3.9284160137176514 3.308791399002075
CurrentTrain: epoch  9, batch     1 | loss: 9.4806452
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned artist, elena smith, tragically lost her life due to a car accident while returning from an exhibition.  
Head Entity: elena smith  
Tail Entity: car accident  

Relation: person cause of death  
Context: following a long struggle with heart disease, mr. thompson succumbed to his illness last night at the hospital.  
Head Entity: mr. thompson  
Tail Entity: heart disease  

Relation: person cause of death  
Context: the community mourned the loss of their beloved mayor, who died from a sudden stroke during a council meeting.  
Head Entity: the mayor  
Tail Entity: stroke  

Relation: person cause of death  
Context: after a courageous fight against diabetes complications, sarah jones passed away, leaving behind a legacy of kindness.  
Head Entity: sarah jones  
Tail Entity: diabetes complications  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote its agenda.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in political advocacy, representing the interests of the Muslim community in the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  

Relation: organization political religious affiliation  
Context: The National Council of Churches works to unite various Christian denominations in the U.S. to address social justice issues and promote interfaith dialogue.  
Head Entity: National Council of Churches  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Jewish Federation of North America plays a crucial role in supporting Jewish communities and advocating for their rights in political spheres.  
Head Entity: Jewish Federation of North America  
Tail Entity: Jewish  

Relation: organization political religious affiliation  
Context: The Hindu American Foundation has been instrumental in raising awareness about Hindu issues and fostering political engagement among the Hindu community in America.  
Head Entity: Hindu American Foundation  
Tail Entity: Hindu  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: headquartered in seattle, washington, amazon.com, inc. is a leading e-commerce and cloud computing company.  
Head Entity: amazon.com, inc.  
Tail Entity: washington  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is based in suwon, south korea, and is known for its electronics and technology.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization stateorprovince of headquarters  
Context: based in redmond, washington, microsoft corporation is a major player in software development and technology solutions.  
Head Entity: microsoft corporation  
Tail Entity: washington  

Relation: organization stateorprovince of headquarters  
Context: the famous fast-food chain mcdonald's has its headquarters in chicago, illinois, serving millions of customers worldwide.  
Head Entity: mcdonald's  
Tail Entity: illinois  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, and their close relationship despite the challenges of fame.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, uncle bob shared stories about his adventures with his niece, emily, who is now pursuing a career in photography.  
Head Entity: emily  
Tail Entity: uncle bob  

Relation: person other family  
Context: the documentary highlighted the bond between singer taylor swift and her younger brother, austin swift, showcasing their supportive relationship throughout her career.  
Head Entity: austin swift  
Tail Entity: taylor swift  

Relation: person other family  
Context: at the wedding, the bride's father, mr. johnson, gave a heartfelt speech about his daughter, sarah, and her close-knit relationship with her cousin, mike.  
Head Entity: mike  
Tail Entity: sarah  

Relation: person other family  
Context: in her memoir, actress drew barrymore reflects on her childhood and the influence of her grandmother, who played a significant role in her life.  
Head Entity: drew barrymore  
Tail Entity: grandmother
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  

Relation: person city of death  
Context: the famous physicist, albert einstein, died on april 18, 1955, in princeton, new jersey, where he had lived for many years while working at the institute for advanced study.  
Head Entity: albert einstein  
Tail Entity: princeton  

Relation: person city of death  
Context: on january 1, 2020, the beloved actor, kobe bryant, tragically lost his life in a helicopter crash in calabasas, california, shocking fans around the world.  
Head Entity: kobe bryant  
Tail Entity: calabasas  

Relation: person city of death  
Context: the legendary musician, freddie mercury, passed away on november 24, 1991, at his home in london, england, leaving a profound impact on the music industry.  
Head Entity: freddie mercury  
Tail Entity: london  
Losses:  26.709726333618164 0.7257874011993408 23.15677833557129
MemoryTrain:  epoch  0, batch     0 | loss: 26.7097263Losses:  26.52794647216797 0.5110037922859192 23.257736206054688
MemoryTrain:  epoch  0, batch     1 | loss: 26.5279465Losses:  21.064687728881836 0.4986039996147156 16.77846336364746
MemoryTrain:  epoch  0, batch     2 | loss: 21.0646877Losses:  17.939313888549805 0.25045013427734375 13.780590057373047
MemoryTrain:  epoch  0, batch     3 | loss: 17.9393139Losses:  28.346431732177734 0.5150240659713745 23.29045295715332
MemoryTrain:  epoch  0, batch     4 | loss: 28.3464317Losses:  24.62238311767578 0.613588809967041 19.91033363342285
MemoryTrain:  epoch  0, batch     5 | loss: 24.6223831Losses:  26.30986976623535 1.2994239330291748 20.261581420898438
MemoryTrain:  epoch  0, batch     6 | loss: 26.3098698Losses:  23.30883026123047 0.504343569278717 19.862756729125977
MemoryTrain:  epoch  0, batch     7 | loss: 23.3088303Losses:  27.392099380493164 0.2422952651977539 23.102426528930664
MemoryTrain:  epoch  0, batch     8 | loss: 27.3920994Losses:  19.230117797851562 1.0642726421356201 13.741928100585938
MemoryTrain:  epoch  0, batch     9 | loss: 19.2301178Losses:  17.692203521728516 0.7285563349723816 13.675896644592285
MemoryTrain:  epoch  0, batch    10 | loss: 17.6922035Losses:  15.58299446105957 1.9003610610961914 10.832364082336426
MemoryTrain:  epoch  0, batch    11 | loss: 15.5829945Losses:  30.23163604736328 -0.0 26.616668701171875
MemoryTrain:  epoch  0, batch    12 | loss: 30.2316360Losses:  14.126678466796875 0.2809230089187622 10.864877700805664
MemoryTrain:  epoch  0, batch    13 | loss: 14.1266785Losses:  21.038888931274414 0.7449344992637634 16.68782615661621
MemoryTrain:  epoch  1, batch     0 | loss: 21.0388889Losses:  24.30794906616211 1.3024972677230835 19.852428436279297
MemoryTrain:  epoch  1, batch     1 | loss: 24.3079491Losses:  26.733848571777344 0.23070088028907776 23.10611343383789
MemoryTrain:  epoch  1, batch     2 | loss: 26.7338486Losses:  23.52511978149414 0.9662954807281494 19.914264678955078
MemoryTrain:  epoch  1, batch     3 | loss: 23.5251198Losses:  22.77093505859375 0.8508862853050232 16.906444549560547
MemoryTrain:  epoch  1, batch     4 | loss: 22.7709351Losses:  23.066579818725586 0.5292822122573853 19.871952056884766
MemoryTrain:  epoch  1, batch     5 | loss: 23.0665798Losses:  29.218812942504883 0.2349831908941269 26.473989486694336
MemoryTrain:  epoch  1, batch     6 | loss: 29.2188129Losses:  19.91139793395996 0.7324352264404297 16.757328033447266
MemoryTrain:  epoch  1, batch     7 | loss: 19.9113979Losses:  26.762805938720703 0.2790977358818054 23.141674041748047
MemoryTrain:  epoch  1, batch     8 | loss: 26.7628059Losses:  22.403766632080078 0.4919985830783844 19.84946632385254
MemoryTrain:  epoch  1, batch     9 | loss: 22.4037666Losses:  26.393953323364258 0.2510754466056824 23.177032470703125
MemoryTrain:  epoch  1, batch    10 | loss: 26.3939533Losses:  31.216754913330078 1.5074162483215332 26.559507369995117
MemoryTrain:  epoch  1, batch    11 | loss: 31.2167549Losses:  13.577723503112793 0.2273077666759491 10.79758071899414
MemoryTrain:  epoch  1, batch    12 | loss: 13.5777235Losses:  13.378117561340332 0.2712932229042053 10.798791885375977
MemoryTrain:  epoch  1, batch    13 | loss: 13.3781176Losses:  29.780445098876953 0.48011910915374756 26.48005485534668
MemoryTrain:  epoch  2, batch     0 | loss: 29.7804451Losses:  29.690147399902344 0.4572843015193939 26.530799865722656
MemoryTrain:  epoch  2, batch     1 | loss: 29.6901474Losses:  25.663330078125 0.24892225861549377 23.113540649414062
MemoryTrain:  epoch  2, batch     2 | loss: 25.6633301Losses:  23.994098663330078 1.5025718212127686 19.832508087158203
MemoryTrain:  epoch  2, batch     3 | loss: 23.9940987Losses:  26.820585250854492 0.7402799725532532 23.209423065185547
MemoryTrain:  epoch  2, batch     4 | loss: 26.8205853Losses:  20.766029357910156 2.113548517227173 16.680227279663086
MemoryTrain:  epoch  2, batch     5 | loss: 20.7660294Losses:  23.7998046875 1.1058372259140015 19.892438888549805
MemoryTrain:  epoch  2, batch     6 | loss: 23.7998047Losses:  16.51365852355957 0.7783228158950806 13.70598316192627
MemoryTrain:  epoch  2, batch     7 | loss: 16.5136585Losses:  23.619699478149414 1.2040035724639893 19.91943359375
MemoryTrain:  epoch  2, batch     8 | loss: 23.6196995Losses:  23.98900604248047 0.5097413063049316 19.872167587280273
MemoryTrain:  epoch  2, batch     9 | loss: 23.9890060Losses:  22.94925880432129 0.7540843486785889 19.845895767211914
MemoryTrain:  epoch  2, batch    10 | loss: 22.9492588Losses:  23.348953247070312 0.5049440264701843 19.86786460876465
MemoryTrain:  epoch  2, batch    11 | loss: 23.3489532Losses:  27.886962890625 0.5678555965423584 23.36590003967285
MemoryTrain:  epoch  2, batch    12 | loss: 27.8869629Losses:  9.141254425048828 -0.0 5.572561740875244
MemoryTrain:  epoch  2, batch    13 | loss: 9.1412544Losses:  23.5646915435791 0.7450546026229858 19.898244857788086
MemoryTrain:  epoch  3, batch     0 | loss: 23.5646915Losses:  22.7486629486084 0.5114706754684448 19.842914581298828
MemoryTrain:  epoch  3, batch     1 | loss: 22.7486629Losses:  22.71536636352539 0.2542448341846466 19.8686580657959
MemoryTrain:  epoch  3, batch     2 | loss: 22.7153664Losses:  23.180238723754883 0.761031448841095 19.960187911987305
MemoryTrain:  epoch  3, batch     3 | loss: 23.1802387Losses:  23.5318603515625 0.47494813799858093 19.908498764038086
MemoryTrain:  epoch  3, batch     4 | loss: 23.5318604Losses:  17.76761817932129 1.2734835147857666 13.663226127624512
MemoryTrain:  epoch  3, batch     5 | loss: 17.7676182Losses:  30.579177856445312 1.4003840684890747 26.640518188476562
MemoryTrain:  epoch  3, batch     6 | loss: 30.5791779Losses:  36.03614044189453 -0.0 33.510555267333984
MemoryTrain:  epoch  3, batch     7 | loss: 36.0361404Losses:  22.9764404296875 1.0674338340759277 19.820985794067383
MemoryTrain:  epoch  3, batch     8 | loss: 22.9764404Losses:  30.38300895690918 1.0719757080078125 26.68408203125
MemoryTrain:  epoch  3, batch     9 | loss: 30.3830090Losses:  19.650293350219727 0.994726300239563 16.71135902404785
MemoryTrain:  epoch  3, batch    10 | loss: 19.6502934Losses:  19.64942169189453 0.7757641077041626 16.689970016479492
MemoryTrain:  epoch  3, batch    11 | loss: 19.6494217Losses:  23.68562126159668 0.5563051700592041 19.910497665405273
MemoryTrain:  epoch  3, batch    12 | loss: 23.6856213Losses:  10.362255096435547 0.2924940884113312 8.075702667236328
MemoryTrain:  epoch  3, batch    13 | loss: 10.3622551Losses:  32.475616455078125 -0.0 29.95056915283203
MemoryTrain:  epoch  4, batch     0 | loss: 32.4756165Losses:  27.237112045288086 0.8614600896835327 23.178964614868164
MemoryTrain:  epoch  4, batch     1 | loss: 27.2371120Losses:  14.551376342773438 1.3924978971481323 10.7988862991333
MemoryTrain:  epoch  4, batch     2 | loss: 14.5513763Losses:  23.142663955688477 0.5273194313049316 19.920751571655273
MemoryTrain:  epoch  4, batch     3 | loss: 23.1426640Losses:  27.04041290283203 1.1217186450958252 23.198698043823242
MemoryTrain:  epoch  4, batch     4 | loss: 27.0404129Losses:  20.276174545288086 0.7716195583343506 16.68064308166504
MemoryTrain:  epoch  4, batch     5 | loss: 20.2761745Losses:  22.394542694091797 0.4747297465801239 19.85706329345703
MemoryTrain:  epoch  4, batch     6 | loss: 22.3945427Losses:  29.036527633666992 0.49103501439094543 26.463407516479492
MemoryTrain:  epoch  4, batch     7 | loss: 29.0365276Losses:  20.610193252563477 1.0917391777038574 16.718868255615234
MemoryTrain:  epoch  4, batch     8 | loss: 20.6101933Losses:  22.908111572265625 0.5128059983253479 19.913721084594727
MemoryTrain:  epoch  4, batch     9 | loss: 22.9081116Losses:  19.55862045288086 0.7532070875167847 16.76432991027832
MemoryTrain:  epoch  4, batch    10 | loss: 19.5586205Losses:  18.226430892944336 2.5244362354278564 13.67882251739502
MemoryTrain:  epoch  4, batch    11 | loss: 18.2264309Losses:  19.908124923706055 1.0103366374969482 16.69297981262207
MemoryTrain:  epoch  4, batch    12 | loss: 19.9081249Losses:  15.617197036743164 -0.0 13.67051887512207
MemoryTrain:  epoch  4, batch    13 | loss: 15.6171970Losses:  23.173715591430664 1.3880398273468018 19.838247299194336
MemoryTrain:  epoch  5, batch     0 | loss: 23.1737156Losses:  29.235164642333984 0.24705839157104492 26.430862426757812
MemoryTrain:  epoch  5, batch     1 | loss: 29.2351646Losses:  20.01561737060547 0.7670048475265503 16.705612182617188
MemoryTrain:  epoch  5, batch     2 | loss: 20.0156174Losses:  17.29076385498047 1.3691487312316895 13.657523155212402
MemoryTrain:  epoch  5, batch     3 | loss: 17.2907639Losses:  20.253820419311523 1.5380722284317017 16.68839454650879
MemoryTrain:  epoch  5, batch     4 | loss: 20.2538204Losses:  23.169719696044922 1.1432859897613525 19.915260314941406
MemoryTrain:  epoch  5, batch     5 | loss: 23.1697197Losses:  22.91758155822754 1.143188238143921 19.871397018432617
MemoryTrain:  epoch  5, batch     6 | loss: 22.9175816Losses:  25.558059692382812 0.2548322081565857 23.178539276123047
MemoryTrain:  epoch  5, batch     7 | loss: 25.5580597Losses:  25.68744659423828 0.24342066049575806 23.201168060302734
MemoryTrain:  epoch  5, batch     8 | loss: 25.6874466Losses:  20.429475784301758 1.087260127067566 16.738689422607422
MemoryTrain:  epoch  5, batch     9 | loss: 20.4294758Losses:  16.53014373779297 0.4987911581993103 13.808351516723633
MemoryTrain:  epoch  5, batch    10 | loss: 16.5301437Losses:  16.453824996948242 0.7615722417831421 13.683588027954102
MemoryTrain:  epoch  5, batch    11 | loss: 16.4538250Losses:  19.368690490722656 0.49747541546821594 16.70536231994629
MemoryTrain:  epoch  5, batch    12 | loss: 19.3686905Losses:  13.094015121459961 -0.0 10.847870826721191
MemoryTrain:  epoch  5, batch    13 | loss: 13.0940151Losses:  16.898643493652344 0.7410842180252075 13.725144386291504
MemoryTrain:  epoch  6, batch     0 | loss: 16.8986435Losses:  22.13428497314453 0.249350443482399 19.841175079345703
MemoryTrain:  epoch  6, batch     1 | loss: 22.1342850Losses:  13.958080291748047 1.1057947874069214 10.793425559997559
MemoryTrain:  epoch  6, batch     2 | loss: 13.9580803Losses:  22.41094207763672 0.48042353987693787 19.83221435546875
MemoryTrain:  epoch  6, batch     3 | loss: 22.4109421Losses:  25.442951202392578 0.2607993483543396 23.136199951171875
MemoryTrain:  epoch  6, batch     4 | loss: 25.4429512Losses:  16.600120544433594 0.7470371723175049 13.670302391052246
MemoryTrain:  epoch  6, batch     5 | loss: 16.6001205Losses:  19.491182327270508 0.7734367251396179 16.70783233642578
MemoryTrain:  epoch  6, batch     6 | loss: 19.4911823Losses:  19.356430053710938 0.7504130601882935 16.67287826538086
MemoryTrain:  epoch  6, batch     7 | loss: 19.3564301Losses:  25.63239860534668 0.5009407997131348 23.134166717529297
MemoryTrain:  epoch  6, batch     8 | loss: 25.6323986Losses:  21.748210906982422 -0.0 19.836406707763672
MemoryTrain:  epoch  6, batch     9 | loss: 21.7482109Losses:  28.920438766479492 0.46675392985343933 26.476470947265625
MemoryTrain:  epoch  6, batch    10 | loss: 28.9204388Losses:  28.811147689819336 0.24515947699546814 26.47018814086914
MemoryTrain:  epoch  6, batch    11 | loss: 28.8111477Losses:  25.68985366821289 0.2657123804092407 23.171646118164062
MemoryTrain:  epoch  6, batch    12 | loss: 25.6898537Losses:  7.723827838897705 0.2792847156524658 5.562253952026367
MemoryTrain:  epoch  6, batch    13 | loss: 7.7238278Losses:  25.50943374633789 0.23697540163993835 23.121015548706055
MemoryTrain:  epoch  7, batch     0 | loss: 25.5094337Losses:  25.76943588256836 0.5200462341308594 23.08761978149414
MemoryTrain:  epoch  7, batch     1 | loss: 25.7694359Losses:  16.444963455200195 0.8031356334686279 13.673396110534668
MemoryTrain:  epoch  7, batch     2 | loss: 16.4449635Losses:  18.949779510498047 0.2574044466018677 16.69179344177246
MemoryTrain:  epoch  7, batch     3 | loss: 18.9497795Losses:  25.585201263427734 0.5173793435096741 23.13388442993164
MemoryTrain:  epoch  7, batch     4 | loss: 25.5852013Losses:  22.231998443603516 0.4857445955276489 19.84063148498535
MemoryTrain:  epoch  7, batch     5 | loss: 22.2319984Losses:  25.575496673583984 0.4954528212547302 23.122013092041016
MemoryTrain:  epoch  7, batch     6 | loss: 25.5754967Losses:  23.076446533203125 1.3525691032409668 19.832311630249023
MemoryTrain:  epoch  7, batch     7 | loss: 23.0764465Losses:  22.216510772705078 0.4964603781700134 19.82437515258789
MemoryTrain:  epoch  7, batch     8 | loss: 22.2165108Losses:  25.435707092285156 0.2987418472766876 23.103933334350586
MemoryTrain:  epoch  7, batch     9 | loss: 25.4357071Losses:  23.003210067749023 1.2344715595245361 19.84719467163086
MemoryTrain:  epoch  7, batch    10 | loss: 23.0032101Losses:  28.877620697021484 0.28003430366516113 26.479991912841797
MemoryTrain:  epoch  7, batch    11 | loss: 28.8776207Losses:  22.646438598632812 0.862982988357544 19.87793731689453
MemoryTrain:  epoch  7, batch    12 | loss: 22.6464386Losses:  8.08481216430664 0.59663987159729 5.576371669769287
MemoryTrain:  epoch  7, batch    13 | loss: 8.0848122Losses:  32.205631256103516 0.24109607934951782 29.905702590942383
MemoryTrain:  epoch  8, batch     0 | loss: 32.2056313Losses:  20.027881622314453 1.3898112773895264 16.68571662902832
MemoryTrain:  epoch  8, batch     1 | loss: 20.0278816Losses:  23.162853240966797 1.3625662326812744 19.837339401245117
MemoryTrain:  epoch  8, batch     2 | loss: 23.1628532Losses:  22.98326301574707 1.115393877029419 19.901050567626953
MemoryTrain:  epoch  8, batch     3 | loss: 22.9832630Losses:  23.286399841308594 1.384687900543213 19.883054733276367
MemoryTrain:  epoch  8, batch     4 | loss: 23.2863998Losses:  19.916563034057617 1.2438364028930664 16.69587516784668
MemoryTrain:  epoch  8, batch     5 | loss: 19.9165630Losses:  22.19338035583496 0.47960978746414185 19.820648193359375
MemoryTrain:  epoch  8, batch     6 | loss: 22.1933804Losses:  14.160847663879395 1.3762941360473633 10.7858304977417
MemoryTrain:  epoch  8, batch     7 | loss: 14.1608477Losses:  23.057842254638672 1.2577784061431885 19.815488815307617
MemoryTrain:  epoch  8, batch     8 | loss: 23.0578423Losses:  25.46619987487793 0.5093311071395874 23.0722713470459
MemoryTrain:  epoch  8, batch     9 | loss: 25.4661999Losses:  22.103816986083984 0.24784697592258453 19.82339096069336
MemoryTrain:  epoch  8, batch    10 | loss: 22.1038170Losses:  22.49957275390625 0.719801127910614 19.852371215820312
MemoryTrain:  epoch  8, batch    11 | loss: 22.4995728Losses:  17.12740707397461 1.5495436191558838 13.671960830688477
MemoryTrain:  epoch  8, batch    12 | loss: 17.1274071Losses:  10.720706939697266 0.519710898399353 8.068904876708984
MemoryTrain:  epoch  8, batch    13 | loss: 10.7207069Losses:  23.122468948364258 1.261486291885376 19.83104705810547
MemoryTrain:  epoch  9, batch     0 | loss: 23.1224689Losses:  19.472911834716797 0.8636178374290466 16.680463790893555
MemoryTrain:  epoch  9, batch     1 | loss: 19.4729118Losses:  28.81839370727539 0.49084407091140747 26.430587768554688
MemoryTrain:  epoch  9, batch     2 | loss: 28.8183937Losses:  19.106807708740234 0.4753509759902954 16.691225051879883
MemoryTrain:  epoch  9, batch     3 | loss: 19.1068077Losses:  20.458492279052734 1.8267436027526855 16.68018913269043
MemoryTrain:  epoch  9, batch     4 | loss: 20.4584923Losses:  25.279682159423828 0.2275378406047821 23.121206283569336
MemoryTrain:  epoch  9, batch     5 | loss: 25.2796822Losses:  21.99329948425293 0.2587732970714569 19.822479248046875
MemoryTrain:  epoch  9, batch     6 | loss: 21.9932995Losses:  24.94678497314453 -0.0 23.082311630249023
MemoryTrain:  epoch  9, batch     7 | loss: 24.9467850Losses:  29.26435089111328 0.8813018798828125 26.455257415771484
MemoryTrain:  epoch  9, batch     8 | loss: 29.2643509Losses:  26.127029418945312 1.0142778158187866 23.109106063842773
MemoryTrain:  epoch  9, batch     9 | loss: 26.1270294Losses:  25.32080841064453 0.2576828598976135 23.129751205444336
MemoryTrain:  epoch  9, batch    10 | loss: 25.3208084Losses:  16.924318313598633 1.3399319648742676 13.667939186096191
MemoryTrain:  epoch  9, batch    11 | loss: 16.9243183Losses:  22.240949630737305 0.4995078444480896 19.8309383392334
MemoryTrain:  epoch  9, batch    12 | loss: 22.2409496Losses:  12.695182800292969 -0.0 10.782146453857422
MemoryTrain:  epoch  9, batch    13 | loss: 12.6951828
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 70.31%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 67.05%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 66.83%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 72.73%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 74.48%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 69.20%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 69.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 69.12%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 69.38%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 71.59%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 72.55%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 76.16%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 77.92%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 78.23%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.71%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 77.84%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 75.92%   [EVAL] batch:   34 | acc: 12.50%,  total acc: 74.11%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 72.40%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 70.78%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 70.56%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 71.19%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 71.58%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 70.20%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 68.61%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 67.08%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 65.62%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 64.63%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 64.45%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 63.65%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 62.88%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 61.89%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 61.06%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 60.38%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 60.53%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 61.14%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 61.72%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 62.06%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 62.39%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 62.71%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 62.71%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 61.99%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 61.79%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 61.71%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 61.52%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 61.15%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 61.08%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 60.82%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 61.21%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 61.41%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 61.34%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 61.18%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 61.20%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 61.73%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 62.25%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 63.72%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 63.86%   [EVAL] batch:   78 | acc: 6.25%,  total acc: 63.13%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 62.34%   [EVAL] batch:   80 | acc: 6.25%,  total acc: 61.65%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 61.05%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 60.47%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 59.82%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 59.63%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 59.38%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 58.98%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 58.95%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 58.64%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 58.33%   [EVAL] batch:   90 | acc: 31.25%,  total acc: 58.04%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 57.40%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 56.79%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 56.32%   [EVAL] batch:   94 | acc: 6.25%,  total acc: 55.79%   [EVAL] batch:   95 | acc: 6.25%,  total acc: 55.27%   [EVAL] batch:   96 | acc: 0.00%,  total acc: 54.70%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 54.34%   [EVAL] batch:   98 | acc: 12.50%,  total acc: 53.91%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 53.87%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 54.33%   [EVAL] batch:  101 | acc: 37.50%,  total acc: 54.17%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 53.94%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 53.61%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 53.63%   [EVAL] batch:  105 | acc: 31.25%,  total acc: 53.42%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 53.15%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 53.24%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 53.27%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 53.35%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 53.72%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 54.13%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 54.42%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 54.55%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 54.46%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 54.42%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 54.49%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 54.71%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 54.94%   
cur_acc:  ['0.8390', '0.7042', '0.8705', '0.7986', '0.5000', '0.4297', '0.6683']
his_acc:  ['0.8390', '0.8125', '0.7705', '0.7204', '0.6594', '0.6262', '0.5494']
Clustering into  19  clusters
Clusters:  [ 4  1  0 11 17 10  4  3  9  8 16  7  4 16  1 18  4 15 18 16 12  0  0  8
 10 16  4 13  5  2  1  3  6 10 14 18  2  7  4  0  6]
Losses:  19.845090866088867 9.102636337280273 5.586721420288086
CurrentTrain: epoch  0, batch     0 | loss: 19.8450909Losses:  12.427263259887695 4.376158237457275 3.3183722496032715
CurrentTrain: epoch  0, batch     1 | loss: 12.4272633Losses:  17.631010055541992 7.828444480895996 5.574766159057617
CurrentTrain: epoch  1, batch     0 | loss: 17.6310101Losses:  9.587540626525879 2.9397425651550293 3.3708839416503906
CurrentTrain: epoch  1, batch     1 | loss: 9.5875406Losses:  15.200103759765625 6.295784950256348 5.610635280609131
CurrentTrain: epoch  2, batch     0 | loss: 15.2001038Losses:  11.005207061767578 2.2342305183410645 5.576262950897217
CurrentTrain: epoch  2, batch     1 | loss: 11.0052071Losses:  14.273981094360352 6.064055442810059 5.600772857666016
CurrentTrain: epoch  3, batch     0 | loss: 14.2739811Losses:  10.209705352783203 1.9262362718582153 5.560046672821045
CurrentTrain: epoch  3, batch     1 | loss: 10.2097054Losses:  14.610503196716309 6.655159950256348 5.578176498413086
CurrentTrain: epoch  4, batch     0 | loss: 14.6105032Losses:  11.272178649902344 3.325510263442993 5.601489543914795
CurrentTrain: epoch  4, batch     1 | loss: 11.2721786Losses:  14.5611572265625 6.751236915588379 5.585970401763916
CurrentTrain: epoch  5, batch     0 | loss: 14.5611572Losses:  10.759255409240723 3.0856144428253174 5.561138153076172
CurrentTrain: epoch  5, batch     1 | loss: 10.7592554Losses:  14.244604110717773 6.680674076080322 5.586445331573486
CurrentTrain: epoch  6, batch     0 | loss: 14.2446041Losses:  7.712532043457031 2.5437216758728027 3.3168444633483887
CurrentTrain: epoch  6, batch     1 | loss: 7.7125320Losses:  13.62649917602539 6.126132965087891 5.573937892913818
CurrentTrain: epoch  7, batch     0 | loss: 13.6264992Losses:  9.588813781738281 2.2280361652374268 5.559354305267334
CurrentTrain: epoch  7, batch     1 | loss: 9.5888138Losses:  13.05521011352539 5.635220527648926 5.557703018188477
CurrentTrain: epoch  8, batch     0 | loss: 13.0552101Losses:  8.96178150177002 1.634318232536316 5.553576946258545
CurrentTrain: epoch  8, batch     1 | loss: 8.9617815Losses:  12.456727981567383 5.108997821807861 5.56063985824585
CurrentTrain: epoch  9, batch     0 | loss: 12.4567280Losses:  8.351188659667969 1.0726697444915771 5.555426597595215
CurrentTrain: epoch  9, batch     1 | loss: 8.3511887
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
1. Relation: organization founded  
   Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, innovative solutions, which has since become a leader in software development.  
   Head Entity: innovative solutions  
   Tail Entity: 1998  

2. Relation: organization founded  
   Context: After years of research and development, the non-profit organization green earth was officially launched in 2010 to promote environmental sustainability.  
   Head Entity: green earth  
   Tail Entity: 2010  

3. Relation: organization founded  
   Context: The famous chef opened his first restaurant, culinary delights, in 2015, which quickly gained popularity for its unique fusion cuisine.  
   Head Entity: culinary delights  
   Tail Entity: 2015  

4. Relation: organization founded  
   Context: In 2001, a group of artists and activists founded the cultural collective, art for change, to support local artists and promote social justice through art.  
   Head Entity: art for change  
   Tail Entity: 2001  

5. Relation: organization founded  
   Context: The tech company, future innovations, was established in 2020 by a team of visionary developers aiming to revolutionize the way we interact with technology.  
   Head Entity: future innovations  
   Tail Entity: 2020  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, emily decided to throw a big party for her friends.  
Head Entity: emily  
Tail Entity: 30  

Relation: person age  
Context: the renowned scientist dr. jane goodall is currently 89 years old.  
Head Entity: dr. jane goodall  
Tail Entity: 89  

Relation: person age  
Context: last year, my grandfather turned 80, and we had a family reunion to celebrate.  
Head Entity: my grandfather  
Tail Entity: 80  

Relation: person age  
Context: the youngest member of the team, 22-year-old sarah, impressed everyone with her skills.  
Head Entity: sarah  
Tail Entity: 22  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: forsberg was born in 1943 in huntsville , ala. , and grew up on long island in new york .
Head Entity: forsberg
Tail Entity: huntsville
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in 1990 in barcelona, spain, where she spent her childhood.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during a press conference, it was revealed that john was born in 1985 in seattle, washington.  
Head Entity: john  
Tail Entity: seattle  

Relation: person city of birth  
Context: the famous author was born in 1975 in dublin, ireland, before moving to london.  
Head Entity: the famous author  
Tail Entity: dublin  

Relation: person city of birth  
Context: after years of research, it was discovered that marie was born in 1867 in warsaw, poland.  
Head Entity: marie  
Tail Entity: warsaw  

Relation: person city of birth  
Context: the musician proudly stated that he was born in 1992 in nashville, tennessee, a city known for its music scene.  
Head Entity: the musician  
Tail Entity: nashville  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians, including members from the Boston Symphony Orchestra.  
Head Entity: Boston Symphony Orchestra  
Tail Entity: New York Philharmonic  

Relation: organization members  
Context: The National Football League has several teams, including the Dallas Cowboys, which has produced many famous players who have gone on to join the Pro Football Hall of Fame.  
Head Entity: Pro Football Hall of Fame  
Tail Entity: Dallas Cowboys  

Relation: organization members  
Context: The United Nations consists of various specialized agencies, including the World Health Organization, which plays a crucial role in global health initiatives.  
Head Entity: World Health Organization  
Tail Entity: United Nations  

Relation: organization members  
Context: The International Olympic Committee oversees the Olympic Games and has various national committees, such as the United States Olympic and Paralympic Committee, which supports American athletes.  
Head Entity: United States Olympic and Paralympic Committee  
Tail Entity: International Olympic Committee  

Relation: organization members  
Context: The American Medical Association has numerous affiliated organizations, including the American College of Physicians, which focuses on internal medicine.  
Head Entity: American College of Physicians  
Tail Entity: American Medical Association  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how every member plays a vital role.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The famous author often discussed her deep connection to Buddhism and how it influenced her writing and personal philosophy.  
Head Entity: author  
Tail Entity: Buddhism  

Relation: person religion  
Context: As a devout follower, he regularly attended the mosque and participated in various community events organized by the Islamic center.  
Head Entity: he  
Tail Entity: Islam  

Relation: person religion  
Context: The singer's latest album reflects her journey through Christianity and the struggles she faced in her faith.  
Head Entity: singer  
Tail Entity: Christianity  

Relation: person religion  
Context: The activist, known for her work in social justice, often cites her roots in Hinduism as a guiding force in her mission.  
Head Entity: activist  
Tail Entity: Hinduism  
Losses:  28.037322998046875 0.4910706877708435 23.312358856201172
MemoryTrain:  epoch  0, batch     0 | loss: 28.0373230Losses:  21.506528854370117 1.3727490901947021 16.714170455932617
MemoryTrain:  epoch  0, batch     1 | loss: 21.5065289Losses:  21.107410430908203 1.1144973039627075 16.78352928161621
MemoryTrain:  epoch  0, batch     2 | loss: 21.1074104Losses:  24.665159225463867 0.809731125831604 19.91950035095215
MemoryTrain:  epoch  0, batch     3 | loss: 24.6651592Losses:  25.815950393676758 -0.0 23.24165153503418
MemoryTrain:  epoch  0, batch     4 | loss: 25.8159504Losses:  27.35146713256836 1.3676866292953491 23.20004653930664
MemoryTrain:  epoch  0, batch     5 | loss: 27.3514671Losses:  27.383508682250977 0.7285090684890747 23.226150512695312
MemoryTrain:  epoch  0, batch     6 | loss: 27.3835087Losses:  27.34564208984375 0.7815447449684143 23.191368103027344
MemoryTrain:  epoch  0, batch     7 | loss: 27.3456421Losses:  30.356794357299805 0.8123297691345215 26.539825439453125
MemoryTrain:  epoch  0, batch     8 | loss: 30.3567944Losses:  23.277563095092773 1.0537664890289307 19.855817794799805
MemoryTrain:  epoch  0, batch     9 | loss: 23.2775631Losses:  24.241172790527344 0.9705320596694946 19.878376007080078
MemoryTrain:  epoch  0, batch    10 | loss: 24.2411728Losses:  23.29385757446289 0.5150458812713623 19.95452880859375
MemoryTrain:  epoch  0, batch    11 | loss: 23.2938576Losses:  22.603595733642578 0.5055203437805176 19.856918334960938
MemoryTrain:  epoch  0, batch    12 | loss: 22.6035957Losses:  17.21932029724121 0.5239838361740112 13.695341110229492
MemoryTrain:  epoch  0, batch    13 | loss: 17.2193203Losses:  29.879138946533203 0.23653006553649902 26.506025314331055
MemoryTrain:  epoch  0, batch    14 | loss: 29.8791389Losses:  9.802331924438477 0.39478060603141785 5.613447189331055
MemoryTrain:  epoch  0, batch    15 | loss: 9.8023319Losses:  29.96409034729004 0.2518126964569092 26.493064880371094
MemoryTrain:  epoch  1, batch     0 | loss: 29.9640903Losses:  23.173105239868164 0.8532342910766602 19.896703720092773
MemoryTrain:  epoch  1, batch     1 | loss: 23.1731052Losses:  9.779875755310059 1.8779420852661133 5.575160980224609
MemoryTrain:  epoch  1, batch     2 | loss: 9.7798758Losses:  29.384183883666992 0.5126641988754272 26.45746421813965
MemoryTrain:  epoch  1, batch     3 | loss: 29.3841839Losses:  18.953969955444336 1.884902000427246 13.672184944152832
MemoryTrain:  epoch  1, batch     4 | loss: 18.9539700Losses:  28.846092224121094 0.2301986962556839 26.49554443359375
MemoryTrain:  epoch  1, batch     5 | loss: 28.8460922Losses:  22.52582550048828 0.5121526718139648 19.853057861328125
MemoryTrain:  epoch  1, batch     6 | loss: 22.5258255Losses:  18.514312744140625 0.7501391768455505 13.793213844299316
MemoryTrain:  epoch  1, batch     7 | loss: 18.5143127Losses:  16.88931655883789 0.9704276323318481 13.698616027832031
MemoryTrain:  epoch  1, batch     8 | loss: 16.8893166Losses:  24.26099967956543 1.090014934539795 19.945329666137695
MemoryTrain:  epoch  1, batch     9 | loss: 24.2609997Losses:  19.932395935058594 0.741795003414154 16.71169090270996
MemoryTrain:  epoch  1, batch    10 | loss: 19.9323959Losses:  20.953454971313477 1.003427267074585 16.79157829284668
MemoryTrain:  epoch  1, batch    11 | loss: 20.9534550Losses:  29.44098663330078 0.7332212924957275 26.440114974975586
MemoryTrain:  epoch  1, batch    12 | loss: 29.4409866Losses:  30.1478328704834 1.3183221817016602 26.48629379272461
MemoryTrain:  epoch  1, batch    13 | loss: 30.1478329Losses:  22.305673599243164 0.46783918142318726 19.841760635375977
MemoryTrain:  epoch  1, batch    14 | loss: 22.3056736Losses:  8.353073120117188 -0.0 5.615273952484131
MemoryTrain:  epoch  1, batch    15 | loss: 8.3530731Losses:  14.309184074401855 1.5449755191802979 10.836198806762695
MemoryTrain:  epoch  2, batch     0 | loss: 14.3091841Losses:  20.879270553588867 0.8293890953063965 16.826908111572266
MemoryTrain:  epoch  2, batch     1 | loss: 20.8792706Losses:  25.47980499267578 0.48661190271377563 23.07486343383789
MemoryTrain:  epoch  2, batch     2 | loss: 25.4798050Losses:  16.809492111206055 0.8519237637519836 13.701583862304688
MemoryTrain:  epoch  2, batch     3 | loss: 16.8094921Losses:  19.48350715637207 0.5114080905914307 16.698442459106445
MemoryTrain:  epoch  2, batch     4 | loss: 19.4835072Losses:  22.684585571289062 0.7175132036209106 19.818809509277344
MemoryTrain:  epoch  2, batch     5 | loss: 22.6845856Losses:  20.0494327545166 0.7492167949676514 16.676742553710938
MemoryTrain:  epoch  2, batch     6 | loss: 20.0494328Losses:  16.566356658935547 0.49123451113700867 13.70077896118164
MemoryTrain:  epoch  2, batch     7 | loss: 16.5663567Losses:  19.511756896972656 0.5456950664520264 16.719451904296875
MemoryTrain:  epoch  2, batch     8 | loss: 19.5117569Losses:  22.526805877685547 0.245596244931221 19.84777069091797
MemoryTrain:  epoch  2, batch     9 | loss: 22.5268059Losses:  23.494752883911133 0.7586820721626282 19.846479415893555
MemoryTrain:  epoch  2, batch    10 | loss: 23.4947529Losses:  23.003562927246094 1.100775957107544 19.86289405822754
MemoryTrain:  epoch  2, batch    11 | loss: 23.0035629Losses:  29.001712799072266 0.5323634147644043 26.440359115600586
MemoryTrain:  epoch  2, batch    12 | loss: 29.0017128Losses:  22.525344848632812 0.25061073899269104 19.893102645874023
MemoryTrain:  epoch  2, batch    13 | loss: 22.5253448Losses:  16.412845611572266 0.7451549172401428 13.655508041381836
MemoryTrain:  epoch  2, batch    14 | loss: 16.4128456Losses:  7.425559043884277 -0.0 5.555639743804932
MemoryTrain:  epoch  2, batch    15 | loss: 7.4255590Losses:  22.333139419555664 0.5095555186271667 19.821306228637695
MemoryTrain:  epoch  3, batch     0 | loss: 22.3331394Losses:  22.598905563354492 0.7549086809158325 19.84161376953125
MemoryTrain:  epoch  3, batch     1 | loss: 22.5989056Losses:  20.436187744140625 1.4320783615112305 16.70243263244629
MemoryTrain:  epoch  3, batch     2 | loss: 20.4361877Losses:  23.290042877197266 0.5767444968223572 19.849956512451172
MemoryTrain:  epoch  3, batch     3 | loss: 23.2900429Losses:  28.702411651611328 0.23708835244178772 26.476903915405273
MemoryTrain:  epoch  3, batch     4 | loss: 28.7024117Losses:  19.55890464782715 0.7618311643600464 16.80841636657715
MemoryTrain:  epoch  3, batch     5 | loss: 19.5589046Losses:  22.40576171875 0.5092122554779053 19.812768936157227
MemoryTrain:  epoch  3, batch     6 | loss: 22.4057617Losses:  19.4794921875 0.48042041063308716 16.68027114868164
MemoryTrain:  epoch  3, batch     7 | loss: 19.4794922Losses:  29.43958282470703 0.5291497707366943 26.54962158203125
MemoryTrain:  epoch  3, batch     8 | loss: 29.4395828Losses:  18.925880432128906 0.24597644805908203 16.699548721313477
MemoryTrain:  epoch  3, batch     9 | loss: 18.9258804Losses:  19.654239654541016 0.5229166746139526 16.68516731262207
MemoryTrain:  epoch  3, batch    10 | loss: 19.6542397Losses:  25.63119125366211 -0.0 23.088151931762695
MemoryTrain:  epoch  3, batch    11 | loss: 25.6311913Losses:  23.62052345275879 1.0602649450302124 19.845144271850586
MemoryTrain:  epoch  3, batch    12 | loss: 23.6205235Losses:  28.91482162475586 0.26614266633987427 26.498180389404297
MemoryTrain:  epoch  3, batch    13 | loss: 28.9148216Losses:  26.368715286254883 0.590753436088562 23.110593795776367
MemoryTrain:  epoch  3, batch    14 | loss: 26.3687153Losses:  7.715706825256348 0.27418252825737 5.551323890686035
MemoryTrain:  epoch  3, batch    15 | loss: 7.7157068Losses:  28.967439651489258 0.5040518045425415 26.44607925415039
MemoryTrain:  epoch  4, batch     0 | loss: 28.9674397Losses:  16.64289093017578 0.4981054961681366 13.668914794921875
MemoryTrain:  epoch  4, batch     1 | loss: 16.6428909Losses:  19.23468017578125 0.507649302482605 16.712610244750977
MemoryTrain:  epoch  4, batch     2 | loss: 19.2346802Losses:  22.671327590942383 0.521432638168335 19.84435272216797
MemoryTrain:  epoch  4, batch     3 | loss: 22.6713276Losses:  32.48295593261719 0.5046677589416504 29.89134979248047
MemoryTrain:  epoch  4, batch     4 | loss: 32.4829559Losses:  19.331506729125977 0.5277537107467651 16.734466552734375
MemoryTrain:  epoch  4, batch     5 | loss: 19.3315067Losses:  22.52566146850586 0.7390210628509521 19.859113693237305
MemoryTrain:  epoch  4, batch     6 | loss: 22.5256615Losses:  23.04491424560547 1.0752304792404175 19.87348747253418
MemoryTrain:  epoch  4, batch     7 | loss: 23.0449142Losses:  22.567092895507812 0.7422937154769897 19.88014793395996
MemoryTrain:  epoch  4, batch     8 | loss: 22.5670929Losses:  25.354248046875 0.2614801824092865 23.128076553344727
MemoryTrain:  epoch  4, batch     9 | loss: 25.3542480Losses:  23.504915237426758 1.6233088970184326 19.82187271118164
MemoryTrain:  epoch  4, batch    10 | loss: 23.5049152Losses:  25.698144912719727 0.7156186699867249 23.083770751953125
MemoryTrain:  epoch  4, batch    11 | loss: 25.6981449Losses:  23.058746337890625 1.3178284168243408 19.837026596069336
MemoryTrain:  epoch  4, batch    12 | loss: 23.0587463Losses:  17.24665641784668 1.1122229099273682 13.65768814086914
MemoryTrain:  epoch  4, batch    13 | loss: 17.2466564Losses:  22.296382904052734 0.48264506459236145 19.839895248413086
MemoryTrain:  epoch  4, batch    14 | loss: 22.2963829Losses:  10.018877029418945 -0.0 8.104959487915039
MemoryTrain:  epoch  4, batch    15 | loss: 10.0188770Losses:  22.049213409423828 0.24935473501682281 19.82831573486328
MemoryTrain:  epoch  5, batch     0 | loss: 22.0492134Losses:  22.217222213745117 0.4642990529537201 19.82564353942871
MemoryTrain:  epoch  5, batch     1 | loss: 22.2172222Losses:  16.229032516479492 0.5232574939727783 13.66085147857666
MemoryTrain:  epoch  5, batch     2 | loss: 16.2290325Losses:  25.563587188720703 0.5192358493804932 23.120859146118164
MemoryTrain:  epoch  5, batch     3 | loss: 25.5635872Losses:  29.332515716552734 0.7659629583358765 26.537734985351562
MemoryTrain:  epoch  5, batch     4 | loss: 29.3325157Losses:  32.509864807128906 0.5641880035400391 29.939353942871094
MemoryTrain:  epoch  5, batch     5 | loss: 32.5098648Losses:  24.57969093322754 2.8043015003204346 19.847318649291992
MemoryTrain:  epoch  5, batch     6 | loss: 24.5796909Losses:  32.112308502197266 0.2315853387117386 29.95415496826172
MemoryTrain:  epoch  5, batch     7 | loss: 32.1123085Losses:  25.27519416809082 0.25135308504104614 23.0808162689209
MemoryTrain:  epoch  5, batch     8 | loss: 25.2751942Losses:  22.73111343383789 1.0001490116119385 19.818592071533203
MemoryTrain:  epoch  5, batch     9 | loss: 22.7311134Losses:  20.001741409301758 1.3661127090454102 16.680870056152344
MemoryTrain:  epoch  5, batch    10 | loss: 20.0017414Losses:  25.2875919342041 0.259005606174469 23.082651138305664
MemoryTrain:  epoch  5, batch    11 | loss: 25.2875919Losses:  25.51595687866211 0.4758531153202057 23.08617401123047
MemoryTrain:  epoch  5, batch    12 | loss: 25.5159569Losses:  13.664985656738281 0.7817999720573425 10.818455696105957
MemoryTrain:  epoch  5, batch    13 | loss: 13.6649857Losses:  19.39811897277832 0.738714337348938 16.728498458862305
MemoryTrain:  epoch  5, batch    14 | loss: 19.3981190Losses:  5.4893412590026855 0.2782173752784729 3.3338048458099365
MemoryTrain:  epoch  5, batch    15 | loss: 5.4893413Losses:  17.030874252319336 1.3574140071868896 13.685516357421875
MemoryTrain:  epoch  6, batch     0 | loss: 17.0308743Losses:  19.982357025146484 1.3430945873260498 16.717342376708984
MemoryTrain:  epoch  6, batch     1 | loss: 19.9823570Losses:  19.57096290588379 0.9656205177307129 16.66707992553711
MemoryTrain:  epoch  6, batch     2 | loss: 19.5709629Losses:  35.66197967529297 0.2513372302055359 33.43296813964844
MemoryTrain:  epoch  6, batch     3 | loss: 35.6619797Losses:  19.718599319458008 0.9965999126434326 16.758548736572266
MemoryTrain:  epoch  6, batch     4 | loss: 19.7185993Losses:  26.155412673950195 1.0295089483261108 23.106185913085938
MemoryTrain:  epoch  6, batch     5 | loss: 26.1554127Losses:  23.072010040283203 1.3597438335418701 19.81842613220215
MemoryTrain:  epoch  6, batch     6 | loss: 23.0720100Losses:  32.0429801940918 0.24568790197372437 29.9089412689209
MemoryTrain:  epoch  6, batch     7 | loss: 32.0429802Losses:  19.6199951171875 1.0896824598312378 16.671735763549805
MemoryTrain:  epoch  6, batch     8 | loss: 19.6199951Losses:  22.728492736816406 0.9597573280334473 19.86528778076172
MemoryTrain:  epoch  6, batch     9 | loss: 22.7284927Losses:  25.50693130493164 0.499952495098114 23.10230827331543
MemoryTrain:  epoch  6, batch    10 | loss: 25.5069313Losses:  21.960824966430664 0.23484034836292267 19.84023094177246
MemoryTrain:  epoch  6, batch    11 | loss: 21.9608250Losses:  22.452362060546875 0.6863796710968018 19.859848022460938
MemoryTrain:  epoch  6, batch    12 | loss: 22.4523621Losses:  28.37364959716797 -0.0 26.427276611328125
MemoryTrain:  epoch  6, batch    13 | loss: 28.3736496Losses:  19.688159942626953 1.102524995803833 16.688276290893555
MemoryTrain:  epoch  6, batch    14 | loss: 19.6881599Losses:  5.462957859039307 0.2776620388031006 3.297304630279541
MemoryTrain:  epoch  6, batch    15 | loss: 5.4629579Losses:  25.731266021728516 0.7509269118309021 23.076913833618164
MemoryTrain:  epoch  7, batch     0 | loss: 25.7312660Losses:  22.553091049194336 0.7754229307174683 19.821500778198242
MemoryTrain:  epoch  7, batch     1 | loss: 22.5530910Losses:  28.639179229736328 0.24858038127422333 26.455730438232422
MemoryTrain:  epoch  7, batch     2 | loss: 28.6391792Losses:  19.54681396484375 0.955213189125061 16.67653465270996
MemoryTrain:  epoch  7, batch     3 | loss: 19.5468140Losses:  22.621280670166016 0.8416794538497925 19.87220001220703
MemoryTrain:  epoch  7, batch     4 | loss: 22.6212807Losses:  25.44144058227539 0.4732905328273773 23.077739715576172
MemoryTrain:  epoch  7, batch     5 | loss: 25.4414406Losses:  22.208829879760742 0.4843423664569855 19.814178466796875
MemoryTrain:  epoch  7, batch     6 | loss: 22.2088299Losses:  25.208675384521484 0.24027365446090698 23.095203399658203
MemoryTrain:  epoch  7, batch     7 | loss: 25.2086754Losses:  22.224519729614258 0.4831416606903076 19.870708465576172
MemoryTrain:  epoch  7, batch     8 | loss: 22.2245197Losses:  22.7849178314209 1.0419607162475586 19.835357666015625
MemoryTrain:  epoch  7, batch     9 | loss: 22.7849178Losses:  29.350982666015625 1.0505157709121704 26.439613342285156
MemoryTrain:  epoch  7, batch    10 | loss: 29.3509827Losses:  21.952428817749023 0.2523684501647949 19.80933952331543
MemoryTrain:  epoch  7, batch    11 | loss: 21.9524288Losses:  16.305747985839844 0.7441068887710571 13.667142868041992
MemoryTrain:  epoch  7, batch    12 | loss: 16.3057480Losses:  29.39275360107422 1.0983747243881226 26.428546905517578
MemoryTrain:  epoch  7, batch    13 | loss: 29.3927536Losses:  16.24018096923828 0.727990448474884 13.644302368164062
MemoryTrain:  epoch  7, batch    14 | loss: 16.2401810Losses:  5.782756805419922 0.5784629583358765 3.2982821464538574
MemoryTrain:  epoch  7, batch    15 | loss: 5.7827568Losses:  22.399681091308594 0.73445725440979 19.807703018188477
MemoryTrain:  epoch  8, batch     0 | loss: 22.3996811Losses:  23.02376937866211 1.292249321937561 19.841535568237305
MemoryTrain:  epoch  8, batch     1 | loss: 23.0237694Losses:  25.46271324157715 0.4859681725502014 23.065698623657227
MemoryTrain:  epoch  8, batch     2 | loss: 25.4627132Losses:  16.03644561767578 0.48666131496429443 13.657194137573242
MemoryTrain:  epoch  8, batch     3 | loss: 16.0364456Losses:  25.72162628173828 0.7083700299263 23.10436248779297
MemoryTrain:  epoch  8, batch     4 | loss: 25.7216263Losses:  24.99049949645996 -0.0 23.109453201293945
MemoryTrain:  epoch  8, batch     5 | loss: 24.9904995Losses:  20.058147430419922 1.3825740814208984 16.71225929260254
MemoryTrain:  epoch  8, batch     6 | loss: 20.0581474Losses:  22.188520431518555 0.4958757758140564 19.8155460357666
MemoryTrain:  epoch  8, batch     7 | loss: 22.1885204Losses:  19.065303802490234 0.5052627921104431 16.682113647460938
MemoryTrain:  epoch  8, batch     8 | loss: 19.0653038Losses:  19.3105411529541 0.7472155094146729 16.657785415649414
MemoryTrain:  epoch  8, batch     9 | loss: 19.3105412Losses:  28.875253677368164 0.508491039276123 26.469053268432617
MemoryTrain:  epoch  8, batch    10 | loss: 28.8752537Losses:  22.46210289001465 0.74468594789505 19.825366973876953
MemoryTrain:  epoch  8, batch    11 | loss: 22.4621029Losses:  28.772701263427734 0.46927711367607117 26.423870086669922
MemoryTrain:  epoch  8, batch    12 | loss: 28.7727013Losses:  25.227535247802734 0.2613677978515625 23.09276008605957
MemoryTrain:  epoch  8, batch    13 | loss: 25.2275352Losses:  21.680553436279297 -0.0 19.80466651916504
MemoryTrain:  epoch  8, batch    14 | loss: 21.6805534Losses:  9.918025016784668 -0.0 8.06476879119873
MemoryTrain:  epoch  8, batch    15 | loss: 9.9180250Losses:  22.212223052978516 0.483966588973999 19.845142364501953
MemoryTrain:  epoch  9, batch     0 | loss: 22.2122231Losses:  19.016204833984375 0.46261918544769287 16.66831398010254
MemoryTrain:  epoch  9, batch     1 | loss: 19.0162048Losses:  18.81964683532715 0.22172483801841736 16.69548988342285
MemoryTrain:  epoch  9, batch     2 | loss: 18.8196468Losses:  16.24675178527832 0.7350599765777588 13.640111923217773
MemoryTrain:  epoch  9, batch     3 | loss: 16.2467518Losses:  21.95590591430664 0.25381797552108765 19.812984466552734
MemoryTrain:  epoch  9, batch     4 | loss: 21.9559059Losses:  25.40509033203125 0.4835507571697235 23.062740325927734
MemoryTrain:  epoch  9, batch     5 | loss: 25.4050903Losses:  19.83949851989746 1.2965213060379028 16.67819595336914
MemoryTrain:  epoch  9, batch     6 | loss: 19.8394985Losses:  24.96254539489746 -0.0 23.086917877197266
MemoryTrain:  epoch  9, batch     7 | loss: 24.9625454Losses:  25.679595947265625 0.721655547618866 23.07708740234375
MemoryTrain:  epoch  9, batch     8 | loss: 25.6795959Losses:  22.437973022460938 0.7299802303314209 19.824935913085938
MemoryTrain:  epoch  9, batch     9 | loss: 22.4379730Losses:  25.93342399597168 0.9312947988510132 23.090452194213867
MemoryTrain:  epoch  9, batch    10 | loss: 25.9334240Losses:  16.269187927246094 0.7351665496826172 13.644874572753906
MemoryTrain:  epoch  9, batch    11 | loss: 16.2691879Losses:  19.63306427001953 1.099171757698059 16.67988395690918
MemoryTrain:  epoch  9, batch    12 | loss: 19.6330643Losses:  32.0556526184082 0.24220792949199677 29.92686653137207
MemoryTrain:  epoch  9, batch    13 | loss: 32.0556526Losses:  25.15357208251953 0.21465490758419037 23.09386444091797
MemoryTrain:  epoch  9, batch    14 | loss: 25.1535721Losses:  9.984057426452637 -0.0 8.068355560302734
MemoryTrain:  epoch  9, batch    15 | loss: 9.9840574
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 98.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 98.96%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 99.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 99.22%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 96.53%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 86.46%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 81.73%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 78.57%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 76.56%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 70.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 69.41%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 70.00%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 71.13%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 72.16%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 73.10%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 78.33%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 78.83%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 79.30%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 78.41%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 76.84%   [EVAL] batch:   34 | acc: 18.75%,  total acc: 75.18%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 73.61%   [EVAL] batch:   36 | acc: 12.50%,  total acc: 71.96%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 71.96%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 72.56%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 71.66%   [EVAL] batch:   43 | acc: 0.00%,  total acc: 70.03%   [EVAL] batch:   44 | acc: 0.00%,  total acc: 68.47%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 66.98%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 65.96%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 64.80%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 64.00%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 62.99%   [EVAL] batch:   51 | acc: 12.50%,  total acc: 62.02%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 61.32%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 61.34%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 61.93%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 62.94%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 63.25%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 63.56%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 62.70%   [EVAL] batch:   61 | acc: 0.00%,  total acc: 61.69%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 61.01%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 60.16%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 59.23%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 58.43%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 57.84%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 58.27%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 58.51%   [EVAL] batch:   69 | acc: 43.75%,  total acc: 58.30%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 58.19%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 58.25%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 58.82%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 59.92%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 60.44%   [EVAL] batch:   76 | acc: 100.00%,  total acc: 60.96%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 61.14%   [EVAL] batch:   78 | acc: 12.50%,  total acc: 60.52%   [EVAL] batch:   79 | acc: 0.00%,  total acc: 59.77%   [EVAL] batch:   80 | acc: 0.00%,  total acc: 59.03%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 58.46%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 57.83%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 57.29%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 56.91%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 56.40%   [EVAL] batch:   86 | acc: 0.00%,  total acc: 55.75%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 55.33%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 54.71%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 54.10%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 53.50%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 52.92%   [EVAL] batch:   92 | acc: 0.00%,  total acc: 52.35%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 51.93%   [EVAL] batch:   94 | acc: 0.00%,  total acc: 51.38%   [EVAL] batch:   95 | acc: 0.00%,  total acc: 50.85%   [EVAL] batch:   96 | acc: 6.25%,  total acc: 50.39%   [EVAL] batch:   97 | acc: 18.75%,  total acc: 50.06%   [EVAL] batch:   98 | acc: 25.00%,  total acc: 49.81%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 49.81%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 50.31%   [EVAL] batch:  101 | acc: 50.00%,  total acc: 50.31%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 50.12%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 49.82%   [EVAL] batch:  104 | acc: 56.25%,  total acc: 49.88%   [EVAL] batch:  105 | acc: 37.50%,  total acc: 49.76%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 49.53%   [EVAL] batch:  107 | acc: 62.50%,  total acc: 49.65%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 49.71%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 49.83%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 50.06%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 50.33%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 50.55%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 50.55%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 50.49%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 50.48%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 50.64%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 50.90%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 51.26%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 51.67%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 52.01%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 52.41%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 52.79%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 53.18%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 53.55%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 53.92%   [EVAL] batch:  126 | acc: 100.00%,  total acc: 54.28%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 54.39%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 54.46%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 54.47%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 54.44%   [EVAL] batch:  131 | acc: 31.25%,  total acc: 54.26%   [EVAL] batch:  132 | acc: 25.00%,  total acc: 54.04%   
cur_acc:  ['0.8390', '0.7042', '0.8705', '0.7986', '0.5000', '0.4297', '0.6683', '0.7857']
his_acc:  ['0.8390', '0.8125', '0.7705', '0.7204', '0.6594', '0.6262', '0.5494', '0.5404']
--------Round  2
seed:  300
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 0 1]
Losses:  20.873132705688477 7.577602386474609 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 20.8731327Losses:  26.73643684387207 13.675281524658203 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 26.7364368Losses:  22.70769691467285 9.575811386108398 -0.0
CurrentTrain: epoch  0, batch     2 | loss: 22.7076969Losses:  24.911352157592773 12.324332237243652 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 24.9113522Losses:  21.69023895263672 8.974827766418457 -0.0
CurrentTrain: epoch  0, batch     4 | loss: 21.6902390Losses:  23.37042236328125 10.890207290649414 -0.0
CurrentTrain: epoch  0, batch     5 | loss: 23.3704224Losses:  21.082523345947266 8.750635147094727 -0.0
CurrentTrain: epoch  0, batch     6 | loss: 21.0825233Losses:  19.503957748413086 7.185709476470947 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 19.5039577Losses:  22.621442794799805 10.480264663696289 -0.0
CurrentTrain: epoch  0, batch     8 | loss: 22.6214428Losses:  20.910682678222656 8.850672721862793 -0.0
CurrentTrain: epoch  0, batch     9 | loss: 20.9106827Losses:  22.895370483398438 11.090075492858887 -0.0
CurrentTrain: epoch  0, batch    10 | loss: 22.8953705Losses:  23.014484405517578 11.312935829162598 -0.0
CurrentTrain: epoch  0, batch    11 | loss: 23.0144844Losses:  22.6234130859375 11.346691131591797 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 22.6234131Losses:  26.42365264892578 14.519632339477539 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 26.4236526Losses:  22.292896270751953 10.477615356445312 -0.0
CurrentTrain: epoch  0, batch    14 | loss: 22.2928963Losses:  20.281410217285156 8.642001152038574 -0.0
CurrentTrain: epoch  0, batch    15 | loss: 20.2814102Losses:  19.24039077758789 7.8919172286987305 -0.0
CurrentTrain: epoch  0, batch    16 | loss: 19.2403908Losses:  19.697452545166016 8.687202453613281 -0.0
CurrentTrain: epoch  0, batch    17 | loss: 19.6974525Losses:  19.63134765625 7.760122299194336 -0.0
CurrentTrain: epoch  0, batch    18 | loss: 19.6313477Losses:  18.570362091064453 7.369297027587891 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 18.5703621Losses:  22.060962677001953 10.437921524047852 -0.0
CurrentTrain: epoch  0, batch    20 | loss: 22.0609627Losses:  21.532760620117188 10.20024299621582 -0.0
CurrentTrain: epoch  0, batch    21 | loss: 21.5327606Losses:  19.854684829711914 8.496742248535156 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 19.8546848Losses:  18.650930404663086 7.934222221374512 -0.0
CurrentTrain: epoch  0, batch    23 | loss: 18.6509304Losses:  17.805723190307617 7.501081466674805 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 17.8057232Losses:  19.432231903076172 9.029664039611816 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 19.4322319Losses:  20.870193481445312 9.382024765014648 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 20.8701935Losses:  23.098642349243164 12.181514739990234 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 23.0986423Losses:  17.259418487548828 6.813049793243408 -0.0
CurrentTrain: epoch  0, batch    28 | loss: 17.2594185Losses:  17.564109802246094 7.007428169250488 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 17.5641098Losses:  17.192367553710938 6.768726348876953 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 17.1923676Losses:  19.269001007080078 8.996573448181152 -0.0
CurrentTrain: epoch  0, batch    31 | loss: 19.2690010Losses:  19.31087875366211 8.27996826171875 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 19.3108788Losses:  22.485130310058594 11.130950927734375 -0.0
CurrentTrain: epoch  0, batch    33 | loss: 22.4851303Losses:  17.529327392578125 7.3221845626831055 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 17.5293274Losses:  19.584510803222656 8.35947036743164 -0.0
CurrentTrain: epoch  0, batch    35 | loss: 19.5845108Losses:  16.998029708862305 7.06117057800293 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 16.9980297Losses:  13.429641723632812 2.8415112495422363 -0.0
CurrentTrain: epoch  0, batch    37 | loss: 13.4296417Losses:  21.933584213256836 11.865291595458984 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 21.9335842Losses:  18.504663467407227 8.36667251586914 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 18.5046635Losses:  17.61937713623047 7.7708306312561035 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 17.6193771Losses:  17.346187591552734 7.47984504699707 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 17.3461876Losses:  18.307113647460938 8.18664264678955 -0.0
CurrentTrain: epoch  1, batch     4 | loss: 18.3071136Losses:  19.98627471923828 9.837274551391602 -0.0
CurrentTrain: epoch  1, batch     5 | loss: 19.9862747Losses:  22.119800567626953 12.032445907592773 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 22.1198006Losses:  17.041648864746094 7.526737213134766 -0.0
CurrentTrain: epoch  1, batch     7 | loss: 17.0416489Losses:  17.637779235839844 8.210857391357422 -0.0
CurrentTrain: epoch  1, batch     8 | loss: 17.6377792Losses:  19.57689666748047 9.759893417358398 -0.0
CurrentTrain: epoch  1, batch     9 | loss: 19.5768967Losses:  15.787622451782227 6.528347015380859 -0.0
CurrentTrain: epoch  1, batch    10 | loss: 15.7876225Losses:  15.84701919555664 6.3324875831604 -0.0
CurrentTrain: epoch  1, batch    11 | loss: 15.8470192Losses:  16.307296752929688 7.46848201751709 -0.0
CurrentTrain: epoch  1, batch    12 | loss: 16.3072968Losses:  18.567737579345703 8.832355499267578 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 18.5677376Losses:  15.879313468933105 6.57955265045166 -0.0
CurrentTrain: epoch  1, batch    14 | loss: 15.8793135Losses:  14.93110179901123 5.437582015991211 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 14.9311018Losses:  21.861095428466797 11.623777389526367 -0.0
CurrentTrain: epoch  1, batch    16 | loss: 21.8610954Losses:  19.210784912109375 9.825516700744629 -0.0
CurrentTrain: epoch  1, batch    17 | loss: 19.2107849Losses:  16.804584503173828 7.323966026306152 -0.0
CurrentTrain: epoch  1, batch    18 | loss: 16.8045845Losses:  18.00790023803711 8.895054817199707 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 18.0079002Losses:  16.147663116455078 6.9259538650512695 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 16.1476631Losses:  18.026397705078125 8.115819931030273 -0.0
CurrentTrain: epoch  1, batch    21 | loss: 18.0263977Losses:  18.614574432373047 10.095595359802246 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 18.6145744Losses:  16.3052921295166 7.326241493225098 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 16.3052921Losses:  18.875370025634766 9.68777084350586 -0.0
CurrentTrain: epoch  1, batch    24 | loss: 18.8753700Losses:  15.476837158203125 6.024775981903076 -0.0
CurrentTrain: epoch  1, batch    25 | loss: 15.4768372Losses:  16.474525451660156 6.973219871520996 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 16.4745255Losses:  16.521747589111328 7.4871954917907715 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 16.5217476Losses:  17.81001853942871 9.859349250793457 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 17.8100185Losses:  15.718945503234863 6.188778877258301 -0.0
CurrentTrain: epoch  1, batch    29 | loss: 15.7189455Losses:  16.070066452026367 6.8147454261779785 -0.0
CurrentTrain: epoch  1, batch    30 | loss: 16.0700665Losses:  17.969816207885742 8.148262023925781 -0.0
CurrentTrain: epoch  1, batch    31 | loss: 17.9698162Losses:  14.360269546508789 6.128342151641846 -0.0
CurrentTrain: epoch  1, batch    32 | loss: 14.3602695Losses:  17.582382202148438 8.276979446411133 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 17.5823822Losses:  18.76028823852539 9.862931251525879 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 18.7602882Losses:  15.966127395629883 7.574962139129639 -0.0
CurrentTrain: epoch  1, batch    35 | loss: 15.9661274Losses:  15.396871566772461 6.653692245483398 -0.0
CurrentTrain: epoch  1, batch    36 | loss: 15.3968716Losses:  11.329435348510742 3.856161594390869 -0.0
CurrentTrain: epoch  1, batch    37 | loss: 11.3294353Losses:  13.60478401184082 6.163477897644043 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 13.6047840Losses:  17.074974060058594 8.328136444091797 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 17.0749741Losses:  14.264238357543945 5.745870590209961 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 14.2642384Losses:  14.699625015258789 6.739377021789551 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 14.6996250Losses:  15.68653678894043 6.883893966674805 -0.0
CurrentTrain: epoch  2, batch     4 | loss: 15.6865368Losses:  15.789790153503418 7.925057411193848 -0.0
CurrentTrain: epoch  2, batch     5 | loss: 15.7897902Losses:  14.003213882446289 5.979989051818848 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 14.0032139Losses:  15.938944816589355 8.373929977416992 -0.0
CurrentTrain: epoch  2, batch     7 | loss: 15.9389448Losses:  14.804141998291016 6.629363536834717 -0.0
CurrentTrain: epoch  2, batch     8 | loss: 14.8041420Losses:  13.515405654907227 6.22003173828125 -0.0
CurrentTrain: epoch  2, batch     9 | loss: 13.5154057Losses:  18.969003677368164 10.224822998046875 -0.0
CurrentTrain: epoch  2, batch    10 | loss: 18.9690037Losses:  18.963422775268555 9.330917358398438 -0.0
CurrentTrain: epoch  2, batch    11 | loss: 18.9634228Losses:  16.54861068725586 7.088191986083984 -0.0
CurrentTrain: epoch  2, batch    12 | loss: 16.5486107Losses:  17.878353118896484 8.382824897766113 -0.0
CurrentTrain: epoch  2, batch    13 | loss: 17.8783531Losses:  12.710599899291992 4.619916915893555 -0.0
CurrentTrain: epoch  2, batch    14 | loss: 12.7105999Losses:  18.182781219482422 9.123095512390137 -0.0
CurrentTrain: epoch  2, batch    15 | loss: 18.1827812Losses:  14.1181640625 5.932860374450684 -0.0
CurrentTrain: epoch  2, batch    16 | loss: 14.1181641Losses:  14.704607963562012 6.351378440856934 -0.0
CurrentTrain: epoch  2, batch    17 | loss: 14.7046080Losses:  14.056769371032715 6.0580339431762695 -0.0
CurrentTrain: epoch  2, batch    18 | loss: 14.0567694Losses:  15.759828567504883 7.0998148918151855 -0.0
CurrentTrain: epoch  2, batch    19 | loss: 15.7598286Losses:  13.55682373046875 5.944334506988525 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 13.5568237Losses:  15.81037425994873 7.066832542419434 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 15.8103743Losses:  15.686631202697754 8.335351943969727 -0.0
CurrentTrain: epoch  2, batch    22 | loss: 15.6866312Losses:  16.754291534423828 7.693699836730957 -0.0
CurrentTrain: epoch  2, batch    23 | loss: 16.7542915Losses:  14.132545471191406 5.811691761016846 -0.0
CurrentTrain: epoch  2, batch    24 | loss: 14.1325455Losses:  14.988855361938477 7.596954345703125 -0.0
CurrentTrain: epoch  2, batch    25 | loss: 14.9888554Losses:  15.587800979614258 6.604885578155518 -0.0
CurrentTrain: epoch  2, batch    26 | loss: 15.5878010Losses:  13.898415565490723 5.456019401550293 -0.0
CurrentTrain: epoch  2, batch    27 | loss: 13.8984156Losses:  14.88076114654541 6.546135902404785 -0.0
CurrentTrain: epoch  2, batch    28 | loss: 14.8807611Losses:  14.916915893554688 6.415322303771973 -0.0
CurrentTrain: epoch  2, batch    29 | loss: 14.9169159Losses:  14.056314468383789 5.989902496337891 -0.0
CurrentTrain: epoch  2, batch    30 | loss: 14.0563145Losses:  14.906994819641113 7.536399841308594 -0.0
CurrentTrain: epoch  2, batch    31 | loss: 14.9069948Losses:  13.649893760681152 5.579889297485352 -0.0
CurrentTrain: epoch  2, batch    32 | loss: 13.6498938Losses:  13.913458824157715 6.693999290466309 -0.0
CurrentTrain: epoch  2, batch    33 | loss: 13.9134588Losses:  16.807395935058594 7.961311340332031 -0.0
CurrentTrain: epoch  2, batch    34 | loss: 16.8073959Losses:  12.179439544677734 4.522236347198486 -0.0
CurrentTrain: epoch  2, batch    35 | loss: 12.1794395Losses:  19.608219146728516 11.713592529296875 -0.0
CurrentTrain: epoch  2, batch    36 | loss: 19.6082191Losses:  9.885903358459473 1.7285757064819336 -0.0
CurrentTrain: epoch  2, batch    37 | loss: 9.8859034Losses:  15.842889785766602 8.241445541381836 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 15.8428898Losses:  18.379791259765625 9.511799812316895 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 18.3797913Losses:  15.903959274291992 8.30656623840332 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 15.9039593Losses:  13.680767059326172 5.336050033569336 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 13.6807671Losses:  13.664588928222656 5.503173351287842 -0.0
CurrentTrain: epoch  3, batch     4 | loss: 13.6645889Losses:  14.935259819030762 7.496459007263184 -0.0
CurrentTrain: epoch  3, batch     5 | loss: 14.9352598Losses:  15.038451194763184 6.75260066986084 -0.0
CurrentTrain: epoch  3, batch     6 | loss: 15.0384512Losses:  13.145736694335938 5.478789806365967 -0.0
CurrentTrain: epoch  3, batch     7 | loss: 13.1457367Losses:  19.209857940673828 13.053047180175781 -0.0
CurrentTrain: epoch  3, batch     8 | loss: 19.2098579Losses:  13.392900466918945 5.724330902099609 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 13.3929005Losses:  12.002557754516602 4.909102439880371 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 12.0025578Losses:  14.45084285736084 5.943389892578125 -0.0
CurrentTrain: epoch  3, batch    11 | loss: 14.4508429Losses:  12.551249504089355 4.525766372680664 -0.0
CurrentTrain: epoch  3, batch    12 | loss: 12.5512495Losses:  13.498085021972656 5.78382682800293 -0.0
CurrentTrain: epoch  3, batch    13 | loss: 13.4980850Losses:  14.696816444396973 7.800662040710449 -0.0
CurrentTrain: epoch  3, batch    14 | loss: 14.6968164Losses:  12.122572898864746 4.926137924194336 -0.0
CurrentTrain: epoch  3, batch    15 | loss: 12.1225729Losses:  12.943862915039062 5.381357192993164 -0.0
CurrentTrain: epoch  3, batch    16 | loss: 12.9438629Losses:  13.149744987487793 6.338360786437988 -0.0
CurrentTrain: epoch  3, batch    17 | loss: 13.1497450Losses:  12.593276977539062 4.831564903259277 -0.0
CurrentTrain: epoch  3, batch    18 | loss: 12.5932770Losses:  17.269908905029297 8.780681610107422 -0.0
CurrentTrain: epoch  3, batch    19 | loss: 17.2699089Losses:  14.207528114318848 5.838894844055176 -0.0
CurrentTrain: epoch  3, batch    20 | loss: 14.2075281Losses:  17.12148666381836 8.699393272399902 -0.0
CurrentTrain: epoch  3, batch    21 | loss: 17.1214867Losses:  16.75882339477539 8.524799346923828 -0.0
CurrentTrain: epoch  3, batch    22 | loss: 16.7588234Losses:  12.369648933410645 6.039338111877441 -0.0
CurrentTrain: epoch  3, batch    23 | loss: 12.3696489Losses:  14.783706665039062 7.487614631652832 -0.0
CurrentTrain: epoch  3, batch    24 | loss: 14.7837067Losses:  15.209858894348145 8.201041221618652 -0.0
CurrentTrain: epoch  3, batch    25 | loss: 15.2098589Losses:  14.66005802154541 7.197040557861328 -0.0
CurrentTrain: epoch  3, batch    26 | loss: 14.6600580Losses:  16.94644546508789 8.927026748657227 -0.0
CurrentTrain: epoch  3, batch    27 | loss: 16.9464455Losses:  18.98227310180664 10.667762756347656 -0.0
CurrentTrain: epoch  3, batch    28 | loss: 18.9822731Losses:  18.553951263427734 9.904093742370605 -0.0
CurrentTrain: epoch  3, batch    29 | loss: 18.5539513Losses:  13.241499900817871 6.366966247558594 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 13.2414999Losses:  12.668429374694824 6.057565689086914 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 12.6684294Losses:  18.7843074798584 11.736342430114746 -0.0
CurrentTrain: epoch  3, batch    32 | loss: 18.7843075Losses:  18.87703514099121 10.561786651611328 -0.0
CurrentTrain: epoch  3, batch    33 | loss: 18.8770351Losses:  12.595256805419922 5.5909247398376465 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 12.5952568Losses:  11.544736862182617 4.588724136352539 -0.0
CurrentTrain: epoch  3, batch    35 | loss: 11.5447369Losses:  13.051240921020508 5.248161315917969 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 13.0512409Losses:  8.64957046508789 1.1122167110443115 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 8.6495705Losses:  18.532045364379883 10.212554931640625 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 18.5320454Losses:  19.925996780395508 13.739648818969727 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 19.9259968Losses:  17.78250503540039 10.349288940429688 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 17.7825050Losses:  13.82620906829834 5.856369972229004 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 13.8262091Losses:  12.43375301361084 5.404261589050293 -0.0
CurrentTrain: epoch  4, batch     4 | loss: 12.4337530Losses:  12.024564743041992 6.245835304260254 -0.0
CurrentTrain: epoch  4, batch     5 | loss: 12.0245647Losses:  12.279289245605469 5.572778701782227 -0.0
CurrentTrain: epoch  4, batch     6 | loss: 12.2792892Losses:  11.290287971496582 4.0239481925964355 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 11.2902880Losses:  16.181255340576172 9.051836013793945 -0.0
CurrentTrain: epoch  4, batch     8 | loss: 16.1812553Losses:  12.912328720092773 5.867298126220703 -0.0
CurrentTrain: epoch  4, batch     9 | loss: 12.9123287Losses:  12.294663429260254 5.300961494445801 -0.0
CurrentTrain: epoch  4, batch    10 | loss: 12.2946634Losses:  12.128615379333496 5.286177635192871 -0.0
CurrentTrain: epoch  4, batch    11 | loss: 12.1286154Losses:  11.49051284790039 4.854793548583984 -0.0
CurrentTrain: epoch  4, batch    12 | loss: 11.4905128Losses:  15.6842041015625 9.879790306091309 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 15.6842041Losses:  14.257073402404785 8.289275169372559 -0.0
CurrentTrain: epoch  4, batch    14 | loss: 14.2570734Losses:  11.090548515319824 4.342221260070801 -0.0
CurrentTrain: epoch  4, batch    15 | loss: 11.0905485Losses:  19.40427017211914 10.988886833190918 -0.0
CurrentTrain: epoch  4, batch    16 | loss: 19.4042702Losses:  12.009058952331543 4.632502555847168 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 12.0090590Losses:  12.712822914123535 5.837347030639648 -0.0
CurrentTrain: epoch  4, batch    18 | loss: 12.7128229Losses:  13.259810447692871 6.424700736999512 -0.0
CurrentTrain: epoch  4, batch    19 | loss: 13.2598104Losses:  14.15069580078125 7.165236949920654 -0.0
CurrentTrain: epoch  4, batch    20 | loss: 14.1506958Losses:  12.906497955322266 5.882429122924805 -0.0
CurrentTrain: epoch  4, batch    21 | loss: 12.9064980Losses:  14.0101318359375 7.174005031585693 -0.0
CurrentTrain: epoch  4, batch    22 | loss: 14.0101318Losses:  13.082077026367188 4.719444274902344 -0.0
CurrentTrain: epoch  4, batch    23 | loss: 13.0820770Losses:  11.667261123657227 5.143372535705566 -0.0
CurrentTrain: epoch  4, batch    24 | loss: 11.6672611Losses:  15.035171508789062 6.219772815704346 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 15.0351715Losses:  15.059795379638672 7.490143775939941 -0.0
CurrentTrain: epoch  4, batch    26 | loss: 15.0597954Losses:  10.752120971679688 4.473931312561035 -0.0
CurrentTrain: epoch  4, batch    27 | loss: 10.7521210Losses:  14.655084609985352 7.209405422210693 -0.0
CurrentTrain: epoch  4, batch    28 | loss: 14.6550846Losses:  10.854747772216797 4.249111175537109 -0.0
CurrentTrain: epoch  4, batch    29 | loss: 10.8547478Losses:  13.127351760864258 5.870292663574219 -0.0
CurrentTrain: epoch  4, batch    30 | loss: 13.1273518Losses:  19.525344848632812 11.75688362121582 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 19.5253448Losses:  13.250043869018555 6.483310222625732 -0.0
CurrentTrain: epoch  4, batch    32 | loss: 13.2500439Losses:  13.279317855834961 7.141772747039795 -0.0
CurrentTrain: epoch  4, batch    33 | loss: 13.2793179Losses:  13.258580207824707 5.777540683746338 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 13.2585802Losses:  14.371243476867676 7.137681484222412 -0.0
CurrentTrain: epoch  4, batch    35 | loss: 14.3712435Losses:  13.484939575195312 6.6136369705200195 -0.0
CurrentTrain: epoch  4, batch    36 | loss: 13.4849396Losses:  10.620211601257324 2.70992112159729 -0.0
CurrentTrain: epoch  4, batch    37 | loss: 10.6202116Losses:  10.773490905761719 4.979675769805908 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 10.7734909Losses:  11.326632499694824 4.986885070800781 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 11.3266325Losses:  19.183717727661133 11.366874694824219 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 19.1837177Losses:  12.428787231445312 5.434768199920654 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 12.4287872Losses:  12.975831985473633 5.604913711547852 -0.0
CurrentTrain: epoch  5, batch     4 | loss: 12.9758320Losses:  11.538129806518555 5.426824569702148 -0.0
CurrentTrain: epoch  5, batch     5 | loss: 11.5381298Losses:  11.340021133422852 5.3454694747924805 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 11.3400211Losses:  12.793456077575684 6.91230583190918 -0.0
CurrentTrain: epoch  5, batch     7 | loss: 12.7934561Losses:  12.797296524047852 4.953938007354736 -0.0
CurrentTrain: epoch  5, batch     8 | loss: 12.7972965Losses:  12.04841423034668 5.444180011749268 -0.0
CurrentTrain: epoch  5, batch     9 | loss: 12.0484142Losses:  12.004182815551758 5.58842658996582 -0.0
CurrentTrain: epoch  5, batch    10 | loss: 12.0041828Losses:  12.192781448364258 5.529318809509277 -0.0
CurrentTrain: epoch  5, batch    11 | loss: 12.1927814Losses:  13.873899459838867 6.921677589416504 -0.0
CurrentTrain: epoch  5, batch    12 | loss: 13.8738995Losses:  16.192529678344727 9.725784301757812 -0.0
CurrentTrain: epoch  5, batch    13 | loss: 16.1925297Losses:  15.975902557373047 9.437580108642578 -0.0
CurrentTrain: epoch  5, batch    14 | loss: 15.9759026Losses:  12.503870010375977 5.700021743774414 -0.0
CurrentTrain: epoch  5, batch    15 | loss: 12.5038700Losses:  12.226495742797852 5.826630115509033 -0.0
CurrentTrain: epoch  5, batch    16 | loss: 12.2264957Losses:  11.781364440917969 4.887262344360352 -0.0
CurrentTrain: epoch  5, batch    17 | loss: 11.7813644Losses:  12.978050231933594 6.417426109313965 -0.0
CurrentTrain: epoch  5, batch    18 | loss: 12.9780502Losses:  11.676172256469727 4.898958206176758 -0.0
CurrentTrain: epoch  5, batch    19 | loss: 11.6761723Losses:  11.982626914978027 5.222580909729004 -0.0
CurrentTrain: epoch  5, batch    20 | loss: 11.9826269Losses:  14.956822395324707 7.645378112792969 -0.0
CurrentTrain: epoch  5, batch    21 | loss: 14.9568224Losses:  17.550764083862305 10.678312301635742 -0.0
CurrentTrain: epoch  5, batch    22 | loss: 17.5507641Losses:  19.96570587158203 13.258501052856445 -0.0
CurrentTrain: epoch  5, batch    23 | loss: 19.9657059Losses:  15.280989646911621 7.362070083618164 -0.0
CurrentTrain: epoch  5, batch    24 | loss: 15.2809896Losses:  11.62667465209961 4.986767768859863 -0.0
CurrentTrain: epoch  5, batch    25 | loss: 11.6266747Losses:  12.253287315368652 5.333069801330566 -0.0
CurrentTrain: epoch  5, batch    26 | loss: 12.2532873Losses:  10.885095596313477 4.6048479080200195 -0.0
CurrentTrain: epoch  5, batch    27 | loss: 10.8850956Losses:  14.487017631530762 7.148366928100586 -0.0
CurrentTrain: epoch  5, batch    28 | loss: 14.4870176Losses:  14.766749382019043 7.42329216003418 -0.0
CurrentTrain: epoch  5, batch    29 | loss: 14.7667494Losses:  12.831551551818848 7.234766960144043 -0.0
CurrentTrain: epoch  5, batch    30 | loss: 12.8315516Losses:  14.002278327941895 7.180788516998291 -0.0
CurrentTrain: epoch  5, batch    31 | loss: 14.0022783Losses:  10.792779922485352 4.817669868469238 -0.0
CurrentTrain: epoch  5, batch    32 | loss: 10.7927799Losses:  11.93917465209961 5.617891788482666 -0.0
CurrentTrain: epoch  5, batch    33 | loss: 11.9391747Losses:  16.38574981689453 9.05557632446289 -0.0
CurrentTrain: epoch  5, batch    34 | loss: 16.3857498Losses:  12.071809768676758 4.774910926818848 -0.0
CurrentTrain: epoch  5, batch    35 | loss: 12.0718098Losses:  12.200750350952148 5.646867752075195 -0.0
CurrentTrain: epoch  5, batch    36 | loss: 12.2007504Losses:  10.152467727661133 3.303208827972412 -0.0
CurrentTrain: epoch  5, batch    37 | loss: 10.1524677Losses:  14.63506031036377 7.628034591674805 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 14.6350603Losses:  10.744609832763672 4.801770210266113 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 10.7446098Losses:  12.299306869506836 6.4280266761779785 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 12.2993069Losses:  14.041000366210938 6.9619221687316895 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 14.0410004Losses:  12.610685348510742 6.282217025756836 -0.0
CurrentTrain: epoch  6, batch     4 | loss: 12.6106853Losses:  14.468746185302734 9.140100479125977 -0.0
CurrentTrain: epoch  6, batch     5 | loss: 14.4687462Losses:  12.369739532470703 6.425283432006836 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 12.3697395Losses:  10.407629013061523 4.163070201873779 -0.0
CurrentTrain: epoch  6, batch     7 | loss: 10.4076290Losses:  11.622932434082031 5.849685192108154 -0.0
CurrentTrain: epoch  6, batch     8 | loss: 11.6229324Losses:  18.299692153930664 10.432950973510742 -0.0
CurrentTrain: epoch  6, batch     9 | loss: 18.2996922Losses:  17.688358306884766 10.037313461303711 -0.0
CurrentTrain: epoch  6, batch    10 | loss: 17.6883583Losses:  11.885503768920898 5.561372756958008 -0.0
CurrentTrain: epoch  6, batch    11 | loss: 11.8855038Losses:  10.547791481018066 4.655546188354492 -0.0
CurrentTrain: epoch  6, batch    12 | loss: 10.5477915Losses:  12.835135459899902 6.8298187255859375 -0.0
CurrentTrain: epoch  6, batch    13 | loss: 12.8351355Losses:  13.144848823547363 6.75990104675293 -0.0
CurrentTrain: epoch  6, batch    14 | loss: 13.1448488Losses:  11.76930046081543 5.5260515213012695 -0.0
CurrentTrain: epoch  6, batch    15 | loss: 11.7693005Losses:  10.973758697509766 4.671899795532227 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 10.9737587Losses:  10.950691223144531 5.2214179039001465 -0.0
CurrentTrain: epoch  6, batch    17 | loss: 10.9506912Losses:  11.271387100219727 5.1479034423828125 -0.0
CurrentTrain: epoch  6, batch    18 | loss: 11.2713871Losses:  11.547940254211426 5.463947296142578 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 11.5479403Losses:  10.878593444824219 5.010010719299316 -0.0
CurrentTrain: epoch  6, batch    20 | loss: 10.8785934Losses:  10.35800552368164 4.264433860778809 -0.0
CurrentTrain: epoch  6, batch    21 | loss: 10.3580055Losses:  11.897746086120605 4.844106674194336 -0.0
CurrentTrain: epoch  6, batch    22 | loss: 11.8977461Losses:  13.24637222290039 7.438582897186279 -0.0
CurrentTrain: epoch  6, batch    23 | loss: 13.2463722Losses:  11.11447525024414 4.893240928649902 -0.0
CurrentTrain: epoch  6, batch    24 | loss: 11.1144753Losses:  14.66911506652832 7.138980865478516 -0.0
CurrentTrain: epoch  6, batch    25 | loss: 14.6691151Losses:  14.686266899108887 7.770951271057129 -0.0
CurrentTrain: epoch  6, batch    26 | loss: 14.6862669Losses:  12.192827224731445 6.414915561676025 -0.0
CurrentTrain: epoch  6, batch    27 | loss: 12.1928272Losses:  12.16610050201416 5.547786712646484 -0.0
CurrentTrain: epoch  6, batch    28 | loss: 12.1661005Losses:  10.959854125976562 5.295774459838867 -0.0
CurrentTrain: epoch  6, batch    29 | loss: 10.9598541Losses:  13.588491439819336 7.497704982757568 -0.0
CurrentTrain: epoch  6, batch    30 | loss: 13.5884914Losses:  13.310094833374023 7.589905738830566 -0.0
CurrentTrain: epoch  6, batch    31 | loss: 13.3100948Losses:  10.071220397949219 4.105162143707275 -0.0
CurrentTrain: epoch  6, batch    32 | loss: 10.0712204Losses:  12.148612976074219 5.359499931335449 -0.0
CurrentTrain: epoch  6, batch    33 | loss: 12.1486130Losses:  11.976151466369629 5.864293098449707 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 11.9761515Losses:  13.183635711669922 8.026468276977539 -0.0
CurrentTrain: epoch  6, batch    35 | loss: 13.1836357Losses:  9.2924165725708 3.685013771057129 -0.0
CurrentTrain: epoch  6, batch    36 | loss: 9.2924166Losses:  6.659012794494629 1.364482045173645 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 6.6590128Losses:  13.112263679504395 6.756257057189941 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 13.1122637Losses:  9.307354927062988 3.5688469409942627 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 9.3073549Losses:  15.855815887451172 9.414304733276367 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 15.8558159Losses:  14.01705551147461 7.675980567932129 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 14.0170555Losses:  11.926164627075195 6.236326694488525 -0.0
CurrentTrain: epoch  7, batch     4 | loss: 11.9261646Losses:  12.986506462097168 7.159132957458496 -0.0
CurrentTrain: epoch  7, batch     5 | loss: 12.9865065Losses:  9.744851112365723 3.91546368598938 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 9.7448511Losses:  9.233623504638672 3.9131898880004883 -0.0
CurrentTrain: epoch  7, batch     7 | loss: 9.2336235Losses:  10.871672630310059 5.483504295349121 -0.0
CurrentTrain: epoch  7, batch     8 | loss: 10.8716726Losses:  13.688041687011719 7.7766337394714355 -0.0
CurrentTrain: epoch  7, batch     9 | loss: 13.6880417Losses:  11.105539321899414 4.204497814178467 -0.0
CurrentTrain: epoch  7, batch    10 | loss: 11.1055393Losses:  11.09616470336914 5.433483123779297 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 11.0961647Losses:  11.760451316833496 6.107933521270752 -0.0
CurrentTrain: epoch  7, batch    12 | loss: 11.7604513Losses:  11.352103233337402 6.006777763366699 -0.0
CurrentTrain: epoch  7, batch    13 | loss: 11.3521032Losses:  12.355230331420898 6.794116020202637 -0.0
CurrentTrain: epoch  7, batch    14 | loss: 12.3552303Losses:  11.889986991882324 6.4690656661987305 -0.0
CurrentTrain: epoch  7, batch    15 | loss: 11.8899870Losses:  10.285625457763672 5.0830254554748535 -0.0
CurrentTrain: epoch  7, batch    16 | loss: 10.2856255Losses:  14.770771980285645 8.283702850341797 -0.0
CurrentTrain: epoch  7, batch    17 | loss: 14.7707720Losses:  13.350647926330566 7.585092067718506 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 13.3506479Losses:  9.259980201721191 3.9590530395507812 -0.0
CurrentTrain: epoch  7, batch    19 | loss: 9.2599802Losses:  9.919136047363281 4.65976095199585 -0.0
CurrentTrain: epoch  7, batch    20 | loss: 9.9191360Losses:  10.140336990356445 5.126221656799316 -0.0
CurrentTrain: epoch  7, batch    21 | loss: 10.1403370Losses:  11.791319847106934 6.533785820007324 -0.0
CurrentTrain: epoch  7, batch    22 | loss: 11.7913198Losses:  17.10308837890625 10.91317367553711 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 17.1030884Losses:  11.51282024383545 5.558708190917969 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 11.5128202Losses:  10.045741081237793 4.3808183670043945 -0.0
CurrentTrain: epoch  7, batch    25 | loss: 10.0457411Losses:  8.965513229370117 3.858381748199463 -0.0
CurrentTrain: epoch  7, batch    26 | loss: 8.9655132Losses:  11.683289527893066 6.448616027832031 -0.0
CurrentTrain: epoch  7, batch    27 | loss: 11.6832895Losses:  10.229806900024414 4.498566627502441 -0.0
CurrentTrain: epoch  7, batch    28 | loss: 10.2298069Losses:  14.569975852966309 8.317710876464844 -0.0
CurrentTrain: epoch  7, batch    29 | loss: 14.5699759Losses:  14.984293937683105 9.634481430053711 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 14.9842939Losses:  11.752164840698242 5.770405292510986 -0.0
CurrentTrain: epoch  7, batch    31 | loss: 11.7521648Losses:  9.673995971679688 4.7138566970825195 -0.0
CurrentTrain: epoch  7, batch    32 | loss: 9.6739960Losses:  10.51509952545166 5.194567680358887 -0.0
CurrentTrain: epoch  7, batch    33 | loss: 10.5150995Losses:  8.98692512512207 3.8637378215789795 -0.0
CurrentTrain: epoch  7, batch    34 | loss: 8.9869251Losses:  9.987959861755371 5.008852958679199 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 9.9879599Losses:  10.355066299438477 4.674554347991943 -0.0
CurrentTrain: epoch  7, batch    36 | loss: 10.3550663Losses:  6.247866630554199 1.354034423828125 -0.0
CurrentTrain: epoch  7, batch    37 | loss: 6.2478666Losses:  9.9154634475708 4.7951555252075195 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 9.9154634Losses:  11.630135536193848 5.774137496948242 -0.0
CurrentTrain: epoch  8, batch     1 | loss: 11.6301355Losses:  9.398195266723633 4.31374454498291 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 9.3981953Losses:  10.016708374023438 4.825933456420898 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 10.0167084Losses:  9.309358596801758 4.295619010925293 -0.0
CurrentTrain: epoch  8, batch     4 | loss: 9.3093586Losses:  12.648313522338867 7.683503150939941 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 12.6483135Losses:  9.74112319946289 4.8941545486450195 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 9.7411232Losses:  13.056754112243652 7.37068510055542 -0.0
CurrentTrain: epoch  8, batch     7 | loss: 13.0567541Losses:  8.787368774414062 3.906278610229492 -0.0
CurrentTrain: epoch  8, batch     8 | loss: 8.7873688Losses:  9.56989860534668 4.398688316345215 -0.0
CurrentTrain: epoch  8, batch     9 | loss: 9.5698986Losses:  10.394847869873047 5.040171146392822 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 10.3948479Losses:  10.984108924865723 5.704026222229004 -0.0
CurrentTrain: epoch  8, batch    11 | loss: 10.9841089Losses:  12.2652006149292 6.973409175872803 -0.0
CurrentTrain: epoch  8, batch    12 | loss: 12.2652006Losses:  13.767022132873535 9.121040344238281 -0.0
CurrentTrain: epoch  8, batch    13 | loss: 13.7670221Losses:  10.8848237991333 5.77143669128418 -0.0
CurrentTrain: epoch  8, batch    14 | loss: 10.8848238Losses:  8.598932266235352 3.819066047668457 -0.0
CurrentTrain: epoch  8, batch    15 | loss: 8.5989323Losses:  12.971012115478516 7.321090221405029 -0.0
CurrentTrain: epoch  8, batch    16 | loss: 12.9710121Losses:  12.455432891845703 7.549185752868652 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 12.4554329Losses:  13.124622344970703 8.001211166381836 -0.0
CurrentTrain: epoch  8, batch    18 | loss: 13.1246223Losses:  10.998943328857422 5.584378242492676 -0.0
CurrentTrain: epoch  8, batch    19 | loss: 10.9989433Losses:  11.06716537475586 5.446497917175293 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 11.0671654Losses:  9.953094482421875 5.151583671569824 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 9.9530945Losses:  12.93486499786377 8.074466705322266 -0.0
CurrentTrain: epoch  8, batch    22 | loss: 12.9348650Losses:  9.165773391723633 4.236759185791016 -0.0
CurrentTrain: epoch  8, batch    23 | loss: 9.1657734Losses:  10.719780921936035 5.387094497680664 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 10.7197809Losses:  11.957799911499023 6.965932846069336 -0.0
CurrentTrain: epoch  8, batch    25 | loss: 11.9577999Losses:  10.243267059326172 4.71267032623291 -0.0
CurrentTrain: epoch  8, batch    26 | loss: 10.2432671Losses:  9.330696105957031 3.961719036102295 -0.0
CurrentTrain: epoch  8, batch    27 | loss: 9.3306961Losses:  8.965421676635742 3.883338451385498 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 8.9654217Losses:  10.44193172454834 5.516809463500977 -0.0
CurrentTrain: epoch  8, batch    29 | loss: 10.4419317Losses:  11.558588981628418 6.165529251098633 -0.0
CurrentTrain: epoch  8, batch    30 | loss: 11.5585890Losses:  8.68257999420166 3.828056573867798 -0.0
CurrentTrain: epoch  8, batch    31 | loss: 8.6825800Losses:  10.950576782226562 6.081244468688965 -0.0
CurrentTrain: epoch  8, batch    32 | loss: 10.9505768Losses:  11.501749992370605 6.842512130737305 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 11.5017500Losses:  10.050369262695312 4.070680618286133 -0.0
CurrentTrain: epoch  8, batch    34 | loss: 10.0503693Losses:  13.440387725830078 7.556491851806641 -0.0
CurrentTrain: epoch  8, batch    35 | loss: 13.4403877Losses:  9.776725769042969 4.2528886795043945 -0.0
CurrentTrain: epoch  8, batch    36 | loss: 9.7767258Losses:  7.0187201499938965 1.561712384223938 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 7.0187201Losses:  10.795171737670898 5.729757308959961 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 10.7951717Losses:  10.326132774353027 5.372656345367432 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 10.3261328Losses:  12.300216674804688 6.053342819213867 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 12.3002167Losses:  10.527185440063477 5.074007987976074 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 10.5271854Losses:  8.635759353637695 3.7901806831359863 -0.0
CurrentTrain: epoch  9, batch     4 | loss: 8.6357594Losses:  10.955522537231445 5.808328628540039 -0.0
CurrentTrain: epoch  9, batch     5 | loss: 10.9555225Losses:  12.398351669311523 7.058952808380127 -0.0
CurrentTrain: epoch  9, batch     6 | loss: 12.3983517Losses:  11.202082633972168 5.839608192443848 -0.0
CurrentTrain: epoch  9, batch     7 | loss: 11.2020826Losses:  8.383703231811523 3.4470090866088867 -0.0
CurrentTrain: epoch  9, batch     8 | loss: 8.3837032Losses:  11.947721481323242 6.622231960296631 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 11.9477215Losses:  14.968509674072266 8.08660888671875 -0.0
CurrentTrain: epoch  9, batch    10 | loss: 14.9685097Losses:  11.346967697143555 5.490517616271973 -0.0
CurrentTrain: epoch  9, batch    11 | loss: 11.3469677Losses:  9.51130485534668 4.729328155517578 -0.0
CurrentTrain: epoch  9, batch    12 | loss: 9.5113049Losses:  10.483833312988281 5.217671871185303 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 10.4838333Losses:  14.420501708984375 9.433866500854492 -0.0
CurrentTrain: epoch  9, batch    14 | loss: 14.4205017Losses:  9.380973815917969 4.1210618019104 -0.0
CurrentTrain: epoch  9, batch    15 | loss: 9.3809738Losses:  9.915026664733887 4.877649307250977 -0.0
CurrentTrain: epoch  9, batch    16 | loss: 9.9150267Losses:  10.09747314453125 5.024178981781006 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 10.0974731Losses:  10.613018035888672 5.501059532165527 -0.0
CurrentTrain: epoch  9, batch    18 | loss: 10.6130180Losses:  9.941694259643555 4.9267578125 -0.0
CurrentTrain: epoch  9, batch    19 | loss: 9.9416943Losses:  11.64539623260498 6.599430084228516 -0.0
CurrentTrain: epoch  9, batch    20 | loss: 11.6453962Losses:  8.493804931640625 3.3568813800811768 -0.0
CurrentTrain: epoch  9, batch    21 | loss: 8.4938049Losses:  11.045966148376465 5.8832902908325195 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 11.0459661Losses:  9.776092529296875 4.775393486022949 -0.0
CurrentTrain: epoch  9, batch    23 | loss: 9.7760925Losses:  11.11822509765625 5.151988983154297 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 11.1182251Losses:  11.147851943969727 5.896921157836914 -0.0
CurrentTrain: epoch  9, batch    25 | loss: 11.1478519Losses:  10.479879379272461 5.614410877227783 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 10.4798794Losses:  12.3257417678833 7.403559684753418 -0.0
CurrentTrain: epoch  9, batch    27 | loss: 12.3257418Losses:  11.097284317016602 5.553440570831299 -0.0
CurrentTrain: epoch  9, batch    28 | loss: 11.0972843Losses:  10.351953506469727 5.008049964904785 -0.0
CurrentTrain: epoch  9, batch    29 | loss: 10.3519535Losses:  10.616776466369629 5.566450119018555 -0.0
CurrentTrain: epoch  9, batch    30 | loss: 10.6167765Losses:  9.702651023864746 4.473645210266113 -0.0
CurrentTrain: epoch  9, batch    31 | loss: 9.7026510Losses:  11.3052978515625 6.436084270477295 -0.0
CurrentTrain: epoch  9, batch    32 | loss: 11.3052979Losses:  9.718437194824219 4.364692211151123 -0.0
CurrentTrain: epoch  9, batch    33 | loss: 9.7184372Losses:  9.576438903808594 4.658730506896973 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 9.5764389Losses:  9.818161010742188 4.682155609130859 -0.0
CurrentTrain: epoch  9, batch    35 | loss: 9.8181610Losses:  16.559707641601562 11.837814331054688 -0.0
CurrentTrain: epoch  9, batch    36 | loss: 16.5597076Losses:  7.927725791931152 2.2842979431152344 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 7.9277258
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, James Smith relocated to Australia to enjoy a more laid-back lifestyle by the beach.  
Head Entity: James Smith  
Tail Entity: Australia  

Relation: person countries of residence  
Context: Despite being born in Canada, Priya Sharma has spent most of her adult life in India, where she has established her career and family.  
Head Entity: Priya Sharma  
Tail Entity: India  

Relation: person countries of residence  
Context: After completing his studies in Germany, Ahmed Al-Farsi chose to settle in the United Arab Emirates, where he found numerous job opportunities.  
Head Entity: Ahmed Al-Farsi  
Tail Entity: United Arab Emirates  

Relation: person countries of residence  
Context: Having lived in Japan for over a decade, Emily Chen has embraced the culture and considers it her second home, even though she was born in the United States.  
Head Entity: Emily Chen  
Tail Entity: Japan  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` customer relationships are the backbone of banking , and banks are constantly striving to meet customer demands , '' said peter garuccio , spokesman for the american bankers association .
Head Entity: american bankers association
Tail Entity: peter garuccio
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
1. Relation: organization top members employees  
   Context: "As the CEO of the tech giant, she has been instrumental in driving innovation," said a company spokesperson about Jane Doe, who leads the organization.  
   Head Entity: tech giant  
   Tail Entity: Jane Doe  

2. Relation: organization top members employees  
   Context: "The board of directors is pleased to announce the appointment of John Smith as the new CFO," stated the press release from the financial services firm.  
   Head Entity: financial services firm  
   Tail Entity: John Smith  

3. Relation: organization top members employees  
   Context: "During the annual meeting, the president of the nonprofit organization highlighted the contributions of their dedicated volunteers," reported the local news outlet.  
   Head Entity: nonprofit organization  
   Tail Entity: president  

4. Relation: organization top members employees  
   Context: "In a recent interview, the founder of the startup discussed the importance of teamwork and credited his lead developer for the success of their latest product," said the tech blog.  
   Head Entity: startup  
   Tail Entity: lead developer  

5. Relation: organization top members employees  
   Context: "The university's chancellor praised the faculty for their commitment to student success during the commencement speech," noted the campus newspaper.  
   Head Entity: university  
   Tail Entity: chancellor  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: linebacker mike peterson rejoined the jacksonville jaguars on friday , two days after coach jack del rio banished him from the locker room of the national football league team .
Head Entity: jacksonville jaguars
Tail Entity: national football league
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: the united nations is an international organization that promotes peace and cooperation among its member states, including countries like france and germany.  
Head Entity: united nations  
Tail Entity: france  

Relation: organization member of  
Context: the world health organization has several member countries that collaborate on global health initiatives, with india being one of the key participants.  
Head Entity: world health organization  
Tail Entity: india  

Relation: organization member of  
Context: the european union consists of various member nations, with italy being a significant contributor to its policies and decisions.  
Head Entity: european union  
Tail Entity: italy  

Relation: organization member of  
Context: the national basketball association includes numerous teams, and the los angeles lakers are one of its most famous member franchises.  
Head Entity: national basketball association  
Tail Entity: los angeles lakers  

Relation: organization member of  
Context: the international olympic committee oversees the olympic games and has numerous member countries, including japan, which actively participates in the events.  
Head Entity: international olympic committee  
Tail Entity: japan  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: iranian atomic chief ali akbar salehi said on wednesday that tehran will address the concerns raised by the vienna group regarding the fuel deal inked by the islamic republic with brazil and turkey .
Head Entity: ali akbar salehi
Tail Entity: iranian
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: The renowned physicist Albert Einstein was born in the Kingdom of Württemberg in the German Empire.  
Head Entity: Albert Einstein  
Tail Entity: German  

Relation: person origin  
Context: The famous author Chimamanda Ngozi Adichie often writes about her experiences growing up in Nigeria.  
Head Entity: Chimamanda Ngozi Adichie  
Tail Entity: Nigerian  

Relation: person origin  
Context: The celebrated artist Frida Kahlo was born in Coyoacán, Mexico City, Mexico.  
Head Entity: Frida Kahlo  
Tail Entity: Mexican  

Relation: person origin  
Context: The legendary musician Bob Marley is widely regarded as a cultural icon of Jamaica.  
Head Entity: Bob Marley  
Tail Entity: Jamaican  

Relation: person origin  
Context: The acclaimed filmmaker Hayao Miyazaki is known for his contributions to Japanese animation.  
Head Entity: Hayao Miyazaki  
Tail Entity: Japanese  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` we are not canceling any of our orders for next year , '' deputy managing director philip chen was quoted as saying in today 's south china morning post .
Head Entity: philip chen
Tail Entity: director
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work, she was finally recognized as the chief executive officer of the company."  
Head Entity: she  
Tail Entity: chief executive officer  

Relation: person title  
Context: "During the ceremony, the mayor presented the award to the outstanding volunteer, who has been a community leader for over a decade."  
Head Entity: mayor  
Tail Entity: community leader  

Relation: person title  
Context: "In his latest interview, the renowned scientist discussed his role as the head of the research department at the university."  
Head Entity: scientist  
Tail Entity: head of the research department  

Relation: person title  
Context: "The board of directors appointed her as the new chief financial officer, a position she has been preparing for her entire career."  
Head Entity: her  
Tail Entity: chief financial officer  

Relation: person title  
Context: "As the founder of the organization, he has taken on the title of executive director, guiding its mission and vision."  
Head Entity: he  
Tail Entity: executive director  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: volkswagen has two joint ventures in china -- one with the state - owned china faw group , or faw-volkswagen automobile co. , and one with saic motor corp. , or shanghai volkswagen automotive co. .
Head Entity: china faw group
Tail Entity: china
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: samsung electronics is a global leader in technology and is headquartered in south korea, where it has established a strong presence in the electronics market.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization country of headquarters  
Context: the headquarters of nestlé is located in switzerland, where the company was founded and has maintained its global operations.  
Head Entity: nestlé  
Tail Entity: switzerland  

Relation: organization country of headquarters  
Context: the multinational corporation unilever has its headquarters in the united kingdom and the netherlands, reflecting its dual nationality.  
Head Entity: unilever  
Tail Entity: united kingdom  

Relation: organization country of headquarters  
Context: toyota motor corporation, one of the largest automobile manufacturers in the world, is headquartered in japan, specifically in the city of toyota.  
Head Entity: toyota motor corporation  
Tail Entity: japan  

Relation: organization country of headquarters  
Context: the tech giant google, known for its search engine and various digital services, is headquartered in the united states, in mountain view, california.  
Head Entity: google  
Tail Entity: united states  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 85.83%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 85.89%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.13%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 84.66%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.03%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.56%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 85.83%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 85.89%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.13%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 84.66%   
cur_acc:  ['0.8466']
his_acc:  ['0.8466']
Clustering into  4  clusters
Clusters:  [1 0 2 1 0 1 1 3 1 0 0]
Losses:  16.728858947753906 8.817428588867188 1.507756233215332
CurrentTrain: epoch  0, batch     0 | loss: 16.7288589Losses:  14.744109153747559 5.369411945343018 1.4070863723754883
CurrentTrain: epoch  0, batch     1 | loss: 14.7441092Losses:  17.461071014404297 9.602174758911133 1.472093939781189
CurrentTrain: epoch  1, batch     0 | loss: 17.4610710Losses:  13.619516372680664 4.780106067657471 1.5421128273010254
CurrentTrain: epoch  1, batch     1 | loss: 13.6195164Losses:  16.102031707763672 8.602307319641113 1.466406226158142
CurrentTrain: epoch  2, batch     0 | loss: 16.1020317Losses:  9.885345458984375 3.181948184967041 1.5182251930236816
CurrentTrain: epoch  2, batch     1 | loss: 9.8853455Losses:  14.430225372314453 8.248223304748535 1.414005994796753
CurrentTrain: epoch  3, batch     0 | loss: 14.4302254Losses:  10.25196361541748 4.050334453582764 1.5605335235595703
CurrentTrain: epoch  3, batch     1 | loss: 10.2519636Losses:  12.498880386352539 6.663843154907227 1.460524082183838
CurrentTrain: epoch  4, batch     0 | loss: 12.4988804Losses:  7.37080192565918 1.6555454730987549 1.4204416275024414
CurrentTrain: epoch  4, batch     1 | loss: 7.3708019Losses:  12.570337295532227 6.998795509338379 1.5049152374267578
CurrentTrain: epoch  5, batch     0 | loss: 12.5703373Losses:  7.944354057312012 2.158045530319214 1.4449529647827148
CurrentTrain: epoch  5, batch     1 | loss: 7.9443541Losses:  12.721918106079102 7.0732221603393555 1.5167303085327148
CurrentTrain: epoch  6, batch     0 | loss: 12.7219181Losses:  6.877410888671875 1.9064909219741821 1.5489513874053955
CurrentTrain: epoch  6, batch     1 | loss: 6.8774109Losses:  11.7581787109375 6.347196102142334 1.4852721691131592
CurrentTrain: epoch  7, batch     0 | loss: 11.7581787Losses:  5.707859516143799 1.3606282472610474 1.4967373609542847
CurrentTrain: epoch  7, batch     1 | loss: 5.7078595Losses:  10.590658187866211 6.3850908279418945 1.4940800666809082
CurrentTrain: epoch  8, batch     0 | loss: 10.5906582Losses:  6.360104084014893 2.1319024562835693 1.4033616781234741
CurrentTrain: epoch  8, batch     1 | loss: 6.3601041Losses:  13.173912048339844 8.613622665405273 1.4901692867279053
CurrentTrain: epoch  9, batch     0 | loss: 13.1739120Losses:  8.212716102600098 4.168062210083008 1.3880800008773804
CurrentTrain: epoch  9, batch     1 | loss: 8.2127161
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones found her new home in san francisco, where she works as a software engineer. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith spent a lot of time in boston, where he attended university and made lifelong friends. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  

Relation: person cities of residence  
Context: -lrb- following her promotion, emily davis relocated to seattle, excited about the opportunities in the tech industry. -rrb-  
Head Entity: emily davis  
Tail Entity: seattle  

Relation: person cities of residence  
Context: -lrb- after retiring, john doe decided to settle in miami, enjoying the warm weather and vibrant culture. -rrb-  
Head Entity: john doe  
Tail Entity: miami  

Relation: person cities of residence  
Context: -lrb- having grown up in new york, alice brown returned to her roots after years of living in los angeles. -rrb-  
Head Entity: alice brown  
Tail Entity: new york  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: born in baltimore in 1922 , parren mitchell was a graduate of morgan state college and earned a master 's degree from the university of maryland , according to biographical information supplied by cummings ' office .
Head Entity: parren mitchell
Tail Entity: university of maryland
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After completing high school in 1995, Sarah Johnson went on to study at the University of California, Los Angeles, where she earned her bachelor's degree in sociology.  
Head Entity: Sarah Johnson  
Tail Entity: University of California, Los Angeles  

Relation: person schools attended  
Context: Mark Thompson, a renowned scientist, received his education at the Massachusetts Institute of Technology, where he developed a passion for robotics.  
Head Entity: Mark Thompson  
Tail Entity: Massachusetts Institute of Technology  

Relation: person schools attended  
Context: Growing up in a small town, Emily Chen attended Stanford University, which played a crucial role in shaping her career as a software engineer.  
Head Entity: Emily Chen  
Tail Entity: Stanford University  

Relation: person schools attended  
Context: James Rodriguez graduated from Harvard University, where he majored in political science and later pursued a career in public service.  
Head Entity: James Rodriguez  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After moving to New York, Lisa Patel enrolled at Columbia University, where she earned her master's degree in business administration.  
Head Entity: Lisa Patel  
Tail Entity: Columbia University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author agatha christie died in her home in wallingford, england  
Head Entity: agatha christie  
Tail Entity: england  

Relation: person country of death  
Context: legendary musician freddie mercury succumbed to aids in london, united kingdom  
Head Entity: freddie mercury  
Tail Entity: united kingdom  

Relation: person country of death  
Context: beloved actor robin williams was found dead in his home in california, usa  
Head Entity: robin williams  
Tail Entity: usa  

Relation: person country of death  
Context: influential civil rights leader nelson mandela passed away in johannesburg, south africa  
Head Entity: nelson mandela  
Tail Entity: south africa  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both very talented.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the divorce, he took custody of his two daughters, lily and rose, who are now thriving in school.  
Head Entity: he  
Tail Entity: rose  

Relation: person children  
Context: the famous actor is a proud father of four, with his youngest being a daughter named sophia.  
Head Entity: the famous actor  
Tail Entity: sophia  

Relation: person children  
Context: they often visit their grandparents, who love spending time with their grandchildren, including max and olivia.  
Head Entity: they  
Tail Entity: max  

Relation: person children  
Context: she often shares stories about her two sons, aiden and ben, who are both passionate about sports.  
Head Entity: she  
Tail Entity: ben  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, which shocked his colleagues at the firm.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith was charged with assault following the altercation at the downtown bar last weekend.  
Head Entity: Smith  
Tail Entity: assault  

Relation: person charges  
Context: In a surprising turn of events, the police revealed that Martinez was charged with fraud after a series of complaints from local businesses.  
Head Entity: Martinez  
Tail Entity: fraud  

Relation: person charges  
Context: Following the evidence presented in court, it was determined that Lee was charged with theft, leading to a significant public outcry.  
Head Entity: Lee  
Tail Entity: theft  

Relation: person charges  
Context: The investigation concluded with the announcement that Thompson was charged with tax evasion, prompting a federal inquiry into his finances.  
Head Entity: Thompson  
Tail Entity: tax evasion  
Losses:  9.694387435913086 2.007859230041504 1.6125762462615967
MemoryTrain:  epoch  0, batch     0 | loss: 9.6943874Losses:  10.319942474365234 1.9607657194137573 1.4899368286132812
MemoryTrain:  epoch  0, batch     1 | loss: 10.3199425Losses:  11.971109390258789 2.215710163116455 3.6051979064941406
MemoryTrain:  epoch  0, batch     2 | loss: 11.9711094Losses:  11.144309997558594 4.333499431610107 1.4962468147277832
MemoryTrain:  epoch  0, batch     3 | loss: 11.1443100Losses:  11.053226470947266 -0.0 -0.0
MemoryTrain:  epoch  0, batch     4 | loss: 11.0532265Losses:  12.142669677734375 3.944512367248535 1.5853161811828613
MemoryTrain:  epoch  1, batch     0 | loss: 12.1426697Losses:  10.985271453857422 2.3158278465270996 3.545897960662842
MemoryTrain:  epoch  1, batch     1 | loss: 10.9852715Losses:  11.886129379272461 2.7231733798980713 3.4215445518493652
MemoryTrain:  epoch  1, batch     2 | loss: 11.8861294Losses:  7.924687385559082 2.188873767852783 1.4134693145751953
MemoryTrain:  epoch  1, batch     3 | loss: 7.9246874Losses:  2.477757453918457 -0.0 -0.0
MemoryTrain:  epoch  1, batch     4 | loss: 2.4777575Losses:  8.10000991821289 2.6783785820007324 1.4001816511154175
MemoryTrain:  epoch  2, batch     0 | loss: 8.1000099Losses:  10.667303085327148 3.0731887817382812 3.425630807876587
MemoryTrain:  epoch  2, batch     1 | loss: 10.6673031Losses:  9.077777862548828 2.6824705600738525 3.4077930450439453
MemoryTrain:  epoch  2, batch     2 | loss: 9.0777779Losses:  7.607631206512451 2.180335521697998 1.5106291770935059
MemoryTrain:  epoch  2, batch     3 | loss: 7.6076312Losses:  2.006525754928589 -0.0 -0.0
MemoryTrain:  epoch  2, batch     4 | loss: 2.0065258Losses:  8.060708999633789 3.2890069484710693 1.4101262092590332
MemoryTrain:  epoch  3, batch     0 | loss: 8.0607090Losses:  9.250831604003906 2.464406967163086 3.4426374435424805
MemoryTrain:  epoch  3, batch     1 | loss: 9.2508316Losses:  8.741315841674805 2.4258337020874023 3.3847241401672363
MemoryTrain:  epoch  3, batch     2 | loss: 8.7413158Losses:  9.459479331970215 2.3937273025512695 3.4253997802734375
MemoryTrain:  epoch  3, batch     3 | loss: 9.4594793Losses:  2.9544997215270996 -0.0 -0.0
MemoryTrain:  epoch  3, batch     4 | loss: 2.9544997Losses:  6.418327331542969 2.2996246814727783 1.495917558670044
MemoryTrain:  epoch  4, batch     0 | loss: 6.4183273Losses:  8.276080131530762 2.0656304359436035 3.3943698406219482
MemoryTrain:  epoch  4, batch     1 | loss: 8.2760801Losses:  8.698307037353516 1.9715449810028076 3.4044978618621826
MemoryTrain:  epoch  4, batch     2 | loss: 8.6983070Losses:  6.992275714874268 2.541259765625 1.4095745086669922
MemoryTrain:  epoch  4, batch     3 | loss: 6.9922757Losses:  3.43320894241333 -0.0 -0.0
MemoryTrain:  epoch  4, batch     4 | loss: 3.4332089Losses:  8.480413436889648 2.4437477588653564 3.3325858116149902
MemoryTrain:  epoch  5, batch     0 | loss: 8.4804134Losses:  9.326730728149414 3.3627588748931885 3.3964061737060547
MemoryTrain:  epoch  5, batch     1 | loss: 9.3267307Losses:  8.220725059509277 2.3468198776245117 3.418950319290161
MemoryTrain:  epoch  5, batch     2 | loss: 8.2207251Losses:  7.973249435424805 2.1026034355163574 3.3942267894744873
MemoryTrain:  epoch  5, batch     3 | loss: 7.9732494Losses:  2.666795492172241 -0.0 -0.0
MemoryTrain:  epoch  5, batch     4 | loss: 2.6667955Losses:  8.400583267211914 2.697747230529785 3.360231399536133
MemoryTrain:  epoch  6, batch     0 | loss: 8.4005833Losses:  7.143149375915527 3.541430950164795 1.42471444606781
MemoryTrain:  epoch  6, batch     1 | loss: 7.1431494Losses:  7.756550312042236 2.163687229156494 3.410709857940674
MemoryTrain:  epoch  6, batch     2 | loss: 7.7565503Losses:  5.097454071044922 2.964705467224121 -0.0
MemoryTrain:  epoch  6, batch     3 | loss: 5.0974541Losses:  1.8714776039123535 -0.0 -0.0
MemoryTrain:  epoch  6, batch     4 | loss: 1.8714776Losses:  8.810441970825195 3.2679643630981445 3.3535938262939453
MemoryTrain:  epoch  7, batch     0 | loss: 8.8104420Losses:  9.919363975524902 4.52039098739624 3.3489534854888916
MemoryTrain:  epoch  7, batch     1 | loss: 9.9193640Losses:  7.633987903594971 2.2222909927368164 3.345419406890869
MemoryTrain:  epoch  7, batch     2 | loss: 7.6339879Losses:  8.294670104980469 2.7253565788269043 3.3837623596191406
MemoryTrain:  epoch  7, batch     3 | loss: 8.2946701Losses:  2.176372766494751 -0.0 -0.0
MemoryTrain:  epoch  7, batch     4 | loss: 2.1763728Losses:  7.549088001251221 2.1404500007629395 3.369844436645508
MemoryTrain:  epoch  8, batch     0 | loss: 7.5490880Losses:  9.957587242126465 4.353916168212891 3.4743242263793945
MemoryTrain:  epoch  8, batch     1 | loss: 9.9575872Losses:  6.353280067443848 2.950741767883301 1.393005609512329
MemoryTrain:  epoch  8, batch     2 | loss: 6.3532801Losses:  6.426102161407471 3.0515308380126953 1.387513518333435
MemoryTrain:  epoch  8, batch     3 | loss: 6.4261022Losses:  2.105560302734375 -0.0 -0.0
MemoryTrain:  epoch  8, batch     4 | loss: 2.1055603Losses:  8.041858673095703 2.7244715690612793 3.357652187347412
MemoryTrain:  epoch  9, batch     0 | loss: 8.0418587Losses:  7.8550333976745605 2.473388671875 3.363460063934326
MemoryTrain:  epoch  9, batch     1 | loss: 7.8550334Losses:  6.691248416900635 3.222782611846924 1.4241422414779663
MemoryTrain:  epoch  9, batch     2 | loss: 6.6912484Losses:  9.101831436157227 3.654343843460083 3.3588995933532715
MemoryTrain:  epoch  9, batch     3 | loss: 9.1018314Losses:  1.9484851360321045 -0.0 -0.0
MemoryTrain:  epoch  9, batch     4 | loss: 1.9484851
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 71.53%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 78.37%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 79.91%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 80.21%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.64%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 85.80%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 85.11%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 84.46%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 84.03%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 83.61%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 83.39%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 83.65%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.06%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 83.23%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.58%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.81%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 84.71%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 85.03%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 84.88%   
cur_acc:  ['0.8466', '0.8021']
his_acc:  ['0.8466', '0.8488']
Clustering into  7  clusters
Clusters:  [1 3 5 1 6 1 1 2 1 0 4 0 1 0 3 1]
Losses:  14.555472373962402 7.379923343658447 1.3930723667144775
CurrentTrain: epoch  0, batch     0 | loss: 14.5554724Losses:  10.235177993774414 2.1844029426574707 1.3930308818817139
CurrentTrain: epoch  0, batch     1 | loss: 10.2351780Losses:  14.440181732177734 7.57318115234375 1.3869433403015137
CurrentTrain: epoch  1, batch     0 | loss: 14.4401817Losses:  9.062664031982422 2.73714017868042 1.41837739944458
CurrentTrain: epoch  1, batch     1 | loss: 9.0626640Losses:  12.620893478393555 7.213157653808594 1.3886884450912476
CurrentTrain: epoch  2, batch     0 | loss: 12.6208935Losses:  8.21567153930664 2.647582530975342 1.3996424674987793
CurrentTrain: epoch  2, batch     1 | loss: 8.2156715Losses:  12.0518159866333 7.093898296356201 1.39798104763031
CurrentTrain: epoch  3, batch     0 | loss: 12.0518160Losses:  6.574497222900391 2.1854259967803955 1.387225866317749
CurrentTrain: epoch  3, batch     1 | loss: 6.5744972Losses:  10.822661399841309 6.708779335021973 1.3956611156463623
CurrentTrain: epoch  4, batch     0 | loss: 10.8226614Losses:  6.609521865844727 2.2278666496276855 1.3874390125274658
CurrentTrain: epoch  4, batch     1 | loss: 6.6095219Losses:  11.934114456176758 7.816413402557373 1.3864864110946655
CurrentTrain: epoch  5, batch     0 | loss: 11.9341145Losses:  6.818359375 2.8835012912750244 1.387730360031128
CurrentTrain: epoch  5, batch     1 | loss: 6.8183594Losses:  11.14588451385498 7.229237079620361 1.3865760564804077
CurrentTrain: epoch  6, batch     0 | loss: 11.1458845Losses:  5.507518768310547 3.154738426208496 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 5.5075188Losses:  10.238780975341797 6.6294755935668945 1.398193120956421
CurrentTrain: epoch  7, batch     0 | loss: 10.2387810Losses:  6.058558464050293 2.4048874378204346 1.4377353191375732
CurrentTrain: epoch  7, batch     1 | loss: 6.0585585Losses:  9.869585037231445 6.171947479248047 1.3887135982513428
CurrentTrain: epoch  8, batch     0 | loss: 9.8695850Losses:  5.290765762329102 1.7983406782150269 1.389564871788025
CurrentTrain: epoch  8, batch     1 | loss: 5.2907658Losses:  9.494071960449219 5.962393283843994 1.3905490636825562
CurrentTrain: epoch  9, batch     0 | loss: 9.4940720Losses:  5.279860973358154 1.85528564453125 1.3866246938705444
CurrentTrain: epoch  9, batch     1 | loss: 5.2798610
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: born of schoolteacher parents in the western town of sabaneta on july 28 , 1954 , chavez studied at the military academy of venezuela in caracas .
Head Entity: chavez
Tail Entity: july 28 , 1954
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: the famous physicist albert einstein was born in ulm, in the kingdom of wurttemberg in the german empire on march 14, 1879.  
Head Entity: albert einstein  
Tail Entity: march 14, 1879  

Relation: person date of birth  
Context: the renowned author jane austen was born on december 16, 1775, in steventon, hampshire, england.  
Head Entity: jane austen  
Tail Entity: december 16, 1775  

Relation: person date of birth  
Context: the legendary musician freddie mercury was born farrokh bulsara on september 5, 1946, in zanzibar, tanzania.  
Head Entity: freddie mercury  
Tail Entity: september 5, 1946  

Relation: person date of birth  
Context: the influential civil rights leader martin luther king jr. was born on january 15, 1929, in atlanta, georgia.  
Head Entity: martin luther king jr.  
Tail Entity: january 15, 1929  

Relation: person date of birth  
Context: the famous painter pablo picasso was born on october 25, 1881, in malaga, spain.  
Head Entity: pablo picasso  
Tail Entity: october 25, 1881  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jefferson joseph deblanc sr. was born in lockport , la. , on feb. 15 , 1921 , and grew up in st. martinville .
Head Entity: jefferson joseph deblanc sr.
Tail Entity: la.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born in jersey city, nj, on august 3, 1941, and later moved to new york.  
Head Entity: martha stewart  
Tail Entity: nj  

Relation: person stateorprovince of birth  
Context: barack obama was born in honolulu, hi, on august 4, 1961, and spent his childhood in indonesia.  
Head Entity: barack obama  
Tail Entity: hi  

Relation: person stateorprovince of birth  
Context: taylor swift was born in reading, pa, on december 13, 1989, and later relocated to nashville, tennessee.  
Head Entity: taylor swift  
Tail Entity: pa  

Relation: person stateorprovince of birth  
Context: oprah winfrey was born in kosciusko, ms, on january 29, 1954, and grew up in milwaukee, wisconsin.  
Head Entity: oprah winfrey  
Tail Entity: ms  

Relation: person stateorprovince of birth  
Context: elon musk was born in pretoria, south africa, on june 28, 1971, and moved to the united states for his studies.  
Head Entity: elon musk  
Tail Entity: south africa  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: After the ceremony, Emily's father, John Smith, gave a heartfelt speech about family values and the importance of love.  
Head Entity: Emily  
Tail Entity: John Smith  

Relation: person parents  
Context: During the family reunion, Michael's mother, Sarah Johnson, shared stories from her childhood that made everyone laugh.  
Head Entity: Michael  
Tail Entity: Sarah Johnson  

Relation: person parents  
Context: At the graduation ceremony, Lisa's father, Robert Brown, proudly watched as she received her diploma, beaming with pride.  
Head Entity: Lisa  
Tail Entity: Robert Brown  

Relation: person parents  
Context: In the documentary, Anna's mother, Patricia Lee, discussed the challenges of raising a child as a single parent.  
Head Entity: Anna  
Tail Entity: Patricia Lee  

Relation: person parents  
Context: As they celebrated his birthday, David's father, Thomas White, reminisced about the day he was born and the joy he brought to their lives.  
Head Entity: David  
Tail Entity: Thomas White  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson has finally landed a job at one of the top tech companies in Silicon Valley, where she will be contributing to innovative projects.  
Head Entity: Sarah Thompson  
Tail Entity: top tech company  

Relation: person employee of  
Context: John Smith, a talented graphic designer, has been working for Creative Solutions for over five years, helping to shape their brand identity.  
Head Entity: John Smith  
Tail Entity: Creative Solutions  

Relation: person employee of  
Context: The renowned chef, Marco Pierre White, has been the head chef at several prestigious restaurants, showcasing his culinary skills to the world.  
Head Entity: Marco Pierre White  
Tail Entity: prestigious restaurants  

Relation: person employee of  
Context: Emily Chen, a passionate environmentalist, has joined Green Earth Initiative, where she will be leading projects aimed at sustainability and conservation.  
Head Entity: Emily Chen  
Tail Entity: Green Earth Initiative  

Relation: person employee of  
Context: After completing her degree in journalism, Lisa Martinez secured a position at The Daily News, where she covers local events and community stories.  
Head Entity: Lisa Martinez  
Tail Entity: The Daily News  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: millender-mcdonald , who was 68 , died late saturday at her home in carson , california , said her chief of staff , bandele mcqueen .
Head Entity: millender-mcdonald
Tail Entity: california
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: johnson, who was 75, passed away peacefully in his sleep at his residence in phoenix, arizona, surrounded by family.  
Head Entity: johnson  
Tail Entity: arizona  

Relation: person stateorprovince of death  
Context: the renowned author, elena, died at the age of 82 in her apartment in new york, new york, after a long battle with illness.  
Head Entity: elena  
Tail Entity: new york  

Relation: person stateorprovince of death  
Context: after a long and fulfilling life, robert succumbed to his illness at the age of 90 in a hospital in miami, florida.  
Head Entity: robert  
Tail Entity: florida  

Relation: person stateorprovince of death  
Context: the beloved actor, sam, tragically lost his life in a car accident in los angeles, california, at the young age of 45.  
Head Entity: sam  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: the famous scientist, dr. harris, passed away at the age of 67 in a research facility in boston, massachusetts, where he had dedicated his life to his work.  
Head Entity: dr. harris  
Tail Entity: massachusetts  
Losses:  11.294578552246094 2.1701865196228027 5.801109313964844
MemoryTrain:  epoch  0, batch     0 | loss: 11.2945786Losses:  14.35693359375 1.8128094673156738 8.32221794128418
MemoryTrain:  epoch  0, batch     1 | loss: 14.3569336Losses:  15.840755462646484 1.3367700576782227 10.98072624206543
MemoryTrain:  epoch  0, batch     2 | loss: 15.8407555Losses:  12.11322021484375 3.19840145111084 5.692073345184326
MemoryTrain:  epoch  0, batch     3 | loss: 12.1132202Losses:  18.425710678100586 1.9610586166381836 11.085329055786133
MemoryTrain:  epoch  0, batch     4 | loss: 18.4257107Losses:  12.131563186645508 1.8696186542510986 5.867616176605225
MemoryTrain:  epoch  0, batch     5 | loss: 12.1315632Losses:  12.716730117797852 1.624950885772705 8.287919998168945
MemoryTrain:  epoch  1, batch     0 | loss: 12.7167301Losses:  11.926923751831055 1.837306261062622 5.73635196685791
MemoryTrain:  epoch  1, batch     1 | loss: 11.9269238Losses:  13.736863136291504 1.6300983428955078 8.210744857788086
MemoryTrain:  epoch  1, batch     2 | loss: 13.7368631Losses:  16.49812889099121 1.4195445775985718 10.938406944274902
MemoryTrain:  epoch  1, batch     3 | loss: 16.4981289Losses:  11.159932136535645 1.7137320041656494 5.725209712982178
MemoryTrain:  epoch  1, batch     4 | loss: 11.1599321Losses:  12.025161743164062 0.8036428689956665 8.22326946258545
MemoryTrain:  epoch  1, batch     5 | loss: 12.0251617Losses:  15.103955268859863 1.0248948335647583 10.904389381408691
MemoryTrain:  epoch  2, batch     0 | loss: 15.1039553Losses:  10.726581573486328 1.314540147781372 5.718024730682373
MemoryTrain:  epoch  2, batch     1 | loss: 10.7265816Losses:  15.482897758483887 1.1120514869689941 10.885675430297852
MemoryTrain:  epoch  2, batch     2 | loss: 15.4828978Losses:  13.27822494506836 2.306142807006836 8.281519889831543
MemoryTrain:  epoch  2, batch     3 | loss: 13.2782249Losses:  15.148588180541992 1.0930428504943848 10.982598304748535
MemoryTrain:  epoch  2, batch     4 | loss: 15.1485882Losses:  15.253154754638672 0.8855320811271667 10.881714820861816
MemoryTrain:  epoch  2, batch     5 | loss: 15.2531548Losses:  8.832908630371094 2.968655824661255 3.303828716278076
MemoryTrain:  epoch  3, batch     0 | loss: 8.8329086Losses:  12.807221412658691 1.4330323934555054 8.149711608886719
MemoryTrain:  epoch  3, batch     1 | loss: 12.8072214Losses:  15.242048263549805 1.04928457736969 10.933402061462402
MemoryTrain:  epoch  3, batch     2 | loss: 15.2420483Losses:  8.641778945922852 1.6034060716629028 3.474618911743164
MemoryTrain:  epoch  3, batch     3 | loss: 8.6417789Losses:  12.135480880737305 1.33162260055542 8.211332321166992
MemoryTrain:  epoch  3, batch     4 | loss: 12.1354809Losses:  12.953004837036133 1.944841980934143 8.215537071228027
MemoryTrain:  epoch  3, batch     5 | loss: 12.9530048Losses:  12.979394912719727 1.8384207487106323 8.198022842407227
MemoryTrain:  epoch  4, batch     0 | loss: 12.9793949Losses:  12.346409797668457 1.3748080730438232 8.179667472839355
MemoryTrain:  epoch  4, batch     1 | loss: 12.3464098Losses:  7.733728885650635 1.6896615028381348 3.3463497161865234
MemoryTrain:  epoch  4, batch     2 | loss: 7.7337289Losses:  11.835796356201172 1.4411429166793823 8.198789596557617
MemoryTrain:  epoch  4, batch     3 | loss: 11.8357964Losses:  11.976348876953125 0.985442042350769 8.185773849487305
MemoryTrain:  epoch  4, batch     4 | loss: 11.9763489Losses:  12.390046119689941 2.0937201976776123 8.120343208312988
MemoryTrain:  epoch  4, batch     5 | loss: 12.3900461Losses:  10.493946075439453 2.3226065635681152 5.694488048553467
MemoryTrain:  epoch  5, batch     0 | loss: 10.4939461Losses:  11.315862655639648 0.7838973999023438 8.165404319763184
MemoryTrain:  epoch  5, batch     1 | loss: 11.3158627Losses:  15.037489891052246 1.4011461734771729 10.859896659851074
MemoryTrain:  epoch  5, batch     2 | loss: 15.0374899Losses:  11.24232006072998 1.0816075801849365 8.096899032592773
MemoryTrain:  epoch  5, batch     3 | loss: 11.2423201Losses:  14.656536102294922 1.6149592399597168 10.97917366027832
MemoryTrain:  epoch  5, batch     4 | loss: 14.6565361Losses:  9.826251983642578 2.1761832237243652 5.659715175628662
MemoryTrain:  epoch  5, batch     5 | loss: 9.8262520Losses:  7.688331604003906 2.162649393081665 3.3306901454925537
MemoryTrain:  epoch  6, batch     0 | loss: 7.6883316Losses:  9.57375717163086 1.8288793563842773 5.625967979431152
MemoryTrain:  epoch  6, batch     1 | loss: 9.5737572Losses:  13.064539909362793 2.5739662647247314 8.21420955657959
MemoryTrain:  epoch  6, batch     2 | loss: 13.0645399Losses:  11.092076301574707 0.8380001187324524 8.107881546020508
MemoryTrain:  epoch  6, batch     3 | loss: 11.0920763Losses:  8.857386589050293 1.0134832859039307 5.6576666831970215
MemoryTrain:  epoch  6, batch     4 | loss: 8.8573866Losses:  11.630802154541016 1.3065851926803589 8.142010688781738
MemoryTrain:  epoch  6, batch     5 | loss: 11.6308022Losses:  11.569397926330566 1.083918571472168 8.125104904174805
MemoryTrain:  epoch  7, batch     0 | loss: 11.5693979Losses:  11.480948448181152 1.1058714389801025 8.186233520507812
MemoryTrain:  epoch  7, batch     1 | loss: 11.4809484Losses:  15.432780265808105 2.4166457653045654 10.932757377624512
MemoryTrain:  epoch  7, batch     2 | loss: 15.4327803Losses:  8.877355575561523 1.0745115280151367 5.643086910247803
MemoryTrain:  epoch  7, batch     3 | loss: 8.8773556Losses:  9.967662811279297 2.2781355381011963 5.557748317718506
MemoryTrain:  epoch  7, batch     4 | loss: 9.9676628Losses:  9.352128982543945 1.7561014890670776 5.606929302215576
MemoryTrain:  epoch  7, batch     5 | loss: 9.3521290Losses:  11.502118110656738 1.2820651531219482 8.1323881149292
MemoryTrain:  epoch  8, batch     0 | loss: 11.5021181Losses:  11.608345031738281 1.510792851448059 8.112527847290039
MemoryTrain:  epoch  8, batch     1 | loss: 11.6083450Losses:  14.602136611938477 1.6839505434036255 10.83313274383545
MemoryTrain:  epoch  8, batch     2 | loss: 14.6021366Losses:  11.76384162902832 1.663869857788086 8.089823722839355
MemoryTrain:  epoch  8, batch     3 | loss: 11.7638416Losses:  11.144904136657715 0.990304172039032 8.167957305908203
MemoryTrain:  epoch  8, batch     4 | loss: 11.1449041Losses:  11.615068435668945 1.3592454195022583 8.229707717895508
MemoryTrain:  epoch  8, batch     5 | loss: 11.6150684Losses:  12.108227729797363 1.9033207893371582 8.230104446411133
MemoryTrain:  epoch  9, batch     0 | loss: 12.1082277Losses:  9.580940246582031 1.9325040578842163 5.575213432312012
MemoryTrain:  epoch  9, batch     1 | loss: 9.5809402Losses:  8.05787467956543 2.7915687561035156 3.308734893798828
MemoryTrain:  epoch  9, batch     2 | loss: 8.0578747Losses:  9.091185569763184 1.4707590341567993 5.593308448791504
MemoryTrain:  epoch  9, batch     3 | loss: 9.0911856Losses:  13.225883483886719 3.126163959503174 8.157328605651855
MemoryTrain:  epoch  9, batch     4 | loss: 13.2258835Losses:  15.709959030151367 2.9027633666992188 10.834908485412598
MemoryTrain:  epoch  9, batch     5 | loss: 15.7099590
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 77.84%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 76.79%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 84.86%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.04%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 85.89%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.13%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 84.85%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 83.27%   [EVAL] batch:   34 | acc: 31.25%,  total acc: 81.79%   [EVAL] batch:   35 | acc: 25.00%,  total acc: 80.21%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 78.89%   [EVAL] batch:   37 | acc: 31.25%,  total acc: 77.63%   [EVAL] batch:   38 | acc: 37.50%,  total acc: 76.60%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 76.88%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 75.76%   [EVAL] batch:   41 | acc: 50.00%,  total acc: 75.15%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 74.56%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 74.57%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 77.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 77.45%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 77.40%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 77.36%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 77.20%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 76.93%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 76.64%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 76.94%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 77.22%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 77.46%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 77.72%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 77.78%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 76.86%   
cur_acc:  ['0.8466', '0.8021', '0.7679']
his_acc:  ['0.8466', '0.8488', '0.7686']
Clustering into  9  clusters
Clusters:  [0 2 4 0 1 0 0 6 0 8 5 3 0 8 2 0 7 3 0 4 1]
Losses:  17.150930404663086 6.913047790527344 5.909775257110596
CurrentTrain: epoch  0, batch     0 | loss: 17.1509304Losses:  13.22728157043457 2.5173251628875732 5.635962009429932
CurrentTrain: epoch  0, batch     1 | loss: 13.2272816Losses:  16.56287384033203 6.879891395568848 5.690446853637695
CurrentTrain: epoch  1, batch     0 | loss: 16.5628738Losses:  11.407320976257324 2.0544328689575195 5.768597602844238
CurrentTrain: epoch  1, batch     1 | loss: 11.4073210Losses:  16.034461975097656 7.299167633056641 5.641484260559082
CurrentTrain: epoch  2, batch     0 | loss: 16.0344620Losses:  9.839679718017578 3.0297956466674805 3.3590354919433594
CurrentTrain: epoch  2, batch     1 | loss: 9.8396797Losses:  15.35623550415039 6.4794487953186035 5.6427130699157715
CurrentTrain: epoch  3, batch     0 | loss: 15.3562355Losses:  10.046537399291992 1.8241331577301025 5.645245552062988
CurrentTrain: epoch  3, batch     1 | loss: 10.0465374Losses:  14.737707138061523 6.4982171058654785 5.6429057121276855
CurrentTrain: epoch  4, batch     0 | loss: 14.7377071Losses:  8.154258728027344 2.3110134601593018 3.3250255584716797
CurrentTrain: epoch  4, batch     1 | loss: 8.1542587Losses:  15.184375762939453 7.025060176849365 5.616397380828857
CurrentTrain: epoch  5, batch     0 | loss: 15.1843758Losses:  8.62625503540039 2.716161012649536 3.39693546295166
CurrentTrain: epoch  5, batch     1 | loss: 8.6262550Losses:  13.947517395019531 6.066240310668945 5.617502689361572
CurrentTrain: epoch  6, batch     0 | loss: 13.9475174Losses:  9.277750015258789 1.6798789501190186 5.610716819763184
CurrentTrain: epoch  6, batch     1 | loss: 9.2777500Losses:  14.977014541625977 7.250863075256348 5.611907958984375
CurrentTrain: epoch  7, batch     0 | loss: 14.9770145Losses:  8.468356132507324 3.2586987018585205 3.314084053039551
CurrentTrain: epoch  7, batch     1 | loss: 8.4683561Losses:  13.033344268798828 5.574094295501709 5.593102931976318
CurrentTrain: epoch  8, batch     0 | loss: 13.0333443Losses:  9.416627883911133 1.80973219871521 5.586442470550537
CurrentTrain: epoch  8, batch     1 | loss: 9.4166279Losses:  14.288619995117188 6.808238983154297 5.588038444519043
CurrentTrain: epoch  9, batch     0 | loss: 14.2886200Losses:  8.544807434082031 3.3308815956115723 3.3053030967712402
CurrentTrain: epoch  9, batch     1 | loss: 8.5448074
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
1. Relation: organization founded  
   Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, innovative solutions, which has since become a leader in software development.  
   Head Entity: innovative solutions  
   Tail Entity: 1998  

2. Relation: organization founded  
   Context: After years of research and development, the non-profit organization green earth was officially launched in 2010 to promote environmental sustainability.  
   Head Entity: green earth  
   Tail Entity: 2010  

3. Relation: organization founded  
   Context: The famous chef opened his first restaurant, culinary delights, in 2015, which quickly gained popularity for its unique fusion cuisine.  
   Head Entity: culinary delights  
   Tail Entity: 2015  

4. Relation: organization founded  
   Context: In 2001, a group of artists and activists founded the cultural collective, art for change, to support local artists and promote social justice through art.  
   Head Entity: art for change  
   Tail Entity: 2001  

5. Relation: organization founded  
   Context: The tech giant, future tech innovations, was established in 2020 by a team of visionary entrepreneurs aiming to revolutionize the industry with cutting-edge technology.  
   Head Entity: future tech innovations  
   Tail Entity: 2020  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, emily decided to throw a big party for her friends.  
Head Entity: emily  
Tail Entity: 30  

Relation: person age  
Context: the renowned scientist albert einstein was born in 1879 and lived to be 76 years old.  
Head Entity: albert einstein  
Tail Entity: 76  

Relation: person age  
Context: during the interview, the journalist revealed that she is only 25 years old.  
Head Entity: the journalist  
Tail Entity: 25  

Relation: person age  
Context: my grandmother turned 85 last month, and she still enjoys gardening every day.  
Head Entity: my grandmother  
Tail Entity: 85  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: forsberg was born in 1943 in huntsville , ala. , and grew up on long island in new york .
Head Entity: forsberg
Tail Entity: huntsville
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in 1990 in barcelona, spain, where she spent her childhood.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during a press conference, it was revealed that john was born in 1985 in seattle, washington.  
Head Entity: john  
Tail Entity: seattle  

Relation: person city of birth  
Context: the famous author was born in 1975 in dublin, ireland, and later moved to london.  
Head Entity: the famous author  
Tail Entity: dublin  

Relation: person city of birth  
Context: after years of research, it was discovered that marie was born in 1867 in warsaw, poland.  
Head Entity: marie  
Tail Entity: warsaw  

Relation: person city of birth  
Context: the musician proudly stated that he was born in 1992 in nashville, tennessee, a city known for its music scene.  
Head Entity: the musician  
Tail Entity: nashville  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians, including members from the London Symphony Orchestra.  
Head Entity: London Symphony Orchestra  
Tail Entity: New York Philharmonic  

Relation: organization members  
Context: The National Football League has several teams, including the Dallas Cowboys, which is known for its iconic cheerleaders who are members of the organization.  
Head Entity: Dallas Cowboys  
Tail Entity: NFL Cheerleaders  

Relation: organization members  
Context: The United Nations consists of various specialized agencies, including the World Health Organization, which plays a crucial role in global health initiatives.  
Head Entity: World Health Organization  
Tail Entity: United Nations  

Relation: organization members  
Context: The International Olympic Committee oversees the Olympic Games and has various national committees, such as the United States Olympic and Paralympic Committee, which is responsible for American athletes.  
Head Entity: United States Olympic and Paralympic Committee  
Tail Entity: International Olympic Committee  

Relation: organization members  
Context: The European Union is made up of several member states, including Germany, which plays a significant role in the EU's economic policies.  
Head Entity: Germany  
Tail Entity: European Union  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: devout catholic miss boyle has been at the centre of a media storm since news emerged of how she wowed britain 's got talent judges simon cowell and piers morgan with her amazing opera voice .
Head Entity: boyle
Tail Entity: catholic
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of dedication to her faith, Maria finally decided to become a nun, embracing her identity as a devoted member of the Catholic Church.  
Head Entity: Maria  
Tail Entity: Catholic Church  

Relation: person religion  
Context: The famous actor, known for his philanthropic work, often speaks about his deep connection to Buddhism and how it influences his life choices.  
Head Entity: actor  
Tail Entity: Buddhism  

Relation: person religion  
Context: Growing up in a Jewish household, David has always celebrated the traditions of his ancestors, feeling a strong bond with his Jewish heritage.  
Head Entity: David  
Tail Entity: Jewish  

Relation: person religion  
Context: As a prominent leader in the community, Imam Ali often shares his insights on Islam, guiding many towards a deeper understanding of their faith.  
Head Entity: Imam Ali  
Tail Entity: Islam  

Relation: person religion  
Context: The renowned author frequently discusses her experiences as a Quaker, highlighting how her beliefs shape her writing and worldview.  
Head Entity: author  
Tail Entity: Quaker  
Losses:  11.88271713256836 1.3432995080947876 5.6115264892578125
MemoryTrain:  epoch  0, batch     0 | loss: 11.8827171Losses:  15.995771408081055 1.7074685096740723 10.8201322555542
MemoryTrain:  epoch  0, batch     1 | loss: 15.9957714Losses:  21.912395477294922 0.5299789309501648 16.81745147705078
MemoryTrain:  epoch  0, batch     2 | loss: 21.9123955Losses:  14.135528564453125 1.8396133184432983 8.249041557312012
MemoryTrain:  epoch  0, batch     3 | loss: 14.1355286Losses:  17.018829345703125 2.4886841773986816 10.88160514831543
MemoryTrain:  epoch  0, batch     4 | loss: 17.0188293Losses:  12.069755554199219 1.7833631038665771 5.693007469177246
MemoryTrain:  epoch  0, batch     5 | loss: 12.0697556Losses:  19.029220581054688 1.0378844738006592 13.757718086242676
MemoryTrain:  epoch  0, batch     6 | loss: 19.0292206Losses:  9.716487884521484 0.49538135528564453 5.687093257904053
MemoryTrain:  epoch  0, batch     7 | loss: 9.7164879Losses:  12.25773811340332 3.0742087364196777 5.573442459106445
MemoryTrain:  epoch  1, batch     0 | loss: 12.2577381Losses:  15.25607681274414 1.0167973041534424 10.891980171203613
MemoryTrain:  epoch  1, batch     1 | loss: 15.2560768Losses:  11.457616806030273 1.3009886741638184 5.888650417327881
MemoryTrain:  epoch  1, batch     2 | loss: 11.4576168Losses:  16.311840057373047 1.9120526313781738 10.946880340576172
MemoryTrain:  epoch  1, batch     3 | loss: 16.3118401Losses:  14.536613464355469 1.9314773082733154 8.216493606567383
MemoryTrain:  epoch  1, batch     4 | loss: 14.5366135Losses:  14.53434944152832 2.0640764236450195 8.12926959991455
MemoryTrain:  epoch  1, batch     5 | loss: 14.5343494Losses:  12.664966583251953 2.0829720497131348 8.139433860778809
MemoryTrain:  epoch  1, batch     6 | loss: 12.6649666Losses:  15.47818374633789 0.8193780779838562 10.886248588562012
MemoryTrain:  epoch  1, batch     7 | loss: 15.4781837Losses:  10.022579193115234 1.4744362831115723 5.717085838317871
MemoryTrain:  epoch  2, batch     0 | loss: 10.0225792Losses:  12.6137056350708 0.8086123466491699 8.144363403320312
MemoryTrain:  epoch  2, batch     1 | loss: 12.6137056Losses:  18.82911491394043 2.1679694652557373 13.76380729675293
MemoryTrain:  epoch  2, batch     2 | loss: 18.8291149Losses:  17.435157775878906 0.49153101444244385 13.711735725402832
MemoryTrain:  epoch  2, batch     3 | loss: 17.4351578Losses:  10.896438598632812 1.4976760149002075 5.611377239227295
MemoryTrain:  epoch  2, batch     4 | loss: 10.8964386Losses:  17.721172332763672 3.471132755279541 10.938694953918457
MemoryTrain:  epoch  2, batch     5 | loss: 17.7211723Losses:  14.086698532104492 2.820918560028076 8.172994613647461
MemoryTrain:  epoch  2, batch     6 | loss: 14.0866985Losses:  12.578437805175781 1.3631149530410767 8.150459289550781
MemoryTrain:  epoch  2, batch     7 | loss: 12.5784378Losses:  17.9520206451416 0.8593118190765381 13.697757720947266
MemoryTrain:  epoch  3, batch     0 | loss: 17.9520206Losses:  15.917707443237305 1.322644829750061 10.837903022766113
MemoryTrain:  epoch  3, batch     1 | loss: 15.9177074Losses:  14.116376876831055 1.0466482639312744 10.852675437927246
MemoryTrain:  epoch  3, batch     2 | loss: 14.1163769Losses:  15.17939567565918 2.1963963508605957 10.888415336608887
MemoryTrain:  epoch  3, batch     3 | loss: 15.1793957Losses:  19.95211410522461 2.2182435989379883 13.767254829406738
MemoryTrain:  epoch  3, batch     4 | loss: 19.9521141Losses:  19.730731964111328 2.7276103496551514 13.760492324829102
MemoryTrain:  epoch  3, batch     5 | loss: 19.7307320Losses:  14.10201644897461 0.7561708688735962 10.803824424743652
MemoryTrain:  epoch  3, batch     6 | loss: 14.1020164Losses:  11.11211109161377 0.9575827717781067 8.102334976196289
MemoryTrain:  epoch  3, batch     7 | loss: 11.1121111Losses:  14.65619945526123 0.8382082581520081 10.852975845336914
MemoryTrain:  epoch  4, batch     0 | loss: 14.6561995Losses:  17.172603607177734 1.0275198221206665 13.73621940612793
MemoryTrain:  epoch  4, batch     1 | loss: 17.1726036Losses:  17.618759155273438 1.1894092559814453 13.72305965423584
MemoryTrain:  epoch  4, batch     2 | loss: 17.6187592Losses:  15.761709213256836 2.0131304264068604 10.8479642868042
MemoryTrain:  epoch  4, batch     3 | loss: 15.7617092Losses:  17.044645309448242 0.7824870944023132 13.733964920043945
MemoryTrain:  epoch  4, batch     4 | loss: 17.0446453Losses:  12.190301895141602 1.764594554901123 8.099259376525879
MemoryTrain:  epoch  4, batch     5 | loss: 12.1903019Losses:  15.067911148071289 0.8695684671401978 10.845999717712402
MemoryTrain:  epoch  4, batch     6 | loss: 15.0679111Losses:  11.615937232971191 1.0381206274032593 8.134989738464355
MemoryTrain:  epoch  4, batch     7 | loss: 11.6159372Losses:  11.972074508666992 1.7971339225769043 8.126314163208008
MemoryTrain:  epoch  5, batch     0 | loss: 11.9720745Losses:  16.002700805664062 2.649611234664917 10.796655654907227
MemoryTrain:  epoch  5, batch     1 | loss: 16.0027008Losses:  12.116923332214355 1.246781826019287 8.083273887634277
MemoryTrain:  epoch  5, batch     2 | loss: 12.1169233Losses:  12.222352981567383 0.8633456230163574 8.093862533569336
MemoryTrain:  epoch  5, batch     3 | loss: 12.2223530Losses:  16.94976806640625 0.7634243965148926 13.771103858947754
MemoryTrain:  epoch  5, batch     4 | loss: 16.9497681Losses:  14.835996627807617 1.0406701564788818 10.845213890075684
MemoryTrain:  epoch  5, batch     5 | loss: 14.8359966Losses:  16.731060028076172 0.4558311104774475 13.727866172790527
MemoryTrain:  epoch  5, batch     6 | loss: 16.7310600Losses:  12.309112548828125 1.6951804161071777 8.088763236999512
MemoryTrain:  epoch  5, batch     7 | loss: 12.3091125Losses:  16.758970260620117 0.7413772344589233 13.723115921020508
MemoryTrain:  epoch  6, batch     0 | loss: 16.7589703Losses:  17.15231704711914 1.057459831237793 13.741809844970703
MemoryTrain:  epoch  6, batch     1 | loss: 17.1523170Losses:  12.561872482299805 0.912187397480011 8.113813400268555
MemoryTrain:  epoch  6, batch     2 | loss: 12.5618725Losses:  14.234649658203125 0.8875682950019836 10.801570892333984
MemoryTrain:  epoch  6, batch     3 | loss: 14.2346497Losses:  9.871273040771484 1.3291804790496826 5.5630879402160645
MemoryTrain:  epoch  6, batch     4 | loss: 9.8712730Losses:  8.817773818969727 1.2802037000656128 5.652408599853516
MemoryTrain:  epoch  6, batch     5 | loss: 8.8177738Losses:  14.535679817199707 1.4024972915649414 10.789902687072754
MemoryTrain:  epoch  6, batch     6 | loss: 14.5356798Losses:  12.897216796875 2.4221410751342773 8.087915420532227
MemoryTrain:  epoch  6, batch     7 | loss: 12.8972168Losses:  14.343406677246094 1.1809885501861572 10.959585189819336
MemoryTrain:  epoch  7, batch     0 | loss: 14.3434067Losses:  14.34468936920166 1.0311470031738281 10.843894004821777
MemoryTrain:  epoch  7, batch     1 | loss: 14.3446894Losses:  17.589237213134766 1.6117327213287354 13.694452285766602
MemoryTrain:  epoch  7, batch     2 | loss: 17.5892372Losses:  16.656810760498047 1.081449270248413 13.679177284240723
MemoryTrain:  epoch  7, batch     3 | loss: 16.6568108Losses:  14.132026672363281 1.0609169006347656 10.795515060424805
MemoryTrain:  epoch  7, batch     4 | loss: 14.1320267Losses:  10.813974380493164 1.6236342191696167 5.602262020111084
MemoryTrain:  epoch  7, batch     5 | loss: 10.8139744Losses:  20.78876495361328 1.378880262374878 16.739336013793945
MemoryTrain:  epoch  7, batch     6 | loss: 20.7887650Losses:  10.311594009399414 1.815728783607483 5.600213050842285
MemoryTrain:  epoch  7, batch     7 | loss: 10.3115940Losses:  11.638997077941895 0.9980725646018982 8.074925422668457
MemoryTrain:  epoch  8, batch     0 | loss: 11.6389971Losses:  11.16143798828125 0.8501759767532349 8.105842590332031
MemoryTrain:  epoch  8, batch     1 | loss: 11.1614380Losses:  14.856990814208984 1.1305204629898071 10.83519458770752
MemoryTrain:  epoch  8, batch     2 | loss: 14.8569908Losses:  15.787721633911133 1.6743825674057007 10.824511528015137
MemoryTrain:  epoch  8, batch     3 | loss: 15.7877216Losses:  20.571041107177734 1.0192980766296387 16.75996208190918
MemoryTrain:  epoch  8, batch     4 | loss: 20.5710411Losses:  12.753870010375977 2.7028632164001465 8.10523796081543
MemoryTrain:  epoch  8, batch     5 | loss: 12.7538700Losses:  12.281551361083984 2.3222391605377197 8.083175659179688
MemoryTrain:  epoch  8, batch     6 | loss: 12.2815514Losses:  13.741501808166504 0.5124118328094482 10.802399635314941
MemoryTrain:  epoch  8, batch     7 | loss: 13.7415018Losses:  16.740339279174805 0.5107966065406799 13.667224884033203
MemoryTrain:  epoch  9, batch     0 | loss: 16.7403393Losses:  17.2644100189209 0.7798113822937012 13.657661437988281
MemoryTrain:  epoch  9, batch     1 | loss: 17.2644100Losses:  13.432464599609375 0.7346119284629822 10.810431480407715
MemoryTrain:  epoch  9, batch     2 | loss: 13.4324646Losses:  8.461616516113281 0.9609524607658386 5.603034973144531
MemoryTrain:  epoch  9, batch     3 | loss: 8.4616165Losses:  8.254076957702637 0.48161932826042175 5.568030834197998
MemoryTrain:  epoch  9, batch     4 | loss: 8.2540770Losses:  16.582395553588867 0.4815525412559509 13.67687702178955
MemoryTrain:  epoch  9, batch     5 | loss: 16.5823956Losses:  13.971429824829102 0.7223959565162659 10.783308029174805
MemoryTrain:  epoch  9, batch     6 | loss: 13.9714298Losses:  14.29216194152832 0.5155197978019714 10.79469108581543
MemoryTrain:  epoch  9, batch     7 | loss: 14.2921619
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 99.31%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 87.05%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 82.21%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.84%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 83.71%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 81.25%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 78.93%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 76.91%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 74.83%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 72.86%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 71.47%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 70.88%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 70.54%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 70.06%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 70.17%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 71.47%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 73.50%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 73.28%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 73.35%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 73.15%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 72.84%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 72.66%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 72.70%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 73.06%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 73.41%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 73.67%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 73.89%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 74.01%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 74.32%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 74.71%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 75.09%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 75.83%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 76.52%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 77.17%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 76.71%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 76.60%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 76.67%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 76.64%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 76.62%   [EVAL] batch:   77 | acc: 6.25%,  total acc: 75.72%   
cur_acc:  ['0.8466', '0.8021', '0.7679', '0.8705']
his_acc:  ['0.8466', '0.8488', '0.7686', '0.7572']
Clustering into  12  clusters
Clusters:  [ 0  1  5  9  1  0  0 10  0  2  6  4  0  2  1  0  7  4  0  5  3  8  3  0
 11  0]
Losses:  17.3431339263916 10.096207618713379 3.372607946395874
CurrentTrain: epoch  0, batch     0 | loss: 17.3431339Losses:  10.236353874206543 6.767991065979004 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 10.2363539Losses:  12.381856918334961 5.80810546875 3.337045907974243
CurrentTrain: epoch  1, batch     0 | loss: 12.3818569Losses:  7.04248046875 1.3287110328674316 3.310459852218628
CurrentTrain: epoch  1, batch     1 | loss: 7.0424805Losses:  12.249967575073242 6.462206840515137 3.3236560821533203
CurrentTrain: epoch  2, batch     0 | loss: 12.2499676Losses:  8.0973482131958 2.4735302925109863 3.3067266941070557
CurrentTrain: epoch  2, batch     1 | loss: 8.0973482Losses:  11.613065719604492 6.005701541900635 3.326448917388916
CurrentTrain: epoch  3, batch     0 | loss: 11.6130657Losses:  6.9963059425354 1.6638948917388916 3.321779251098633
CurrentTrain: epoch  3, batch     1 | loss: 6.9963059Losses:  11.512609481811523 6.216882228851318 3.315208673477173
CurrentTrain: epoch  4, batch     0 | loss: 11.5126095Losses:  7.336618423461914 2.035062789916992 3.3149847984313965
CurrentTrain: epoch  4, batch     1 | loss: 7.3366184Losses:  12.191554069519043 6.94911003112793 3.314312219619751
CurrentTrain: epoch  5, batch     0 | loss: 12.1915541Losses:  6.415221214294434 3.242445707321167 1.3900277614593506
CurrentTrain: epoch  5, batch     1 | loss: 6.4152212Losses:  11.112689971923828 5.975056171417236 3.3138608932495117
CurrentTrain: epoch  6, batch     0 | loss: 11.1126900Losses:  7.34783935546875 2.249465227127075 3.308858871459961
CurrentTrain: epoch  6, batch     1 | loss: 7.3478394Losses:  10.506664276123047 5.4142866134643555 3.3103411197662354
CurrentTrain: epoch  7, batch     0 | loss: 10.5066643Losses:  6.732822418212891 1.6733156442642212 3.3115596771240234
CurrentTrain: epoch  7, batch     1 | loss: 6.7328224Losses:  11.446161270141602 6.093929290771484 3.3192973136901855
CurrentTrain: epoch  8, batch     0 | loss: 11.4461613Losses:  7.329442977905273 2.1746115684509277 3.3019261360168457
CurrentTrain: epoch  8, batch     1 | loss: 7.3294430Losses:  10.194536209106445 4.985149383544922 3.3041505813598633
CurrentTrain: epoch  9, batch     0 | loss: 10.1945362Losses:  6.2756147384643555 0.9965866208076477 3.316453456878662
CurrentTrain: epoch  9, batch     1 | loss: 6.2756147
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned author, jane smith, tragically lost her life due to a car accident while returning from a book signing event.  
Head Entity: jane smith  
Tail Entity: car accident  

Relation: person cause of death  
Context: following a long struggle with heart disease, mr. thompson succumbed to his illness last night at the hospital.  
Head Entity: mr. thompson  
Tail Entity: heart disease  

Relation: person cause of death  
Context: the community mourned the loss of their beloved mayor, who died from a sudden stroke during a council meeting.  
Head Entity: the mayor  
Tail Entity: stroke  

Relation: person cause of death  
Context: after a courageous fight against pancreatic cancer, elizabeth jones passed away, leaving behind a legacy of kindness and compassion.  
Head Entity: elizabeth jones  
Tail Entity: pancreatic cancer  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the head of the organisation of the islamic conference -lrb- oic -rrb- said us president barack obama 's speech on thursday would help win hearts in the muslim world .
Head Entity: oic
Tail Entity: muslim
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
1. Relation: organization political religious affiliation  
   Context: The Catholic Church has been a significant player in global politics, influencing various governments and policies.  
   Head Entity: Catholic Church  
   Tail Entity: Catholicism  

2. Relation: organization political religious affiliation  
   Context: The World Council of Churches aims to promote unity among Christian denominations and has been involved in various political discussions.  
   Head Entity: World Council of Churches  
   Tail Entity: Christianity  

3. Relation: organization political religious affiliation  
   Context: The Islamic Society of North America has been actively engaging with political leaders to advocate for Muslim rights in the United States.  
   Head Entity: Islamic Society of North America  
   Tail Entity: Islam  

4. Relation: organization political religious affiliation  
   Context: The Jewish Federation has been instrumental in lobbying for policies that support Jewish communities worldwide.  
   Head Entity: Jewish Federation  
   Tail Entity: Judaism  

5. Relation: organization political religious affiliation  
   Context: The National Association of Evangelicals has been a vocal advocate for issues that align with evangelical Christian values in the political arena.  
   Head Entity: National Association of Evangelicals  
   Tail Entity: Evangelical Christianity  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: andrew lebow , an oil trader with mf global in new york , said investors have been discouraged by lower-than-expected oil imports in china and the disappointing growth in the u.s. economy .
Head Entity: mf global
Tail Entity: new york
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been a hub for innovation and development.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation unilever has its main office situated in london, england, which serves as a strategic location for its global operations.  
Head Entity: unilever  
Tail Entity: england  

Relation: organization stateorprovince of headquarters  
Context: according to recent reports, the financial services firm goldman sachs has its headquarters in new york city, new york, making it a key player in the financial district.  
Head Entity: goldman sachs  
Tail Entity: new york  

Relation: organization stateorprovince of headquarters  
Context: the automotive manufacturer toyota has its North American headquarters in plano, texas, which has become a central point for its operations in the region.  
Head Entity: toyota  
Tail Entity: texas  

Relation: organization stateorprovince of headquarters  
Context: the global consulting firm accenture is headquartered in dublin, ireland, where it coordinates its services across various countries.  
Head Entity: accenture  
Tail Entity: ireland  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: The famous actor's cousin, Sarah, often shares stories about their childhood adventures together.  
Head Entity: The famous actor  
Tail Entity: Sarah  

Relation: person other family  
Context: During the family reunion, Michael introduced his aunt, Linda, who had traveled from out of state to join the festivities.  
Head Entity: Michael  
Tail Entity: Linda  

Relation: person other family  
Context: In her memoir, the author reflects on her relationship with her grandmother, who played a significant role in her upbringing.  
Head Entity: the author  
Tail Entity: her grandmother  

Relation: person other family  
Context: At the wedding, Emily was thrilled to see her brother, Jake, who had flown in from another country to celebrate with her.  
Head Entity: Emily  
Tail Entity: Jake  

Relation: person other family  
Context: The documentary highlighted the bond between the musician and his father, who was also a talented artist in his own right.  
Head Entity: the musician  
Tail Entity: his father  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment in new york city after a long battle with cancer.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: the famous musician, elena rodriguez, died unexpectedly on july 12 while visiting friends in los angeles, california.  
Head Entity: elena rodriguez  
Tail Entity: los angeles  

Relation: person city of death  
Context: after a long and fulfilling life, the beloved teacher, mrs. jones, succumbed to heart failure on january 20 in her hometown of springfield, illinois.  
Head Entity: mrs. jones  
Tail Entity: springfield  

Relation: person city of death  
Context: the legendary actor, robert downey, passed away peacefully on february 14 in his luxurious home in beverly hills, california, surrounded by family.  
Head Entity: robert downey  
Tail Entity: beverly hills  

Relation: person city of death  
Context: the influential politician, senator thomas, died on april 30 in a hospital in washington, d.c., after a brief illness.  
Head Entity: senator thomas  
Tail Entity: washington, d.c.  
Losses:  13.747613906860352 0.9910106658935547 8.367914199829102
MemoryTrain:  epoch  0, batch     0 | loss: 13.7476139Losses:  17.494800567626953 0.44648218154907227 13.811590194702148
MemoryTrain:  epoch  0, batch     1 | loss: 17.4948006Losses:  19.163034439086914 0.8019624352455139 13.71278190612793
MemoryTrain:  epoch  0, batch     2 | loss: 19.1630344Losses:  11.281068801879883 1.5570008754730225 5.681314945220947
MemoryTrain:  epoch  0, batch     3 | loss: 11.2810688Losses:  20.62084197998047 1.7013707160949707 14.097517013549805
MemoryTrain:  epoch  0, batch     4 | loss: 20.6208420Losses:  15.696264266967773 0.7754484415054321 10.791703224182129
MemoryTrain:  epoch  0, batch     5 | loss: 15.6962643Losses:  21.18499183654785 1.3553804159164429 14.06550407409668
MemoryTrain:  epoch  0, batch     6 | loss: 21.1849918Losses:  14.746520042419434 0.534386396408081 10.903303146362305
MemoryTrain:  epoch  0, batch     7 | loss: 14.7465200Losses:  18.497785568237305 0.48787862062454224 13.723011016845703
MemoryTrain:  epoch  0, batch     8 | loss: 18.4977856Losses:  13.888540267944336 0.5196039080619812 8.169046401977539
MemoryTrain:  epoch  0, batch     9 | loss: 13.8885403Losses:  18.395397186279297 0.9698936939239502 13.754372596740723
MemoryTrain:  epoch  1, batch     0 | loss: 18.3953972Losses:  16.549617767333984 0.7949727773666382 11.132674217224121
MemoryTrain:  epoch  1, batch     1 | loss: 16.5496178Losses:  17.356245040893555 0.7594609260559082 13.764732360839844
MemoryTrain:  epoch  1, batch     2 | loss: 17.3562450Losses:  19.733463287353516 3.0610063076019287 11.09162425994873
MemoryTrain:  epoch  1, batch     3 | loss: 19.7334633Losses:  18.811275482177734 1.3670880794525146 13.687091827392578
MemoryTrain:  epoch  1, batch     4 | loss: 18.8112755Losses:  15.747151374816895 0.4879203736782074 10.870416641235352
MemoryTrain:  epoch  1, batch     5 | loss: 15.7471514Losses:  16.8714656829834 1.1246857643127441 11.136250495910645
MemoryTrain:  epoch  1, batch     6 | loss: 16.8714657Losses:  18.456233978271484 1.35843825340271 14.131753921508789
MemoryTrain:  epoch  1, batch     7 | loss: 18.4562340Losses:  14.371109008789062 1.7206735610961914 8.088249206542969
MemoryTrain:  epoch  1, batch     8 | loss: 14.3711090Losses:  20.255224227905273 0.2622677683830261 16.91104507446289
MemoryTrain:  epoch  1, batch     9 | loss: 20.2552242Losses:  16.08469581604004 1.4543931484222412 10.843852043151855
MemoryTrain:  epoch  2, batch     0 | loss: 16.0846958Losses:  24.36377716064453 0.7346792221069336 19.959930419921875
MemoryTrain:  epoch  2, batch     1 | loss: 24.3637772Losses:  14.878894805908203 0.8384699821472168 10.855051040649414
MemoryTrain:  epoch  2, batch     2 | loss: 14.8788948Losses:  16.12628173828125 0.7863622903823853 10.948301315307617
MemoryTrain:  epoch  2, batch     3 | loss: 16.1262817Losses:  12.945655822753906 1.2143323421478271 8.101301193237305
MemoryTrain:  epoch  2, batch     4 | loss: 12.9456558Losses:  11.957727432250977 1.0013223886489868 8.221473693847656
MemoryTrain:  epoch  2, batch     5 | loss: 11.9577274Losses:  16.126577377319336 2.22119140625 10.82491397857666
MemoryTrain:  epoch  2, batch     6 | loss: 16.1265774Losses:  25.217483520507812 0.6281650066375732 20.019811630249023
MemoryTrain:  epoch  2, batch     7 | loss: 25.2174835Losses:  19.022842407226562 1.2401666641235352 13.83946704864502
MemoryTrain:  epoch  2, batch     8 | loss: 19.0228424Losses:  13.652624130249023 1.7322982549667358 8.130759239196777
MemoryTrain:  epoch  2, batch     9 | loss: 13.6526241Losses:  18.05292320251465 0.8371902704238892 13.668794631958008
MemoryTrain:  epoch  3, batch     0 | loss: 18.0529232Losses:  20.532146453857422 0.2660023272037506 16.794918060302734
MemoryTrain:  epoch  3, batch     1 | loss: 20.5321465Losses:  13.03007698059082 2.249027967453003 5.6551642417907715
MemoryTrain:  epoch  3, batch     2 | loss: 13.0300770Losses:  20.648284912109375 1.0941100120544434 16.788040161132812
MemoryTrain:  epoch  3, batch     3 | loss: 20.6482849Losses:  11.462577819824219 0.7376652359962463 8.103456497192383
MemoryTrain:  epoch  3, batch     4 | loss: 11.4625778Losses:  12.483193397521973 1.5401368141174316 8.080546379089355
MemoryTrain:  epoch  3, batch     5 | loss: 12.4831934Losses:  19.450942993164062 1.6027541160583496 13.789356231689453
MemoryTrain:  epoch  3, batch     6 | loss: 19.4509430Losses:  21.495792388916016 1.5420928001403809 16.744428634643555
MemoryTrain:  epoch  3, batch     7 | loss: 21.4957924Losses:  23.757600784301758 1.0092703104019165 19.919477462768555
MemoryTrain:  epoch  3, batch     8 | loss: 23.7576008Losses:  9.56501293182373 0.9288350939750671 5.573565483093262
MemoryTrain:  epoch  3, batch     9 | loss: 9.5650129Losses:  18.012283325195312 1.62636137008667 13.698097229003906
MemoryTrain:  epoch  4, batch     0 | loss: 18.0122833Losses:  19.828319549560547 0.23948323726654053 16.814672470092773
MemoryTrain:  epoch  4, batch     1 | loss: 19.8283195Losses:  24.046022415161133 0.7500234842300415 19.9169979095459
MemoryTrain:  epoch  4, batch     2 | loss: 24.0460224Losses:  18.17818260192871 1.8323085308074951 13.789410591125488
MemoryTrain:  epoch  4, batch     3 | loss: 18.1781826Losses:  20.764612197875977 0.897006630897522 16.769142150878906
MemoryTrain:  epoch  4, batch     4 | loss: 20.7646122Losses:  18.4118595123291 1.4565916061401367 13.706768989562988
MemoryTrain:  epoch  4, batch     5 | loss: 18.4118595Losses:  13.706924438476562 2.162135124206543 8.100103378295898
MemoryTrain:  epoch  4, batch     6 | loss: 13.7069244Losses:  17.34211540222168 0.7324123978614807 13.789813995361328
MemoryTrain:  epoch  4, batch     7 | loss: 17.3421154Losses:  15.13939094543457 1.0605874061584473 10.90011978149414
MemoryTrain:  epoch  4, batch     8 | loss: 15.1393909Losses:  4.793763160705566 0.5408642292022705 1.395505666732788
MemoryTrain:  epoch  4, batch     9 | loss: 4.7937632Losses:  14.037787437438965 1.0092473030090332 10.846329689025879
MemoryTrain:  epoch  5, batch     0 | loss: 14.0377874Losses:  12.803315162658691 1.7869566679000854 8.06971263885498
MemoryTrain:  epoch  5, batch     1 | loss: 12.8033152Losses:  13.938838958740234 0.4912427067756653 10.859038352966309
MemoryTrain:  epoch  5, batch     2 | loss: 13.9388390Losses:  18.506179809570312 1.32509183883667 13.700529098510742
MemoryTrain:  epoch  5, batch     3 | loss: 18.5061798Losses:  17.709022521972656 0.9857744574546814 13.675992965698242
MemoryTrain:  epoch  5, batch     4 | loss: 17.7090225Losses:  20.949769973754883 1.1559154987335205 16.7249698638916
MemoryTrain:  epoch  5, batch     5 | loss: 20.9497700Losses:  18.273086547851562 1.3586037158966064 13.934921264648438
MemoryTrain:  epoch  5, batch     6 | loss: 18.2730865Losses:  15.103523254394531 2.190948963165283 10.777948379516602
MemoryTrain:  epoch  5, batch     7 | loss: 15.1035233Losses:  16.94301414489746 0.749409556388855 13.670719146728516
MemoryTrain:  epoch  5, batch     8 | loss: 16.9430141Losses:  13.623703002929688 0.47419530153274536 10.825400352478027
MemoryTrain:  epoch  5, batch     9 | loss: 13.6237030Losses:  13.880401611328125 0.5114309191703796 10.83533763885498
MemoryTrain:  epoch  6, batch     0 | loss: 13.8804016Losses:  17.971534729003906 0.7599730491638184 13.73478889465332
MemoryTrain:  epoch  6, batch     1 | loss: 17.9715347Losses:  11.654126167297363 1.0392851829528809 8.077603340148926
MemoryTrain:  epoch  6, batch     2 | loss: 11.6541262Losses:  19.695077896118164 0.5070545673370361 16.75165367126465
MemoryTrain:  epoch  6, batch     3 | loss: 19.6950779Losses:  16.808231353759766 0.7817729711532593 13.683868408203125
MemoryTrain:  epoch  6, batch     4 | loss: 16.8082314Losses:  17.045866012573242 0.9653623104095459 13.706131935119629
MemoryTrain:  epoch  6, batch     5 | loss: 17.0458660Losses:  17.228919982910156 1.0000442266464233 13.675728797912598
MemoryTrain:  epoch  6, batch     6 | loss: 17.2289200Losses:  14.910758018493652 1.7742910385131836 10.885103225708008
MemoryTrain:  epoch  6, batch     7 | loss: 14.9107580Losses:  11.512744903564453 0.7081591486930847 8.107240676879883
MemoryTrain:  epoch  6, batch     8 | loss: 11.5127449Losses:  16.587934494018555 0.563198983669281 13.72930908203125
MemoryTrain:  epoch  6, batch     9 | loss: 16.5879345Losses:  15.032814025878906 1.124215841293335 10.836432456970215
MemoryTrain:  epoch  7, batch     0 | loss: 15.0328140Losses:  16.102582931518555 0.46068325638771057 13.687564849853516
MemoryTrain:  epoch  7, batch     1 | loss: 16.1025829Losses:  23.201553344726562 1.0031437873840332 19.8916015625
MemoryTrain:  epoch  7, batch     2 | loss: 23.2015533Losses:  20.4991455078125 1.1050944328308105 16.713104248046875
MemoryTrain:  epoch  7, batch     3 | loss: 20.4991455Losses:  19.251447677612305 0.4959133565425873 16.72686195373535
MemoryTrain:  epoch  7, batch     4 | loss: 19.2514477Losses:  19.799753189086914 1.0802955627441406 16.731319427490234
MemoryTrain:  epoch  7, batch     5 | loss: 19.7997532Losses:  17.70047950744629 1.8877454996109009 13.80202579498291
MemoryTrain:  epoch  7, batch     6 | loss: 17.7004795Losses:  9.673551559448242 1.3716018199920654 5.595067977905273
MemoryTrain:  epoch  7, batch     7 | loss: 9.6735516Losses:  17.0650691986084 1.0637922286987305 13.731433868408203
MemoryTrain:  epoch  7, batch     8 | loss: 17.0650692Losses:  8.405394554138184 0.6409382224082947 5.584677219390869
MemoryTrain:  epoch  7, batch     9 | loss: 8.4053946Losses:  14.144231796264648 1.0920348167419434 10.810721397399902
MemoryTrain:  epoch  8, batch     0 | loss: 14.1442318Losses:  9.099588394165039 1.3296252489089966 5.570400238037109
MemoryTrain:  epoch  8, batch     1 | loss: 9.0995884Losses:  19.164281845092773 0.46267569065093994 16.760467529296875
MemoryTrain:  epoch  8, batch     2 | loss: 19.1642818Losses:  16.530582427978516 0.7401512265205383 13.686273574829102
MemoryTrain:  epoch  8, batch     3 | loss: 16.5305824Losses:  19.651325225830078 0.772912859916687 16.712890625
MemoryTrain:  epoch  8, batch     4 | loss: 19.6513252Losses:  14.136129379272461 1.3182309865951538 10.797074317932129
MemoryTrain:  epoch  8, batch     5 | loss: 14.1361294Losses:  14.570903778076172 1.6832587718963623 10.82194709777832
MemoryTrain:  epoch  8, batch     6 | loss: 14.5709038Losses:  14.062883377075195 1.155709981918335 10.771927833557129
MemoryTrain:  epoch  8, batch     7 | loss: 14.0628834Losses:  16.610942840576172 0.9927108287811279 13.680290222167969
MemoryTrain:  epoch  8, batch     8 | loss: 16.6109428Losses:  15.86903190612793 -0.0 13.723684310913086
MemoryTrain:  epoch  8, batch     9 | loss: 15.8690319Losses:  22.74448013305664 0.8437259197235107 19.880949020385742
MemoryTrain:  epoch  9, batch     0 | loss: 22.7444801Losses:  19.076580047607422 0.2240457832813263 16.8677978515625
MemoryTrain:  epoch  9, batch     1 | loss: 19.0765800Losses:  13.624993324279785 0.7814861536026001 10.812015533447266
MemoryTrain:  epoch  9, batch     2 | loss: 13.6249933Losses:  11.3169527053833 1.144216537475586 8.082315444946289
MemoryTrain:  epoch  9, batch     3 | loss: 11.3169527Losses:  11.429729461669922 1.3606312274932861 8.074095726013184
MemoryTrain:  epoch  9, batch     4 | loss: 11.4297295Losses:  16.495071411132812 0.7616710662841797 13.696212768554688
MemoryTrain:  epoch  9, batch     5 | loss: 16.4950714Losses:  11.86252212524414 1.627889633178711 8.094504356384277
MemoryTrain:  epoch  9, batch     6 | loss: 11.8625221Losses:  17.619901657104492 1.8377114534378052 13.683551788330078
MemoryTrain:  epoch  9, batch     7 | loss: 17.6199017Losses:  22.05414390563965 0.27572277188301086 19.84651756286621
MemoryTrain:  epoch  9, batch     8 | loss: 22.0541439Losses:  20.007890701293945 1.1689571142196655 16.730539321899414
MemoryTrain:  epoch  9, batch     9 | loss: 20.0078907
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 61.81%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 61.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 64.20%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 64.42%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 82.14%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.41%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 77.96%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 82.58%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 80.15%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 77.86%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 75.87%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 73.82%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 71.88%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 70.83%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 70.27%   [EVAL] batch:   41 | acc: 37.50%,  total acc: 69.49%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 68.90%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 68.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 72.38%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 72.06%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 71.51%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 70.87%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 70.25%   [EVAL] batch:   54 | acc: 31.25%,  total acc: 69.55%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 68.64%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 68.64%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 69.07%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 69.49%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 69.69%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 69.77%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 70.06%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 70.14%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 70.51%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 72.24%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 72.64%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 73.42%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 73.70%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 73.37%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 73.48%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 73.67%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 73.77%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 73.78%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 73.40%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 73.34%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 73.05%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 72.79%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 72.52%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 72.54%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 72.60%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 72.41%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 72.59%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 72.68%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 72.50%   
cur_acc:  ['0.8466', '0.8021', '0.7679', '0.8705', '0.6442']
his_acc:  ['0.8466', '0.8488', '0.7686', '0.7572', '0.7250']
Clustering into  14  clusters
Clusters:  [ 0 13 11  9  1  5  0 10  0  1  8  6  0  1 13  0  7  6  0 11  3  2  3  5
  4  0  0  2 12  1  1]
Losses:  18.90015411376953 8.77370834350586 3.316310405731201
CurrentTrain: epoch  0, batch     0 | loss: 18.9001541Losses:  12.339149475097656 3.3626532554626465 3.315992593765259
CurrentTrain: epoch  0, batch     1 | loss: 12.3391495Losses:  18.06203842163086 8.72551155090332 3.315725803375244
CurrentTrain: epoch  1, batch     0 | loss: 18.0620384Losses:  10.040820121765137 3.072892427444458 3.329355478286743
CurrentTrain: epoch  1, batch     1 | loss: 10.0408201Losses:  15.486143112182617 7.666955947875977 3.314394474029541
CurrentTrain: epoch  2, batch     0 | loss: 15.4861431Losses:  8.674118995666504 3.2133891582489014 1.398090124130249
CurrentTrain: epoch  2, batch     1 | loss: 8.6741190Losses:  15.391416549682617 9.089946746826172 3.3169302940368652
CurrentTrain: epoch  3, batch     0 | loss: 15.3914165Losses:  12.883113861083984 6.793030738830566 1.3894898891448975
CurrentTrain: epoch  3, batch     1 | loss: 12.8831139Losses:  15.502111434936523 8.199430465698242 3.3261640071868896
CurrentTrain: epoch  4, batch     0 | loss: 15.5021114Losses:  8.656232833862305 2.7256853580474854 3.3268909454345703
CurrentTrain: epoch  4, batch     1 | loss: 8.6562328Losses:  14.076242446899414 7.405407905578613 3.3201451301574707
CurrentTrain: epoch  5, batch     0 | loss: 14.0762424Losses:  8.4373140335083 2.741642951965332 3.3199455738067627
CurrentTrain: epoch  5, batch     1 | loss: 8.4373140Losses:  13.24993896484375 7.063957214355469 3.3037986755371094
CurrentTrain: epoch  6, batch     0 | loss: 13.2499390Losses:  7.562303066253662 2.086181879043579 3.3135790824890137
CurrentTrain: epoch  6, batch     1 | loss: 7.5623031Losses:  13.30971908569336 7.548830986022949 3.3126864433288574
CurrentTrain: epoch  7, batch     0 | loss: 13.3097191Losses:  8.950807571411133 4.616187572479248 1.3890012502670288
CurrentTrain: epoch  7, batch     1 | loss: 8.9508076Losses:  11.470449447631836 5.848516464233398 3.3012256622314453
CurrentTrain: epoch  8, batch     0 | loss: 11.4704494Losses:  7.521323204040527 1.82952082157135 3.301095962524414
CurrentTrain: epoch  8, batch     1 | loss: 7.5213232Losses:  10.829864501953125 5.321275234222412 3.2994801998138428
CurrentTrain: epoch  9, batch     0 | loss: 10.8298645Losses:  6.855301856994629 1.2499881982803345 3.3013882637023926
CurrentTrain: epoch  9, batch     1 | loss: 6.8553019
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: ny-schools-chief -lrb- new york -rrb- -- cathleen p. black , mayor michael r. bloomberg 's choice to be the next chancellor of new york city 's public-school system , has during more than 40 years in the media business broken numerous glass ceilings -- and amassed a fortune -- with quick and cold-blooded decision making , crystal-clear goal setting , and an all-surpassing attention to the bottom line .
Head Entity: cathleen p. black
Tail Entity: new york
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After years of living in the bustling city, actor Tom Hanks has decided to settle down in a quieter place, choosing to make his home in the picturesque state of California.  
Head Entity: Tom Hanks  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: The renowned author J.K. Rowling has spent much of her life in Edinburgh, where she found inspiration for her beloved Harry Potter series.  
Head Entity: J.K. Rowling  
Tail Entity: Edinburgh  

Relation: person stateorprovinces of residence  
Context: Following his successful career in the tech industry, entrepreneur Elon Musk has moved to Texas, where he plans to expand his business ventures.  
Head Entity: Elon Musk  
Tail Entity: Texas  

Relation: person stateorprovinces of residence  
Context: After retiring from professional basketball, Michael Jordan chose to reside in North Carolina, where he continues to be involved in the local community.  
Head Entity: Michael Jordan  
Tail Entity: North Carolina  

Relation: person stateorprovinces of residence  
Context: The famous singer Taylor Swift has made her home in Nashville, Tennessee, where she began her music career and still finds inspiration.  
Head Entity: Taylor Swift  
Tail Entity: Nashville
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: today the nypd upgraded the charges to include murder , in the case of brooklyn gay-bashing/robbery victim michael sandy , who died on friday after being taken off life-support .
Head Entity: michael sandy
Tail Entity: friday
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned physicist Stephen Hawking passed away peacefully at his home in Cambridge on March 14, 2018, at the age of 76.  
Head Entity: Stephen Hawking  
Tail Entity: March 14, 2018  

Relation: person date of death  
Context: The beloved actor Robin Williams was found dead in his home in Paradise Cay, California, on August 11, 2014, which shocked fans around the world.  
Head Entity: Robin Williams  
Tail Entity: August 11, 2014  

Relation: person date of death  
Context: The famous singer Whitney Houston was found dead in a bathtub at the Beverly Hilton Hotel on February 11, 2012, just before the Grammy Awards.  
Head Entity: Whitney Houston  
Tail Entity: February 11, 2012  

Relation: person date of death  
Context: The legendary civil rights leader Martin Luther King Jr. was assassinated on April 4, 1968, in Memphis, Tennessee, leaving a lasting legacy.  
Head Entity: Martin Luther King Jr.  
Tail Entity: April 4, 1968  

Relation: person date of death  
Context: The iconic fashion designer Karl Lagerfeld passed away on February 19, 2019, in Paris, France, after a brief illness, marking the end of an era in fashion.  
Head Entity: Karl Lagerfeld  
Tail Entity: February 19, 2019  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: with the sweep of a federal regulator 's pen , massachusetts stands to gain a new life-science giant in april : covidien , a medical - supplies maker with thousands of products and more than 43,000 employees worldwide .
Head Entity: covidien
Tail Entity: 43,000
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: The tech company Apple has been expanding its workforce significantly, currently employing over 147,000 people across the globe.  
Head Entity: Apple  
Tail Entity: 147,000  

Relation: organization number of employees members  
Context: In 2022, the multinational corporation Amazon reported a staggering number of employees, reaching approximately 1.5 million worldwide.  
Head Entity: Amazon  
Tail Entity: 1.5 million  

Relation: organization number of employees members  
Context: Google, known for its innovative technology, has a workforce that exceeds 156,000 employees as of the latest reports.  
Head Entity: Google  
Tail Entity: 156,000  

Relation: organization number of employees members  
Context: The automotive giant Toyota has a global workforce of around 360,000 employees, making it one of the largest employers in the industry.  
Head Entity: Toyota  
Tail Entity: 360,000  

Relation: organization number of employees members  
Context: With a commitment to sustainability, the energy company Shell employs approximately 86,000 people in various sectors worldwide.  
Head Entity: Shell  
Tail Entity: 86,000  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Langhorne Clemens, better known by his pen name Mark Twain, is celebrated for his novels like "The Adventures of Tom Sawyer."  
Head Entity: Samuel Langhorne Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician Stefani Joanne Angelina Germanotta, who is widely recognized as Lady Gaga, has made a significant impact on the pop music scene.  
Head Entity: Stefani Joanne Angelina Germanotta  
Tail Entity: Lady Gaga  

Relation: person alternate names  
Context: The actor and filmmaker known as Orson Welles was born George Orson Welles and is famous for his groundbreaking work in cinema.  
Head Entity: George Orson Welles  
Tail Entity: Orson Welles  

Relation: person alternate names  
Context: The renowned physicist Albert Einstein, who is often referred to simply as Einstein, developed the theory of relativity.  
Head Entity: Albert Einstein  
Tail Entity: Einstein  

Relation: person alternate names  
Context: The pop star Robyn Rihanna Fenty, better known as Rihanna, has won numerous awards for her music and philanthropy.  
Head Entity: Robyn Rihanna Fenty  
Tail Entity: Rihanna  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: beverly hills , california 2008-08-17 21:15:39 utc ------ there was much dancing : ellen degeneres and portia de rossi are married , according to reports .
Head Entity: ellen degeneres
Tail Entity: portia de rossi
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: in a beautiful ceremony held in new york city, 2015-06-20 15:30:00 utc ------ the couple exchanged vows: john legend and chrissy teigen are now officially husband and wife.  
Head Entity: john legend  
Tail Entity: chrissy teigen  

Relation: person spouse  
Context: during the star-studded event in los angeles, 2019-01-15 18:45:00 utc ------ the couple celebrated their anniversary: blake lively and ryan reynolds have been married for several years.  
Head Entity: blake lively  
Tail Entity: ryan reynolds  

Relation: person spouse  
Context: at a private beach ceremony in hawaii, 2021-07-10 12:00:00 utc ------ they tied the knot: meghan markle and prince harry are now husband and wife.  
Head Entity: meghan markle  
Tail Entity: prince harry  

Relation: person spouse  
Context: in a lavish wedding in italy, 2018-09-22 14:00:00 utc ------ the couple said "I do": justin timberlake and jessica biel have officially become spouses.  
Head Entity: justin timberlake  
Tail Entity: jessica biel  

Relation: person spouse  
Context: at a romantic ceremony in the countryside, 2020-05-30 16:00:00 utc ------ they exchanged rings: emma stone and dave mcary are now married.  
Head Entity: emma stone  
Tail Entity: dave mcary  
Losses:  17.604629516601562 0.6964530944824219 13.70157241821289
MemoryTrain:  epoch  0, batch     0 | loss: 17.6046295Losses:  20.91193199157715 0.7270291447639465 17.0035343170166
MemoryTrain:  epoch  0, batch     1 | loss: 20.9119320Losses:  23.07436752319336 0.5072264671325684 19.976192474365234
MemoryTrain:  epoch  0, batch     2 | loss: 23.0743675Losses:  23.149063110351562 0.2407485544681549 19.86832618713379
MemoryTrain:  epoch  0, batch     3 | loss: 23.1490631Losses:  18.207975387573242 1.0703222751617432 13.683794021606445
MemoryTrain:  epoch  0, batch     4 | loss: 18.2079754Losses:  20.253589630126953 0.7794040441513062 16.820228576660156
MemoryTrain:  epoch  0, batch     5 | loss: 20.2535896Losses:  16.940471649169922 1.7459547519683838 10.856757164001465
MemoryTrain:  epoch  0, batch     6 | loss: 16.9404716Losses:  22.08523941040039 1.0968108177185059 17.04723358154297
MemoryTrain:  epoch  0, batch     7 | loss: 22.0852394Losses:  23.910024642944336 0.2321355640888214 19.9925594329834
MemoryTrain:  epoch  0, batch     8 | loss: 23.9100246Losses:  16.067180633544922 1.0728542804718018 10.94616985321045
MemoryTrain:  epoch  0, batch     9 | loss: 16.0671806Losses:  14.946171760559082 0.9435266256332397 10.93326187133789
MemoryTrain:  epoch  0, batch    10 | loss: 14.9461718Losses:  18.309017181396484 0.44054895639419556 13.697755813598633
MemoryTrain:  epoch  0, batch    11 | loss: 18.3090172Losses:  19.867006301879883 1.8731818199157715 13.785253524780273
MemoryTrain:  epoch  1, batch     0 | loss: 19.8670063Losses:  12.967085838317871 1.0289748907089233 8.069657325744629
MemoryTrain:  epoch  1, batch     1 | loss: 12.9670858Losses:  15.273744583129883 1.7287880182266235 10.797722816467285
MemoryTrain:  epoch  1, batch     2 | loss: 15.2737446Losses:  20.939403533935547 1.9120707511901855 16.79244613647461
MemoryTrain:  epoch  1, batch     3 | loss: 20.9394035Losses:  24.03825569152832 1.0856634378433228 19.882104873657227
MemoryTrain:  epoch  1, batch     4 | loss: 24.0382557Losses:  19.638019561767578 0.27208009362220764 16.726388931274414
MemoryTrain:  epoch  1, batch     5 | loss: 19.6380196Losses:  10.649643898010254 2.6548032760620117 5.573602676391602
MemoryTrain:  epoch  1, batch     6 | loss: 10.6496439Losses:  20.34868812561035 0.2361849844455719 16.902490615844727
MemoryTrain:  epoch  1, batch     7 | loss: 20.3486881Losses:  15.109249114990234 1.9364572763442993 10.855854988098145
MemoryTrain:  epoch  1, batch     8 | loss: 15.1092491Losses:  19.967384338378906 0.7759199142456055 16.831262588500977
MemoryTrain:  epoch  1, batch     9 | loss: 19.9673843Losses:  22.420978546142578 0.2906760275363922 19.914743423461914
MemoryTrain:  epoch  1, batch    10 | loss: 22.4209785Losses:  11.492894172668457 0.2864047586917877 8.134614944458008
MemoryTrain:  epoch  1, batch    11 | loss: 11.4928942Losses:  26.738685607910156 1.4302974939346313 23.14435577392578
MemoryTrain:  epoch  2, batch     0 | loss: 26.7386856Losses:  18.01366424560547 2.0556178092956543 13.681303024291992
MemoryTrain:  epoch  2, batch     1 | loss: 18.0136642Losses:  19.589336395263672 0.5101717710494995 16.756723403930664
MemoryTrain:  epoch  2, batch     2 | loss: 19.5893364Losses:  14.371244430541992 1.0821340084075928 10.803278923034668
MemoryTrain:  epoch  2, batch     3 | loss: 14.3712444Losses:  23.008045196533203 1.0627083778381348 19.846179962158203
MemoryTrain:  epoch  2, batch     4 | loss: 23.0080452Losses:  17.52130699157715 1.4334207773208618 13.711076736450195
MemoryTrain:  epoch  2, batch     5 | loss: 17.5213070Losses:  15.444412231445312 1.5630536079406738 10.834637641906738
MemoryTrain:  epoch  2, batch     6 | loss: 15.4444122Losses:  16.998117446899414 0.7628616690635681 13.762619018554688
MemoryTrain:  epoch  2, batch     7 | loss: 16.9981174Losses:  20.227333068847656 0.8144181966781616 16.784807205200195
MemoryTrain:  epoch  2, batch     8 | loss: 20.2273331Losses:  20.77585220336914 1.125045657157898 16.737308502197266
MemoryTrain:  epoch  2, batch     9 | loss: 20.7758522Losses:  19.647296905517578 0.7628822326660156 16.71001625061035
MemoryTrain:  epoch  2, batch    10 | loss: 19.6472969Losses:  10.752487182617188 0.29815614223480225 8.131522178649902
MemoryTrain:  epoch  2, batch    11 | loss: 10.7524872Losses:  22.86506462097168 0.529151976108551 19.881011962890625
MemoryTrain:  epoch  3, batch     0 | loss: 22.8650646Losses:  17.436437606811523 1.3109921216964722 13.725276947021484
MemoryTrain:  epoch  3, batch     1 | loss: 17.4364376Losses:  25.90137481689453 0.7508902549743652 23.116241455078125
MemoryTrain:  epoch  3, batch     2 | loss: 25.9013748Losses:  19.206546783447266 0.47967737913131714 16.69842529296875
MemoryTrain:  epoch  3, batch     3 | loss: 19.2065468Losses:  17.045839309692383 1.1297214031219482 13.685580253601074
MemoryTrain:  epoch  3, batch     4 | loss: 17.0458393Losses:  17.712779998779297 1.5390981435775757 13.708605766296387
MemoryTrain:  epoch  3, batch     5 | loss: 17.7127800Losses:  19.018587112426758 0.2462216317653656 16.75423812866211
MemoryTrain:  epoch  3, batch     6 | loss: 19.0185871Losses:  11.647388458251953 0.7866705656051636 8.167853355407715
MemoryTrain:  epoch  3, batch     7 | loss: 11.6473885Losses:  20.75248908996582 1.736479640007019 16.71908187866211
MemoryTrain:  epoch  3, batch     8 | loss: 20.7524891Losses:  12.082317352294922 1.6595898866653442 8.170166015625
MemoryTrain:  epoch  3, batch     9 | loss: 12.0823174Losses:  10.732190132141113 0.48547273874282837 8.113787651062012
MemoryTrain:  epoch  3, batch    10 | loss: 10.7321901Losses:  8.585768699645996 1.0467575788497925 5.575987815856934
MemoryTrain:  epoch  3, batch    11 | loss: 8.5857687Losses:  13.634272575378418 0.5490654110908508 10.8068265914917
MemoryTrain:  epoch  4, batch     0 | loss: 13.6342726Losses:  19.73740005493164 1.0440325736999512 16.724781036376953
MemoryTrain:  epoch  4, batch     1 | loss: 19.7374001Losses:  14.175802230834961 1.1261874437332153 10.837202072143555
MemoryTrain:  epoch  4, batch     2 | loss: 14.1758022Losses:  22.28244400024414 0.4778107702732086 19.873075485229492
MemoryTrain:  epoch  4, batch     3 | loss: 22.2824440Losses:  16.25706672668457 0.26011577248573303 13.724117279052734
MemoryTrain:  epoch  4, batch     4 | loss: 16.2570667Losses:  17.241897583007812 0.8570113778114319 13.704842567443848
MemoryTrain:  epoch  4, batch     5 | loss: 17.2418976Losses:  17.935136795043945 2.234689235687256 13.733285903930664
MemoryTrain:  epoch  4, batch     6 | loss: 17.9351368Losses:  19.82444190979004 0.7569022178649902 16.737234115600586
MemoryTrain:  epoch  4, batch     7 | loss: 19.8244419Losses:  24.067821502685547 1.2631367444992065 19.88606834411621
MemoryTrain:  epoch  4, batch     8 | loss: 24.0678215Losses:  19.393774032592773 0.7131125926971436 16.73041534423828
MemoryTrain:  epoch  4, batch     9 | loss: 19.3937740Losses:  19.25627899169922 0.504062831401825 16.776838302612305
MemoryTrain:  epoch  4, batch    10 | loss: 19.2562790Losses:  12.750391006469727 -0.0 10.80745792388916
MemoryTrain:  epoch  4, batch    11 | loss: 12.7503910Losses:  14.990896224975586 1.6268484592437744 10.841001510620117
MemoryTrain:  epoch  5, batch     0 | loss: 14.9908962Losses:  11.109712600708008 1.0475913286209106 8.133966445922852
MemoryTrain:  epoch  5, batch     1 | loss: 11.1097126Losses:  11.428796768188477 1.063406229019165 8.065441131591797
MemoryTrain:  epoch  5, batch     2 | loss: 11.4287968Losses:  20.21464729309082 1.1078851222991943 16.71031379699707
MemoryTrain:  epoch  5, batch     3 | loss: 20.2146473Losses:  21.752216339111328 2.9360287189483643 16.70865821838379
MemoryTrain:  epoch  5, batch     4 | loss: 21.7522163Losses:  15.93513011932373 0.242900550365448 13.685317993164062
MemoryTrain:  epoch  5, batch     5 | loss: 15.9351301Losses:  16.46860122680664 0.7652050852775574 13.690074920654297
MemoryTrain:  epoch  5, batch     6 | loss: 16.4686012Losses:  23.06944465637207 0.7731103897094727 19.881759643554688
MemoryTrain:  epoch  5, batch     7 | loss: 23.0694447Losses:  19.87835121154785 1.3041801452636719 16.691986083984375
MemoryTrain:  epoch  5, batch     8 | loss: 19.8783512Losses:  22.00226593017578 0.2430637776851654 19.814245223999023
MemoryTrain:  epoch  5, batch     9 | loss: 22.0022659Losses:  13.814031600952148 1.0969793796539307 10.793601036071777
MemoryTrain:  epoch  5, batch    10 | loss: 13.8140316Losses:  7.740603923797607 0.2929403483867645 5.571125507354736
MemoryTrain:  epoch  5, batch    11 | loss: 7.7406039Losses:  10.992321968078613 0.7444790601730347 8.096163749694824
MemoryTrain:  epoch  6, batch     0 | loss: 10.9923220Losses:  16.366687774658203 0.7330822944641113 13.66537094116211
MemoryTrain:  epoch  6, batch     1 | loss: 16.3666878Losses:  17.109636306762695 1.3461393117904663 13.70355224609375
MemoryTrain:  epoch  6, batch     2 | loss: 17.1096363Losses:  19.944639205932617 1.3417651653289795 16.67916488647461
MemoryTrain:  epoch  6, batch     3 | loss: 19.9446392Losses:  22.555761337280273 0.7188435196876526 19.880773544311523
MemoryTrain:  epoch  6, batch     4 | loss: 22.5557613Losses:  22.348716735839844 0.5162559151649475 19.858612060546875
MemoryTrain:  epoch  6, batch     5 | loss: 22.3487167Losses:  13.779129028320312 1.0023844242095947 10.82198429107666
MemoryTrain:  epoch  6, batch     6 | loss: 13.7791290Losses:  13.942876815795898 0.7171242237091064 10.782185554504395
MemoryTrain:  epoch  6, batch     7 | loss: 13.9428768Losses:  16.036762237548828 3.0683605670928955 10.796652793884277
MemoryTrain:  epoch  6, batch     8 | loss: 16.0367622Losses:  19.29974365234375 0.5041787624359131 16.709726333618164
MemoryTrain:  epoch  6, batch     9 | loss: 19.2997437Losses:  16.391368865966797 0.7510929107666016 13.702713012695312
MemoryTrain:  epoch  6, batch    10 | loss: 16.3913689Losses:  16.394777297973633 0.5222672820091248 13.676959037780762
MemoryTrain:  epoch  6, batch    11 | loss: 16.3947773Losses:  13.564411163330078 0.8043383359909058 10.807003021240234
MemoryTrain:  epoch  7, batch     0 | loss: 13.5644112Losses:  20.203655242919922 1.547882318496704 16.723066329956055
MemoryTrain:  epoch  7, batch     1 | loss: 20.2036552Losses:  20.09920883178711 1.324967384338379 16.69711685180664
MemoryTrain:  epoch  7, batch     2 | loss: 20.0992088Losses:  11.173431396484375 0.9757566452026367 8.090911865234375
MemoryTrain:  epoch  7, batch     3 | loss: 11.1734314Losses:  16.25283432006836 0.7368203997612 13.646612167358398
MemoryTrain:  epoch  7, batch     4 | loss: 16.2528343Losses:  19.168109893798828 0.4809805750846863 16.6884765625
MemoryTrain:  epoch  7, batch     5 | loss: 19.1681099Losses:  13.638225555419922 0.8198424577713013 10.782266616821289
MemoryTrain:  epoch  7, batch     6 | loss: 13.6382256Losses:  25.721111297607422 0.7517108917236328 23.08212661743164
MemoryTrain:  epoch  7, batch     7 | loss: 25.7211113Losses:  19.024551391601562 0.47321105003356934 16.67050552368164
MemoryTrain:  epoch  7, batch     8 | loss: 19.0245514Losses:  13.683816909790039 0.7983982563018799 10.813398361206055
MemoryTrain:  epoch  7, batch     9 | loss: 13.6838169Losses:  13.822731971740723 1.0509412288665771 10.78090763092041
MemoryTrain:  epoch  7, batch    10 | loss: 13.8227320Losses:  10.35771656036377 -0.0 8.173641204833984
MemoryTrain:  epoch  7, batch    11 | loss: 10.3577166Losses:  19.62997817993164 0.9980372190475464 16.69639015197754
MemoryTrain:  epoch  8, batch     0 | loss: 19.6299782Losses:  16.10296630859375 0.49625712633132935 13.695306777954102
MemoryTrain:  epoch  8, batch     1 | loss: 16.1029663Losses:  16.193063735961914 0.4877985715866089 13.68032455444336
MemoryTrain:  epoch  8, batch     2 | loss: 16.1930637Losses:  11.04301643371582 1.068037986755371 8.067061424255371
MemoryTrain:  epoch  8, batch     3 | loss: 11.0430164Losses:  13.7669677734375 0.9567650556564331 10.796147346496582
MemoryTrain:  epoch  8, batch     4 | loss: 13.7669678Losses:  19.312110900878906 0.4796793460845947 16.685882568359375
MemoryTrain:  epoch  8, batch     5 | loss: 19.3121109Losses:  25.537412643432617 0.5027480721473694 23.112092971801758
MemoryTrain:  epoch  8, batch     6 | loss: 25.5374126Losses:  14.125082969665527 1.3774712085723877 10.784937858581543
MemoryTrain:  epoch  8, batch     7 | loss: 14.1250830Losses:  17.07070541381836 1.3704450130462646 13.678400993347168
MemoryTrain:  epoch  8, batch     8 | loss: 17.0707054Losses:  22.509498596191406 0.7782710790634155 19.856801986694336
MemoryTrain:  epoch  8, batch     9 | loss: 22.5094986Losses:  22.372020721435547 0.5329653024673462 19.84219741821289
MemoryTrain:  epoch  8, batch    10 | loss: 22.3720207Losses:  15.622215270996094 -0.0 13.67293930053711
MemoryTrain:  epoch  8, batch    11 | loss: 15.6222153Losses:  19.11493492126465 0.4951207637786865 16.69423484802246
MemoryTrain:  epoch  9, batch     0 | loss: 19.1149349Losses:  16.462120056152344 0.8213275074958801 13.646506309509277
MemoryTrain:  epoch  9, batch     1 | loss: 16.4621201Losses:  22.332138061523438 0.5130263566970825 19.82638931274414
MemoryTrain:  epoch  9, batch     2 | loss: 22.3321381Losses:  16.501888275146484 0.9496005773544312 13.652434349060059
MemoryTrain:  epoch  9, batch     3 | loss: 16.5018883Losses:  19.054729461669922 0.47198742628097534 16.661237716674805
MemoryTrain:  epoch  9, batch     4 | loss: 19.0547295Losses:  16.0296630859375 0.4548838436603546 13.677396774291992
MemoryTrain:  epoch  9, batch     5 | loss: 16.0296631Losses:  16.166706085205078 0.5094853639602661 13.688098907470703
MemoryTrain:  epoch  9, batch     6 | loss: 16.1667061Losses:  11.815881729125977 1.6865925788879395 8.098823547363281
MemoryTrain:  epoch  9, batch     7 | loss: 11.8158817Losses:  19.476749420166016 0.787782609462738 16.730037689208984
MemoryTrain:  epoch  9, batch     8 | loss: 19.4767494Losses:  13.207486152648926 0.4670006036758423 10.789247512817383
MemoryTrain:  epoch  9, batch     9 | loss: 13.2074862Losses:  20.26778793334961 1.5805325508117676 16.70288848876953
MemoryTrain:  epoch  9, batch    10 | loss: 20.2677879Losses:  10.73267936706543 0.5881592631340027 8.073172569274902
MemoryTrain:  epoch  9, batch    11 | loss: 10.7326794
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 85.62%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 0.00%,  total acc: 73.44%   [EVAL] batch:   12 | acc: 0.00%,  total acc: 67.79%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 62.95%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 58.75%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 80.80%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 78.52%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 76.64%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 76.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 79.35%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 79.95%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 80.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 81.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.14%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.76%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 82.29%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 82.26%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 82.62%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 81.25%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 78.86%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 76.61%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 74.48%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 72.47%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 70.56%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 69.55%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 70.00%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 69.05%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 68.90%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 68.89%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 69.58%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 70.24%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 70.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 72.07%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 72.38%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 71.81%   [EVAL] batch:   51 | acc: 37.50%,  total acc: 71.15%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 70.40%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 69.68%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 68.64%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 67.75%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 67.54%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 67.89%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 68.65%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 68.65%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 68.95%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 68.85%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 69.14%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 70.52%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 70.96%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 72.40%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 72.00%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 72.04%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 72.25%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 72.37%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 72.40%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 72.04%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 71.84%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 71.72%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 71.60%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 71.99%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 72.25%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 72.21%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 72.31%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 72.13%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 72.16%   [EVAL] batch:   88 | acc: 75.00%,  total acc: 72.19%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 72.29%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 72.49%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 72.45%   [EVAL] batch:   93 | acc: 75.00%,  total acc: 72.47%   [EVAL] batch:   94 | acc: 93.75%,  total acc: 72.70%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 73.56%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 72.83%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 72.12%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 71.42%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 70.73%   [EVAL] batch:  104 | acc: 0.00%,  total acc: 70.06%   
cur_acc:  ['0.8466', '0.8021', '0.7679', '0.8705', '0.6442', '0.5875']
his_acc:  ['0.8466', '0.8488', '0.7686', '0.7572', '0.7250', '0.7006']
Clustering into  17  clusters
Clusters:  [ 1  0 16 11  2  6  1 14  7  2 12  3  1  2  0  7  4  3  1 16  9  5  9  6
 13  7  1  5 10  2  2  1 15  8  4  0]
Losses:  18.808197021484375 8.88773250579834 5.63770055770874
CurrentTrain: epoch  0, batch     0 | loss: 18.8081970Losses:  15.757331848144531 6.254286766052246 1.5363669395446777
CurrentTrain: epoch  0, batch     1 | loss: 15.7573318Losses:  18.349252700805664 7.085452079772949 5.793191432952881
CurrentTrain: epoch  1, batch     0 | loss: 18.3492527Losses:  10.990760803222656 1.3834184408187866 5.6178436279296875
CurrentTrain: epoch  1, batch     1 | loss: 10.9907608Losses:  16.583105087280273 7.206168174743652 5.769410133361816
CurrentTrain: epoch  2, batch     0 | loss: 16.5831051Losses:  16.429372787475586 4.003604412078857 5.7831268310546875
CurrentTrain: epoch  2, batch     1 | loss: 16.4293728Losses:  17.01980972290039 7.363325119018555 5.69154691696167
CurrentTrain: epoch  3, batch     0 | loss: 17.0198097Losses:  14.014888763427734 3.509058713912964 5.684613227844238
CurrentTrain: epoch  3, batch     1 | loss: 14.0148888Losses:  18.68562126159668 8.505707740783691 5.789674282073975
CurrentTrain: epoch  4, batch     0 | loss: 18.6856213Losses:  10.606849670410156 3.6993021965026855 3.3213295936584473
CurrentTrain: epoch  4, batch     1 | loss: 10.6068497Losses:  16.493907928466797 7.084235668182373 5.641049385070801
CurrentTrain: epoch  5, batch     0 | loss: 16.4939079Losses:  10.320671081542969 3.3428215980529785 3.3431148529052734
CurrentTrain: epoch  5, batch     1 | loss: 10.3206711Losses:  14.864531517028809 6.579045295715332 5.6462907791137695
CurrentTrain: epoch  6, batch     0 | loss: 14.8645315Losses:  12.492792129516602 3.0319931507110596 5.6641154289245605
CurrentTrain: epoch  6, batch     1 | loss: 12.4927921Losses:  15.97976016998291 7.84539794921875 5.605865478515625
CurrentTrain: epoch  7, batch     0 | loss: 15.9797602Losses:  11.242013931274414 4.472115993499756 3.338606834411621
CurrentTrain: epoch  7, batch     1 | loss: 11.2420139Losses:  14.67447280883789 6.187025547027588 5.5883917808532715
CurrentTrain: epoch  8, batch     0 | loss: 14.6744728Losses:  9.558646202087402 1.6517906188964844 5.577518939971924
CurrentTrain: epoch  8, batch     1 | loss: 9.5586462Losses:  14.716379165649414 6.460577487945557 5.5818400382995605
CurrentTrain: epoch  9, batch     0 | loss: 14.7163792Losses:  8.110090255737305 2.1837844848632812 3.318068027496338
CurrentTrain: epoch  9, batch     1 | loss: 8.1100903
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she revealed that she was born in the picturesque town of florence, which greatly influenced her artistic career.  
Head Entity: she  
Tail Entity: italy  

Relation: person country of birth  
Context: the famous actor, who has starred in numerous blockbuster films, was actually born in the vibrant city of los angeles, california.  
Head Entity: the famous actor  
Tail Entity: united states  

Relation: person country of birth  
Context: as a child, he often spoke about his early years in the serene landscapes of auckland, where he was born and raised.  
Head Entity: he  
Tail Entity: new zealand  

Relation: person country of birth  
Context: the renowned scientist, known for her groundbreaking research, was born in the historic city of cairo, egypt, before moving to europe.  
Head Entity: the renowned scientist  
Tail Entity: egypt  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit our official site at https://www.techinnovators.com for the latest updates.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For more information, check out the website of Green Earth Solutions at http://www.greenearthsolutions.org.  
Head Entity: Green Earth Solutions  
Tail Entity: http://www.greenearthsolutions.org  

Relation: organization website  
Context: The blog post can be found on the website of Creative Minds at https://www.creativemindsblog.com.  
Head Entity: Creative Minds  
Tail Entity: https://www.creativemindsblog.com  

Relation: organization website  
Context: You can learn more about our services by visiting http://www.healthcareexperts.com.  
Head Entity: Healthcare Experts  
Tail Entity: http://www.healthcareexperts.com  

Relation: organization website  
Context: Explore the latest projects on the official site of Urban Developers at https://www.urbanddevelopers.com.  
Head Entity: Urban Developers  
Tail Entity: https://www.urbanddevelopers.com  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: ------ liberty media acquired a 41 percent stake in directv in late february by exchanging it for a 16 percent stake in news corp plus $ 625 million -lrb- euro402 5 million -rrb- in cash .
Head Entity: directv
Tail Entity: liberty media
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: In 2020, Amazon acquired a significant share in Zoox, a self-driving car startup, to enhance its logistics capabilities.  
Head Entity: Zoox  
Tail Entity: Amazon  

Relation: organization shareholders  
Context: Tesla announced that it had sold a portion of its shares to a group of investors, including several prominent venture capital firms.  
Head Entity: Tesla  
Tail Entity: venture capital firms  

Relation: organization shareholders  
Context: Facebook's recent investment in Jio Platforms has made it one of the largest shareholders in the Indian telecommunications company.  
Head Entity: Jio Platforms  
Tail Entity: Facebook  

Relation: organization shareholders  
Context: In a strategic move, Microsoft purchased a 10% stake in OpenAI to bolster its artificial intelligence initiatives.  
Head Entity: OpenAI  
Tail Entity: Microsoft  

Relation: organization shareholders  
Context: The merger between Disney and 21st Century Fox resulted in Disney becoming the majority shareholder of the entertainment giant.  
Head Entity: 21st Century Fox  
Tail Entity: Disney  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: After years of financial difficulties, the local arts council announced its dissolution in January 2019, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: January 2019  

Relation: organization dissolved  
Context: The historic bookstore, Pages & Co., closed its doors for good in July 2021, marking the end of an era for the community.  
Head Entity: Pages & Co.  
Tail Entity: July 2021  

Relation: organization dissolved  
Context: Following a series of scandals, the charity organization, Helping Hands, was officially dissolved in February 2022.  
Head Entity: Helping Hands  
Tail Entity: February 2022  

Relation: organization dissolved  
Context: The environmental group, Green Future, announced its dissolution in October 2023 due to a lack of funding and support.  
Head Entity: Green Future  
Tail Entity: October 2023  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. John Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. John Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the famous actress and philanthropist, Emily Johnson, to support underprivileged children.  
Head Entity: Hope for Tomorrow  
Tail Entity: Emily Johnson  

Relation: organization founded by  
Context: In the early 2000s, the tech startup, GreenTech Solutions, was founded by environmentalist and engineer, Mark Thompson, to develop sustainable energy solutions.  
Head Entity: GreenTech Solutions  
Tail Entity: Mark Thompson  

Relation: organization founded by  
Context: The global non-profit organization, Clean Oceans Initiative, was established in 2015 by marine biologist, Dr. Sarah Lee, to combat ocean pollution.  
Head Entity: Clean Oceans Initiative  
Tail Entity: Dr. Sarah Lee  

Relation: organization founded by  
Context: The innovative design firm, Creative Minds Studio, was launched in 2018 by renowned architect, Lisa Chen, to push the boundaries of modern architecture.  
Head Entity: Creative Minds Studio  
Tail Entity: Lisa Chen  
Losses:  14.691222190856934 0.741624653339386 10.823210716247559
MemoryTrain:  epoch  0, batch     0 | loss: 14.6912222Losses:  24.786149978637695 1.0819203853607178 19.846805572509766
MemoryTrain:  epoch  0, batch     1 | loss: 24.7861500Losses:  20.00162696838379 0.49863240122795105 16.6885986328125
MemoryTrain:  epoch  0, batch     2 | loss: 20.0016270Losses:  28.512800216674805 1.0983840227127075 23.428630828857422
MemoryTrain:  epoch  0, batch     3 | loss: 28.5128002Losses:  30.535959243774414 0.28918954730033875 26.544261932373047
MemoryTrain:  epoch  0, batch     4 | loss: 30.5359592Losses:  22.463613510131836 1.71942138671875 16.852643966674805
MemoryTrain:  epoch  0, batch     5 | loss: 22.4636135Losses:  21.89034652709961 1.064046859741211 16.728477478027344
MemoryTrain:  epoch  0, batch     6 | loss: 21.8903465Losses:  26.487205505371094 0.4890623688697815 23.16768455505371
MemoryTrain:  epoch  0, batch     7 | loss: 26.4872055Losses:  18.432077407836914 1.4088882207870483 13.73883056640625
MemoryTrain:  epoch  0, batch     8 | loss: 18.4320774Losses:  20.044509887695312 0.5027860403060913 16.704309463500977
MemoryTrain:  epoch  0, batch     9 | loss: 20.0445099Losses:  21.452594757080078 1.412135362625122 16.751615524291992
MemoryTrain:  epoch  0, batch    10 | loss: 21.4525948Losses:  20.481319427490234 0.5024223327636719 16.72247886657715
MemoryTrain:  epoch  0, batch    11 | loss: 20.4813194Losses:  24.129560470581055 0.7238367795944214 19.880735397338867
MemoryTrain:  epoch  0, batch    12 | loss: 24.1295605Losses:  10.485269546508789 0.5646736025810242 5.635970115661621
MemoryTrain:  epoch  0, batch    13 | loss: 10.4852695Losses:  26.691974639892578 0.2458903193473816 23.150300979614258
MemoryTrain:  epoch  1, batch     0 | loss: 26.6919746Losses:  30.304012298583984 0.5111579895019531 26.500656127929688
MemoryTrain:  epoch  1, batch     1 | loss: 30.3040123Losses:  23.516706466674805 0.7937589287757874 19.868152618408203
MemoryTrain:  epoch  1, batch     2 | loss: 23.5167065Losses:  28.383853912353516 1.0871437788009644 23.112058639526367
MemoryTrain:  epoch  1, batch     3 | loss: 28.3838539Losses:  21.408647537231445 0.869359016418457 16.674467086791992
MemoryTrain:  epoch  1, batch     4 | loss: 21.4086475Losses:  17.19979476928711 0.7393810749053955 13.679067611694336
MemoryTrain:  epoch  1, batch     5 | loss: 17.1997948Losses:  26.494647979736328 0.4960700273513794 23.114334106445312
MemoryTrain:  epoch  1, batch     6 | loss: 26.4946480Losses:  24.47618293762207 1.566673994064331 19.868764877319336
MemoryTrain:  epoch  1, batch     7 | loss: 24.4761829Losses:  23.447696685791016 0.5258927941322327 19.85749053955078
MemoryTrain:  epoch  1, batch     8 | loss: 23.4476967Losses:  27.16690444946289 0.5104530453681946 23.178974151611328
MemoryTrain:  epoch  1, batch     9 | loss: 27.1669044Losses:  12.043485641479492 1.129594326019287 8.10094165802002
MemoryTrain:  epoch  1, batch    10 | loss: 12.0434856Losses:  22.500518798828125 0.7260257601737976 19.83924102783203
MemoryTrain:  epoch  1, batch    11 | loss: 22.5005188Losses:  26.792232513427734 1.3572214841842651 23.087976455688477
MemoryTrain:  epoch  1, batch    12 | loss: 26.7922325Losses:  10.849639892578125 -0.0 8.088004112243652
MemoryTrain:  epoch  1, batch    13 | loss: 10.8496399Losses:  25.742876052856445 0.2356702983379364 23.09381103515625
MemoryTrain:  epoch  2, batch     0 | loss: 25.7428761Losses:  28.874732971191406 0.22723326086997986 26.457687377929688
MemoryTrain:  epoch  2, batch     1 | loss: 28.8747330Losses:  22.935382843017578 0.7487772703170776 19.896879196166992
MemoryTrain:  epoch  2, batch     2 | loss: 22.9353828Losses:  25.865615844726562 0.4852227568626404 23.15045928955078
MemoryTrain:  epoch  2, batch     3 | loss: 25.8656158Losses:  25.90460205078125 0.27468061447143555 23.213701248168945
MemoryTrain:  epoch  2, batch     4 | loss: 25.9046021Losses:  23.35218620300293 0.780571699142456 19.861698150634766
MemoryTrain:  epoch  2, batch     5 | loss: 23.3521862Losses:  29.698219299316406 0.5662254691123962 26.47802734375
MemoryTrain:  epoch  2, batch     6 | loss: 29.6982193Losses:  17.21895408630371 1.5314775705337524 13.654885292053223
MemoryTrain:  epoch  2, batch     7 | loss: 17.2189541Losses:  23.78913116455078 1.1336109638214111 19.85757827758789
MemoryTrain:  epoch  2, batch     8 | loss: 23.7891312Losses:  22.657852172851562 0.703328013420105 19.83331871032715
MemoryTrain:  epoch  2, batch     9 | loss: 22.6578522Losses:  18.90335464477539 -0.0 16.754491806030273
MemoryTrain:  epoch  2, batch    10 | loss: 18.9033546Losses:  14.774892807006836 1.4590193033218384 10.795214653015137
MemoryTrain:  epoch  2, batch    11 | loss: 14.7748928Losses:  22.645248413085938 0.5046321153640747 19.874008178710938
MemoryTrain:  epoch  2, batch    12 | loss: 22.6452484Losses:  8.251032829284668 0.3004327416419983 5.665506839752197
MemoryTrain:  epoch  2, batch    13 | loss: 8.2510328Losses:  22.411222457885742 0.46898871660232544 19.849218368530273
MemoryTrain:  epoch  3, batch     0 | loss: 22.4112225Losses:  28.552654266357422 -0.0 26.453292846679688
MemoryTrain:  epoch  3, batch     1 | loss: 28.5526543Losses:  25.61855697631836 0.505246102809906 23.139223098754883
MemoryTrain:  epoch  3, batch     2 | loss: 25.6185570Losses:  16.23757553100586 0.516491711139679 13.672203063964844
MemoryTrain:  epoch  3, batch     3 | loss: 16.2375755Losses:  22.79898452758789 0.8027676939964294 19.848546981811523
MemoryTrain:  epoch  3, batch     4 | loss: 22.7989845Losses:  22.3804931640625 0.49059179425239563 19.886133193969727
MemoryTrain:  epoch  3, batch     5 | loss: 22.3804932Losses:  25.72020721435547 0.5091853737831116 23.1651554107666
MemoryTrain:  epoch  3, batch     6 | loss: 25.7202072Losses:  16.556060791015625 0.9919226169586182 13.670492172241211
MemoryTrain:  epoch  3, batch     7 | loss: 16.5560608Losses:  19.45074462890625 0.7629133462905884 16.69729232788086
MemoryTrain:  epoch  3, batch     8 | loss: 19.4507446Losses:  17.960813522338867 1.0909610986709595 13.809110641479492
MemoryTrain:  epoch  3, batch     9 | loss: 17.9608135Losses:  22.49773406982422 0.5287069082260132 19.853992462158203
MemoryTrain:  epoch  3, batch    10 | loss: 22.4977341Losses:  26.317087173461914 0.5184125900268555 23.106155395507812
MemoryTrain:  epoch  3, batch    11 | loss: 26.3170872Losses:  23.38548469543457 1.0549626350402832 19.864328384399414
MemoryTrain:  epoch  3, batch    12 | loss: 23.3854847Losses:  6.324368000030518 0.289453387260437 3.3161725997924805
MemoryTrain:  epoch  3, batch    13 | loss: 6.3243680Losses:  25.67134666442871 0.5169996023178101 23.1053524017334
MemoryTrain:  epoch  4, batch     0 | loss: 25.6713467Losses:  22.146562576293945 0.25138112902641296 19.87323570251465
MemoryTrain:  epoch  4, batch     1 | loss: 22.1465626Losses:  23.64153289794922 1.0551658868789673 19.8470516204834
MemoryTrain:  epoch  4, batch     2 | loss: 23.6415329Losses:  19.808273315429688 0.7319886684417725 16.702890396118164
MemoryTrain:  epoch  4, batch     3 | loss: 19.8082733Losses:  25.770156860351562 0.24202147126197815 23.1517391204834
MemoryTrain:  epoch  4, batch     4 | loss: 25.7701569Losses:  15.975074768066406 0.2538447976112366 13.666513442993164
MemoryTrain:  epoch  4, batch     5 | loss: 15.9750748Losses:  25.65595245361328 0.4589828848838806 23.189661026000977
MemoryTrain:  epoch  4, batch     6 | loss: 25.6559525Losses:  25.50537872314453 0.2556251287460327 23.134885787963867
MemoryTrain:  epoch  4, batch     7 | loss: 25.5053787Losses:  19.600318908691406 0.7312209606170654 16.773038864135742
MemoryTrain:  epoch  4, batch     8 | loss: 19.6003189Losses:  19.928335189819336 0.9859249591827393 16.69023323059082
MemoryTrain:  epoch  4, batch     9 | loss: 19.9283352Losses:  26.37539291381836 1.1127283573150635 23.094451904296875
MemoryTrain:  epoch  4, batch    10 | loss: 26.3753929Losses:  22.090513229370117 -0.0 19.82538414001465
MemoryTrain:  epoch  4, batch    11 | loss: 22.0905132Losses:  28.667861938476562 -0.0 26.456623077392578
MemoryTrain:  epoch  4, batch    12 | loss: 28.6678619Losses:  15.590337753295898 -0.0 13.67349910736084
MemoryTrain:  epoch  4, batch    13 | loss: 15.5903378Losses:  17.477928161621094 1.8634886741638184 13.689225196838379
MemoryTrain:  epoch  5, batch     0 | loss: 17.4779282Losses:  15.162906646728516 1.6102943420410156 10.897364616394043
MemoryTrain:  epoch  5, batch     1 | loss: 15.1629066Losses:  22.205198287963867 0.2587224841117859 19.86875343322754
MemoryTrain:  epoch  5, batch     2 | loss: 22.2051983Losses:  22.305994033813477 0.5154904127120972 19.820219039916992
MemoryTrain:  epoch  5, batch     3 | loss: 22.3059940Losses:  22.812664031982422 0.9720816612243652 19.848556518554688
MemoryTrain:  epoch  5, batch     4 | loss: 22.8126640Losses:  19.95240020751953 1.332324743270874 16.676347732543945
MemoryTrain:  epoch  5, batch     5 | loss: 19.9524002Losses:  17.17470932006836 1.6041442155838013 13.682254791259766
MemoryTrain:  epoch  5, batch     6 | loss: 17.1747093Losses:  28.707855224609375 0.2574848532676697 26.467304229736328
MemoryTrain:  epoch  5, batch     7 | loss: 28.7078552Losses:  25.388874053955078 0.27440598607063293 23.084243774414062
MemoryTrain:  epoch  5, batch     8 | loss: 25.3888741Losses:  25.466228485107422 0.4577256739139557 23.095335006713867
MemoryTrain:  epoch  5, batch     9 | loss: 25.4662285Losses:  16.390459060668945 0.7607046961784363 13.678781509399414
MemoryTrain:  epoch  5, batch    10 | loss: 16.3904591Losses:  19.23811149597168 0.24634569883346558 16.723962783813477
MemoryTrain:  epoch  5, batch    11 | loss: 19.2381115Losses:  28.926809310913086 0.5242139101028442 26.474376678466797
MemoryTrain:  epoch  5, batch    12 | loss: 28.9268093Losses:  15.609211921691895 -0.0 13.695547103881836
MemoryTrain:  epoch  5, batch    13 | loss: 15.6092119Losses:  29.148590087890625 0.7808900475502014 26.43130111694336
MemoryTrain:  epoch  6, batch     0 | loss: 29.1485901Losses:  19.93955421447754 1.3362805843353271 16.681535720825195
MemoryTrain:  epoch  6, batch     1 | loss: 19.9395542Losses:  25.817705154418945 0.7887245416641235 23.140024185180664
MemoryTrain:  epoch  6, batch     2 | loss: 25.8177052Losses:  18.844684600830078 0.2258749157190323 16.682376861572266
MemoryTrain:  epoch  6, batch     3 | loss: 18.8446846Losses:  25.26957130432129 0.24412351846694946 23.114999771118164
MemoryTrain:  epoch  6, batch     4 | loss: 25.2695713Losses:  22.729089736938477 0.986025333404541 19.827138900756836
MemoryTrain:  epoch  6, batch     5 | loss: 22.7290897Losses:  19.401508331298828 0.7500986456871033 16.688257217407227
MemoryTrain:  epoch  6, batch     6 | loss: 19.4015083Losses:  19.90207862854004 1.0658899545669556 16.731958389282227
MemoryTrain:  epoch  6, batch     7 | loss: 19.9020786Losses:  19.454147338867188 0.7718890905380249 16.71381378173828
MemoryTrain:  epoch  6, batch     8 | loss: 19.4541473Losses:  23.485933303833008 1.6969436407089233 19.81971549987793
MemoryTrain:  epoch  6, batch     9 | loss: 23.4859333Losses:  22.243850708007812 0.466330349445343 19.844375610351562
MemoryTrain:  epoch  6, batch    10 | loss: 22.2438507Losses:  19.220842361450195 0.5535309910774231 16.706512451171875
MemoryTrain:  epoch  6, batch    11 | loss: 19.2208424Losses:  16.450767517089844 0.7546305656433105 13.721203804016113
MemoryTrain:  epoch  6, batch    12 | loss: 16.4507675Losses:  12.682406425476074 -0.0 10.791688919067383
MemoryTrain:  epoch  6, batch    13 | loss: 12.6824064Losses:  19.254440307617188 0.5448580980300903 16.703346252441406
MemoryTrain:  epoch  7, batch     0 | loss: 19.2544403Losses:  19.112506866455078 0.5140496492385864 16.689056396484375
MemoryTrain:  epoch  7, batch     1 | loss: 19.1125069Losses:  28.972909927368164 0.5133942365646362 26.525619506835938
MemoryTrain:  epoch  7, batch     2 | loss: 28.9729099Losses:  25.277557373046875 0.24709287285804749 23.103370666503906
MemoryTrain:  epoch  7, batch     3 | loss: 25.2775574Losses:  19.16724967956543 0.5445135831832886 16.676652908325195
MemoryTrain:  epoch  7, batch     4 | loss: 19.1672497Losses:  25.309309005737305 0.24526363611221313 23.097515106201172
MemoryTrain:  epoch  7, batch     5 | loss: 25.3093090Losses:  18.829570770263672 0.23657946288585663 16.701759338378906
MemoryTrain:  epoch  7, batch     6 | loss: 18.8295708Losses:  22.2008113861084 0.4663088321685791 19.824012756347656
MemoryTrain:  epoch  7, batch     7 | loss: 22.2008114Losses:  22.26456069946289 0.5128505229949951 19.86111068725586
MemoryTrain:  epoch  7, batch     8 | loss: 22.2645607Losses:  23.700929641723633 1.9142343997955322 19.849740982055664
MemoryTrain:  epoch  7, batch     9 | loss: 23.7009296Losses:  23.008777618408203 1.2973244190216064 19.826194763183594
MemoryTrain:  epoch  7, batch    10 | loss: 23.0087776Losses:  25.814903259277344 0.7957444190979004 23.087657928466797
MemoryTrain:  epoch  7, batch    11 | loss: 25.8149033Losses:  18.834041595458984 0.24877969920635223 16.681201934814453
MemoryTrain:  epoch  7, batch    12 | loss: 18.8340416Losses:  12.648896217346191 -0.0 10.764472007751465
MemoryTrain:  epoch  7, batch    13 | loss: 12.6488962Losses:  13.730146408081055 1.0014605522155762 10.82761287689209
MemoryTrain:  epoch  8, batch     0 | loss: 13.7301464Losses:  22.787399291992188 1.083815097808838 19.8114013671875
MemoryTrain:  epoch  8, batch     1 | loss: 22.7873993Losses:  22.41197395324707 0.731753408908844 19.828611373901367
MemoryTrain:  epoch  8, batch     2 | loss: 22.4119740Losses:  19.43470001220703 0.8037019371986389 16.69721221923828
MemoryTrain:  epoch  8, batch     3 | loss: 19.4347000Losses:  25.743228912353516 0.7364283800125122 23.11543846130371
MemoryTrain:  epoch  8, batch     4 | loss: 25.7432289Losses:  22.455154418945312 0.732067346572876 19.838260650634766
MemoryTrain:  epoch  8, batch     5 | loss: 22.4551544Losses:  28.532272338867188 0.23618584871292114 26.41252899169922
MemoryTrain:  epoch  8, batch     6 | loss: 28.5322723Losses:  19.105445861816406 0.5184605121612549 16.702255249023438
MemoryTrain:  epoch  8, batch     7 | loss: 19.1054459Losses:  25.522274017333984 0.514205813407898 23.123321533203125
MemoryTrain:  epoch  8, batch     8 | loss: 25.5222740Losses:  22.281896591186523 0.47826558351516724 19.886615753173828
MemoryTrain:  epoch  8, batch     9 | loss: 22.2818966Losses:  21.98253059387207 0.24185961484909058 19.833768844604492
MemoryTrain:  epoch  8, batch    10 | loss: 21.9825306Losses:  16.278051376342773 0.7321158647537231 13.666736602783203
MemoryTrain:  epoch  8, batch    11 | loss: 16.2780514Losses:  25.477094650268555 0.48420900106430054 23.0938777923584
MemoryTrain:  epoch  8, batch    12 | loss: 25.4770947Losses:  12.716901779174805 -0.0 10.808645248413086
MemoryTrain:  epoch  8, batch    13 | loss: 12.7169018Losses:  19.628219604492188 1.0693995952606201 16.684396743774414
MemoryTrain:  epoch  9, batch     0 | loss: 19.6282196Losses:  20.024946212768555 1.3674829006195068 16.74454689025879
MemoryTrain:  epoch  9, batch     1 | loss: 20.0249462Losses:  25.23554039001465 0.2502039074897766 23.101818084716797
MemoryTrain:  epoch  9, batch     2 | loss: 25.2355404Losses:  19.908193588256836 1.356553554534912 16.67149543762207
MemoryTrain:  epoch  9, batch     3 | loss: 19.9081936Losses:  25.492942810058594 0.5021333694458008 23.1015682220459
MemoryTrain:  epoch  9, batch     4 | loss: 25.4929428Losses:  16.07094383239746 0.4713265597820282 13.694087982177734
MemoryTrain:  epoch  9, batch     5 | loss: 16.0709438Losses:  19.746089935302734 1.1839172840118408 16.68259048461914
MemoryTrain:  epoch  9, batch     6 | loss: 19.7460899Losses:  16.02661895751953 0.47611790895462036 13.660100936889648
MemoryTrain:  epoch  9, batch     7 | loss: 16.0266190Losses:  22.227603912353516 0.49033650755882263 19.80986213684082
MemoryTrain:  epoch  9, batch     8 | loss: 22.2276039Losses:  16.777292251586914 1.2265396118164062 13.675174713134766
MemoryTrain:  epoch  9, batch     9 | loss: 16.7772923Losses:  22.468158721923828 0.7443190813064575 19.80548667907715
MemoryTrain:  epoch  9, batch    10 | loss: 22.4681587Losses:  25.186338424682617 0.2421455979347229 23.07709312438965
MemoryTrain:  epoch  9, batch    11 | loss: 25.1863384Losses:  22.77377700805664 1.0315464735031128 19.836963653564453
MemoryTrain:  epoch  9, batch    12 | loss: 22.7737770Losses:  12.678349494934082 -0.0 10.770517349243164
MemoryTrain:  epoch  9, batch    13 | loss: 12.6783495
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 68.75%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 51.56%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 46.88%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 46.02%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 44.23%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 42.86%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 45.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 45.70%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 47.43%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 48.26%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 49.01%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 50.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 52.38%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 54.26%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 55.98%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 57.55%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 59.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 60.58%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 61.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 63.17%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 64.79%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 65.53%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 63.60%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 61.79%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 60.07%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 58.45%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 56.91%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 57.03%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 56.55%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 56.70%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 56.83%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 57.24%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 58.19%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 59.10%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 59.97%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 60.81%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 61.61%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 62.12%   [EVAL] batch:   50 | acc: 37.50%,  total acc: 61.64%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 61.06%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 60.50%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 59.95%   [EVAL] batch:   54 | acc: 12.50%,  total acc: 59.09%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 58.37%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 58.84%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 59.43%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 60.00%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 60.14%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 60.58%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 60.71%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 61.13%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 61.63%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 62.12%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 62.69%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 63.77%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 64.79%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 65.10%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 64.73%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 64.36%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 64.67%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 64.80%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 64.94%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 64.66%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 64.56%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 64.53%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 64.51%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 64.79%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 64.91%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 65.25%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 65.29%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 65.48%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 65.37%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 65.80%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 65.90%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 65.93%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 66.10%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 66.06%   [EVAL] batch:   93 | acc: 75.00%,  total acc: 66.16%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 66.47%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 66.82%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 67.09%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 67.30%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 66.52%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 65.87%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 65.23%   [EVAL] batch:  103 | acc: 0.00%,  total acc: 64.60%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 64.70%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 65.13%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 65.22%   [EVAL] batch:  108 | acc: 62.50%,  total acc: 65.19%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 65.28%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 65.32%   [EVAL] batch:  111 | acc: 18.75%,  total acc: 64.90%   
cur_acc:  ['0.8466', '0.8021', '0.7679', '0.8705', '0.6442', '0.5875', '0.6875']
his_acc:  ['0.8466', '0.8488', '0.7686', '0.7572', '0.7250', '0.7006', '0.6490']
Clustering into  19  clusters
Clusters:  [ 4  1  0 11 17 10  4 15 18 16 12  7  4 16  1 18  2  7  4  0  6  3  6 10
 14 18  4  3  9  8 16  4 13  5  2  1  0  0  8 10 16]
Losses:  20.758363723754883 8.523609161376953 3.7355175018310547
CurrentTrain: epoch  0, batch     0 | loss: 20.7583637Losses:  14.516152381896973 3.036613941192627 3.558478355407715
CurrentTrain: epoch  0, batch     1 | loss: 14.5161524Losses:  19.305255889892578 8.475059509277344 3.6235170364379883
CurrentTrain: epoch  1, batch     0 | loss: 19.3052559Losses:  13.239490509033203 2.6981124877929688 3.608116626739502
CurrentTrain: epoch  1, batch     1 | loss: 13.2394905Losses:  17.399141311645508 7.191658020019531 3.63850736618042
CurrentTrain: epoch  2, batch     0 | loss: 17.3991413Losses:  13.195696830749512 2.419065237045288 3.5971977710723877
CurrentTrain: epoch  2, batch     1 | loss: 13.1956968Losses:  18.721370697021484 8.428130149841309 3.613873243331909
CurrentTrain: epoch  3, batch     0 | loss: 18.7213707Losses:  11.746376037597656 2.3424112796783447 3.5321717262268066
CurrentTrain: epoch  3, batch     1 | loss: 11.7463760Losses:  16.54436492919922 7.386579513549805 3.6067588329315186
CurrentTrain: epoch  4, batch     0 | loss: 16.5443649Losses:  13.245552062988281 3.3423163890838623 3.565049171447754
CurrentTrain: epoch  4, batch     1 | loss: 13.2455521Losses:  17.606992721557617 8.461627006530762 3.5771045684814453
CurrentTrain: epoch  5, batch     0 | loss: 17.6069927Losses:  11.19046401977539 2.8124499320983887 3.566348075866699
CurrentTrain: epoch  5, batch     1 | loss: 11.1904640Losses:  16.135595321655273 7.44866943359375 3.500124454498291
CurrentTrain: epoch  6, batch     0 | loss: 16.1355953Losses:  9.535331726074219 1.7233132123947144 3.503833770751953
CurrentTrain: epoch  6, batch     1 | loss: 9.5353317Losses:  14.889552116394043 7.157870292663574 3.4516351222991943
CurrentTrain: epoch  7, batch     0 | loss: 14.8895521Losses:  10.577546119689941 2.3474230766296387 3.422647714614868
CurrentTrain: epoch  7, batch     1 | loss: 10.5775461Losses:  14.782215118408203 7.268692493438721 3.3933005332946777
CurrentTrain: epoch  8, batch     0 | loss: 14.7822151Losses:  9.854592323303223 2.362668752670288 3.430209159851074
CurrentTrain: epoch  8, batch     1 | loss: 9.8545923Losses:  15.257085800170898 7.5416460037231445 3.409536361694336
CurrentTrain: epoch  9, batch     0 | loss: 15.2570858Losses:  8.135732650756836 1.874535083770752 3.374826431274414
CurrentTrain: epoch  9, batch     1 | loss: 8.1357327
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: `` firstgroup 's acquisition of laidlaw will considerably enhance firstgroup 's existing activities in north america , which themselves have grown strongly since we first invested in the u.s. in 1999 , '' said firstgroup chief executive moir lockhead .
Head Entity: firstgroup
Tail Entity: laidlaw
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: `` in 2018, the tech giant google announced its acquisition of nest labs, a company specializing in smart home products, which has since become a key subsidiary in google's hardware division. ''  
Head Entity: google  
Tail Entity: nest labs  

Relation: organization subsidiaries  
Context: `` after the merger, the newly formed entity will operate under the name of united technologies, with pratt & whitney as one of its prominent subsidiaries focusing on aerospace manufacturing. ''  
Head Entity: united technologies  
Tail Entity: pratt & whitney  

Relation: organization subsidiaries  
Context: `` the beverage company pepsico has expanded its portfolio by acquiring the popular snack brand, fritos, which now operates as a subsidiary under the pepsico umbrella. ''  
Head Entity: pepsico  
Tail Entity: fritos  

Relation: organization subsidiaries  
Context: `` in a strategic move to enhance its market presence, the automotive manufacturer ford has integrated the electric vehicle startup rivian as a subsidiary to accelerate its EV development. ''  
Head Entity: ford  
Tail Entity: rivian  

Relation: organization subsidiaries  
Context: `` the pharmaceutical company pfizer has strengthened its research capabilities by acquiring the biotech firm array biopharma, which now functions as a subsidiary focused on cancer treatments. ''  
Head Entity: pfizer  
Tail Entity: array biopharma  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: The tech giant Alphabet Inc. has been making headlines with its innovative projects, but many are unaware that it is a subsidiary of the much larger Google LLC, which has been a leader in the tech industry for over two decades.  
Head Entity: Alphabet Inc.  
Tail Entity: Google LLC  

Relation: organization parents  
Context: In the world of finance, JPMorgan Chase & Co. has a long history, but it is important to note that it operates under the parent company, JPMorgan Chase Bank, which has been a key player in the banking sector since its inception.  
Head Entity: JPMorgan Chase & Co.  
Tail Entity: JPMorgan Chase Bank  

Relation: organization parents  
Context: The popular social media platform Instagram has gained immense popularity, but it is often overlooked that it is owned by the parent company Meta Platforms, Inc., which also owns Facebook and WhatsApp.  
Head Entity: Instagram  
Tail Entity: Meta Platforms, Inc.  

Relation: organization parents  
Context: The renowned car manufacturer Tesla, Inc. has revolutionized the automotive industry, yet it is a subsidiary of the larger entity, Tesla Motors, which was established to focus on electric vehicles.  
Head Entity: Tesla, Inc.  
Tail Entity: Tesla Motors  

Relation: organization parents  
Context: The global fast-food chain McDonald's Corporation is widely recognized for its burgers and fries, but it is essential to remember that it operates under the parent organization, McDonald's Holdings Company, which oversees its international franchises.  
Head Entity: McDonald's Corporation  
Tail Entity: McDonald's Holdings Company  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health crises.  
Head Entity: WHO  
Tail Entity: World Health Organization  

Relation: organization alternate names  
Context: The National Aeronautics and Space Administration, abbreviated as NASA, is responsible for the nation's civilian space program.  
Head Entity: NASA  
Tail Entity: National Aeronautics and Space Administration  

Relation: organization alternate names  
Context: The Federal Bureau of Investigation, or FBI, is the principal federal investigative agency in the United States.  
Head Entity: FBI  
Tail Entity: Federal Bureau of Investigation  

Relation: organization alternate names  
Context: The United Nations Educational, Scientific and Cultural Organization, known as UNESCO, aims to promote world peace and security through international cooperation.  
Head Entity: UNESCO  
Tail Entity: United Nations Educational, Scientific and Cultural Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2010-11-15 12:00:00 utc salesforce.com, a cloud computing company, announced its expansion into new offices in the heart of san francisco, aiming to accommodate its growing workforce.  
Head Entity: salesforce.com  
Tail Entity: san francisco  

Relation: organization city of headquarters  
Context: ------ new york 2015-03-10 09:30:00 utc the headquarters of the international business machines corporation, commonly known as ibm, is located in the bustling city of new york, where it has been a key player in technology for decades.  
Head Entity: ibm  
Tail Entity: new york  

Relation: organization city of headquarters  
Context: ------ seattle 2018-07-22 14:45:00 utc amazon.com, the e-commerce giant, continues to thrive with its headquarters situated in seattle, washington, contributing significantly to the local economy.  
Head Entity: amazon.com  
Tail Entity: seattle  

Relation: organization city of headquarters  
Context: ------ boston 2021-01-05 11:15:00 utc the biopharmaceutical company moderna, known for its innovative vaccine technology, has its headquarters in boston, massachusetts, a hub for life sciences.  
Head Entity: moderna  
Tail Entity: boston  

Relation: organization city of headquarters  
Context: ------ austin 2019-09-30 16:00:00 utc the tech startup indeed.com, which specializes in job search engines, has established its headquarters in austin, texas, attracting talent from across the country.  
Head Entity: indeed.com  
Tail Entity: austin  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: more than three decades ago , kerry 's work against the vietnam war set him on course to the senate - and , he often hoped , on to the presidency .
Head Entity: kerry
Tail Entity: he
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: During their childhood, Emma and her brother often played together in the backyard, creating memories that would last a lifetime.  
Head Entity: Emma  
Tail Entity: her brother  

Relation: person siblings  
Context: After the family reunion, Sarah realized how much she missed her sister, who had moved to another state for work.  
Head Entity: Sarah  
Tail Entity: her sister  

Relation: person siblings  
Context: The documentary highlighted the bond between the two brothers, showcasing their adventures growing up in a small town.  
Head Entity: the two brothers  
Tail Entity: their adventures  

Relation: person siblings  
Context: When Michael received the news about his sister's promotion, he couldn't help but feel proud of her achievements.  
Head Entity: Michael  
Tail Entity: his sister  

Relation: person siblings  
Context: At the wedding, Jessica shared a heartfelt speech about her brother, reminiscing about their childhood and the support they provided each other.  
Head Entity: Jessica  
Tail Entity: her brother  
Losses:  23.409881591796875 0.5205725431442261 19.884815216064453
MemoryTrain:  epoch  0, batch     0 | loss: 23.4098816Losses:  29.695926666259766 0.5168468952178955 26.452238082885742
MemoryTrain:  epoch  0, batch     1 | loss: 29.6959267Losses:  21.56570816040039 1.3677153587341309 16.69565773010254
MemoryTrain:  epoch  0, batch     2 | loss: 21.5657082Losses:  24.130277633666992 0.8332318663597107 19.914505004882812
MemoryTrain:  epoch  0, batch     3 | loss: 24.1302776Losses:  27.205062866210938 0.7579408884048462 23.136566162109375
MemoryTrain:  epoch  0, batch     4 | loss: 27.2050629Losses:  36.914737701416016 0.273110032081604 33.563987731933594
MemoryTrain:  epoch  0, batch     5 | loss: 36.9147377Losses:  31.18433380126953 0.4972056448459625 26.418025970458984
MemoryTrain:  epoch  0, batch     6 | loss: 31.1843338Losses:  22.75566864013672 0.838817834854126 19.87330436706543
MemoryTrain:  epoch  0, batch     7 | loss: 22.7556686Losses:  24.188892364501953 0.9678565263748169 19.832889556884766
MemoryTrain:  epoch  0, batch     8 | loss: 24.1888924Losses:  27.274560928344727 0.5251871943473816 23.34343910217285
MemoryTrain:  epoch  0, batch     9 | loss: 27.2745609Losses:  24.195728302001953 0.793357789516449 20.103490829467773
MemoryTrain:  epoch  0, batch    10 | loss: 24.1957283Losses:  18.040889739990234 1.0795743465423584 13.679811477661133
MemoryTrain:  epoch  0, batch    11 | loss: 18.0408897Losses:  23.179101943969727 0.4684832990169525 19.881380081176758
MemoryTrain:  epoch  0, batch    12 | loss: 23.1791019Losses:  31.439620971679688 0.5030924081802368 26.644325256347656
MemoryTrain:  epoch  0, batch    13 | loss: 31.4396210Losses:  26.857540130615234 0.25016117095947266 23.185836791992188
MemoryTrain:  epoch  0, batch    14 | loss: 26.8575401Losses:  10.15181827545166 -0.0 8.076282501220703
MemoryTrain:  epoch  0, batch    15 | loss: 10.1518183Losses:  18.607656478881836 1.1300781965255737 13.886760711669922
MemoryTrain:  epoch  1, batch     0 | loss: 18.6076565Losses:  23.141311645507812 0.5293371081352234 19.8599910736084
MemoryTrain:  epoch  1, batch     1 | loss: 23.1413116Losses:  27.338764190673828 1.1021111011505127 23.11925506591797
MemoryTrain:  epoch  1, batch     2 | loss: 27.3387642Losses:  26.22135353088379 -0.0 23.202713012695312
MemoryTrain:  epoch  1, batch     3 | loss: 26.2213535Losses:  26.95783233642578 1.3342859745025635 23.09281349182129
MemoryTrain:  epoch  1, batch     4 | loss: 26.9578323Losses:  23.44696807861328 0.25606268644332886 19.86591339111328
MemoryTrain:  epoch  1, batch     5 | loss: 23.4469681Losses:  26.068471908569336 0.737957239151001 23.08603858947754
MemoryTrain:  epoch  1, batch     6 | loss: 26.0684719Losses:  20.639545440673828 0.45682233572006226 16.822315216064453
MemoryTrain:  epoch  1, batch     7 | loss: 20.6395454Losses:  29.92778968811035 0.23600870370864868 26.5675048828125
MemoryTrain:  epoch  1, batch     8 | loss: 29.9277897Losses:  23.503948211669922 1.0062731504440308 19.83662986755371
MemoryTrain:  epoch  1, batch     9 | loss: 23.5039482Losses:  29.204360961914062 0.2551689147949219 26.464401245117188
MemoryTrain:  epoch  1, batch    10 | loss: 29.2043610Losses:  23.19713592529297 0.5133941173553467 19.92942237854004
MemoryTrain:  epoch  1, batch    11 | loss: 23.1971359Losses:  27.419160842895508 1.043438196182251 23.321775436401367
MemoryTrain:  epoch  1, batch    12 | loss: 27.4191608Losses:  22.672704696655273 0.2338147610425949 19.91031837463379
MemoryTrain:  epoch  1, batch    13 | loss: 22.6727047Losses:  17.136674880981445 1.106562614440918 13.689569473266602
MemoryTrain:  epoch  1, batch    14 | loss: 17.1366749Losses:  6.470102310180664 -0.0 3.314453601837158
MemoryTrain:  epoch  1, batch    15 | loss: 6.4701023Losses:  20.02410888671875 1.090471625328064 16.68048095703125
MemoryTrain:  epoch  2, batch     0 | loss: 20.0241089Losses:  23.022924423217773 0.47688713669776917 19.82619285583496
MemoryTrain:  epoch  2, batch     1 | loss: 23.0229244Losses:  22.765878677368164 0.48188817501068115 19.860074996948242
MemoryTrain:  epoch  2, batch     2 | loss: 22.7658787Losses:  26.034194946289062 0.22459568083286285 23.074649810791016
MemoryTrain:  epoch  2, batch     3 | loss: 26.0341949Losses:  26.08576011657715 0.7141762971878052 23.108062744140625
MemoryTrain:  epoch  2, batch     4 | loss: 26.0857601Losses:  20.285478591918945 1.1000347137451172 16.73828125
MemoryTrain:  epoch  2, batch     5 | loss: 20.2854786Losses:  26.56728744506836 0.7675579786300659 23.17649269104004
MemoryTrain:  epoch  2, batch     6 | loss: 26.5672874Losses:  26.30177879333496 0.6179765462875366 23.09952735900879
MemoryTrain:  epoch  2, batch     7 | loss: 26.3017788Losses:  35.42327117919922 -0.0 33.4461784362793
MemoryTrain:  epoch  2, batch     8 | loss: 35.4232712Losses:  29.175018310546875 -0.0 26.457622528076172
MemoryTrain:  epoch  2, batch     9 | loss: 29.1750183Losses:  14.705116271972656 1.3420336246490479 10.794163703918457
MemoryTrain:  epoch  2, batch    10 | loss: 14.7051163Losses:  25.856861114501953 0.24548667669296265 23.1051025390625
MemoryTrain:  epoch  2, batch    11 | loss: 25.8568611Losses:  24.430543899536133 0.6044448614120483 20.073484420776367
MemoryTrain:  epoch  2, batch    12 | loss: 24.4305439Losses:  22.632314682006836 0.26448607444763184 19.816404342651367
MemoryTrain:  epoch  2, batch    13 | loss: 22.6323147Losses:  23.7782039642334 0.8160398006439209 19.939006805419922
MemoryTrain:  epoch  2, batch    14 | loss: 23.7782040Losses:  9.9551362991333 -0.0 8.061352729797363
MemoryTrain:  epoch  2, batch    15 | loss: 9.9551363Losses:  23.33992576599121 1.120700478553772 19.844831466674805
MemoryTrain:  epoch  3, batch     0 | loss: 23.3399258Losses:  26.07470703125 0.24775078892707825 23.087406158447266
MemoryTrain:  epoch  3, batch     1 | loss: 26.0747070Losses:  22.375362396240234 -0.0 19.926342010498047
MemoryTrain:  epoch  3, batch     2 | loss: 22.3753624Losses:  17.584205627441406 1.59010648727417 13.645698547363281
MemoryTrain:  epoch  3, batch     3 | loss: 17.5842056Losses:  33.04187774658203 0.4980497360229492 29.964448928833008
MemoryTrain:  epoch  3, batch     4 | loss: 33.0418777Losses:  16.93476676940918 0.7905770540237427 13.685882568359375
MemoryTrain:  epoch  3, batch     5 | loss: 16.9347668Losses:  25.77626609802246 -0.0 23.174015045166016
MemoryTrain:  epoch  3, batch     6 | loss: 25.7762661Losses:  25.933645248413086 0.764235258102417 23.126054763793945
MemoryTrain:  epoch  3, batch     7 | loss: 25.9336452Losses:  32.34449768066406 0.25067031383514404 30.01340103149414
MemoryTrain:  epoch  3, batch     8 | loss: 32.3444977Losses:  19.284425735473633 0.562838077545166 16.686264038085938
MemoryTrain:  epoch  3, batch     9 | loss: 19.2844257Losses:  13.64124870300293 0.5049930810928345 10.766094207763672
MemoryTrain:  epoch  3, batch    10 | loss: 13.6412487Losses:  26.3011474609375 0.980198860168457 23.117584228515625
MemoryTrain:  epoch  3, batch    11 | loss: 26.3011475Losses:  16.640625 1.0358901023864746 13.674200057983398
MemoryTrain:  epoch  3, batch    12 | loss: 16.6406250Losses:  25.75185775756836 0.49592387676239014 23.092260360717773
MemoryTrain:  epoch  3, batch    13 | loss: 25.7518578Losses:  23.490201950073242 0.7359678745269775 19.828521728515625
MemoryTrain:  epoch  3, batch    14 | loss: 23.4902020Losses:  8.127771377563477 -0.0 5.616652011871338
MemoryTrain:  epoch  3, batch    15 | loss: 8.1277714Losses:  26.077468872070312 0.25255200266838074 23.142053604125977
MemoryTrain:  epoch  4, batch     0 | loss: 26.0774689Losses:  22.473876953125 0.4846515953540802 19.834074020385742
MemoryTrain:  epoch  4, batch     1 | loss: 22.4738770Losses:  28.980968475341797 0.493705689907074 26.466552734375
MemoryTrain:  epoch  4, batch     2 | loss: 28.9809685Losses:  25.505313873291016 0.23493514955043793 23.097612380981445
MemoryTrain:  epoch  4, batch     3 | loss: 25.5053139Losses:  28.882518768310547 0.2606271803379059 26.445144653320312
MemoryTrain:  epoch  4, batch     4 | loss: 28.8825188Losses:  25.735584259033203 0.25437110662460327 23.113916397094727
MemoryTrain:  epoch  4, batch     5 | loss: 25.7355843Losses:  26.059009552001953 1.0692903995513916 23.074445724487305
MemoryTrain:  epoch  4, batch     6 | loss: 26.0590096Losses:  20.53799057006836 1.5620447397232056 16.674768447875977
MemoryTrain:  epoch  4, batch     7 | loss: 20.5379906Losses:  32.395015716552734 0.5182770490646362 29.908302307128906
MemoryTrain:  epoch  4, batch     8 | loss: 32.3950157Losses:  21.919130325317383 -0.0 19.843170166015625
MemoryTrain:  epoch  4, batch     9 | loss: 21.9191303Losses:  28.417097091674805 -0.0 26.470029830932617
MemoryTrain:  epoch  4, batch    10 | loss: 28.4170971Losses:  28.83463478088379 0.24700935184955597 26.466276168823242
MemoryTrain:  epoch  4, batch    11 | loss: 28.8346348Losses:  17.17534828186035 0.6885162591934204 13.714970588684082
MemoryTrain:  epoch  4, batch    12 | loss: 17.1753483Losses:  28.664106369018555 0.23889365792274475 26.45926284790039
MemoryTrain:  epoch  4, batch    13 | loss: 28.6641064Losses:  22.978670120239258 1.0909055471420288 19.837297439575195
MemoryTrain:  epoch  4, batch    14 | loss: 22.9786701Losses:  7.651867389678955 -0.0 5.573544979095459
MemoryTrain:  epoch  4, batch    15 | loss: 7.6518674Losses:  16.381134033203125 0.7560975551605225 13.674383163452148
MemoryTrain:  epoch  5, batch     0 | loss: 16.3811340Losses:  28.853307723999023 0.24485856294631958 26.450130462646484
MemoryTrain:  epoch  5, batch     1 | loss: 28.8533077Losses:  25.318151473999023 0.24693390727043152 23.087295532226562
MemoryTrain:  epoch  5, batch     2 | loss: 25.3181515Losses:  31.867786407470703 -0.0 29.859638214111328
MemoryTrain:  epoch  5, batch     3 | loss: 31.8677864Losses:  29.34004783630371 0.4726974070072174 26.431936264038086
MemoryTrain:  epoch  5, batch     4 | loss: 29.3400478Losses:  11.361252784729004 0.8716151714324951 8.062061309814453
MemoryTrain:  epoch  5, batch     5 | loss: 11.3612528Losses:  25.47274398803711 0.23470556735992432 23.09466552734375
MemoryTrain:  epoch  5, batch     6 | loss: 25.4727440Losses:  22.146095275878906 0.2744831442832947 19.844478607177734
MemoryTrain:  epoch  5, batch     7 | loss: 22.1460953Losses:  14.504440307617188 1.6623027324676514 10.766495704650879
MemoryTrain:  epoch  5, batch     8 | loss: 14.5044403Losses:  26.178178787231445 0.59163498878479 23.10254669189453
MemoryTrain:  epoch  5, batch     9 | loss: 26.1781788Losses:  25.36870002746582 0.23621955513954163 23.12044334411621
MemoryTrain:  epoch  5, batch    10 | loss: 25.3687000Losses:  25.876737594604492 0.8543253540992737 23.09423065185547
MemoryTrain:  epoch  5, batch    11 | loss: 25.8767376Losses:  17.540197372436523 1.7540161609649658 13.673042297363281
MemoryTrain:  epoch  5, batch    12 | loss: 17.5401974Losses:  19.7040958404541 1.0958839654922485 16.687395095825195
MemoryTrain:  epoch  5, batch    13 | loss: 19.7040958Losses:  28.64252471923828 0.2537539005279541 26.462661743164062
MemoryTrain:  epoch  5, batch    14 | loss: 28.6425247Losses:  7.880857944488525 0.312714159488678 5.647082805633545
MemoryTrain:  epoch  5, batch    15 | loss: 7.8808579Losses:  16.9788818359375 0.8058351278305054 13.695289611816406
MemoryTrain:  epoch  6, batch     0 | loss: 16.9788818Losses:  22.854215621948242 0.9969289302825928 19.808979034423828
MemoryTrain:  epoch  6, batch     1 | loss: 22.8542156Losses:  25.780656814575195 0.5486786365509033 23.088342666625977
MemoryTrain:  epoch  6, batch     2 | loss: 25.7806568Losses:  19.799163818359375 1.0325844287872314 16.74160385131836
MemoryTrain:  epoch  6, batch     3 | loss: 19.7991638Losses:  18.152324676513672 2.3264760971069336 13.688983917236328
MemoryTrain:  epoch  6, batch     4 | loss: 18.1523247Losses:  26.032428741455078 1.099638819694519 23.0606632232666
MemoryTrain:  epoch  6, batch     5 | loss: 26.0324287Losses:  28.412128448486328 -0.0 26.42938804626465
MemoryTrain:  epoch  6, batch     6 | loss: 28.4121284Losses:  19.322721481323242 0.7325617074966431 16.70308494567871
MemoryTrain:  epoch  6, batch     7 | loss: 19.3227215Losses:  22.599109649658203 0.8271070718765259 19.801244735717773
MemoryTrain:  epoch  6, batch     8 | loss: 22.5991096Losses:  23.194246292114258 1.3472323417663574 19.814054489135742
MemoryTrain:  epoch  6, batch     9 | loss: 23.1942463Losses:  16.63594627380371 1.006399154663086 13.663802146911621
MemoryTrain:  epoch  6, batch    10 | loss: 16.6359463Losses:  22.551563262939453 0.7125189900398254 19.84851837158203
MemoryTrain:  epoch  6, batch    11 | loss: 22.5515633Losses:  25.836978912353516 0.594603419303894 23.125368118286133
MemoryTrain:  epoch  6, batch    12 | loss: 25.8369789Losses:  16.134315490722656 0.49715447425842285 13.689664840698242
MemoryTrain:  epoch  6, batch    13 | loss: 16.1343155Losses:  25.46841812133789 0.24914169311523438 23.078933715820312
MemoryTrain:  epoch  6, batch    14 | loss: 25.4684181Losses:  9.921345710754395 -0.0 8.0613431930542
MemoryTrain:  epoch  6, batch    15 | loss: 9.9213457Losses:  19.26249122619629 0.33325257897377014 16.696786880493164
MemoryTrain:  epoch  7, batch     0 | loss: 19.2624912Losses:  25.44940185546875 0.4889748692512512 23.05999755859375
MemoryTrain:  epoch  7, batch     1 | loss: 25.4494019Losses:  25.58172607421875 0.5328960418701172 23.08003044128418
MemoryTrain:  epoch  7, batch     2 | loss: 25.5817261Losses:  22.619483947753906 0.8751035332679749 19.825410842895508
MemoryTrain:  epoch  7, batch     3 | loss: 22.6194839Losses:  25.794845581054688 0.4960048794746399 23.088764190673828
MemoryTrain:  epoch  7, batch     4 | loss: 25.7948456Losses:  19.16050910949707 0.515596330165863 16.690298080444336
MemoryTrain:  epoch  7, batch     5 | loss: 19.1605091Losses:  25.28476333618164 0.23847869038581848 23.13268280029297
MemoryTrain:  epoch  7, batch     6 | loss: 25.2847633Losses:  22.451541900634766 0.7500244379043579 19.832265853881836
MemoryTrain:  epoch  7, batch     7 | loss: 22.4515419Losses:  21.956485748291016 0.2432764768600464 19.806486129760742
MemoryTrain:  epoch  7, batch     8 | loss: 21.9564857Losses:  25.663711547851562 0.7247966527938843 23.06682586669922
MemoryTrain:  epoch  7, batch     9 | loss: 25.6637115Losses:  19.789445877075195 0.9441545009613037 16.696727752685547
MemoryTrain:  epoch  7, batch    10 | loss: 19.7894459Losses:  25.309385299682617 0.24838373064994812 23.099468231201172
MemoryTrain:  epoch  7, batch    11 | loss: 25.3093853Losses:  28.583236694335938 0.2520885467529297 26.444746017456055
MemoryTrain:  epoch  7, batch    12 | loss: 28.5832367Losses:  28.79824447631836 0.4676724672317505 26.430416107177734
MemoryTrain:  epoch  7, batch    13 | loss: 28.7982445Losses:  21.98392105102539 0.24634283781051636 19.828487396240234
MemoryTrain:  epoch  7, batch    14 | loss: 21.9839211Losses:  6.286313056945801 0.6539445519447327 3.3008432388305664
MemoryTrain:  epoch  7, batch    15 | loss: 6.2863131Losses:  28.968568801879883 0.48762911558151245 26.468822479248047
MemoryTrain:  epoch  8, batch     0 | loss: 28.9685688Losses:  22.02815055847168 0.2270858734846115 19.8189754486084
MemoryTrain:  epoch  8, batch     1 | loss: 22.0281506Losses:  22.602127075195312 0.7697932720184326 19.839868545532227
MemoryTrain:  epoch  8, batch     2 | loss: 22.6021271Losses:  28.820768356323242 0.49686434864997864 26.430465698242188
MemoryTrain:  epoch  8, batch     3 | loss: 28.8207684Losses:  22.383514404296875 0.5377808809280396 19.87418556213379
MemoryTrain:  epoch  8, batch     4 | loss: 22.3835144Losses:  16.65032958984375 1.001205325126648 13.702864646911621
MemoryTrain:  epoch  8, batch     5 | loss: 16.6503296Losses:  28.379169464111328 -0.0 26.428693771362305
MemoryTrain:  epoch  8, batch     6 | loss: 28.3791695Losses:  19.225812911987305 0.5220996141433716 16.716943740844727
MemoryTrain:  epoch  8, batch     7 | loss: 19.2258129Losses:  28.676860809326172 0.2480841875076294 26.444576263427734
MemoryTrain:  epoch  8, batch     8 | loss: 28.6768608Losses:  13.450725555419922 0.8320857882499695 10.764837265014648
MemoryTrain:  epoch  8, batch     9 | loss: 13.4507256Losses:  25.648096084594727 0.7214832305908203 23.060871124267578
MemoryTrain:  epoch  8, batch    10 | loss: 25.6480961Losses:  16.457738876342773 0.9592850208282471 13.643685340881348
MemoryTrain:  epoch  8, batch    11 | loss: 16.4577389Losses:  19.139087677001953 0.5178934931755066 16.70049476623535
MemoryTrain:  epoch  8, batch    12 | loss: 19.1390877Losses:  25.395362854003906 0.48649245500564575 23.053863525390625
MemoryTrain:  epoch  8, batch    13 | loss: 25.3953629Losses:  22.861936569213867 1.0774089097976685 19.847434997558594
MemoryTrain:  epoch  8, batch    14 | loss: 22.8619366Losses:  7.497994899749756 -0.0 5.556490421295166
MemoryTrain:  epoch  8, batch    15 | loss: 7.4979949Losses:  29.49367904663086 1.1353933811187744 26.446170806884766
MemoryTrain:  epoch  9, batch     0 | loss: 29.4936790Losses:  28.371620178222656 -0.0 26.473915100097656
MemoryTrain:  epoch  9, batch     1 | loss: 28.3716202Losses:  18.845783233642578 0.25606608390808105 16.66360092163086
MemoryTrain:  epoch  9, batch     2 | loss: 18.8457832Losses:  22.249223709106445 0.5037664175033569 19.808866500854492
MemoryTrain:  epoch  9, batch     3 | loss: 22.2492237Losses:  28.611713409423828 0.2507254481315613 26.449462890625
MemoryTrain:  epoch  9, batch     4 | loss: 28.6117134Losses:  28.548099517822266 0.2486112415790558 26.420146942138672
MemoryTrain:  epoch  9, batch     5 | loss: 28.5480995Losses:  25.447345733642578 0.5018831491470337 23.058950424194336
MemoryTrain:  epoch  9, batch     6 | loss: 25.4473457Losses:  25.25211524963379 0.2435404658317566 23.078676223754883
MemoryTrain:  epoch  9, batch     7 | loss: 25.2521152Losses:  17.956634521484375 2.382754325866699 13.662298202514648
MemoryTrain:  epoch  9, batch     8 | loss: 17.9566345Losses:  25.711624145507812 0.7075390815734863 23.080947875976562
MemoryTrain:  epoch  9, batch     9 | loss: 25.7116241Losses:  19.069351196289062 0.5185828804969788 16.669511795043945
MemoryTrain:  epoch  9, batch    10 | loss: 19.0693512Losses:  29.05992889404297 0.7310870289802551 26.42709732055664
MemoryTrain:  epoch  9, batch    11 | loss: 29.0599289Losses:  19.4180965423584 0.8001497387886047 16.66077423095703
MemoryTrain:  epoch  9, batch    12 | loss: 19.4180965Losses:  19.979995727539062 1.3178914785385132 16.708200454711914
MemoryTrain:  epoch  9, batch    13 | loss: 19.9799957Losses:  19.57761001586914 0.9876582026481628 16.6721134185791
MemoryTrain:  epoch  9, batch    14 | loss: 19.5776100Losses:  7.396096229553223 -0.0 5.549764633178711
MemoryTrain:  epoch  9, batch    15 | loss: 7.3960962
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 18.75%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 20.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 22.32%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 29.69%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 31.94%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 35.00%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 38.07%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 40.10%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 40.87%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 37.95%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 35.83%   [EVAL] batch:   15 | acc: 31.25%,  total acc: 35.55%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 33.46%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 31.60%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 30.94%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 30.65%   [EVAL] batch:   21 | acc: 18.75%,  total acc: 30.11%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 60.71%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 53.91%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 52.08%   [EVAL] batch:    9 | acc: 18.75%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 47.16%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 47.40%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 44.71%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 43.30%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 45.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 45.70%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 47.43%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 48.26%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 49.01%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 50.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 52.38%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 53.98%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 55.71%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 57.29%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 59.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 60.34%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 61.57%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 64.22%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 65.00%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 65.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 66.60%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 65.72%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 63.79%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 61.96%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 60.24%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 58.61%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 57.07%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 56.41%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 57.19%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 56.55%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 56.68%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 57.64%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 58.56%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 59.44%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 60.29%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 61.10%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 61.62%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 61.27%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 60.94%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 60.26%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 59.72%   [EVAL] batch:   54 | acc: 18.75%,  total acc: 58.98%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 58.26%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 58.33%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 58.84%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 59.43%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 60.00%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 60.14%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 60.58%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 60.91%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 61.33%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 61.83%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 62.31%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 63.42%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 63.95%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 64.46%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 65.28%   [EVAL] batch:   72 | acc: 12.50%,  total acc: 64.55%   [EVAL] batch:   73 | acc: 6.25%,  total acc: 63.77%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 64.00%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 64.14%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 64.29%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 64.02%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 63.92%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 63.98%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 63.97%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 64.25%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 64.38%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 64.78%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 64.68%   [EVAL] batch:   86 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 64.70%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 64.89%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 65.07%   [EVAL] batch:   90 | acc: 62.50%,  total acc: 65.04%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 65.22%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 65.19%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 65.23%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 65.39%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 65.56%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 66.20%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 66.41%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 66.25%   [EVAL] batch:  100 | acc: 0.00%,  total acc: 65.59%   [EVAL] batch:  101 | acc: 0.00%,  total acc: 64.95%   [EVAL] batch:  102 | acc: 0.00%,  total acc: 64.32%   [EVAL] batch:  103 | acc: 6.25%,  total acc: 63.76%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 63.81%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 64.08%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 64.24%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 64.33%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 64.55%   [EVAL] batch:  110 | acc: 93.75%,  total acc: 64.81%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 64.51%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 64.16%   [EVAL] batch:  113 | acc: 18.75%,  total acc: 63.76%   [EVAL] batch:  114 | acc: 18.75%,  total acc: 63.37%   [EVAL] batch:  115 | acc: 18.75%,  total acc: 62.98%   [EVAL] batch:  116 | acc: 25.00%,  total acc: 62.66%   [EVAL] batch:  117 | acc: 37.50%,  total acc: 62.45%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 62.45%   [EVAL] batch:  119 | acc: 56.25%,  total acc: 62.40%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 62.40%   [EVAL] batch:  121 | acc: 68.75%,  total acc: 62.45%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:  123 | acc: 56.25%,  total acc: 62.45%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 61.95%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 61.46%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 61.22%   [EVAL] batch:  127 | acc: 6.25%,  total acc: 60.79%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 60.32%   [EVAL] batch:  129 | acc: 25.00%,  total acc: 60.05%   [EVAL] batch:  130 | acc: 25.00%,  total acc: 59.78%   [EVAL] batch:  131 | acc: 18.75%,  total acc: 59.47%   [EVAL] batch:  132 | acc: 25.00%,  total acc: 59.21%   
cur_acc:  ['0.8466', '0.8021', '0.7679', '0.8705', '0.6442', '0.5875', '0.6875', '0.3011']
his_acc:  ['0.8466', '0.8488', '0.7686', '0.7572', '0.7250', '0.7006', '0.6490', '0.5921']
--------Round  3
seed:  400
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 0 1]
Losses:  25.682666778564453 12.589941024780273 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 25.6826668Losses:  24.88964080810547 11.69279670715332 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 24.8896408Losses:  20.427949905395508 7.431611061096191 -0.0
CurrentTrain: epoch  0, batch     2 | loss: 20.4279499Losses:  21.990135192871094 9.077369689941406 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 21.9901352Losses:  24.58681869506836 11.546134948730469 -0.0
CurrentTrain: epoch  0, batch     4 | loss: 24.5868187Losses:  22.063631057739258 9.39781379699707 -0.0
CurrentTrain: epoch  0, batch     5 | loss: 22.0636311Losses:  21.196243286132812 8.571844100952148 -0.0
CurrentTrain: epoch  0, batch     6 | loss: 21.1962433Losses:  21.679492950439453 8.998739242553711 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 21.6794930Losses:  25.18987464904785 12.610332489013672 -0.0
CurrentTrain: epoch  0, batch     8 | loss: 25.1898746Losses:  19.15936851501465 6.851724147796631 -0.0
CurrentTrain: epoch  0, batch     9 | loss: 19.1593685Losses:  21.21807098388672 9.10126781463623 -0.0
CurrentTrain: epoch  0, batch    10 | loss: 21.2180710Losses:  22.244380950927734 10.090155601501465 -0.0
CurrentTrain: epoch  0, batch    11 | loss: 22.2443810Losses:  21.214527130126953 9.514575004577637 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 21.2145271Losses:  18.977298736572266 7.202720642089844 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 18.9772987Losses:  20.289236068725586 8.4110107421875 -0.0
CurrentTrain: epoch  0, batch    14 | loss: 20.2892361Losses:  20.255077362060547 8.339569091796875 -0.0
CurrentTrain: epoch  0, batch    15 | loss: 20.2550774Losses:  23.42721176147461 11.983478546142578 -0.0
CurrentTrain: epoch  0, batch    16 | loss: 23.4272118Losses:  19.414031982421875 7.976731777191162 -0.0
CurrentTrain: epoch  0, batch    17 | loss: 19.4140320Losses:  19.30270004272461 7.9735822677612305 -0.0
CurrentTrain: epoch  0, batch    18 | loss: 19.3027000Losses:  20.887256622314453 9.873083114624023 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 20.8872566Losses:  19.175037384033203 7.945736408233643 -0.0
CurrentTrain: epoch  0, batch    20 | loss: 19.1750374Losses:  23.463075637817383 11.778610229492188 -0.0
CurrentTrain: epoch  0, batch    21 | loss: 23.4630756Losses:  20.42401123046875 8.620155334472656 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 20.4240112Losses:  19.296466827392578 8.008835792541504 -0.0
CurrentTrain: epoch  0, batch    23 | loss: 19.2964668Losses:  18.403165817260742 7.439013957977295 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 18.4031658Losses:  20.9534854888916 9.705619812011719 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 20.9534855Losses:  18.215309143066406 7.575067520141602 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 18.2153091Losses:  21.615257263183594 10.256553649902344 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 21.6152573Losses:  22.247051239013672 11.891883850097656 -0.0
CurrentTrain: epoch  0, batch    28 | loss: 22.2470512Losses:  18.589603424072266 8.074652671813965 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 18.5896034Losses:  19.264305114746094 8.82435131072998 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 19.2643051Losses:  16.486019134521484 6.027044296264648 -0.0
CurrentTrain: epoch  0, batch    31 | loss: 16.4860191Losses:  20.072429656982422 9.237096786499023 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 20.0724297Losses:  24.766462326049805 14.133846282958984 -0.0
CurrentTrain: epoch  0, batch    33 | loss: 24.7664623Losses:  16.361770629882812 6.114754676818848 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 16.3617706Losses:  17.95578956604004 7.314313888549805 -0.0
CurrentTrain: epoch  0, batch    35 | loss: 17.9557896Losses:  18.559505462646484 7.771894454956055 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 18.5595055Losses:  12.686856269836426 2.904435396194458 -0.0
CurrentTrain: epoch  0, batch    37 | loss: 12.6868563Losses:  19.05051040649414 8.836954116821289 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 19.0505104Losses:  17.7834529876709 7.396581649780273 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 17.7834530Losses:  27.095354080200195 16.731204986572266 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 27.0953541Losses:  17.183208465576172 7.438852787017822 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 17.1832085Losses:  16.738521575927734 6.9447221755981445 -0.0
CurrentTrain: epoch  1, batch     4 | loss: 16.7385216Losses:  19.745182037353516 9.950637817382812 -0.0
CurrentTrain: epoch  1, batch     5 | loss: 19.7451820Losses:  21.583423614501953 12.065135955810547 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 21.5834236Losses:  17.77338409423828 8.000975608825684 -0.0
CurrentTrain: epoch  1, batch     7 | loss: 17.7733841Losses:  18.579975128173828 8.837629318237305 -0.0
CurrentTrain: epoch  1, batch     8 | loss: 18.5799751Losses:  21.391721725463867 12.06005859375 -0.0
CurrentTrain: epoch  1, batch     9 | loss: 21.3917217Losses:  19.102628707885742 9.159881591796875 -0.0
CurrentTrain: epoch  1, batch    10 | loss: 19.1026287Losses:  17.017547607421875 8.047792434692383 -0.0
CurrentTrain: epoch  1, batch    11 | loss: 17.0175476Losses:  17.834186553955078 8.265012741088867 -0.0
CurrentTrain: epoch  1, batch    12 | loss: 17.8341866Losses:  15.838813781738281 6.60552978515625 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 15.8388138Losses:  16.870262145996094 7.3856916427612305 -0.0
CurrentTrain: epoch  1, batch    14 | loss: 16.8702621Losses:  15.99441146850586 6.250601768493652 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 15.9944115Losses:  16.196544647216797 6.825838088989258 -0.0
CurrentTrain: epoch  1, batch    16 | loss: 16.1965446Losses:  16.561317443847656 7.473324775695801 -0.0
CurrentTrain: epoch  1, batch    17 | loss: 16.5613174Losses:  18.409221649169922 9.190507888793945 -0.0
CurrentTrain: epoch  1, batch    18 | loss: 18.4092216Losses:  16.96601104736328 7.7549357414245605 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 16.9660110Losses:  15.545312881469727 6.7437214851379395 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 15.5453129Losses:  15.681318283081055 6.406056880950928 -0.0
CurrentTrain: epoch  1, batch    21 | loss: 15.6813183Losses:  15.876997947692871 6.6514892578125 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 15.8769979Losses:  17.72010040283203 7.487390518188477 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 17.7201004Losses:  16.010923385620117 6.428117275238037 -0.0
CurrentTrain: epoch  1, batch    24 | loss: 16.0109234Losses:  17.591228485107422 8.18187427520752 -0.0
CurrentTrain: epoch  1, batch    25 | loss: 17.5912285Losses:  14.998628616333008 5.825481414794922 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 14.9986286Losses:  15.919425964355469 7.25648307800293 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 15.9194260Losses:  18.450023651123047 9.19163703918457 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 18.4500237Losses:  15.88088607788086 6.2360100746154785 -0.0
CurrentTrain: epoch  1, batch    29 | loss: 15.8808861Losses:  18.61367416381836 8.876260757446289 -0.0
CurrentTrain: epoch  1, batch    30 | loss: 18.6136742Losses:  16.3243408203125 6.998449325561523 -0.0
CurrentTrain: epoch  1, batch    31 | loss: 16.3243408Losses:  17.237918853759766 7.858042240142822 -0.0
CurrentTrain: epoch  1, batch    32 | loss: 17.2379189Losses:  13.984901428222656 5.075594902038574 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 13.9849014Losses:  15.130818367004395 6.6118974685668945 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 15.1308184Losses:  13.741973876953125 5.366979122161865 -0.0
CurrentTrain: epoch  1, batch    35 | loss: 13.7419739Losses:  15.812958717346191 7.209235191345215 -0.0
CurrentTrain: epoch  1, batch    36 | loss: 15.8129587Losses:  9.69404125213623 0.9484415054321289 -0.0
CurrentTrain: epoch  1, batch    37 | loss: 9.6940413Losses:  16.49230194091797 7.785030364990234 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 16.4923019Losses:  16.155717849731445 7.231383800506592 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 16.1557178Losses:  14.576286315917969 6.259244918823242 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 14.5762863Losses:  14.731169700622559 6.974987983703613 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 14.7311697Losses:  14.282196044921875 6.116351127624512 -0.0
CurrentTrain: epoch  2, batch     4 | loss: 14.2821960Losses:  12.708826065063477 5.319835186004639 -0.0
CurrentTrain: epoch  2, batch     5 | loss: 12.7088261Losses:  16.09471321105957 7.398731231689453 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 16.0947132Losses:  15.003894805908203 6.314485549926758 -0.0
CurrentTrain: epoch  2, batch     7 | loss: 15.0038948Losses:  15.22315788269043 6.472963333129883 -0.0
CurrentTrain: epoch  2, batch     8 | loss: 15.2231579Losses:  13.64213752746582 6.251189708709717 -0.0
CurrentTrain: epoch  2, batch     9 | loss: 13.6421375Losses:  14.024553298950195 6.087628364562988 -0.0
CurrentTrain: epoch  2, batch    10 | loss: 14.0245533Losses:  16.277950286865234 7.593963623046875 -0.0
CurrentTrain: epoch  2, batch    11 | loss: 16.2779503Losses:  16.483047485351562 7.525188446044922 -0.0
CurrentTrain: epoch  2, batch    12 | loss: 16.4830475Losses:  17.128787994384766 7.79750394821167 -0.0
CurrentTrain: epoch  2, batch    13 | loss: 17.1287880Losses:  12.55749225616455 4.579232215881348 -0.0
CurrentTrain: epoch  2, batch    14 | loss: 12.5574923Losses:  18.443368911743164 10.171272277832031 -0.0
CurrentTrain: epoch  2, batch    15 | loss: 18.4433689Losses:  14.566810607910156 6.21611213684082 -0.0
CurrentTrain: epoch  2, batch    16 | loss: 14.5668106Losses:  15.836462020874023 7.910394668579102 -0.0
CurrentTrain: epoch  2, batch    17 | loss: 15.8364620Losses:  14.351442337036133 6.074557781219482 -0.0
CurrentTrain: epoch  2, batch    18 | loss: 14.3514423Losses:  12.14763355255127 4.593553066253662 -0.0
CurrentTrain: epoch  2, batch    19 | loss: 12.1476336Losses:  17.065265655517578 9.844720840454102 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 17.0652657Losses:  14.936674118041992 7.2184882164001465 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 14.9366741Losses:  13.660467147827148 5.575479984283447 -0.0
CurrentTrain: epoch  2, batch    22 | loss: 13.6604671Losses:  19.750751495361328 10.170777320861816 -0.0
CurrentTrain: epoch  2, batch    23 | loss: 19.7507515Losses:  15.064403533935547 6.927279472351074 -0.0
CurrentTrain: epoch  2, batch    24 | loss: 15.0644035Losses:  17.223941802978516 7.044831275939941 -0.0
CurrentTrain: epoch  2, batch    25 | loss: 17.2239418Losses:  13.25589656829834 5.651932716369629 -0.0
CurrentTrain: epoch  2, batch    26 | loss: 13.2558966Losses:  14.427080154418945 7.414330005645752 -0.0
CurrentTrain: epoch  2, batch    27 | loss: 14.4270802Losses:  16.232128143310547 7.593857765197754 -0.0
CurrentTrain: epoch  2, batch    28 | loss: 16.2321281Losses:  15.594686508178711 6.99186372756958 -0.0
CurrentTrain: epoch  2, batch    29 | loss: 15.5946865Losses:  15.86337661743164 7.813873291015625 -0.0
CurrentTrain: epoch  2, batch    30 | loss: 15.8633766Losses:  19.21550750732422 10.55639362335205 -0.0
CurrentTrain: epoch  2, batch    31 | loss: 19.2155075Losses:  16.803970336914062 9.511016845703125 -0.0
CurrentTrain: epoch  2, batch    32 | loss: 16.8039703Losses:  15.014973640441895 7.353397369384766 -0.0
CurrentTrain: epoch  2, batch    33 | loss: 15.0149736Losses:  15.18697452545166 6.5390424728393555 -0.0
CurrentTrain: epoch  2, batch    34 | loss: 15.1869745Losses:  21.302040100097656 13.105966567993164 -0.0
CurrentTrain: epoch  2, batch    35 | loss: 21.3020401Losses:  14.038949012756348 6.216696739196777 -0.0
CurrentTrain: epoch  2, batch    36 | loss: 14.0389490Losses:  9.873740196228027 1.7111454010009766 -0.0
CurrentTrain: epoch  2, batch    37 | loss: 9.8737402Losses:  20.729232788085938 13.533147811889648 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 20.7292328Losses:  13.847789764404297 6.919135093688965 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 13.8477898Losses:  17.0927734375 9.141311645507812 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 17.0927734Losses:  13.247673034667969 5.39561653137207 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 13.2476730Losses:  15.22698974609375 7.815273284912109 -0.0
CurrentTrain: epoch  3, batch     4 | loss: 15.2269897Losses:  15.007296562194824 7.161250114440918 -0.0
CurrentTrain: epoch  3, batch     5 | loss: 15.0072966Losses:  13.5872163772583 5.521914482116699 -0.0
CurrentTrain: epoch  3, batch     6 | loss: 13.5872164Losses:  12.461957931518555 5.326900482177734 -0.0
CurrentTrain: epoch  3, batch     7 | loss: 12.4619579Losses:  13.889860153198242 6.2853546142578125 -0.0
CurrentTrain: epoch  3, batch     8 | loss: 13.8898602Losses:  13.772579193115234 6.187356472015381 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 13.7725792Losses:  12.374300003051758 5.1021647453308105 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 12.3743000Losses:  16.261594772338867 7.903028964996338 -0.0
CurrentTrain: epoch  3, batch    11 | loss: 16.2615948Losses:  13.157157897949219 4.653749465942383 -0.0
CurrentTrain: epoch  3, batch    12 | loss: 13.1571579Losses:  13.815421104431152 6.792021751403809 -0.0
CurrentTrain: epoch  3, batch    13 | loss: 13.8154211Losses:  16.564781188964844 7.828015327453613 -0.0
CurrentTrain: epoch  3, batch    14 | loss: 16.5647812Losses:  15.822940826416016 7.227144718170166 -0.0
CurrentTrain: epoch  3, batch    15 | loss: 15.8229408Losses:  15.421083450317383 6.080244064331055 -0.0
CurrentTrain: epoch  3, batch    16 | loss: 15.4210835Losses:  13.03979778289795 4.9033613204956055 -0.0
CurrentTrain: epoch  3, batch    17 | loss: 13.0397978Losses:  14.486858367919922 6.5918169021606445 -0.0
CurrentTrain: epoch  3, batch    18 | loss: 14.4868584Losses:  18.932815551757812 9.952720642089844 -0.0
CurrentTrain: epoch  3, batch    19 | loss: 18.9328156Losses:  12.922616958618164 5.881125450134277 -0.0
CurrentTrain: epoch  3, batch    20 | loss: 12.9226170Losses:  13.841776847839355 6.608712673187256 -0.0
CurrentTrain: epoch  3, batch    21 | loss: 13.8417768Losses:  13.21670150756836 6.09010124206543 -0.0
CurrentTrain: epoch  3, batch    22 | loss: 13.2167015Losses:  11.802902221679688 4.252161026000977 -0.0
CurrentTrain: epoch  3, batch    23 | loss: 11.8029022Losses:  13.712745666503906 6.199545383453369 -0.0
CurrentTrain: epoch  3, batch    24 | loss: 13.7127457Losses:  13.27398681640625 5.507627964019775 -0.0
CurrentTrain: epoch  3, batch    25 | loss: 13.2739868Losses:  15.772073745727539 7.60592794418335 -0.0
CurrentTrain: epoch  3, batch    26 | loss: 15.7720737Losses:  13.535125732421875 6.143898010253906 -0.0
CurrentTrain: epoch  3, batch    27 | loss: 13.5351257Losses:  13.34754467010498 7.060185432434082 -0.0
CurrentTrain: epoch  3, batch    28 | loss: 13.3475447Losses:  19.095813751220703 9.863443374633789 -0.0
CurrentTrain: epoch  3, batch    29 | loss: 19.0958138Losses:  14.575475692749023 7.63569450378418 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 14.5754757Losses:  13.625408172607422 6.079258918762207 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 13.6254082Losses:  11.921943664550781 4.252264976501465 -0.0
CurrentTrain: epoch  3, batch    32 | loss: 11.9219437Losses:  14.807953834533691 7.046767711639404 -0.0
CurrentTrain: epoch  3, batch    33 | loss: 14.8079538Losses:  14.419084548950195 6.034474849700928 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 14.4190845Losses:  13.09318733215332 5.679159164428711 -0.0
CurrentTrain: epoch  3, batch    35 | loss: 13.0931873Losses:  13.781060218811035 6.7068376541137695 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 13.7810602Losses:  9.363595962524414 2.153204917907715 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 9.3635960Losses:  11.531771659851074 4.526194095611572 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 11.5317717Losses:  12.32223892211914 5.738312721252441 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 12.3222389Losses:  13.139698028564453 5.907352447509766 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 13.1396980Losses:  11.584187507629395 4.66773796081543 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 11.5841875Losses:  14.905071258544922 6.751441955566406 -0.0
CurrentTrain: epoch  4, batch     4 | loss: 14.9050713Losses:  13.260790824890137 5.261425971984863 -0.0
CurrentTrain: epoch  4, batch     5 | loss: 13.2607908Losses:  17.722707748413086 10.157491683959961 -0.0
CurrentTrain: epoch  4, batch     6 | loss: 17.7227077Losses:  12.13991641998291 5.269317150115967 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 12.1399164Losses:  14.00631332397461 7.288119316101074 -0.0
CurrentTrain: epoch  4, batch     8 | loss: 14.0063133Losses:  14.44917106628418 7.390866756439209 -0.0
CurrentTrain: epoch  4, batch     9 | loss: 14.4491711Losses:  14.385574340820312 6.566064357757568 -0.0
CurrentTrain: epoch  4, batch    10 | loss: 14.3855743Losses:  13.245929718017578 6.615437984466553 -0.0
CurrentTrain: epoch  4, batch    11 | loss: 13.2459297Losses:  11.730596542358398 4.819808006286621 -0.0
CurrentTrain: epoch  4, batch    12 | loss: 11.7305965Losses:  15.79288387298584 8.352368354797363 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 15.7928839Losses:  17.315805435180664 11.564123153686523 -0.0
CurrentTrain: epoch  4, batch    14 | loss: 17.3158054Losses:  11.607746124267578 4.648702621459961 -0.0
CurrentTrain: epoch  4, batch    15 | loss: 11.6077461Losses:  12.660350799560547 4.927506923675537 -0.0
CurrentTrain: epoch  4, batch    16 | loss: 12.6603508Losses:  13.507412910461426 5.219451904296875 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 13.5074129Losses:  13.466875076293945 6.795596122741699 -0.0
CurrentTrain: epoch  4, batch    18 | loss: 13.4668751Losses:  15.444068908691406 7.641071796417236 -0.0
CurrentTrain: epoch  4, batch    19 | loss: 15.4440689Losses:  15.660964965820312 6.703325271606445 -0.0
CurrentTrain: epoch  4, batch    20 | loss: 15.6609650Losses:  12.989130973815918 5.812582015991211 -0.0
CurrentTrain: epoch  4, batch    21 | loss: 12.9891310Losses:  14.079107284545898 5.600149631500244 -0.0
CurrentTrain: epoch  4, batch    22 | loss: 14.0791073Losses:  14.230002403259277 6.266160488128662 -0.0
CurrentTrain: epoch  4, batch    23 | loss: 14.2300024Losses:  15.063243865966797 7.623513221740723 -0.0
CurrentTrain: epoch  4, batch    24 | loss: 15.0632439Losses:  13.11147689819336 5.663208961486816 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 13.1114769Losses:  12.567652702331543 4.83072566986084 -0.0
CurrentTrain: epoch  4, batch    26 | loss: 12.5676527Losses:  12.620649337768555 4.976961135864258 -0.0
CurrentTrain: epoch  4, batch    27 | loss: 12.6206493Losses:  13.326604843139648 5.713071823120117 -0.0
CurrentTrain: epoch  4, batch    28 | loss: 13.3266048Losses:  17.616506576538086 9.940362930297852 -0.0
CurrentTrain: epoch  4, batch    29 | loss: 17.6165066Losses:  15.114313125610352 7.373428821563721 -0.0
CurrentTrain: epoch  4, batch    30 | loss: 15.1143131Losses:  10.926685333251953 3.929291248321533 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 10.9266853Losses:  15.456718444824219 7.049013614654541 -0.0
CurrentTrain: epoch  4, batch    32 | loss: 15.4567184Losses:  13.274600982666016 6.627678871154785 -0.0
CurrentTrain: epoch  4, batch    33 | loss: 13.2746010Losses:  14.350220680236816 7.312358856201172 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 14.3502207Losses:  20.2048282623291 13.819938659667969 -0.0
CurrentTrain: epoch  4, batch    35 | loss: 20.2048283Losses:  22.64216423034668 13.790374755859375 -0.0
CurrentTrain: epoch  4, batch    36 | loss: 22.6421642Losses:  9.205700874328613 2.3356854915618896 -0.0
CurrentTrain: epoch  4, batch    37 | loss: 9.2057009Losses:  15.767704010009766 7.543480396270752 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 15.7677040Losses:  13.488571166992188 6.314667224884033 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 13.4885712Losses:  11.968709945678711 5.803075790405273 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 11.9687099Losses:  13.21807861328125 5.505019187927246 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 13.2180786Losses:  11.206780433654785 4.550714492797852 -0.0
CurrentTrain: epoch  5, batch     4 | loss: 11.2067804Losses:  16.380582809448242 8.21924114227295 -0.0
CurrentTrain: epoch  5, batch     5 | loss: 16.3805828Losses:  12.167287826538086 4.960837364196777 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 12.1672878Losses:  12.986263275146484 5.932281494140625 -0.0
CurrentTrain: epoch  5, batch     7 | loss: 12.9862633Losses:  13.413189888000488 6.379859924316406 -0.0
CurrentTrain: epoch  5, batch     8 | loss: 13.4131899Losses:  13.735441207885742 6.58092737197876 -0.0
CurrentTrain: epoch  5, batch     9 | loss: 13.7354412Losses:  12.751300811767578 6.393213272094727 -0.0
CurrentTrain: epoch  5, batch    10 | loss: 12.7513008Losses:  14.48847484588623 8.489053726196289 -0.0
CurrentTrain: epoch  5, batch    11 | loss: 14.4884748Losses:  12.100854873657227 5.0039873123168945 -0.0
CurrentTrain: epoch  5, batch    12 | loss: 12.1008549Losses:  12.338662147521973 5.732327938079834 -0.0
CurrentTrain: epoch  5, batch    13 | loss: 12.3386621Losses:  17.505306243896484 10.729562759399414 -0.0
CurrentTrain: epoch  5, batch    14 | loss: 17.5053062Losses:  11.391929626464844 5.84717321395874 -0.0
CurrentTrain: epoch  5, batch    15 | loss: 11.3919296Losses:  13.174718856811523 7.212439060211182 -0.0
CurrentTrain: epoch  5, batch    16 | loss: 13.1747189Losses:  12.078020095825195 6.1397600173950195 -0.0
CurrentTrain: epoch  5, batch    17 | loss: 12.0780201Losses:  11.057062149047852 4.500383377075195 -0.0
CurrentTrain: epoch  5, batch    18 | loss: 11.0570621Losses:  10.739097595214844 4.653772354125977 -0.0
CurrentTrain: epoch  5, batch    19 | loss: 10.7390976Losses:  9.918099403381348 3.8814849853515625 -0.0
CurrentTrain: epoch  5, batch    20 | loss: 9.9180994Losses:  11.750439643859863 5.0578932762146 -0.0
CurrentTrain: epoch  5, batch    21 | loss: 11.7504396Losses:  16.235734939575195 10.844017028808594 -0.0
CurrentTrain: epoch  5, batch    22 | loss: 16.2357349Losses:  12.410032272338867 6.284582138061523 -0.0
CurrentTrain: epoch  5, batch    23 | loss: 12.4100323Losses:  13.602147102355957 6.53713846206665 -0.0
CurrentTrain: epoch  5, batch    24 | loss: 13.6021471Losses:  11.577648162841797 5.4532976150512695 -0.0
CurrentTrain: epoch  5, batch    25 | loss: 11.5776482Losses:  11.585372924804688 5.1083831787109375 -0.0
CurrentTrain: epoch  5, batch    26 | loss: 11.5853729Losses:  14.846353530883789 9.834968566894531 -0.0
CurrentTrain: epoch  5, batch    27 | loss: 14.8463535Losses:  10.606328010559082 4.401020050048828 -0.0
CurrentTrain: epoch  5, batch    28 | loss: 10.6063280Losses:  13.70980453491211 6.355700969696045 -0.0
CurrentTrain: epoch  5, batch    29 | loss: 13.7098045Losses:  12.572912216186523 5.684028625488281 -0.0
CurrentTrain: epoch  5, batch    30 | loss: 12.5729122Losses:  15.369283676147461 8.584983825683594 -0.0
CurrentTrain: epoch  5, batch    31 | loss: 15.3692837Losses:  11.232704162597656 5.434068202972412 -0.0
CurrentTrain: epoch  5, batch    32 | loss: 11.2327042Losses:  10.63226318359375 4.854305267333984 -0.0
CurrentTrain: epoch  5, batch    33 | loss: 10.6322632Losses:  18.154869079589844 10.470239639282227 -0.0
CurrentTrain: epoch  5, batch    34 | loss: 18.1548691Losses:  12.22653579711914 5.802614212036133 -0.0
CurrentTrain: epoch  5, batch    35 | loss: 12.2265358Losses:  13.636497497558594 7.041121006011963 -0.0
CurrentTrain: epoch  5, batch    36 | loss: 13.6364975Losses:  8.114126205444336 1.9478726387023926 -0.0
CurrentTrain: epoch  5, batch    37 | loss: 8.1141262Losses:  15.10855484008789 7.908091068267822 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 15.1085548Losses:  13.958065032958984 7.070112705230713 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 13.9580650Losses:  11.651695251464844 5.037511348724365 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 11.6516953Losses:  12.299851417541504 6.13344144821167 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 12.2998514Losses:  12.698751449584961 7.504255771636963 -0.0
CurrentTrain: epoch  6, batch     4 | loss: 12.6987514Losses:  12.713255882263184 6.41443395614624 -0.0
CurrentTrain: epoch  6, batch     5 | loss: 12.7132559Losses:  11.179118156433105 5.451883316040039 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 11.1791182Losses:  13.111568450927734 6.271794319152832 -0.0
CurrentTrain: epoch  6, batch     7 | loss: 13.1115685Losses:  13.146186828613281 6.475719451904297 -0.0
CurrentTrain: epoch  6, batch     8 | loss: 13.1461868Losses:  14.301738739013672 7.554649353027344 -0.0
CurrentTrain: epoch  6, batch     9 | loss: 14.3017387Losses:  12.873052597045898 5.584524154663086 -0.0
CurrentTrain: epoch  6, batch    10 | loss: 12.8730526Losses:  16.980485916137695 9.37034797668457 -0.0
CurrentTrain: epoch  6, batch    11 | loss: 16.9804859Losses:  10.725299835205078 4.580480575561523 -0.0
CurrentTrain: epoch  6, batch    12 | loss: 10.7252998Losses:  12.611455917358398 6.480753421783447 -0.0
CurrentTrain: epoch  6, batch    13 | loss: 12.6114559Losses:  12.94833755493164 7.199357986450195 -0.0
CurrentTrain: epoch  6, batch    14 | loss: 12.9483376Losses:  11.49758529663086 4.810030937194824 -0.0
CurrentTrain: epoch  6, batch    15 | loss: 11.4975853Losses:  11.619516372680664 5.541603088378906 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 11.6195164Losses:  11.624930381774902 5.365988731384277 -0.0
CurrentTrain: epoch  6, batch    17 | loss: 11.6249304Losses:  16.392532348632812 9.735109329223633 -0.0
CurrentTrain: epoch  6, batch    18 | loss: 16.3925323Losses:  11.86894702911377 6.303009986877441 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 11.8689470Losses:  10.579824447631836 3.9376306533813477 -0.0
CurrentTrain: epoch  6, batch    20 | loss: 10.5798244Losses:  11.586613655090332 4.687041282653809 -0.0
CurrentTrain: epoch  6, batch    21 | loss: 11.5866137Losses:  13.807022094726562 7.799282550811768 -0.0
CurrentTrain: epoch  6, batch    22 | loss: 13.8070221Losses:  10.803462982177734 4.033307075500488 -0.0
CurrentTrain: epoch  6, batch    23 | loss: 10.8034630Losses:  12.34193229675293 5.333422660827637 -0.0
CurrentTrain: epoch  6, batch    24 | loss: 12.3419323Losses:  10.269232749938965 3.761241912841797 -0.0
CurrentTrain: epoch  6, batch    25 | loss: 10.2692327Losses:  9.420450210571289 3.5914392471313477 -0.0
CurrentTrain: epoch  6, batch    26 | loss: 9.4204502Losses:  14.245262145996094 9.173704147338867 -0.0
CurrentTrain: epoch  6, batch    27 | loss: 14.2452621Losses:  15.295733451843262 9.240861892700195 -0.0
CurrentTrain: epoch  6, batch    28 | loss: 15.2957335Losses:  10.674171447753906 4.406294345855713 -0.0
CurrentTrain: epoch  6, batch    29 | loss: 10.6741714Losses:  12.530632019042969 6.462943077087402 -0.0
CurrentTrain: epoch  6, batch    30 | loss: 12.5306320Losses:  11.40011215209961 5.792816162109375 -0.0
CurrentTrain: epoch  6, batch    31 | loss: 11.4001122Losses:  15.172571182250977 7.748079299926758 -0.0
CurrentTrain: epoch  6, batch    32 | loss: 15.1725712Losses:  10.90048599243164 5.43486213684082 -0.0
CurrentTrain: epoch  6, batch    33 | loss: 10.9004860Losses:  13.28335189819336 8.053423881530762 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 13.2833519Losses:  11.698148727416992 5.101120948791504 -0.0
CurrentTrain: epoch  6, batch    35 | loss: 11.6981487Losses:  10.222569465637207 4.4532694816589355 -0.0
CurrentTrain: epoch  6, batch    36 | loss: 10.2225695Losses:  7.229541778564453 1.4007117748260498 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 7.2295418Losses:  9.91046142578125 4.316547870635986 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 9.9104614Losses:  14.743178367614746 8.216976165771484 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 14.7431784Losses:  12.204721450805664 6.547142505645752 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 12.2047215Losses:  9.900764465332031 4.71305513381958 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 9.9007645Losses:  11.984729766845703 6.711763381958008 -0.0
CurrentTrain: epoch  7, batch     4 | loss: 11.9847298Losses:  13.09919548034668 6.801833152770996 -0.0
CurrentTrain: epoch  7, batch     5 | loss: 13.0991955Losses:  10.696425437927246 5.227897644042969 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 10.6964254Losses:  15.752220153808594 9.473258972167969 -0.0
CurrentTrain: epoch  7, batch     7 | loss: 15.7522202Losses:  10.239498138427734 5.226696014404297 -0.0
CurrentTrain: epoch  7, batch     8 | loss: 10.2394981Losses:  11.267939567565918 5.577886581420898 -0.0
CurrentTrain: epoch  7, batch     9 | loss: 11.2679396Losses:  11.535174369812012 6.138992786407471 -0.0
CurrentTrain: epoch  7, batch    10 | loss: 11.5351744Losses:  10.209327697753906 4.359280586242676 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 10.2093277Losses:  12.642672538757324 7.1120829582214355 -0.0
CurrentTrain: epoch  7, batch    12 | loss: 12.6426725Losses:  17.201087951660156 11.126501083374023 -0.0
CurrentTrain: epoch  7, batch    13 | loss: 17.2010880Losses:  13.652088165283203 7.79958438873291 -0.0
CurrentTrain: epoch  7, batch    14 | loss: 13.6520882Losses:  13.680036544799805 7.102299690246582 -0.0
CurrentTrain: epoch  7, batch    15 | loss: 13.6800365Losses:  18.161579132080078 12.10146713256836 -0.0
CurrentTrain: epoch  7, batch    16 | loss: 18.1615791Losses:  10.53599739074707 4.539921283721924 -0.0
CurrentTrain: epoch  7, batch    17 | loss: 10.5359974Losses:  16.480501174926758 9.72572135925293 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 16.4805012Losses:  9.580852508544922 4.038876056671143 -0.0
CurrentTrain: epoch  7, batch    19 | loss: 9.5808525Losses:  10.416705131530762 5.299350738525391 -0.0
CurrentTrain: epoch  7, batch    20 | loss: 10.4167051Losses:  10.191858291625977 4.394415378570557 -0.0
CurrentTrain: epoch  7, batch    21 | loss: 10.1918583Losses:  13.968796730041504 8.188438415527344 -0.0
CurrentTrain: epoch  7, batch    22 | loss: 13.9687967Losses:  10.875375747680664 5.014096260070801 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 10.8753757Losses:  10.665873527526855 4.919229030609131 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 10.6658735Losses:  13.587093353271484 8.053960800170898 -0.0
CurrentTrain: epoch  7, batch    25 | loss: 13.5870934Losses:  10.136122703552246 4.748544692993164 -0.0
CurrentTrain: epoch  7, batch    26 | loss: 10.1361227Losses:  9.94913101196289 4.851370334625244 -0.0
CurrentTrain: epoch  7, batch    27 | loss: 9.9491310Losses:  12.273272514343262 6.496460914611816 -0.0
CurrentTrain: epoch  7, batch    28 | loss: 12.2732725Losses:  11.327108383178711 5.206204414367676 -0.0
CurrentTrain: epoch  7, batch    29 | loss: 11.3271084Losses:  14.205429077148438 9.252628326416016 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 14.2054291Losses:  10.955269813537598 5.917403221130371 -0.0
CurrentTrain: epoch  7, batch    31 | loss: 10.9552698Losses:  12.443111419677734 7.397788047790527 -0.0
CurrentTrain: epoch  7, batch    32 | loss: 12.4431114Losses:  10.892739295959473 4.963536262512207 -0.0
CurrentTrain: epoch  7, batch    33 | loss: 10.8927393Losses:  11.048720359802246 5.639594078063965 -0.0
CurrentTrain: epoch  7, batch    34 | loss: 11.0487204Losses:  9.112998008728027 4.257469177246094 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 9.1129980Losses:  11.747200012207031 6.412975311279297 -0.0
CurrentTrain: epoch  7, batch    36 | loss: 11.7472000Losses:  10.446589469909668 5.631844520568848 -0.0
CurrentTrain: epoch  7, batch    37 | loss: 10.4465895Losses:  11.932061195373535 6.577190399169922 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 11.9320612Losses:  12.21718692779541 6.812753677368164 -0.0
CurrentTrain: epoch  8, batch     1 | loss: 12.2171869Losses:  9.909393310546875 4.8411993980407715 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 9.9093933Losses:  9.61679458618164 4.754932403564453 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 9.6167946Losses:  8.783417701721191 3.9250264167785645 -0.0
CurrentTrain: epoch  8, batch     4 | loss: 8.7834177Losses:  9.705324172973633 4.755051612854004 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 9.7053242Losses:  9.942068099975586 4.926107406616211 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 9.9420681Losses:  12.42420768737793 7.049096584320068 -0.0
CurrentTrain: epoch  8, batch     7 | loss: 12.4242077Losses:  11.059179306030273 6.076546669006348 -0.0
CurrentTrain: epoch  8, batch     8 | loss: 11.0591793Losses:  11.739825248718262 6.588679313659668 -0.0
CurrentTrain: epoch  8, batch     9 | loss: 11.7398252Losses:  9.100128173828125 4.249406814575195 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 9.1001282Losses:  10.3717679977417 5.251678466796875 -0.0
CurrentTrain: epoch  8, batch    11 | loss: 10.3717680Losses:  9.988288879394531 5.142275810241699 -0.0
CurrentTrain: epoch  8, batch    12 | loss: 9.9882889Losses:  10.664835929870605 5.313302993774414 -0.0
CurrentTrain: epoch  8, batch    13 | loss: 10.6648359Losses:  10.578956604003906 5.694150447845459 -0.0
CurrentTrain: epoch  8, batch    14 | loss: 10.5789566Losses:  11.787952423095703 6.9433183670043945 -0.0
CurrentTrain: epoch  8, batch    15 | loss: 11.7879524Losses:  14.244810104370117 9.134161949157715 -0.0
CurrentTrain: epoch  8, batch    16 | loss: 14.2448101Losses:  8.972284317016602 4.152836799621582 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 8.9722843Losses:  12.303922653198242 7.445952415466309 -0.0
CurrentTrain: epoch  8, batch    18 | loss: 12.3039227Losses:  9.118587493896484 4.19224739074707 -0.0
CurrentTrain: epoch  8, batch    19 | loss: 9.1185875Losses:  11.131701469421387 5.0884013175964355 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 11.1317015Losses:  10.529585838317871 5.488646507263184 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 10.5295858Losses:  16.85439682006836 11.948991775512695 -0.0
CurrentTrain: epoch  8, batch    22 | loss: 16.8543968Losses:  9.887624740600586 4.789133071899414 -0.0
CurrentTrain: epoch  8, batch    23 | loss: 9.8876247Losses:  11.830446243286133 7.040637969970703 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 11.8304462Losses:  13.494178771972656 6.709071159362793 -0.0
CurrentTrain: epoch  8, batch    25 | loss: 13.4941788Losses:  10.904178619384766 5.907528400421143 -0.0
CurrentTrain: epoch  8, batch    26 | loss: 10.9041786Losses:  13.307733535766602 8.515913963317871 -0.0
CurrentTrain: epoch  8, batch    27 | loss: 13.3077335Losses:  11.350069046020508 4.959502220153809 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 11.3500690Losses:  12.669719696044922 6.264958381652832 -0.0
CurrentTrain: epoch  8, batch    29 | loss: 12.6697197Losses:  11.803804397583008 5.574501037597656 -0.0
CurrentTrain: epoch  8, batch    30 | loss: 11.8038044Losses:  10.822474479675293 5.0915069580078125 -0.0
CurrentTrain: epoch  8, batch    31 | loss: 10.8224745Losses:  14.565889358520508 9.36300277709961 -0.0
CurrentTrain: epoch  8, batch    32 | loss: 14.5658894Losses:  13.018173217773438 7.466711044311523 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 13.0181732Losses:  11.125825881958008 5.181862831115723 -0.0
CurrentTrain: epoch  8, batch    34 | loss: 11.1258259Losses:  10.32597541809082 4.574367046356201 -0.0
CurrentTrain: epoch  8, batch    35 | loss: 10.3259754Losses:  10.65231704711914 4.868684768676758 -0.0
CurrentTrain: epoch  8, batch    36 | loss: 10.6523170Losses:  6.510160446166992 0.4491078853607178 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 6.5101604Losses:  10.949125289916992 5.260993003845215 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 10.9491253Losses:  12.500141143798828 6.955043792724609 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 12.5001411Losses:  8.871177673339844 3.493039608001709 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 8.8711777Losses:  10.202461242675781 4.617406368255615 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 10.2024612Losses:  10.804189682006836 6.0224714279174805 -0.0
CurrentTrain: epoch  9, batch     4 | loss: 10.8041897Losses:  11.820568084716797 5.787575721740723 -0.0
CurrentTrain: epoch  9, batch     5 | loss: 11.8205681Losses:  11.693233489990234 6.105535507202148 -0.0
CurrentTrain: epoch  9, batch     6 | loss: 11.6932335Losses:  10.38235855102539 5.13069486618042 -0.0
CurrentTrain: epoch  9, batch     7 | loss: 10.3823586Losses:  9.521425247192383 4.604069709777832 -0.0
CurrentTrain: epoch  9, batch     8 | loss: 9.5214252Losses:  13.976300239562988 9.053112030029297 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 13.9763002Losses:  14.475404739379883 9.422910690307617 -0.0
CurrentTrain: epoch  9, batch    10 | loss: 14.4754047Losses:  9.8427734375 5.12686824798584 -0.0
CurrentTrain: epoch  9, batch    11 | loss: 9.8427734Losses:  11.971095085144043 7.308642387390137 -0.0
CurrentTrain: epoch  9, batch    12 | loss: 11.9710951Losses:  10.626119613647461 5.932905673980713 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 10.6261196Losses:  10.341838836669922 5.503488540649414 -0.0
CurrentTrain: epoch  9, batch    14 | loss: 10.3418388Losses:  9.935829162597656 4.637808322906494 -0.0
CurrentTrain: epoch  9, batch    15 | loss: 9.9358292Losses:  9.219959259033203 4.30615234375 -0.0
CurrentTrain: epoch  9, batch    16 | loss: 9.2199593Losses:  12.128021240234375 7.25252628326416 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 12.1280212Losses:  10.661949157714844 5.870163917541504 -0.0
CurrentTrain: epoch  9, batch    18 | loss: 10.6619492Losses:  9.086821556091309 4.208596229553223 -0.0
CurrentTrain: epoch  9, batch    19 | loss: 9.0868216Losses:  8.93145751953125 4.233701705932617 -0.0
CurrentTrain: epoch  9, batch    20 | loss: 8.9314575Losses:  10.931299209594727 5.796490669250488 -0.0
CurrentTrain: epoch  9, batch    21 | loss: 10.9312992Losses:  9.783679008483887 5.052772521972656 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 9.7836790Losses:  10.502592086791992 5.537727355957031 -0.0
CurrentTrain: epoch  9, batch    23 | loss: 10.5025921Losses:  9.350259780883789 4.656093120574951 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 9.3502598Losses:  12.603809356689453 7.339439392089844 -0.0
CurrentTrain: epoch  9, batch    25 | loss: 12.6038094Losses:  10.232831954956055 4.446953773498535 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 10.2328320Losses:  16.219980239868164 11.540693283081055 -0.0
CurrentTrain: epoch  9, batch    27 | loss: 16.2199802Losses:  10.405845642089844 4.9294023513793945 -0.0
CurrentTrain: epoch  9, batch    28 | loss: 10.4058456Losses:  9.89638900756836 4.969742298126221 -0.0
CurrentTrain: epoch  9, batch    29 | loss: 9.8963890Losses:  12.006528854370117 7.390728950500488 -0.0
CurrentTrain: epoch  9, batch    30 | loss: 12.0065289Losses:  11.69546127319336 6.6363525390625 -0.0
CurrentTrain: epoch  9, batch    31 | loss: 11.6954613Losses:  10.224985122680664 5.173795223236084 -0.0
CurrentTrain: epoch  9, batch    32 | loss: 10.2249851Losses:  11.949979782104492 7.082561016082764 -0.0
CurrentTrain: epoch  9, batch    33 | loss: 11.9499798Losses:  15.202414512634277 10.238822937011719 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 15.2024145Losses:  8.921957015991211 4.0765180587768555 -0.0
CurrentTrain: epoch  9, batch    35 | loss: 8.9219570Losses:  11.954930305480957 6.9713640213012695 -0.0
CurrentTrain: epoch  9, batch    36 | loss: 11.9549303Losses:  7.142067909240723 2.390644073486328 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 7.1420679
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, James Smith relocated to Australia to enjoy a more relaxed lifestyle by the beach.  
Head Entity: James Smith  
Tail Entity: Australia  

Relation: person countries of residence  
Context: Despite being born in Canada, Priya Sharma has spent most of her adult life in India, where she has established her career and family.  
Head Entity: Priya Sharma  
Tail Entity: India  

Relation: person countries of residence  
Context: After completing his studies in Germany, Ahmed Al-Farsi moved to Qatar to work in the oil industry, where he has lived for the past five years.  
Head Entity: Ahmed Al-Farsi  
Tail Entity: Qatar  

Relation: person countries of residence  
Context: Having spent a decade in Japan, Emily Chen has embraced the culture and now considers Tokyo her second home, although she was originally from Canada.  
Head Entity: Emily Chen  
Tail Entity: Japan  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` customer relationships are the backbone of banking , and banks are constantly striving to meet customer demands , '' said peter garuccio , spokesman for the american bankers association .
Head Entity: american bankers association
Tail Entity: peter garuccio
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
1. Relation: organization top members employees  
   Context: "As the CEO of the tech giant, she has been instrumental in driving innovation," said a company spokesperson about Jane Doe, who leads the organization.  
   Head Entity: tech giant  
   Tail Entity: Jane Doe  

2. Relation: organization top members employees  
   Context: "The board of directors is pleased to announce the appointment of John Smith as the new CFO," stated the press release from the financial services firm.  
   Head Entity: financial services firm  
   Tail Entity: John Smith  

3. Relation: organization top members employees  
   Context: "During the annual meeting, the president of the nonprofit organization highlighted the contributions of their dedicated volunteers," reported the local news outlet.  
   Head Entity: nonprofit organization  
   Tail Entity: president  

4. Relation: organization top members employees  
   Context: "In a recent interview, the founder of the startup discussed the importance of teamwork and praised his lead developer for her exceptional work," the article noted.  
   Head Entity: startup  
   Tail Entity: lead developer  

5. Relation: organization top members employees  
   Context: "The university's chancellor expressed gratitude towards the faculty for their commitment to student success during the commencement speech," the university's newsletter stated.  
   Head Entity: university  
   Tail Entity: chancellor  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: ipsco shares rose $ 1506 , or 12 percent , to close at $ 14596 on the new york stock exchange .
Head Entity: ipsco
Tail Entity: new york stock exchange
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The United Nations has been a key player in international diplomacy, with various countries participating in its initiatives.  
Head Entity: United Nations  
Tail Entity: countries  

Relation: organization member of  
Context: The National Football League has expanded its reach by including several new teams in its roster.  
Head Entity: National Football League  
Tail Entity: new teams  

Relation: organization member of  
Context: The European Union has been working closely with member states to address climate change issues.  
Head Entity: European Union  
Tail Entity: member states  

Relation: organization member of  
Context: The World Health Organization collaborates with various health agencies to combat global pandemics.  
Head Entity: World Health Organization  
Tail Entity: health agencies  

Relation: organization member of  
Context: The International Olympic Committee oversees the participation of numerous national teams in the Olympic Games.  
Head Entity: International Olympic Committee  
Tail Entity: national teams  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: urgent chadian pm dies in paris hospital chad 's prime minister pascal yoadimnadji has died from a cerebral hemorrhage in a paris hospital , the chadian ambassador said friday .
Head Entity: pascal yoadimnadji
Tail Entity: chadian
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned scientist albert einstein was born in ulm, germany, and later became a citizen of the united states.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author chimamanda ngozi adichie hails from enugu, nigeria, where she developed her passion for storytelling.  
Head Entity: chimamanda ngozi adichie  
Tail Entity: nigerian  

Relation: person origin  
Context: the legendary musician bob marley was born in nine mile, jamaica, and is celebrated worldwide for his contributions to reggae music.  
Head Entity: bob marley  
Tail Entity: jamaican  

Relation: person origin  
Context: the acclaimed filmmaker akira kurosawa was born in tokyo, japan, and is known for his influential works in cinema.  
Head Entity: akira kurosawa  
Tail Entity: japanese  

Relation: person origin  
Context: the famous physicist stephen hawking was born in oxford, england, and made groundbreaking contributions to theoretical physics.  
Head Entity: stephen hawking  
Tail Entity: british  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she leads the finance team."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the conference, Dr. Alan Smith, the lead researcher at BioHealth Labs, presented groundbreaking findings in cancer treatment."  
Head Entity: Dr. Alan Smith  
Tail Entity: lead researcher  

Relation: person title  
Context: "As the new Director of Marketing, James Lee is responsible for overseeing all promotional activities at Creative Solutions."  
Head Entity: James Lee  
Tail Entity: Director of Marketing  

Relation: person title  
Context: "In her role as Senior Editor at the National Review, Sarah Johnson curates content and manages a team of writers."  
Head Entity: Sarah Johnson  
Tail Entity: Senior Editor  

Relation: person title  
Context: "With over two decades of experience, Robert Chen has taken on the role of Chief Technology Officer at Future Tech Corp."  
Head Entity: Robert Chen  
Tail Entity: Chief Technology Officer  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: the credit crisis spread to the largest us bond insurer thursday , sending shares of mbia inc plunging and calling into question the safety of tens of billions of dollars of company and local government debt held by investors .
Head Entity: mbia
Tail Entity: us
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: After years of expansion, the tech giant Google announced plans to open a new office in Dublin, further solidifying its presence in Europe.  
Head Entity: Google  
Tail Entity: Ireland  

Relation: organization country of headquarters  
Context: The multinational corporation Toyota has been a leader in the automotive industry, with its headquarters located in the bustling city of Toyota City, Japan.  
Head Entity: Toyota  
Tail Entity: Japan  

Relation: organization country of headquarters  
Context: With its innovative approach to finance, the fintech company Revolut has gained significant traction, operating from its headquarters in London, United Kingdom.  
Head Entity: Revolut  
Tail Entity: United Kingdom  

Relation: organization country of headquarters  
Context: The pharmaceutical company Bayer, known for its contributions to healthcare, is headquartered in Leverkusen, Germany, where it conducts much of its research and development.  
Head Entity: Bayer  
Tail Entity: Germany  

Relation: organization country of headquarters  
Context: The global consulting firm McKinsey & Company has established its headquarters in New York City, USA, serving clients across various industries worldwide.  
Head Entity: McKinsey & Company  
Tail Entity: United States  
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.93%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 88.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.66%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 85.33%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 87.27%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 88.15%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 88.31%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.48%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.93%   
cur_acc:  ['0.8693']
his_acc:  ['0.8693']
Clustering into  4  clusters
Clusters:  [2 0 1 2 0 2 2 0 1 3 0]
Losses:  18.772960662841797 7.94570255279541 3.7098472118377686
CurrentTrain: epoch  0, batch     0 | loss: 18.7729607Losses:  12.235042572021484 1.99568772315979 3.8776936531066895
CurrentTrain: epoch  0, batch     1 | loss: 12.2350426Losses:  18.88644790649414 9.096960067749023 3.6202192306518555
CurrentTrain: epoch  1, batch     0 | loss: 18.8864479Losses:  8.728053092956543 2.457744836807251 1.4290132522583008
CurrentTrain: epoch  1, batch     1 | loss: 8.7280531Losses:  15.228028297424316 7.304749488830566 3.4897453784942627
CurrentTrain: epoch  2, batch     0 | loss: 15.2280283Losses:  12.087323188781738 3.5967533588409424 3.3867595195770264
CurrentTrain: epoch  2, batch     1 | loss: 12.0873232Losses:  13.970760345458984 6.305024147033691 3.421499729156494
CurrentTrain: epoch  3, batch     0 | loss: 13.9707603Losses:  10.591007232666016 2.286628007888794 3.3596951961517334
CurrentTrain: epoch  3, batch     1 | loss: 10.5910072Losses:  13.443719863891602 6.28997278213501 3.3972411155700684
CurrentTrain: epoch  4, batch     0 | loss: 13.4437199Losses:  10.831040382385254 2.636570930480957 3.428025007247925
CurrentTrain: epoch  4, batch     1 | loss: 10.8310404Losses:  16.612890243530273 8.636619567871094 3.461169719696045
CurrentTrain: epoch  5, batch     0 | loss: 16.6128902Losses:  6.970228672027588 2.6733648777008057 1.4087843894958496
CurrentTrain: epoch  5, batch     1 | loss: 6.9702287Losses:  12.86262035369873 6.027827262878418 3.385472297668457
CurrentTrain: epoch  6, batch     0 | loss: 12.8626204Losses:  11.092206954956055 3.105276346206665 3.3870739936828613
CurrentTrain: epoch  6, batch     1 | loss: 11.0922070Losses:  14.356649398803711 7.3556132316589355 3.425889492034912
CurrentTrain: epoch  7, batch     0 | loss: 14.3566494Losses:  8.281426429748535 2.3725779056549072 3.3411483764648438
CurrentTrain: epoch  7, batch     1 | loss: 8.2814264Losses:  12.395828247070312 6.25946044921875 3.3549304008483887
CurrentTrain: epoch  8, batch     0 | loss: 12.3958282Losses:  8.901166915893555 2.194218873977661 3.4369020462036133
CurrentTrain: epoch  8, batch     1 | loss: 8.9011669Losses:  11.339010238647461 5.644068717956543 3.3350043296813965
CurrentTrain: epoch  9, batch     0 | loss: 11.3390102Losses:  10.062817573547363 2.7622292041778564 3.3559679985046387
CurrentTrain: epoch  9, batch     1 | loss: 10.0628176
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she mentioned that despite living in the united states for over a decade, her roots are firmly planted in the vibrant landscapes of brazil where she was born.  
Head Entity: she  
Tail Entity: brazil  

Relation: person country of birth  
Context: the documentary highlighted the life of the famous artist, who was born in spain and later moved to france to pursue his career.  
Head Entity: the famous artist  
Tail Entity: spain  

Relation: person country of birth  
Context: as a child, emma often reminisced about her early years spent in the picturesque countryside of ireland, where she was born.  
Head Entity: emma  
Tail Entity: ireland  

Relation: person country of birth  
Context: the renowned scientist, who made groundbreaking discoveries in genetics, was originally from south africa, where he was born and raised.  
Head Entity: the renowned scientist  
Tail Entity: south africa  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit our official site at https://www.techinnovators.com for the latest updates.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For more information, check out the website of Green Earth Initiative at http://www.greenearth.org.  
Head Entity: Green Earth Initiative  
Tail Entity: http://www.greenearth.org  

Relation: organization website  
Context: The blog post can be found at https://www.culinarycreations.com/blog, which is maintained by Culinary Creations.  
Head Entity: Culinary Creations  
Tail Entity: https://www.culinarycreations.com/blog  

Relation: organization website  
Context: You can explore our services at http://www.fitnessworld.com, the official website of Fitness World.  
Head Entity: Fitness World  
Tail Entity: http://www.fitnessworld.com  

Relation: organization website  
Context: Learn more about our mission at https://www.educationforall.org, the website for Education for All.  
Head Entity: Education for All  
Tail Entity: https://www.educationforall.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant apple has seen significant investments from warren buffett's berkshire hathaway.  
Head Entity: apple  
Tail Entity: berkshire hathaway  

Relation: organization shareholders  
Context: the famous electric car manufacturer tesla has attracted funding from various investors, including fidelity investments.  
Head Entity: tesla  
Tail Entity: fidelity investments  

Relation: organization shareholders  
Context: the pharmaceutical company pfizer has received substantial backing from the investment firm blackrock.  
Head Entity: pfizer  
Tail Entity: blackrock  

Relation: organization shareholders  
Context: the popular social media platform facebook has been significantly funded by the venture capital firm sequoia capital.  
Head Entity: facebook  
Tail Entity: sequoia capital  

Relation: organization shareholders  
Context: the leading online retailer amazon has garnered investments from the multinational investment bank goldman sachs.  
Head Entity: amazon  
Tail Entity: goldman sachs  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: After years of financial difficulties, the local arts council announced its dissolution in January 2019, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: January 2019  

Relation: organization dissolved  
Context: The historic bookstore, Pages & Co., closed its doors for good in July 2021, marking the end of an era for the community.  
Head Entity: Pages & Co.  
Tail Entity: July 2021  

Relation: organization dissolved  
Context: Following a series of scandals, the charity organization, Helping Hands, was dissolved in February 2022, prompting an investigation into its finances.  
Head Entity: Helping Hands  
Tail Entity: February 2022  

Relation: organization dissolved  
Context: The environmental group, Green Future, announced its dissolution in October 2018 due to a lack of public interest and funding.  
Head Entity: Green Future  
Tail Entity: October 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. John Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. John Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the famous actress and philanthropist, Emily Johnson, to support underprivileged children.  
Head Entity: Hope for Tomorrow  
Tail Entity: Emily Johnson  

Relation: organization founded by  
Context: In the early 2000s, the tech startup, GreenTech Solutions, was founded by environmentalist and engineer, Mark Thompson, to develop sustainable energy solutions.  
Head Entity: GreenTech Solutions  
Tail Entity: Mark Thompson  

Relation: organization founded by  
Context: The historical society, Heritage Keepers, was established in 1998 by local historian, Sarah Williams, to preserve the town's rich cultural heritage.  
Head Entity: Heritage Keepers  
Tail Entity: Sarah Williams  

Relation: organization founded by  
Context: The innovative design firm, Creative Minds, was launched in 2015 by renowned architect, Lisa Chen, to revolutionize urban architecture.  
Head Entity: Creative Minds  
Tail Entity: Lisa Chen  
Losses:  12.883096694946289 3.152958393096924 3.409371852874756
MemoryTrain:  epoch  0, batch     0 | loss: 12.8830967Losses:  10.636744499206543 2.008040189743042 3.453425645828247
MemoryTrain:  epoch  0, batch     1 | loss: 10.6367445Losses:  14.066095352172852 2.7633349895477295 3.372685194015503
MemoryTrain:  epoch  0, batch     2 | loss: 14.0660954Losses:  14.556544303894043 3.5071170330047607 3.412856101989746
MemoryTrain:  epoch  0, batch     3 | loss: 14.5565443Losses:  6.4511799812316895 -0.0 -0.0
MemoryTrain:  epoch  0, batch     4 | loss: 6.4511800Losses:  11.712072372436523 3.477820873260498 1.4688875675201416
MemoryTrain:  epoch  1, batch     0 | loss: 11.7120724Losses:  11.500724792480469 2.75517201423645 3.3873419761657715
MemoryTrain:  epoch  1, batch     1 | loss: 11.5007248Losses:  12.538459777832031 2.8937034606933594 3.3688578605651855
MemoryTrain:  epoch  1, batch     2 | loss: 12.5384598Losses:  10.724430084228516 1.8395572900772095 3.3863582611083984
MemoryTrain:  epoch  1, batch     3 | loss: 10.7244301Losses:  2.2475790977478027 -0.0 -0.0
MemoryTrain:  epoch  1, batch     4 | loss: 2.2475791Losses:  10.457405090332031 2.0319530963897705 3.3452916145324707
MemoryTrain:  epoch  2, batch     0 | loss: 10.4574051Losses:  11.525140762329102 3.7124791145324707 3.363450527191162
MemoryTrain:  epoch  2, batch     1 | loss: 11.5251408Losses:  10.185113906860352 3.408018112182617 1.4087594747543335
MemoryTrain:  epoch  2, batch     2 | loss: 10.1851139Losses:  11.590645790100098 2.1724328994750977 3.3433713912963867
MemoryTrain:  epoch  2, batch     3 | loss: 11.5906458Losses:  5.20725154876709 -0.0 -0.0
MemoryTrain:  epoch  2, batch     4 | loss: 5.2072515Losses:  10.262317657470703 3.1140596866607666 1.39768648147583
MemoryTrain:  epoch  3, batch     0 | loss: 10.2623177Losses:  9.668142318725586 2.1555442810058594 3.350539445877075
MemoryTrain:  epoch  3, batch     1 | loss: 9.6681423Losses:  11.345675468444824 2.7761852741241455 3.339305877685547
MemoryTrain:  epoch  3, batch     2 | loss: 11.3456755Losses:  11.31032657623291 3.256317138671875 3.337644100189209
MemoryTrain:  epoch  3, batch     3 | loss: 11.3103266Losses:  1.931691288948059 -0.0 -0.0
MemoryTrain:  epoch  3, batch     4 | loss: 1.9316913Losses:  7.327534198760986 2.351710796356201 1.4085534811019897
MemoryTrain:  epoch  4, batch     0 | loss: 7.3275342Losses:  9.936760902404785 1.9577198028564453 3.332331895828247
MemoryTrain:  epoch  4, batch     1 | loss: 9.9367609Losses:  10.721927642822266 2.1746554374694824 3.3422722816467285
MemoryTrain:  epoch  4, batch     2 | loss: 10.7219276Losses:  9.597746849060059 1.8437811136245728 3.322685718536377
MemoryTrain:  epoch  4, batch     3 | loss: 9.5977468Losses:  9.765588760375977 -0.0 -0.0
MemoryTrain:  epoch  4, batch     4 | loss: 9.7655888Losses:  10.382699966430664 2.363726854324341 3.3382861614227295
MemoryTrain:  epoch  5, batch     0 | loss: 10.3827000Losses:  7.959753036499023 3.2312843799591064 1.399815559387207
MemoryTrain:  epoch  5, batch     1 | loss: 7.9597530Losses:  11.909991264343262 2.918267011642456 3.3384640216827393
MemoryTrain:  epoch  5, batch     2 | loss: 11.9099913Losses:  9.388387680053711 1.3904993534088135 3.320988655090332
MemoryTrain:  epoch  5, batch     3 | loss: 9.3883877Losses:  1.9912570714950562 -0.0 -0.0
MemoryTrain:  epoch  5, batch     4 | loss: 1.9912571Losses:  11.323890686035156 4.156296730041504 3.3127191066741943
MemoryTrain:  epoch  6, batch     0 | loss: 11.3238907Losses:  8.532830238342285 1.5474040508270264 3.320113182067871
MemoryTrain:  epoch  6, batch     1 | loss: 8.5328302Losses:  9.98617935180664 2.0142083168029785 3.3044610023498535
MemoryTrain:  epoch  6, batch     2 | loss: 9.9861794Losses:  10.507234573364258 2.29874849319458 3.3233416080474854
MemoryTrain:  epoch  6, batch     3 | loss: 10.5072346Losses:  3.906153917312622 -0.0 -0.0
MemoryTrain:  epoch  6, batch     4 | loss: 3.9061539Losses:  8.453346252441406 1.8277950286865234 3.316361665725708
MemoryTrain:  epoch  7, batch     0 | loss: 8.4533463Losses:  11.277237892150879 3.0603888034820557 3.313464879989624
MemoryTrain:  epoch  7, batch     1 | loss: 11.2772379Losses:  9.233572006225586 2.362611770629883 3.305661201477051
MemoryTrain:  epoch  7, batch     2 | loss: 9.2335720Losses:  9.372645378112793 2.37174129486084 3.3280229568481445
MemoryTrain:  epoch  7, batch     3 | loss: 9.3726454Losses:  3.3079628944396973 -0.0 -0.0
MemoryTrain:  epoch  7, batch     4 | loss: 3.3079629Losses:  12.118614196777344 3.884913206100464 3.335052490234375
MemoryTrain:  epoch  8, batch     0 | loss: 12.1186142Losses:  8.435349464416504 2.14526629447937 3.329864501953125
MemoryTrain:  epoch  8, batch     1 | loss: 8.4353495Losses:  11.449878692626953 4.444605827331543 3.3180289268493652
MemoryTrain:  epoch  8, batch     2 | loss: 11.4498787Losses:  8.985496520996094 2.281630277633667 3.331035852432251
MemoryTrain:  epoch  8, batch     3 | loss: 8.9854965Losses:  1.8983168601989746 -0.0 -0.0
MemoryTrain:  epoch  8, batch     4 | loss: 1.8983169Losses:  8.355385780334473 1.6975960731506348 3.3095436096191406
MemoryTrain:  epoch  9, batch     0 | loss: 8.3553858Losses:  8.957353591918945 2.1183948516845703 3.3488035202026367
MemoryTrain:  epoch  9, batch     1 | loss: 8.9573536Losses:  8.238945007324219 3.0417211055755615 1.4112434387207031
MemoryTrain:  epoch  9, batch     2 | loss: 8.2389450Losses:  8.517427444458008 1.372176170349121 3.3100743293762207
MemoryTrain:  epoch  9, batch     3 | loss: 8.5174274Losses:  2.8057034015655518 -0.0 -0.0
MemoryTrain:  epoch  9, batch     4 | loss: 2.8057034
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 57.81%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 51.44%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 49.55%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 50.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 51.17%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 52.21%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 52.78%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 52.96%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 53.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 61.46%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 64.42%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 68.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 70.51%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 70.64%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 71.79%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 71.53%   [EVAL] batch:   36 | acc: 37.50%,  total acc: 70.61%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 70.23%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 70.03%   [EVAL] batch:   39 | acc: 25.00%,  total acc: 68.91%   
cur_acc:  ['0.8693', '0.5781']
his_acc:  ['0.8693', '0.6891']
Clustering into  7  clusters
Clusters:  [1 0 3 1 0 1 1 6 5 2 0 2 4 1 3 0]
Losses:  17.338157653808594 6.931451320648193 5.789527416229248
CurrentTrain: epoch  0, batch     0 | loss: 17.3381577Losses:  13.985723495483398 2.421916961669922 5.830921649932861
CurrentTrain: epoch  0, batch     1 | loss: 13.9857235Losses:  15.419958114624023 8.633034706115723 3.388507604598999
CurrentTrain: epoch  1, batch     0 | loss: 15.4199581Losses:  16.372522354125977 6.985727787017822 3.413085699081421
CurrentTrain: epoch  1, batch     1 | loss: 16.3725224Losses:  15.772806167602539 6.347814559936523 5.694305419921875
CurrentTrain: epoch  2, batch     0 | loss: 15.7728062Losses:  10.426734924316406 1.495392084121704 5.700519561767578
CurrentTrain: epoch  2, batch     1 | loss: 10.4267349Losses:  15.661897659301758 6.562755584716797 5.683704376220703
CurrentTrain: epoch  3, batch     0 | loss: 15.6618977Losses:  10.736330032348633 2.180159330368042 5.659972667694092
CurrentTrain: epoch  3, batch     1 | loss: 10.7363300Losses:  15.403985977172852 6.509120941162109 5.615863800048828
CurrentTrain: epoch  4, batch     0 | loss: 15.4039860Losses:  10.43231201171875 1.824405312538147 5.638935565948486
CurrentTrain: epoch  4, batch     1 | loss: 10.4323120Losses:  14.15492057800293 5.726645469665527 5.617424964904785
CurrentTrain: epoch  5, batch     0 | loss: 14.1549206Losses:  10.731595993041992 2.1241908073425293 5.63210391998291
CurrentTrain: epoch  5, batch     1 | loss: 10.7315960Losses:  13.433380126953125 5.304260730743408 5.619287967681885
CurrentTrain: epoch  6, batch     0 | loss: 13.4333801Losses:  9.427345275878906 1.2875653505325317 5.587588787078857
CurrentTrain: epoch  6, batch     1 | loss: 9.4273453Losses:  13.900847434997559 5.807255744934082 5.609320640563965
CurrentTrain: epoch  7, batch     0 | loss: 13.9008474Losses:  9.59614372253418 1.8314435482025146 5.611544132232666
CurrentTrain: epoch  7, batch     1 | loss: 9.5961437Losses:  14.813475608825684 6.711921691894531 5.599806785583496
CurrentTrain: epoch  8, batch     0 | loss: 14.8134756Losses:  10.683563232421875 2.993312120437622 5.613469123840332
CurrentTrain: epoch  8, batch     1 | loss: 10.6835632Losses:  14.685659408569336 6.741194725036621 5.592763423919678
CurrentTrain: epoch  9, batch     0 | loss: 14.6856594Losses:  7.927635192871094 2.549372434616089 3.3368442058563232
CurrentTrain: epoch  9, batch     1 | loss: 7.9276352
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the company was founded as a hobby in 1979 by the husband and wife team of tim and nina zagat , a pair of lawyers who started recuiting friends to provide their own ratings of the food , decor and service of restaurants they frequented .
Head Entity: zagat
Tail Entity: 1979
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, the tech startup was established by a group of engineers who aimed to revolutionize the way people interact with technology.  
Head Entity: tech startup  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization was created in 2005 to support local artists and promote cultural events in the community.  
Head Entity: non-profit organization  
Tail Entity: 2005  

Relation: organization founded  
Context: After years of planning, the environmental group was officially launched in 2010 to address climate change issues.  
Head Entity: environmental group  
Tail Entity: 2010  

Relation: organization founded  
Context: The educational institution was founded in 1856, making it one of the oldest universities in the region.  
Head Entity: educational institution  
Tail Entity: 1856  

Relation: organization founded  
Context: The fashion label was established in 2015 by a renowned designer who wanted to create sustainable clothing options.  
Head Entity: fashion label  
Tail Entity: 2015  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, emily decided to throw a big party for her friends.  
Head Entity: emily  
Tail Entity: 30  

Relation: person age  
Context: the renowned scientist albert einstein was born in 1879 and lived for 76 years.  
Head Entity: albert einstein  
Tail Entity: 76  

Relation: person age  
Context: during the interview, the author revealed that she is currently 45 years old.  
Head Entity: the author  
Tail Entity: 45  

Relation: person age  
Context: my grandfather turned 80 last month, and we celebrated with a family gathering.  
Head Entity: my grandfather  
Tail Entity: 80  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: after spending his early years in new york, he moved to los angeles where he found success in the film industry.  
Head Entity: he  
Tail Entity: los angeles  

Relation: person city of birth  
Context: the famous author was born in a small town near boston, which greatly influenced her writing style.  
Head Entity: the famous author  
Tail Entity: boston  

Relation: person city of birth  
Context: during an interview, she revealed that she was born in a quaint village in the heart of italy.  
Head Entity: she  
Tail Entity: italy  

Relation: person city of birth  
Context: the scientist, known for his groundbreaking research, hails from a vibrant city in india.  
Head Entity: the scientist  
Tail Entity: india  

Relation: person city of birth  
Context: he often reminisces about his childhood in a bustling neighborhood of chicago, where he was born.  
Head Entity: he  
Tail Entity: chicago  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians, including members from the Boston Symphony Orchestra.  
Head Entity: Boston Symphony Orchestra  
Tail Entity: New York Philharmonic  

Relation: organization members  
Context: The National Football League has various teams, and the Dallas Cowboys are one of its most famous members, often competing against the New England Patriots.  
Head Entity: New England Patriots  
Tail Entity: Dallas Cowboys  

Relation: organization members  
Context: The United Nations consists of various specialized agencies, including the World Health Organization, which plays a crucial role in global health initiatives.  
Head Entity: World Health Organization  
Tail Entity: United Nations  

Relation: organization members  
Context: The International Olympic Committee oversees the Olympic Games and includes various national committees, such as the United States Olympic and Paralympic Committee.  
Head Entity: United States Olympic and Paralympic Committee  
Tail Entity: International Olympic Committee  

Relation: organization members  
Context: The European Union is a political and economic union of member states, including countries like Germany and France, which play significant roles in its governance.  
Head Entity: Germany  
Tail Entity: European Union  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how every member plays a vital role.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The imam led the prayers at the mosque, reminding the congregation of their duties as followers of Islam and the significance of their beliefs.  
Head Entity: imam  
Tail Entity: Islam  

Relation: person religion  
Context: As a devout follower, she often shared her experiences in the church, highlighting how her Christian faith guided her through difficult times.  
Head Entity: she  
Tail Entity: Christian  

Relation: person religion  
Context: The monk dedicated his life to Buddhism, practicing meditation and teaching others about the path to enlightenment.  
Head Entity: monk  
Tail Entity: Buddhism  

Relation: person religion  
Context: He often discussed the teachings of Hinduism, explaining how they shaped his worldview and influenced his daily life.  
Head Entity: he  
Tail Entity: Hinduism  
Losses:  12.708826065063477 1.9467427730560303 5.857662677764893
MemoryTrain:  epoch  0, batch     0 | loss: 12.7088261Losses:  16.18606185913086 1.8659462928771973 8.147943496704102
MemoryTrain:  epoch  0, batch     1 | loss: 16.1860619Losses:  15.397735595703125 1.5735840797424316 8.178123474121094
MemoryTrain:  epoch  0, batch     2 | loss: 15.3977356Losses:  16.863218307495117 1.559149980545044 8.327631950378418
MemoryTrain:  epoch  0, batch     3 | loss: 16.8632183Losses:  15.590799331665039 1.360661506652832 8.184858322143555
MemoryTrain:  epoch  0, batch     4 | loss: 15.5907993Losses:  16.1793212890625 1.4010567665100098 8.580272674560547
MemoryTrain:  epoch  0, batch     5 | loss: 16.1793213Losses:  16.10428237915039 1.3059158325195312 8.25769329071045
MemoryTrain:  epoch  1, batch     0 | loss: 16.1042824Losses:  18.745262145996094 3.4154586791992188 10.962202072143555
MemoryTrain:  epoch  1, batch     1 | loss: 18.7452621Losses:  15.206181526184082 2.5491943359375 8.381200790405273
MemoryTrain:  epoch  1, batch     2 | loss: 15.2061815Losses:  16.546611785888672 2.7434234619140625 8.319458961486816
MemoryTrain:  epoch  1, batch     3 | loss: 16.5466118Losses:  13.533994674682617 3.0264768600463867 5.606810092926025
MemoryTrain:  epoch  1, batch     4 | loss: 13.5339947Losses:  13.166740417480469 2.409773349761963 5.612508296966553
MemoryTrain:  epoch  1, batch     5 | loss: 13.1667404Losses:  12.179805755615234 1.9796866178512573 5.586513042449951
MemoryTrain:  epoch  2, batch     0 | loss: 12.1798058Losses:  14.884523391723633 2.118134021759033 8.28830337524414
MemoryTrain:  epoch  2, batch     1 | loss: 14.8845234Losses:  15.063895225524902 2.070969820022583 8.239689826965332
MemoryTrain:  epoch  2, batch     2 | loss: 15.0638952Losses:  13.079105377197266 0.7572158575057983 8.128250122070312
MemoryTrain:  epoch  2, batch     3 | loss: 13.0791054Losses:  9.352743148803711 1.7207738161087036 3.335599899291992
MemoryTrain:  epoch  2, batch     4 | loss: 9.3527431Losses:  13.531679153442383 1.2674305438995361 8.12061882019043
MemoryTrain:  epoch  2, batch     5 | loss: 13.5316792Losses:  12.641424179077148 2.2738118171691895 5.6426262855529785
MemoryTrain:  epoch  3, batch     0 | loss: 12.6414242Losses:  8.988359451293945 1.3952784538269043 3.3353843688964844
MemoryTrain:  epoch  3, batch     1 | loss: 8.9883595Losses:  10.976678848266602 1.9160447120666504 5.675118446350098
MemoryTrain:  epoch  3, batch     2 | loss: 10.9766788Losses:  17.369356155395508 2.637169361114502 10.863591194152832
MemoryTrain:  epoch  3, batch     3 | loss: 17.3693562Losses:  15.333833694458008 1.1724457740783691 10.84859561920166
MemoryTrain:  epoch  3, batch     4 | loss: 15.3338337Losses:  12.755087852478027 1.0927200317382812 8.121536254882812
MemoryTrain:  epoch  3, batch     5 | loss: 12.7550879Losses:  13.553789138793945 1.6564974784851074 8.222148895263672
MemoryTrain:  epoch  4, batch     0 | loss: 13.5537891Losses:  15.913042068481445 2.001943349838257 10.865527153015137
MemoryTrain:  epoch  4, batch     1 | loss: 15.9130421Losses:  8.848348617553711 2.037309169769287 3.325521469116211
MemoryTrain:  epoch  4, batch     2 | loss: 8.8483486Losses:  16.13235855102539 2.409052848815918 10.82089900970459
MemoryTrain:  epoch  4, batch     3 | loss: 16.1323586Losses:  11.237829208374023 2.4357635974884033 5.662500858306885
MemoryTrain:  epoch  4, batch     4 | loss: 11.2378292Losses:  15.109021186828613 1.114831805229187 10.815773963928223
MemoryTrain:  epoch  4, batch     5 | loss: 15.1090212Losses:  14.920106887817383 1.3970911502838135 10.88646411895752
MemoryTrain:  epoch  5, batch     0 | loss: 14.9201069Losses:  14.665336608886719 1.0051231384277344 10.832921981811523
MemoryTrain:  epoch  5, batch     1 | loss: 14.6653366Losses:  12.33764362335205 1.670227289199829 8.17654037475586
MemoryTrain:  epoch  5, batch     2 | loss: 12.3376436Losses:  9.716361999511719 1.0938366651535034 5.608208656311035
MemoryTrain:  epoch  5, batch     3 | loss: 9.7163620Losses:  14.539863586425781 2.6510589122772217 8.240036010742188
MemoryTrain:  epoch  5, batch     4 | loss: 14.5398636Losses:  10.935465812683105 2.478692054748535 5.600681304931641
MemoryTrain:  epoch  5, batch     5 | loss: 10.9354658Losses:  9.392403602600098 1.0921657085418701 5.583909034729004
MemoryTrain:  epoch  6, batch     0 | loss: 9.3924036Losses:  12.017452239990234 0.8731900453567505 8.157026290893555
MemoryTrain:  epoch  6, batch     1 | loss: 12.0174522Losses:  10.587672233581543 2.2841033935546875 5.611179351806641
MemoryTrain:  epoch  6, batch     2 | loss: 10.5876722Losses:  12.82617473602295 1.7699942588806152 8.153690338134766
MemoryTrain:  epoch  6, batch     3 | loss: 12.8261747Losses:  10.877628326416016 2.9092459678649902 5.646483898162842
MemoryTrain:  epoch  6, batch     4 | loss: 10.8776283Losses:  14.77342414855957 1.3492587804794312 10.792245864868164
MemoryTrain:  epoch  6, batch     5 | loss: 14.7734241Losses:  15.179874420166016 1.4574978351593018 10.881964683532715
MemoryTrain:  epoch  7, batch     0 | loss: 15.1798744Losses:  11.503255844116211 0.8455190062522888 8.091551780700684
MemoryTrain:  epoch  7, batch     1 | loss: 11.5032558Losses:  10.006513595581055 2.288322925567627 5.57850980758667
MemoryTrain:  epoch  7, batch     2 | loss: 10.0065136Losses:  13.046859741210938 2.5687813758850098 8.075410842895508
MemoryTrain:  epoch  7, batch     3 | loss: 13.0468597Losses:  13.13812255859375 2.3585705757141113 8.187023162841797
MemoryTrain:  epoch  7, batch     4 | loss: 13.1381226Losses:  12.581674575805664 1.639265775680542 8.115336418151855
MemoryTrain:  epoch  7, batch     5 | loss: 12.5816746Losses:  9.60822868347168 1.678888201713562 5.5750956535339355
MemoryTrain:  epoch  8, batch     0 | loss: 9.6082287Losses:  11.637941360473633 1.041481375694275 8.146360397338867
MemoryTrain:  epoch  8, batch     1 | loss: 11.6379414Losses:  15.06850814819336 2.087815284729004 10.823582649230957
MemoryTrain:  epoch  8, batch     2 | loss: 15.0685081Losses:  9.636879920959473 1.8648431301116943 5.610875129699707
MemoryTrain:  epoch  8, batch     3 | loss: 9.6368799Losses:  11.844456672668457 2.876605987548828 5.568526744842529
MemoryTrain:  epoch  8, batch     4 | loss: 11.8444567Losses:  10.033336639404297 2.0454773902893066 5.593440532684326
MemoryTrain:  epoch  8, batch     5 | loss: 10.0333366Losses:  10.804962158203125 2.7849273681640625 5.580852031707764
MemoryTrain:  epoch  9, batch     0 | loss: 10.8049622Losses:  10.663703918457031 2.870086431503296 5.583521366119385
MemoryTrain:  epoch  9, batch     1 | loss: 10.6637039Losses:  9.16200065612793 1.2471215724945068 5.636976718902588
MemoryTrain:  epoch  9, batch     2 | loss: 9.1620007Losses:  11.11231517791748 0.718536913394928 8.093498229980469
MemoryTrain:  epoch  9, batch     3 | loss: 11.1123152Losses:  14.874338150024414 1.8431153297424316 10.831942558288574
MemoryTrain:  epoch  9, batch     4 | loss: 14.8743382Losses:  15.519826889038086 2.0758395195007324 10.800149917602539
MemoryTrain:  epoch  9, batch     5 | loss: 15.5198269
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 95.83%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 90.34%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 81.70%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 58.04%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 49.31%   [EVAL] batch:    9 | acc: 12.50%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 42.61%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 41.15%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 38.94%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 37.95%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 40.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 41.41%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 43.38%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 44.44%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 45.39%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 47.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 49.70%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 51.99%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 54.08%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 55.99%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 57.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 61.83%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 63.15%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 64.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 65.32%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 66.48%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 67.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 68.58%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 69.39%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 69.84%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 74.07%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.61%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 73.98%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 73.88%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 73.77%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 73.56%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 73.11%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 71.99%   
cur_acc:  ['0.8693', '0.5781', '0.8170']
his_acc:  ['0.8693', '0.6891', '0.7199']
Clustering into  9  clusters
Clusters:  [1 0 3 1 0 1 1 4 5 2 0 2 6 1 3 0 1 8 1 0 7]
Losses:  17.918533325195312 8.332576751708984 3.389005661010742
CurrentTrain: epoch  0, batch     0 | loss: 17.9185333Losses:  11.687827110290527 2.601386308670044 3.3448429107666016
CurrentTrain: epoch  0, batch     1 | loss: 11.6878271Losses:  17.717937469482422 8.881584167480469 3.3484842777252197
CurrentTrain: epoch  1, batch     0 | loss: 17.7179375Losses:  11.28406810760498 3.4089229106903076 3.367009162902832
CurrentTrain: epoch  1, batch     1 | loss: 11.2840681Losses:  15.263174057006836 7.770760536193848 3.349605083465576
CurrentTrain: epoch  2, batch     0 | loss: 15.2631741Losses:  10.699246406555176 3.2653419971466064 3.3285577297210693
CurrentTrain: epoch  2, batch     1 | loss: 10.6992464Losses:  16.2321834564209 9.11371898651123 3.317391872406006
CurrentTrain: epoch  3, batch     0 | loss: 16.2321835Losses:  11.917755126953125 4.932131767272949 3.3491053581237793
CurrentTrain: epoch  3, batch     1 | loss: 11.9177551Losses:  13.705373764038086 6.982156753540039 3.3111114501953125
CurrentTrain: epoch  4, batch     0 | loss: 13.7053738Losses:  8.723384857177734 2.2530694007873535 3.3055152893066406
CurrentTrain: epoch  4, batch     1 | loss: 8.7233849Losses:  14.182697296142578 7.523360729217529 3.309751510620117
CurrentTrain: epoch  5, batch     0 | loss: 14.1826973Losses:  9.108933448791504 2.630981922149658 3.310659408569336
CurrentTrain: epoch  5, batch     1 | loss: 9.1089334Losses:  14.16420841217041 7.765328407287598 3.3162851333618164
CurrentTrain: epoch  6, batch     0 | loss: 14.1642084Losses:  9.322726249694824 3.477138042449951 3.297398805618286
CurrentTrain: epoch  6, batch     1 | loss: 9.3227262Losses:  13.005724906921387 6.875016212463379 3.3029892444610596
CurrentTrain: epoch  7, batch     0 | loss: 13.0057249Losses:  7.869777679443359 1.9815983772277832 3.3238277435302734
CurrentTrain: epoch  7, batch     1 | loss: 7.8697777Losses:  13.14725112915039 7.179566383361816 3.311399459838867
CurrentTrain: epoch  8, batch     0 | loss: 13.1472511Losses:  7.698005676269531 2.2173759937286377 3.3358471393585205
CurrentTrain: epoch  8, batch     1 | loss: 7.6980057Losses:  11.855062484741211 6.273344039916992 3.3156068325042725
CurrentTrain: epoch  9, batch     0 | loss: 11.8550625Losses:  7.3875555992126465 1.9046436548233032 3.310356616973877
CurrentTrain: epoch  9, batch     1 | loss: 7.3875556
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones found her new home in san francisco, where she works as a software engineer. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith spent a lot of time in boston, where he attended university and made lifelong friends. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  

Relation: person cities of residence  
Context: -lrb- following her promotion, emily davis relocated to seattle, excited about the opportunities in the tech industry. -rrb-  
Head Entity: emily davis  
Tail Entity: seattle  

Relation: person cities of residence  
Context: -lrb- after retiring, john doe decided to settle down in miami, enjoying the warm weather and vibrant culture. -rrb-  
Head Entity: john doe  
Tail Entity: miami  

Relation: person cities of residence  
Context: -lrb- having grown up in new york, alice brown returned to her roots after years of living in los angeles. -rrb-  
Head Entity: alice brown  
Tail Entity: new york  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, John enrolled at Stanford University to study computer science.  
Head Entity: John  
Tail Entity: Stanford University  

Relation: person schools attended  
Context: Maria attended the University of Oxford for her master's degree in literature, where she developed a passion for writing.  
Head Entity: Maria  
Tail Entity: University of Oxford  

Relation: person schools attended  
Context: Following his time in the military, David went to the Massachusetts Institute of Technology to study engineering.  
Head Entity: David  
Tail Entity: Massachusetts Institute of Technology  

Relation: person schools attended  
Context: Lisa completed her undergraduate studies at the University of California, Berkeley, before moving on to law school.  
Head Entity: Lisa  
Tail Entity: University of California, Berkeley  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: the famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  

Relation: person country of death  
Context: legendary musician prince was found dead in his home in minnesota, united states  
Head Entity: prince  
Tail Entity: united states  

Relation: person country of death  
Context: actress and humanitarian audrey hepburn passed away in switzerland, where she spent her final years  
Head Entity: audrey hepburn  
Tail Entity: switzerland  

Relation: person country of death  
Context: the influential civil rights leader martin luther king jr. was assassinated in memphis, tennessee, united states  
Head Entity: martin luther king jr.  
Tail Entity: united states  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by two stepdaughters , barbara a. hammond and brenda l. stevenson ; a stepson , michael a. taylor ; two grandchildren and one great-grandchild .
Head Entity: he
Tail Entity: brenda l. stevenson
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: she has three children, including her son, john, and her daughters, emily and sarah, who are both very talented.  
Head Entity: she  
Tail Entity: emily  

Relation: person children  
Context: after the divorce, he took custody of his two daughters, lily and rose, who are now thriving in school.  
Head Entity: he  
Tail Entity: rose  

Relation: person children  
Context: the famous author often mentioned his son, alex, in interviews, highlighting their close relationship.  
Head Entity: the famous author  
Tail Entity: alex  

Relation: person children  
Context: during the family reunion, she proudly introduced her children, including her youngest, max, who just graduated from high school.  
Head Entity: she  
Tail Entity: max  

Relation: person children  
Context: he often shares stories about his daughter, mia, who is an aspiring artist and has already held her first exhibition.  
Head Entity: he  
Tail Entity: mia  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: flowers always contended politics was behind the extortion investigation , but appeals courts ruled against him .
Head Entity: him
Tail Entity: extortion
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: The prosecutor announced that the former mayor was charged with corruption after a lengthy investigation.  
Head Entity: former mayor  
Tail Entity: corruption  

Relation: person charges  
Context: After the incident, the police confirmed that the suspect was charged with assault and battery.  
Head Entity: suspect  
Tail Entity: assault and battery  

Relation: person charges  
Context: The district attorney revealed that the celebrity was charged with DUI following the car accident.  
Head Entity: celebrity  
Tail Entity: DUI  

Relation: person charges  
Context: Following the investigation, the authorities stated that the accountant was charged with fraud.  
Head Entity: accountant  
Tail Entity: fraud  

Relation: person charges  
Context: The judge announced that the activist was charged with trespassing during the protest.  
Head Entity: activist  
Tail Entity: trespassing  
Losses:  18.003633499145508 0.8203742504119873 13.76440143585205
MemoryTrain:  epoch  0, batch     0 | loss: 18.0036335Losses:  13.400949478149414 1.088276982307434 8.164819717407227
MemoryTrain:  epoch  0, batch     1 | loss: 13.4009495Losses:  15.99254035949707 1.6614580154418945 10.851789474487305
MemoryTrain:  epoch  0, batch     2 | loss: 15.9925404Losses:  18.329341888427734 1.6090030670166016 13.717013359069824
MemoryTrain:  epoch  0, batch     3 | loss: 18.3293419Losses:  13.383752822875977 1.9698467254638672 8.076635360717773
MemoryTrain:  epoch  0, batch     4 | loss: 13.3837528Losses:  17.072017669677734 1.5633617639541626 10.865032196044922
MemoryTrain:  epoch  0, batch     5 | loss: 17.0720177Losses:  17.704612731933594 0.5075855255126953 13.697305679321289
MemoryTrain:  epoch  0, batch     6 | loss: 17.7046127Losses:  13.687224388122559 1.2164957523345947 8.103340148925781
MemoryTrain:  epoch  0, batch     7 | loss: 13.6872244Losses:  10.485628128051758 1.972735047340393 3.3093996047973633
MemoryTrain:  epoch  1, batch     0 | loss: 10.4856281Losses:  12.349630355834961 1.0192978382110596 8.15997314453125
MemoryTrain:  epoch  1, batch     1 | loss: 12.3496304Losses:  14.832083702087402 0.882000744342804 10.794285774230957
MemoryTrain:  epoch  1, batch     2 | loss: 14.8320837Losses:  14.818438529968262 1.5000836849212646 10.8115873336792
MemoryTrain:  epoch  1, batch     3 | loss: 14.8184385Losses:  12.560810089111328 1.0944504737854004 8.092693328857422
MemoryTrain:  epoch  1, batch     4 | loss: 12.5608101Losses:  14.711424827575684 0.5741742849349976 10.82496452331543
MemoryTrain:  epoch  1, batch     5 | loss: 14.7114248Losses:  19.899091720581055 1.0052995681762695 16.706960678100586
MemoryTrain:  epoch  1, batch     6 | loss: 19.8990917Losses:  17.017683029174805 -0.0 13.708948135375977
MemoryTrain:  epoch  1, batch     7 | loss: 17.0176830Losses:  12.031612396240234 1.0408070087432861 8.129138946533203
MemoryTrain:  epoch  2, batch     0 | loss: 12.0316124Losses:  12.526891708374023 0.9109663963317871 8.070894241333008
MemoryTrain:  epoch  2, batch     1 | loss: 12.5268917Losses:  12.665513038635254 1.9376373291015625 8.104604721069336
MemoryTrain:  epoch  2, batch     2 | loss: 12.6655130Losses:  12.869536399841309 1.7750468254089355 8.06845474243164
MemoryTrain:  epoch  2, batch     3 | loss: 12.8695364Losses:  11.342498779296875 0.2692735493183136 8.100619316101074
MemoryTrain:  epoch  2, batch     4 | loss: 11.3424988Losses:  11.550945281982422 1.0617282390594482 8.075841903686523
MemoryTrain:  epoch  2, batch     5 | loss: 11.5509453Losses:  12.44290828704834 1.6629514694213867 8.121316909790039
MemoryTrain:  epoch  2, batch     6 | loss: 12.4429083Losses:  14.359129905700684 0.8809139132499695 10.784152030944824
MemoryTrain:  epoch  2, batch     7 | loss: 14.3591299Losses:  11.837100982666016 1.032787561416626 8.108781814575195
MemoryTrain:  epoch  3, batch     0 | loss: 11.8371010Losses:  9.81121826171875 1.547687292098999 5.575479984283447
MemoryTrain:  epoch  3, batch     1 | loss: 9.8112183Losses:  12.513113021850586 1.7018005847930908 8.091590881347656
MemoryTrain:  epoch  3, batch     2 | loss: 12.5131130Losses:  17.50395965576172 1.3753738403320312 13.667831420898438
MemoryTrain:  epoch  3, batch     3 | loss: 17.5039597Losses:  10.408926010131836 1.8809744119644165 5.648976802825928
MemoryTrain:  epoch  3, batch     4 | loss: 10.4089260Losses:  16.863609313964844 0.8580151200294495 13.651538848876953
MemoryTrain:  epoch  3, batch     5 | loss: 16.8636093Losses:  11.057390213012695 0.7582629323005676 8.085108757019043
MemoryTrain:  epoch  3, batch     6 | loss: 11.0573902Losses:  11.14976692199707 0.3507290184497833 8.070941925048828
MemoryTrain:  epoch  3, batch     7 | loss: 11.1497669Losses:  9.658378601074219 1.3937710523605347 5.634739875793457
MemoryTrain:  epoch  4, batch     0 | loss: 9.6583786Losses:  9.716343879699707 1.7104835510253906 5.569332122802734
MemoryTrain:  epoch  4, batch     1 | loss: 9.7163439Losses:  13.580583572387695 0.7175801992416382 10.775864601135254
MemoryTrain:  epoch  4, batch     2 | loss: 13.5805836Losses:  17.322694778442383 1.3637288808822632 13.677291870117188
MemoryTrain:  epoch  4, batch     3 | loss: 17.3226948Losses:  13.789200782775879 0.5783759355545044 10.815567970275879
MemoryTrain:  epoch  4, batch     4 | loss: 13.7892008Losses:  9.468587875366211 1.6539673805236816 5.578031063079834
MemoryTrain:  epoch  4, batch     5 | loss: 9.4685879Losses:  12.467047691345215 1.9675191640853882 8.077890396118164
MemoryTrain:  epoch  4, batch     6 | loss: 12.4670477Losses:  13.11430549621582 0.28064870834350586 10.78348445892334
MemoryTrain:  epoch  4, batch     7 | loss: 13.1143055Losses:  11.445557594299316 1.028185486793518 8.060640335083008
MemoryTrain:  epoch  5, batch     0 | loss: 11.4455576Losses:  11.76059627532959 1.5410386323928833 8.072389602661133
MemoryTrain:  epoch  5, batch     1 | loss: 11.7605963Losses:  13.914058685302734 1.0023211240768433 10.78538990020752
MemoryTrain:  epoch  5, batch     2 | loss: 13.9140587Losses:  12.779796600341797 2.413573741912842 8.100374221801758
MemoryTrain:  epoch  5, batch     3 | loss: 12.7797966Losses:  12.626710891723633 2.4328460693359375 8.092734336853027
MemoryTrain:  epoch  5, batch     4 | loss: 12.6267109Losses:  14.332683563232422 1.0993385314941406 10.840391159057617
MemoryTrain:  epoch  5, batch     5 | loss: 14.3326836Losses:  9.439117431640625 1.5058554410934448 5.5707855224609375
MemoryTrain:  epoch  5, batch     6 | loss: 9.4391174Losses:  13.582914352416992 0.7867382168769836 10.873217582702637
MemoryTrain:  epoch  5, batch     7 | loss: 13.5829144Losses:  10.958390235900879 0.7533966898918152 8.105814933776855
MemoryTrain:  epoch  6, batch     0 | loss: 10.9583902Losses:  13.069859504699707 2.389504909515381 8.092116355895996
MemoryTrain:  epoch  6, batch     1 | loss: 13.0698595Losses:  14.478893280029297 1.504180908203125 10.787800788879395
MemoryTrain:  epoch  6, batch     2 | loss: 14.4788933Losses:  9.181499481201172 1.5075637102127075 5.561704635620117
MemoryTrain:  epoch  6, batch     3 | loss: 9.1814995Losses:  13.689554214477539 0.995400607585907 10.78243637084961
MemoryTrain:  epoch  6, batch     4 | loss: 13.6895542Losses:  16.510225296020508 0.7645412683486938 13.646102905273438
MemoryTrain:  epoch  6, batch     5 | loss: 16.5102253Losses:  16.088939666748047 0.49951881170272827 13.670938491821289
MemoryTrain:  epoch  6, batch     6 | loss: 16.0889397Losses:  12.30805492401123 2.0757806301116943 8.094249725341797
MemoryTrain:  epoch  6, batch     7 | loss: 12.3080549Losses:  11.333833694458008 1.0637884140014648 8.082221031188965
MemoryTrain:  epoch  7, batch     0 | loss: 11.3338337Losses:  9.112079620361328 1.355332851409912 5.587240695953369
MemoryTrain:  epoch  7, batch     1 | loss: 9.1120796Losses:  11.103759765625 1.0029230117797852 8.087015151977539
MemoryTrain:  epoch  7, batch     2 | loss: 11.1037598Losses:  12.633703231811523 2.45615291595459 8.079431533813477
MemoryTrain:  epoch  7, batch     3 | loss: 12.6337032Losses:  12.105011940002441 1.7589244842529297 8.07805061340332
MemoryTrain:  epoch  7, batch     4 | loss: 12.1050119Losses:  11.802532196044922 1.782888650894165 8.065793991088867
MemoryTrain:  epoch  7, batch     5 | loss: 11.8025322Losses:  16.392253875732422 0.7703021764755249 13.664104461669922
MemoryTrain:  epoch  7, batch     6 | loss: 16.3922539Losses:  11.60893726348877 1.4447757005691528 8.111808776855469
MemoryTrain:  epoch  7, batch     7 | loss: 11.6089373Losses:  16.593353271484375 1.0057733058929443 13.657819747924805
MemoryTrain:  epoch  8, batch     0 | loss: 16.5933533Losses:  14.437849044799805 1.6668095588684082 10.780350685119629
MemoryTrain:  epoch  8, batch     1 | loss: 14.4378490Losses:  13.676553726196289 0.7542436718940735 10.78235149383545
MemoryTrain:  epoch  8, batch     2 | loss: 13.6765537Losses:  10.360958099365234 2.660789728164673 5.570005893707275
MemoryTrain:  epoch  8, batch     3 | loss: 10.3609581Losses:  14.722824096679688 1.9095982313156128 10.778166770935059
MemoryTrain:  epoch  8, batch     4 | loss: 14.7228241Losses:  11.23044490814209 1.2045214176177979 8.057586669921875
MemoryTrain:  epoch  8, batch     5 | loss: 11.2304449Losses:  8.722208023071289 1.0557801723480225 5.563694953918457
MemoryTrain:  epoch  8, batch     6 | loss: 8.7222080Losses:  11.056215286254883 0.6519274711608887 8.07945728302002
MemoryTrain:  epoch  8, batch     7 | loss: 11.0562153Losses:  14.007952690124512 1.0191950798034668 10.772595405578613
MemoryTrain:  epoch  9, batch     0 | loss: 14.0079527Losses:  13.535551071166992 0.7422722578048706 10.77939510345459
MemoryTrain:  epoch  9, batch     1 | loss: 13.5355511Losses:  12.112863540649414 1.8858466148376465 8.07111644744873
MemoryTrain:  epoch  9, batch     2 | loss: 12.1128635Losses:  13.587586402893066 0.7808476686477661 10.758373260498047
MemoryTrain:  epoch  9, batch     3 | loss: 13.5875864Losses:  11.16572380065918 1.1312289237976074 8.074457168579102
MemoryTrain:  epoch  9, batch     4 | loss: 11.1657238Losses:  13.763042449951172 0.983614981174469 10.796077728271484
MemoryTrain:  epoch  9, batch     5 | loss: 13.7630424Losses:  16.14443588256836 0.4996199607849121 13.67292594909668
MemoryTrain:  epoch  9, batch     6 | loss: 16.1444359Losses:  10.567571640014648 0.5116788148880005 8.070056915283203
MemoryTrain:  epoch  9, batch     7 | loss: 10.5675716
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 87.13%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 83.68%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 67.71%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 55.00%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 51.04%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 47.60%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 45.98%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 47.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 48.44%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 50.69%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 51.32%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 52.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.10%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 58.97%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 64.35%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 69.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 70.89%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 71.01%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 70.44%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 70.72%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 70.35%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 70.88%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 71.43%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 72.09%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 73.91%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 74.47%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 74.23%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 74.12%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 74.02%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 73.80%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 73.11%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 73.03%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 72.95%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 73.10%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 72.81%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 72.84%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 73.09%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 73.33%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 73.77%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 73.29%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 74.91%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 75.28%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 76.00%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 75.97%   
cur_acc:  ['0.8693', '0.5781', '0.8170', '0.8368']
his_acc:  ['0.8693', '0.6891', '0.7199', '0.7597']
Clustering into  12  clusters
Clusters:  [ 0  1  7  0  1  0  0 11  6  2  1  2  5  0  7  3  0 10  0  1  8  9  3  0
  4  0]
Losses:  15.277981758117676 8.128904342651367 3.3654727935791016
CurrentTrain: epoch  0, batch     0 | loss: 15.2779818Losses:  8.690421104431152 3.69771671295166 1.3984079360961914
CurrentTrain: epoch  0, batch     1 | loss: 8.6904211Losses:  14.016559600830078 7.621798515319824 3.3469390869140625
CurrentTrain: epoch  1, batch     0 | loss: 14.0165596Losses:  7.314417362213135 3.4771597385406494 1.4140418767929077
CurrentTrain: epoch  1, batch     1 | loss: 7.3144174Losses:  12.452346801757812 6.787185192108154 3.360459804534912
CurrentTrain: epoch  2, batch     0 | loss: 12.4523468Losses:  8.746877670288086 3.1776909828186035 3.317279815673828
CurrentTrain: epoch  2, batch     1 | loss: 8.7468777Losses:  12.513266563415527 7.200784683227539 3.3170623779296875
CurrentTrain: epoch  3, batch     0 | loss: 12.5132666Losses:  8.65540885925293 3.2846837043762207 3.3530383110046387
CurrentTrain: epoch  3, batch     1 | loss: 8.6554089Losses:  11.16971206665039 5.84982442855835 3.3169851303100586
CurrentTrain: epoch  4, batch     0 | loss: 11.1697121Losses:  6.955522537231445 1.7240402698516846 3.332141399383545
CurrentTrain: epoch  4, batch     1 | loss: 6.9555225Losses:  11.895903587341309 6.686559677124023 3.331218719482422
CurrentTrain: epoch  5, batch     0 | loss: 11.8959036Losses:  5.771477699279785 2.5680737495422363 1.3985977172851562
CurrentTrain: epoch  5, batch     1 | loss: 5.7714777Losses:  11.249652862548828 6.089315414428711 3.3237578868865967
CurrentTrain: epoch  6, batch     0 | loss: 11.2496529Losses:  5.161467552185059 1.9435721635818481 1.395944595336914
CurrentTrain: epoch  6, batch     1 | loss: 5.1614676Losses:  11.432255744934082 6.315082550048828 3.323385238647461
CurrentTrain: epoch  7, batch     0 | loss: 11.4322557Losses:  8.132823944091797 3.0012364387512207 3.3101494312286377
CurrentTrain: epoch  7, batch     1 | loss: 8.1328239Losses:  12.082355499267578 7.0453667640686035 3.327552318572998
CurrentTrain: epoch  8, batch     0 | loss: 12.0823555Losses:  7.035937309265137 3.833799123764038 1.3949928283691406
CurrentTrain: epoch  8, batch     1 | loss: 7.0359373Losses:  10.434679985046387 5.399714946746826 3.318840980529785
CurrentTrain: epoch  9, batch     0 | loss: 10.4346800Losses:  6.623073577880859 1.626345157623291 3.315913438796997
CurrentTrain: epoch  9, batch     1 | loss: 6.6230736
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned artist, elena smith, tragically lost her life due to a car accident while returning from an exhibition.  
Head Entity: elena smith  
Tail Entity: car accident  

Relation: person cause of death  
Context: following a long struggle with heart disease, mr. thompson succumbed to his illness last night at the hospital.  
Head Entity: mr. thompson  
Tail Entity: heart disease  

Relation: person cause of death  
Context: the community mourned the loss of their beloved teacher, ms. jones, who died from complications related to diabetes.  
Head Entity: ms. jones  
Tail Entity: complications related to diabetes  

Relation: person cause of death  
Context: after a heroic fight against leukemia, the young boy, sammy, passed away, leaving his family heartbroken.  
Head Entity: sammy  
Tail Entity: leukemia  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote its agenda.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in political advocacy, representing the interests of the Muslim community in the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  

Relation: organization political religious affiliation  
Context: The National Council of Churches works to unite various Christian denominations in the U.S. to address social justice issues and promote interfaith dialogue.  
Head Entity: National Council of Churches  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Jewish Federation of North America plays a crucial role in supporting Jewish communities and advocating for their rights in political spheres.  
Head Entity: Jewish Federation of North America  
Tail Entity: Jewish  

Relation: organization political religious affiliation  
Context: The Hindu American Foundation has been instrumental in raising awareness about Hindu issues and fostering political engagement among the Hindu community in America.  
Head Entity: Hindu American Foundation  
Tail Entity: Hindu  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: andrew lebow , an oil trader with mf global in new york , said investors have been discouraged by lower-than-expected oil imports in china and the disappointing growth in the u.s. economy .
Head Entity: mf global
Tail Entity: new york
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been a major player in the technology industry.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation unilever has its main office in london, england, overseeing operations across various countries.  
Head Entity: unilever  
Tail Entity: england  

Relation: organization stateorprovince of headquarters  
Context: according to the latest reports, the financial services firm goldman sachs is headquartered in new york city, new york, which is a hub for finance.  
Head Entity: goldman sachs  
Tail Entity: new york  

Relation: organization stateorprovince of headquarters  
Context: the famous car manufacturer toyota has its headquarters in toyota city, aichi prefecture, japan, where it was originally founded.  
Head Entity: toyota  
Tail Entity: aichi prefecture  

Relation: organization stateorprovince of headquarters  
Context: the global consulting firm accenture is based in dublin, ireland, where it has established a significant presence in the business sector.  
Head Entity: accenture  
Tail Entity: ireland  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, and their close bond despite the challenges they faced growing up in the spotlight.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, uncle bob shared stories about his adventures with his niece, emily, who has always looked up to him as a role model.  
Head Entity: uncle bob  
Tail Entity: emily  

Relation: person other family  
Context: the documentary highlighted the relationship between singer taylor swift and her mother, and how they have supported each other through thick and thin.  
Head Entity: taylor swift  
Tail Entity: her mother  

Relation: person other family  
Context: at the wedding, the bride's father gave a heartfelt speech about his daughter, sarah, and how proud he is of her accomplishments.  
Head Entity: the bride's father  
Tail Entity: sarah  

Relation: person other family  
Context: in his memoir, actor will smith reflects on his relationship with his cousin, who inspired him to pursue a career in entertainment.  
Head Entity: will smith  
Tail Entity: his cousin  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  

Relation: person city of death  
Context: the famous physicist, albert einstein, died on april 18, 1955, in princeton, new jersey, where he had lived for many years while working at the institute for advanced study.  
Head Entity: albert einstein  
Tail Entity: princeton  

Relation: person city of death  
Context: on january 1, 2020, the beloved actor, kobe bryant, tragically lost his life in a helicopter crash in calabasas, california, shocking fans around the world.  
Head Entity: kobe bryant  
Tail Entity: calabasas  

Relation: person city of death  
Context: the legendary musician, freddie mercury, passed away on november 24, 1991, at his home in london, england, leaving a profound impact on the music industry.  
Head Entity: freddie mercury  
Tail Entity: london  
Losses:  17.324066162109375 1.9035148620605469 11.34945297241211
MemoryTrain:  epoch  0, batch     0 | loss: 17.3240662Losses:  15.190035820007324 1.916637659072876 8.429689407348633
MemoryTrain:  epoch  0, batch     1 | loss: 15.1900358Losses:  10.714642524719238 1.1014320850372314 5.609419822692871
MemoryTrain:  epoch  0, batch     2 | loss: 10.7146425Losses:  15.706488609313965 1.2560534477233887 10.829493522644043
MemoryTrain:  epoch  0, batch     3 | loss: 15.7064886Losses:  18.85074234008789 1.7161799669265747 13.661283493041992
MemoryTrain:  epoch  0, batch     4 | loss: 18.8507423Losses:  21.486427307128906 0.7135874629020691 16.74057960510254
MemoryTrain:  epoch  0, batch     5 | loss: 21.4864273Losses:  21.79901885986328 0.24203842878341675 16.89247703552246
MemoryTrain:  epoch  0, batch     6 | loss: 21.7990189Losses:  21.588024139404297 0.7298548221588135 16.809057235717773
MemoryTrain:  epoch  0, batch     7 | loss: 21.5880241Losses:  16.18492317199707 1.092321753501892 10.831157684326172
MemoryTrain:  epoch  0, batch     8 | loss: 16.1849232Losses:  14.848176956176758 0.25866442918777466 10.911362648010254
MemoryTrain:  epoch  0, batch     9 | loss: 14.8481770Losses:  16.159420013427734 0.8021924495697021 11.143404960632324
MemoryTrain:  epoch  1, batch     0 | loss: 16.1594200Losses:  13.680082321166992 0.8142217993736267 8.136241912841797
MemoryTrain:  epoch  1, batch     1 | loss: 13.6800823Losses:  11.917963027954102 2.1776821613311768 5.577397346496582
MemoryTrain:  epoch  1, batch     2 | loss: 11.9179630Losses:  24.26327133178711 0.5358048677444458 19.855802536010742
MemoryTrain:  epoch  1, batch     3 | loss: 24.2632713Losses:  16.57986831665039 0.467465341091156 13.683090209960938
MemoryTrain:  epoch  1, batch     4 | loss: 16.5798683Losses:  18.893081665039062 0.7933040857315063 13.724266052246094
MemoryTrain:  epoch  1, batch     5 | loss: 18.8930817Losses:  15.526060104370117 0.8074527382850647 10.866364479064941
MemoryTrain:  epoch  1, batch     6 | loss: 15.5260601Losses:  27.625125885009766 0.797598123550415 23.31461524963379
MemoryTrain:  epoch  1, batch     7 | loss: 27.6251259Losses:  24.541271209716797 0.6225185990333557 19.866361618041992
MemoryTrain:  epoch  1, batch     8 | loss: 24.5412712Losses:  11.508146286010742 0.5551099181175232 8.142420768737793
MemoryTrain:  epoch  1, batch     9 | loss: 11.5081463Losses:  12.79319953918457 1.30587637424469 8.097699165344238
MemoryTrain:  epoch  2, batch     0 | loss: 12.7931995Losses:  15.736124038696289 1.0830836296081543 10.94957447052002
MemoryTrain:  epoch  2, batch     1 | loss: 15.7361240Losses:  16.28933334350586 0.5223010778427124 11.086380958557129
MemoryTrain:  epoch  2, batch     2 | loss: 16.2893333Losses:  15.525177955627441 1.3224716186523438 10.809988975524902
MemoryTrain:  epoch  2, batch     3 | loss: 15.5251780Losses:  16.084091186523438 1.2079328298568726 10.881669044494629
MemoryTrain:  epoch  2, batch     4 | loss: 16.0840912Losses:  18.089399337768555 1.122741937637329 13.670025825500488
MemoryTrain:  epoch  2, batch     5 | loss: 18.0893993Losses:  18.103118896484375 0.9423753619194031 13.706830978393555
MemoryTrain:  epoch  2, batch     6 | loss: 18.1031189Losses:  17.361953735351562 0.2549270987510681 13.668746948242188
MemoryTrain:  epoch  2, batch     7 | loss: 17.3619537Losses:  20.583948135375977 0.800174355506897 16.741281509399414
MemoryTrain:  epoch  2, batch     8 | loss: 20.5839481Losses:  13.66274356842041 0.5227221846580505 10.807865142822266
MemoryTrain:  epoch  2, batch     9 | loss: 13.6627436Losses:  17.354408264160156 0.5350728034973145 13.662864685058594
MemoryTrain:  epoch  3, batch     0 | loss: 17.3544083Losses:  14.507678985595703 0.7645565271377563 10.815693855285645
MemoryTrain:  epoch  3, batch     1 | loss: 14.5076790Losses:  17.1953182220459 0.7263602018356323 13.756980895996094
MemoryTrain:  epoch  3, batch     2 | loss: 17.1953182Losses:  18.423381805419922 0.806494951248169 13.74981689453125
MemoryTrain:  epoch  3, batch     3 | loss: 18.4233818Losses:  17.981245040893555 0.6941987872123718 13.896108627319336
MemoryTrain:  epoch  3, batch     4 | loss: 17.9812450Losses:  12.485916137695312 0.7294318675994873 8.37325382232666
MemoryTrain:  epoch  3, batch     5 | loss: 12.4859161Losses:  18.01644515991211 1.139955759048462 13.679691314697266
MemoryTrain:  epoch  3, batch     6 | loss: 18.0164452Losses:  20.72750473022461 0.7660832405090332 16.78474235534668
MemoryTrain:  epoch  3, batch     7 | loss: 20.7275047Losses:  19.77149200439453 0.5203054547309875 16.67813491821289
MemoryTrain:  epoch  3, batch     8 | loss: 19.7714920Losses:  14.743809700012207 0.6310625076293945 10.782047271728516
MemoryTrain:  epoch  3, batch     9 | loss: 14.7438097Losses:  14.601460456848145 0.7560024857521057 10.796067237854004
MemoryTrain:  epoch  4, batch     0 | loss: 14.6014605Losses:  16.06794548034668 1.167746901512146 11.15064525604248
MemoryTrain:  epoch  4, batch     1 | loss: 16.0679455Losses:  14.203264236450195 0.4795767068862915 10.939208030700684
MemoryTrain:  epoch  4, batch     2 | loss: 14.2032642Losses:  13.160822868347168 1.629647970199585 8.074237823486328
MemoryTrain:  epoch  4, batch     3 | loss: 13.1608229Losses:  22.14622688293457 0.24368733167648315 19.837268829345703
MemoryTrain:  epoch  4, batch     4 | loss: 22.1462269Losses:  19.400636672973633 0.7415598630905151 16.703540802001953
MemoryTrain:  epoch  4, batch     5 | loss: 19.4006367Losses:  17.713420867919922 1.330814003944397 13.654123306274414
MemoryTrain:  epoch  4, batch     6 | loss: 17.7134209Losses:  14.537027359008789 1.0787839889526367 10.828949928283691
MemoryTrain:  epoch  4, batch     7 | loss: 14.5370274Losses:  17.030908584594727 0.9930147528648376 13.733890533447266
MemoryTrain:  epoch  4, batch     8 | loss: 17.0309086Losses:  11.014450073242188 -0.0 8.129688262939453
MemoryTrain:  epoch  4, batch     9 | loss: 11.0144501Losses:  17.13106918334961 0.7684613466262817 13.700800895690918
MemoryTrain:  epoch  5, batch     0 | loss: 17.1310692Losses:  14.157697677612305 0.5423778295516968 10.773622512817383
MemoryTrain:  epoch  5, batch     1 | loss: 14.1576977Losses:  20.883869171142578 0.9046663045883179 16.738569259643555
MemoryTrain:  epoch  5, batch     2 | loss: 20.8838692Losses:  8.919672012329102 1.3778080940246582 5.558003902435303
MemoryTrain:  epoch  5, batch     3 | loss: 8.9196720Losses:  13.585490226745605 0.4788834750652313 10.78159236907959
MemoryTrain:  epoch  5, batch     4 | loss: 13.5854902Losses:  14.908220291137695 1.1391412019729614 10.7783784866333
MemoryTrain:  epoch  5, batch     5 | loss: 14.9082203Losses:  14.330875396728516 1.4815270900726318 10.809220314025879
MemoryTrain:  epoch  5, batch     6 | loss: 14.3308754Losses:  17.900924682617188 1.122266411781311 13.665830612182617
MemoryTrain:  epoch  5, batch     7 | loss: 17.9009247Losses:  16.58155059814453 0.5899028778076172 13.690393447875977
MemoryTrain:  epoch  5, batch     8 | loss: 16.5815506Losses:  11.014176368713379 0.26691168546676636 8.18185043334961
MemoryTrain:  epoch  5, batch     9 | loss: 11.0141764Losses:  20.080366134643555 0.5174259543418884 16.685007095336914
MemoryTrain:  epoch  6, batch     0 | loss: 20.0803661Losses:  19.73063850402832 0.9867309927940369 16.67490005493164
MemoryTrain:  epoch  6, batch     1 | loss: 19.7306385Losses:  16.16061019897461 0.22359010577201843 13.644516944885254
MemoryTrain:  epoch  6, batch     2 | loss: 16.1606102Losses:  16.967050552368164 0.7308425903320312 13.669837951660156
MemoryTrain:  epoch  6, batch     3 | loss: 16.9670506Losses:  14.06675910949707 0.7930755615234375 10.779362678527832
MemoryTrain:  epoch  6, batch     4 | loss: 14.0667591Losses:  19.604450225830078 0.7470171451568604 16.68868637084961
MemoryTrain:  epoch  6, batch     5 | loss: 19.6044502Losses:  13.131729125976562 1.617803692817688 8.103803634643555
MemoryTrain:  epoch  6, batch     6 | loss: 13.1317291Losses:  12.627169609069824 2.4918973445892334 8.074433326721191
MemoryTrain:  epoch  6, batch     7 | loss: 12.6271696Losses:  14.032809257507324 1.0134680271148682 10.77568244934082
MemoryTrain:  epoch  6, batch     8 | loss: 14.0328093Losses:  8.450610160827637 0.2779427170753479 5.556025981903076
MemoryTrain:  epoch  6, batch     9 | loss: 8.4506102Losses:  19.47295379638672 0.4858606457710266 16.68343734741211
MemoryTrain:  epoch  7, batch     0 | loss: 19.4729538Losses:  22.601882934570312 0.25835472345352173 19.844236373901367
MemoryTrain:  epoch  7, batch     1 | loss: 22.6018829Losses:  18.587881088256836 1.467245101928711 13.677902221679688
MemoryTrain:  epoch  7, batch     2 | loss: 18.5878811Losses:  16.551137924194336 0.7631590962409973 13.67441463470459
MemoryTrain:  epoch  7, batch     3 | loss: 16.5511379Losses:  16.60430145263672 0.5029243230819702 13.655771255493164
MemoryTrain:  epoch  7, batch     4 | loss: 16.6043015Losses:  11.725225448608398 0.8629127740859985 8.063642501831055
MemoryTrain:  epoch  7, batch     5 | loss: 11.7252254Losses:  12.259028434753418 2.176265001296997 8.090323448181152
MemoryTrain:  epoch  7, batch     6 | loss: 12.2590284Losses:  11.144062995910645 0.754277229309082 8.081026077270508
MemoryTrain:  epoch  7, batch     7 | loss: 11.1440630Losses:  14.198990821838379 0.49324190616607666 10.762434005737305
MemoryTrain:  epoch  7, batch     8 | loss: 14.1989908Losses:  10.903087615966797 0.8184143304824829 8.09419059753418
MemoryTrain:  epoch  7, batch     9 | loss: 10.9030876Losses:  20.30328369140625 1.1133036613464355 16.684139251708984
MemoryTrain:  epoch  8, batch     0 | loss: 20.3032837Losses:  14.082592964172363 0.7841188907623291 10.852262496948242
MemoryTrain:  epoch  8, batch     1 | loss: 14.0825930Losses:  14.414928436279297 1.0370246171951294 10.794587135314941
MemoryTrain:  epoch  8, batch     2 | loss: 14.4149284Losses:  19.93386459350586 0.947293221950531 16.693347930908203
MemoryTrain:  epoch  8, batch     3 | loss: 19.9338646Losses:  16.368980407714844 0.4798845052719116 13.641032218933105
MemoryTrain:  epoch  8, batch     4 | loss: 16.3689804Losses:  16.293506622314453 0.7271629571914673 13.671051025390625
MemoryTrain:  epoch  8, batch     5 | loss: 16.2935066Losses:  16.679058074951172 0.512613832950592 13.686290740966797
MemoryTrain:  epoch  8, batch     6 | loss: 16.6790581Losses:  13.713092803955078 1.0209720134735107 10.78235149383545
MemoryTrain:  epoch  8, batch     7 | loss: 13.7130928Losses:  13.81277084350586 0.7690617442131042 10.779521942138672
MemoryTrain:  epoch  8, batch     8 | loss: 13.8127708Losses:  13.695817947387695 0.8345664739608765 10.824015617370605
MemoryTrain:  epoch  8, batch     9 | loss: 13.6958179Losses:  13.335419654846191 0.5261586308479309 10.779787063598633
MemoryTrain:  epoch  9, batch     0 | loss: 13.3354197Losses:  16.75446319580078 1.036332130432129 13.675878524780273
MemoryTrain:  epoch  9, batch     1 | loss: 16.7544632Losses:  17.805889129638672 1.8200058937072754 13.653219223022461
MemoryTrain:  epoch  9, batch     2 | loss: 17.8058891Losses:  16.32924461364746 0.5217018127441406 13.681661605834961
MemoryTrain:  epoch  9, batch     3 | loss: 16.3292446Losses:  16.827106475830078 1.0059752464294434 13.713339805603027
MemoryTrain:  epoch  9, batch     4 | loss: 16.8271065Losses:  8.443570137023926 0.7277689576148987 5.557541847229004
MemoryTrain:  epoch  9, batch     5 | loss: 8.4435701Losses:  16.758756637573242 0.7680193781852722 13.652812957763672
MemoryTrain:  epoch  9, batch     6 | loss: 16.7587566Losses:  18.970396041870117 0.2336505651473999 16.708873748779297
MemoryTrain:  epoch  9, batch     7 | loss: 18.9703960Losses:  19.3271427154541 0.7123414874076843 16.67547035217285
MemoryTrain:  epoch  9, batch     8 | loss: 19.3271427Losses:  10.904593467712402 0.7976130247116089 8.069401741027832
MemoryTrain:  epoch  9, batch     9 | loss: 10.9045935
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 48.44%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 52.34%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 54.86%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 63.46%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 63.75%   [EVAL] batch:   10 | acc: 18.75%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 58.85%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 54.81%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 52.68%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 54.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 54.30%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 55.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 55.90%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.52%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.36%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 64.32%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 66.83%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 67.59%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 70.21%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 70.97%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 71.59%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 72.40%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 72.13%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 72.53%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 72.28%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 72.34%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 73.36%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 76.69%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 75.89%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 75.75%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 75.74%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 75.72%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 75.35%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 74.19%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 72.84%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 71.54%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 70.29%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 69.07%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 67.90%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 67.40%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 67.93%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 67.34%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 66.96%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 66.50%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 67.33%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 69.20%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 69.10%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 68.92%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 68.66%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 68.24%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 68.17%   [EVAL] batch:   75 | acc: 56.25%,  total acc: 68.01%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 67.78%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 67.63%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 67.80%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 67.81%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 68.13%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:   82 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 68.08%   
cur_acc:  ['0.8693', '0.5781', '0.8170', '0.8368', '0.6346']
his_acc:  ['0.8693', '0.6891', '0.7199', '0.7597', '0.6808']
Clustering into  14  clusters
Clusters:  [ 0  1  4  0  1 12  0 13  7  2  1  2  5  0  4  3  0  6  0  8  9 10  3 12
 11  0  5  0  8  1  0]
Losses:  16.65821647644043 7.647648811340332 3.3983445167541504
CurrentTrain: epoch  0, batch     0 | loss: 16.6582165Losses:  11.048643112182617 2.3973636627197266 3.312467336654663
CurrentTrain: epoch  0, batch     1 | loss: 11.0486431Losses:  17.212934494018555 9.4950532913208 3.3124423027038574
CurrentTrain: epoch  1, batch     0 | loss: 17.2129345Losses:  8.516334533691406 4.76801872253418 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 8.5163345Losses:  13.511733055114746 6.826274871826172 3.3459150791168213
CurrentTrain: epoch  2, batch     0 | loss: 13.5117331Losses:  8.083309173583984 2.0624632835388184 3.3083577156066895
CurrentTrain: epoch  2, batch     1 | loss: 8.0833092Losses:  12.96684455871582 6.588075637817383 3.336383819580078
CurrentTrain: epoch  3, batch     0 | loss: 12.9668446Losses:  7.602470397949219 1.9963726997375488 3.306764602661133
CurrentTrain: epoch  3, batch     1 | loss: 7.6024704Losses:  12.999765396118164 6.974069595336914 3.3095927238464355
CurrentTrain: epoch  4, batch     0 | loss: 12.9997654Losses:  5.846773147583008 2.259111166000366 1.39865243434906
CurrentTrain: epoch  4, batch     1 | loss: 5.8467731Losses:  12.039811134338379 6.322745323181152 3.3098983764648438
CurrentTrain: epoch  5, batch     0 | loss: 12.0398111Losses:  7.531887054443359 1.9686368703842163 3.34236478805542
CurrentTrain: epoch  5, batch     1 | loss: 7.5318871Losses:  11.043416976928711 5.587703704833984 3.3238165378570557
CurrentTrain: epoch  6, batch     0 | loss: 11.0434170Losses:  6.864544868469238 1.3610644340515137 3.311922550201416
CurrentTrain: epoch  6, batch     1 | loss: 6.8645449Losses:  11.380044937133789 5.9121623039245605 3.3057096004486084
CurrentTrain: epoch  7, batch     0 | loss: 11.3800449Losses:  7.2153096199035645 1.873487949371338 3.3193156719207764
CurrentTrain: epoch  7, batch     1 | loss: 7.2153096Losses:  12.283641815185547 6.867197036743164 3.3156628608703613
CurrentTrain: epoch  8, batch     0 | loss: 12.2836418Losses:  5.816826820373535 2.5749146938323975 1.4108407497406006
CurrentTrain: epoch  8, batch     1 | loss: 5.8168268Losses:  12.80335807800293 7.457991600036621 3.330575466156006
CurrentTrain: epoch  9, batch     0 | loss: 12.8033581Losses:  9.391401290893555 4.141523838043213 3.3768138885498047
CurrentTrain: epoch  9, batch     1 | loss: 9.3914013
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: born in 1985 in a small town in Texas, Sarah Johnson grew up with dreams of becoming a scientist.  
Head Entity: Sarah Johnson  
Tail Entity: 1985  

Relation: person date of birth  
Context: The famous actor, who was born on July 4, 1976, in Los Angeles, has captivated audiences for decades.  
Head Entity: The famous actor  
Tail Entity: July 4, 1976  

Relation: person date of birth  
Context: Albert Einstein, known for his contributions to physics, was born in Ulm, Germany, on March 14, 1879.  
Head Entity: Albert Einstein  
Tail Entity: March 14, 1879  

Relation: person date of birth  
Context: In 1990, a remarkable child was born in a small village in India, destined to become a renowned author.  
Head Entity: a remarkable child  
Tail Entity: 1990  

Relation: person date of birth  
Context: The legendary musician was born in Liverpool, England, on October 9, 1940, and changed the face of music forever.  
Head Entity: The legendary musician  
Tail Entity: October 9, 1940  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jefferson joseph deblanc sr. was born in lockport , la. , on feb. 15 , 1921 , and grew up in st. martinville .
Head Entity: jefferson joseph deblanc sr.
Tail Entity: la.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born in jersey city, nj, on august 3, 1941, and later moved to new york.  
Head Entity: martha stewart  
Tail Entity: nj  

Relation: person stateorprovince of birth  
Context: barack obama was born in honolulu, hi, on august 4, 1961, and spent his childhood in indonesia.  
Head Entity: barack obama  
Tail Entity: hi  

Relation: person stateorprovince of birth  
Context: taylor swift was born in reading, pa, on december 13, 1989, and later relocated to nashville, tennessee.  
Head Entity: taylor swift  
Tail Entity: pa  

Relation: person stateorprovince of birth  
Context: oprah winfrey was born in kosciusko, ms, on january 29, 1954, and grew up in milwaukee, wisconsin.  
Head Entity: oprah winfrey  
Tail Entity: ms  

Relation: person stateorprovince of birth  
Context: elon musk was born in pretoria, south africa, on june 28, 1971, and moved to the united states for his studies.  
Head Entity: elon musk  
Tail Entity: south africa  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: lynne spears told the court that lutfi had treated her daughter like a hostage in her own home , drugged her and took over her finances .
Head Entity: her
Tail Entity: lynne spears
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
1. Relation: person parents  
   Context: After years of hard work, Sarah finally made her parents proud by graduating at the top of her class.  
   Head Entity: her  
   Tail Entity: parents  

2. Relation: person parents  
   Context: During the family reunion, Michael shared stories about how his father taught him valuable life lessons.  
   Head Entity: his  
   Tail Entity: father  

3. Relation: person parents  
   Context: Emily often reminisces about the summer vacations she spent with her mother at the beach.  
   Head Entity: her  
   Tail Entity: mother  

4. Relation: person parents  
   Context: The documentary highlighted how the struggles of his parents shaped his career as an artist.  
   Head Entity: his  
   Tail Entity: parents  

5. Relation: person parents  
   Context: Jessica credits her success to the unwavering support she received from her father throughout her life.  
   Head Entity: her  
   Tail Entity: father  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson has finally landed a job at one of the top tech companies in Silicon Valley, where she will be contributing to innovative projects.  
Head Entity: Sarah Thompson  
Tail Entity: top tech company  

Relation: person employee of  
Context: John Smith, a talented graphic designer, has been working for Creative Solutions for over five years, helping to shape their brand identity.  
Head Entity: John Smith  
Tail Entity: Creative Solutions  

Relation: person employee of  
Context: The renowned chef, Marco Pierre White, has been the head chef at several prestigious restaurants, showcasing his culinary skills to the world.  
Head Entity: Marco Pierre White  
Tail Entity: prestigious restaurants  

Relation: person employee of  
Context: Emily Chen, a passionate environmentalist, has joined Green Earth Initiative, where she will be leading projects aimed at sustainability and conservation.  
Head Entity: Emily Chen  
Tail Entity: Green Earth Initiative  

Relation: person employee of  
Context: After completing her degree in journalism, Lisa Martinez secured a position at Global News Network, where she reports on current events and social issues.  
Head Entity: Lisa Martinez  
Tail Entity: Global News Network  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his thrilling novels, passed away peacefully in his sleep at his residence in los angeles, ca.  
Head Entity: john doe  
Tail Entity: ca.  

Relation: person stateorprovince of death  
Context: after a long battle with illness, sarah connor, the famous scientist, succumbed to her condition in a hospital in boston, ma.  
Head Entity: sarah connor  
Tail Entity: ma.  

Relation: person stateorprovince of death  
Context: the beloved actor, tom hanks, was reported to have died unexpectedly while on vacation in miami, fl.  
Head Entity: tom hanks  
Tail Entity: fl.  

Relation: person stateorprovince of death  
Context: in a tragic turn of events, the legendary musician, prince, was found dead in his home in minneapolis, mn.  
Head Entity: prince  
Tail Entity: mn.  

Relation: person stateorprovince of death  
Context: the influential politician, jane smith, passed away due to health complications in her hometown of austin, tx.  
Head Entity: jane smith  
Tail Entity: tx.  
Losses:  16.588951110839844 1.0689440965652466 11.061192512512207
MemoryTrain:  epoch  0, batch     0 | loss: 16.5889511Losses:  21.278724670410156 0.9943097829818726 16.725595474243164
MemoryTrain:  epoch  0, batch     1 | loss: 21.2787247Losses:  20.698143005371094 1.0242840051651 16.71114730834961
MemoryTrain:  epoch  0, batch     2 | loss: 20.6981430Losses:  20.979581832885742 0.5317359566688538 16.72672462463379
MemoryTrain:  epoch  0, batch     3 | loss: 20.9795818Losses:  17.493295669555664 0.8008865714073181 13.799212455749512
MemoryTrain:  epoch  0, batch     4 | loss: 17.4932957Losses:  23.12005043029785 0.7373732924461365 19.879667282104492
MemoryTrain:  epoch  0, batch     5 | loss: 23.1200504Losses:  18.456745147705078 0.8002270460128784 13.71971321105957
MemoryTrain:  epoch  0, batch     6 | loss: 18.4567451Losses:  21.040576934814453 0.8487188816070557 16.77803611755371
MemoryTrain:  epoch  0, batch     7 | loss: 21.0405769Losses:  15.053436279296875 0.8168621063232422 10.97498607635498
MemoryTrain:  epoch  0, batch     8 | loss: 15.0534363Losses:  21.81626319885254 1.4449888467788696 16.970726013183594
MemoryTrain:  epoch  0, batch     9 | loss: 21.8162632Losses:  20.16203498840332 2.0676345825195312 13.85772705078125
MemoryTrain:  epoch  0, batch    10 | loss: 20.1620350Losses:  17.83507537841797 0.2957102954387665 13.773004531860352
MemoryTrain:  epoch  0, batch    11 | loss: 17.8350754Losses:  16.903724670410156 0.7091503143310547 13.69314193725586
MemoryTrain:  epoch  1, batch     0 | loss: 16.9037247Losses:  21.036142349243164 1.9364057779312134 16.724164962768555
MemoryTrain:  epoch  1, batch     1 | loss: 21.0361423Losses:  16.159339904785156 3.046112060546875 8.215187072753906
MemoryTrain:  epoch  1, batch     2 | loss: 16.1593399Losses:  17.556140899658203 0.82418292760849 13.75117301940918
MemoryTrain:  epoch  1, batch     3 | loss: 17.5561409Losses:  23.72794532775879 0.8336150646209717 20.065059661865234
MemoryTrain:  epoch  1, batch     4 | loss: 23.7279453Losses:  12.337495803833008 0.7619322538375854 8.122627258300781
MemoryTrain:  epoch  1, batch     5 | loss: 12.3374958Losses:  20.02368927001953 0.7434154152870178 16.716489791870117
MemoryTrain:  epoch  1, batch     6 | loss: 20.0236893Losses:  19.902063369750977 0.24348098039627075 16.702417373657227
MemoryTrain:  epoch  1, batch     7 | loss: 19.9020634Losses:  19.978801727294922 0.7702682018280029 16.744945526123047
MemoryTrain:  epoch  1, batch     8 | loss: 19.9788017Losses:  10.964146614074707 0.24354249238967896 8.081011772155762
MemoryTrain:  epoch  1, batch     9 | loss: 10.9641466Losses:  20.3148250579834 0.5016003251075745 16.71993637084961
MemoryTrain:  epoch  1, batch    10 | loss: 20.3148251Losses:  16.268495559692383 0.2847498953342438 13.790494918823242
MemoryTrain:  epoch  1, batch    11 | loss: 16.2684956Losses:  19.808513641357422 0.5264014005661011 16.696481704711914
MemoryTrain:  epoch  2, batch     0 | loss: 19.8085136Losses:  19.76648712158203 0.7343704104423523 16.711854934692383
MemoryTrain:  epoch  2, batch     1 | loss: 19.7664871Losses:  20.444129943847656 0.7219088077545166 16.768068313598633
MemoryTrain:  epoch  2, batch     2 | loss: 20.4441299Losses:  19.700944900512695 0.29348450899124146 16.741573333740234
MemoryTrain:  epoch  2, batch     3 | loss: 19.7009449Losses:  23.097675323486328 0.3466162085533142 19.881378173828125
MemoryTrain:  epoch  2, batch     4 | loss: 23.0976753Losses:  20.44355010986328 0.5723307132720947 16.758543014526367
MemoryTrain:  epoch  2, batch     5 | loss: 20.4435501Losses:  16.903217315673828 0.508098840713501 13.739890098571777
MemoryTrain:  epoch  2, batch     6 | loss: 16.9032173Losses:  20.516454696655273 0.8457902669906616 16.74613380432129
MemoryTrain:  epoch  2, batch     7 | loss: 20.5164547Losses:  10.883953094482422 0.7566618323326111 8.073079109191895
MemoryTrain:  epoch  2, batch     8 | loss: 10.8839531Losses:  23.00494384765625 0.9930754899978638 19.828487396240234
MemoryTrain:  epoch  2, batch     9 | loss: 23.0049438Losses:  16.360084533691406 0.2680850028991699 13.692070960998535
MemoryTrain:  epoch  2, batch    10 | loss: 16.3600845Losses:  13.645493507385254 0.3110373616218567 10.846124649047852
MemoryTrain:  epoch  2, batch    11 | loss: 13.6454935Losses:  16.703330993652344 0.5418117046356201 13.697145462036133
MemoryTrain:  epoch  3, batch     0 | loss: 16.7033310Losses:  22.40319061279297 -0.0 19.84754753112793
MemoryTrain:  epoch  3, batch     1 | loss: 22.4031906Losses:  20.647586822509766 1.1482446193695068 16.737106323242188
MemoryTrain:  epoch  3, batch     2 | loss: 20.6475868Losses:  19.532499313354492 0.4984859228134155 16.69439125061035
MemoryTrain:  epoch  3, batch     3 | loss: 19.5324993Losses:  14.463830947875977 1.1766208410263062 10.852489471435547
MemoryTrain:  epoch  3, batch     4 | loss: 14.4638309Losses:  25.318208694458008 2.4465432167053223 19.852964401245117
MemoryTrain:  epoch  3, batch     5 | loss: 25.3182087Losses:  14.359838485717773 1.065516710281372 10.846149444580078
MemoryTrain:  epoch  3, batch     6 | loss: 14.3598385Losses:  19.97447967529297 1.038846492767334 16.71417236328125
MemoryTrain:  epoch  3, batch     7 | loss: 19.9744797Losses:  19.75105857849121 0.8654047846794128 16.711580276489258
MemoryTrain:  epoch  3, batch     8 | loss: 19.7510586Losses:  11.141534805297852 0.7793977856636047 8.116975784301758
MemoryTrain:  epoch  3, batch     9 | loss: 11.1415348Losses:  22.848739624023438 -0.0 19.87483787536621
MemoryTrain:  epoch  3, batch    10 | loss: 22.8487396Losses:  16.099040985107422 0.2669663429260254 13.693055152893066
MemoryTrain:  epoch  3, batch    11 | loss: 16.0990410Losses:  23.367109298706055 0.7741839289665222 19.852771759033203
MemoryTrain:  epoch  4, batch     0 | loss: 23.3671093Losses:  22.741647720336914 0.5192596912384033 19.819046020507812
MemoryTrain:  epoch  4, batch     1 | loss: 22.7416477Losses:  13.101038932800293 1.989739179611206 8.083560943603516
MemoryTrain:  epoch  4, batch     2 | loss: 13.1010389Losses:  22.958017349243164 0.995888352394104 19.85721206665039
MemoryTrain:  epoch  4, batch     3 | loss: 22.9580173Losses:  7.360118865966797 1.2196769714355469 3.329488754272461
MemoryTrain:  epoch  4, batch     4 | loss: 7.3601189Losses:  19.8339900970459 0.5073296427726746 16.7404727935791
MemoryTrain:  epoch  4, batch     5 | loss: 19.8339901Losses:  22.601329803466797 0.4952733814716339 19.819303512573242
MemoryTrain:  epoch  4, batch     6 | loss: 22.6013298Losses:  20.25314712524414 1.1521095037460327 16.73812484741211
MemoryTrain:  epoch  4, batch     7 | loss: 20.2531471Losses:  23.1679744720459 1.1475423574447632 19.854461669921875
MemoryTrain:  epoch  4, batch     8 | loss: 23.1679745Losses:  13.466089248657227 0.49041855335235596 10.797216415405273
MemoryTrain:  epoch  4, batch     9 | loss: 13.4660892Losses:  16.50742530822754 0.7167936563491821 13.706223487854004
MemoryTrain:  epoch  4, batch    10 | loss: 16.5074253Losses:  10.939826011657715 0.34569692611694336 8.07610034942627
MemoryTrain:  epoch  4, batch    11 | loss: 10.9398260Losses:  17.2523193359375 1.32427978515625 13.66873550415039
MemoryTrain:  epoch  5, batch     0 | loss: 17.2523193Losses:  19.738203048706055 0.8455911874771118 16.71497917175293
MemoryTrain:  epoch  5, batch     1 | loss: 19.7382030Losses:  13.732020378112793 0.776788592338562 10.81186580657959
MemoryTrain:  epoch  5, batch     2 | loss: 13.7320204Losses:  16.928661346435547 0.7625421285629272 13.673636436462402
MemoryTrain:  epoch  5, batch     3 | loss: 16.9286613Losses:  20.237091064453125 1.4071240425109863 16.69642448425293
MemoryTrain:  epoch  5, batch     4 | loss: 20.2370911Losses:  19.37811279296875 0.5007680654525757 16.673797607421875
MemoryTrain:  epoch  5, batch     5 | loss: 19.3781128Losses:  20.100269317626953 1.1088781356811523 16.74706268310547
MemoryTrain:  epoch  5, batch     6 | loss: 20.1002693Losses:  17.037342071533203 1.2448710203170776 13.651963233947754
MemoryTrain:  epoch  5, batch     7 | loss: 17.0373421Losses:  22.45264434814453 -0.0 19.85737419128418
MemoryTrain:  epoch  5, batch     8 | loss: 22.4526443Losses:  16.32372283935547 0.7447468042373657 13.647825241088867
MemoryTrain:  epoch  5, batch     9 | loss: 16.3237228Losses:  22.85158920288086 0.8104310631752014 19.85536003112793
MemoryTrain:  epoch  5, batch    10 | loss: 22.8515892Losses:  5.411920070648193 -0.0 3.321143627166748
MemoryTrain:  epoch  5, batch    11 | loss: 5.4119201Losses:  20.118778228759766 1.0335619449615479 16.69803237915039
MemoryTrain:  epoch  6, batch     0 | loss: 20.1187782Losses:  19.328704833984375 0.7149763107299805 16.669551849365234
MemoryTrain:  epoch  6, batch     1 | loss: 19.3287048Losses:  13.445297241210938 0.7395911812782288 10.797710418701172
MemoryTrain:  epoch  6, batch     2 | loss: 13.4452972Losses:  16.72042465209961 1.0053062438964844 13.699808120727539
MemoryTrain:  epoch  6, batch     3 | loss: 16.7204247Losses:  19.900192260742188 0.8595199584960938 16.673664093017578
MemoryTrain:  epoch  6, batch     4 | loss: 19.9001923Losses:  19.311885833740234 0.4901570677757263 16.684635162353516
MemoryTrain:  epoch  6, batch     5 | loss: 19.3118858Losses:  22.45392608642578 0.5117126703262329 19.83871841430664
MemoryTrain:  epoch  6, batch     6 | loss: 22.4539261Losses:  18.982051849365234 0.23527732491493225 16.66421127319336
MemoryTrain:  epoch  6, batch     7 | loss: 18.9820518Losses:  21.10453987121582 1.8027126789093018 16.69082260131836
MemoryTrain:  epoch  6, batch     8 | loss: 21.1045399Losses:  16.447032928466797 0.7406399846076965 13.678230285644531
MemoryTrain:  epoch  6, batch     9 | loss: 16.4470329Losses:  19.040807723999023 0.2527298331260681 16.708351135253906
MemoryTrain:  epoch  6, batch    10 | loss: 19.0408077Losses:  8.051925659179688 0.5408245325088501 5.577688694000244
MemoryTrain:  epoch  6, batch    11 | loss: 8.0519257Losses:  14.188165664672852 1.5205090045928955 10.787907600402832
MemoryTrain:  epoch  7, batch     0 | loss: 14.1881657Losses:  19.782291412353516 0.6354150772094727 16.71966552734375
MemoryTrain:  epoch  7, batch     1 | loss: 19.7822914Losses:  14.03300952911377 1.0512266159057617 10.794333457946777
MemoryTrain:  epoch  7, batch     2 | loss: 14.0330095Losses:  16.68707847595215 1.109555721282959 13.679527282714844
MemoryTrain:  epoch  7, batch     3 | loss: 16.6870785Losses:  22.612152099609375 0.7805714011192322 19.847158432006836
MemoryTrain:  epoch  7, batch     4 | loss: 22.6121521Losses:  12.153279304504395 2.0726709365844727 8.072903633117676
MemoryTrain:  epoch  7, batch     5 | loss: 12.1532793Losses:  25.55008888244629 0.4764149785041809 23.099821090698242
MemoryTrain:  epoch  7, batch     6 | loss: 25.5500889Losses:  16.194957733154297 0.5064107179641724 13.66640853881836
MemoryTrain:  epoch  7, batch     7 | loss: 16.1949577Losses:  16.958768844604492 1.3210902214050293 13.675841331481934
MemoryTrain:  epoch  7, batch     8 | loss: 16.9587688Losses:  22.72555923461914 0.829267680644989 19.831790924072266
MemoryTrain:  epoch  7, batch     9 | loss: 22.7255592Losses:  17.164480209350586 1.4432452917099 13.671841621398926
MemoryTrain:  epoch  7, batch    10 | loss: 17.1644802Losses:  10.07072925567627 -0.0 8.1578950881958
MemoryTrain:  epoch  7, batch    11 | loss: 10.0707293Losses:  22.677175521850586 0.7711735963821411 19.84858512878418
MemoryTrain:  epoch  8, batch     0 | loss: 22.6771755Losses:  25.89470863342285 0.5033723711967468 23.08925437927246
MemoryTrain:  epoch  8, batch     1 | loss: 25.8947086Losses:  13.398900985717773 0.700529158115387 10.77998161315918
MemoryTrain:  epoch  8, batch     2 | loss: 13.3989010Losses:  13.824824333190918 1.0397579669952393 10.798059463500977
MemoryTrain:  epoch  8, batch     3 | loss: 13.8248243Losses:  17.537609100341797 1.9767603874206543 13.663641929626465
MemoryTrain:  epoch  8, batch     4 | loss: 17.5376091Losses:  22.777544021606445 1.0443353652954102 19.828981399536133
MemoryTrain:  epoch  8, batch     5 | loss: 22.7775440Losses:  16.82345199584961 1.1004157066345215 13.680633544921875
MemoryTrain:  epoch  8, batch     6 | loss: 16.8234520Losses:  19.097881317138672 0.5097968578338623 16.676340103149414
MemoryTrain:  epoch  8, batch     7 | loss: 19.0978813Losses:  19.737302780151367 1.0093824863433838 16.70059585571289
MemoryTrain:  epoch  8, batch     8 | loss: 19.7373028Losses:  13.56017780303955 0.754351019859314 10.775809288024902
MemoryTrain:  epoch  8, batch     9 | loss: 13.5601778Losses:  13.689604759216309 0.9953013062477112 10.771953582763672
MemoryTrain:  epoch  8, batch    10 | loss: 13.6896048Losses:  10.313285827636719 0.26026615500450134 8.061872482299805
MemoryTrain:  epoch  8, batch    11 | loss: 10.3132858Losses:  25.01876449584961 -0.0 23.079927444458008
MemoryTrain:  epoch  9, batch     0 | loss: 25.0187645Losses:  16.565811157226562 0.9959120154380798 13.652851104736328
MemoryTrain:  epoch  9, batch     1 | loss: 16.5658112Losses:  13.519744873046875 0.7752905488014221 10.80406665802002
MemoryTrain:  epoch  9, batch     2 | loss: 13.5197449Losses:  22.424541473388672 0.7006889581680298 19.822023391723633
MemoryTrain:  epoch  9, batch     3 | loss: 22.4245415Losses:  16.538204193115234 0.9819462299346924 13.637347221374512
MemoryTrain:  epoch  9, batch     4 | loss: 16.5382042Losses:  19.184642791748047 0.4889492988586426 16.701021194458008
MemoryTrain:  epoch  9, batch     5 | loss: 19.1846428Losses:  19.408445358276367 0.7205261588096619 16.682857513427734
MemoryTrain:  epoch  9, batch     6 | loss: 19.4084454Losses:  20.47665023803711 1.6422061920166016 16.700544357299805
MemoryTrain:  epoch  9, batch     7 | loss: 20.4766502Losses:  18.874767303466797 0.24384240806102753 16.666515350341797
MemoryTrain:  epoch  9, batch     8 | loss: 18.8747673Losses:  13.256315231323242 0.5080114006996155 10.779339790344238
MemoryTrain:  epoch  9, batch     9 | loss: 13.2563152Losses:  21.99652671813965 0.2355038970708847 19.845718383789062
MemoryTrain:  epoch  9, batch    10 | loss: 21.9965267Losses:  7.721933364868164 0.2707768976688385 5.552186965942383
MemoryTrain:  epoch  9, batch    11 | loss: 7.7219334
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 6.25%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 26.04%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 46.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 50.57%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 55.29%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 54.91%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 25.00%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 59.03%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 25.00%,  total acc: 53.41%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 49.04%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 47.32%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 49.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 49.61%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 51.10%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 51.74%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 52.30%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 53.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.95%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.95%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 59.78%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 61.20%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 63.94%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 64.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 69.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 70.89%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 71.35%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 71.28%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 71.96%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 72.03%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 72.56%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 75.54%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 76.06%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 75.77%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 75.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 75.86%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 75.72%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 75.35%   [EVAL] batch:   53 | acc: 25.00%,  total acc: 74.42%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 73.07%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 71.88%   [EVAL] batch:   56 | acc: 6.25%,  total acc: 70.72%   [EVAL] batch:   57 | acc: 6.25%,  total acc: 69.61%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 68.54%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 68.02%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 68.14%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 67.34%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 66.67%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 66.02%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 66.15%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 67.16%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 67.65%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 68.57%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 68.40%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 68.23%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 67.98%   [EVAL] batch:   73 | acc: 37.50%,  total acc: 67.57%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 67.33%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 66.94%   [EVAL] batch:   76 | acc: 43.75%,  total acc: 66.64%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 66.11%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 65.98%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 65.86%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 65.90%   [EVAL] batch:   81 | acc: 100.00%,  total acc: 66.31%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 66.57%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 66.25%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 65.77%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 65.09%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 64.42%   [EVAL] batch:   88 | acc: 18.75%,  total acc: 63.90%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 63.54%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 63.80%   [EVAL] batch:   91 | acc: 93.75%,  total acc: 64.13%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 64.69%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 64.87%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 65.10%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 64.95%   
cur_acc:  ['0.8693', '0.5781', '0.8170', '0.8368', '0.6346', '0.5491']
his_acc:  ['0.8693', '0.6891', '0.7199', '0.7597', '0.6808', '0.6495']
Clustering into  17  clusters
Clusters:  [ 2  1  0 12 14  9  2 16  8  5  1  5  6  2  0  4  2 13  2  3 10 11  4  9
 15  2  6  2  3  1  2  0  0  7  9  3]
Losses:  19.609878540039062 7.664073467254639 3.748769760131836
CurrentTrain: epoch  0, batch     0 | loss: 19.6098785Losses:  15.825101852416992 2.71525502204895 3.689169406890869
CurrentTrain: epoch  0, batch     1 | loss: 15.8251019Losses:  19.295644760131836 8.024930953979492 3.5809314250946045
CurrentTrain: epoch  1, batch     0 | loss: 19.2956448Losses:  12.93448257446289 3.1266188621520996 1.482622504234314
CurrentTrain: epoch  1, batch     1 | loss: 12.9344826Losses:  19.79814910888672 8.240402221679688 3.7398080825805664
CurrentTrain: epoch  2, batch     0 | loss: 19.7981491Losses:  12.01988697052002 2.475358247756958 3.6002767086029053
CurrentTrain: epoch  2, batch     1 | loss: 12.0198870Losses:  18.833942413330078 8.676811218261719 3.6844701766967773
CurrentTrain: epoch  3, batch     0 | loss: 18.8339424Losses:  11.642565727233887 2.8576979637145996 3.481586217880249
CurrentTrain: epoch  3, batch     1 | loss: 11.6425657Losses:  17.586883544921875 7.976615905761719 3.5808582305908203
CurrentTrain: epoch  4, batch     0 | loss: 17.5868835Losses:  11.519262313842773 2.4642374515533447 3.5997471809387207
CurrentTrain: epoch  4, batch     1 | loss: 11.5192623Losses:  17.977664947509766 8.605510711669922 3.640712261199951
CurrentTrain: epoch  5, batch     0 | loss: 17.9776649Losses:  10.20499324798584 3.4179160594940186 1.4573715925216675
CurrentTrain: epoch  5, batch     1 | loss: 10.2049932Losses:  15.743906021118164 7.2811737060546875 3.5804219245910645
CurrentTrain: epoch  6, batch     0 | loss: 15.7439060Losses:  12.471640586853027 2.9655418395996094 3.559147596359253
CurrentTrain: epoch  6, batch     1 | loss: 12.4716406Losses:  17.22042465209961 10.269649505615234 3.4333391189575195
CurrentTrain: epoch  7, batch     0 | loss: 17.2204247Losses:  16.490896224975586 9.098503112792969 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 16.4908962Losses:  18.346923828125 9.228679656982422 3.617920398712158
CurrentTrain: epoch  8, batch     0 | loss: 18.3469238Losses:  9.960850715637207 3.2440738677978516 3.415090560913086
CurrentTrain: epoch  8, batch     1 | loss: 9.9608507Losses:  15.76435661315918 7.97428560256958 3.554053783416748
CurrentTrain: epoch  9, batch     0 | loss: 15.7643566Losses:  10.064301490783691 3.7529382705688477 1.508798599243164
CurrentTrain: epoch  9, batch     1 | loss: 10.0643015
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: `` firstgroup 's acquisition of laidlaw will considerably enhance firstgroup 's existing activities in north america , which themselves have grown strongly since we first invested in the u.s. in 1999 , '' said firstgroup chief executive moir lockhead .
Head Entity: firstgroup
Tail Entity: laidlaw
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: `` in 2018, the tech giant google announced its acquisition of nest labs, a company specializing in smart home products, which has allowed google to expand its reach in the home automation market. ''  
Head Entity: google  
Tail Entity: nest labs  

Relation: organization subsidiaries  
Context: `` the merger between disney and pixar in 2006 created a powerhouse in animated films, allowing disney to leverage pixar's innovative technology and storytelling expertise. ''  
Head Entity: disney  
Tail Entity: pixar  

Relation: organization subsidiaries  
Context: `` after acquiring the popular video game company, activision blizzard, microsoft significantly boosted its gaming portfolio and market presence in the entertainment industry. ''  
Head Entity: microsoft  
Tail Entity: activision blizzard  

Relation: organization subsidiaries  
Context: `` the recent purchase of whole foods by amazon has transformed the grocery shopping experience, integrating online and offline retail in a unique way. ''  
Head Entity: amazon  
Tail Entity: whole foods  

Relation: organization subsidiaries  
Context: `` in a strategic move to enhance its telecommunications services, at&t acquired time warner, allowing it to offer bundled services that include media and entertainment. ''  
Head Entity: at&t  
Tail Entity: time warner  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: The tech giant Alphabet Inc. has been making headlines with its innovative projects, but many are unaware that it is a subsidiary of the much larger Google LLC, which has been a leader in the tech industry for over two decades.  
Head Entity: Alphabet Inc.  
Tail Entity: Google LLC  

Relation: organization parents  
Context: In the world of finance, JPMorgan Chase & Co. has a long history, but it is important to note that it operates under the parent company, JPMorgan Chase Bank, which has been a key player in the banking sector since its inception.  
Head Entity: JPMorgan Chase & Co.  
Tail Entity: JPMorgan Chase Bank  

Relation: organization parents  
Context: The popular social media platform Instagram has gained immense popularity, but it is actually owned by Facebook, Inc., which has expanded its reach across various digital platforms.  
Head Entity: Instagram  
Tail Entity: Facebook, Inc.  

Relation: organization parents  
Context: The renowned car manufacturer Tesla, Inc. has revolutionized the automotive industry, yet it is a subsidiary of the larger entity, Tesla Motors, which was established to promote electric vehicles.  
Head Entity: Tesla, Inc.  
Tail Entity: Tesla Motors  

Relation: organization parents  
Context: The global beverage company Coca-Cola Company has a vast portfolio of drinks, but it is essential to recognize that it operates under the parent organization, The Coca-Cola Company, which has been a staple in the industry for over a century.  
Head Entity: Coca-Cola Company  
Tail Entity: The Coca-Cola Company  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: the talks between graham and pak ui-chun were held in a `` friendly '' manner , the official korean central news agency -lrb- kcna -rrb- said , without giving details .
Head Entity: kcna
Tail Entity: korean central news agency
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, has released its latest economic forecast.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has declared a public health emergency.  
Head Entity: WHO  
Tail Entity: World Health Organization  

Relation: organization alternate names  
Context: The National Aeronautics and Space Administration, abbreviated as NASA, is planning a new mission to Mars.  
Head Entity: NASA  
Tail Entity: National Aeronautics and Space Administration  

Relation: organization alternate names  
Context: The Federal Bureau of Investigation, or FBI, has launched an investigation into the matter.  
Head Entity: FBI  
Tail Entity: Federal Bureau of Investigation  

Relation: organization alternate names  
Context: The Central Intelligence Agency, often called the CIA, has been involved in various international operations.  
Head Entity: CIA  
Tail Entity: Central Intelligence Agency  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2021-03-15 10:00:00 utc tech giant google has announced plans to expand its headquarters in the heart of san francisco, aiming to create more job opportunities in the area.  
Head Entity: google  
Tail Entity: san francisco  

Relation: organization city of headquarters  
Context: ------ new york 2019-11-10 14:30:00 utc the financial services firm jp morgan chase has its main headquarters located in new york city, where it has been a key player in the finance industry for over a century.  
Head Entity: jp morgan chase  
Tail Entity: new york city  

Relation: organization city of headquarters  
Context: ------ seattle 2020-07-22 09:45:00 utc amazon has confirmed that its headquarters will remain in seattle, despite rumors of a potential move to another city, emphasizing its commitment to the local community.  
Head Entity: amazon  
Tail Entity: seattle  

Relation: organization city of headquarters  
Context: ------ boston 2022-01-05 16:20:00 utc the biotechnology company moderna has its headquarters in boston, where it conducts groundbreaking research in vaccine development.  
Head Entity: moderna  
Tail Entity: boston  

Relation: organization city of headquarters  
Context: ------ austin 2023-02-18 11:15:00 utc the tech startup indeed has established its headquarters in austin, texas, contributing to the city's growing reputation as a tech hub.  
Head Entity: indeed  
Tail Entity: austin  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: During the family reunion, John introduced his sister, Emily, who had just returned from studying abroad.  
Head Entity: John  
Tail Entity: Emily  

Relation: person siblings  
Context: After the game, Sarah celebrated her victory with her brother, Michael, who had been cheering for her from the stands.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person siblings  
Context: In her memoir, Lisa writes fondly about her childhood adventures with her brother, Tom, who always had her back.  
Head Entity: Lisa  
Tail Entity: Tom  

Relation: person siblings  
Context: At the wedding, Anna was thrilled to see her brother, David, who had flown in from another state to be her best man.  
Head Entity: Anna  
Tail Entity: David  

Relation: person siblings  
Context: The documentary featured interviews with Rachel and her sister, Jessica, discussing their close bond and shared experiences growing up.  
Head Entity: Rachel  
Tail Entity: Jessica  
Losses:  27.563180923461914 0.34463363885879517 23.425443649291992
MemoryTrain:  epoch  0, batch     0 | loss: 27.5631809Losses:  20.32185173034668 1.7600085735321045 14.156161308288574
MemoryTrain:  epoch  0, batch     1 | loss: 20.3218517Losses:  18.02652359008789 0.9152282476425171 13.896903991699219
MemoryTrain:  epoch  0, batch     2 | loss: 18.0265236Losses:  21.06690216064453 0.8272695541381836 16.819311141967773
MemoryTrain:  epoch  0, batch     3 | loss: 21.0669022Losses:  18.040504455566406 1.0832051038742065 13.710819244384766
MemoryTrain:  epoch  0, batch     4 | loss: 18.0405045Losses:  20.512662887573242 0.9177483320236206 16.71669578552246
MemoryTrain:  epoch  0, batch     5 | loss: 20.5126629Losses:  14.593976974487305 0.9304758310317993 10.799907684326172
MemoryTrain:  epoch  0, batch     6 | loss: 14.5939770Losses:  20.076709747314453 0.2874625623226166 16.69413185119629
MemoryTrain:  epoch  0, batch     7 | loss: 20.0767097Losses:  19.363584518432617 1.0498729944229126 13.963811874389648
MemoryTrain:  epoch  0, batch     8 | loss: 19.3635845Losses:  20.478132247924805 0.2427879422903061 16.71417999267578
MemoryTrain:  epoch  0, batch     9 | loss: 20.4781322Losses:  18.41954803466797 0.8644000291824341 13.691628456115723
MemoryTrain:  epoch  0, batch    10 | loss: 18.4195480Losses:  23.669591903686523 0.4943277835845947 19.942371368408203
MemoryTrain:  epoch  0, batch    11 | loss: 23.6695919Losses:  30.492076873779297 0.4971262514591217 26.456466674804688
MemoryTrain:  epoch  0, batch    12 | loss: 30.4920769Losses:  12.379461288452148 0.26345714926719666 8.445235252380371
MemoryTrain:  epoch  0, batch    13 | loss: 12.3794613Losses:  17.720470428466797 0.8135887980461121 13.806346893310547
MemoryTrain:  epoch  1, batch     0 | loss: 17.7204704Losses:  26.689830780029297 0.49199628829956055 23.119674682617188
MemoryTrain:  epoch  1, batch     1 | loss: 26.6898308Losses:  27.468713760375977 0.5421427488327026 23.222890853881836
MemoryTrain:  epoch  1, batch     2 | loss: 27.4687138Losses:  23.584232330322266 0.7661422491073608 19.950536727905273
MemoryTrain:  epoch  1, batch     3 | loss: 23.5842323Losses:  14.955549240112305 0.8020994067192078 10.858362197875977
MemoryTrain:  epoch  1, batch     4 | loss: 14.9555492Losses:  17.332624435424805 0.7423180937767029 13.66295337677002
MemoryTrain:  epoch  1, batch     5 | loss: 17.3326244Losses:  19.80912208557129 0.498375803232193 16.70024299621582
MemoryTrain:  epoch  1, batch     6 | loss: 19.8091221Losses:  24.94178009033203 1.4140291213989258 19.9664363861084
MemoryTrain:  epoch  1, batch     7 | loss: 24.9417801Losses:  18.096847534179688 0.8118115663528442 13.688034057617188
MemoryTrain:  epoch  1, batch     8 | loss: 18.0968475Losses:  20.939729690551758 1.7439522743225098 16.72163963317871
MemoryTrain:  epoch  1, batch     9 | loss: 20.9397297Losses:  23.489336013793945 1.0650734901428223 19.841718673706055
MemoryTrain:  epoch  1, batch    10 | loss: 23.4893360Losses:  20.43943977355957 1.1796048879623413 16.745689392089844
MemoryTrain:  epoch  1, batch    11 | loss: 20.4394398Losses:  27.030202865600586 0.4781062602996826 23.124164581298828
MemoryTrain:  epoch  1, batch    12 | loss: 27.0302029Losses:  11.477989196777344 1.2627891302108765 8.058588981628418
MemoryTrain:  epoch  1, batch    13 | loss: 11.4779892Losses:  22.935260772705078 0.7567889094352722 19.813440322875977
MemoryTrain:  epoch  2, batch     0 | loss: 22.9352608Losses:  20.702714920043945 1.0801966190338135 16.747217178344727
MemoryTrain:  epoch  2, batch     1 | loss: 20.7027149Losses:  19.90561294555664 0.7774998545646667 16.751819610595703
MemoryTrain:  epoch  2, batch     2 | loss: 19.9056129Losses:  30.053050994873047 1.1582260131835938 26.474782943725586
MemoryTrain:  epoch  2, batch     3 | loss: 30.0530510Losses:  16.76570701599121 0.5080397129058838 13.663690567016602
MemoryTrain:  epoch  2, batch     4 | loss: 16.7657070Losses:  17.378252029418945 0.7902051210403442 13.664240837097168
MemoryTrain:  epoch  2, batch     5 | loss: 17.3782520Losses:  19.88556671142578 1.0235989093780518 16.7913875579834
MemoryTrain:  epoch  2, batch     6 | loss: 19.8855667Losses:  13.808995246887207 0.992330014705658 10.784369468688965
MemoryTrain:  epoch  2, batch     7 | loss: 13.8089952Losses:  19.753259658813477 0.7850745320320129 16.717926025390625
MemoryTrain:  epoch  2, batch     8 | loss: 19.7532597Losses:  26.22002601623535 0.3068426847457886 23.14512062072754
MemoryTrain:  epoch  2, batch     9 | loss: 26.2200260Losses:  18.368881225585938 1.781982421875 13.685772895812988
MemoryTrain:  epoch  2, batch    10 | loss: 18.3688812Losses:  20.094135284423828 0.7260079979896545 16.731781005859375
MemoryTrain:  epoch  2, batch    11 | loss: 20.0941353Losses:  19.373804092407227 0.7256033420562744 16.68942642211914
MemoryTrain:  epoch  2, batch    12 | loss: 19.3738041Losses:  10.527403831481934 -0.0 8.087295532226562
MemoryTrain:  epoch  2, batch    13 | loss: 10.5274038Losses:  22.89251708984375 0.2784147262573242 19.985984802246094
MemoryTrain:  epoch  3, batch     0 | loss: 22.8925171Losses:  24.972850799560547 2.0624985694885254 19.896560668945312
MemoryTrain:  epoch  3, batch     1 | loss: 24.9728508Losses:  16.44002342224121 0.7395983934402466 13.69472885131836
MemoryTrain:  epoch  3, batch     2 | loss: 16.4400234Losses:  22.55348014831543 0.738636314868927 19.820749282836914
MemoryTrain:  epoch  3, batch     3 | loss: 22.5534801Losses:  19.18379783630371 0.49779778718948364 16.677738189697266
MemoryTrain:  epoch  3, batch     4 | loss: 19.1837978Losses:  23.018062591552734 0.586004376411438 19.870136260986328
MemoryTrain:  epoch  3, batch     5 | loss: 23.0180626Losses:  14.403797149658203 1.5493595600128174 10.787405967712402
MemoryTrain:  epoch  3, batch     6 | loss: 14.4037971Losses:  16.19426727294922 0.48123711347579956 13.665199279785156
MemoryTrain:  epoch  3, batch     7 | loss: 16.1942673Losses:  29.633968353271484 0.2682270407676697 26.501995086669922
MemoryTrain:  epoch  3, batch     8 | loss: 29.6339684Losses:  22.60501480102539 0.4961733818054199 19.822998046875
MemoryTrain:  epoch  3, batch     9 | loss: 22.6050148Losses:  16.61587905883789 0.5594620704650879 13.677268981933594
MemoryTrain:  epoch  3, batch    10 | loss: 16.6158791Losses:  22.838436126708984 0.8362960815429688 19.813690185546875
MemoryTrain:  epoch  3, batch    11 | loss: 22.8384361Losses:  16.737245559692383 0.7803370952606201 13.691381454467773
MemoryTrain:  epoch  3, batch    12 | loss: 16.7372456Losses:  11.017240524291992 -0.0 8.088325500488281
MemoryTrain:  epoch  3, batch    13 | loss: 11.0172405Losses:  16.30109405517578 0.5566695332527161 13.672845840454102
MemoryTrain:  epoch  4, batch     0 | loss: 16.3010941Losses:  16.594486236572266 0.28773510456085205 13.684985160827637
MemoryTrain:  epoch  4, batch     1 | loss: 16.5944862Losses:  14.192418098449707 0.8807213306427002 10.788548469543457
MemoryTrain:  epoch  4, batch     2 | loss: 14.1924181Losses:  20.042831420898438 1.1400163173675537 16.729949951171875
MemoryTrain:  epoch  4, batch     3 | loss: 20.0428314Losses:  28.483055114746094 -0.0 26.472389221191406
MemoryTrain:  epoch  4, batch     4 | loss: 28.4830551Losses:  18.75621223449707 -0.0 16.721567153930664
MemoryTrain:  epoch  4, batch     5 | loss: 18.7562122Losses:  18.900211334228516 0.26744234561920166 16.68203353881836
MemoryTrain:  epoch  4, batch     6 | loss: 18.9002113Losses:  11.069857597351074 0.8447327613830566 8.120665550231934
MemoryTrain:  epoch  4, batch     7 | loss: 11.0698576Losses:  17.33051300048828 1.6150083541870117 13.669210433959961
MemoryTrain:  epoch  4, batch     8 | loss: 17.3305130Losses:  16.940113067626953 1.128389835357666 13.676079750061035
MemoryTrain:  epoch  4, batch     9 | loss: 16.9401131Losses:  23.303171157836914 0.9688111543655396 19.849796295166016
MemoryTrain:  epoch  4, batch    10 | loss: 23.3031712Losses:  19.383026123046875 0.7135660648345947 16.725940704345703
MemoryTrain:  epoch  4, batch    11 | loss: 19.3830261Losses:  28.887956619262695 0.2493489533662796 26.523391723632812
MemoryTrain:  epoch  4, batch    12 | loss: 28.8879566Losses:  7.730265140533447 0.2769329845905304 5.567044734954834
MemoryTrain:  epoch  4, batch    13 | loss: 7.7302651Losses:  25.424528121948242 0.24700188636779785 23.11222267150879
MemoryTrain:  epoch  5, batch     0 | loss: 25.4245281Losses:  19.30414581298828 0.27516430616378784 16.828039169311523
MemoryTrain:  epoch  5, batch     1 | loss: 19.3041458Losses:  17.279197692871094 1.6297880411148071 13.650671005249023
MemoryTrain:  epoch  5, batch     2 | loss: 17.2791977Losses:  19.7860050201416 0.8350531458854675 16.738765716552734
MemoryTrain:  epoch  5, batch     3 | loss: 19.7860050Losses:  19.317874908447266 0.48023468255996704 16.691957473754883
MemoryTrain:  epoch  5, batch     4 | loss: 19.3178749Losses:  19.211196899414062 0.483642578125 16.700815200805664
MemoryTrain:  epoch  5, batch     5 | loss: 19.2111969Losses:  19.476913452148438 0.8502488136291504 16.672887802124023
MemoryTrain:  epoch  5, batch     6 | loss: 19.4769135Losses:  19.701133728027344 0.732583224773407 16.695383071899414
MemoryTrain:  epoch  5, batch     7 | loss: 19.7011337Losses:  16.342451095581055 0.7582459449768066 13.66163444519043
MemoryTrain:  epoch  5, batch     8 | loss: 16.3424511Losses:  19.744726181030273 0.7540541291236877 16.6809139251709
MemoryTrain:  epoch  5, batch     9 | loss: 19.7447262Losses:  16.838985443115234 1.0968217849731445 13.641692161560059
MemoryTrain:  epoch  5, batch    10 | loss: 16.8389854Losses:  22.394556045532227 0.5073543190956116 19.849607467651367
MemoryTrain:  epoch  5, batch    11 | loss: 22.3945560Losses:  23.183841705322266 1.3641273975372314 19.875083923339844
MemoryTrain:  epoch  5, batch    12 | loss: 23.1838417Losses:  8.54510498046875 0.46892139315605164 5.554891586303711
MemoryTrain:  epoch  5, batch    13 | loss: 8.5451050Losses:  19.36624526977539 0.6961041688919067 16.70481300354004
MemoryTrain:  epoch  6, batch     0 | loss: 19.3662453Losses:  23.2579288482666 1.0888051986694336 19.850767135620117
MemoryTrain:  epoch  6, batch     1 | loss: 23.2579288Losses:  16.156904220581055 0.5454928874969482 13.645066261291504
MemoryTrain:  epoch  6, batch     2 | loss: 16.1569042Losses:  22.767745971679688 0.975555419921875 19.830116271972656
MemoryTrain:  epoch  6, batch     3 | loss: 22.7677460Losses:  29.14927864074707 0.5854508876800537 26.524169921875
MemoryTrain:  epoch  6, batch     4 | loss: 29.1492786Losses:  19.24986457824707 0.5056231021881104 16.712507247924805
MemoryTrain:  epoch  6, batch     5 | loss: 19.2498646Losses:  22.152389526367188 0.2594359815120697 19.875028610229492
MemoryTrain:  epoch  6, batch     6 | loss: 22.1523895Losses:  19.785263061523438 1.1011574268341064 16.674306869506836
MemoryTrain:  epoch  6, batch     7 | loss: 19.7852631Losses:  20.19647789001465 1.0421779155731201 16.720399856567383
MemoryTrain:  epoch  6, batch     8 | loss: 20.1964779Losses:  19.764137268066406 1.0340642929077148 16.73411750793457
MemoryTrain:  epoch  6, batch     9 | loss: 19.7641373Losses:  19.36852264404297 0.7153074145317078 16.720722198486328
MemoryTrain:  epoch  6, batch    10 | loss: 19.3685226Losses:  19.35390853881836 0.7468568086624146 16.69499397277832
MemoryTrain:  epoch  6, batch    11 | loss: 19.3539085Losses:  25.630849838256836 0.47804611921310425 23.1487979888916
MemoryTrain:  epoch  6, batch    12 | loss: 25.6308498Losses:  7.790284156799316 0.2999180555343628 5.591310024261475
MemoryTrain:  epoch  6, batch    13 | loss: 7.7902842Losses:  22.289886474609375 0.5179480314254761 19.81886100769043
MemoryTrain:  epoch  7, batch     0 | loss: 22.2898865Losses:  22.049964904785156 0.23578131198883057 19.853879928588867
MemoryTrain:  epoch  7, batch     1 | loss: 22.0499649Losses:  25.30182647705078 0.25854647159576416 23.123254776000977
MemoryTrain:  epoch  7, batch     2 | loss: 25.3018265Losses:  28.83473014831543 0.4599878191947937 26.48458480834961
MemoryTrain:  epoch  7, batch     3 | loss: 28.8347301Losses:  17.448978424072266 1.7990882396697998 13.672073364257812
MemoryTrain:  epoch  7, batch     4 | loss: 17.4489784Losses:  22.195419311523438 0.4700281620025635 19.8447208404541
MemoryTrain:  epoch  7, batch     5 | loss: 22.1954193Losses:  19.044374465942383 0.2672627866268158 16.69877052307129
MemoryTrain:  epoch  7, batch     6 | loss: 19.0443745Losses:  22.55233383178711 0.7805445194244385 19.83001136779785
MemoryTrain:  epoch  7, batch     7 | loss: 22.5523338Losses:  29.45631217956543 0.8184797763824463 26.47051239013672
MemoryTrain:  epoch  7, batch     8 | loss: 29.4563122Losses:  19.170034408569336 0.28137344121932983 16.720109939575195
MemoryTrain:  epoch  7, batch     9 | loss: 19.1700344Losses:  19.93682098388672 1.1219645738601685 16.764474868774414
MemoryTrain:  epoch  7, batch    10 | loss: 19.9368210Losses:  16.041149139404297 0.4929163455963135 13.65402603149414
MemoryTrain:  epoch  7, batch    11 | loss: 16.0411491Losses:  22.70870018005371 0.9690759181976318 19.829206466674805
MemoryTrain:  epoch  7, batch    12 | loss: 22.7087002Losses:  10.2404146194458 0.27459192276000977 8.06536865234375
MemoryTrain:  epoch  7, batch    13 | loss: 10.2404146Losses:  17.055593490600586 1.3658795356750488 13.682455062866211
MemoryTrain:  epoch  8, batch     0 | loss: 17.0555935Losses:  19.574552536010742 0.79189133644104 16.70267677307129
MemoryTrain:  epoch  8, batch     1 | loss: 19.5745525Losses:  16.159177780151367 0.4971124529838562 13.67153263092041
MemoryTrain:  epoch  8, batch     2 | loss: 16.1591778Losses:  22.573049545288086 0.830367386341095 19.857112884521484
MemoryTrain:  epoch  8, batch     3 | loss: 22.5730495Losses:  22.38105010986328 0.4960538148880005 19.817678451538086
MemoryTrain:  epoch  8, batch     4 | loss: 22.3810501Losses:  16.41451644897461 0.7552111148834229 13.683849334716797
MemoryTrain:  epoch  8, batch     5 | loss: 16.4145164Losses:  26.10927391052246 1.0014804601669312 23.08911895751953
MemoryTrain:  epoch  8, batch     6 | loss: 26.1092739Losses:  25.76962661743164 0.7102687358856201 23.15089988708496
MemoryTrain:  epoch  8, batch     7 | loss: 25.7696266Losses:  13.797029495239258 1.1056410074234009 10.763042449951172
MemoryTrain:  epoch  8, batch     8 | loss: 13.7970295Losses:  19.19015884399414 0.4988609254360199 16.68667221069336
MemoryTrain:  epoch  8, batch     9 | loss: 19.1901588Losses:  19.497676849365234 0.8774756193161011 16.694000244140625
MemoryTrain:  epoch  8, batch    10 | loss: 19.4976768Losses:  25.45511245727539 0.48908787965774536 23.07048797607422
MemoryTrain:  epoch  8, batch    11 | loss: 25.4551125Losses:  25.721885681152344 0.7360154390335083 23.081581115722656
MemoryTrain:  epoch  8, batch    12 | loss: 25.7218857Losses:  12.90249252319336 0.2820173501968384 10.777430534362793
MemoryTrain:  epoch  8, batch    13 | loss: 12.9024925Losses:  25.22453498840332 0.24632439017295837 23.075817108154297
MemoryTrain:  epoch  9, batch     0 | loss: 25.2245350Losses:  20.885204315185547 2.2970974445343018 16.670602798461914
MemoryTrain:  epoch  9, batch     1 | loss: 20.8852043Losses:  25.1052303314209 -0.0 23.08913230895996
MemoryTrain:  epoch  9, batch     2 | loss: 25.1052303Losses:  19.407564163208008 0.7514342665672302 16.7154541015625
MemoryTrain:  epoch  9, batch     3 | loss: 19.4075642Losses:  18.58892250061035 -0.0 16.662569046020508
MemoryTrain:  epoch  9, batch     4 | loss: 18.5889225Losses:  28.745792388916016 0.4520609378814697 26.435903549194336
MemoryTrain:  epoch  9, batch     5 | loss: 28.7457924Losses:  25.49823760986328 0.4869579076766968 23.0625057220459
MemoryTrain:  epoch  9, batch     6 | loss: 25.4982376Losses:  13.574050903320312 0.8097785711288452 10.787753105163574
MemoryTrain:  epoch  9, batch     7 | loss: 13.5740509Losses:  20.217756271362305 1.592695713043213 16.70916748046875
MemoryTrain:  epoch  9, batch     8 | loss: 20.2177563Losses:  15.856531143188477 0.2598356604576111 13.664608001708984
MemoryTrain:  epoch  9, batch     9 | loss: 15.8565311Losses:  11.576629638671875 1.5527230501174927 8.079530715942383
MemoryTrain:  epoch  9, batch    10 | loss: 11.5766296Losses:  16.37109375 0.759289026260376 13.659280776977539
MemoryTrain:  epoch  9, batch    11 | loss: 16.3710938Losses:  16.35752296447754 0.7518792748451233 13.68971061706543
MemoryTrain:  epoch  9, batch    12 | loss: 16.3575230Losses:  13.042879104614258 0.3041611313819885 10.786748886108398
MemoryTrain:  epoch  9, batch    13 | loss: 13.0428791
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 17.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 21.88%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 23.21%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 21.88%   [EVAL] batch:    8 | acc: 12.50%,  total acc: 20.83%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 21.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 24.43%   [EVAL] batch:   11 | acc: 12.50%,  total acc: 23.44%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 23.56%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 21.88%   [EVAL] batch:   14 | acc: 0.00%,  total acc: 20.42%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 19.92%   [EVAL] batch:   16 | acc: 0.00%,  total acc: 18.75%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 17.71%   [EVAL] batch:   18 | acc: 43.75%,  total acc: 19.08%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 21.25%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 23.21%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 23.58%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 55.47%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 54.17%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 51.88%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 50.52%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 47.60%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 45.98%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 47.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 48.44%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 50.00%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 50.69%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 51.32%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 52.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 55.06%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 57.10%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 58.97%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 62.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 63.22%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 64.12%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 67.29%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 67.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 68.95%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 69.13%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 69.11%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 69.10%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 69.41%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 69.23%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 69.97%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.67%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 73.34%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 72.00%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 71.69%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 71.75%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 71.58%   [EVAL] batch:   53 | acc: 31.25%,  total acc: 70.83%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 69.55%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 68.30%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 67.11%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 65.95%   [EVAL] batch:   58 | acc: 6.25%,  total acc: 64.94%   [EVAL] batch:   59 | acc: 37.50%,  total acc: 64.48%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 64.65%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 63.91%   [EVAL] batch:   62 | acc: 0.00%,  total acc: 62.90%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 61.91%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 62.12%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 62.69%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 63.25%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 64.31%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 64.82%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 64.79%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:   72 | acc: 50.00%,  total acc: 64.38%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 64.19%   [EVAL] batch:   74 | acc: 56.25%,  total acc: 64.08%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 63.73%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 63.56%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 62.98%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 62.66%   [EVAL] batch:   79 | acc: 31.25%,  total acc: 62.27%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 61.73%   [EVAL] batch:   81 | acc: 93.75%,  total acc: 62.12%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 62.35%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 62.57%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 62.28%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 61.85%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 61.21%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 60.58%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 60.18%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 59.93%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 60.23%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 60.67%   [EVAL] batch:   92 | acc: 81.25%,  total acc: 60.89%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 61.24%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 61.38%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 61.72%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 61.60%   [EVAL] batch:   97 | acc: 12.50%,  total acc: 61.10%   [EVAL] batch:   98 | acc: 12.50%,  total acc: 60.61%   [EVAL] batch:   99 | acc: 6.25%,  total acc: 60.06%   [EVAL] batch:  100 | acc: 31.25%,  total acc: 59.78%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 59.50%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 59.41%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 59.01%   [EVAL] batch:  104 | acc: 12.50%,  total acc: 58.57%   [EVAL] batch:  105 | acc: 25.00%,  total acc: 58.25%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 58.00%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 57.81%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 57.51%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 57.10%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 56.59%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 56.08%   [EVAL] batch:  112 | acc: 12.50%,  total acc: 55.70%   [EVAL] batch:  113 | acc: 0.00%,  total acc: 55.21%   [EVAL] batch:  114 | acc: 12.50%,  total acc: 54.84%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 54.80%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 54.81%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 54.87%   [EVAL] batch:  118 | acc: 18.75%,  total acc: 54.57%   
cur_acc:  ['0.8693', '0.5781', '0.8170', '0.8368', '0.6346', '0.5491', '0.2358']
his_acc:  ['0.8693', '0.6891', '0.7199', '0.7597', '0.6808', '0.6495', '0.5457']
Clustering into  19  clusters
Clusters:  [ 4  1  0 11 17 10  4 13 12  2  1  2  7  4  0  6  4 15 18 16  5  3  6 10
 14 18  7  4 16  1 18  0  0  8 10 16  4  3  9  8 16]
Losses:  21.99435043334961 9.220932006835938 5.833271026611328
CurrentTrain: epoch  0, batch     0 | loss: 21.9943504Losses:  12.865673065185547 4.186106204986572 3.301250457763672
CurrentTrain: epoch  0, batch     1 | loss: 12.8656731Losses:  17.118000030517578 6.695403099060059 5.6410112380981445
CurrentTrain: epoch  1, batch     0 | loss: 17.1180000Losses:  12.771106719970703 1.9196351766586304 5.695889949798584
CurrentTrain: epoch  1, batch     1 | loss: 12.7711067Losses:  16.675121307373047 7.069859504699707 5.6008620262146
CurrentTrain: epoch  2, batch     0 | loss: 16.6751213Losses:  12.95517635345459 2.5534698963165283 5.575390815734863
CurrentTrain: epoch  2, batch     1 | loss: 12.9551764Losses:  16.783966064453125 6.713286399841309 5.613753318786621
CurrentTrain: epoch  3, batch     0 | loss: 16.7839661Losses:  10.352048873901367 1.3990260362625122 5.560836315155029
CurrentTrain: epoch  3, batch     1 | loss: 10.3520489Losses:  17.30926513671875 7.717565536499023 5.553110122680664
CurrentTrain: epoch  4, batch     0 | loss: 17.3092651Losses:  10.848013877868652 2.4652514457702637 5.553920269012451
CurrentTrain: epoch  4, batch     1 | loss: 10.8480139Losses:  16.783008575439453 7.822396278381348 5.565586566925049
CurrentTrain: epoch  5, batch     0 | loss: 16.7830086Losses:  10.067882537841797 3.63657808303833 3.339202404022217
CurrentTrain: epoch  5, batch     1 | loss: 10.0678825Losses:  14.770913124084473 6.489996433258057 5.584747314453125
CurrentTrain: epoch  6, batch     0 | loss: 14.7709131Losses:  11.198978424072266 2.4906156063079834 5.564162731170654
CurrentTrain: epoch  6, batch     1 | loss: 11.1989784Losses:  14.496177673339844 6.433053016662598 5.567081928253174
CurrentTrain: epoch  7, batch     0 | loss: 14.4961777Losses:  8.996192932128906 2.490783214569092 3.321650981903076
CurrentTrain: epoch  7, batch     1 | loss: 8.9961929Losses:  15.587860107421875 7.051555633544922 5.58046293258667
CurrentTrain: epoch  8, batch     0 | loss: 15.5878601Losses:  7.60369873046875 2.144895076751709 3.33345627784729
CurrentTrain: epoch  8, batch     1 | loss: 7.6036987Losses:  14.020378112792969 5.907436370849609 5.574245452880859
CurrentTrain: epoch  9, batch     0 | loss: 14.0203781Losses:  10.036076545715332 1.7702076435089111 5.564961910247803
CurrentTrain: epoch  9, batch     1 | loss: 10.0360765
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: san jose , ca , usa speaking of k-fed , him and ex-wife britney spears are in court today , dealing with their custody battle .
Head Entity: britney spears
Tail Entity: ca
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving from New York, John settled in Austin, Texas, where he found a job in tech.  
Head Entity: John  
Tail Entity: Texas  

Relation: person stateorprovinces of residence  
Context: The famous actor, who was born in London, now resides in the sunny state of California.  
Head Entity: The famous actor  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: Following her studies in Boston, Maria decided to make Seattle her home due to its vibrant culture.  
Head Entity: Maria  
Tail Entity: Seattle  

Relation: person stateorprovinces of residence  
Context: After years of living in Chicago, Mark has finally moved to the beautiful state of Colorado for a fresh start.  
Head Entity: Mark  
Tail Entity: Colorado  

Relation: person stateorprovinces of residence  
Context: Despite being born in Miami, she has spent the last decade living in the picturesque state of Vermont.  
Head Entity: She  
Tail Entity: Vermont  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he passed away on saturday .
Head Entity: he
Tail Entity: saturday
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The famous author died on July 4th, 1965.  
Head Entity: The famous author  
Tail Entity: July 4th, 1965  

Relation: person date of death  
Context: She left this world on her birthday, March 15th.  
Head Entity: She  
Tail Entity: March 15th  

Relation: person date of death  
Context: He was pronounced dead on the evening of December 31st.  
Head Entity: He  
Tail Entity: December 31st  

Relation: person date of death  
Context: The scientist passed away unexpectedly in the spring of 2020.  
Head Entity: The scientist  
Tail Entity: spring of 2020  

Relation: person date of death  
Context: They commemorated his life on the anniversary of his death, April 10th.  
Head Entity: They  
Tail Entity: April 10th  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, employs approximately 5,500 people across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: The nonprofit organization Helping Hands has grown significantly and now boasts a workforce of over 1,200 dedicated volunteers.  
Head Entity: Helping Hands  
Tail Entity: 1,200  

Relation: organization number of employees members  
Context: With a workforce of around 3,000, Green Energy Solutions is making strides in renewable energy initiatives.  
Head Entity: Green Energy Solutions  
Tail Entity: 3,000  

Relation: organization number of employees members  
Context: Last year, Global Finance reported that it has expanded its team to include 10,000 employees to meet increasing demand.  
Head Entity: Global Finance  
Tail Entity: 10,000  

Relation: organization number of employees members  
Context: The startup InnovateTech has rapidly scaled its operations and now employs 250 individuals in various roles.  
Head Entity: InnovateTech  
Tail Entity: 250  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Langhorne Clemens, better known by his pen name Mark Twain, is celebrated for his novels like "The Adventures of Tom Sawyer."  
Head Entity: Samuel Langhorne Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician Stefani Joanne Angelina Germanotta, who is widely recognized as Lady Gaga, has made a significant impact on the pop music scene.  
Head Entity: Stefani Joanne Angelina Germanotta  
Tail Entity: Lady Gaga  

Relation: person alternate names  
Context: The actor and filmmaker known as Orson Welles was born George Orson Welles and is famous for his groundbreaking work in cinema.  
Head Entity: George Orson Welles  
Tail Entity: Orson Welles  

Relation: person alternate names  
Context: The renowned physicist Albert Einstein, who is often referred to simply as Einstein, developed the theory of relativity.  
Head Entity: Albert Einstein  
Tail Entity: Einstein  

Relation: person alternate names  
Context: The pop star Robyn Rihanna Fenty, better known as Rihanna, has won numerous awards for her contributions to music and fashion.  
Head Entity: Robyn Rihanna Fenty  
Tail Entity: Rihanna  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: smits stands at the center of this multigenerational saga as alex vega , the adopted son of rum and sugar baron pancho duque -lrb- elizondo -rrb- and his wife , amalia -lrb- moreno -rrb- .
Head Entity: elizondo
Tail Entity: moreno
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: After years of friendship, Sarah finally married her college sweetheart, John, who has always been her biggest supporter.  
Head Entity: Sarah  
Tail Entity: John  

Relation: person spouse  
Context: In the small town of Maplewood, everyone knows that Linda and Tom have been inseparable since high school, and they recently celebrated their 25th wedding anniversary.  
Head Entity: Linda  
Tail Entity: Tom  

Relation: person spouse  
Context: The famous actor, Robert, often credits his wife, Emily, for his success, stating that her unwavering belief in him has been a driving force in his career.  
Head Entity: Robert  
Tail Entity: Emily  

Relation: person spouse  
Context: During the gala, Jessica introduced her husband, Michael, to the guests, highlighting their journey together from college friends to life partners.  
Head Entity: Jessica  
Tail Entity: Michael  

Relation: person spouse  
Context: As the community gathered to celebrate their golden anniversary, everyone admired how gracefully Helen and George have aged together as a couple.  
Head Entity: Helen  
Tail Entity: George  
Losses:  16.5119686126709 0.2305964082479477 13.762781143188477
MemoryTrain:  epoch  0, batch     0 | loss: 16.5119686Losses:  30.3926944732666 0.2577630281448364 26.481727600097656
MemoryTrain:  epoch  0, batch     1 | loss: 30.3926945Losses:  23.94092559814453 0.5515376329421997 19.844118118286133
MemoryTrain:  epoch  0, batch     2 | loss: 23.9409256Losses:  17.624025344848633 0.2657400965690613 13.720510482788086
MemoryTrain:  epoch  0, batch     3 | loss: 17.6240253Losses:  28.86880874633789 1.0782160758972168 23.3399658203125
MemoryTrain:  epoch  0, batch     4 | loss: 28.8688087Losses:  21.13435173034668 1.1010041236877441 16.665786743164062
MemoryTrain:  epoch  0, batch     5 | loss: 21.1343517Losses:  29.445343017578125 0.23600658774375916 26.470157623291016
MemoryTrain:  epoch  0, batch     6 | loss: 29.4453430Losses:  20.236934661865234 0.7171555757522583 16.686126708984375
MemoryTrain:  epoch  0, batch     7 | loss: 20.2369347Losses:  21.268890380859375 1.3382091522216797 16.741092681884766
MemoryTrain:  epoch  0, batch     8 | loss: 21.2688904Losses:  20.44924545288086 0.49592769145965576 16.70112419128418
MemoryTrain:  epoch  0, batch     9 | loss: 20.4492455Losses:  23.626934051513672 1.0573945045471191 19.950302124023438
MemoryTrain:  epoch  0, batch    10 | loss: 23.6269341Losses:  26.004722595214844 0.2619199752807617 23.090808868408203
MemoryTrain:  epoch  0, batch    11 | loss: 26.0047226Losses:  20.511831283569336 0.7439998388290405 16.695274353027344
MemoryTrain:  epoch  0, batch    12 | loss: 20.5118313Losses:  29.967880249023438 1.0669673681259155 26.484859466552734
MemoryTrain:  epoch  0, batch    13 | loss: 29.9678802Losses:  32.756866455078125 -0.0 29.91541862487793
MemoryTrain:  epoch  0, batch    14 | loss: 32.7568665Losses:  11.308204650878906 -0.0 8.219352722167969
MemoryTrain:  epoch  0, batch    15 | loss: 11.3082047Losses:  16.52457618713379 0.7268961668014526 13.689842224121094
MemoryTrain:  epoch  1, batch     0 | loss: 16.5245762Losses:  20.853057861328125 1.3883709907531738 16.727752685546875
MemoryTrain:  epoch  1, batch     1 | loss: 20.8530579Losses:  23.5557861328125 0.9030987620353699 19.82819175720215
MemoryTrain:  epoch  1, batch     2 | loss: 23.5557861Losses:  17.000349044799805 0.8757022023200989 13.712690353393555
MemoryTrain:  epoch  1, batch     3 | loss: 17.0003490Losses:  25.987010955810547 -0.0 23.099491119384766
MemoryTrain:  epoch  1, batch     4 | loss: 25.9870110Losses:  29.591949462890625 0.24664583802223206 26.476730346679688
MemoryTrain:  epoch  1, batch     5 | loss: 29.5919495Losses:  29.969236373901367 0.7251763939857483 26.452802658081055
MemoryTrain:  epoch  1, batch     6 | loss: 29.9692364Losses:  26.120878219604492 0.7383792400360107 23.14535903930664
MemoryTrain:  epoch  1, batch     7 | loss: 26.1208782Losses:  17.795204162597656 1.0485267639160156 13.652177810668945
MemoryTrain:  epoch  1, batch     8 | loss: 17.7952042Losses:  18.396947860717773 1.033410668373108 13.800281524658203
MemoryTrain:  epoch  1, batch     9 | loss: 18.3969479Losses:  17.707130432128906 1.0777807235717773 13.742834091186523
MemoryTrain:  epoch  1, batch    10 | loss: 17.7071304Losses:  18.419979095458984 0.7089756727218628 13.71622085571289
MemoryTrain:  epoch  1, batch    11 | loss: 18.4199791Losses:  23.661407470703125 1.561036229133606 19.836978912353516
MemoryTrain:  epoch  1, batch    12 | loss: 23.6614075Losses:  19.481433868408203 0.725662350654602 16.711496353149414
MemoryTrain:  epoch  1, batch    13 | loss: 19.4814339Losses:  19.81169891357422 -0.0 16.76797866821289
MemoryTrain:  epoch  1, batch    14 | loss: 19.8116989Losses:  7.867394924163818 0.26191702485084534 5.554419994354248
MemoryTrain:  epoch  1, batch    15 | loss: 7.8673949Losses:  36.35960006713867 0.25741398334503174 33.48651123046875
MemoryTrain:  epoch  2, batch     0 | loss: 36.3596001Losses:  19.2670955657959 0.48754817247390747 16.707340240478516
MemoryTrain:  epoch  2, batch     1 | loss: 19.2670956Losses:  20.18008804321289 1.2665600776672363 16.66916847229004
MemoryTrain:  epoch  2, batch     2 | loss: 20.1800880Losses:  20.128623962402344 1.1189136505126953 16.72165298461914
MemoryTrain:  epoch  2, batch     3 | loss: 20.1286240Losses:  23.423583984375 0.49835634231567383 19.9414005279541
MemoryTrain:  epoch  2, batch     4 | loss: 23.4235840Losses:  25.98647689819336 0.47804081439971924 23.105541229248047
MemoryTrain:  epoch  2, batch     5 | loss: 25.9864769Losses:  29.992443084716797 1.099302887916565 26.45631217956543
MemoryTrain:  epoch  2, batch     6 | loss: 29.9924431Losses:  33.17369079589844 0.7538572549819946 29.974634170532227
MemoryTrain:  epoch  2, batch     7 | loss: 33.1736908Losses:  25.60968017578125 0.2406419813632965 23.129404067993164
MemoryTrain:  epoch  2, batch     8 | loss: 25.6096802Losses:  15.26252555847168 1.4180974960327148 10.800317764282227
MemoryTrain:  epoch  2, batch     9 | loss: 15.2625256Losses:  18.643917083740234 1.8870670795440674 13.66893196105957
MemoryTrain:  epoch  2, batch    10 | loss: 18.6439171Losses:  23.884563446044922 1.3178939819335938 19.834909439086914
MemoryTrain:  epoch  2, batch    11 | loss: 23.8845634Losses:  29.065061569213867 0.2715935707092285 26.51783561706543
MemoryTrain:  epoch  2, batch    12 | loss: 29.0650616Losses:  19.664262771606445 1.0290651321411133 16.703365325927734
MemoryTrain:  epoch  2, batch    13 | loss: 19.6642628Losses:  24.98033332824707 1.3539717197418213 19.86482048034668
MemoryTrain:  epoch  2, batch    14 | loss: 24.9803333Losses:  8.451594352722168 0.27564865350723267 5.556640148162842
MemoryTrain:  epoch  2, batch    15 | loss: 8.4515944Losses:  25.380325317382812 1.969501256942749 19.960031509399414
MemoryTrain:  epoch  3, batch     0 | loss: 25.3803253Losses:  20.459861755371094 1.7688008546829224 16.708864212036133
MemoryTrain:  epoch  3, batch     1 | loss: 20.4598618Losses:  23.293811798095703 0.7792354822158813 19.96051597595215
MemoryTrain:  epoch  3, batch     2 | loss: 23.2938118Losses:  24.052616119384766 0.6040904521942139 19.814001083374023
MemoryTrain:  epoch  3, batch     3 | loss: 24.0526161Losses:  22.286518096923828 0.22650006413459778 19.839548110961914
MemoryTrain:  epoch  3, batch     4 | loss: 22.2865181Losses:  25.202482223510742 -0.0 23.130199432373047
MemoryTrain:  epoch  3, batch     5 | loss: 25.2024822Losses:  29.310632705688477 0.23374322056770325 26.455326080322266
MemoryTrain:  epoch  3, batch     6 | loss: 29.3106327Losses:  19.929542541503906 0.8883945941925049 16.757394790649414
MemoryTrain:  epoch  3, batch     7 | loss: 19.9295425Losses:  19.400951385498047 0.7608094215393066 16.673263549804688
MemoryTrain:  epoch  3, batch     8 | loss: 19.4009514Losses:  25.9835147857666 0.4965401887893677 23.106170654296875
MemoryTrain:  epoch  3, batch     9 | loss: 25.9835148Losses:  20.53474998474121 1.0590306520462036 16.70008659362793
MemoryTrain:  epoch  3, batch    10 | loss: 20.5347500Losses:  23.444416046142578 1.0674550533294678 19.877685546875
MemoryTrain:  epoch  3, batch    11 | loss: 23.4444160Losses:  19.31759262084961 0.7231110334396362 16.675642013549805
MemoryTrain:  epoch  3, batch    12 | loss: 19.3175926Losses:  19.277812957763672 0.2894092798233032 16.729766845703125
MemoryTrain:  epoch  3, batch    13 | loss: 19.2778130Losses:  19.34136390686035 0.7620664238929749 16.677562713623047
MemoryTrain:  epoch  3, batch    14 | loss: 19.3413639Losses:  7.593851566314697 -0.0 5.5804290771484375
MemoryTrain:  epoch  3, batch    15 | loss: 7.5938516Losses:  26.01738739013672 1.0600948333740234 23.072656631469727
MemoryTrain:  epoch  4, batch     0 | loss: 26.0173874Losses:  22.940092086791992 0.7273218631744385 19.866653442382812
MemoryTrain:  epoch  4, batch     1 | loss: 22.9400921Losses:  27.04900360107422 0.5709565877914429 23.12908363342285
MemoryTrain:  epoch  4, batch     2 | loss: 27.0490036Losses:  29.056440353393555 -0.0 26.476261138916016
MemoryTrain:  epoch  4, batch     3 | loss: 29.0564404Losses:  22.315813064575195 0.48599621653556824 19.86899185180664
MemoryTrain:  epoch  4, batch     4 | loss: 22.3158131Losses:  19.422182083129883 0.7147641777992249 16.693410873413086
MemoryTrain:  epoch  4, batch     5 | loss: 19.4221821Losses:  25.923099517822266 0.48289304971694946 23.136316299438477
MemoryTrain:  epoch  4, batch     6 | loss: 25.9230995Losses:  19.70688247680664 0.7795101404190063 16.73995018005371
MemoryTrain:  epoch  4, batch     7 | loss: 19.7068825Losses:  19.9691162109375 0.7649372220039368 16.693124771118164
MemoryTrain:  epoch  4, batch     8 | loss: 19.9691162Losses:  14.888606071472168 0.9323062896728516 10.802693367004395
MemoryTrain:  epoch  4, batch     9 | loss: 14.8886061Losses:  26.58216094970703 1.1244964599609375 23.105993270874023
MemoryTrain:  epoch  4, batch    10 | loss: 26.5821609Losses:  23.18465232849121 1.0965906381607056 19.866907119750977
MemoryTrain:  epoch  4, batch    11 | loss: 23.1846523Losses:  19.973899841308594 0.7895841598510742 16.81905174255371
MemoryTrain:  epoch  4, batch    12 | loss: 19.9738998Losses:  26.170490264892578 0.9947556257247925 23.096328735351562
MemoryTrain:  epoch  4, batch    13 | loss: 26.1704903Losses:  28.440471649169922 -0.0 26.43446922302246
MemoryTrain:  epoch  4, batch    14 | loss: 28.4404716Losses:  9.915799140930176 -0.0 8.062986373901367
MemoryTrain:  epoch  4, batch    15 | loss: 9.9157991Losses:  18.773086547851562 -0.0 16.66767692565918
MemoryTrain:  epoch  5, batch     0 | loss: 18.7730865Losses:  18.83344841003418 0.2624717354774475 16.689088821411133
MemoryTrain:  epoch  5, batch     1 | loss: 18.8334484Losses:  32.202857971191406 0.23600779473781586 29.89912223815918
MemoryTrain:  epoch  5, batch     2 | loss: 32.2028580Losses:  28.806438446044922 0.26042628288269043 26.490131378173828
MemoryTrain:  epoch  5, batch     3 | loss: 28.8064384Losses:  22.571439743041992 0.26080024242401123 19.85432243347168
MemoryTrain:  epoch  5, batch     4 | loss: 22.5714397Losses:  20.3156795501709 1.1083409786224365 16.6892032623291
MemoryTrain:  epoch  5, batch     5 | loss: 20.3156796Losses:  17.470094680786133 1.09446120262146 13.658020973205566
MemoryTrain:  epoch  5, batch     6 | loss: 17.4700947Losses:  22.335268020629883 0.24372076988220215 19.861127853393555
MemoryTrain:  epoch  5, batch     7 | loss: 22.3352680Losses:  22.40706443786621 0.24472378194332123 19.822147369384766
MemoryTrain:  epoch  5, batch     8 | loss: 22.4070644Losses:  19.749752044677734 0.26583921909332275 16.71392822265625
MemoryTrain:  epoch  5, batch     9 | loss: 19.7497520Losses:  25.758338928222656 0.7415554523468018 23.0895938873291
MemoryTrain:  epoch  5, batch    10 | loss: 25.7583389Losses:  22.686254501342773 0.5669698715209961 19.865671157836914
MemoryTrain:  epoch  5, batch    11 | loss: 22.6862545Losses:  19.05249786376953 0.4740874767303467 16.680322647094727
MemoryTrain:  epoch  5, batch    12 | loss: 19.0524979Losses:  22.3432559967041 0.48931294679641724 19.810867309570312
MemoryTrain:  epoch  5, batch    13 | loss: 22.3432560Losses:  17.792158126831055 2.056511402130127 13.663945198059082
MemoryTrain:  epoch  5, batch    14 | loss: 17.7921581Losses:  11.226016998291016 -0.0 8.13009262084961
MemoryTrain:  epoch  5, batch    15 | loss: 11.2260170Losses:  25.508028030395508 0.25289368629455566 23.13035011291504
MemoryTrain:  epoch  6, batch     0 | loss: 25.5080280Losses:  19.611112594604492 0.794684648513794 16.76901626586914
MemoryTrain:  epoch  6, batch     1 | loss: 19.6111126Losses:  22.59298324584961 0.7416048049926758 19.875377655029297
MemoryTrain:  epoch  6, batch     2 | loss: 22.5929832Losses:  22.30671501159668 0.5191264152526855 19.88587760925293
MemoryTrain:  epoch  6, batch     3 | loss: 22.3067150Losses:  19.46194839477539 0.7333517074584961 16.657419204711914
MemoryTrain:  epoch  6, batch     4 | loss: 19.4619484Losses:  22.293977737426758 0.5211190581321716 19.813058853149414
MemoryTrain:  epoch  6, batch     5 | loss: 22.2939777Losses:  28.73779296875 0.24928772449493408 26.443052291870117
MemoryTrain:  epoch  6, batch     6 | loss: 28.7377930Losses:  22.491769790649414 0.7778538465499878 19.824996948242188
MemoryTrain:  epoch  6, batch     7 | loss: 22.4917698Losses:  29.855863571166992 1.0969324111938477 26.54574203491211
MemoryTrain:  epoch  6, batch     8 | loss: 29.8558636Losses:  20.69698715209961 1.6426715850830078 16.688629150390625
MemoryTrain:  epoch  6, batch     9 | loss: 20.6969872Losses:  13.500407218933105 0.4925529360771179 10.775140762329102
MemoryTrain:  epoch  6, batch    10 | loss: 13.5004072Losses:  19.951459884643555 0.856731653213501 16.704742431640625
MemoryTrain:  epoch  6, batch    11 | loss: 19.9514599Losses:  20.94330596923828 2.0585575103759766 16.662439346313477
MemoryTrain:  epoch  6, batch    12 | loss: 20.9433060Losses:  15.875008583068848 0.2513378858566284 13.65322494506836
MemoryTrain:  epoch  6, batch    13 | loss: 15.8750086Losses:  32.076717376708984 0.22640599310398102 29.862451553344727
MemoryTrain:  epoch  6, batch    14 | loss: 32.0767174Losses:  10.253533363342285 -0.0 8.125736236572266
MemoryTrain:  epoch  6, batch    15 | loss: 10.2535334Losses:  16.38258934020996 0.7354005575180054 13.640958786010742
MemoryTrain:  epoch  7, batch     0 | loss: 16.3825893Losses:  32.08847427368164 0.25068485736846924 29.930992126464844
MemoryTrain:  epoch  7, batch     1 | loss: 32.0884743Losses:  23.04967498779297 1.3316482305526733 19.814476013183594
MemoryTrain:  epoch  7, batch     2 | loss: 23.0496750Losses:  16.271135330200195 0.7297728061676025 13.652385711669922
MemoryTrain:  epoch  7, batch     3 | loss: 16.2711353Losses:  25.602205276489258 0.4756683111190796 23.12102699279785
MemoryTrain:  epoch  7, batch     4 | loss: 25.6022053Losses:  23.309368133544922 1.2793294191360474 19.833703994750977
MemoryTrain:  epoch  7, batch     5 | loss: 23.3093681Losses:  16.14806365966797 0.4987434446811676 13.634939193725586
MemoryTrain:  epoch  7, batch     6 | loss: 16.1480637Losses:  28.583789825439453 -0.0 26.448528289794922
MemoryTrain:  epoch  7, batch     7 | loss: 28.5837898Losses:  29.17727279663086 0.49650484323501587 26.486129760742188
MemoryTrain:  epoch  7, batch     8 | loss: 29.1772728Losses:  22.46175193786621 0.7476540803909302 19.846891403198242
MemoryTrain:  epoch  7, batch     9 | loss: 22.4617519Losses:  19.463241577148438 0.4899335503578186 16.67108154296875
MemoryTrain:  epoch  7, batch    10 | loss: 19.4632416Losses:  22.906932830810547 0.7835904359817505 19.865446090698242
MemoryTrain:  epoch  7, batch    11 | loss: 22.9069328Losses:  26.075756072998047 1.091005802154541 23.082136154174805
MemoryTrain:  epoch  7, batch    12 | loss: 26.0757561Losses:  25.911428451538086 0.7231653928756714 23.136930465698242
MemoryTrain:  epoch  7, batch    13 | loss: 25.9114285Losses:  25.552841186523438 0.4986383318901062 23.06941795349121
MemoryTrain:  epoch  7, batch    14 | loss: 25.5528412Losses:  10.080763816833496 -0.0 8.154928207397461
MemoryTrain:  epoch  7, batch    15 | loss: 10.0807638Losses:  25.560239791870117 0.5169071555137634 23.11375617980957
MemoryTrain:  epoch  8, batch     0 | loss: 25.5602398Losses:  20.049510955810547 1.5235073566436768 16.676025390625
MemoryTrain:  epoch  8, batch     1 | loss: 20.0495110Losses:  18.977493286132812 0.26055529713630676 16.67293357849121
MemoryTrain:  epoch  8, batch     2 | loss: 18.9774933Losses:  23.080806732177734 0.82163006067276 19.837976455688477
MemoryTrain:  epoch  8, batch     3 | loss: 23.0808067Losses:  25.554412841796875 0.5029503107070923 23.099308013916016
MemoryTrain:  epoch  8, batch     4 | loss: 25.5544128Losses:  16.184490203857422 0.5214627981185913 13.707012176513672
MemoryTrain:  epoch  8, batch     5 | loss: 16.1844902Losses:  25.525014877319336 0.4561540484428406 23.122007369995117
MemoryTrain:  epoch  8, batch     6 | loss: 25.5250149Losses:  25.457204818725586 0.2644157409667969 23.162128448486328
MemoryTrain:  epoch  8, batch     7 | loss: 25.4572048Losses:  16.312604904174805 0.5248255729675293 13.662372589111328
MemoryTrain:  epoch  8, batch     8 | loss: 16.3126049Losses:  25.525684356689453 0.4679966866970062 23.057891845703125
MemoryTrain:  epoch  8, batch     9 | loss: 25.5256844Losses:  22.269681930541992 0.4793703854084015 19.82733726501465
MemoryTrain:  epoch  8, batch    10 | loss: 22.2696819Losses:  25.964435577392578 1.0221285820007324 23.075082778930664
MemoryTrain:  epoch  8, batch    11 | loss: 25.9644356Losses:  14.319937705993652 1.4050722122192383 10.811990737915039
MemoryTrain:  epoch  8, batch    12 | loss: 14.3199377Losses:  25.818077087402344 0.7520356178283691 23.10788345336914
MemoryTrain:  epoch  8, batch    13 | loss: 25.8180771Losses:  19.432415008544922 0.7478481531143188 16.706022262573242
MemoryTrain:  epoch  8, batch    14 | loss: 19.4324150Losses:  10.179010391235352 -0.0 8.062003135681152
MemoryTrain:  epoch  8, batch    15 | loss: 10.1790104Losses:  21.941774368286133 -0.0 19.829626083374023
MemoryTrain:  epoch  9, batch     0 | loss: 21.9417744Losses:  21.76906394958496 -0.0 19.838226318359375
MemoryTrain:  epoch  9, batch     1 | loss: 21.7690639Losses:  28.845012664794922 0.5140323638916016 26.434316635131836
MemoryTrain:  epoch  9, batch     2 | loss: 28.8450127Losses:  16.522653579711914 0.7986899018287659 13.637444496154785
MemoryTrain:  epoch  9, batch     3 | loss: 16.5226536Losses:  28.42259979248047 -0.0 26.518596649169922
MemoryTrain:  epoch  9, batch     4 | loss: 28.4225998Losses:  28.679323196411133 0.2477099597454071 26.447063446044922
MemoryTrain:  epoch  9, batch     5 | loss: 28.6793232Losses:  23.016536712646484 1.3353546857833862 19.82549476623535
MemoryTrain:  epoch  9, batch     6 | loss: 23.0165367Losses:  16.368003845214844 0.7484911680221558 13.65633773803711
MemoryTrain:  epoch  9, batch     7 | loss: 16.3680038Losses:  16.72354507446289 1.06098210811615 13.686951637268066
MemoryTrain:  epoch  9, batch     8 | loss: 16.7235451Losses:  22.29465675354004 0.49652180075645447 19.85711097717285
MemoryTrain:  epoch  9, batch     9 | loss: 22.2946568Losses:  25.56686782836914 0.5159363746643066 23.089263916015625
MemoryTrain:  epoch  9, batch    10 | loss: 25.5668678Losses:  25.638267517089844 0.512919545173645 23.11498260498047
MemoryTrain:  epoch  9, batch    11 | loss: 25.6382675Losses:  22.416397094726562 0.7129165530204773 19.83009147644043
MemoryTrain:  epoch  9, batch    12 | loss: 22.4163971Losses:  31.972742080688477 0.2421773225069046 29.861888885498047
MemoryTrain:  epoch  9, batch    13 | loss: 31.9727421Losses:  25.068561553955078 -0.0 23.11587905883789
MemoryTrain:  epoch  9, batch    14 | loss: 25.0685616Losses:  7.649575710296631 -0.0 5.557755947113037
MemoryTrain:  epoch  9, batch    15 | loss: 7.6495757
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.89%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 25.00%,  total acc: 67.31%   [EVAL] batch:   13 | acc: 0.00%,  total acc: 62.50%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 58.75%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 31.25%,  total acc: 51.56%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 50.69%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 47.73%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 45.67%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 44.20%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 46.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 48.53%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 49.31%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 51.56%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 53.57%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 55.68%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 57.34%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 58.85%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 60.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 61.78%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 62.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 65.83%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 66.53%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 67.80%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 67.86%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 68.06%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 67.91%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 68.59%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 69.22%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 69.82%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 70.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 72.36%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.54%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 74.09%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 72.96%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 71.62%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 71.20%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 71.15%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 70.64%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 69.56%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 68.30%   [EVAL] batch:   55 | acc: 0.00%,  total acc: 67.08%   [EVAL] batch:   56 | acc: 0.00%,  total acc: 65.90%   [EVAL] batch:   57 | acc: 0.00%,  total acc: 64.76%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 63.67%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 63.12%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 63.42%   [EVAL] batch:   61 | acc: 18.75%,  total acc: 62.70%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 61.81%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 61.04%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 61.25%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 61.84%   [EVAL] batch:   66 | acc: 100.00%,  total acc: 62.41%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 62.96%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 64.02%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 64.08%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 63.89%   [EVAL] batch:   72 | acc: 43.75%,  total acc: 63.61%   [EVAL] batch:   73 | acc: 50.00%,  total acc: 63.43%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 63.42%   [EVAL] batch:   75 | acc: 50.00%,  total acc: 63.24%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 63.15%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 62.74%   [EVAL] batch:   78 | acc: 18.75%,  total acc: 62.18%   [EVAL] batch:   79 | acc: 12.50%,  total acc: 61.56%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 61.03%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 61.36%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 61.37%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 61.38%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 61.03%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 60.61%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 60.06%   [EVAL] batch:   87 | acc: 6.25%,  total acc: 59.45%   [EVAL] batch:   88 | acc: 18.75%,  total acc: 58.99%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 58.61%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 58.93%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 59.68%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 59.97%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 60.13%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 60.48%   [EVAL] batch:   96 | acc: 31.25%,  total acc: 60.18%   [EVAL] batch:   97 | acc: 12.50%,  total acc: 59.69%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 59.15%   [EVAL] batch:   99 | acc: 6.25%,  total acc: 58.63%   [EVAL] batch:  100 | acc: 37.50%,  total acc: 58.42%   [EVAL] batch:  101 | acc: 43.75%,  total acc: 58.27%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 58.19%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 57.81%   [EVAL] batch:  104 | acc: 6.25%,  total acc: 57.32%   [EVAL] batch:  105 | acc: 18.75%,  total acc: 56.96%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 56.72%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 56.42%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 56.08%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 55.68%   [EVAL] batch:  110 | acc: 0.00%,  total acc: 55.18%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 54.69%   [EVAL] batch:  112 | acc: 6.25%,  total acc: 54.26%   [EVAL] batch:  113 | acc: 0.00%,  total acc: 53.78%   [EVAL] batch:  114 | acc: 6.25%,  total acc: 53.37%   [EVAL] batch:  115 | acc: 56.25%,  total acc: 53.39%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 53.47%   [EVAL] batch:  117 | acc: 56.25%,  total acc: 53.50%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 53.68%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 53.75%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 53.77%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 54.00%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 54.12%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 54.44%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 54.75%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 55.11%   [EVAL] batch:  126 | acc: 87.50%,  total acc: 55.36%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 55.62%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 55.43%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 55.10%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 54.96%   [EVAL] batch:  131 | acc: 6.25%,  total acc: 54.59%   [EVAL] batch:  132 | acc: 6.25%,  total acc: 54.23%   
cur_acc:  ['0.8693', '0.5781', '0.8170', '0.8368', '0.6346', '0.5491', '0.2358', '0.5875']
his_acc:  ['0.8693', '0.6891', '0.7199', '0.7597', '0.6808', '0.6495', '0.5457', '0.5423']
--------Round  4
seed:  500
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 0 1]
Losses:  26.824790954589844 13.643329620361328 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 26.8247910Losses:  22.252674102783203 9.001459121704102 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 22.2526741Losses:  21.053895950317383 8.07665729522705 -0.0
CurrentTrain: epoch  0, batch     2 | loss: 21.0538960Losses:  21.48456573486328 8.501873016357422 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 21.4845657Losses:  21.80508804321289 8.948936462402344 -0.0
CurrentTrain: epoch  0, batch     4 | loss: 21.8050880Losses:  21.366653442382812 8.518367767333984 -0.0
CurrentTrain: epoch  0, batch     5 | loss: 21.3666534Losses:  23.948909759521484 11.38849925994873 -0.0
CurrentTrain: epoch  0, batch     6 | loss: 23.9489098Losses:  21.598705291748047 9.244752883911133 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 21.5987053Losses:  22.977108001708984 10.745570182800293 -0.0
CurrentTrain: epoch  0, batch     8 | loss: 22.9771080Losses:  24.56943130493164 12.215713500976562 -0.0
CurrentTrain: epoch  0, batch     9 | loss: 24.5694313Losses:  20.925981521606445 8.855588912963867 -0.0
CurrentTrain: epoch  0, batch    10 | loss: 20.9259815Losses:  21.114395141601562 9.22116756439209 -0.0
CurrentTrain: epoch  0, batch    11 | loss: 21.1143951Losses:  18.703968048095703 6.919242858886719 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 18.7039680Losses:  19.91463851928711 8.111639022827148 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 19.9146385Losses:  20.67290496826172 9.09363842010498 -0.0
CurrentTrain: epoch  0, batch    14 | loss: 20.6729050Losses:  23.243772506713867 11.081567764282227 -0.0
CurrentTrain: epoch  0, batch    15 | loss: 23.2437725Losses:  21.964336395263672 10.245582580566406 -0.0
CurrentTrain: epoch  0, batch    16 | loss: 21.9643364Losses:  23.553386688232422 12.308975219726562 -0.0
CurrentTrain: epoch  0, batch    17 | loss: 23.5533867Losses:  21.18484115600586 9.508941650390625 -0.0
CurrentTrain: epoch  0, batch    18 | loss: 21.1848412Losses:  19.822509765625 8.525314331054688 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 19.8225098Losses:  19.984619140625 8.64084529876709 -0.0
CurrentTrain: epoch  0, batch    20 | loss: 19.9846191Losses:  20.885822296142578 9.389453887939453 -0.0
CurrentTrain: epoch  0, batch    21 | loss: 20.8858223Losses:  18.637958526611328 7.440783500671387 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 18.6379585Losses:  19.266098022460938 8.254202842712402 -0.0
CurrentTrain: epoch  0, batch    23 | loss: 19.2660980Losses:  19.675519943237305 8.640453338623047 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 19.6755199Losses:  19.238601684570312 8.254600524902344 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 19.2386017Losses:  18.15146255493164 7.421070575714111 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 18.1514626Losses:  19.163156509399414 8.26333999633789 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 19.1631565Losses:  18.294431686401367 7.224070072174072 -0.0
CurrentTrain: epoch  0, batch    28 | loss: 18.2944317Losses:  20.695953369140625 9.95289421081543 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 20.6959534Losses:  18.325578689575195 7.783599853515625 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 18.3255787Losses:  20.318370819091797 9.704907417297363 -0.0
CurrentTrain: epoch  0, batch    31 | loss: 20.3183708Losses:  20.296466827392578 9.511629104614258 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 20.2964668Losses:  16.65624237060547 6.813549995422363 -0.0
CurrentTrain: epoch  0, batch    33 | loss: 16.6562424Losses:  19.93512725830078 9.929121017456055 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 19.9351273Losses:  16.90258026123047 6.673496246337891 -0.0
CurrentTrain: epoch  0, batch    35 | loss: 16.9025803Losses:  16.45724105834961 6.678282737731934 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 16.4572411Losses:  17.454055786132812 6.8517560958862305 -0.0
CurrentTrain: epoch  0, batch    37 | loss: 17.4540558Losses:  20.381195068359375 11.295675277709961 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 20.3811951Losses:  19.06009292602539 9.318599700927734 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 19.0600929Losses:  18.263416290283203 8.724899291992188 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 18.2634163Losses:  19.386802673339844 9.605672836303711 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 19.3868027Losses:  18.03670883178711 8.619522094726562 -0.0
CurrentTrain: epoch  1, batch     4 | loss: 18.0367088Losses:  19.06711196899414 8.849222183227539 -0.0
CurrentTrain: epoch  1, batch     5 | loss: 19.0671120Losses:  19.380001068115234 9.19872760772705 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 19.3800011Losses:  16.684398651123047 6.798305511474609 -0.0
CurrentTrain: epoch  1, batch     7 | loss: 16.6843987Losses:  16.455720901489258 6.743707180023193 -0.0
CurrentTrain: epoch  1, batch     8 | loss: 16.4557209Losses:  21.13551902770996 10.921056747436523 -0.0
CurrentTrain: epoch  1, batch     9 | loss: 21.1355190Losses:  14.456001281738281 5.422758102416992 -0.0
CurrentTrain: epoch  1, batch    10 | loss: 14.4560013Losses:  17.327402114868164 7.44261360168457 -0.0
CurrentTrain: epoch  1, batch    11 | loss: 17.3274021Losses:  15.922784805297852 6.8449883460998535 -0.0
CurrentTrain: epoch  1, batch    12 | loss: 15.9227848Losses:  16.265979766845703 7.548908233642578 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 16.2659798Losses:  17.167293548583984 7.236449718475342 -0.0
CurrentTrain: epoch  1, batch    14 | loss: 17.1672935Losses:  20.313499450683594 10.24233341217041 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 20.3134995Losses:  16.00534439086914 7.017677307128906 -0.0
CurrentTrain: epoch  1, batch    16 | loss: 16.0053444Losses:  15.696922302246094 7.022284507751465 -0.0
CurrentTrain: epoch  1, batch    17 | loss: 15.6969223Losses:  14.472871780395508 5.656208515167236 -0.0
CurrentTrain: epoch  1, batch    18 | loss: 14.4728718Losses:  20.00426483154297 9.944046974182129 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 20.0042648Losses:  16.504074096679688 7.1169819831848145 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 16.5040741Losses:  18.682950973510742 8.757125854492188 -0.0
CurrentTrain: epoch  1, batch    21 | loss: 18.6829510Losses:  17.078453063964844 7.102752685546875 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 17.0784531Losses:  17.040361404418945 7.243803024291992 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 17.0403614Losses:  15.742582321166992 7.587327480316162 -0.0
CurrentTrain: epoch  1, batch    24 | loss: 15.7425823Losses:  17.74484634399414 8.905001640319824 -0.0
CurrentTrain: epoch  1, batch    25 | loss: 17.7448463Losses:  15.902581214904785 6.9571428298950195 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 15.9025812Losses:  16.439373016357422 7.285822868347168 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 16.4393730Losses:  16.634145736694336 7.658799171447754 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 16.6341457Losses:  15.219537734985352 6.114810466766357 -0.0
CurrentTrain: epoch  1, batch    29 | loss: 15.2195377Losses:  15.723243713378906 6.914438247680664 -0.0
CurrentTrain: epoch  1, batch    30 | loss: 15.7232437Losses:  16.197921752929688 6.568758010864258 -0.0
CurrentTrain: epoch  1, batch    31 | loss: 16.1979218Losses:  13.692561149597168 5.180695533752441 -0.0
CurrentTrain: epoch  1, batch    32 | loss: 13.6925611Losses:  20.14268684387207 10.758787155151367 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 20.1426868Losses:  16.721426010131836 7.976132392883301 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 16.7214260Losses:  16.738845825195312 7.432690620422363 -0.0
CurrentTrain: epoch  1, batch    35 | loss: 16.7388458Losses:  15.612972259521484 6.249377250671387 -0.0
CurrentTrain: epoch  1, batch    36 | loss: 15.6129723Losses:  10.137306213378906 1.1480753421783447 -0.0
CurrentTrain: epoch  1, batch    37 | loss: 10.1373062Losses:  18.24471664428711 8.43551254272461 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 18.2447166Losses:  15.423293113708496 6.274007797241211 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 15.4232931Losses:  13.98197078704834 5.634087562561035 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 13.9819708Losses:  16.410200119018555 8.09937858581543 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 16.4102001Losses:  18.179895401000977 10.424896240234375 -0.0
CurrentTrain: epoch  2, batch     4 | loss: 18.1798954Losses:  15.70665454864502 6.341876029968262 -0.0
CurrentTrain: epoch  2, batch     5 | loss: 15.7066545Losses:  14.930179595947266 7.03064489364624 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 14.9301796Losses:  24.022058486938477 14.858057022094727 -0.0
CurrentTrain: epoch  2, batch     7 | loss: 24.0220585Losses:  14.963239669799805 6.996190071105957 -0.0
CurrentTrain: epoch  2, batch     8 | loss: 14.9632397Losses:  15.014070510864258 7.299922466278076 -0.0
CurrentTrain: epoch  2, batch     9 | loss: 15.0140705Losses:  14.959551811218262 5.882063865661621 -0.0
CurrentTrain: epoch  2, batch    10 | loss: 14.9595518Losses:  14.494834899902344 6.905123233795166 -0.0
CurrentTrain: epoch  2, batch    11 | loss: 14.4948349Losses:  16.776268005371094 7.803422927856445 -0.0
CurrentTrain: epoch  2, batch    12 | loss: 16.7762680Losses:  15.895340919494629 7.302384376525879 -0.0
CurrentTrain: epoch  2, batch    13 | loss: 15.8953409Losses:  13.606042861938477 6.014832973480225 -0.0
CurrentTrain: epoch  2, batch    14 | loss: 13.6060429Losses:  14.706069946289062 6.026639938354492 -0.0
CurrentTrain: epoch  2, batch    15 | loss: 14.7060699Losses:  15.371679306030273 7.14411735534668 -0.0
CurrentTrain: epoch  2, batch    16 | loss: 15.3716793Losses:  14.234806060791016 7.1407790184021 -0.0
CurrentTrain: epoch  2, batch    17 | loss: 14.2348061Losses:  15.489463806152344 6.398666858673096 -0.0
CurrentTrain: epoch  2, batch    18 | loss: 15.4894638Losses:  16.37398338317871 7.994564056396484 -0.0
CurrentTrain: epoch  2, batch    19 | loss: 16.3739834Losses:  19.350114822387695 10.298822402954102 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 19.3501148Losses:  13.352670669555664 5.387853622436523 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 13.3526707Losses:  17.40854263305664 8.17170524597168 -0.0
CurrentTrain: epoch  2, batch    22 | loss: 17.4085426Losses:  15.741931915283203 7.8729705810546875 -0.0
CurrentTrain: epoch  2, batch    23 | loss: 15.7419319Losses:  17.387317657470703 8.661996841430664 -0.0
CurrentTrain: epoch  2, batch    24 | loss: 17.3873177Losses:  15.706958770751953 6.54788064956665 -0.0
CurrentTrain: epoch  2, batch    25 | loss: 15.7069588Losses:  14.43546199798584 6.855709552764893 -0.0
CurrentTrain: epoch  2, batch    26 | loss: 14.4354620Losses:  13.245658874511719 5.290529251098633 -0.0
CurrentTrain: epoch  2, batch    27 | loss: 13.2456589Losses:  16.76442527770996 8.784332275390625 -0.0
CurrentTrain: epoch  2, batch    28 | loss: 16.7644253Losses:  13.278382301330566 5.882867813110352 -0.0
CurrentTrain: epoch  2, batch    29 | loss: 13.2783823Losses:  13.329878807067871 5.348448753356934 -0.0
CurrentTrain: epoch  2, batch    30 | loss: 13.3298788Losses:  15.408077239990234 7.453536033630371 -0.0
CurrentTrain: epoch  2, batch    31 | loss: 15.4080772Losses:  19.143997192382812 10.171041488647461 -0.0
CurrentTrain: epoch  2, batch    32 | loss: 19.1439972Losses:  12.488933563232422 5.275278091430664 -0.0
CurrentTrain: epoch  2, batch    33 | loss: 12.4889336Losses:  12.92397689819336 5.45064640045166 -0.0
CurrentTrain: epoch  2, batch    34 | loss: 12.9239769Losses:  12.618223190307617 4.442448616027832 -0.0
CurrentTrain: epoch  2, batch    35 | loss: 12.6182232Losses:  13.274582862854004 5.131014823913574 -0.0
CurrentTrain: epoch  2, batch    36 | loss: 13.2745829Losses:  8.961085319519043 0.7814435958862305 -0.0
CurrentTrain: epoch  2, batch    37 | loss: 8.9610853Losses:  12.804821014404297 4.903292655944824 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 12.8048210Losses:  16.08086395263672 8.565908432006836 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 16.0808640Losses:  14.032307624816895 7.07893180847168 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 14.0323076Losses:  13.021522521972656 5.8403472900390625 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 13.0215225Losses:  20.901405334472656 13.862043380737305 -0.0
CurrentTrain: epoch  3, batch     4 | loss: 20.9014053Losses:  16.140296936035156 8.20403003692627 -0.0
CurrentTrain: epoch  3, batch     5 | loss: 16.1402969Losses:  14.894658088684082 7.5664215087890625 -0.0
CurrentTrain: epoch  3, batch     6 | loss: 14.8946581Losses:  15.101734161376953 6.7000203132629395 -0.0
CurrentTrain: epoch  3, batch     7 | loss: 15.1017342Losses:  13.659565925598145 7.513510704040527 -0.0
CurrentTrain: epoch  3, batch     8 | loss: 13.6595659Losses:  14.33485221862793 6.4413371086120605 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 14.3348522Losses:  16.865081787109375 6.841032028198242 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 16.8650818Losses:  13.173161506652832 6.488730430603027 -0.0
CurrentTrain: epoch  3, batch    11 | loss: 13.1731615Losses:  15.857246398925781 8.158731460571289 -0.0
CurrentTrain: epoch  3, batch    12 | loss: 15.8572464Losses:  20.024600982666016 10.368669509887695 -0.0
CurrentTrain: epoch  3, batch    13 | loss: 20.0246010Losses:  12.129426956176758 4.906142234802246 -0.0
CurrentTrain: epoch  3, batch    14 | loss: 12.1294270Losses:  11.690179824829102 5.261110782623291 -0.0
CurrentTrain: epoch  3, batch    15 | loss: 11.6901798Losses:  13.841838836669922 5.94570779800415 -0.0
CurrentTrain: epoch  3, batch    16 | loss: 13.8418388Losses:  13.490936279296875 6.320021152496338 -0.0
CurrentTrain: epoch  3, batch    17 | loss: 13.4909363Losses:  12.820837020874023 6.2991623878479 -0.0
CurrentTrain: epoch  3, batch    18 | loss: 12.8208370Losses:  12.477435111999512 4.604867935180664 -0.0
CurrentTrain: epoch  3, batch    19 | loss: 12.4774351Losses:  12.120458602905273 4.7441840171813965 -0.0
CurrentTrain: epoch  3, batch    20 | loss: 12.1204586Losses:  12.186615943908691 4.712061882019043 -0.0
CurrentTrain: epoch  3, batch    21 | loss: 12.1866159Losses:  12.7447509765625 6.378388404846191 -0.0
CurrentTrain: epoch  3, batch    22 | loss: 12.7447510Losses:  14.290409088134766 6.452698230743408 -0.0
CurrentTrain: epoch  3, batch    23 | loss: 14.2904091Losses:  14.021768569946289 6.794753551483154 -0.0
CurrentTrain: epoch  3, batch    24 | loss: 14.0217686Losses:  17.087255477905273 9.226881980895996 -0.0
CurrentTrain: epoch  3, batch    25 | loss: 17.0872555Losses:  14.734307289123535 6.7026472091674805 -0.0
CurrentTrain: epoch  3, batch    26 | loss: 14.7343073Losses:  16.287841796875 8.176504135131836 -0.0
CurrentTrain: epoch  3, batch    27 | loss: 16.2878418Losses:  12.843963623046875 5.715476989746094 -0.0
CurrentTrain: epoch  3, batch    28 | loss: 12.8439636Losses:  13.2841796875 6.052608013153076 -0.0
CurrentTrain: epoch  3, batch    29 | loss: 13.2841797Losses:  13.472963333129883 6.985257148742676 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 13.4729633Losses:  14.746441841125488 6.2139997482299805 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 14.7464418Losses:  12.561080932617188 5.242530345916748 -0.0
CurrentTrain: epoch  3, batch    32 | loss: 12.5610809Losses:  14.487512588500977 6.506497859954834 -0.0
CurrentTrain: epoch  3, batch    33 | loss: 14.4875126Losses:  15.524768829345703 7.533795356750488 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 15.5247688Losses:  12.819120407104492 4.962294101715088 -0.0
CurrentTrain: epoch  3, batch    35 | loss: 12.8191204Losses:  16.427221298217773 8.670940399169922 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 16.4272213Losses:  9.151359558105469 1.679168462753296 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 9.1513596Losses:  12.895578384399414 6.298645496368408 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 12.8955784Losses:  16.12151527404785 8.194815635681152 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 16.1215153Losses:  17.835731506347656 8.714999198913574 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 17.8357315Losses:  15.22103500366211 7.521444320678711 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 15.2210350Losses:  11.881725311279297 4.42426872253418 -0.0
CurrentTrain: epoch  4, batch     4 | loss: 11.8817253Losses:  12.536737442016602 5.2080183029174805 -0.0
CurrentTrain: epoch  4, batch     5 | loss: 12.5367374Losses:  15.307175636291504 8.365713119506836 -0.0
CurrentTrain: epoch  4, batch     6 | loss: 15.3071756Losses:  17.629470825195312 10.024322509765625 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 17.6294708Losses:  14.253145217895508 6.952879905700684 -0.0
CurrentTrain: epoch  4, batch     8 | loss: 14.2531452Losses:  14.82872486114502 6.962250709533691 -0.0
CurrentTrain: epoch  4, batch     9 | loss: 14.8287249Losses:  18.019960403442383 11.081796646118164 -0.0
CurrentTrain: epoch  4, batch    10 | loss: 18.0199604Losses:  12.226398468017578 5.085921764373779 -0.0
CurrentTrain: epoch  4, batch    11 | loss: 12.2263985Losses:  15.32364273071289 7.251646041870117 -0.0
CurrentTrain: epoch  4, batch    12 | loss: 15.3236427Losses:  17.933029174804688 10.077249526977539 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 17.9330292Losses:  16.26861000061035 9.213078498840332 -0.0
CurrentTrain: epoch  4, batch    14 | loss: 16.2686100Losses:  11.41666030883789 4.404528617858887 -0.0
CurrentTrain: epoch  4, batch    15 | loss: 11.4166603Losses:  12.16182804107666 4.9034423828125 -0.0
CurrentTrain: epoch  4, batch    16 | loss: 12.1618280Losses:  14.275347709655762 6.225127220153809 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 14.2753477Losses:  15.759881973266602 8.293754577636719 -0.0
CurrentTrain: epoch  4, batch    18 | loss: 15.7598820Losses:  15.294124603271484 7.421688556671143 -0.0
CurrentTrain: epoch  4, batch    19 | loss: 15.2941246Losses:  21.416513442993164 13.50212287902832 -0.0
CurrentTrain: epoch  4, batch    20 | loss: 21.4165134Losses:  11.184516906738281 5.04863166809082 -0.0
CurrentTrain: epoch  4, batch    21 | loss: 11.1845169Losses:  14.824438095092773 7.570204734802246 -0.0
CurrentTrain: epoch  4, batch    22 | loss: 14.8244381Losses:  18.14346694946289 9.840482711791992 -0.0
CurrentTrain: epoch  4, batch    23 | loss: 18.1434669Losses:  12.908806800842285 5.508739948272705 -0.0
CurrentTrain: epoch  4, batch    24 | loss: 12.9088068Losses:  11.267946243286133 4.687262058258057 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 11.2679462Losses:  15.183008193969727 8.376269340515137 -0.0
CurrentTrain: epoch  4, batch    26 | loss: 15.1830082Losses:  16.995065689086914 8.081132888793945 -0.0
CurrentTrain: epoch  4, batch    27 | loss: 16.9950657Losses:  14.876581192016602 7.059000492095947 -0.0
CurrentTrain: epoch  4, batch    28 | loss: 14.8765812Losses:  14.418416976928711 6.9553327560424805 -0.0
CurrentTrain: epoch  4, batch    29 | loss: 14.4184170Losses:  11.981884002685547 4.713536739349365 -0.0
CurrentTrain: epoch  4, batch    30 | loss: 11.9818840Losses:  11.825874328613281 4.5263671875 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 11.8258743Losses:  16.260780334472656 8.343415260314941 -0.0
CurrentTrain: epoch  4, batch    32 | loss: 16.2607803Losses:  11.788308143615723 4.347188949584961 -0.0
CurrentTrain: epoch  4, batch    33 | loss: 11.7883081Losses:  13.309349060058594 6.56547212600708 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 13.3093491Losses:  15.01724624633789 7.761961936950684 -0.0
CurrentTrain: epoch  4, batch    35 | loss: 15.0172462Losses:  16.42084503173828 9.812063217163086 -0.0
CurrentTrain: epoch  4, batch    36 | loss: 16.4208450Losses:  7.756034851074219 1.755218744277954 -0.0
CurrentTrain: epoch  4, batch    37 | loss: 7.7560349Losses:  13.409246444702148 5.679169654846191 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 13.4092464Losses:  13.817543029785156 7.429792404174805 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 13.8175430Losses:  13.73565673828125 5.751019477844238 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 13.7356567Losses:  11.707990646362305 4.780704498291016 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 11.7079906Losses:  15.49931526184082 7.805531024932861 -0.0
CurrentTrain: epoch  5, batch     4 | loss: 15.4993153Losses:  12.132399559020996 4.5393171310424805 -0.0
CurrentTrain: epoch  5, batch     5 | loss: 12.1323996Losses:  13.050909042358398 6.002750396728516 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 13.0509090Losses:  16.446054458618164 9.74105167388916 -0.0
CurrentTrain: epoch  5, batch     7 | loss: 16.4460545Losses:  13.09602165222168 5.9382500648498535 -0.0
CurrentTrain: epoch  5, batch     8 | loss: 13.0960217Losses:  12.503241539001465 4.83836030960083 -0.0
CurrentTrain: epoch  5, batch     9 | loss: 12.5032415Losses:  15.833019256591797 8.670646667480469 -0.0
CurrentTrain: epoch  5, batch    10 | loss: 15.8330193Losses:  14.062981605529785 7.223527908325195 -0.0
CurrentTrain: epoch  5, batch    11 | loss: 14.0629816Losses:  16.57265853881836 8.717435836791992 -0.0
CurrentTrain: epoch  5, batch    12 | loss: 16.5726585Losses:  15.4957857131958 9.308038711547852 -0.0
CurrentTrain: epoch  5, batch    13 | loss: 15.4957857Losses:  12.052688598632812 5.36061954498291 -0.0
CurrentTrain: epoch  5, batch    14 | loss: 12.0526886Losses:  12.662464141845703 5.794265270233154 -0.0
CurrentTrain: epoch  5, batch    15 | loss: 12.6624641Losses:  11.21656322479248 4.334038734436035 -0.0
CurrentTrain: epoch  5, batch    16 | loss: 11.2165632Losses:  13.262273788452148 6.991053581237793 -0.0
CurrentTrain: epoch  5, batch    17 | loss: 13.2622738Losses:  13.289043426513672 6.931439399719238 -0.0
CurrentTrain: epoch  5, batch    18 | loss: 13.2890434Losses:  11.373956680297852 5.37790060043335 -0.0
CurrentTrain: epoch  5, batch    19 | loss: 11.3739567Losses:  10.669967651367188 4.622570991516113 -0.0
CurrentTrain: epoch  5, batch    20 | loss: 10.6699677Losses:  11.944680213928223 5.7352495193481445 -0.0
CurrentTrain: epoch  5, batch    21 | loss: 11.9446802Losses:  10.658782958984375 4.852657318115234 -0.0
CurrentTrain: epoch  5, batch    22 | loss: 10.6587830Losses:  14.891725540161133 7.18304443359375 -0.0
CurrentTrain: epoch  5, batch    23 | loss: 14.8917255Losses:  11.936246871948242 5.031383514404297 -0.0
CurrentTrain: epoch  5, batch    24 | loss: 11.9362469Losses:  11.49610710144043 4.622878074645996 -0.0
CurrentTrain: epoch  5, batch    25 | loss: 11.4961071Losses:  11.917818069458008 6.076111316680908 -0.0
CurrentTrain: epoch  5, batch    26 | loss: 11.9178181Losses:  12.472736358642578 5.096583843231201 -0.0
CurrentTrain: epoch  5, batch    27 | loss: 12.4727364Losses:  11.911494255065918 5.8174333572387695 -0.0
CurrentTrain: epoch  5, batch    28 | loss: 11.9114943Losses:  12.391286849975586 5.207123756408691 -0.0
CurrentTrain: epoch  5, batch    29 | loss: 12.3912868Losses:  16.014835357666016 9.556907653808594 -0.0
CurrentTrain: epoch  5, batch    30 | loss: 16.0148354Losses:  13.79148006439209 6.9423627853393555 -0.0
CurrentTrain: epoch  5, batch    31 | loss: 13.7914801Losses:  12.51889419555664 6.193131446838379 -0.0
CurrentTrain: epoch  5, batch    32 | loss: 12.5188942Losses:  11.076225280761719 5.0842437744140625 -0.0
CurrentTrain: epoch  5, batch    33 | loss: 11.0762253Losses:  10.744729042053223 4.743900299072266 -0.0
CurrentTrain: epoch  5, batch    34 | loss: 10.7447290Losses:  12.0747652053833 6.32326602935791 -0.0
CurrentTrain: epoch  5, batch    35 | loss: 12.0747652Losses:  12.362241744995117 5.997008323669434 -0.0
CurrentTrain: epoch  5, batch    36 | loss: 12.3622417Losses:  8.878363609313965 1.9567316770553589 -0.0
CurrentTrain: epoch  5, batch    37 | loss: 8.8783636Losses:  10.711009979248047 5.359717845916748 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 10.7110100Losses:  13.155229568481445 7.081287860870361 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 13.1552296Losses:  10.960002899169922 5.373008728027344 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 10.9600029Losses:  14.185426712036133 8.100461959838867 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 14.1854267Losses:  11.512994766235352 5.646409511566162 -0.0
CurrentTrain: epoch  6, batch     4 | loss: 11.5129948Losses:  15.103605270385742 9.915628433227539 -0.0
CurrentTrain: epoch  6, batch     5 | loss: 15.1036053Losses:  12.212606430053711 6.301388740539551 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 12.2126064Losses:  15.396376609802246 9.732213973999023 -0.0
CurrentTrain: epoch  6, batch     7 | loss: 15.3963766Losses:  12.010009765625 6.211883544921875 -0.0
CurrentTrain: epoch  6, batch     8 | loss: 12.0100098Losses:  9.277349472045898 4.042079925537109 -0.0
CurrentTrain: epoch  6, batch     9 | loss: 9.2773495Losses:  10.127820014953613 4.45655632019043 -0.0
CurrentTrain: epoch  6, batch    10 | loss: 10.1278200Losses:  10.526518821716309 4.658559799194336 -0.0
CurrentTrain: epoch  6, batch    11 | loss: 10.5265188Losses:  13.411388397216797 7.512563228607178 -0.0
CurrentTrain: epoch  6, batch    12 | loss: 13.4113884Losses:  15.573151588439941 8.345254898071289 -0.0
CurrentTrain: epoch  6, batch    13 | loss: 15.5731516Losses:  12.198137283325195 5.650372505187988 -0.0
CurrentTrain: epoch  6, batch    14 | loss: 12.1981373Losses:  14.60083293914795 7.724880218505859 -0.0
CurrentTrain: epoch  6, batch    15 | loss: 14.6008329Losses:  11.880632400512695 6.048518180847168 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 11.8806324Losses:  10.71877670288086 4.909476280212402 -0.0
CurrentTrain: epoch  6, batch    17 | loss: 10.7187767Losses:  10.34915828704834 4.9680280685424805 -0.0
CurrentTrain: epoch  6, batch    18 | loss: 10.3491583Losses:  10.938426971435547 4.809149265289307 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 10.9384270Losses:  11.770676612854004 5.9481353759765625 -0.0
CurrentTrain: epoch  6, batch    20 | loss: 11.7706766Losses:  15.889522552490234 9.542884826660156 -0.0
CurrentTrain: epoch  6, batch    21 | loss: 15.8895226Losses:  10.864778518676758 5.371826171875 -0.0
CurrentTrain: epoch  6, batch    22 | loss: 10.8647785Losses:  12.128449440002441 6.716811180114746 -0.0
CurrentTrain: epoch  6, batch    23 | loss: 12.1284494Losses:  10.190590858459473 4.893432140350342 -0.0
CurrentTrain: epoch  6, batch    24 | loss: 10.1905909Losses:  9.953263282775879 4.819075584411621 -0.0
CurrentTrain: epoch  6, batch    25 | loss: 9.9532633Losses:  12.393287658691406 6.59173583984375 -0.0
CurrentTrain: epoch  6, batch    26 | loss: 12.3932877Losses:  12.491508483886719 6.518134593963623 -0.0
CurrentTrain: epoch  6, batch    27 | loss: 12.4915085Losses:  11.54716682434082 5.043035507202148 -0.0
CurrentTrain: epoch  6, batch    28 | loss: 11.5471668Losses:  10.717775344848633 4.726118087768555 -0.0
CurrentTrain: epoch  6, batch    29 | loss: 10.7177753Losses:  11.786169052124023 5.405961036682129 -0.0
CurrentTrain: epoch  6, batch    30 | loss: 11.7861691Losses:  10.573928833007812 4.249849319458008 -0.0
CurrentTrain: epoch  6, batch    31 | loss: 10.5739288Losses:  10.82194995880127 5.367166519165039 -0.0
CurrentTrain: epoch  6, batch    32 | loss: 10.8219500Losses:  12.325237274169922 6.317102432250977 -0.0
CurrentTrain: epoch  6, batch    33 | loss: 12.3252373Losses:  14.345455169677734 9.009013175964355 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 14.3454552Losses:  11.11415958404541 4.805153846740723 -0.0
CurrentTrain: epoch  6, batch    35 | loss: 11.1141596Losses:  9.798698425292969 4.221251487731934 -0.0
CurrentTrain: epoch  6, batch    36 | loss: 9.7986984Losses:  6.5990424156188965 0.7091041803359985 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 6.5990424Losses:  10.58444595336914 4.96091890335083 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 10.5844460Losses:  15.788979530334473 9.317452430725098 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 15.7889795Losses:  11.905956268310547 6.880218505859375 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 11.9059563Losses:  10.735551834106445 5.262089729309082 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 10.7355518Losses:  12.30662727355957 7.035979747772217 -0.0
CurrentTrain: epoch  7, batch     4 | loss: 12.3066273Losses:  9.59817123413086 3.9448204040527344 -0.0
CurrentTrain: epoch  7, batch     5 | loss: 9.5981712Losses:  10.7433443069458 5.532039642333984 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 10.7433443Losses:  13.203927993774414 6.625510215759277 -0.0
CurrentTrain: epoch  7, batch     7 | loss: 13.2039280Losses:  9.295732498168945 3.874781370162964 -0.0
CurrentTrain: epoch  7, batch     8 | loss: 9.2957325Losses:  8.909268379211426 3.5651159286499023 -0.0
CurrentTrain: epoch  7, batch     9 | loss: 8.9092684Losses:  9.734643936157227 4.143707752227783 -0.0
CurrentTrain: epoch  7, batch    10 | loss: 9.7346439Losses:  11.819806098937988 6.145230293273926 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 11.8198061Losses:  12.0045166015625 6.891396522521973 -0.0
CurrentTrain: epoch  7, batch    12 | loss: 12.0045166Losses:  9.736151695251465 4.674696445465088 -0.0
CurrentTrain: epoch  7, batch    13 | loss: 9.7361517Losses:  12.470322608947754 7.399245262145996 -0.0
CurrentTrain: epoch  7, batch    14 | loss: 12.4703226Losses:  11.271002769470215 4.889721870422363 -0.0
CurrentTrain: epoch  7, batch    15 | loss: 11.2710028Losses:  10.929521560668945 4.685981750488281 -0.0
CurrentTrain: epoch  7, batch    16 | loss: 10.9295216Losses:  11.809407234191895 6.939249038696289 -0.0
CurrentTrain: epoch  7, batch    17 | loss: 11.8094072Losses:  12.89156723022461 7.640810012817383 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 12.8915672Losses:  9.272822380065918 3.94547176361084 -0.0
CurrentTrain: epoch  7, batch    19 | loss: 9.2728224Losses:  9.29234504699707 4.2333831787109375 -0.0
CurrentTrain: epoch  7, batch    20 | loss: 9.2923450Losses:  11.108423233032227 5.696649551391602 -0.0
CurrentTrain: epoch  7, batch    21 | loss: 11.1084232Losses:  15.735929489135742 9.585638046264648 -0.0
CurrentTrain: epoch  7, batch    22 | loss: 15.7359295Losses:  11.600167274475098 4.652008533477783 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 11.6001673Losses:  13.665746688842773 6.710949897766113 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 13.6657467Losses:  11.787551879882812 5.721492767333984 -0.0
CurrentTrain: epoch  7, batch    25 | loss: 11.7875519Losses:  13.770223617553711 7.545192718505859 -0.0
CurrentTrain: epoch  7, batch    26 | loss: 13.7702236Losses:  10.949350357055664 5.2140607833862305 -0.0
CurrentTrain: epoch  7, batch    27 | loss: 10.9493504Losses:  12.528526306152344 6.238684177398682 -0.0
CurrentTrain: epoch  7, batch    28 | loss: 12.5285263Losses:  15.279150009155273 8.806367874145508 -0.0
CurrentTrain: epoch  7, batch    29 | loss: 15.2791500Losses:  14.052558898925781 7.987963676452637 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 14.0525589Losses:  10.479646682739258 5.094852447509766 -0.0
CurrentTrain: epoch  7, batch    31 | loss: 10.4796467Losses:  9.978544235229492 4.753640174865723 -0.0
CurrentTrain: epoch  7, batch    32 | loss: 9.9785442Losses:  13.52267837524414 8.161310195922852 -0.0
CurrentTrain: epoch  7, batch    33 | loss: 13.5226784Losses:  11.76198959350586 5.375552177429199 -0.0
CurrentTrain: epoch  7, batch    34 | loss: 11.7619896Losses:  9.700983047485352 3.9507594108581543 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 9.7009830Losses:  9.760790824890137 3.902501106262207 -0.0
CurrentTrain: epoch  7, batch    36 | loss: 9.7607908Losses:  6.756258964538574 1.305598497390747 -0.0
CurrentTrain: epoch  7, batch    37 | loss: 6.7562590Losses:  11.474027633666992 5.579381942749023 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 11.4740276Losses:  9.788270950317383 4.550431728363037 -0.0
CurrentTrain: epoch  8, batch     1 | loss: 9.7882710Losses:  11.617603302001953 6.279258728027344 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 11.6176033Losses:  13.03868293762207 7.868703365325928 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 13.0386829Losses:  12.484430313110352 6.915411949157715 -0.0
CurrentTrain: epoch  8, batch     4 | loss: 12.4844303Losses:  11.073627471923828 5.281923294067383 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 11.0736275Losses:  17.531251907348633 12.243749618530273 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 17.5312519Losses:  11.916590690612793 6.122714996337891 -0.0
CurrentTrain: epoch  8, batch     7 | loss: 11.9165907Losses:  13.74070930480957 8.389230728149414 -0.0
CurrentTrain: epoch  8, batch     8 | loss: 13.7407093Losses:  10.392658233642578 5.159329414367676 -0.0
CurrentTrain: epoch  8, batch     9 | loss: 10.3926582Losses:  12.809946060180664 6.818310260772705 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 12.8099461Losses:  8.739596366882324 3.421377182006836 -0.0
CurrentTrain: epoch  8, batch    11 | loss: 8.7395964Losses:  11.024864196777344 5.959043502807617 -0.0
CurrentTrain: epoch  8, batch    12 | loss: 11.0248642Losses:  9.92501163482666 4.629880905151367 -0.0
CurrentTrain: epoch  8, batch    13 | loss: 9.9250116Losses:  9.568029403686523 4.222970962524414 -0.0
CurrentTrain: epoch  8, batch    14 | loss: 9.5680294Losses:  13.260688781738281 7.564165115356445 -0.0
CurrentTrain: epoch  8, batch    15 | loss: 13.2606888Losses:  9.986213684082031 4.643260955810547 -0.0
CurrentTrain: epoch  8, batch    16 | loss: 9.9862137Losses:  14.192609786987305 6.676726341247559 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 14.1926098Losses:  11.49717903137207 5.147708892822266 -0.0
CurrentTrain: epoch  8, batch    18 | loss: 11.4971790Losses:  9.22750186920166 4.256185531616211 -0.0
CurrentTrain: epoch  8, batch    19 | loss: 9.2275019Losses:  10.279733657836914 5.083927154541016 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 10.2797337Losses:  13.374641418457031 6.799813270568848 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 13.3746414Losses:  10.310775756835938 5.096432685852051 -0.0
CurrentTrain: epoch  8, batch    22 | loss: 10.3107758Losses:  11.904264450073242 6.107443809509277 -0.0
CurrentTrain: epoch  8, batch    23 | loss: 11.9042645Losses:  10.101903915405273 4.1036152839660645 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 10.1019039Losses:  10.357728958129883 4.960041522979736 -0.0
CurrentTrain: epoch  8, batch    25 | loss: 10.3577290Losses:  12.685312271118164 7.356993198394775 -0.0
CurrentTrain: epoch  8, batch    26 | loss: 12.6853123Losses:  11.633023262023926 5.703586578369141 -0.0
CurrentTrain: epoch  8, batch    27 | loss: 11.6330233Losses:  11.079076766967773 5.670795917510986 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 11.0790768Losses:  9.519614219665527 4.261394500732422 -0.0
CurrentTrain: epoch  8, batch    29 | loss: 9.5196142Losses:  9.76527214050293 4.257160186767578 -0.0
CurrentTrain: epoch  8, batch    30 | loss: 9.7652721Losses:  11.578893661499023 6.5875163078308105 -0.0
CurrentTrain: epoch  8, batch    31 | loss: 11.5788937Losses:  11.645340919494629 6.860275745391846 -0.0
CurrentTrain: epoch  8, batch    32 | loss: 11.6453409Losses:  10.123196601867676 4.222631931304932 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 10.1231966Losses:  11.440006256103516 5.430340766906738 -0.0
CurrentTrain: epoch  8, batch    34 | loss: 11.4400063Losses:  14.24073314666748 9.22948169708252 -0.0
CurrentTrain: epoch  8, batch    35 | loss: 14.2407331Losses:  9.187379837036133 4.234480857849121 -0.0
CurrentTrain: epoch  8, batch    36 | loss: 9.1873798Losses:  8.884195327758789 2.7990660667419434 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 8.8841953Losses:  10.801645278930664 5.712587356567383 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 10.8016453Losses:  10.922149658203125 5.637494087219238 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 10.9221497Losses:  14.288619041442871 9.079078674316406 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 14.2886190Losses:  10.93037223815918 6.045205593109131 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 10.9303722Losses:  12.216573715209961 7.299242973327637 -0.0
CurrentTrain: epoch  9, batch     4 | loss: 12.2165737Losses:  9.56675910949707 4.702901840209961 -0.0
CurrentTrain: epoch  9, batch     5 | loss: 9.5667591Losses:  10.512578010559082 5.635524749755859 -0.0
CurrentTrain: epoch  9, batch     6 | loss: 10.5125780Losses:  15.286626815795898 10.322493553161621 -0.0
CurrentTrain: epoch  9, batch     7 | loss: 15.2866268Losses:  14.367057800292969 9.40170669555664 -0.0
CurrentTrain: epoch  9, batch     8 | loss: 14.3670578Losses:  9.436938285827637 4.630062103271484 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 9.4369383Losses:  13.4822998046875 7.71198034286499 -0.0
CurrentTrain: epoch  9, batch    10 | loss: 13.4822998Losses:  10.0933198928833 4.946422576904297 -0.0
CurrentTrain: epoch  9, batch    11 | loss: 10.0933199Losses:  11.798640251159668 6.549077033996582 -0.0
CurrentTrain: epoch  9, batch    12 | loss: 11.7986403Losses:  9.144564628601074 4.167695045471191 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 9.1445646Losses:  11.380499839782715 6.533013343811035 -0.0
CurrentTrain: epoch  9, batch    14 | loss: 11.3804998Losses:  10.170808792114258 4.90360164642334 -0.0
CurrentTrain: epoch  9, batch    15 | loss: 10.1708088Losses:  10.749534606933594 5.883183002471924 -0.0
CurrentTrain: epoch  9, batch    16 | loss: 10.7495346Losses:  11.549870491027832 6.542293548583984 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 11.5498705Losses:  10.05510139465332 4.922774314880371 -0.0
CurrentTrain: epoch  9, batch    18 | loss: 10.0551014Losses:  9.550199508666992 4.623327255249023 -0.0
CurrentTrain: epoch  9, batch    19 | loss: 9.5501995Losses:  8.29977798461914 3.4041242599487305 -0.0
CurrentTrain: epoch  9, batch    20 | loss: 8.2997780Losses:  10.830816268920898 5.9042582511901855 -0.0
CurrentTrain: epoch  9, batch    21 | loss: 10.8308163Losses:  10.185259819030762 5.46116828918457 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 10.1852598Losses:  11.531781196594238 6.7462239265441895 -0.0
CurrentTrain: epoch  9, batch    23 | loss: 11.5317812Losses:  10.872690200805664 6.132818698883057 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 10.8726902Losses:  9.528116226196289 4.660893440246582 -0.0
CurrentTrain: epoch  9, batch    25 | loss: 9.5281162Losses:  8.909383773803711 4.139741897583008 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 8.9093838Losses:  10.100753784179688 5.5040602684021 -0.0
CurrentTrain: epoch  9, batch    27 | loss: 10.1007538Losses:  10.71240234375 6.0530595779418945 -0.0
CurrentTrain: epoch  9, batch    28 | loss: 10.7124023Losses:  11.477917671203613 6.582787036895752 -0.0
CurrentTrain: epoch  9, batch    29 | loss: 11.4779177Losses:  9.99652099609375 5.058981895446777 -0.0
CurrentTrain: epoch  9, batch    30 | loss: 9.9965210Losses:  8.93884563446045 4.250591278076172 -0.0
CurrentTrain: epoch  9, batch    31 | loss: 8.9388456Losses:  14.932568550109863 10.272768020629883 -0.0
CurrentTrain: epoch  9, batch    32 | loss: 14.9325686Losses:  9.176909446716309 4.536078929901123 -0.0
CurrentTrain: epoch  9, batch    33 | loss: 9.1769094Losses:  12.991682052612305 7.307877540588379 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 12.9916821Losses:  10.1267671585083 5.423943519592285 -0.0
CurrentTrain: epoch  9, batch    35 | loss: 10.1267672Losses:  10.82147216796875 6.0008368492126465 -0.0
CurrentTrain: epoch  9, batch    36 | loss: 10.8214722Losses:  5.104595184326172 0.46295061707496643 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 5.1045952
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: khamenei , 67 , has final say on all state matters in iran as supreme leader , a post he has held since 1989 .
Head Entity: khamenei
Tail Entity: iran
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the United States, Maria decided to return to her roots in Mexico.  
Head Entity: Maria  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: The famous actor, Leonardo DiCaprio, has spent a significant amount of time in both the United States and Australia.  
Head Entity: Leonardo DiCaprio  
Tail Entity: Australia  

Relation: person countries of residence  
Context: Following his promotion, Ahmed moved from Egypt to Saudi Arabia for his new job.  
Head Entity: Ahmed  
Tail Entity: Saudi Arabia  

Relation: person countries of residence  
Context: Despite being born in Canada, she has made her home in France for the past decade.  
Head Entity: She  
Tail Entity: France  

Relation: person countries of residence  
Context: After retiring, John and his wife decided to settle in Spain, where they enjoy the warm climate.  
Head Entity: John  
Tail Entity: Spain  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` since the salinas episode , the bank 's policy on accepting public figures as customers has toughened , '' said tom lahiff , a vice president and lawyer for citibank .
Head Entity: citibank
Tail Entity: tom lahiff
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
Relation: organization top members employees  
Context: `` during the annual meeting, the board of directors praised the contributions of their chief technology officer, who has been instrumental in driving innovation at the company, '' said a spokesperson for tech solutions inc.  
Head Entity: tech solutions inc.  
Tail Entity: chief technology officer  

Relation: organization top members employees  
Context: `` the new marketing strategy was presented by the chief marketing officer, who emphasized the importance of customer engagement, '' reported the press release from global brands ltd.  
Head Entity: global brands ltd.  
Tail Entity: chief marketing officer  

Relation: organization top members employees  
Context: `` after the merger, the chief financial officer outlined the financial benefits to shareholders during the quarterly earnings call, '' stated a representative from united finance group.  
Head Entity: united finance group  
Tail Entity: chief financial officer  

Relation: organization top members employees  
Context: `` the founder of the startup shared insights on leadership and company culture at the tech conference, highlighting the role of the head of operations, '' noted an attendee from innovative tech.  
Head Entity: innovative tech  
Tail Entity: head of operations  

Relation: organization top members employees  
Context: `` the president of the non-profit organization announced a new initiative to support local communities, with the executive director leading the charge, '' said a volunteer at community uplift.  
Head Entity: community uplift  
Tail Entity: executive director  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: on april 12 ipsco was valued at about 6.13 billion dollars on the new york stock exchange and the company last year produced about four million tonnes of steel .
Head Entity: ipsco
Tail Entity: new york stock exchange
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: In 2021, the International Federation of Red Cross and Red Crescent Societies announced that the American Red Cross had joined as a member organization to enhance disaster response efforts globally.  
Head Entity: American Red Cross  
Tail Entity: International Federation of Red Cross and Red Crescent Societies  

Relation: organization member of  
Context: The European Union has welcomed several new member states, including Croatia, which officially joined the organization in 2013.  
Head Entity: Croatia  
Tail Entity: European Union  

Relation: organization member of  
Context: The World Health Organization held a summit where the National Health Service was recognized as a key member organization contributing to global health initiatives.  
Head Entity: National Health Service  
Tail Entity: World Health Organization  

Relation: organization member of  
Context: The United Nations announced that the World Wildlife Fund has been an active member organization in promoting sustainable development and conservation efforts.  
Head Entity: World Wildlife Fund  
Tail Entity: United Nations  

Relation: organization member of  
Context: The International Olympic Committee confirmed that the United States Olympic and Paralympic Committee is a member organization that plays a crucial role in the Olympic movement.  
Head Entity: United States Olympic and Paralympic Committee  
Tail Entity: International Olympic Committee  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: tehran , may 6 -lrb- xinhua -rrb- `` the ban will be effective until the arab-language news network apologizes for the insult , '' iranian majlis speaker gholam ali haddad adel was quoted as saying .
Head Entity: gholam ali haddad adel
Tail Entity: iranian
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: During the international conference, the renowned scientist from Tokyo shared his groundbreaking research on climate change.  
Head Entity: Dr. Hiroshi Tanaka  
Tail Entity: Japanese  

Relation: person origin  
Context: The famous artist showcased her latest collection in Paris, drawing inspiration from her homeland.  
Head Entity: Marie Dupont  
Tail Entity: French  

Relation: person origin  
Context: At the summit, the president emphasized the importance of cultural heritage and its impact on national identity.  
Head Entity: Carlos Mendoza  
Tail Entity: Mexican  

Relation: person origin  
Context: The documentary highlighted the life of a young athlete who overcame numerous challenges to represent his country on the world stage.  
Head Entity: Amina El-Sayed  
Tail Entity: Egyptian  

Relation: person origin  
Context: In a recent interview, the author discussed how her upbringing in a small village influenced her writing style.  
Head Entity: Priya Sharma  
Tail Entity: Indian  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: marcel ospel , the bank 's former chairman who stepped down earlier this year amid heavy criticism of ubs ' staggering losses on its us subprime home loan exposure , accounted for more than two thirds of the sum , pr agency balanx said in a statement tuesday .
Head Entity: marcel ospel
Tail Entity: chairman
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: After years of dedicated service, Dr. Emily Carter was honored as the chief medical officer of the hospital, a role she embraced with great enthusiasm.  
Head Entity: Dr. Emily Carter  
Tail Entity: chief medical officer  

Relation: person title  
Context: During the annual awards ceremony, John Smith was recognized as the lead engineer for his innovative contributions to the project.  
Head Entity: John Smith  
Tail Entity: lead engineer  

Relation: person title  
Context: In her new role, Sarah Johnson will serve as the director of marketing, where she plans to implement several groundbreaking strategies.  
Head Entity: Sarah Johnson  
Tail Entity: director of marketing  

Relation: person title  
Context: The board of directors appointed Michael Lee as the chief financial officer, a position that will require him to oversee the company's financial health.  
Head Entity: Michael Lee  
Tail Entity: chief financial officer  

Relation: person title  
Context: As the founder of the organization, Lisa Brown has taken on the title of executive director, guiding the mission and vision of the nonprofit.  
Head Entity: Lisa Brown  
Tail Entity: executive director  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: the credit crisis spread to the largest us bond insurer thursday , sending shares of mbia inc plunging and calling into question the safety of tens of billions of dollars of company and local government debt held by investors .
Head Entity: mbia
Tail Entity: us
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: After years of expansion, the tech giant Google announced its plans to establish a new headquarters in the vibrant city of Dublin, Ireland, to better serve its European clients.  
Head Entity: Google  
Tail Entity: Ireland  

Relation: organization country of headquarters  
Context: The renowned automotive manufacturer Toyota has its main office located in Toyota City, Japan, where it oversees operations for its global production.  
Head Entity: Toyota  
Tail Entity: Japan  

Relation: organization country of headquarters  
Context: The financial services firm HSBC has its headquarters situated in London, England, making it one of the largest banking and financial services organizations in the world.  
Head Entity: HSBC  
Tail Entity: England  

Relation: organization country of headquarters  
Context: The multinational corporation Nestlé, known for its food and beverage products, is headquartered in Vevey, Switzerland, where it was founded over a century ago.  
Head Entity: Nestlé  
Tail Entity: Switzerland  

Relation: organization country of headquarters  
Context: The software company SAP, which specializes in enterprise software, has its global headquarters in Walldorf, Germany, serving clients worldwide.  
Head Entity: SAP  
Tail Entity: Germany  
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.98%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 90.62%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 81.56%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 85.98%   
cur_acc:  ['0.8598']
his_acc:  ['0.8598']
Clustering into  4  clusters
Clusters:  [1 0 0 1 0 1 3 0 1 2 3]
Losses:  15.357600212097168 7.6893486976623535 3.467343330383301
CurrentTrain: epoch  0, batch     0 | loss: 15.3576002Losses:  13.091806411743164 4.24521541595459 3.388796806335449
CurrentTrain: epoch  0, batch     1 | loss: 13.0918064Losses:  13.478057861328125 6.183023452758789 3.3486785888671875
CurrentTrain: epoch  1, batch     0 | loss: 13.4780579Losses:  10.805418014526367 2.026353597640991 3.4480128288269043
CurrentTrain: epoch  1, batch     1 | loss: 10.8054180Losses:  15.259370803833008 7.701014041900635 3.401419162750244
CurrentTrain: epoch  2, batch     0 | loss: 15.2593708Losses:  11.228920936584473 3.5136826038360596 3.3480308055877686
CurrentTrain: epoch  2, batch     1 | loss: 11.2289209Losses:  14.070706367492676 7.045550346374512 3.3632962703704834
CurrentTrain: epoch  3, batch     0 | loss: 14.0707064Losses:  8.764596939086914 2.05414080619812 3.340742588043213
CurrentTrain: epoch  3, batch     1 | loss: 8.7645969Losses:  12.701933860778809 6.659603118896484 3.3198070526123047
CurrentTrain: epoch  4, batch     0 | loss: 12.7019339Losses:  8.60973072052002 2.2871487140655518 3.353888988494873
CurrentTrain: epoch  4, batch     1 | loss: 8.6097307Losses:  12.322022438049316 6.196949481964111 3.310220718383789
CurrentTrain: epoch  5, batch     0 | loss: 12.3220224Losses:  7.276000499725342 1.7318141460418701 3.3392510414123535
CurrentTrain: epoch  5, batch     1 | loss: 7.2760005Losses:  11.90665340423584 6.468462944030762 3.316610097885132
CurrentTrain: epoch  6, batch     0 | loss: 11.9066534Losses:  9.440690994262695 3.1986687183380127 3.3841516971588135
CurrentTrain: epoch  6, batch     1 | loss: 9.4406910Losses:  12.07500171661377 6.624835968017578 3.3090524673461914
CurrentTrain: epoch  7, batch     0 | loss: 12.0750017Losses:  8.620586395263672 2.918323516845703 3.370368480682373
CurrentTrain: epoch  7, batch     1 | loss: 8.6205864Losses:  11.76339340209961 6.260202884674072 3.311979293823242
CurrentTrain: epoch  8, batch     0 | loss: 11.7633934Losses:  5.252200603485107 1.9106422662734985 1.3891657590866089
CurrentTrain: epoch  8, batch     1 | loss: 5.2522006Losses:  10.768877029418945 5.563115119934082 3.303591728210449
CurrentTrain: epoch  9, batch     0 | loss: 10.7688770Losses:  7.152389049530029 1.788171410560608 3.3425121307373047
CurrentTrain: epoch  9, batch     1 | loss: 7.1523890
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: mr scheider had suffered from multiple myeloma for several years , and died of complications from a staph infection , his wife , brenda siemer , said .
Head Entity: mr scheider
Tail Entity: complications from a staph infection
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: after battling lung cancer for over a year, john doe passed away peacefully in his sleep, surrounded by family.  
Head Entity: john doe  
Tail Entity: lung cancer  

Relation: person cause of death  
Context: the renowned author, who had been diagnosed with heart disease, succumbed to a heart attack last night.  
Head Entity: the renowned author  
Tail Entity: heart attack  

Relation: person cause of death  
Context: following a long struggle with diabetes, mary jane lost her life due to complications related to the disease.  
Head Entity: mary jane  
Tail Entity: complications related to diabetes  

Relation: person cause of death  
Context: after a tragic accident on the highway, the young athlete was pronounced dead at the scene.  
Head Entity: the young athlete  
Tail Entity: tragic accident on the highway  

Relation: person cause of death  
Context: the famous actor died from a drug overdose, leaving behind a legacy of unforgettable performances.  
Head Entity: the famous actor  
Tail Entity: drug overdose  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the militia has also suffered from allegations of being involved in clashes with rival shiite militias , particularly the badr corp , the military wing linked to supreme iraqi islamic council , the party led by powerful politician abdel aziz al-hakim .
Head Entity: badr corp
Tail Entity: shiite
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
Relation: organization political religious affiliation  
Context: The Christian Democratic Union has been a significant player in German politics, often aligning itself with various religious groups to promote its agenda.  
Head Entity: Christian Democratic Union  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Islamic Society of North America has been actively involved in political advocacy, representing the interests of the Muslim community in the United States.  
Head Entity: Islamic Society of North America  
Tail Entity: Muslim  

Relation: organization political religious affiliation  
Context: The National Council of Churches has historically worked to influence legislation in favor of various Christian denominations across the country.  
Head Entity: National Council of Churches  
Tail Entity: Christian  

Relation: organization political religious affiliation  
Context: The Jewish Federation of North America plays a crucial role in advocating for policies that benefit the Jewish community and its interests.  
Head Entity: Jewish Federation of North America  
Tail Entity: Jewish  

Relation: organization political religious affiliation  
Context: The Hindu American Foundation has been instrumental in promoting Hindu values and interests within the political landscape of the United States.  
Head Entity: Hindu American Foundation  
Tail Entity: Hindu  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: based in armonk , new york , mbia insures $ 670 billion -lrb- euro452 .18 billion -rrb- in debt .
Head Entity: mbia
Tail Entity: new york
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the tech giant apple inc. has its headquarters in cupertino, california, where it develops innovative products.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: headquartered in seattle, washington, amazon.com, inc. is a leading e-commerce and cloud computing company.  
Head Entity: amazon.com, inc.  
Tail Entity: washington  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation samsung electronics is based in suwon, south korea, and is known for its electronics and technology.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization stateorprovince of headquarters  
Context: headquartered in redmond, washington, microsoft corporation is a major player in software development and technology solutions.  
Head Entity: microsoft corporation  
Tail Entity: washington  

Relation: organization stateorprovince of headquarters  
Context: the global automotive manufacturer toyota motor corporation is headquartered in toyota city, aichi prefecture, japan.  
Head Entity: toyota motor corporation  
Tail Entity: japan  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: parren mitchell 's sister-in-law , juanita jackson mitchell , was the long - time head and legal counsel of the maryland naacp .
Head Entity: parren mitchell
Tail Entity: juanita jackson mitchell
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: barack obama's half-sister, maya soetoro-ng, is a prominent educator and author.  
Head Entity: barack obama  
Tail Entity: maya soetoro-ng  

Relation: person other family  
Context: the famous actor, tom hanks, has a brother named jim hanks who is also involved in the film industry.  
Head Entity: tom hanks  
Tail Entity: jim hanks  

Relation: person other family  
Context: queen elizabeth ii's cousin, prince michael of kent, often attends royal events and ceremonies.  
Head Entity: queen elizabeth ii  
Tail Entity: prince michael of kent  

Relation: person other family  
Context: serena williams often credits her older sister, venus williams, for inspiring her tennis career.  
Head Entity: serena williams  
Tail Entity: venus williams  

Relation: person other family  
Context: the renowned scientist, albert einstein, had a sister named maria einstein who was a talented musician.  
Head Entity: albert einstein  
Tail Entity: maria einstein  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment in new york city after a long battle with cancer.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: the famous musician, elena rodriguez, died unexpectedly on july 12 while visiting friends in los angeles, california.  
Head Entity: elena rodriguez  
Tail Entity: los angeles  

Relation: person city of death  
Context: after a long and fulfilling life, grandmother edith jones left this world on january 15 in her hometown of boston, massachusetts.  
Head Entity: edith jones  
Tail Entity: boston  

Relation: person city of death  
Context: the beloved actor, michael thompson, tragically passed away on february 20 in a hotel in las vegas, nevada, while attending a film festival.  
Head Entity: michael thompson  
Tail Entity: las vegas  

Relation: person city of death  
Context: renowned scientist dr. alice green died on april 30 in her laboratory in san francisco, california, leaving behind a legacy of groundbreaking research.  
Head Entity: dr. alice green  
Tail Entity: san francisco  
Losses:  12.926534652709961 2.536301374435425 1.6592669486999512
MemoryTrain:  epoch  0, batch     0 | loss: 12.9265347Losses:  14.571514129638672 4.136096954345703 3.6181082725524902
MemoryTrain:  epoch  0, batch     1 | loss: 14.5715141Losses:  12.438983917236328 1.8212218284606934 3.5457301139831543
MemoryTrain:  epoch  0, batch     2 | loss: 12.4389839Losses:  13.882993698120117 3.52993106842041 3.495908260345459
MemoryTrain:  epoch  0, batch     3 | loss: 13.8829937Losses:  5.264873504638672 -0.0 -0.0
MemoryTrain:  epoch  0, batch     4 | loss: 5.2648735Losses:  12.273097038269043 3.132296562194824 1.4399465322494507
MemoryTrain:  epoch  1, batch     0 | loss: 12.2730970Losses:  13.463704109191895 3.0094707012176514 3.777163505554199
MemoryTrain:  epoch  1, batch     1 | loss: 13.4637041Losses:  12.157214164733887 1.9987258911132812 3.8242828845977783
MemoryTrain:  epoch  1, batch     2 | loss: 12.1572142Losses:  12.480157852172852 3.1776578426361084 3.3784613609313965
MemoryTrain:  epoch  1, batch     3 | loss: 12.4801579Losses:  5.7506937980651855 -0.0 -0.0
MemoryTrain:  epoch  1, batch     4 | loss: 5.7506938Losses:  14.982694625854492 5.9971113204956055 3.410827159881592
MemoryTrain:  epoch  2, batch     0 | loss: 14.9826946Losses:  11.435185432434082 2.6874146461486816 3.3852319717407227
MemoryTrain:  epoch  2, batch     1 | loss: 11.4351854Losses:  12.569989204406738 2.2537169456481934 3.8188276290893555
MemoryTrain:  epoch  2, batch     2 | loss: 12.5699892Losses:  11.171409606933594 5.211543083190918 1.5866897106170654
MemoryTrain:  epoch  2, batch     3 | loss: 11.1714096Losses:  5.488889217376709 -0.0 -0.0
MemoryTrain:  epoch  2, batch     4 | loss: 5.4888892Losses:  8.850865364074707 3.1270315647125244 1.5315330028533936
MemoryTrain:  epoch  3, batch     0 | loss: 8.8508654Losses:  9.330938339233398 1.26510751247406 3.5216927528381348
MemoryTrain:  epoch  3, batch     1 | loss: 9.3309383Losses:  11.114836692810059 2.762557029724121 3.495042324066162
MemoryTrain:  epoch  3, batch     2 | loss: 11.1148367Losses:  10.042797088623047 2.4997870922088623 3.329522132873535
MemoryTrain:  epoch  3, batch     3 | loss: 10.0427971Losses:  7.861583709716797 -0.0 -0.0
MemoryTrain:  epoch  3, batch     4 | loss: 7.8615837Losses:  10.771993637084961 2.928572177886963 3.4033994674682617
MemoryTrain:  epoch  4, batch     0 | loss: 10.7719936Losses:  9.062057495117188 3.199343204498291 1.5358154773712158
MemoryTrain:  epoch  4, batch     1 | loss: 9.0620575Losses:  9.512939453125 2.661836862564087 3.4199094772338867
MemoryTrain:  epoch  4, batch     2 | loss: 9.5129395Losses:  11.049243927001953 2.7719287872314453 3.36604642868042
MemoryTrain:  epoch  4, batch     3 | loss: 11.0492439Losses:  2.3112549781799316 -0.0 -0.0
MemoryTrain:  epoch  4, batch     4 | loss: 2.3112550Losses:  11.708221435546875 3.1967616081237793 3.3502559661865234
MemoryTrain:  epoch  5, batch     0 | loss: 11.7082214Losses:  9.50169563293457 2.0974907875061035 3.367708206176758
MemoryTrain:  epoch  5, batch     1 | loss: 9.5016956Losses:  8.521730422973633 3.8082454204559326 1.4728606939315796
MemoryTrain:  epoch  5, batch     2 | loss: 8.5217304Losses:  8.726635932922363 2.247115135192871 3.417142391204834
MemoryTrain:  epoch  5, batch     3 | loss: 8.7266359Losses:  2.389697551727295 -0.0 -0.0
MemoryTrain:  epoch  5, batch     4 | loss: 2.3896976Losses:  8.219589233398438 1.5605947971343994 3.4129409790039062
MemoryTrain:  epoch  6, batch     0 | loss: 8.2195892Losses:  9.258743286132812 4.195796489715576 1.3965675830841064
MemoryTrain:  epoch  6, batch     1 | loss: 9.2587433Losses:  11.26840877532959 3.4762094020843506 3.482830047607422
MemoryTrain:  epoch  6, batch     2 | loss: 11.2684088Losses:  7.562417507171631 1.5453264713287354 3.3865113258361816
MemoryTrain:  epoch  6, batch     3 | loss: 7.5624175Losses:  5.479855060577393 -0.0 -0.0
MemoryTrain:  epoch  6, batch     4 | loss: 5.4798551Losses:  9.498746871948242 2.3930675983428955 3.317187786102295
MemoryTrain:  epoch  7, batch     0 | loss: 9.4987469Losses:  9.170615196228027 2.815920352935791 3.3703815937042236
MemoryTrain:  epoch  7, batch     1 | loss: 9.1706152Losses:  8.511752128601074 3.1382288932800293 1.400925636291504
MemoryTrain:  epoch  7, batch     2 | loss: 8.5117521Losses:  9.920501708984375 4.016149997711182 3.3484349250793457
MemoryTrain:  epoch  7, batch     3 | loss: 9.9205017Losses:  4.368896961212158 -0.0 -0.0
MemoryTrain:  epoch  7, batch     4 | loss: 4.3688970Losses:  9.05319595336914 2.3836328983306885 3.365413188934326
MemoryTrain:  epoch  8, batch     0 | loss: 9.0531960Losses:  7.061550140380859 2.724292516708374 1.4395029544830322
MemoryTrain:  epoch  8, batch     1 | loss: 7.0615501Losses:  9.844812393188477 2.423388957977295 3.44160795211792
MemoryTrain:  epoch  8, batch     2 | loss: 9.8448124Losses:  8.308517456054688 1.8111419677734375 3.3431873321533203
MemoryTrain:  epoch  8, batch     3 | loss: 8.3085175Losses:  2.167238712310791 -0.0 -0.0
MemoryTrain:  epoch  8, batch     4 | loss: 2.1672387Losses:  7.509231090545654 2.6059842109680176 1.4396915435791016
MemoryTrain:  epoch  9, batch     0 | loss: 7.5092311Losses:  9.390469551086426 2.8074822425842285 3.3717708587646484
MemoryTrain:  epoch  9, batch     1 | loss: 9.3904696Losses:  9.41781234741211 2.8423001766204834 3.3222224712371826
MemoryTrain:  epoch  9, batch     2 | loss: 9.4178123Losses:  8.28118896484375 2.148287057876587 3.3698368072509766
MemoryTrain:  epoch  9, batch     3 | loss: 8.2811890Losses:  2.169135093688965 -0.0 -0.0
MemoryTrain:  epoch  9, batch     4 | loss: 2.1691351
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 75.48%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 87.08%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.56%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 83.33%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 87.30%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 87.88%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 87.87%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 87.86%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   36 | acc: 31.25%,  total acc: 85.98%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 85.03%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 84.13%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 84.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.45%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.59%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 84.80%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 85.00%   
cur_acc:  ['0.8598', '0.7548']
his_acc:  ['0.8598', '0.8500']
Clustering into  7  clusters
Clusters:  [2 0 5 6 4 2 1 5 2 3 1 2 1 0 4 4]
Losses:  18.204130172729492 8.107450485229492 3.395369052886963
CurrentTrain: epoch  0, batch     0 | loss: 18.2041302Losses:  14.349891662597656 3.46775484085083 3.3557071685791016
CurrentTrain: epoch  0, batch     1 | loss: 14.3498917Losses:  17.57654571533203 7.572819709777832 3.382018566131592
CurrentTrain: epoch  1, batch     0 | loss: 17.5765457Losses:  11.234602928161621 2.180436134338379 3.3151304721832275
CurrentTrain: epoch  1, batch     1 | loss: 11.2346029Losses:  17.403339385986328 7.9333367347717285 3.3467323780059814
CurrentTrain: epoch  2, batch     0 | loss: 17.4033394Losses:  10.350103378295898 1.964509129524231 3.387822151184082
CurrentTrain: epoch  2, batch     1 | loss: 10.3501034Losses:  15.87906265258789 7.197268486022949 3.362335205078125
CurrentTrain: epoch  3, batch     0 | loss: 15.8790627Losses:  9.585543632507324 2.035892963409424 3.3419065475463867
CurrentTrain: epoch  3, batch     1 | loss: 9.5855436Losses:  14.467212677001953 6.479862689971924 3.353389263153076
CurrentTrain: epoch  4, batch     0 | loss: 14.4672127Losses:  9.391108512878418 1.6669502258300781 3.3579583168029785
CurrentTrain: epoch  4, batch     1 | loss: 9.3911085Losses:  13.776615142822266 6.553459167480469 3.3541617393493652
CurrentTrain: epoch  5, batch     0 | loss: 13.7766151Losses:  10.424487113952637 2.4347903728485107 3.355287790298462
CurrentTrain: epoch  5, batch     1 | loss: 10.4244871Losses:  14.41876220703125 6.680788040161133 3.3550596237182617
CurrentTrain: epoch  6, batch     0 | loss: 14.4187622Losses:  9.097335815429688 2.190316915512085 3.318143129348755
CurrentTrain: epoch  6, batch     1 | loss: 9.0973358Losses:  13.053763389587402 6.3240580558776855 3.3880319595336914
CurrentTrain: epoch  7, batch     0 | loss: 13.0537634Losses:  9.860898971557617 2.499271869659424 3.3602395057678223
CurrentTrain: epoch  7, batch     1 | loss: 9.8608990Losses:  12.55123519897461 6.097947120666504 3.3504996299743652
CurrentTrain: epoch  8, batch     0 | loss: 12.5512352Losses:  8.063892364501953 1.4067423343658447 3.39558744430542
CurrentTrain: epoch  8, batch     1 | loss: 8.0638924Losses:  15.291187286376953 8.571009635925293 3.384439468383789
CurrentTrain: epoch  9, batch     0 | loss: 15.2911873Losses:  7.986877918243408 3.9402101039886475 1.4313052892684937
CurrentTrain: epoch  9, batch     1 | loss: 7.9868779
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: the chairman of the senate foreign relations committee , massachusetts democrat john kerry , and the panel 's top republican , richard lugar of indiana , were at the white house meeting , which was led by vice president joe biden , a former chairman of the foreign relations panel .
Head Entity: john kerry
Tail Entity: massachusetts
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving to California for his new job, Michael found himself enjoying the sunny weather and vibrant culture of Los Angeles.  
Head Entity: Michael  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: During her time in New York City, Sarah developed a passion for the arts and frequently visited the Metropolitan Museum of Art.  
Head Entity: Sarah  
Tail Entity: New York  

Relation: person stateorprovinces of residence  
Context: Following his retirement, David decided to settle down in Florida, where he could enjoy the warm climate and beautiful beaches.  
Head Entity: David  
Tail Entity: Florida  

Relation: person stateorprovinces of residence  
Context: Growing up in Texas, Emily often reminisced about the vast landscapes and friendly communities that shaped her childhood.  
Head Entity: Emily  
Tail Entity: Texas  

Relation: person stateorprovinces of residence  
Context: After completing her studies in Illinois, Jessica moved to Washington, D.C., to pursue a career in politics.  
Head Entity: Jessica  
Tail Entity: Illinois  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he was taken off life support on feb. 14 .
Head Entity: he
Tail Entity: feb. 14
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The renowned author passed away on March 5, 2020, after a long illness.  
Head Entity: The renowned author  
Tail Entity: March 5, 2020  

Relation: person date of death  
Context: She left this world on July 22, 2018, surrounded by her family.  
Head Entity: She  
Tail Entity: July 22, 2018  

Relation: person date of death  
Context: The scientist's contributions were celebrated after he died on November 10, 2019.  
Head Entity: The scientist  
Tail Entity: November 10, 2019  

Relation: person date of death  
Context: He was remembered fondly by his friends after he passed away on January 1, 2021.  
Head Entity: He  
Tail Entity: January 1, 2021  

Relation: person date of death  
Context: The beloved actor's death on September 15, 2022, shocked the entertainment industry.  
Head Entity: The beloved actor  
Tail Entity: September 15, 2022  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, employs approximately 5,500 individuals across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: The nonprofit organization GreenEarth has grown significantly and now boasts a workforce of over 1,200 dedicated staff members.  
Head Entity: GreenEarth  
Tail Entity: 1,200  

Relation: organization number of employees members  
Context: With a workforce of around 3,000, MegaRetail continues to expand its operations in various regions.  
Head Entity: MegaRetail  
Tail Entity: 3,000  

Relation: organization number of employees members  
Context: After the merger, GlobalFinance reported that it now has a total of 8,000 employees working in various departments.  
Head Entity: GlobalFinance  
Tail Entity: 8,000  

Relation: organization number of employees members  
Context: The startup InnovateX has rapidly increased its team size to 150 employees since its inception just two years ago.  
Head Entity: InnovateX  
Tail Entity: 150  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: `` i am known in the hospice as the man who would n't die , '' buchwald wrote in march .
Head Entity: buchwald
Tail Entity: man who would n't die
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: `` the world knows her as the queen of pop, but her friends call her madge. ''  
Head Entity: madge  
Tail Entity: queen of pop  

Relation: person alternate names  
Context: `` in the film industry, he is often referred to as the king of action, but his real name is john. ''  
Head Entity: john  
Tail Entity: king of action  

Relation: person alternate names  
Context: `` she was famously known as the iron lady during her time in office. ''  
Head Entity: iron lady  
Tail Entity: margaret thatcher  

Relation: person alternate names  
Context: `` the artist is recognized worldwide as the king of rock and roll, but he was born as elvis presley. ''  
Head Entity: elvis presley  
Tail Entity: king of rock and roll  

Relation: person alternate names  
Context: `` in the literary world, she is often called the queen of crime, but her real name is agatha. ''  
Head Entity: agatha  
Tail Entity: queen of crime  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: kerry katona feel out with arnold just before she married bryan mcfadden , kerry cancelled his wedding invitation and have n't spoken since .
Head Entity: bryan mcfadden
Tail Entity: kerry katona
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
Relation: person spouse  
Context: After years of dating, Emma finally tied the knot with Liam, and they celebrated their wedding in a beautiful ceremony surrounded by family and friends.  
Head Entity: Liam  
Tail Entity: Emma  

Relation: person spouse  
Context: During the interview, Sarah mentioned how supportive her husband, Mark, has been throughout her career, always encouraging her to pursue her dreams.  
Head Entity: Mark  
Tail Entity: Sarah  

Relation: person spouse  
Context: Following their engagement, Jessica and Tom planned a lavish wedding, which was the talk of the town for months.  
Head Entity: Tom  
Tail Entity: Jessica  

Relation: person spouse  
Context: After a whirlwind romance, Mia and Jake decided to get married in a small, intimate ceremony at the beach, where they first met.  
Head Entity: Jake  
Tail Entity: Mia  

Relation: person spouse  
Context: At the family reunion, everyone was excited to see how much Alex and his wife, Rachel, had grown together over the years.  
Head Entity: Rachel  
Tail Entity: Alex  
Losses:  16.606868743896484 1.039501667022705 11.24596881866455
MemoryTrain:  epoch  0, batch     0 | loss: 16.6068687Losses:  14.271618843078613 1.658855676651001 8.156635284423828
MemoryTrain:  epoch  0, batch     1 | loss: 14.2716188Losses:  16.49930191040039 3.1430258750915527 8.409847259521484
MemoryTrain:  epoch  0, batch     2 | loss: 16.4993019Losses:  14.673812866210938 1.3144190311431885 8.194591522216797
MemoryTrain:  epoch  0, batch     3 | loss: 14.6738129Losses:  18.447980880737305 1.9678075313568115 10.976534843444824
MemoryTrain:  epoch  0, batch     4 | loss: 18.4479809Losses:  15.372419357299805 2.124563694000244 8.317410469055176
MemoryTrain:  epoch  0, batch     5 | loss: 15.3724194Losses:  14.766003608703613 2.62923526763916 8.354330062866211
MemoryTrain:  epoch  1, batch     0 | loss: 14.7660036Losses:  17.24323272705078 2.245163917541504 10.921589851379395
MemoryTrain:  epoch  1, batch     1 | loss: 17.2432327Losses:  14.359098434448242 1.820303201675415 8.149252891540527
MemoryTrain:  epoch  1, batch     2 | loss: 14.3590984Losses:  13.959146499633789 3.0286459922790527 5.727155685424805
MemoryTrain:  epoch  1, batch     3 | loss: 13.9591465Losses:  16.659271240234375 2.0016427040100098 10.956783294677734
MemoryTrain:  epoch  1, batch     4 | loss: 16.6592712Losses:  16.9102840423584 2.0521750450134277 10.894745826721191
MemoryTrain:  epoch  1, batch     5 | loss: 16.9102840Losses:  13.398351669311523 1.5624909400939941 8.223437309265137
MemoryTrain:  epoch  2, batch     0 | loss: 13.3983517Losses:  16.21298599243164 1.486755132675171 10.986860275268555
MemoryTrain:  epoch  2, batch     1 | loss: 16.2129860Losses:  14.435796737670898 1.037858009338379 10.865533828735352
MemoryTrain:  epoch  2, batch     2 | loss: 14.4357967Losses:  15.170042037963867 1.06943941116333 10.917220115661621
MemoryTrain:  epoch  2, batch     3 | loss: 15.1700420Losses:  14.635772705078125 2.8331098556518555 8.193753242492676
MemoryTrain:  epoch  2, batch     4 | loss: 14.6357727Losses:  15.594735145568848 1.300847053527832 10.878621101379395
MemoryTrain:  epoch  2, batch     5 | loss: 15.5947351Losses:  10.573074340820312 3.2442855834960938 3.332155227661133
MemoryTrain:  epoch  3, batch     0 | loss: 10.5730743Losses:  15.100702285766602 1.1584033966064453 10.979208946228027
MemoryTrain:  epoch  3, batch     1 | loss: 15.1007023Losses:  13.054410934448242 1.8530378341674805 8.160457611083984
MemoryTrain:  epoch  3, batch     2 | loss: 13.0544109Losses:  14.823670387268066 0.8796448707580566 10.911236763000488
MemoryTrain:  epoch  3, batch     3 | loss: 14.8236704Losses:  15.186659812927246 1.3509340286254883 10.924589157104492
MemoryTrain:  epoch  3, batch     4 | loss: 15.1866598Losses:  12.685789108276367 1.0705223083496094 8.115882873535156
MemoryTrain:  epoch  3, batch     5 | loss: 12.6857891Losses:  12.236873626708984 1.396181344985962 8.143514633178711
MemoryTrain:  epoch  4, batch     0 | loss: 12.2368736Losses:  14.112054824829102 2.1834774017333984 8.099374771118164
MemoryTrain:  epoch  4, batch     1 | loss: 14.1120548Losses:  9.879755020141602 1.5088226795196533 5.575706958770752
MemoryTrain:  epoch  4, batch     2 | loss: 9.8797550Losses:  15.450984001159668 1.5972044467926025 10.905783653259277
MemoryTrain:  epoch  4, batch     3 | loss: 15.4509840Losses:  14.988032341003418 1.6598453521728516 10.845335006713867
MemoryTrain:  epoch  4, batch     4 | loss: 14.9880323Losses:  13.287317276000977 2.0208897590637207 8.113012313842773
MemoryTrain:  epoch  4, batch     5 | loss: 13.2873173Losses:  15.632128715515137 1.953314185142517 10.80509090423584
MemoryTrain:  epoch  5, batch     0 | loss: 15.6321287Losses:  9.748069763183594 1.2694405317306519 5.5967512130737305
MemoryTrain:  epoch  5, batch     1 | loss: 9.7480698Losses:  11.602067947387695 1.0957155227661133 8.08954906463623
MemoryTrain:  epoch  5, batch     2 | loss: 11.6020679Losses:  13.923954010009766 2.4457952976226807 8.178256034851074
MemoryTrain:  epoch  5, batch     3 | loss: 13.9239540Losses:  15.462003707885742 1.7942686080932617 10.821123123168945
MemoryTrain:  epoch  5, batch     4 | loss: 15.4620037Losses:  12.31894588470459 1.3638032674789429 8.142566680908203
MemoryTrain:  epoch  5, batch     5 | loss: 12.3189459Losses:  12.193048477172852 1.202155590057373 8.11686897277832
MemoryTrain:  epoch  6, batch     0 | loss: 12.1930485Losses:  12.267597198486328 1.5220071077346802 8.097091674804688
MemoryTrain:  epoch  6, batch     1 | loss: 12.2675972Losses:  16.479228973388672 2.824108362197876 10.82194709777832
MemoryTrain:  epoch  6, batch     2 | loss: 16.4792290Losses:  11.738260269165039 1.4310712814331055 8.09592056274414
MemoryTrain:  epoch  6, batch     3 | loss: 11.7382603Losses:  12.761714935302734 1.4748330116271973 8.132670402526855
MemoryTrain:  epoch  6, batch     4 | loss: 12.7617149Losses:  8.023335456848145 2.4031877517700195 3.324951410293579
MemoryTrain:  epoch  6, batch     5 | loss: 8.0233355Losses:  10.382186889648438 2.7158262729644775 5.6134562492370605
MemoryTrain:  epoch  7, batch     0 | loss: 10.3821869Losses:  12.136266708374023 1.110114574432373 8.129985809326172
MemoryTrain:  epoch  7, batch     1 | loss: 12.1362667Losses:  14.537841796875 1.443727970123291 10.808537483215332
MemoryTrain:  epoch  7, batch     2 | loss: 14.5378418Losses:  10.316574096679688 2.3698675632476807 5.584960460662842
MemoryTrain:  epoch  7, batch     3 | loss: 10.3165741Losses:  14.603611946105957 1.3968770503997803 10.834319114685059
MemoryTrain:  epoch  7, batch     4 | loss: 14.6036119Losses:  10.89670181274414 2.6017355918884277 5.573315620422363
MemoryTrain:  epoch  7, batch     5 | loss: 10.8967018Losses:  10.914053916931152 0.4967382550239563 8.08879280090332
MemoryTrain:  epoch  8, batch     0 | loss: 10.9140539Losses:  12.40383529663086 2.2159366607666016 8.11497974395752
MemoryTrain:  epoch  8, batch     1 | loss: 12.4038353Losses:  9.384977340698242 1.3607004880905151 5.578347682952881
MemoryTrain:  epoch  8, batch     2 | loss: 9.3849773Losses:  11.866729736328125 1.2520921230316162 8.096563339233398
MemoryTrain:  epoch  8, batch     3 | loss: 11.8667297Losses:  14.790853500366211 1.5937526226043701 10.861184120178223
MemoryTrain:  epoch  8, batch     4 | loss: 14.7908535Losses:  14.648658752441406 1.6706860065460205 10.824175834655762
MemoryTrain:  epoch  8, batch     5 | loss: 14.6486588Losses:  12.003247261047363 1.4280188083648682 8.11011791229248
MemoryTrain:  epoch  9, batch     0 | loss: 12.0032473Losses:  13.788996696472168 0.9552714824676514 10.80627727508545
MemoryTrain:  epoch  9, batch     1 | loss: 13.7889967Losses:  12.49744987487793 1.373577356338501 8.094871520996094
MemoryTrain:  epoch  9, batch     2 | loss: 12.4974499Losses:  14.54449462890625 1.4512921571731567 10.804303169250488
MemoryTrain:  epoch  9, batch     3 | loss: 14.5444946Losses:  12.127884864807129 1.8780326843261719 8.075579643249512
MemoryTrain:  epoch  9, batch     4 | loss: 12.1278849Losses:  9.897153854370117 2.248467206954956 5.561749458312988
MemoryTrain:  epoch  9, batch     5 | loss: 9.8971539
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 81.70%   [EVAL] batch:   14 | acc: 37.50%,  total acc: 78.75%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 91.35%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 89.73%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 84.03%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.78%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 86.06%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 86.69%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.91%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 87.12%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 87.32%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 87.32%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 87.15%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 87.33%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 87.34%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 87.19%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 85.98%   [EVAL] batch:   41 | acc: 43.75%,  total acc: 84.97%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 84.45%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 83.95%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 83.47%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 83.29%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 83.64%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 84.06%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 84.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.31%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 84.91%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 84.55%   [EVAL] batch:   55 | acc: 50.00%,  total acc: 83.93%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 83.77%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 83.51%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 83.05%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 82.19%   
cur_acc:  ['0.8598', '0.7548', '0.7875']
his_acc:  ['0.8598', '0.8500', '0.8219']
Clustering into  9  clusters
Clusters:  [2 8 1 0 0 3 4 1 3 5 2 2 4 7 6 0 1 1 6 3 5]
Losses:  21.303255081176758 9.050437927246094 3.778043508529663
CurrentTrain: epoch  0, batch     0 | loss: 21.3032551Losses:  16.235416412353516 4.920734882354736 1.723024606704712
CurrentTrain: epoch  0, batch     1 | loss: 16.2354164Losses:  20.91511344909668 9.07419204711914 3.72815203666687
CurrentTrain: epoch  1, batch     0 | loss: 20.9151134Losses:  15.216718673706055 3.640873908996582 3.790306568145752
CurrentTrain: epoch  1, batch     1 | loss: 15.2167187Losses:  18.474456787109375 7.569016456604004 3.63147234916687
CurrentTrain: epoch  2, batch     0 | loss: 18.4744568Losses:  13.568790435791016 2.098487138748169 3.727034091949463
CurrentTrain: epoch  2, batch     1 | loss: 13.5687904Losses:  18.93838119506836 8.640973091125488 3.743781566619873
CurrentTrain: epoch  3, batch     0 | loss: 18.9383812Losses:  12.999103546142578 3.7116730213165283 1.7381279468536377
CurrentTrain: epoch  3, batch     1 | loss: 12.9991035Losses:  19.7542724609375 9.596973419189453 3.524890422821045
CurrentTrain: epoch  4, batch     0 | loss: 19.7542725Losses:  14.728364944458008 4.377016544342041 3.7066056728363037
CurrentTrain: epoch  4, batch     1 | loss: 14.7283649Losses:  17.124103546142578 7.920690536499023 3.684488296508789
CurrentTrain: epoch  5, batch     0 | loss: 17.1241035Losses:  11.926353454589844 3.4650394916534424 1.4905390739440918
CurrentTrain: epoch  5, batch     1 | loss: 11.9263535Losses:  18.185640335083008 8.322259902954102 3.553518295288086
CurrentTrain: epoch  6, batch     0 | loss: 18.1856403Losses:  9.184871673583984 2.0222134590148926 3.4191763401031494
CurrentTrain: epoch  6, batch     1 | loss: 9.1848717Losses:  15.656081199645996 7.47736120223999 3.4588136672973633
CurrentTrain: epoch  7, batch     0 | loss: 15.6560812Losses:  11.454856872558594 2.479485511779785 3.4371964931488037
CurrentTrain: epoch  7, batch     1 | loss: 11.4548569Losses:  18.317886352539062 9.138107299804688 3.465554714202881
CurrentTrain: epoch  8, batch     0 | loss: 18.3178864Losses:  11.44859790802002 3.957061767578125 3.4622926712036133
CurrentTrain: epoch  8, batch     1 | loss: 11.4485979Losses:  16.599281311035156 8.338364601135254 3.4870643615722656
CurrentTrain: epoch  9, batch     0 | loss: 16.5992813Losses:  9.21263313293457 3.345632314682007 1.4232964515686035
CurrentTrain: epoch  9, batch     1 | loss: 9.2126331
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: firstgroup , britain 's largest bus operator , entered the north american market in 1999 when it acquired ryder public transportation services inc. .
Head Entity: firstgroup
Tail Entity: ryder public transportation services inc.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: Alphabet Inc. has several subsidiaries, including YouTube, which it acquired in 2006 for $1.65 billion.  
Head Entity: Alphabet Inc.  
Tail Entity: YouTube  

Relation: organization subsidiaries  
Context: In 2015, Facebook acquired WhatsApp, a messaging service that has become one of its key subsidiaries.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: organization subsidiaries  
Context: The Coca-Cola Company owns several subsidiaries, including Minute Maid, which specializes in fruit juices.  
Head Entity: The Coca-Cola Company  
Tail Entity: Minute Maid  

Relation: organization subsidiaries  
Context: Amazon's acquisition of Whole Foods Market in 2017 added a significant subsidiary to its portfolio in the grocery sector.  
Head Entity: Amazon  
Tail Entity: Whole Foods Market  

Relation: organization subsidiaries  
Context: Berkshire Hathaway has a diverse range of subsidiaries, one of which is Geico, a well-known insurance provider.  
Head Entity: Berkshire Hathaway  
Tail Entity: Geico  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: in addition his ability to communicate with people has been recognized by his winning of the robert e. knox master teacher award and by his service on the american psychological association 's , public information committee .
Head Entity: american psychological association
Tail Entity: public information committee
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: The National Aeronautics and Space Administration, commonly known as NASA, has been a pioneer in space exploration, and its parent organization, the United States government, provides essential funding and oversight.  
Head Entity: National Aeronautics and Space Administration  
Tail Entity: United States government  

Relation: organization parents  
Context: The World Wildlife Fund, an international non-governmental organization, operates under the auspices of the World Wide Fund for Nature, which serves as its parent organization.  
Head Entity: World Wildlife Fund  
Tail Entity: World Wide Fund for Nature  

Relation: organization parents  
Context: The American Red Cross, a humanitarian organization, is part of a larger network that includes the International Federation of Red Cross and Red Crescent Societies as its parent organization.  
Head Entity: American Red Cross  
Tail Entity: International Federation of Red Cross and Red Crescent Societies  

Relation: organization parents  
Context: The Mozilla Corporation, known for its Firefox web browser, is a subsidiary of the Mozilla Foundation, which acts as its parent organization.  
Head Entity: Mozilla Corporation  
Tail Entity: Mozilla Foundation  

Relation: organization parents  
Context: The National Football League, a professional American football league, is governed by the National Football League Management Council, which serves as its parent organization.  
Head Entity: National Football League  
Tail Entity: National Football League Management Council  
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: it also needs the green light from the 45-nation nuclear suppliers group -lrb- nsg -rrb- , which regulates global civilian nuclear trade , before it can begin buying nuclear reactors and fuel .
Head Entity: nsg
Tail Entity: nuclear suppliers group
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: IMF  
Tail Entity: International Monetary Fund  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global response to health crises.  
Head Entity: WHO  
Tail Entity: World Health Organization  

Relation: organization alternate names  
Context: The National Aeronautics and Space Administration, abbreviated as NASA, is responsible for the nation's civilian space program and for aeronautics and aerospace research.  
Head Entity: NASA  
Tail Entity: National Aeronautics and Space Administration  

Relation: organization alternate names  
Context: The Federal Bureau of Investigation, or FBI, is the principal federal investigative agency and domestic intelligence service of the United States.  
Head Entity: FBI  
Tail Entity: Federal Bureau of Investigation  

Relation: organization alternate names  
Context: The United Nations Educational, Scientific and Cultural Organization, known as UNESCO, aims to promote world peace and security through international cooperation in education, the sciences, and culture.  
Head Entity: UNESCO  
Tail Entity: United Nations Educational, Scientific and Cultural Organization  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: ------ london 2008-05-20 07:23:45 utc enodis plc endorses sweetened takeover bid by us company manitowoc illinois tool works of glenville , illinois , which had offered 282 pence -lrb- us$ 551 euro3 54 -rrb- per share , said monday that it was considering its position .
Head Entity: illinois tool works
Tail Entity: glenville
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: ------ san francisco 2010-11-15 12:00:00 utc salesforce.com, a cloud computing company, announced its expansion into new offices in the heart of san francisco, aiming to accommodate its growing workforce.  
Head Entity: salesforce.com  
Tail Entity: san francisco  

Relation: organization city of headquarters  
Context: ------ new york 2015-03-10 09:30:00 utc the headquarters of the international business machines corporation, commonly known as ibm, is located in the bustling city of new york, where it has been a key player in technology for decades.  
Head Entity: ibm  
Tail Entity: new york  

Relation: organization city of headquarters  
Context: ------ seattle 2018-07-22 14:45:00 utc amazon.com, the e-commerce giant, continues to thrive with its headquarters situated in seattle, washington, contributing significantly to the local economy.  
Head Entity: amazon.com  
Tail Entity: seattle  

Relation: organization city of headquarters  
Context: ------ boston 2021-01-05 11:15:00 utc the biopharmaceutical company moderna, known for its innovative vaccine technology, has its headquarters in boston, massachusetts, a hub for life sciences.  
Head Entity: moderna  
Tail Entity: boston  

Relation: organization city of headquarters  
Context: ------ austin 2019-09-30 16:00:00 utc the tech startup indeed.com, which specializes in job search engines, has established its headquarters in austin, texas, attracting talent from across the country.  
Head Entity: indeed.com  
Tail Entity: austin  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: forsberg , a political science professor at city college of new york , died oct. 19 in a bronx hospital of cancer , said her sister , celia seupel .
Head Entity: forsberg
Tail Entity: celia seupel
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: During the family reunion, John introduced his sister, Emily, who had just returned from studying abroad.  
Head Entity: John  
Tail Entity: Emily  

Relation: person siblings  
Context: After the game, Sarah celebrated her victory with her brother, Michael, who had been cheering for her from the stands.  
Head Entity: Sarah  
Tail Entity: Michael  

Relation: person siblings  
Context: In her memoir, Lisa writes fondly about her childhood adventures with her brother, Tom, who always had her back.  
Head Entity: Lisa  
Tail Entity: Tom  

Relation: person siblings  
Context: At the wedding, Anna was thrilled to see her brother, David, who had flown in from another state to be her best man.  
Head Entity: Anna  
Tail Entity: David  

Relation: person siblings  
Context: The documentary featured interviews with Rachel and her sister, Jessica, discussing their close bond and shared experiences growing up.  
Head Entity: Rachel  
Tail Entity: Jessica  
Losses:  17.068885803222656 1.3136879205703735 10.912615776062012
MemoryTrain:  epoch  0, batch     0 | loss: 17.0688858Losses:  16.2637882232666 1.5616588592529297 10.935309410095215
MemoryTrain:  epoch  0, batch     1 | loss: 16.2637882Losses:  16.816524505615234 3.4073104858398438 8.283930778503418
MemoryTrain:  epoch  0, batch     2 | loss: 16.8165245Losses:  19.408348083496094 1.0572340488433838 14.13711166381836
MemoryTrain:  epoch  0, batch     3 | loss: 19.4083481Losses:  20.686452865600586 0.8771326541900635 14.092084884643555
MemoryTrain:  epoch  0, batch     4 | loss: 20.6864529Losses:  15.946674346923828 2.6972947120666504 8.336860656738281
MemoryTrain:  epoch  0, batch     5 | loss: 15.9466743Losses:  21.383554458618164 0.285685658454895 16.90785789489746
MemoryTrain:  epoch  0, batch     6 | loss: 21.3835545Losses:  20.04531478881836 0.7861670851707458 14.01041316986084
MemoryTrain:  epoch  0, batch     7 | loss: 20.0453148Losses:  20.63468360900879 0.8229326605796814 16.845699310302734
MemoryTrain:  epoch  1, batch     0 | loss: 20.6346836Losses:  20.14342498779297 1.6233656406402588 14.013818740844727
MemoryTrain:  epoch  1, batch     1 | loss: 20.1434250Losses:  14.013456344604492 1.4192529916763306 8.173362731933594
MemoryTrain:  epoch  1, batch     2 | loss: 14.0134563Losses:  19.608623504638672 1.383558988571167 13.786603927612305
MemoryTrain:  epoch  1, batch     3 | loss: 19.6086235Losses:  20.80571746826172 1.846305012702942 14.141382217407227
MemoryTrain:  epoch  1, batch     4 | loss: 20.8057175Losses:  19.126453399658203 0.5355246067047119 13.981971740722656
MemoryTrain:  epoch  1, batch     5 | loss: 19.1264534Losses:  19.950847625732422 1.57326340675354 13.900897979736328
MemoryTrain:  epoch  1, batch     6 | loss: 19.9508476Losses:  17.769765853881836 0.5426867008209229 13.722475051879883
MemoryTrain:  epoch  1, batch     7 | loss: 17.7697659Losses:  18.184101104736328 1.941375732421875 13.806550025939941
MemoryTrain:  epoch  2, batch     0 | loss: 18.1841011Losses:  12.972515106201172 1.1116703748703003 8.168246269226074
MemoryTrain:  epoch  2, batch     1 | loss: 12.9725151Losses:  16.478759765625 1.5312600135803223 10.863755226135254
MemoryTrain:  epoch  2, batch     2 | loss: 16.4787598Losses:  12.865890502929688 0.7628496885299683 8.217487335205078
MemoryTrain:  epoch  2, batch     3 | loss: 12.8658905Losses:  21.778884887695312 0.8444457054138184 16.849098205566406
MemoryTrain:  epoch  2, batch     4 | loss: 21.7788849Losses:  15.913036346435547 1.7699075937271118 10.816460609436035
MemoryTrain:  epoch  2, batch     5 | loss: 15.9130363Losses:  18.481212615966797 0.8510991930961609 13.856607437133789
MemoryTrain:  epoch  2, batch     6 | loss: 18.4812126Losses:  17.659114837646484 0.8859890699386597 13.722514152526855
MemoryTrain:  epoch  2, batch     7 | loss: 17.6591148Losses:  14.936186790466309 1.379851222038269 10.855265617370605
MemoryTrain:  epoch  3, batch     0 | loss: 14.9361868Losses:  12.2558012008667 1.5613728761672974 8.151359558105469
MemoryTrain:  epoch  3, batch     1 | loss: 12.2558012Losses:  16.203990936279297 4.54375696182251 8.143119812011719
MemoryTrain:  epoch  3, batch     2 | loss: 16.2039909Losses:  15.867603302001953 1.4726276397705078 11.035297393798828
MemoryTrain:  epoch  3, batch     3 | loss: 15.8676033Losses:  17.13932991027832 0.5798208117485046 13.802703857421875
MemoryTrain:  epoch  3, batch     4 | loss: 17.1393299Losses:  10.940153121948242 1.815122365951538 5.7154107093811035
MemoryTrain:  epoch  3, batch     5 | loss: 10.9401531Losses:  15.205648422241211 0.8729708790779114 11.042160987854004
MemoryTrain:  epoch  3, batch     6 | loss: 15.2056484Losses:  17.403852462768555 0.5337679386138916 13.72977352142334
MemoryTrain:  epoch  3, batch     7 | loss: 17.4038525Losses:  18.898902893066406 1.579646110534668 13.802865982055664
MemoryTrain:  epoch  4, batch     0 | loss: 18.8989029Losses:  15.674944877624512 2.1579513549804688 10.833672523498535
MemoryTrain:  epoch  4, batch     1 | loss: 15.6749449Losses:  16.607133865356445 0.7180994153022766 13.721525192260742
MemoryTrain:  epoch  4, batch     2 | loss: 16.6071339Losses:  15.784582138061523 1.1809332370758057 10.9678316116333
MemoryTrain:  epoch  4, batch     3 | loss: 15.7845821Losses:  17.81594467163086 1.1006940603256226 13.725173950195312
MemoryTrain:  epoch  4, batch     4 | loss: 17.8159447Losses:  13.974614143371582 3.2847719192504883 8.159387588500977
MemoryTrain:  epoch  4, batch     5 | loss: 13.9746141Losses:  18.23506736755371 1.2063796520233154 13.954566955566406
MemoryTrain:  epoch  4, batch     6 | loss: 18.2350674Losses:  16.798580169677734 0.8069158792495728 13.736149787902832
MemoryTrain:  epoch  4, batch     7 | loss: 16.7985802Losses:  9.151420593261719 1.2840757369995117 5.576706409454346
MemoryTrain:  epoch  5, batch     0 | loss: 9.1514206Losses:  18.011333465576172 1.265371322631836 13.77007007598877
MemoryTrain:  epoch  5, batch     1 | loss: 18.0113335Losses:  18.958019256591797 1.8257856369018555 13.888516426086426
MemoryTrain:  epoch  5, batch     2 | loss: 18.9580193Losses:  17.31604766845703 1.1603221893310547 13.711702346801758
MemoryTrain:  epoch  5, batch     3 | loss: 17.3160477Losses:  14.506412506103516 1.0409538745880127 10.878386497497559
MemoryTrain:  epoch  5, batch     4 | loss: 14.5064125Losses:  11.852874755859375 1.6383572816848755 8.088309288024902
MemoryTrain:  epoch  5, batch     5 | loss: 11.8528748Losses:  15.08176040649414 1.6662359237670898 10.803106307983398
MemoryTrain:  epoch  5, batch     6 | loss: 15.0817604Losses:  15.667585372924805 1.7133394479751587 10.857773780822754
MemoryTrain:  epoch  5, batch     7 | loss: 15.6675854Losses:  15.47784423828125 2.341573715209961 10.845806121826172
MemoryTrain:  epoch  6, batch     0 | loss: 15.4778442Losses:  16.76228904724121 0.8218030333518982 13.683897018432617
MemoryTrain:  epoch  6, batch     1 | loss: 16.7622890Losses:  18.190719604492188 1.6779654026031494 13.81304931640625
MemoryTrain:  epoch  6, batch     2 | loss: 18.1907196Losses:  14.195058822631836 0.7622402906417847 10.797689437866211
MemoryTrain:  epoch  6, batch     3 | loss: 14.1950588Losses:  18.469043731689453 1.7450854778289795 13.760211944580078
MemoryTrain:  epoch  6, batch     4 | loss: 18.4690437Losses:  15.235444068908691 1.7529685497283936 10.862706184387207
MemoryTrain:  epoch  6, batch     5 | loss: 15.2354441Losses:  17.3782901763916 1.3780169486999512 13.71829891204834
MemoryTrain:  epoch  6, batch     6 | loss: 17.3782902Losses:  19.065872192382812 2.4231958389282227 13.718146324157715
MemoryTrain:  epoch  6, batch     7 | loss: 19.0658722Losses:  11.253679275512695 1.0101051330566406 8.083881378173828
MemoryTrain:  epoch  7, batch     0 | loss: 11.2536793Losses:  17.4569034576416 1.3205816745758057 13.767972946166992
MemoryTrain:  epoch  7, batch     1 | loss: 17.4569035Losses:  17.229068756103516 1.1616744995117188 13.712366104125977
MemoryTrain:  epoch  7, batch     2 | loss: 17.2290688Losses:  14.197952270507812 1.3279783725738525 10.820865631103516
MemoryTrain:  epoch  7, batch     3 | loss: 14.1979523Losses:  15.017635345458984 2.05501651763916 10.786136627197266
MemoryTrain:  epoch  7, batch     4 | loss: 15.0176353Losses:  17.851686477661133 1.2337498664855957 13.81197738647461
MemoryTrain:  epoch  7, batch     5 | loss: 17.8516865Losses:  15.356204986572266 1.8827991485595703 10.903242111206055
MemoryTrain:  epoch  7, batch     6 | loss: 15.3562050Losses:  17.941883087158203 0.7876378893852234 13.771017074584961
MemoryTrain:  epoch  7, batch     7 | loss: 17.9418831Losses:  16.18726921081543 2.1656782627105713 10.993406295776367
MemoryTrain:  epoch  8, batch     0 | loss: 16.1872692Losses:  9.640199661254883 1.9458154439926147 5.562471866607666
MemoryTrain:  epoch  8, batch     1 | loss: 9.6401997Losses:  20.291719436645508 0.8931102752685547 16.715784072875977
MemoryTrain:  epoch  8, batch     2 | loss: 20.2917194Losses:  19.45762062072754 0.5220624208450317 16.77536964416504
MemoryTrain:  epoch  8, batch     3 | loss: 19.4576206Losses:  12.017641067504883 1.6215592622756958 8.11313533782959
MemoryTrain:  epoch  8, batch     4 | loss: 12.0176411Losses:  11.622600555419922 1.5639073848724365 8.104272842407227
MemoryTrain:  epoch  8, batch     5 | loss: 11.6226006Losses:  18.055692672729492 1.4812672138214111 13.827425956726074
MemoryTrain:  epoch  8, batch     6 | loss: 18.0556927Losses:  13.734538078308105 0.7864097952842712 10.807129859924316
MemoryTrain:  epoch  8, batch     7 | loss: 13.7345381Losses:  11.739372253417969 1.0542495250701904 8.13971996307373
MemoryTrain:  epoch  9, batch     0 | loss: 11.7393723Losses:  17.173357009887695 1.248201608657837 13.686840057373047
MemoryTrain:  epoch  9, batch     1 | loss: 17.1733570Losses:  14.088240623474121 1.1061115264892578 10.789102554321289
MemoryTrain:  epoch  9, batch     2 | loss: 14.0882406Losses:  13.859366416931152 0.9968442320823669 10.794733047485352
MemoryTrain:  epoch  9, batch     3 | loss: 13.8593664Losses:  14.096721649169922 1.0140087604522705 10.807171821594238
MemoryTrain:  epoch  9, batch     4 | loss: 14.0967216Losses:  11.925168991088867 1.4713506698608398 8.088140487670898
MemoryTrain:  epoch  9, batch     5 | loss: 11.9251690Losses:  16.894622802734375 0.7747432589530945 13.678171157836914
MemoryTrain:  epoch  9, batch     6 | loss: 16.8946228Losses:  14.337697982788086 1.4751441478729248 10.817197799682617
MemoryTrain:  epoch  9, batch     7 | loss: 14.3376980
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 47.92%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 26.79%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 32.03%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 32.64%   [EVAL] batch:    9 | acc: 43.75%,  total acc: 33.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 37.50%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 38.54%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 39.42%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 37.50%   [EVAL] batch:   14 | acc: 6.25%,  total acc: 35.42%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 34.77%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 33.46%   [EVAL] batch:   17 | acc: 0.00%,  total acc: 31.60%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 33.22%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 35.62%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 37.50%   [EVAL] batch:   21 | acc: 43.75%,  total acc: 37.78%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 89.42%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 85.08%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 85.80%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 86.03%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 86.07%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 86.11%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 85.64%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 85.53%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 85.10%   [EVAL] batch:   39 | acc: 50.00%,  total acc: 84.22%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 82.62%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 80.65%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 78.92%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 77.98%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 77.50%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 77.45%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 78.26%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 78.44%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 78.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 79.04%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 79.45%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 79.72%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 79.98%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 79.55%   [EVAL] batch:   55 | acc: 6.25%,  total acc: 78.24%   [EVAL] batch:   56 | acc: 18.75%,  total acc: 77.19%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 76.08%   [EVAL] batch:   58 | acc: 0.00%,  total acc: 74.79%   [EVAL] batch:   59 | acc: 25.00%,  total acc: 73.96%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 73.67%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 73.29%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 72.32%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 71.29%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 70.48%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 69.41%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 69.31%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 69.03%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 68.57%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 68.57%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 68.31%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 68.14%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 67.55%   [EVAL] batch:   73 | acc: 12.50%,  total acc: 66.81%   [EVAL] batch:   74 | acc: 12.50%,  total acc: 66.08%   [EVAL] batch:   75 | acc: 18.75%,  total acc: 65.46%   [EVAL] batch:   76 | acc: 6.25%,  total acc: 64.69%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 64.42%   [EVAL] batch:   78 | acc: 68.75%,  total acc: 64.48%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 64.53%   [EVAL] batch:   80 | acc: 81.25%,  total acc: 64.74%   
cur_acc:  ['0.8598', '0.7548', '0.7875', '0.3778']
his_acc:  ['0.8598', '0.8500', '0.8219', '0.6474']
Clustering into  12  clusters
Clusters:  [ 6  0  1  7  0  4  3  1  4  2  5  6  3 11  8  2  1  1  8  4  2  6 10  5
  2  9]
Losses:  19.758792877197266 8.586923599243164 5.726238250732422
CurrentTrain: epoch  0, batch     0 | loss: 19.7587929Losses:  12.880878448486328 2.612929105758667 5.657908916473389
CurrentTrain: epoch  0, batch     1 | loss: 12.8808784Losses:  17.679906845092773 7.725820064544678 5.68475866317749
CurrentTrain: epoch  1, batch     0 | loss: 17.6799068Losses:  10.68700885772705 3.136141538619995 3.3440628051757812
CurrentTrain: epoch  1, batch     1 | loss: 10.6870089Losses:  17.051055908203125 8.033123970031738 5.638613700866699
CurrentTrain: epoch  2, batch     0 | loss: 17.0510559Losses:  9.351553916931152 3.0168914794921875 3.3366575241088867
CurrentTrain: epoch  2, batch     1 | loss: 9.3515539Losses:  15.733366966247559 7.149269104003906 5.596630096435547
CurrentTrain: epoch  3, batch     0 | loss: 15.7333670Losses:  9.662586212158203 2.779216766357422 3.3883261680603027
CurrentTrain: epoch  3, batch     1 | loss: 9.6625862Losses:  14.711271286010742 5.960023880004883 5.632562160491943
CurrentTrain: epoch  4, batch     0 | loss: 14.7112713Losses:  10.223134994506836 1.5350180864334106 5.568803310394287
CurrentTrain: epoch  4, batch     1 | loss: 10.2231350Losses:  14.948030471801758 6.708625316619873 5.577045917510986
CurrentTrain: epoch  5, batch     0 | loss: 14.9480305Losses:  8.704703330993652 2.5632901191711426 3.3706605434417725
CurrentTrain: epoch  5, batch     1 | loss: 8.7047033Losses:  14.036954879760742 6.111592769622803 5.577513217926025
CurrentTrain: epoch  6, batch     0 | loss: 14.0369549Losses:  9.961368560791016 2.0148684978485107 5.573212146759033
CurrentTrain: epoch  6, batch     1 | loss: 9.9613686Losses:  14.358576774597168 6.305564880371094 5.566347122192383
CurrentTrain: epoch  7, batch     0 | loss: 14.3585768Losses:  9.423648834228516 1.7555334568023682 5.579351902008057
CurrentTrain: epoch  7, batch     1 | loss: 9.4236488Losses:  13.441953659057617 5.576786518096924 5.573636531829834
CurrentTrain: epoch  8, batch     0 | loss: 13.4419537Losses:  8.945125579833984 1.2869864702224731 5.5812273025512695
CurrentTrain: epoch  8, batch     1 | loss: 8.9451256Losses:  13.888874053955078 6.172226905822754 5.563568592071533
CurrentTrain: epoch  9, batch     0 | loss: 13.8888741Losses:  9.440410614013672 1.7606173753738403 5.556005001068115
CurrentTrain: epoch  9, batch     1 | loss: 9.4404106
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones found her new home in san francisco, where she works as a software engineer. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith spent a lot of time in boston, where he attended university and made lifelong friends. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  

Relation: person cities of residence  
Context: -lrb- following her promotion, emily davis relocated to austin, a city known for its vibrant music scene and tech industry. -rrb-  
Head Entity: emily davis  
Tail Entity: austin  

Relation: person cities of residence  
Context: -lrb- after retiring, john doe decided to settle down in miami, enjoying the warm weather and beautiful beaches. -rrb-  
Head Entity: john doe  
Tail Entity: miami  

Relation: person cities of residence  
Context: -lrb- having grown up in new york, alice brown returned to her roots after years of living in los angeles. -rrb-  
Head Entity: alice brown  
Tail Entity: new york  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: born in baltimore in 1922 , parren mitchell was a graduate of morgan state college and earned a master 's degree from the university of maryland , according to biographical information supplied by cummings ' office .
Head Entity: parren mitchell
Tail Entity: university of maryland
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: After completing high school in 1995, Sarah Johnson went on to study at the University of California, Los Angeles, where she earned her bachelor's degree in sociology.  
Head Entity: Sarah Johnson  
Tail Entity: University of California, Los Angeles  

Relation: person schools attended  
Context: Mark Thompson, a renowned physicist, received his education at the Massachusetts Institute of Technology, where he completed both his undergraduate and graduate studies.  
Head Entity: Mark Thompson  
Tail Entity: Massachusetts Institute of Technology  

Relation: person schools attended  
Context: Growing up in a small town, Emily Chen attended Stanford University, where she majored in computer science and graduated with honors.  
Head Entity: Emily Chen  
Tail Entity: Stanford University  

Relation: person schools attended  
Context: James Rodriguez, a celebrated author, was a student at the University of Texas at Austin, where he developed his passion for creative writing.  
Head Entity: James Rodriguez  
Tail Entity: University of Texas at Austin  

Relation: person schools attended  
Context: After moving to New York City, Lisa Patel enrolled at Columbia University, where she pursued her master's degree in public health.  
Head Entity: Lisa Patel  
Tail Entity: Columbia University  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: u.s. rep. parren mitchell , founding member of congressional black caucus , dies at 85
Head Entity: parren mitchell
Tail Entity: u.s.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england at the age of 76  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: the famous author gabriel garcia marquez died in mexico city, mexico, leaving behind a legacy of magical realism  
Head Entity: gabriel garcia marquez  
Tail Entity: mexico  

Relation: person country of death  
Context: legendary musician freddie mercury succumbed to aids-related complications in london, united kingdom  
Head Entity: freddie mercury  
Tail Entity: united kingdom  

Relation: person country of death  
Context: former south african president nelson mandela passed away peacefully in his home in johannesburg, south africa  
Head Entity: nelson mandela  
Tail Entity: south africa  

Relation: person country of death  
Context: iconic actress audrey hepburn died in tolochenaz, switzerland, after a long battle with cancer  
Head Entity: audrey hepburn  
Tail Entity: switzerland  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: in october 2004 , her son keith mcdonald , a los angeles water district official , was convicted of extortion in a case involving municipal contracts .
Head Entity: her
Tail Entity: keith mcdonald
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: After the birth of her daughter, Emily, she decided to take a break from her career to focus on raising her child.  
Head Entity: she  
Tail Entity: Emily  

Relation: person children  
Context: The famous actor often shares stories about his son, who has shown a keen interest in following in his father's footsteps.  
Head Entity: his  
Tail Entity: son  

Relation: person children  
Context: During the family reunion, her two children played together with their cousins, creating memories that would last a lifetime.  
Head Entity: her  
Tail Entity: children  

Relation: person children  
Context: He proudly announced that his daughter had been accepted into a prestigious university, marking a significant achievement for their family.  
Head Entity: his  
Tail Entity: daughter  

Relation: person children  
Context: The author dedicated her latest book to her son, who inspired many of the stories within its pages.  
Head Entity: her  
Tail Entity: son  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, which shocked his colleagues at the firm.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith was charged with assault following the altercation at the downtown bar last weekend.  
Head Entity: Smith  
Tail Entity: assault  

Relation: person charges  
Context: In a surprising turn of events, the police revealed that Martinez was charged with fraud after an extensive audit of his business practices.  
Head Entity: Martinez  
Tail Entity: fraud  

Relation: person charges  
Context: Following the protests, the city council announced that Thompson was charged with inciting violence during the demonstration.  
Head Entity: Thompson  
Tail Entity: inciting violence  

Relation: person charges  
Context: The court documents indicated that Lee was charged with theft after being caught on surveillance cameras stealing merchandise from the store.  
Head Entity: Lee  
Tail Entity: theft  
Losses:  24.531105041503906 0.9490818977355957 20.16161346435547
MemoryTrain:  epoch  0, batch     0 | loss: 24.5311050Losses:  23.30681800842285 1.5596944093704224 17.308347702026367
MemoryTrain:  epoch  0, batch     1 | loss: 23.3068180Losses:  14.930218696594238 1.0592834949493408 10.799863815307617
MemoryTrain:  epoch  0, batch     2 | loss: 14.9302187Losses:  21.091562271118164 0.7647536993026733 16.773334503173828
MemoryTrain:  epoch  0, batch     3 | loss: 21.0915623Losses:  24.339759826660156 0.5204697847366333 20.04371452331543
MemoryTrain:  epoch  0, batch     4 | loss: 24.3397598Losses:  21.185697555541992 0.8397853970527649 16.72960662841797
MemoryTrain:  epoch  0, batch     5 | loss: 21.1856976Losses:  17.69013214111328 1.0596270561218262 13.733442306518555
MemoryTrain:  epoch  0, batch     6 | loss: 17.6901321Losses:  18.251571655273438 0.921650767326355 13.769344329833984
MemoryTrain:  epoch  0, batch     7 | loss: 18.2515717Losses:  18.2152099609375 0.8773743510246277 13.811771392822266
MemoryTrain:  epoch  0, batch     8 | loss: 18.2152100Losses:  17.380168914794922 0.2647479176521301 13.698980331420898
MemoryTrain:  epoch  0, batch     9 | loss: 17.3801689Losses:  13.765420913696289 0.5205976366996765 10.795096397399902
MemoryTrain:  epoch  1, batch     0 | loss: 13.7654209Losses:  14.869129180908203 1.6006948947906494 10.792948722839355
MemoryTrain:  epoch  1, batch     1 | loss: 14.8691292Losses:  22.941028594970703 0.46539026498794556 19.924959182739258
MemoryTrain:  epoch  1, batch     2 | loss: 22.9410286Losses:  19.641223907470703 2.1694319248199463 13.81222915649414
MemoryTrain:  epoch  1, batch     3 | loss: 19.6412239Losses:  20.422475814819336 0.47286635637283325 16.84348487854004
MemoryTrain:  epoch  1, batch     4 | loss: 20.4224758Losses:  22.988388061523438 0.4959252178668976 19.895238876342773
MemoryTrain:  epoch  1, batch     5 | loss: 22.9883881Losses:  15.312862396240234 0.6790655255317688 10.877789497375488
MemoryTrain:  epoch  1, batch     6 | loss: 15.3128624Losses:  22.156574249267578 1.8196697235107422 16.828102111816406
MemoryTrain:  epoch  1, batch     7 | loss: 22.1565742Losses:  17.61962127685547 1.3124234676361084 13.717957496643066
MemoryTrain:  epoch  1, batch     8 | loss: 17.6196213Losses:  17.13149642944336 0.27621519565582275 13.820322036743164
MemoryTrain:  epoch  1, batch     9 | loss: 17.1314964Losses:  18.255970001220703 1.486736536026001 13.70543098449707
MemoryTrain:  epoch  2, batch     0 | loss: 18.2559700Losses:  26.713823318481445 1.3946852684020996 23.18694496154785
MemoryTrain:  epoch  2, batch     1 | loss: 26.7138233Losses:  13.997894287109375 0.7868937253952026 10.908865928649902
MemoryTrain:  epoch  2, batch     2 | loss: 13.9978943Losses:  14.141101837158203 1.0313776731491089 10.868639945983887
MemoryTrain:  epoch  2, batch     3 | loss: 14.1411018Losses:  12.444400787353516 1.7290844917297363 8.127693176269531
MemoryTrain:  epoch  2, batch     4 | loss: 12.4444008Losses:  20.529067993164062 0.8965407013893127 16.754079818725586
MemoryTrain:  epoch  2, batch     5 | loss: 20.5290680Losses:  17.007692337036133 0.8019782304763794 13.836402893066406
MemoryTrain:  epoch  2, batch     6 | loss: 17.0076923Losses:  15.212830543518066 1.7199656963348389 10.823269844055176
MemoryTrain:  epoch  2, batch     7 | loss: 15.2128305Losses:  20.123458862304688 1.0114198923110962 16.8090763092041
MemoryTrain:  epoch  2, batch     8 | loss: 20.1234589Losses:  12.137714385986328 1.4199970960617065 8.15688419342041
MemoryTrain:  epoch  2, batch     9 | loss: 12.1377144Losses:  19.72714614868164 0.7877359986305237 16.726634979248047
MemoryTrain:  epoch  3, batch     0 | loss: 19.7271461Losses:  20.26287841796875 0.7930988073348999 16.739845275878906
MemoryTrain:  epoch  3, batch     1 | loss: 20.2628784Losses:  22.750558853149414 0.7675647735595703 19.869646072387695
MemoryTrain:  epoch  3, batch     2 | loss: 22.7505589Losses:  20.130619049072266 1.088802456855774 16.758453369140625
MemoryTrain:  epoch  3, batch     3 | loss: 20.1306190Losses:  17.418636322021484 1.3752192258834839 13.757572174072266
MemoryTrain:  epoch  3, batch     4 | loss: 17.4186363Losses:  11.713183403015137 1.4022719860076904 8.06508731842041
MemoryTrain:  epoch  3, batch     5 | loss: 11.7131834Losses:  26.037378311157227 0.7615898847579956 23.14684295654297
MemoryTrain:  epoch  3, batch     6 | loss: 26.0373783Losses:  26.11697769165039 0.7814557552337646 23.11474609375
MemoryTrain:  epoch  3, batch     7 | loss: 26.1169777Losses:  16.764278411865234 0.7541639804840088 13.773834228515625
MemoryTrain:  epoch  3, batch     8 | loss: 16.7642784Losses:  13.511632919311523 0.5592800378799438 10.8001127243042
MemoryTrain:  epoch  3, batch     9 | loss: 13.5116329Losses:  20.189077377319336 1.1745059490203857 16.790802001953125
MemoryTrain:  epoch  4, batch     0 | loss: 20.1890774Losses:  22.591222763061523 0.5341646671295166 19.94569206237793
MemoryTrain:  epoch  4, batch     1 | loss: 22.5912228Losses:  14.376614570617676 1.1959857940673828 10.84557819366455
MemoryTrain:  epoch  4, batch     2 | loss: 14.3766146Losses:  11.524024963378906 0.8508844375610352 8.079212188720703
MemoryTrain:  epoch  4, batch     3 | loss: 11.5240250Losses:  13.817682266235352 0.7453774213790894 10.84294319152832
MemoryTrain:  epoch  4, batch     4 | loss: 13.8176823Losses:  20.128332138061523 1.3901309967041016 16.738115310668945
MemoryTrain:  epoch  4, batch     5 | loss: 20.1283321Losses:  16.692928314208984 1.0021682977676392 13.712295532226562
MemoryTrain:  epoch  4, batch     6 | loss: 16.6929283Losses:  22.751510620117188 0.5769357681274414 19.841766357421875
MemoryTrain:  epoch  4, batch     7 | loss: 22.7515106Losses:  22.96790313720703 0.8903205394744873 19.92148208618164
MemoryTrain:  epoch  4, batch     8 | loss: 22.9679031Losses:  16.32378578186035 0.5551135540008545 13.724618911743164
MemoryTrain:  epoch  4, batch     9 | loss: 16.3237858Losses:  20.046396255493164 1.0496793985366821 16.75559425354004
MemoryTrain:  epoch  5, batch     0 | loss: 20.0463963Losses:  19.989063262939453 1.0573930740356445 16.755321502685547
MemoryTrain:  epoch  5, batch     1 | loss: 19.9890633Losses:  19.116031646728516 0.25221776962280273 16.73911476135254
MemoryTrain:  epoch  5, batch     2 | loss: 19.1160316Losses:  19.65050506591797 0.5764012336730957 16.77004623413086
MemoryTrain:  epoch  5, batch     3 | loss: 19.6505051Losses:  19.56214141845703 0.776496171951294 16.710548400878906
MemoryTrain:  epoch  5, batch     4 | loss: 19.5621414Losses:  17.140003204345703 1.3565618991851807 13.7138032913208
MemoryTrain:  epoch  5, batch     5 | loss: 17.1400032Losses:  16.469043731689453 0.7497009038925171 13.72083854675293
MemoryTrain:  epoch  5, batch     6 | loss: 16.4690437Losses:  19.1463680267334 0.5101653337478638 16.706470489501953
MemoryTrain:  epoch  5, batch     7 | loss: 19.1463680Losses:  21.209552764892578 2.416555404663086 16.727930068969727
MemoryTrain:  epoch  5, batch     8 | loss: 21.2095528Losses:  10.942752838134766 0.8080824613571167 8.070211410522461
MemoryTrain:  epoch  5, batch     9 | loss: 10.9427528Losses:  19.398244857788086 0.506308913230896 16.704938888549805
MemoryTrain:  epoch  6, batch     0 | loss: 19.3982449Losses:  22.431367874145508 0.5250292420387268 19.8342227935791
MemoryTrain:  epoch  6, batch     1 | loss: 22.4313679Losses:  17.11049461364746 1.0820398330688477 13.701448440551758
MemoryTrain:  epoch  6, batch     2 | loss: 17.1104946Losses:  19.824573516845703 1.0335530042648315 16.759302139282227
MemoryTrain:  epoch  6, batch     3 | loss: 19.8245735Losses:  13.5466890335083 0.5099982023239136 10.83177661895752
MemoryTrain:  epoch  6, batch     4 | loss: 13.5466890Losses:  22.655113220214844 0.7696672677993774 19.900632858276367
MemoryTrain:  epoch  6, batch     5 | loss: 22.6551132Losses:  8.944186210632324 1.4039037227630615 5.567354679107666
MemoryTrain:  epoch  6, batch     6 | loss: 8.9441862Losses:  16.187585830688477 0.49660515785217285 13.683950424194336
MemoryTrain:  epoch  6, batch     7 | loss: 16.1875858Losses:  13.774415016174316 0.9937905669212341 10.830218315124512
MemoryTrain:  epoch  6, batch     8 | loss: 13.7744150Losses:  8.528253555297852 0.916368842124939 5.582871913909912
MemoryTrain:  epoch  6, batch     9 | loss: 8.5282536Losses:  22.65781021118164 0.7542279362678528 19.877103805541992
MemoryTrain:  epoch  7, batch     0 | loss: 22.6578102Losses:  16.52823257446289 0.7870705127716064 13.660953521728516
MemoryTrain:  epoch  7, batch     1 | loss: 16.5282326Losses:  18.326881408691406 2.5979318618774414 13.666559219360352
MemoryTrain:  epoch  7, batch     2 | loss: 18.3268814Losses:  20.072376251220703 1.27180016040802 16.72603988647461
MemoryTrain:  epoch  7, batch     3 | loss: 20.0723763Losses:  16.69228744506836 0.964780330657959 13.73095989227295
MemoryTrain:  epoch  7, batch     4 | loss: 16.6922874Losses:  22.652597427368164 0.8023141026496887 19.883283615112305
MemoryTrain:  epoch  7, batch     5 | loss: 22.6525974Losses:  19.67055892944336 0.9625118970870972 16.719627380371094
MemoryTrain:  epoch  7, batch     6 | loss: 19.6705589Losses:  13.852147102355957 1.0449329614639282 10.825472831726074
MemoryTrain:  epoch  7, batch     7 | loss: 13.8521471Losses:  19.230806350708008 0.5380728840827942 16.688425064086914
MemoryTrain:  epoch  7, batch     8 | loss: 19.2308064Losses:  10.683331489562988 0.5561046004295349 8.096628189086914
MemoryTrain:  epoch  7, batch     9 | loss: 10.6833315Losses:  18.027624130249023 2.414430618286133 13.687145233154297
MemoryTrain:  epoch  8, batch     0 | loss: 18.0276241Losses:  16.412492752075195 0.759671151638031 13.733078002929688
MemoryTrain:  epoch  8, batch     1 | loss: 16.4124928Losses:  13.937601089477539 1.0801901817321777 10.833863258361816
MemoryTrain:  epoch  8, batch     2 | loss: 13.9376011Losses:  17.111661911010742 1.3350669145584106 13.692666053771973
MemoryTrain:  epoch  8, batch     3 | loss: 17.1116619Losses:  19.733823776245117 1.070481300354004 16.722339630126953
MemoryTrain:  epoch  8, batch     4 | loss: 19.7338238Losses:  16.69209098815918 0.9823859930038452 13.695086479187012
MemoryTrain:  epoch  8, batch     5 | loss: 16.6920910Losses:  14.384169578552246 1.5637004375457764 10.79782772064209
MemoryTrain:  epoch  8, batch     6 | loss: 14.3841696Losses:  16.49616241455078 0.8147122859954834 13.664247512817383
MemoryTrain:  epoch  8, batch     7 | loss: 16.4961624Losses:  17.26015853881836 1.6531339883804321 13.716009140014648
MemoryTrain:  epoch  8, batch     8 | loss: 17.2601585Losses:  13.515246391296387 0.8001854419708252 10.79663372039795
MemoryTrain:  epoch  8, batch     9 | loss: 13.5152464Losses:  18.95633316040039 0.25320178270339966 16.746944427490234
MemoryTrain:  epoch  9, batch     0 | loss: 18.9563332Losses:  16.774250030517578 1.0870108604431152 13.695158958435059
MemoryTrain:  epoch  9, batch     1 | loss: 16.7742500Losses:  16.451602935791016 0.7283482551574707 13.710881233215332
MemoryTrain:  epoch  9, batch     2 | loss: 16.4516029Losses:  22.077354431152344 0.24998310208320618 19.845523834228516
MemoryTrain:  epoch  9, batch     3 | loss: 22.0773544Losses:  13.203706741333008 0.5048781633377075 10.767746925354004
MemoryTrain:  epoch  9, batch     4 | loss: 13.2037067Losses:  22.22559928894043 0.48645031452178955 19.866683959960938
MemoryTrain:  epoch  9, batch     5 | loss: 22.2255993Losses:  22.262475967407227 0.5104267597198486 19.849790573120117
MemoryTrain:  epoch  9, batch     6 | loss: 22.2624760Losses:  19.349340438842773 0.7464947700500488 16.678333282470703
MemoryTrain:  epoch  9, batch     7 | loss: 19.3493404Losses:  17.349960327148438 1.649165153503418 13.705422401428223
MemoryTrain:  epoch  9, batch     8 | loss: 17.3499603Losses:  16.003929138183594 0.29666873812675476 13.71111011505127
MemoryTrain:  epoch  9, batch     9 | loss: 16.0039291
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 77.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 77.43%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 93.18%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 90.38%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 82.81%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 79.93%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 80.36%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 83.80%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.91%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 85.28%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.74%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 84.66%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 83.09%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 82.14%   [EVAL] batch:   35 | acc: 43.75%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 80.57%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 80.26%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 80.13%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 79.22%   [EVAL] batch:   40 | acc: 18.75%,  total acc: 77.74%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 75.89%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 74.27%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 73.44%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 73.06%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 73.10%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 73.27%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 73.44%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 73.60%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 73.62%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 74.64%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 75.12%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 75.35%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 74.00%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 73.36%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 72.41%   [EVAL] batch:   58 | acc: 25.00%,  total acc: 71.61%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 71.25%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 70.90%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 70.46%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 69.64%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 68.85%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 68.17%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 67.23%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 66.88%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 66.54%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 66.03%   [EVAL] batch:   69 | acc: 50.00%,  total acc: 65.80%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 65.32%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 65.02%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 64.38%   [EVAL] batch:   73 | acc: 0.00%,  total acc: 63.51%   [EVAL] batch:   74 | acc: 6.25%,  total acc: 62.75%   [EVAL] batch:   75 | acc: 6.25%,  total acc: 62.01%   [EVAL] batch:   76 | acc: 0.00%,  total acc: 61.20%   [EVAL] batch:   77 | acc: 43.75%,  total acc: 60.98%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 61.00%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 61.02%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 61.11%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 61.28%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 61.22%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 61.46%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 61.62%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 61.70%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 61.78%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 62.14%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 62.36%   [EVAL] batch:   89 | acc: 43.75%,  total acc: 62.15%   [EVAL] batch:   90 | acc: 68.75%,  total acc: 62.23%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 62.30%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 62.70%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 63.10%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 63.49%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 63.87%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 64.24%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 64.60%   [EVAL] batch:   98 | acc: 6.25%,  total acc: 64.02%   
cur_acc:  ['0.8598', '0.7548', '0.7875', '0.3778', '0.7743']
his_acc:  ['0.8598', '0.8500', '0.8219', '0.6474', '0.6402']
Clustering into  14  clusters
Clusters:  [ 2 13  4  7  1 10  0  6 10 12  2  2  0 11  3  1  4  4  3 10  1  2  5  2
  1  9  8  0  2  4  6]
Losses:  18.6396484375 7.346983909606934 5.644761085510254
CurrentTrain: epoch  0, batch     0 | loss: 18.6396484Losses:  12.064451217651367 1.5643246173858643 5.606454372406006
CurrentTrain: epoch  0, batch     1 | loss: 12.0644512Losses:  17.1295108795166 7.261706352233887 5.653328895568848
CurrentTrain: epoch  1, batch     0 | loss: 17.1295109Losses:  12.466690063476562 2.4055309295654297 5.610238552093506
CurrentTrain: epoch  1, batch     1 | loss: 12.4666901Losses:  15.659817695617676 6.1303019523620605 5.607089996337891
CurrentTrain: epoch  2, batch     0 | loss: 15.6598177Losses:  9.707664489746094 1.3792362213134766 5.6506667137146
CurrentTrain: epoch  2, batch     1 | loss: 9.7076645Losses:  17.042381286621094 8.572729110717773 5.655551910400391
CurrentTrain: epoch  3, batch     0 | loss: 17.0423813Losses:  9.263250350952148 4.488199234008789 1.412192702293396
CurrentTrain: epoch  3, batch     1 | loss: 9.2632504Losses:  14.80295467376709 6.591490268707275 5.605050086975098
CurrentTrain: epoch  4, batch     0 | loss: 14.8029547Losses:  8.085136413574219 2.2000925540924072 3.3847391605377197
CurrentTrain: epoch  4, batch     1 | loss: 8.0851364Losses:  14.000871658325195 6.00469446182251 5.618387699127197
CurrentTrain: epoch  5, batch     0 | loss: 14.0008717Losses:  9.490537643432617 1.7586517333984375 5.63412618637085
CurrentTrain: epoch  5, batch     1 | loss: 9.4905376Losses:  14.365304946899414 6.724720001220703 5.618991374969482
CurrentTrain: epoch  6, batch     0 | loss: 14.3653049Losses:  8.014976501464844 2.5744521617889404 3.3377938270568848
CurrentTrain: epoch  6, batch     1 | loss: 8.0149765Losses:  13.393903732299805 5.75422477722168 5.642017841339111
CurrentTrain: epoch  7, batch     0 | loss: 13.3939037Losses:  9.244511604309082 1.746725082397461 5.6132330894470215
CurrentTrain: epoch  7, batch     1 | loss: 9.2445116Losses:  13.166213989257812 5.689121246337891 5.620450496673584
CurrentTrain: epoch  8, batch     0 | loss: 13.1662140Losses:  9.081022262573242 1.688172459602356 5.570889472961426
CurrentTrain: epoch  8, batch     1 | loss: 9.0810223Losses:  13.16097640991211 5.750011444091797 5.595666408538818
CurrentTrain: epoch  9, batch     0 | loss: 13.1609764Losses:  9.10931396484375 1.6316853761672974 5.575667858123779
CurrentTrain: epoch  9, batch     1 | loss: 9.1093140
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: the company was founded as a hobby in 1979 by the husband and wife team of tim and nina zagat , a pair of lawyers who started recuiting friends to provide their own ratings of the food , decor and service of restaurants they frequented .
Head Entity: zagat
Tail Entity: 1979
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
Relation: organization founded  
Context: In 1998, the tech startup was established by a group of engineers who aimed to revolutionize the way people interact with technology.  
Head Entity: tech startup  
Tail Entity: 1998  

Relation: organization founded  
Context: The non-profit organization was created in 2005 to support local artists and promote cultural events in the community.  
Head Entity: non-profit organization  
Tail Entity: 2005  

Relation: organization founded  
Context: Founded in 2010, the environmental group has been at the forefront of advocating for sustainable practices and policies.  
Head Entity: environmental group  
Tail Entity: 2010  

Relation: organization founded  
Context: The famous coffee chain was born in 1971 when three friends opened their first store in Seattle, focusing on high-quality coffee beans.  
Head Entity: coffee chain  
Tail Entity: 1971  

Relation: organization founded  
Context: The educational institution was established in 1885, aiming to provide quality education to underprivileged children in the region.  
Head Entity: educational institution  
Tail Entity: 1885  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: virginia republican jo ann davis passed away on saturday at the age of 57 .
Head Entity: jo ann davis
Tail Entity: 57
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: the famous actor robert downey jr. celebrated his 56th birthday last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: on her 30th birthday, emily decided to throw a big party for her friends.  
Head Entity: emily  
Tail Entity: 30  

Relation: person age  
Context: the renowned scientist albert einstein was born in 1879, making him 76 years old when he passed away.  
Head Entity: albert einstein  
Tail Entity: 76  

Relation: person age  
Context: at the age of 45, sarah finally achieved her dream of becoming a published author.  
Head Entity: sarah  
Tail Entity: 45  

Relation: person age  
Context: during the interview, the 22-year-old athlete shared his journey to success.  
Head Entity: the athlete  
Tail Entity: 22  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in barcelona and spent her childhood there before moving to madrid.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during the summer of 1985, john was born in new orleans, a city known for its vibrant culture.  
Head Entity: john  
Tail Entity: new orleans  

Relation: person city of birth  
Context: after years of research, we discovered that the famous artist was born in amsterdam in the late 19th century.  
Head Entity: the famous artist  
Tail Entity: amsterdam  

Relation: person city of birth  
Context: in a small town near the coast, maria was born in san diego, where she developed her love for the ocean.  
Head Entity: maria  
Tail Entity: san diego  

Relation: person city of birth  
Context: the renowned scientist was born in tokyo, where he later returned to conduct groundbreaking research.  
Head Entity: the renowned scientist  
Tail Entity: tokyo  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: it was berger who made clarke a member of the white house principals committee when it met to discuss terrorist threats , allowing an otherwise middle-ranking nsc bureaucrat to treat tenet and secretary of state madeleine albright as equals -lrb- which the empire-building clarke was pleased to do -rrb- .
Head Entity: nsc
Tail Entity: white house principals committee
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
1. Relation: organization members  
   Context: The board of directors at Tech Innovations Inc. decided to appoint Sarah as a member of the advisory committee to enhance their strategic planning.  
   Head Entity: Tech Innovations Inc.  
   Tail Entity: advisory committee  

2. Relation: organization members  
   Context: During the annual conference, the president of the National Wildlife Federation announced that John would be joining the team as a member of the conservation committee.  
   Head Entity: National Wildlife Federation  
   Tail Entity: conservation committee  

3. Relation: organization members  
   Context: After a rigorous selection process, the University Alumni Association welcomed Dr. Smith as a new member of its executive board to help foster connections among graduates.  
   Head Entity: University Alumni Association  
   Tail Entity: executive board  

4. Relation: organization members  
   Context: The local chapter of the Red Cross proudly introduced Maria as a member of their emergency response team during the volunteer recruitment event.  
   Head Entity: Red Cross  
   Tail Entity: emergency response team  

5. Relation: organization members  
   Context: The International Business Council recognized David for his contributions and officially made him a member of the global trade committee.  
   Head Entity: International Business Council  
   Tail Entity: global trade committee  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: the pope defended his action on the grounds that he could not refuse an audience to a head of state from a country with a strong catholic tradition unless he had clear-cut proof of the allegations against him .
Head Entity: he
Tail Entity: catholic
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: During the ceremony, the rabbi spoke about the importance of faith and community in Judaism, emphasizing how every member plays a vital role.  
Head Entity: rabbi  
Tail Entity: Judaism  

Relation: person religion  
Context: The famous author often drew inspiration from his Hindu upbringing, which shaped many of his philosophical views and literary themes.  
Head Entity: author  
Tail Entity: Hindu  

Relation: person religion  
Context: She often participates in the local mosque's events, reflecting her deep commitment to her Islamic faith and community service.  
Head Entity: She  
Tail Entity: Islamic  

Relation: person religion  
Context: The activist's speeches frequently highlight the teachings of Buddhism, which guide her approach to social justice and compassion.  
Head Entity: activist  
Tail Entity: Buddhism  

Relation: person religion  
Context: The politician's campaign was heavily influenced by his upbringing in a Christian household, which he credits for his moral compass.  
Head Entity: politician  
Tail Entity: Christian  
Losses:  16.46224594116211 1.412139892578125 10.889738082885742
MemoryTrain:  epoch  0, batch     0 | loss: 16.4622459Losses:  19.893701553344727 0.2626854479312897 16.824983596801758
MemoryTrain:  epoch  0, batch     1 | loss: 19.8937016Losses:  23.888904571533203 1.3412765264511108 19.87017250061035
MemoryTrain:  epoch  0, batch     2 | loss: 23.8889046Losses:  21.09337615966797 0.799034595489502 16.81277084350586
MemoryTrain:  epoch  0, batch     3 | loss: 21.0933762Losses:  14.694738388061523 0.709213376045227 10.815832138061523
MemoryTrain:  epoch  0, batch     4 | loss: 14.6947384Losses:  22.47602653503418 1.4058303833007812 16.792322158813477
MemoryTrain:  epoch  0, batch     5 | loss: 22.4760265Losses:  23.573211669921875 0.566043496131897 19.99777603149414
MemoryTrain:  epoch  0, batch     6 | loss: 23.5732117Losses:  17.18011474609375 0.7104471921920776 13.679464340209961
MemoryTrain:  epoch  0, batch     7 | loss: 17.1801147Losses:  18.188148498535156 0.7890831232070923 13.714025497436523
MemoryTrain:  epoch  0, batch     8 | loss: 18.1881485Losses:  23.292713165283203 0.24931758642196655 19.87436866760254
MemoryTrain:  epoch  0, batch     9 | loss: 23.2927132Losses:  15.77922248840332 1.592942237854004 10.782236099243164
MemoryTrain:  epoch  0, batch    10 | loss: 15.7792225Losses:  14.40355110168457 1.0876981019973755 10.889103889465332
MemoryTrain:  epoch  0, batch    11 | loss: 14.4035511Losses:  15.148706436157227 1.136322021484375 10.802611351013184
MemoryTrain:  epoch  1, batch     0 | loss: 15.1487064Losses:  20.44819450378418 1.0095677375793457 16.79258155822754
MemoryTrain:  epoch  1, batch     1 | loss: 20.4481945Losses:  18.088905334472656 0.3356734812259674 13.69112777709961
MemoryTrain:  epoch  1, batch     2 | loss: 18.0889053Losses:  19.571212768554688 0.24309727549552917 16.723426818847656
MemoryTrain:  epoch  1, batch     3 | loss: 19.5712128Losses:  14.433847427368164 1.3166892528533936 10.839159965515137
MemoryTrain:  epoch  1, batch     4 | loss: 14.4338474Losses:  17.9183406829834 0.5295947790145874 13.69965934753418
MemoryTrain:  epoch  1, batch     5 | loss: 17.9183407Losses:  23.386205673217773 0.5173540711402893 19.922611236572266
MemoryTrain:  epoch  1, batch     6 | loss: 23.3862057Losses:  16.715351104736328 3.860745668411255 10.811751365661621
MemoryTrain:  epoch  1, batch     7 | loss: 16.7153511Losses:  19.095966339111328 2.6448419094085693 13.7008695602417
MemoryTrain:  epoch  1, batch     8 | loss: 19.0959663Losses:  17.420791625976562 0.8453896045684814 13.770063400268555
MemoryTrain:  epoch  1, batch     9 | loss: 17.4207916Losses:  16.925495147705078 0.5240532159805298 13.779420852661133
MemoryTrain:  epoch  1, batch    10 | loss: 16.9254951Losses:  12.808077812194824 -0.0 10.843602180480957
MemoryTrain:  epoch  1, batch    11 | loss: 12.8080778Losses:  17.380130767822266 0.869239091873169 13.688017845153809
MemoryTrain:  epoch  2, batch     0 | loss: 17.3801308Losses:  19.38780975341797 0.504111647605896 16.751352310180664
MemoryTrain:  epoch  2, batch     1 | loss: 19.3878098Losses:  19.515941619873047 0.5489272475242615 16.713531494140625
MemoryTrain:  epoch  2, batch     2 | loss: 19.5159416Losses:  32.37501907348633 0.23761782050132751 29.953372955322266
MemoryTrain:  epoch  2, batch     3 | loss: 32.3750191Losses:  16.80754852294922 0.775090217590332 13.68599796295166
MemoryTrain:  epoch  2, batch     4 | loss: 16.8075485Losses:  22.819229125976562 0.8659080266952515 19.84857177734375
MemoryTrain:  epoch  2, batch     5 | loss: 22.8192291Losses:  14.433323860168457 1.443974494934082 10.800139427185059
MemoryTrain:  epoch  2, batch     6 | loss: 14.4333239Losses:  19.649036407470703 0.8013079166412354 16.73549461364746
MemoryTrain:  epoch  2, batch     7 | loss: 19.6490364Losses:  19.238500595092773 0.2576063275337219 16.70632553100586
MemoryTrain:  epoch  2, batch     8 | loss: 19.2385006Losses:  16.739181518554688 0.9811939597129822 13.655731201171875
MemoryTrain:  epoch  2, batch     9 | loss: 16.7391815Losses:  22.6834774017334 0.7690883874893188 19.85871696472168
MemoryTrain:  epoch  2, batch    10 | loss: 22.6834774Losses:  10.859423637390137 0.3388975262641907 8.110066413879395
MemoryTrain:  epoch  2, batch    11 | loss: 10.8594236Losses:  16.810972213745117 1.0592304468154907 13.70538330078125
MemoryTrain:  epoch  3, batch     0 | loss: 16.8109722Losses:  19.23262596130371 0.49791666865348816 16.713918685913086
MemoryTrain:  epoch  3, batch     1 | loss: 19.2326260Losses:  19.66497039794922 0.7537963390350342 16.740896224975586
MemoryTrain:  epoch  3, batch     2 | loss: 19.6649704Losses:  22.70803451538086 0.5245727300643921 19.872730255126953
MemoryTrain:  epoch  3, batch     3 | loss: 22.7080345Losses:  13.580573081970215 0.7328054904937744 10.856034278869629
MemoryTrain:  epoch  3, batch     4 | loss: 13.5805731Losses:  14.27383804321289 1.4737350940704346 10.787591934204102
MemoryTrain:  epoch  3, batch     5 | loss: 14.2738380Losses:  19.74722671508789 1.015519380569458 16.72685432434082
MemoryTrain:  epoch  3, batch     6 | loss: 19.7472267Losses:  23.030105590820312 0.7721232175827026 19.9906063079834
MemoryTrain:  epoch  3, batch     7 | loss: 23.0301056Losses:  19.484325408935547 0.7762746810913086 16.715543746948242
MemoryTrain:  epoch  3, batch     8 | loss: 19.4843254Losses:  16.450870513916016 0.7511494159698486 13.712507247924805
MemoryTrain:  epoch  3, batch     9 | loss: 16.4508705Losses:  26.047992706298828 0.9053770303726196 23.115190505981445
MemoryTrain:  epoch  3, batch    10 | loss: 26.0479927Losses:  10.605672836303711 0.5681540369987488 8.108341217041016
MemoryTrain:  epoch  3, batch    11 | loss: 10.6056728Losses:  16.737873077392578 0.8483823537826538 13.714200973510742
MemoryTrain:  epoch  4, batch     0 | loss: 16.7378731Losses:  19.181482315063477 0.480907142162323 16.6961612701416
MemoryTrain:  epoch  4, batch     1 | loss: 19.1814823Losses:  14.389413833618164 1.5522785186767578 10.800477027893066
MemoryTrain:  epoch  4, batch     2 | loss: 14.3894138Losses:  22.6298828125 0.7576735019683838 19.903797149658203
MemoryTrain:  epoch  4, batch     3 | loss: 22.6298828Losses:  17.3483829498291 1.6405575275421143 13.701988220214844
MemoryTrain:  epoch  4, batch     4 | loss: 17.3483829Losses:  14.038291931152344 1.3101634979248047 10.805437088012695
MemoryTrain:  epoch  4, batch     5 | loss: 14.0382919Losses:  19.565265655517578 0.9796007871627808 16.684946060180664
MemoryTrain:  epoch  4, batch     6 | loss: 19.5652657Losses:  17.45182228088379 1.6982791423797607 13.666108131408691
MemoryTrain:  epoch  4, batch     7 | loss: 17.4518223Losses:  16.422096252441406 0.7456507682800293 13.684064865112305
MemoryTrain:  epoch  4, batch     8 | loss: 16.4220963Losses:  10.810298919677734 0.7665640115737915 8.075590133666992
MemoryTrain:  epoch  4, batch     9 | loss: 10.8102989Losses:  19.88070297241211 1.1442532539367676 16.731321334838867
MemoryTrain:  epoch  4, batch    10 | loss: 19.8807030Losses:  13.795633316040039 1.0728203058242798 10.817492485046387
MemoryTrain:  epoch  4, batch    11 | loss: 13.7956333Losses:  22.22319793701172 0.4856536388397217 19.81814956665039
MemoryTrain:  epoch  5, batch     0 | loss: 22.2231979Losses:  22.256622314453125 0.4857621490955353 19.831378936767578
MemoryTrain:  epoch  5, batch     1 | loss: 22.2566223Losses:  22.39350700378418 0.5516254901885986 19.8459529876709
MemoryTrain:  epoch  5, batch     2 | loss: 22.3935070Losses:  16.584842681884766 0.8125839233398438 13.74294376373291
MemoryTrain:  epoch  5, batch     3 | loss: 16.5848427Losses:  22.86783218383789 1.08673095703125 19.83924674987793
MemoryTrain:  epoch  5, batch     4 | loss: 22.8678322Losses:  19.656694412231445 0.9947768449783325 16.69072914123535
MemoryTrain:  epoch  5, batch     5 | loss: 19.6566944Losses:  16.740962982177734 1.2083089351654053 13.655391693115234
MemoryTrain:  epoch  5, batch     6 | loss: 16.7409630Losses:  19.682363510131836 1.0188382863998413 16.71354866027832
MemoryTrain:  epoch  5, batch     7 | loss: 19.6823635Losses:  18.913053512573242 0.2584240734577179 16.707082748413086
MemoryTrain:  epoch  5, batch     8 | loss: 18.9130535Losses:  17.279010772705078 1.6470396518707275 13.648797035217285
MemoryTrain:  epoch  5, batch     9 | loss: 17.2790108Losses:  16.12824058532715 0.5080388784408569 13.685782432556152
MemoryTrain:  epoch  5, batch    10 | loss: 16.1282406Losses:  12.732924461364746 -0.0 10.82462215423584
MemoryTrain:  epoch  5, batch    11 | loss: 12.7329245Losses:  23.019058227539062 1.2951292991638184 19.84630012512207
MemoryTrain:  epoch  6, batch     0 | loss: 23.0190582Losses:  21.96441650390625 0.24836915731430054 19.84469985961914
MemoryTrain:  epoch  6, batch     1 | loss: 21.9644165Losses:  16.042892456054688 0.5053597092628479 13.657630920410156
MemoryTrain:  epoch  6, batch     2 | loss: 16.0428925Losses:  19.46213150024414 0.7938951253890991 16.688982009887695
MemoryTrain:  epoch  6, batch     3 | loss: 19.4621315Losses:  18.853992462158203 0.2371303141117096 16.665456771850586
MemoryTrain:  epoch  6, batch     4 | loss: 18.8539925Losses:  13.964940071105957 1.2864782810211182 10.806852340698242
MemoryTrain:  epoch  6, batch     5 | loss: 13.9649401Losses:  16.114524841308594 0.5277219414710999 13.672895431518555
MemoryTrain:  epoch  6, batch     6 | loss: 16.1145248Losses:  16.29365348815918 0.5907014608383179 13.655941009521484
MemoryTrain:  epoch  6, batch     7 | loss: 16.2936535Losses:  13.450888633728027 0.7551611661911011 10.775975227355957
MemoryTrain:  epoch  6, batch     8 | loss: 13.4508886Losses:  19.089324951171875 0.5139942169189453 16.68086814880371
MemoryTrain:  epoch  6, batch     9 | loss: 19.0893250Losses:  20.030380249023438 1.2988250255584717 16.70191192626953
MemoryTrain:  epoch  6, batch    10 | loss: 20.0303802Losses:  8.019769668579102 0.5732465982437134 5.552981853485107
MemoryTrain:  epoch  6, batch    11 | loss: 8.0197697Losses:  16.482559204101562 0.928407609462738 13.683709144592285
MemoryTrain:  epoch  7, batch     0 | loss: 16.4825592Losses:  19.71129608154297 1.0326379537582397 16.715145111083984
MemoryTrain:  epoch  7, batch     1 | loss: 19.7112961Losses:  13.494129180908203 0.7566955089569092 10.816866874694824
MemoryTrain:  epoch  7, batch     2 | loss: 13.4941292Losses:  17.004817962646484 0.9861658811569214 13.671733856201172
MemoryTrain:  epoch  7, batch     3 | loss: 17.0048180Losses:  16.285888671875 0.7511060833930969 13.66064167022705
MemoryTrain:  epoch  7, batch     4 | loss: 16.2858887Losses:  22.21649169921875 0.4683186411857605 19.840579986572266
MemoryTrain:  epoch  7, batch     5 | loss: 22.2164917Losses:  16.13201141357422 0.4854894280433655 13.707489013671875
MemoryTrain:  epoch  7, batch     6 | loss: 16.1320114Losses:  16.29668426513672 0.7258329391479492 13.63824462890625
MemoryTrain:  epoch  7, batch     7 | loss: 16.2966843Losses:  16.100067138671875 0.5054821968078613 13.642210960388184
MemoryTrain:  epoch  7, batch     8 | loss: 16.1000671Losses:  16.76974868774414 1.1044477224349976 13.683757781982422
MemoryTrain:  epoch  7, batch     9 | loss: 16.7697487Losses:  19.344825744628906 0.7471209764480591 16.67374038696289
MemoryTrain:  epoch  7, batch    10 | loss: 19.3448257Losses:  12.667689323425293 -0.0 10.779803276062012
MemoryTrain:  epoch  7, batch    11 | loss: 12.6676893Losses:  14.22415542602539 1.5533666610717773 10.790797233581543
MemoryTrain:  epoch  8, batch     0 | loss: 14.2241554Losses:  19.48675537109375 0.8396070003509521 16.676984786987305
MemoryTrain:  epoch  8, batch     1 | loss: 19.4867554Losses:  17.460922241210938 1.7298409938812256 13.66855239868164
MemoryTrain:  epoch  8, batch     2 | loss: 17.4609222Losses:  16.615543365478516 0.9726640582084656 13.690067291259766
MemoryTrain:  epoch  8, batch     3 | loss: 16.6155434Losses:  18.953481674194336 0.2807864546775818 16.681936264038086
MemoryTrain:  epoch  8, batch     4 | loss: 18.9534817Losses:  18.826499938964844 0.25195634365081787 16.670034408569336
MemoryTrain:  epoch  8, batch     5 | loss: 18.8264999Losses:  22.16120147705078 0.4696270227432251 19.816312789916992
MemoryTrain:  epoch  8, batch     6 | loss: 22.1612015Losses:  22.266613006591797 0.5420024394989014 19.81644630432129
MemoryTrain:  epoch  8, batch     7 | loss: 22.2666130Losses:  25.81023406982422 0.7524563670158386 23.13162612915039
MemoryTrain:  epoch  8, batch     8 | loss: 25.8102341Losses:  22.146697998046875 0.46545106172561646 19.809844970703125
MemoryTrain:  epoch  8, batch     9 | loss: 22.1466980Losses:  20.8768367767334 2.3282246589660645 16.698461532592773
MemoryTrain:  epoch  8, batch    10 | loss: 20.8768368Losses:  12.690415382385254 -0.0 10.783553123474121
MemoryTrain:  epoch  8, batch    11 | loss: 12.6904154Losses:  16.428434371948242 0.7407034039497375 13.673544883728027
MemoryTrain:  epoch  9, batch     0 | loss: 16.4284344Losses:  14.217705726623535 1.572645902633667 10.77946949005127
MemoryTrain:  epoch  9, batch     1 | loss: 14.2177057Losses:  16.13799285888672 0.5264807939529419 13.686052322387695
MemoryTrain:  epoch  9, batch     2 | loss: 16.1379929Losses:  22.790189743041992 1.0921071767807007 19.820905685424805
MemoryTrain:  epoch  9, batch     3 | loss: 22.7901897Losses:  21.942298889160156 0.24760188162326813 19.812837600708008
MemoryTrain:  epoch  9, batch     4 | loss: 21.9422989Losses:  22.579933166503906 0.8677175641059875 19.822887420654297
MemoryTrain:  epoch  9, batch     5 | loss: 22.5799332Losses:  21.951635360717773 0.2417466640472412 19.847444534301758
MemoryTrain:  epoch  9, batch     6 | loss: 21.9516354Losses:  22.403430938720703 0.714368462562561 19.835620880126953
MemoryTrain:  epoch  9, batch     7 | loss: 22.4034309Losses:  13.139379501342773 0.4650547504425049 10.775461196899414
MemoryTrain:  epoch  9, batch     8 | loss: 13.1393795Losses:  16.112451553344727 0.5109734535217285 13.655206680297852
MemoryTrain:  epoch  9, batch     9 | loss: 16.1124516Losses:  17.290910720825195 1.7525510787963867 13.648805618286133
MemoryTrain:  epoch  9, batch    10 | loss: 17.2909107Losses:  10.243586540222168 0.2875668704509735 8.054716110229492
MemoryTrain:  epoch  9, batch    11 | loss: 10.2435865
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 98.61%   [EVAL] batch:    9 | acc: 25.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 83.04%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 90.10%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 83.93%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 81.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 79.78%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 77.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.73%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 81.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 82.18%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 83.14%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 82.54%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 81.61%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 80.90%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 80.91%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 81.09%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 80.62%   [EVAL] batch:   40 | acc: 25.00%,  total acc: 79.27%   [EVAL] batch:   41 | acc: 12.50%,  total acc: 77.68%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 76.16%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 75.99%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 75.83%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 75.54%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 75.53%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 75.52%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 75.64%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 75.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 75.86%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 76.77%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 76.70%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 75.67%   [EVAL] batch:   56 | acc: 25.00%,  total acc: 74.78%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 73.71%   [EVAL] batch:   58 | acc: 12.50%,  total acc: 72.67%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 71.98%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 71.41%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 70.77%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 69.94%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 69.14%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 68.27%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 67.33%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 67.16%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 66.91%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 66.76%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:   70 | acc: 50.00%,  total acc: 66.64%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 66.67%   [EVAL] batch:   72 | acc: 18.75%,  total acc: 66.01%   [EVAL] batch:   73 | acc: 0.00%,  total acc: 65.12%   [EVAL] batch:   74 | acc: 0.00%,  total acc: 64.25%   [EVAL] batch:   75 | acc: 0.00%,  total acc: 63.40%   [EVAL] batch:   76 | acc: 0.00%,  total acc: 62.58%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 62.10%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 62.10%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 61.95%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 61.88%   [EVAL] batch:   81 | acc: 12.50%,  total acc: 61.28%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 60.69%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 60.12%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 59.41%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 58.79%   [EVAL] batch:   86 | acc: 6.25%,  total acc: 58.19%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 58.52%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 58.78%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 58.82%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 59.00%   [EVAL] batch:   91 | acc: 81.25%,  total acc: 59.24%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 59.68%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 60.11%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 60.53%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 60.94%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 61.34%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 61.73%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 62.12%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 64.29%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 64.89%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 64.47%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 64.33%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 64.49%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 64.47%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 64.45%   
cur_acc:  ['0.8598', '0.7548', '0.7875', '0.3778', '0.7743', '0.8304']
his_acc:  ['0.8598', '0.8500', '0.8219', '0.6474', '0.6402', '0.6445']
Clustering into  17  clusters
Clusters:  [ 2 14  0 15 16 10  1  4 10 13  7  2  1 12  3  5  0  0  3 10  5  2 11  7
  5  8  9  6  2  0  4  6  2  5 14  7]
Losses:  18.195457458496094 7.589507102966309 5.707301139831543
CurrentTrain: epoch  0, batch     0 | loss: 18.1954575Losses:  12.386905670166016 2.760100841522217 3.410048007965088
CurrentTrain: epoch  0, batch     1 | loss: 12.3869057Losses:  16.879981994628906 7.032276153564453 5.620100498199463
CurrentTrain: epoch  1, batch     0 | loss: 16.8799820Losses:  11.541475296020508 2.0324270725250244 5.617778301239014
CurrentTrain: epoch  1, batch     1 | loss: 11.5414753Losses:  16.040231704711914 6.798086643218994 5.6211981773376465
CurrentTrain: epoch  2, batch     0 | loss: 16.0402317Losses:  10.690412521362305 2.0177431106567383 5.602206707000732
CurrentTrain: epoch  2, batch     1 | loss: 10.6904125Losses:  14.438590049743652 6.219304084777832 5.576568603515625
CurrentTrain: epoch  3, batch     0 | loss: 14.4385900Losses:  10.678262710571289 2.1720542907714844 5.60087251663208
CurrentTrain: epoch  3, batch     1 | loss: 10.6782627Losses:  14.300270080566406 6.310546398162842 5.6145243644714355
CurrentTrain: epoch  4, batch     0 | loss: 14.3002701Losses:  9.948776245117188 1.8607250452041626 5.589625835418701
CurrentTrain: epoch  4, batch     1 | loss: 9.9487762Losses:  15.269620895385742 7.134295463562012 5.569021701812744
CurrentTrain: epoch  5, batch     0 | loss: 15.2696209Losses:  8.076753616333008 2.645928382873535 3.3255457878112793
CurrentTrain: epoch  5, batch     1 | loss: 8.0767536Losses:  15.280807495117188 7.417579650878906 5.569921970367432
CurrentTrain: epoch  6, batch     0 | loss: 15.2808075Losses:  8.764509201049805 3.424280881881714 3.321049690246582
CurrentTrain: epoch  6, batch     1 | loss: 8.7645092Losses:  15.216869354248047 7.371740341186523 5.57287073135376
CurrentTrain: epoch  7, batch     0 | loss: 15.2168694Losses:  8.498147010803223 3.3413329124450684 3.3128037452697754
CurrentTrain: epoch  7, batch     1 | loss: 8.4981470Losses:  13.311396598815918 5.783154487609863 5.559621334075928
CurrentTrain: epoch  8, batch     0 | loss: 13.3113966Losses:  9.261629104614258 1.7699986696243286 5.55483341217041
CurrentTrain: epoch  8, batch     1 | loss: 9.2616291Losses:  15.256721496582031 7.799370288848877 5.562741756439209
CurrentTrain: epoch  9, batch     0 | loss: 15.2567215Losses:  7.888787746429443 4.3896050453186035 1.3915265798568726
CurrentTrain: epoch  9, batch     1 | loss: 7.8887877
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: born in 1985 in a small town in Texas, Sarah Johnson grew up with dreams of becoming a scientist.  
Head Entity: Sarah Johnson  
Tail Entity: 1985  

Relation: person date of birth  
Context: The famous actor, who was born in 1970, has captivated audiences for decades with his performances.  
Head Entity: The famous actor  
Tail Entity: 1970  

Relation: person date of birth  
Context: Albert Einstein, known for his groundbreaking theories, was born in the city of Ulm in 1879.  
Head Entity: Albert Einstein  
Tail Entity: 1879  

Relation: person date of birth  
Context: In 1992, a talented musician was born in Seattle, Washington, who would later change the music industry.  
Head Entity: a talented musician  
Tail Entity: 1992  

Relation: person date of birth  
Context: The renowned author, celebrated for her novels, was born in 1965 in a small village in England.  
Head Entity: The renowned author  
Tail Entity: 1965  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jane matilda bolin was born on april 11 , 1908 , in poughkeepsie , ny .
Head Entity: jane matilda bolin
Tail Entity: ny
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: albert einstein was born on march 14, 1879, in ulm, württemberg, germany.  
Head Entity: albert einstein  
Tail Entity: württemberg  

Relation: person stateorprovince of birth  
Context: marilyn monroe was born on june 1, 1926, in los angeles, california.  
Head Entity: marilyn monroe  
Tail Entity: california  

Relation: person stateorprovince of birth  
Context: barack obama was born on august 4, 1961, in honolulu, hawaii.  
Head Entity: barack obama  
Tail Entity: hawaii  

Relation: person stateorprovince of birth  
Context: agatha christie was born on september 15, 1890, in torquay, devon.  
Head Entity: agatha christie  
Tail Entity: devon  

Relation: person stateorprovince of birth  
Context: nelson mandela was born on july 18, 1918, in mvezo, eastern cape.  
Head Entity: nelson mandela  
Tail Entity: eastern cape  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: After the ceremony, Emily's father, John Smith, gave a heartfelt speech about family values and the importance of love.  
Head Entity: Emily  
Tail Entity: John Smith  

Relation: person parents  
Context: During the family reunion, Michael introduced his mother, Sarah Johnson, to his friends, highlighting her amazing cooking skills.  
Head Entity: Michael  
Tail Entity: Sarah Johnson  

Relation: person parents  
Context: In her memoir, Lisa shared stories about her father, Robert Brown, and the lessons he taught her about perseverance and hard work.  
Head Entity: Lisa  
Tail Entity: Robert Brown  

Relation: person parents  
Context: At the graduation ceremony, David's mother, Patricia Lee, proudly watched as he received his diploma, beaming with pride.  
Head Entity: David  
Tail Entity: Patricia Lee  

Relation: person parents  
Context: As they reminisced about their childhood, Anna spoke fondly of her father, George White, who always encouraged her to follow her dreams.  
Head Entity: Anna  
Tail Entity: George White  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: kell hath no fury : publicist and mtv reality star kelly cutrone is wasting no time in kicking her brands -lrb- including her p.r. firm people 's revolution and , increasingly , kelly cutrone herself -rrb- into high gear in 2010 .
Head Entity: kelly cutrone
Tail Entity: mtv
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work and dedication, Sarah Thompson has finally landed a job at one of the top tech companies in Silicon Valley, where she will be contributing to innovative projects.  
Head Entity: Sarah Thompson  
Tail Entity: top tech company  

Relation: person employee of  
Context: John Smith, a talented graphic designer, has been working for Creative Solutions for over five years, helping to shape their brand identity.  
Head Entity: John Smith  
Tail Entity: Creative Solutions  

Relation: person employee of  
Context: The renowned chef, Marco Pierre White, has been the head chef at several prestigious restaurants, showcasing his culinary skills to the world.  
Head Entity: Marco Pierre White  
Tail Entity: prestigious restaurants  

Relation: person employee of  
Context: Emily Chen, a passionate environmentalist, has joined Green Earth Initiative, where she will be leading projects aimed at sustainability and conservation.  
Head Entity: Emily Chen  
Tail Entity: Green Earth Initiative  

Relation: person employee of  
Context: After completing her degree in journalism, Lisa Martinez secured a position at Global News Network, where she reports on current events and social issues.  
Head Entity: Lisa Martinez  
Tail Entity: Global News Network  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john smith, 75, passed away peacefully on march 5 in his residence located in phoenix, arizona, surrounded by family and friends who cherished his memory.  
Head Entity: john smith  
Tail Entity: arizona  

Relation: person stateorprovince of death  
Context: after a long battle with cancer, elizabeth taylor, 79, died on march 23 at a hospital in los angeles, california, leaving behind a legacy of film and philanthropy.  
Head Entity: elizabeth taylor  
Tail Entity: california  

Relation: person stateorprovince of death  
Context: the renowned author, mark twain, died on april 21, 1910, in his home in stormfield, connecticut, where he spent his final years writing and reflecting on his life.  
Head Entity: mark twain  
Tail Entity: connecticut  

Relation: person stateorprovince of death  
Context: on january 15, 2020, the beloved musician, prince, was found dead in his home in minneapolis, minnesota, shocking fans around the world.  
Head Entity: prince  
Tail Entity: minnesota  

Relation: person stateorprovince of death  
Context: the famous civil rights leader, martin luther king jr., was assassinated on april 4, 1968, in memphis, tennessee, a tragic event that changed the course of history.  
Head Entity: martin luther king jr.  
Tail Entity: tennessee  
Losses:  33.83852005004883 0.5374565720558167 30.031574249267578
MemoryTrain:  epoch  0, batch     0 | loss: 33.8385201Losses:  23.155855178833008 0.7503034472465515 19.866741180419922
MemoryTrain:  epoch  0, batch     1 | loss: 23.1558552Losses:  27.61650276184082 0.28275489807128906 23.3515625
MemoryTrain:  epoch  0, batch     2 | loss: 27.6165028Losses:  24.745132446289062 1.0420455932617188 20.0153865814209
MemoryTrain:  epoch  0, batch     3 | loss: 24.7451324Losses:  27.11960220336914 1.0506199598312378 23.086828231811523
MemoryTrain:  epoch  0, batch     4 | loss: 27.1196022Losses:  26.978229522705078 0.24252891540527344 23.165868759155273
MemoryTrain:  epoch  0, batch     5 | loss: 26.9782295Losses:  18.428333282470703 1.6999166011810303 13.692220687866211
MemoryTrain:  epoch  0, batch     6 | loss: 18.4283333Losses:  31.010223388671875 0.26759278774261475 26.74554443359375
MemoryTrain:  epoch  0, batch     7 | loss: 31.0102234Losses:  21.14800262451172 0.25839513540267944 16.913618087768555
MemoryTrain:  epoch  0, batch     8 | loss: 21.1480026Losses:  26.249588012695312 0.25296834111213684 23.112163543701172
MemoryTrain:  epoch  0, batch     9 | loss: 26.2495880Losses:  21.147974014282227 0.7714735269546509 16.87859535217285
MemoryTrain:  epoch  0, batch    10 | loss: 21.1479740Losses:  20.659954071044922 0.8779639005661011 16.692060470581055
MemoryTrain:  epoch  0, batch    11 | loss: 20.6599541Losses:  15.497859954833984 1.8505529165267944 10.827295303344727
MemoryTrain:  epoch  0, batch    12 | loss: 15.4978600Losses:  16.319509506225586 0.40603986382484436 10.808740615844727
MemoryTrain:  epoch  0, batch    13 | loss: 16.3195095Losses:  23.013172149658203 0.25389254093170166 19.843307495117188
MemoryTrain:  epoch  1, batch     0 | loss: 23.0131721Losses:  26.126569747924805 0.23455624282360077 23.09308433532715
MemoryTrain:  epoch  1, batch     1 | loss: 26.1265697Losses:  32.92440414428711 0.2460746467113495 29.941417694091797
MemoryTrain:  epoch  1, batch     2 | loss: 32.9244041Losses:  27.15536117553711 1.044372797012329 23.10576057434082
MemoryTrain:  epoch  1, batch     3 | loss: 27.1553612Losses:  21.011987686157227 1.349518060684204 16.72794532775879
MemoryTrain:  epoch  1, batch     4 | loss: 21.0119877Losses:  26.869508743286133 1.0175285339355469 23.170289993286133
MemoryTrain:  epoch  1, batch     5 | loss: 26.8695087Losses:  23.542583465576172 0.5111179947853088 19.883264541625977
MemoryTrain:  epoch  1, batch     6 | loss: 23.5425835Losses:  20.384864807128906 1.0868735313415527 16.77001953125
MemoryTrain:  epoch  1, batch     7 | loss: 20.3848648Losses:  16.72722625732422 0.7723231315612793 13.712995529174805
MemoryTrain:  epoch  1, batch     8 | loss: 16.7272263Losses:  17.923728942871094 0.8734312057495117 13.716349601745605
MemoryTrain:  epoch  1, batch     9 | loss: 17.9237289Losses:  17.60909080505371 1.0603141784667969 13.732697486877441
MemoryTrain:  epoch  1, batch    10 | loss: 17.6090908Losses:  16.543983459472656 0.21849852800369263 13.660700798034668
MemoryTrain:  epoch  1, batch    11 | loss: 16.5439835Losses:  16.999191284179688 1.3210368156433105 13.67188549041748
MemoryTrain:  epoch  1, batch    12 | loss: 16.9991913Losses:  12.637406349182129 -0.0 10.77048110961914
MemoryTrain:  epoch  1, batch    13 | loss: 12.6374063Losses:  17.3221492767334 1.051501750946045 13.682366371154785
MemoryTrain:  epoch  2, batch     0 | loss: 17.3221493Losses:  23.226261138916016 0.7487010955810547 19.838315963745117
MemoryTrain:  epoch  2, batch     1 | loss: 23.2262611Losses:  25.453472137451172 0.3122403025627136 23.13495445251465
MemoryTrain:  epoch  2, batch     2 | loss: 25.4534721Losses:  23.152332305908203 0.9670989513397217 19.99178123474121
MemoryTrain:  epoch  2, batch     3 | loss: 23.1523323Losses:  22.704071044921875 0.5316592454910278 19.952880859375
MemoryTrain:  epoch  2, batch     4 | loss: 22.7040710Losses:  17.825315475463867 1.6196784973144531 13.735387802124023
MemoryTrain:  epoch  2, batch     5 | loss: 17.8253155Losses:  17.315784454345703 0.8631891012191772 13.753512382507324
MemoryTrain:  epoch  2, batch     6 | loss: 17.3157845Losses:  19.281648635864258 0.5156878232955933 16.74892807006836
MemoryTrain:  epoch  2, batch     7 | loss: 19.2816486Losses:  19.957998275756836 0.9416825771331787 16.6976261138916
MemoryTrain:  epoch  2, batch     8 | loss: 19.9579983Losses:  23.421218872070312 1.3056142330169678 19.873926162719727
MemoryTrain:  epoch  2, batch     9 | loss: 23.4212189Losses:  19.688220977783203 0.7567040920257568 16.707258224487305
MemoryTrain:  epoch  2, batch    10 | loss: 19.6882210Losses:  23.1368408203125 0.5390610694885254 19.940414428710938
MemoryTrain:  epoch  2, batch    11 | loss: 23.1368408Losses:  26.449893951416016 1.3264837265014648 23.13904571533203
MemoryTrain:  epoch  2, batch    12 | loss: 26.4498940Losses:  12.81586742401123 -0.0 10.79044246673584
MemoryTrain:  epoch  2, batch    13 | loss: 12.8158674Losses:  22.71946144104004 0.5860990881919861 19.869159698486328
MemoryTrain:  epoch  3, batch     0 | loss: 22.7194614Losses:  26.583417892456055 1.0614416599273682 23.111587524414062
MemoryTrain:  epoch  3, batch     1 | loss: 26.5834179Losses:  16.801132202148438 1.253843069076538 13.644376754760742
MemoryTrain:  epoch  3, batch     2 | loss: 16.8011322Losses:  22.52334213256836 0.27176764607429504 19.877286911010742
MemoryTrain:  epoch  3, batch     3 | loss: 22.5233421Losses:  22.684371948242188 0.5312626361846924 19.83133316040039
MemoryTrain:  epoch  3, batch     4 | loss: 22.6843719Losses:  19.93997573852539 0.6072666049003601 16.743940353393555
MemoryTrain:  epoch  3, batch     5 | loss: 19.9399757Losses:  22.73129653930664 0.9898645877838135 19.843652725219727
MemoryTrain:  epoch  3, batch     6 | loss: 22.7312965Losses:  19.871021270751953 0.9798016548156738 16.72340965270996
MemoryTrain:  epoch  3, batch     7 | loss: 19.8710213Losses:  22.527082443237305 0.4917009472846985 19.84636878967285
MemoryTrain:  epoch  3, batch     8 | loss: 22.5270824Losses:  25.781450271606445 0.5512024164199829 23.16473960876465
MemoryTrain:  epoch  3, batch     9 | loss: 25.7814503Losses:  19.435096740722656 0.7514058947563171 16.75174903869629
MemoryTrain:  epoch  3, batch    10 | loss: 19.4350967Losses:  32.260128021240234 -0.0 29.946815490722656
MemoryTrain:  epoch  3, batch    11 | loss: 32.2601280Losses:  19.729915618896484 0.7607297301292419 16.70034408569336
MemoryTrain:  epoch  3, batch    12 | loss: 19.7299156Losses:  8.085864067077637 0.26219964027404785 5.598215103149414
MemoryTrain:  epoch  3, batch    13 | loss: 8.0858641Losses:  32.53160858154297 0.47395747900009155 29.962018966674805
MemoryTrain:  epoch  4, batch     0 | loss: 32.5316086Losses:  19.99152374267578 1.1270923614501953 16.741374969482422
MemoryTrain:  epoch  4, batch     1 | loss: 19.9915237Losses:  22.414213180541992 0.5387322902679443 19.86728858947754
MemoryTrain:  epoch  4, batch     2 | loss: 22.4142132Losses:  22.73272705078125 0.9628119468688965 19.8256893157959
MemoryTrain:  epoch  4, batch     3 | loss: 22.7327271Losses:  19.58397674560547 0.7514183521270752 16.68522071838379
MemoryTrain:  epoch  4, batch     4 | loss: 19.5839767Losses:  19.268598556518555 0.4765319228172302 16.697025299072266
MemoryTrain:  epoch  4, batch     5 | loss: 19.2685986Losses:  20.50756072998047 1.4699009656906128 16.706510543823242
MemoryTrain:  epoch  4, batch     6 | loss: 20.5075607Losses:  22.30141830444336 0.522174060344696 19.842384338378906
MemoryTrain:  epoch  4, batch     7 | loss: 22.3014183Losses:  16.508800506591797 0.770209550857544 13.651033401489258
MemoryTrain:  epoch  4, batch     8 | loss: 16.5088005Losses:  22.305831909179688 0.47328412532806396 19.855274200439453
MemoryTrain:  epoch  4, batch     9 | loss: 22.3058319Losses:  25.840312957763672 0.5151443481445312 23.163463592529297
MemoryTrain:  epoch  4, batch    10 | loss: 25.8403130Losses:  22.284896850585938 0.49022459983825684 19.88435935974121
MemoryTrain:  epoch  4, batch    11 | loss: 22.2848969Losses:  22.560287475585938 0.7598551511764526 19.839635848999023
MemoryTrain:  epoch  4, batch    12 | loss: 22.5602875Losses:  10.49415397644043 0.2789385914802551 8.085678100585938
MemoryTrain:  epoch  4, batch    13 | loss: 10.4941540Losses:  23.02104949951172 1.2948641777038574 19.822128295898438
MemoryTrain:  epoch  5, batch     0 | loss: 23.0210495Losses:  13.642250061035156 0.751987099647522 10.787201881408691
MemoryTrain:  epoch  5, batch     1 | loss: 13.6422501Losses:  22.6483211517334 0.7093887329101562 19.816537857055664
MemoryTrain:  epoch  5, batch     2 | loss: 22.6483212Losses:  26.29484748840332 0.7561248540878296 23.138126373291016
MemoryTrain:  epoch  5, batch     3 | loss: 26.2948475Losses:  22.629384994506836 0.7221049070358276 19.86679458618164
MemoryTrain:  epoch  5, batch     4 | loss: 22.6293850Losses:  22.557218551635742 0.4908069670200348 19.87434959411621
MemoryTrain:  epoch  5, batch     5 | loss: 22.5572186Losses:  25.124608993530273 -0.0 23.123878479003906
MemoryTrain:  epoch  5, batch     6 | loss: 25.1246090Losses:  19.634050369262695 1.0060350894927979 16.7348690032959
MemoryTrain:  epoch  5, batch     7 | loss: 19.6340504Losses:  19.40867042541504 0.7319983243942261 16.721731185913086
MemoryTrain:  epoch  5, batch     8 | loss: 19.4086704Losses:  20.008075714111328 1.3373093605041504 16.723453521728516
MemoryTrain:  epoch  5, batch     9 | loss: 20.0080757Losses:  22.539901733398438 0.754381000995636 19.84242820739746
MemoryTrain:  epoch  5, batch    10 | loss: 22.5399017Losses:  25.081262588500977 -0.0 23.117095947265625
MemoryTrain:  epoch  5, batch    11 | loss: 25.0812626Losses:  22.4803524017334 0.5016641616821289 19.83124351501465
MemoryTrain:  epoch  5, batch    12 | loss: 22.4803524Losses:  12.924009323120117 -0.0 10.807811737060547
MemoryTrain:  epoch  5, batch    13 | loss: 12.9240093Losses:  19.588191986083984 0.9865962862968445 16.708045959472656
MemoryTrain:  epoch  6, batch     0 | loss: 19.5881920Losses:  21.92753791809082 -0.0 19.86475944519043
MemoryTrain:  epoch  6, batch     1 | loss: 21.9275379Losses:  18.879915237426758 0.23822572827339172 16.68871307373047
MemoryTrain:  epoch  6, batch     2 | loss: 18.8799152Losses:  25.264198303222656 0.2462175488471985 23.076383590698242
MemoryTrain:  epoch  6, batch     3 | loss: 25.2641983Losses:  26.849157333374023 1.8246768712997437 23.13652992248535
MemoryTrain:  epoch  6, batch     4 | loss: 26.8491573Losses:  16.928943634033203 1.3322978019714355 13.684794425964355
MemoryTrain:  epoch  6, batch     5 | loss: 16.9289436Losses:  22.528547286987305 0.7344586253166199 19.82506561279297
MemoryTrain:  epoch  6, batch     6 | loss: 22.5285473Losses:  28.828022003173828 0.4896107614040375 26.429393768310547
MemoryTrain:  epoch  6, batch     7 | loss: 28.8280220Losses:  19.341445922851562 0.5613442659378052 16.676321029663086
MemoryTrain:  epoch  6, batch     8 | loss: 19.3414459Losses:  26.049110412597656 0.9757834672927856 23.165088653564453
MemoryTrain:  epoch  6, batch     9 | loss: 26.0491104Losses:  19.099124908447266 0.481451153755188 16.706329345703125
MemoryTrain:  epoch  6, batch    10 | loss: 19.0991249Losses:  22.912824630737305 1.1239173412322998 19.85181999206543
MemoryTrain:  epoch  6, batch    11 | loss: 22.9128246Losses:  19.13962173461914 0.49345892667770386 16.730037689208984
MemoryTrain:  epoch  6, batch    12 | loss: 19.1396217Losses:  7.709482192993164 0.2985779941082001 5.5564093589782715
MemoryTrain:  epoch  6, batch    13 | loss: 7.7094822Losses:  28.95499038696289 0.5218379497528076 26.47809410095215
MemoryTrain:  epoch  7, batch     0 | loss: 28.9549904Losses:  19.430835723876953 0.8178268074989319 16.71874237060547
MemoryTrain:  epoch  7, batch     1 | loss: 19.4308357Losses:  22.56446075439453 0.781951367855072 19.83230972290039
MemoryTrain:  epoch  7, batch     2 | loss: 22.5644608Losses:  25.75556182861328 0.7462005615234375 23.118968963623047
MemoryTrain:  epoch  7, batch     3 | loss: 25.7555618Losses:  25.722217559814453 0.726477861404419 23.11317253112793
MemoryTrain:  epoch  7, batch     4 | loss: 25.7222176Losses:  19.022981643676758 0.4851401746273041 16.656158447265625
MemoryTrain:  epoch  7, batch     5 | loss: 19.0229816Losses:  19.69609260559082 1.0655450820922852 16.685100555419922
MemoryTrain:  epoch  7, batch     6 | loss: 19.6960926Losses:  22.026546478271484 0.2615470588207245 19.816082000732422
MemoryTrain:  epoch  7, batch     7 | loss: 22.0265465Losses:  22.271636962890625 0.49438828229904175 19.875085830688477
MemoryTrain:  epoch  7, batch     8 | loss: 22.2716370Losses:  19.65496826171875 1.0553443431854248 16.714431762695312
MemoryTrain:  epoch  7, batch     9 | loss: 19.6549683Losses:  25.183551788330078 0.23322844505310059 23.06972885131836
MemoryTrain:  epoch  7, batch    10 | loss: 25.1835518Losses:  16.943771362304688 1.3670485019683838 13.66836929321289
MemoryTrain:  epoch  7, batch    11 | loss: 16.9437714Losses:  22.503875732421875 0.7354916334152222 19.855905532836914
MemoryTrain:  epoch  7, batch    12 | loss: 22.5038757Losses:  15.634286880493164 -0.0 13.697498321533203
MemoryTrain:  epoch  7, batch    13 | loss: 15.6342869Losses:  13.252251625061035 0.5268296003341675 10.768942832946777
MemoryTrain:  epoch  8, batch     0 | loss: 13.2522516Losses:  19.290515899658203 0.726014256477356 16.677961349487305
MemoryTrain:  epoch  8, batch     1 | loss: 19.2905159Losses:  22.90685272216797 1.1006731986999512 19.90043067932129
MemoryTrain:  epoch  8, batch     2 | loss: 22.9068527Losses:  25.268728256225586 0.2664877474308014 23.11638069152832
MemoryTrain:  epoch  8, batch     3 | loss: 25.2687283Losses:  22.721466064453125 0.9997046589851379 19.816347122192383
MemoryTrain:  epoch  8, batch     4 | loss: 22.7214661Losses:  19.833791732788086 1.2716314792633057 16.679426193237305
MemoryTrain:  epoch  8, batch     5 | loss: 19.8337917Losses:  22.810632705688477 1.11891508102417 19.821035385131836
MemoryTrain:  epoch  8, batch     6 | loss: 22.8106327Losses:  25.260391235351562 0.24677088856697083 23.0999698638916
MemoryTrain:  epoch  8, batch     7 | loss: 25.2603912Losses:  22.44200325012207 0.7313106656074524 19.828947067260742
MemoryTrain:  epoch  8, batch     8 | loss: 22.4420033Losses:  19.80511474609375 1.2150355577468872 16.676477432250977
MemoryTrain:  epoch  8, batch     9 | loss: 19.8051147Losses:  19.080228805541992 0.49087586998939514 16.684093475341797
MemoryTrain:  epoch  8, batch    10 | loss: 19.0802288Losses:  28.91288948059082 0.5250266790390015 26.47234344482422
MemoryTrain:  epoch  8, batch    11 | loss: 28.9128895Losses:  28.346813201904297 -0.0 26.451866149902344
MemoryTrain:  epoch  8, batch    12 | loss: 28.3468132Losses:  15.627657890319824 -0.0 13.735843658447266
MemoryTrain:  epoch  8, batch    13 | loss: 15.6276579Losses:  25.929611206054688 0.9680332541465759 23.08489227294922
MemoryTrain:  epoch  9, batch     0 | loss: 25.9296112Losses:  22.71604347229004 0.9721716046333313 19.8436279296875
MemoryTrain:  epoch  9, batch     1 | loss: 22.7160435Losses:  19.494155883789062 0.9406089782714844 16.684810638427734
MemoryTrain:  epoch  9, batch     2 | loss: 19.4941559Losses:  22.030376434326172 0.2826671302318573 19.833805084228516
MemoryTrain:  epoch  9, batch     3 | loss: 22.0303764Losses:  26.3365478515625 1.3323554992675781 23.13154411315918
MemoryTrain:  epoch  9, batch     4 | loss: 26.3365479Losses:  19.43265151977539 0.8000929355621338 16.702482223510742
MemoryTrain:  epoch  9, batch     5 | loss: 19.4326515Losses:  22.200435638427734 0.48122158646583557 19.836637496948242
MemoryTrain:  epoch  9, batch     6 | loss: 22.2004356Losses:  25.94742774963379 0.9129288196563721 23.088098526000977
MemoryTrain:  epoch  9, batch     7 | loss: 25.9474277Losses:  19.721202850341797 1.1100664138793945 16.7028751373291
MemoryTrain:  epoch  9, batch     8 | loss: 19.7212029Losses:  16.315052032470703 0.7623112201690674 13.674995422363281
MemoryTrain:  epoch  9, batch     9 | loss: 16.3150520Losses:  19.080062866210938 0.5010271072387695 16.69995880126953
MemoryTrain:  epoch  9, batch    10 | loss: 19.0800629Losses:  22.256973266601562 0.49403029680252075 19.850645065307617
MemoryTrain:  epoch  9, batch    11 | loss: 22.2569733Losses:  25.70931625366211 0.7248985767364502 23.107397079467773
MemoryTrain:  epoch  9, batch    12 | loss: 25.7093163Losses:  12.69236946105957 -0.0 10.792171478271484
MemoryTrain:  epoch  9, batch    13 | loss: 12.6923695
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 65.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 74.11%   
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 84.82%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 79.55%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 82.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 82.87%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 84.05%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.96%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 83.90%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 81.96%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 81.25%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 80.24%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 79.61%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 79.65%   [EVAL] batch:   39 | acc: 37.50%,  total acc: 78.59%   [EVAL] batch:   40 | acc: 0.00%,  total acc: 76.68%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 74.85%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 72.44%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 72.08%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 71.94%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 71.48%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 71.81%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 71.75%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 72.72%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 73.11%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 73.50%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 72.54%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 72.37%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 71.98%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 71.29%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 70.94%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 70.29%   [EVAL] batch:   61 | acc: 25.00%,  total acc: 69.56%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 68.65%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 67.77%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 66.73%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 65.72%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 65.49%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 65.07%   [EVAL] batch:   68 | acc: 56.25%,  total acc: 64.95%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 64.91%   [EVAL] batch:   70 | acc: 43.75%,  total acc: 64.61%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:   72 | acc: 25.00%,  total acc: 64.04%   [EVAL] batch:   73 | acc: 0.00%,  total acc: 63.18%   [EVAL] batch:   74 | acc: 0.00%,  total acc: 62.33%   [EVAL] batch:   75 | acc: 6.25%,  total acc: 61.60%   [EVAL] batch:   76 | acc: 0.00%,  total acc: 60.80%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 60.34%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 60.28%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 60.23%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 60.19%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 59.98%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 59.41%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 59.15%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 58.60%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 58.14%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 57.83%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 58.17%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 58.50%   [EVAL] batch:   89 | acc: 56.25%,  total acc: 58.47%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 58.24%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 58.15%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 58.60%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 59.04%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 59.47%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 59.90%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 60.31%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 60.65%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 61.05%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 61.44%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 61.82%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 62.19%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 62.56%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 62.92%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 63.27%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 63.73%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 63.37%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 63.19%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 63.41%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 63.40%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 63.50%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 63.44%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 63.49%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 63.53%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 63.52%   [EVAL] batch:  116 | acc: 75.00%,  total acc: 63.62%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 63.61%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 63.71%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 63.96%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 64.40%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 64.53%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 64.72%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 64.90%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 64.58%   
cur_acc:  ['0.8598', '0.7548', '0.7875', '0.3778', '0.7743', '0.8304', '0.7411']
his_acc:  ['0.8598', '0.8500', '0.8219', '0.6474', '0.6402', '0.6445', '0.6458']
Clustering into  19  clusters
Clusters:  [ 4  1  0 11 17 10  3  6 10 14 18  4  3  9  8 16  0  0  8 10 16  4 15 18
 16 12  2  7  4  0  6  7  4 16  1 18  4 13  5  2  1]
Losses:  18.095182418823242 8.019285202026367 5.758533477783203
CurrentTrain: epoch  0, batch     0 | loss: 18.0951824Losses:  12.455536842346191 3.542789936065674 3.492718458175659
CurrentTrain: epoch  0, batch     1 | loss: 12.4555368Losses:  16.850093841552734 7.002687931060791 5.722077369689941
CurrentTrain: epoch  1, batch     0 | loss: 16.8500938Losses:  12.216450691223145 2.488910675048828 5.587538242340088
CurrentTrain: epoch  1, batch     1 | loss: 12.2164507Losses:  16.54759407043457 7.0133442878723145 5.722876071929932
CurrentTrain: epoch  2, batch     0 | loss: 16.5475941Losses:  11.13580322265625 1.9806488752365112 5.586548805236816
CurrentTrain: epoch  2, batch     1 | loss: 11.1358032Losses:  17.478113174438477 8.045258522033691 5.628493785858154
CurrentTrain: epoch  3, batch     0 | loss: 17.4781132Losses:  11.119199752807617 2.408456563949585 5.613553524017334
CurrentTrain: epoch  3, batch     1 | loss: 11.1191998Losses:  15.333732604980469 6.7198357582092285 5.597690105438232
CurrentTrain: epoch  4, batch     0 | loss: 15.3337326Losses:  12.576763153076172 3.044679641723633 5.651699066162109
CurrentTrain: epoch  4, batch     1 | loss: 12.5767632Losses:  16.182106018066406 7.01209831237793 5.636455059051514
CurrentTrain: epoch  5, batch     0 | loss: 16.1821060Losses:  9.604421615600586 1.693222999572754 5.579653263092041
CurrentTrain: epoch  5, batch     1 | loss: 9.6044216Losses:  15.622831344604492 6.876275062561035 5.616175651550293
CurrentTrain: epoch  6, batch     0 | loss: 15.6228313Losses:  8.482619285583496 2.105579376220703 3.323786497116089
CurrentTrain: epoch  6, batch     1 | loss: 8.4826193Losses:  14.064642906188965 6.116631984710693 5.586178779602051
CurrentTrain: epoch  7, batch     0 | loss: 14.0646429Losses:  12.925657272338867 3.3000059127807617 5.62216329574585
CurrentTrain: epoch  7, batch     1 | loss: 12.9256573Losses:  14.537418365478516 5.8941450119018555 5.624903678894043
CurrentTrain: epoch  8, batch     0 | loss: 14.5374184Losses:  9.470797538757324 1.060719609260559 5.573643207550049
CurrentTrain: epoch  8, batch     1 | loss: 9.4707975Losses:  15.508880615234375 6.890467166900635 5.611264705657959
CurrentTrain: epoch  9, batch     0 | loss: 15.5088806Losses:  11.185895919799805 3.018113851547241 5.593495845794678
CurrentTrain: epoch  9, batch     1 | loss: 11.1858959
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she mentioned that despite living in the united states for over a decade, her roots are firmly planted in the vibrant landscapes of brazil where she was born.  
Head Entity: she  
Tail Entity: brazil  

Relation: person country of birth  
Context: as a celebrated author, j.k. rowling often reflects on her upbringing in the united kingdom, where she was born and raised before achieving global fame.  
Head Entity: j.k. rowling  
Tail Entity: united kingdom  

Relation: person country of birth  
Context: the documentary highlighted the life of the famous musician who was born in nigeria and later moved to the united states to pursue his career.  
Head Entity: the famous musician  
Tail Entity: nigeria  

Relation: person country of birth  
Context: although he has lived in canada for many years, his heart remains in the land of his birth, which is the beautiful country of italy.  
Head Entity: he  
Tail Entity: italy  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: barnes foundation : http://www.barnesfoundation.org
Head Entity: barnes foundation
Tail Entity: http://www.barnesfoundation.org
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: national geographic society : https://www.nationalgeographic.com  
Head Entity: national geographic society  
Tail Entity: https://www.nationalgeographic.com  

Relation: organization website  
Context: world wildlife fund : https://www.worldwildlife.org  
Head Entity: world wildlife fund  
Tail Entity: https://www.worldwildlife.org  

Relation: organization website  
Context: american red cross : https://www.redcross.org  
Head Entity: american red cross  
Tail Entity: https://www.redcross.org  

Relation: organization website  
Context: united nations : https://www.un.org  
Head Entity: united nations  
Tail Entity: https://www.un.org  

Relation: organization website  
Context: google : https://www.google.com  
Head Entity: google  
Tail Entity: https://www.google.com  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: marsans already owns the madrid-based air comet and 95 percent of aerolineas argentinas .
Head Entity: aerolineas argentinas
Tail Entity: marsans
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: tech giant Apple has seen significant investments from Warren Buffett's Berkshire Hathaway.  
Head Entity: Apple  
Tail Entity: Berkshire Hathaway  

Relation: organization shareholders  
Context: The investment firm Vanguard Group has acquired a substantial stake in the renewable energy company NextEra Energy.  
Head Entity: NextEra Energy  
Tail Entity: Vanguard Group  

Relation: organization shareholders  
Context: Tesla's stock has attracted attention from various investors, including the well-known hedge fund manager Cathie Wood.  
Head Entity: Tesla  
Tail Entity: Cathie Wood  

Relation: organization shareholders  
Context: The pharmaceutical company Pfizer has received funding from the investment group BlackRock to support its research initiatives.  
Head Entity: Pfizer  
Tail Entity: BlackRock  

Relation: organization shareholders  
Context: The social media platform Facebook has been a target for investments from various venture capitalists, notably Sequoia Capital.  
Head Entity: Facebook  
Tail Entity: Sequoia Capital  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: After years of financial difficulties, the local arts council announced its dissolution in January 2019, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: January 2019  

Relation: organization dissolved  
Context: The historic publishing house, Classic Reads, was dissolved in July 2021, marking the end of an era in literary history.  
Head Entity: Classic Reads  
Tail Entity: July 2021  

Relation: organization dissolved  
Context: Following a series of scandals, the charity organization, Helping Hands, was officially dissolved in February 2022.  
Head Entity: Helping Hands  
Tail Entity: February 2022  

Relation: organization dissolved  
Context: The environmental group, Green Future, announced its dissolution in October 2018 due to a lack of funding and support.  
Head Entity: Green Future  
Tail Entity: October 2018  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. John Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. John Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the famous actress and philanthropist, Emily Johnson, to support underprivileged children.  
Head Entity: Hope for Tomorrow  
Tail Entity: Emily Johnson  

Relation: organization founded by  
Context: In the early 2000s, a group of environmental activists led by Sarah Thompson founded Green Earth Initiative to combat climate change and promote sustainability.  
Head Entity: Green Earth Initiative  
Tail Entity: Sarah Thompson  

Relation: organization founded by  
Context: The tech startup, Innovatech, was co-founded by Mark Lee and his partner, Lisa Chen, in a small garage in Silicon Valley, aiming to revolutionize mobile applications.  
Head Entity: Innovatech  
Tail Entity: Mark Lee  

Relation: organization founded by  
Context: The historical society, Heritage Keepers, was established in 1988 by local historian and author, Robert Williams, to preserve the town's rich cultural history.  
Head Entity: Heritage Keepers  
Tail Entity: Robert Williams  
Losses:  25.71393585205078 0.23618343472480774 23.099376678466797
MemoryTrain:  epoch  0, batch     0 | loss: 25.7139359Losses:  23.049989700317383 0.7411907315254211 19.93500328063965
MemoryTrain:  epoch  0, batch     1 | loss: 23.0499897Losses:  20.538101196289062 0.7962711453437805 16.781198501586914
MemoryTrain:  epoch  0, batch     2 | loss: 20.5381012Losses:  29.42782211303711 0.46391934156417847 26.55809783935547
MemoryTrain:  epoch  0, batch     3 | loss: 29.4278221Losses:  25.813846588134766 0.22993284463882446 23.09612464904785
MemoryTrain:  epoch  0, batch     4 | loss: 25.8138466Losses:  24.63577651977539 0.9980951547622681 19.815204620361328
MemoryTrain:  epoch  0, batch     5 | loss: 24.6357765Losses:  30.150779724121094 -0.0 26.484256744384766
MemoryTrain:  epoch  0, batch     6 | loss: 30.1507797Losses:  29.576438903808594 -0.0 26.48212432861328
MemoryTrain:  epoch  0, batch     7 | loss: 29.5764389Losses:  21.082435607910156 0.7452057003974915 16.93748664855957
MemoryTrain:  epoch  0, batch     8 | loss: 21.0824356Losses:  23.345117568969727 0.22877413034439087 19.814252853393555
MemoryTrain:  epoch  0, batch     9 | loss: 23.3451176Losses:  20.57761001586914 0.7550815343856812 16.7060489654541
MemoryTrain:  epoch  0, batch    10 | loss: 20.5776100Losses:  24.12289047241211 0.7411454916000366 19.86749267578125
MemoryTrain:  epoch  0, batch    11 | loss: 24.1228905Losses:  30.17550277709961 0.5024653673171997 26.50257682800293
MemoryTrain:  epoch  0, batch    12 | loss: 30.1755028Losses:  20.501909255981445 1.082479476928711 16.693227767944336
MemoryTrain:  epoch  0, batch    13 | loss: 20.5019093Losses:  24.72966766357422 0.5327857732772827 20.217817306518555
MemoryTrain:  epoch  0, batch    14 | loss: 24.7296677Losses:  7.864616394042969 -0.0 5.5852227210998535
MemoryTrain:  epoch  0, batch    15 | loss: 7.8646164Losses:  27.898311614990234 1.1860747337341309 23.13482666015625
MemoryTrain:  epoch  1, batch     0 | loss: 27.8983116Losses:  29.027204513549805 0.2703101634979248 26.48952293395996
MemoryTrain:  epoch  1, batch     1 | loss: 29.0272045Losses:  28.924217224121094 0.47139593958854675 26.43971061706543
MemoryTrain:  epoch  1, batch     2 | loss: 28.9242172Losses:  29.622751235961914 0.46780043840408325 26.43646240234375
MemoryTrain:  epoch  1, batch     3 | loss: 29.6227512Losses:  23.64164924621582 1.0157603025436401 19.846134185791016
MemoryTrain:  epoch  1, batch     4 | loss: 23.6416492Losses:  19.73682403564453 0.8180723786354065 16.672422409057617
MemoryTrain:  epoch  1, batch     5 | loss: 19.7368240Losses:  19.21335220336914 0.25226879119873047 16.670391082763672
MemoryTrain:  epoch  1, batch     6 | loss: 19.2133522Losses:  29.392112731933594 0.5209723711013794 26.500301361083984
MemoryTrain:  epoch  1, batch     7 | loss: 29.3921127Losses:  20.080703735351562 1.0678821802139282 16.694124221801758
MemoryTrain:  epoch  1, batch     8 | loss: 20.0807037Losses:  30.013078689575195 -0.0 26.50759506225586
MemoryTrain:  epoch  1, batch     9 | loss: 30.0130787Losses:  25.399765014648438 -0.0 23.08016014099121
MemoryTrain:  epoch  1, batch    10 | loss: 25.3997650Losses:  20.08654022216797 0.8632069826126099 16.699871063232422
MemoryTrain:  epoch  1, batch    11 | loss: 20.0865402Losses:  21.108556747436523 1.8219281435012817 14.030044555664062
MemoryTrain:  epoch  1, batch    12 | loss: 21.1085567Losses:  17.94070053100586 0.5470617413520813 13.671470642089844
MemoryTrain:  epoch  1, batch    13 | loss: 17.9407005Losses:  16.87803077697754 1.1181437969207764 13.652475357055664
MemoryTrain:  epoch  1, batch    14 | loss: 16.8780308Losses:  8.363470077514648 -0.0 5.567864418029785
MemoryTrain:  epoch  1, batch    15 | loss: 8.3634701Losses:  20.51964569091797 1.3454532623291016 16.69059944152832
MemoryTrain:  epoch  2, batch     0 | loss: 20.5196457Losses:  27.435644149780273 0.953083872795105 23.090166091918945
MemoryTrain:  epoch  2, batch     1 | loss: 27.4356441Losses:  25.555395126342773 0.23267507553100586 23.099918365478516
MemoryTrain:  epoch  2, batch     2 | loss: 25.5553951Losses:  24.380931854248047 1.128359079360962 19.849729537963867
MemoryTrain:  epoch  2, batch     3 | loss: 24.3809319Losses:  29.416038513183594 0.22729232907295227 26.49163818359375
MemoryTrain:  epoch  2, batch     4 | loss: 29.4160385Losses:  22.481189727783203 0.7485584616661072 19.813766479492188
MemoryTrain:  epoch  2, batch     5 | loss: 22.4811897Losses:  25.84824562072754 -0.0 23.124380111694336
MemoryTrain:  epoch  2, batch     6 | loss: 25.8482456Losses:  19.912458419799805 0.7787545323371887 16.663436889648438
MemoryTrain:  epoch  2, batch     7 | loss: 19.9124584Losses:  22.5679988861084 0.5428575277328491 19.8752498626709
MemoryTrain:  epoch  2, batch     8 | loss: 22.5679989Losses:  27.112119674682617 0.8496419191360474 23.111209869384766
MemoryTrain:  epoch  2, batch     9 | loss: 27.1121197Losses:  23.846309661865234 1.0864753723144531 19.81883430480957
MemoryTrain:  epoch  2, batch    10 | loss: 23.8463097Losses:  23.262126922607422 0.5348539352416992 19.983041763305664
MemoryTrain:  epoch  2, batch    11 | loss: 23.2621269Losses:  22.772598266601562 0.5013236999511719 19.81013298034668
MemoryTrain:  epoch  2, batch    12 | loss: 22.7725983Losses:  22.849348068237305 0.5192108154296875 19.824352264404297
MemoryTrain:  epoch  2, batch    13 | loss: 22.8493481Losses:  23.72098731994629 1.413841724395752 19.912519454956055
MemoryTrain:  epoch  2, batch    14 | loss: 23.7209873Losses:  5.206719398498535 -0.0 3.3320555686950684
MemoryTrain:  epoch  2, batch    15 | loss: 5.2067194Losses:  29.35504913330078 0.505236804485321 26.434982299804688
MemoryTrain:  epoch  3, batch     0 | loss: 29.3550491Losses:  25.82411766052246 0.49489593505859375 23.097522735595703
MemoryTrain:  epoch  3, batch     1 | loss: 25.8241177Losses:  25.33993148803711 0.23436269164085388 23.08319091796875
MemoryTrain:  epoch  3, batch     2 | loss: 25.3399315Losses:  26.457469940185547 0.5027385354042053 23.112661361694336
MemoryTrain:  epoch  3, batch     3 | loss: 26.4574699Losses:  26.378219604492188 1.1426929235458374 23.108346939086914
MemoryTrain:  epoch  3, batch     4 | loss: 26.3782196Losses:  17.270336151123047 1.3299009799957275 13.684484481811523
MemoryTrain:  epoch  3, batch     5 | loss: 17.2703362Losses:  21.779842376708984 -0.0 19.815692901611328
MemoryTrain:  epoch  3, batch     6 | loss: 21.7798424Losses:  22.81990623474121 2.7789909839630127 16.671232223510742
MemoryTrain:  epoch  3, batch     7 | loss: 22.8199062Losses:  28.584068298339844 0.2477525770664215 26.424663543701172
MemoryTrain:  epoch  3, batch     8 | loss: 28.5840683Losses:  19.18762969970703 0.490377813577652 16.732099533081055
MemoryTrain:  epoch  3, batch     9 | loss: 19.1876297Losses:  17.169897079467773 0.6219109296798706 13.701820373535156
MemoryTrain:  epoch  3, batch    10 | loss: 17.1698971Losses:  32.84156036376953 0.4863385260105133 29.979774475097656
MemoryTrain:  epoch  3, batch    11 | loss: 32.8415604Losses:  25.268482208251953 0.2470824420452118 23.085641860961914
MemoryTrain:  epoch  3, batch    12 | loss: 25.2684822Losses:  22.4027099609375 0.507475733757019 19.84274673461914
MemoryTrain:  epoch  3, batch    13 | loss: 22.4027100Losses:  28.407596588134766 -0.0 26.455739974975586
MemoryTrain:  epoch  3, batch    14 | loss: 28.4075966Losses:  7.8707594871521 -0.0 5.570934295654297
MemoryTrain:  epoch  3, batch    15 | loss: 7.8707595Losses:  20.134733200073242 1.3606855869293213 16.720048904418945
MemoryTrain:  epoch  4, batch     0 | loss: 20.1347332Losses:  16.945810317993164 1.3885644674301147 13.66813850402832
MemoryTrain:  epoch  4, batch     1 | loss: 16.9458103Losses:  29.168861389160156 0.4672510027885437 26.49038314819336
MemoryTrain:  epoch  4, batch     2 | loss: 29.1688614Losses:  22.658592224121094 0.7097387313842773 19.817914962768555
MemoryTrain:  epoch  4, batch     3 | loss: 22.6585922Losses:  20.500120162963867 1.0632479190826416 16.718225479125977
MemoryTrain:  epoch  4, batch     4 | loss: 20.5001202Losses:  20.186302185058594 0.7382696270942688 16.715538024902344
MemoryTrain:  epoch  4, batch     5 | loss: 20.1863022Losses:  25.54949951171875 0.5030779838562012 23.073223114013672
MemoryTrain:  epoch  4, batch     6 | loss: 25.5494995Losses:  17.473852157592773 0.8472522497177124 13.67601203918457
MemoryTrain:  epoch  4, batch     7 | loss: 17.4738522Losses:  20.152027130126953 0.7928670644760132 16.701730728149414
MemoryTrain:  epoch  4, batch     8 | loss: 20.1520271Losses:  19.733577728271484 0.4834775924682617 16.676177978515625
MemoryTrain:  epoch  4, batch     9 | loss: 19.7335777Losses:  22.443557739257812 0.755940854549408 19.812292098999023
MemoryTrain:  epoch  4, batch    10 | loss: 22.4435577Losses:  22.691364288330078 0.5180603265762329 19.870847702026367
MemoryTrain:  epoch  4, batch    11 | loss: 22.6913643Losses:  22.29842758178711 0.5539673566818237 19.833219528198242
MemoryTrain:  epoch  4, batch    12 | loss: 22.2984276Losses:  18.205657958984375 1.5742043256759644 13.711737632751465
MemoryTrain:  epoch  4, batch    13 | loss: 18.2056580Losses:  26.06219482421875 1.1029858589172363 23.074148178100586
MemoryTrain:  epoch  4, batch    14 | loss: 26.0621948Losses:  7.781307220458984 0.2638082802295685 5.61958646774292
MemoryTrain:  epoch  4, batch    15 | loss: 7.7813072Losses:  26.138290405273438 0.8641709685325623 23.102033615112305
MemoryTrain:  epoch  5, batch     0 | loss: 26.1382904Losses:  29.01132583618164 0.4913486838340759 26.486745834350586
MemoryTrain:  epoch  5, batch     1 | loss: 29.0113258Losses:  22.79703712463379 0.8451080322265625 19.858890533447266
MemoryTrain:  epoch  5, batch     2 | loss: 22.7970371Losses:  25.24420928955078 0.2522951662540436 23.098711013793945
MemoryTrain:  epoch  5, batch     3 | loss: 25.2442093Losses:  22.126352310180664 0.26459556818008423 19.82505226135254
MemoryTrain:  epoch  5, batch     4 | loss: 22.1263523Losses:  20.53807830810547 1.3696271181106567 16.694934844970703
MemoryTrain:  epoch  5, batch     5 | loss: 20.5380783Losses:  26.403362274169922 0.70377117395401 23.074003219604492
MemoryTrain:  epoch  5, batch     6 | loss: 26.4033623Losses:  22.55561065673828 0.743634045124054 19.812963485717773
MemoryTrain:  epoch  5, batch     7 | loss: 22.5556107Losses:  22.63391876220703 0.5198158621788025 19.807199478149414
MemoryTrain:  epoch  5, batch     8 | loss: 22.6339188Losses:  26.408733367919922 1.1322818994522095 23.074951171875
MemoryTrain:  epoch  5, batch     9 | loss: 26.4087334Losses:  19.644662857055664 0.7314029335975647 16.76212501525879
MemoryTrain:  epoch  5, batch    10 | loss: 19.6446629Losses:  26.018457412719727 0.768548846244812 23.075220108032227
MemoryTrain:  epoch  5, batch    11 | loss: 26.0184574Losses:  16.51238441467285 0.4773690104484558 13.654738426208496
MemoryTrain:  epoch  5, batch    12 | loss: 16.5123844Losses:  28.85030746459961 0.4893985986709595 26.470516204833984
MemoryTrain:  epoch  5, batch    13 | loss: 28.8503075Losses:  17.408424377441406 1.550976276397705 13.680364608764648
MemoryTrain:  epoch  5, batch    14 | loss: 17.4084244Losses:  5.474233627319336 -0.0 3.312610149383545
MemoryTrain:  epoch  5, batch    15 | loss: 5.4742336Losses:  25.460033416748047 0.235543891787529 23.09295082092285
MemoryTrain:  epoch  6, batch     0 | loss: 25.4600334Losses:  19.370403289794922 0.7408348321914673 16.699033737182617
MemoryTrain:  epoch  6, batch     1 | loss: 19.3704033Losses:  25.861671447753906 0.732474148273468 23.11896324157715
MemoryTrain:  epoch  6, batch     2 | loss: 25.8616714Losses:  25.24201011657715 0.26735174655914307 23.100364685058594
MemoryTrain:  epoch  6, batch     3 | loss: 25.2420101Losses:  21.794204711914062 -0.0 19.842573165893555
MemoryTrain:  epoch  6, batch     4 | loss: 21.7942047Losses:  19.06245994567871 0.263313353061676 16.69291114807129
MemoryTrain:  epoch  6, batch     5 | loss: 19.0624599Losses:  16.677013397216797 0.7510821223258972 13.715112686157227
MemoryTrain:  epoch  6, batch     6 | loss: 16.6770134Losses:  22.216442108154297 0.49595963954925537 19.82404899597168
MemoryTrain:  epoch  6, batch     7 | loss: 22.2164421Losses:  19.279455184936523 0.4712870121002197 16.689359664916992
MemoryTrain:  epoch  6, batch     8 | loss: 19.2794552Losses:  21.73772621154785 -0.0 19.814252853393555
MemoryTrain:  epoch  6, batch     9 | loss: 21.7377262Losses:  28.76002311706543 0.4502886235713959 26.432079315185547
MemoryTrain:  epoch  6, batch    10 | loss: 28.7600231Losses:  26.1018009185791 1.0868728160858154 23.06888198852539
MemoryTrain:  epoch  6, batch    11 | loss: 26.1018009Losses:  23.093353271484375 1.0721395015716553 19.871896743774414
MemoryTrain:  epoch  6, batch    12 | loss: 23.0933533Losses:  31.95833969116211 -0.0 29.900419235229492
MemoryTrain:  epoch  6, batch    13 | loss: 31.9583397Losses:  25.743427276611328 0.5230828523635864 23.148488998413086
MemoryTrain:  epoch  6, batch    14 | loss: 25.7434273Losses:  7.596954345703125 -0.0 5.553324222564697
MemoryTrain:  epoch  6, batch    15 | loss: 7.5969543Losses:  28.65049171447754 0.23505240678787231 26.453359603881836
MemoryTrain:  epoch  7, batch     0 | loss: 28.6504917Losses:  20.235082626342773 1.6468337774276733 16.67510986328125
MemoryTrain:  epoch  7, batch     1 | loss: 20.2350826Losses:  22.27614402770996 0.4939812123775482 19.82805824279785
MemoryTrain:  epoch  7, batch     2 | loss: 22.2761440Losses:  18.83856964111328 0.24303004145622253 16.670907974243164
MemoryTrain:  epoch  7, batch     3 | loss: 18.8385696Losses:  32.27634048461914 0.48674851655960083 29.899391174316406
MemoryTrain:  epoch  7, batch     4 | loss: 32.2763405Losses:  16.291166305541992 0.7274319529533386 13.650367736816406
MemoryTrain:  epoch  7, batch     5 | loss: 16.2911663Losses:  25.89824867248535 0.726233184337616 23.103063583374023
MemoryTrain:  epoch  7, batch     6 | loss: 25.8982487Losses:  25.6890869140625 0.5419290065765381 23.095947265625
MemoryTrain:  epoch  7, batch     7 | loss: 25.6890869Losses:  16.07028579711914 0.5088935494422913 13.645630836486816
MemoryTrain:  epoch  7, batch     8 | loss: 16.0702858Losses:  22.017913818359375 0.25965043902397156 19.850744247436523
MemoryTrain:  epoch  7, batch     9 | loss: 22.0179138Losses:  22.432666778564453 0.7518905997276306 19.816259384155273
MemoryTrain:  epoch  7, batch    10 | loss: 22.4326668Losses:  25.746734619140625 0.7162463665008545 23.100433349609375
MemoryTrain:  epoch  7, batch    11 | loss: 25.7467346Losses:  22.004322052001953 0.2721185088157654 19.82419776916504
MemoryTrain:  epoch  7, batch    12 | loss: 22.0043221Losses:  35.65666580200195 0.24709667265415192 33.42203903198242
MemoryTrain:  epoch  7, batch    13 | loss: 35.6566658Losses:  25.42694091796875 0.48070481419563293 23.07468605041504
MemoryTrain:  epoch  7, batch    14 | loss: 25.4269409Losses:  9.964734077453613 -0.0 8.075662612915039
MemoryTrain:  epoch  7, batch    15 | loss: 9.9647341Losses:  25.655902862548828 0.544457733631134 23.13080406188965
MemoryTrain:  epoch  8, batch     0 | loss: 25.6559029Losses:  28.562116622924805 0.2441176474094391 26.43923568725586
MemoryTrain:  epoch  8, batch     1 | loss: 28.5621166Losses:  22.30093002319336 0.4863075017929077 19.83969497680664
MemoryTrain:  epoch  8, batch     2 | loss: 22.3009300Losses:  22.831857681274414 1.0905933380126953 19.8525447845459
MemoryTrain:  epoch  8, batch     3 | loss: 22.8318577Losses:  20.25214958190918 1.6854078769683838 16.679264068603516
MemoryTrain:  epoch  8, batch     4 | loss: 20.2521496Losses:  28.581466674804688 0.23646892607212067 26.427898406982422
MemoryTrain:  epoch  8, batch     5 | loss: 28.5814667Losses:  32.02676773071289 0.2537597417831421 29.904645919799805
MemoryTrain:  epoch  8, batch     6 | loss: 32.0267677Losses:  19.330406188964844 0.7472131848335266 16.661333084106445
MemoryTrain:  epoch  8, batch     7 | loss: 19.3304062Losses:  28.52260398864746 0.22769172489643097 26.421117782592773
MemoryTrain:  epoch  8, batch     8 | loss: 28.5226040Losses:  19.279869079589844 0.712286114692688 16.70021629333496
MemoryTrain:  epoch  8, batch     9 | loss: 19.2798691Losses:  29.191905975341797 0.855756402015686 26.412015914916992
MemoryTrain:  epoch  8, batch    10 | loss: 29.1919060Losses:  22.153087615966797 0.4902111291885376 19.813291549682617
MemoryTrain:  epoch  8, batch    11 | loss: 22.1530876Losses:  19.542001724243164 0.9809774160385132 16.669109344482422
MemoryTrain:  epoch  8, batch    12 | loss: 19.5420017Losses:  22.464866638183594 0.7465615272521973 19.835195541381836
MemoryTrain:  epoch  8, batch    13 | loss: 22.4648666Losses:  14.618444442749023 1.8552156686782837 10.874239921569824
MemoryTrain:  epoch  8, batch    14 | loss: 14.6184444Losses:  4.0344061851501465 0.7693299055099487 1.3969444036483765
MemoryTrain:  epoch  8, batch    15 | loss: 4.0344062Losses:  31.7293701171875 -0.0 29.883373260498047
MemoryTrain:  epoch  9, batch     0 | loss: 31.7293701Losses:  25.531375885009766 0.5013368129730225 23.09221839904785
MemoryTrain:  epoch  9, batch     1 | loss: 25.5313759Losses:  22.02924346923828 0.25072282552719116 19.84965705871582
MemoryTrain:  epoch  9, batch     2 | loss: 22.0292435Losses:  16.77100944519043 1.2242412567138672 13.67990493774414
MemoryTrain:  epoch  9, batch     3 | loss: 16.7710094Losses:  22.7678279876709 1.0010939836502075 19.831985473632812
MemoryTrain:  epoch  9, batch     4 | loss: 22.7678280Losses:  22.481483459472656 0.7387086153030396 19.83261489868164
MemoryTrain:  epoch  9, batch     5 | loss: 22.4814835Losses:  25.447059631347656 0.5098368525505066 23.06339454650879
MemoryTrain:  epoch  9, batch     6 | loss: 25.4470596Losses:  25.1756591796875 0.22707608342170715 23.086013793945312
MemoryTrain:  epoch  9, batch     7 | loss: 25.1756592Losses:  25.48453140258789 0.49081212282180786 23.078750610351562
MemoryTrain:  epoch  9, batch     8 | loss: 25.4845314Losses:  18.831817626953125 0.25586578249931335 16.675195693969727
MemoryTrain:  epoch  9, batch     9 | loss: 18.8318176Losses:  29.014530181884766 0.7147820591926575 26.422439575195312
MemoryTrain:  epoch  9, batch    10 | loss: 29.0145302Losses:  16.737804412841797 1.0990469455718994 13.702703475952148
MemoryTrain:  epoch  9, batch    11 | loss: 16.7378044Losses:  21.958343505859375 0.25618940591812134 19.803512573242188
MemoryTrain:  epoch  9, batch    12 | loss: 21.9583435Losses:  22.924480438232422 1.2733701467514038 19.795732498168945
MemoryTrain:  epoch  9, batch    13 | loss: 22.9244804Losses:  21.92901039123535 0.23563317954540253 19.810304641723633
MemoryTrain:  epoch  9, batch    14 | loss: 21.9290104Losses:  7.459846496582031 -0.0 5.57247257232666
MemoryTrain:  epoch  9, batch    15 | loss: 7.4598465
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 66.41%   
[EVAL] batch:    0 | acc: 50.00%,  total acc: 50.00%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 35.94%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 35.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 35.71%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 36.72%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 40.91%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 42.19%   [EVAL] batch:   12 | acc: 18.75%,  total acc: 40.38%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 39.73%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 40.42%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 41.41%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 43.38%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 44.44%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 45.07%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 46.25%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 48.51%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 50.57%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 52.72%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 54.69%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 56.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 57.93%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 59.03%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 60.27%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 61.64%   [EVAL] batch:   29 | acc: 75.00%,  total acc: 62.08%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 62.70%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 63.87%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 63.64%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 63.42%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 63.21%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 62.85%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 63.18%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 64.42%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 63.91%   [EVAL] batch:   40 | acc: 6.25%,  total acc: 62.50%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 61.01%   [EVAL] batch:   42 | acc: 6.25%,  total acc: 59.74%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 59.52%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 59.58%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 59.65%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 59.97%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 59.64%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 59.95%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 59.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 60.54%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 61.06%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 61.67%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 62.27%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 62.16%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 61.61%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 61.73%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 61.53%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 61.44%   [EVAL] batch:   59 | acc: 31.25%,  total acc: 60.94%   [EVAL] batch:   60 | acc: 12.50%,  total acc: 60.14%   [EVAL] batch:   61 | acc: 6.25%,  total acc: 59.27%   [EVAL] batch:   62 | acc: 6.25%,  total acc: 58.43%   [EVAL] batch:   63 | acc: 18.75%,  total acc: 57.81%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 57.02%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 56.16%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 55.78%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 55.51%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 55.25%   [EVAL] batch:   69 | acc: 62.50%,  total acc: 55.36%   [EVAL] batch:   70 | acc: 31.25%,  total acc: 55.02%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 54.86%   [EVAL] batch:   72 | acc: 6.25%,  total acc: 54.20%   [EVAL] batch:   73 | acc: 0.00%,  total acc: 53.46%   [EVAL] batch:   74 | acc: 0.00%,  total acc: 52.75%   [EVAL] batch:   75 | acc: 6.25%,  total acc: 52.14%   [EVAL] batch:   76 | acc: 0.00%,  total acc: 51.46%   [EVAL] batch:   77 | acc: 12.50%,  total acc: 50.96%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 51.03%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 51.09%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 51.23%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 51.14%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 50.68%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 50.45%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 49.93%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 49.56%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 49.35%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 49.79%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 50.28%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 50.42%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 50.48%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 50.54%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 51.08%   [EVAL] batch:   93 | acc: 100.00%,  total acc: 51.60%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 52.11%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 52.60%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 53.09%   [EVAL] batch:   97 | acc: 93.75%,  total acc: 53.51%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 53.98%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 54.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 54.76%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 55.21%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 55.64%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 56.07%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 56.49%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 56.90%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 57.07%   [EVAL] batch:  107 | acc: 6.25%,  total acc: 56.60%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 56.31%   [EVAL] batch:  109 | acc: 87.50%,  total acc: 56.59%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 56.64%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 56.70%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 56.64%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 56.74%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 56.79%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 56.90%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 56.94%   [EVAL] batch:  117 | acc: 56.25%,  total acc: 56.94%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 57.04%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 57.34%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 57.64%   [EVAL] batch:  121 | acc: 87.50%,  total acc: 57.89%   [EVAL] batch:  122 | acc: 81.25%,  total acc: 58.08%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 58.27%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 58.50%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 58.63%   [EVAL] batch:  126 | acc: 93.75%,  total acc: 58.91%   [EVAL] batch:  127 | acc: 87.50%,  total acc: 59.13%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 59.25%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 59.23%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 59.40%   [EVAL] batch:  131 | acc: 43.75%,  total acc: 59.28%   [EVAL] batch:  132 | acc: 50.00%,  total acc: 59.21%   
cur_acc:  ['0.8598', '0.7548', '0.7875', '0.3778', '0.7743', '0.8304', '0.7411', '0.6641']
his_acc:  ['0.8598', '0.8500', '0.8219', '0.6474', '0.6402', '0.6445', '0.6458', '0.5921']
--------Round  5
seed:  600
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLTacred/CFRLdata_6_100_5_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
Clustering into  2  clusters
Clusters:  [1 0 0 1 0 1]
Losses:  23.41692352294922 10.087031364440918 -0.0
CurrentTrain: epoch  0, batch     0 | loss: 23.4169235Losses:  26.78462028503418 13.651811599731445 -0.0
CurrentTrain: epoch  0, batch     1 | loss: 26.7846203Losses:  21.774940490722656 8.612234115600586 -0.0
CurrentTrain: epoch  0, batch     2 | loss: 21.7749405Losses:  25.14874839782715 12.117682456970215 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 25.1487484Losses:  24.447729110717773 11.515798568725586 -0.0
CurrentTrain: epoch  0, batch     4 | loss: 24.4477291Losses:  20.19947624206543 7.658660411834717 -0.0
CurrentTrain: epoch  0, batch     5 | loss: 20.1994762Losses:  23.314353942871094 10.873665809631348 -0.0
CurrentTrain: epoch  0, batch     6 | loss: 23.3143539Losses:  22.662010192871094 9.995401382446289 -0.0
CurrentTrain: epoch  0, batch     7 | loss: 22.6620102Losses:  28.57248878479004 16.55156898498535 -0.0
CurrentTrain: epoch  0, batch     8 | loss: 28.5724888Losses:  20.106725692749023 7.790808200836182 -0.0
CurrentTrain: epoch  0, batch     9 | loss: 20.1067257Losses:  20.596065521240234 8.310562133789062 -0.0
CurrentTrain: epoch  0, batch    10 | loss: 20.5960655Losses:  24.611400604248047 12.052056312561035 -0.0
CurrentTrain: epoch  0, batch    11 | loss: 24.6114006Losses:  21.389192581176758 9.275882720947266 -0.0
CurrentTrain: epoch  0, batch    12 | loss: 21.3891926Losses:  19.810283660888672 7.756986618041992 -0.0
CurrentTrain: epoch  0, batch    13 | loss: 19.8102837Losses:  26.168560028076172 14.714345932006836 -0.0
CurrentTrain: epoch  0, batch    14 | loss: 26.1685600Losses:  20.531055450439453 8.846151351928711 -0.0
CurrentTrain: epoch  0, batch    15 | loss: 20.5310555Losses:  19.909481048583984 8.079788208007812 -0.0
CurrentTrain: epoch  0, batch    16 | loss: 19.9094810Losses:  21.587379455566406 9.986481666564941 -0.0
CurrentTrain: epoch  0, batch    17 | loss: 21.5873795Losses:  21.37991714477539 9.952503204345703 -0.0
CurrentTrain: epoch  0, batch    18 | loss: 21.3799171Losses:  19.9822998046875 8.39290714263916 -0.0
CurrentTrain: epoch  0, batch    19 | loss: 19.9822998Losses:  21.473970413208008 9.892854690551758 -0.0
CurrentTrain: epoch  0, batch    20 | loss: 21.4739704Losses:  20.105010986328125 8.487062454223633 -0.0
CurrentTrain: epoch  0, batch    21 | loss: 20.1050110Losses:  18.183408737182617 6.757063865661621 -0.0
CurrentTrain: epoch  0, batch    22 | loss: 18.1834087Losses:  18.204029083251953 6.895638465881348 -0.0
CurrentTrain: epoch  0, batch    23 | loss: 18.2040291Losses:  18.089609146118164 6.641342639923096 -0.0
CurrentTrain: epoch  0, batch    24 | loss: 18.0896091Losses:  18.820232391357422 7.699095726013184 -0.0
CurrentTrain: epoch  0, batch    25 | loss: 18.8202324Losses:  22.240259170532227 10.736043930053711 -0.0
CurrentTrain: epoch  0, batch    26 | loss: 22.2402592Losses:  20.67557144165039 9.939746856689453 -0.0
CurrentTrain: epoch  0, batch    27 | loss: 20.6755714Losses:  18.758630752563477 7.926337718963623 -0.0
CurrentTrain: epoch  0, batch    28 | loss: 18.7586308Losses:  18.4819393157959 7.599531173706055 -0.0
CurrentTrain: epoch  0, batch    29 | loss: 18.4819393Losses:  21.004207611083984 9.858696937561035 -0.0
CurrentTrain: epoch  0, batch    30 | loss: 21.0042076Losses:  19.069011688232422 8.424633979797363 -0.0
CurrentTrain: epoch  0, batch    31 | loss: 19.0690117Losses:  18.73232650756836 7.449463844299316 -0.0
CurrentTrain: epoch  0, batch    32 | loss: 18.7323265Losses:  18.803192138671875 9.105026245117188 -0.0
CurrentTrain: epoch  0, batch    33 | loss: 18.8031921Losses:  18.251869201660156 8.028669357299805 -0.0
CurrentTrain: epoch  0, batch    34 | loss: 18.2518692Losses:  17.938915252685547 7.834312438964844 -0.0
CurrentTrain: epoch  0, batch    35 | loss: 17.9389153Losses:  17.773181915283203 8.263818740844727 -0.0
CurrentTrain: epoch  0, batch    36 | loss: 17.7731819Losses:  12.833364486694336 2.2595040798187256 -0.0
CurrentTrain: epoch  0, batch    37 | loss: 12.8333645Losses:  17.80887222290039 8.064292907714844 -0.0
CurrentTrain: epoch  1, batch     0 | loss: 17.8088722Losses:  23.455013275146484 13.520818710327148 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 23.4550133Losses:  19.560646057128906 9.280725479125977 -0.0
CurrentTrain: epoch  1, batch     2 | loss: 19.5606461Losses:  16.609106063842773 6.604619026184082 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 16.6091061Losses:  17.805688858032227 8.058990478515625 -0.0
CurrentTrain: epoch  1, batch     4 | loss: 17.8056889Losses:  17.74646759033203 7.720853805541992 -0.0
CurrentTrain: epoch  1, batch     5 | loss: 17.7464676Losses:  19.467805862426758 9.15256404876709 -0.0
CurrentTrain: epoch  1, batch     6 | loss: 19.4678059Losses:  18.60017967224121 8.331703186035156 -0.0
CurrentTrain: epoch  1, batch     7 | loss: 18.6001797Losses:  17.237035751342773 7.948781490325928 -0.0
CurrentTrain: epoch  1, batch     8 | loss: 17.2370358Losses:  19.68288230895996 10.998085021972656 -0.0
CurrentTrain: epoch  1, batch     9 | loss: 19.6828823Losses:  18.719528198242188 7.9735822677612305 -0.0
CurrentTrain: epoch  1, batch    10 | loss: 18.7195282Losses:  18.260202407836914 8.428044319152832 -0.0
CurrentTrain: epoch  1, batch    11 | loss: 18.2602024Losses:  18.164794921875 8.407866477966309 -0.0
CurrentTrain: epoch  1, batch    12 | loss: 18.1647949Losses:  17.56583023071289 8.040504455566406 -0.0
CurrentTrain: epoch  1, batch    13 | loss: 17.5658302Losses:  15.423149108886719 6.374461650848389 -0.0
CurrentTrain: epoch  1, batch    14 | loss: 15.4231491Losses:  16.076885223388672 6.66227912902832 -0.0
CurrentTrain: epoch  1, batch    15 | loss: 16.0768852Losses:  16.004009246826172 7.1645097732543945 -0.0
CurrentTrain: epoch  1, batch    16 | loss: 16.0040092Losses:  28.442455291748047 17.329730987548828 -0.0
CurrentTrain: epoch  1, batch    17 | loss: 28.4424553Losses:  16.154071807861328 6.808008670806885 -0.0
CurrentTrain: epoch  1, batch    18 | loss: 16.1540718Losses:  20.459829330444336 11.229816436767578 -0.0
CurrentTrain: epoch  1, batch    19 | loss: 20.4598293Losses:  16.86713981628418 6.774080276489258 -0.0
CurrentTrain: epoch  1, batch    20 | loss: 16.8671398Losses:  19.24388885498047 10.196441650390625 -0.0
CurrentTrain: epoch  1, batch    21 | loss: 19.2438889Losses:  18.81473159790039 8.485806465148926 -0.0
CurrentTrain: epoch  1, batch    22 | loss: 18.8147316Losses:  15.081064224243164 5.856593132019043 -0.0
CurrentTrain: epoch  1, batch    23 | loss: 15.0810642Losses:  18.28464126586914 9.117939949035645 -0.0
CurrentTrain: epoch  1, batch    24 | loss: 18.2846413Losses:  16.414051055908203 7.749829292297363 -0.0
CurrentTrain: epoch  1, batch    25 | loss: 16.4140511Losses:  23.169391632080078 13.344634056091309 -0.0
CurrentTrain: epoch  1, batch    26 | loss: 23.1693916Losses:  18.221389770507812 10.35213851928711 -0.0
CurrentTrain: epoch  1, batch    27 | loss: 18.2213898Losses:  17.157472610473633 8.220842361450195 -0.0
CurrentTrain: epoch  1, batch    28 | loss: 17.1574726Losses:  18.908733367919922 9.351487159729004 -0.0
CurrentTrain: epoch  1, batch    29 | loss: 18.9087334Losses:  17.170455932617188 7.808842182159424 -0.0
CurrentTrain: epoch  1, batch    30 | loss: 17.1704559Losses:  14.41818904876709 5.900237083435059 -0.0
CurrentTrain: epoch  1, batch    31 | loss: 14.4181890Losses:  15.695392608642578 7.857883453369141 -0.0
CurrentTrain: epoch  1, batch    32 | loss: 15.6953926Losses:  19.061656951904297 9.357372283935547 -0.0
CurrentTrain: epoch  1, batch    33 | loss: 19.0616570Losses:  16.038848876953125 7.254456043243408 -0.0
CurrentTrain: epoch  1, batch    34 | loss: 16.0388489Losses:  15.689455032348633 8.128211975097656 -0.0
CurrentTrain: epoch  1, batch    35 | loss: 15.6894550Losses:  14.508235931396484 5.631992340087891 -0.0
CurrentTrain: epoch  1, batch    36 | loss: 14.5082359Losses:  11.248623847961426 3.6242449283599854 -0.0
CurrentTrain: epoch  1, batch    37 | loss: 11.2486238Losses:  16.88772964477539 8.026686668395996 -0.0
CurrentTrain: epoch  2, batch     0 | loss: 16.8877296Losses:  15.322953224182129 6.926299095153809 -0.0
CurrentTrain: epoch  2, batch     1 | loss: 15.3229532Losses:  16.176559448242188 7.260618209838867 -0.0
CurrentTrain: epoch  2, batch     2 | loss: 16.1765594Losses:  20.80248260498047 12.752099990844727 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 20.8024826Losses:  15.87050724029541 6.51602840423584 -0.0
CurrentTrain: epoch  2, batch     4 | loss: 15.8705072Losses:  16.721410751342773 8.708026885986328 -0.0
CurrentTrain: epoch  2, batch     5 | loss: 16.7214108Losses:  16.051494598388672 6.79455041885376 -0.0
CurrentTrain: epoch  2, batch     6 | loss: 16.0514946Losses:  14.633295059204102 5.89194917678833 -0.0
CurrentTrain: epoch  2, batch     7 | loss: 14.6332951Losses:  13.981544494628906 6.755895137786865 -0.0
CurrentTrain: epoch  2, batch     8 | loss: 13.9815445Losses:  13.618963241577148 5.336419105529785 -0.0
CurrentTrain: epoch  2, batch     9 | loss: 13.6189632Losses:  13.933440208435059 6.051183223724365 -0.0
CurrentTrain: epoch  2, batch    10 | loss: 13.9334402Losses:  16.502965927124023 8.315963745117188 -0.0
CurrentTrain: epoch  2, batch    11 | loss: 16.5029659Losses:  13.211782455444336 5.253657341003418 -0.0
CurrentTrain: epoch  2, batch    12 | loss: 13.2117825Losses:  13.778632164001465 5.634591102600098 -0.0
CurrentTrain: epoch  2, batch    13 | loss: 13.7786322Losses:  17.83413314819336 10.541600227355957 -0.0
CurrentTrain: epoch  2, batch    14 | loss: 17.8341331Losses:  16.330429077148438 7.680795669555664 -0.0
CurrentTrain: epoch  2, batch    15 | loss: 16.3304291Losses:  13.306949615478516 5.504141807556152 -0.0
CurrentTrain: epoch  2, batch    16 | loss: 13.3069496Losses:  14.645169258117676 7.222225189208984 -0.0
CurrentTrain: epoch  2, batch    17 | loss: 14.6451693Losses:  14.225261688232422 6.135441303253174 -0.0
CurrentTrain: epoch  2, batch    18 | loss: 14.2252617Losses:  20.46257972717285 12.228031158447266 -0.0
CurrentTrain: epoch  2, batch    19 | loss: 20.4625797Losses:  14.78510856628418 6.819708347320557 -0.0
CurrentTrain: epoch  2, batch    20 | loss: 14.7851086Losses:  16.989524841308594 8.868486404418945 -0.0
CurrentTrain: epoch  2, batch    21 | loss: 16.9895248Losses:  13.847637176513672 7.22707462310791 -0.0
CurrentTrain: epoch  2, batch    22 | loss: 13.8476372Losses:  12.434012413024902 5.172658920288086 -0.0
CurrentTrain: epoch  2, batch    23 | loss: 12.4340124Losses:  14.020809173583984 5.800760746002197 -0.0
CurrentTrain: epoch  2, batch    24 | loss: 14.0208092Losses:  18.079681396484375 7.652886390686035 -0.0
CurrentTrain: epoch  2, batch    25 | loss: 18.0796814Losses:  14.706319808959961 6.499778747558594 -0.0
CurrentTrain: epoch  2, batch    26 | loss: 14.7063198Losses:  16.897171020507812 8.589415550231934 -0.0
CurrentTrain: epoch  2, batch    27 | loss: 16.8971710Losses:  14.321526527404785 5.837002754211426 -0.0
CurrentTrain: epoch  2, batch    28 | loss: 14.3215265Losses:  15.53329849243164 7.155109882354736 -0.0
CurrentTrain: epoch  2, batch    29 | loss: 15.5332985Losses:  19.216753005981445 9.461945533752441 -0.0
CurrentTrain: epoch  2, batch    30 | loss: 19.2167530Losses:  14.948368072509766 6.146999835968018 -0.0
CurrentTrain: epoch  2, batch    31 | loss: 14.9483681Losses:  16.544645309448242 7.781053066253662 -0.0
CurrentTrain: epoch  2, batch    32 | loss: 16.5446453Losses:  15.867887496948242 7.391003131866455 -0.0
CurrentTrain: epoch  2, batch    33 | loss: 15.8678875Losses:  14.390052795410156 5.995509147644043 -0.0
CurrentTrain: epoch  2, batch    34 | loss: 14.3900528Losses:  14.024221420288086 7.261264324188232 -0.0
CurrentTrain: epoch  2, batch    35 | loss: 14.0242214Losses:  14.614410400390625 6.600454807281494 -0.0
CurrentTrain: epoch  2, batch    36 | loss: 14.6144104Losses:  9.080866813659668 1.479417085647583 -0.0
CurrentTrain: epoch  2, batch    37 | loss: 9.0808668Losses:  13.53384017944336 6.088053226470947 -0.0
CurrentTrain: epoch  3, batch     0 | loss: 13.5338402Losses:  14.228597640991211 7.435554504394531 -0.0
CurrentTrain: epoch  3, batch     1 | loss: 14.2285976Losses:  16.128807067871094 7.590850353240967 -0.0
CurrentTrain: epoch  3, batch     2 | loss: 16.1288071Losses:  14.515570640563965 7.960402488708496 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 14.5155706Losses:  13.405101776123047 6.290226459503174 -0.0
CurrentTrain: epoch  3, batch     4 | loss: 13.4051018Losses:  13.02811050415039 5.789319038391113 -0.0
CurrentTrain: epoch  3, batch     5 | loss: 13.0281105Losses:  14.255385398864746 6.262590408325195 -0.0
CurrentTrain: epoch  3, batch     6 | loss: 14.2553854Losses:  14.546350479125977 7.7214131355285645 -0.0
CurrentTrain: epoch  3, batch     7 | loss: 14.5463505Losses:  15.935243606567383 8.130599021911621 -0.0
CurrentTrain: epoch  3, batch     8 | loss: 15.9352436Losses:  12.488178253173828 5.557914733886719 -0.0
CurrentTrain: epoch  3, batch     9 | loss: 12.4881783Losses:  14.753782272338867 6.138667106628418 -0.0
CurrentTrain: epoch  3, batch    10 | loss: 14.7537823Losses:  15.920955657958984 7.476526737213135 -0.0
CurrentTrain: epoch  3, batch    11 | loss: 15.9209557Losses:  17.86117172241211 9.715133666992188 -0.0
CurrentTrain: epoch  3, batch    12 | loss: 17.8611717Losses:  17.832042694091797 9.271597862243652 -0.0
CurrentTrain: epoch  3, batch    13 | loss: 17.8320427Losses:  16.94478988647461 7.448427200317383 -0.0
CurrentTrain: epoch  3, batch    14 | loss: 16.9447899Losses:  14.219247817993164 5.822729587554932 -0.0
CurrentTrain: epoch  3, batch    15 | loss: 14.2192478Losses:  13.602998733520508 6.167777061462402 -0.0
CurrentTrain: epoch  3, batch    16 | loss: 13.6029987Losses:  16.398693084716797 8.677733421325684 -0.0
CurrentTrain: epoch  3, batch    17 | loss: 16.3986931Losses:  20.894550323486328 11.529882431030273 -0.0
CurrentTrain: epoch  3, batch    18 | loss: 20.8945503Losses:  15.20661735534668 7.926096439361572 -0.0
CurrentTrain: epoch  3, batch    19 | loss: 15.2066174Losses:  19.852510452270508 9.908964157104492 -0.0
CurrentTrain: epoch  3, batch    20 | loss: 19.8525105Losses:  11.736777305603027 4.5010576248168945 -0.0
CurrentTrain: epoch  3, batch    21 | loss: 11.7367773Losses:  14.299713134765625 6.548847198486328 -0.0
CurrentTrain: epoch  3, batch    22 | loss: 14.2997131Losses:  14.671014785766602 6.950688362121582 -0.0
CurrentTrain: epoch  3, batch    23 | loss: 14.6710148Losses:  14.554274559020996 6.718616485595703 -0.0
CurrentTrain: epoch  3, batch    24 | loss: 14.5542746Losses:  14.702052116394043 6.343674659729004 -0.0
CurrentTrain: epoch  3, batch    25 | loss: 14.7020521Losses:  14.458953857421875 6.256711959838867 -0.0
CurrentTrain: epoch  3, batch    26 | loss: 14.4589539Losses:  13.394776344299316 5.190506935119629 -0.0
CurrentTrain: epoch  3, batch    27 | loss: 13.3947763Losses:  13.853131294250488 5.965757369995117 -0.0
CurrentTrain: epoch  3, batch    28 | loss: 13.8531313Losses:  15.537144660949707 7.516648292541504 -0.0
CurrentTrain: epoch  3, batch    29 | loss: 15.5371447Losses:  12.410745620727539 5.726252555847168 -0.0
CurrentTrain: epoch  3, batch    30 | loss: 12.4107456Losses:  13.50714111328125 5.305416107177734 -0.0
CurrentTrain: epoch  3, batch    31 | loss: 13.5071411Losses:  13.115486145019531 5.290088653564453 -0.0
CurrentTrain: epoch  3, batch    32 | loss: 13.1154861Losses:  11.710226058959961 4.627598762512207 -0.0
CurrentTrain: epoch  3, batch    33 | loss: 11.7102261Losses:  14.583675384521484 7.085732936859131 -0.0
CurrentTrain: epoch  3, batch    34 | loss: 14.5836754Losses:  12.592218399047852 5.362524032592773 -0.0
CurrentTrain: epoch  3, batch    35 | loss: 12.5922184Losses:  13.903707504272461 7.450450897216797 -0.0
CurrentTrain: epoch  3, batch    36 | loss: 13.9037075Losses:  9.672971725463867 1.8360507488250732 -0.0
CurrentTrain: epoch  3, batch    37 | loss: 9.6729717Losses:  13.946260452270508 6.587986469268799 -0.0
CurrentTrain: epoch  4, batch     0 | loss: 13.9462605Losses:  12.460027694702148 5.53859806060791 -0.0
CurrentTrain: epoch  4, batch     1 | loss: 12.4600277Losses:  13.441780090332031 6.471839427947998 -0.0
CurrentTrain: epoch  4, batch     2 | loss: 13.4417801Losses:  17.615314483642578 9.66865348815918 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 17.6153145Losses:  13.052949905395508 6.439177513122559 -0.0
CurrentTrain: epoch  4, batch     4 | loss: 13.0529499Losses:  11.967267990112305 5.394135475158691 -0.0
CurrentTrain: epoch  4, batch     5 | loss: 11.9672680Losses:  12.929107666015625 6.127995491027832 -0.0
CurrentTrain: epoch  4, batch     6 | loss: 12.9291077Losses:  22.117202758789062 12.53627872467041 -0.0
CurrentTrain: epoch  4, batch     7 | loss: 22.1172028Losses:  14.392313003540039 6.386807918548584 -0.0
CurrentTrain: epoch  4, batch     8 | loss: 14.3923130Losses:  13.987649917602539 6.090002059936523 -0.0
CurrentTrain: epoch  4, batch     9 | loss: 13.9876499Losses:  16.19376564025879 10.092883110046387 -0.0
CurrentTrain: epoch  4, batch    10 | loss: 16.1937656Losses:  14.311288833618164 7.693950176239014 -0.0
CurrentTrain: epoch  4, batch    11 | loss: 14.3112888Losses:  13.535318374633789 6.546782970428467 -0.0
CurrentTrain: epoch  4, batch    12 | loss: 13.5353184Losses:  16.05557632446289 8.321261405944824 -0.0
CurrentTrain: epoch  4, batch    13 | loss: 16.0555763Losses:  16.564449310302734 8.055435180664062 -0.0
CurrentTrain: epoch  4, batch    14 | loss: 16.5644493Losses:  13.502523422241211 6.025041103363037 -0.0
CurrentTrain: epoch  4, batch    15 | loss: 13.5025234Losses:  13.822966575622559 7.077939987182617 -0.0
CurrentTrain: epoch  4, batch    16 | loss: 13.8229666Losses:  16.24797821044922 8.122323989868164 -0.0
CurrentTrain: epoch  4, batch    17 | loss: 16.2479782Losses:  16.921138763427734 8.511140823364258 -0.0
CurrentTrain: epoch  4, batch    18 | loss: 16.9211388Losses:  13.284387588500977 6.276676654815674 -0.0
CurrentTrain: epoch  4, batch    19 | loss: 13.2843876Losses:  15.044509887695312 7.3421430587768555 -0.0
CurrentTrain: epoch  4, batch    20 | loss: 15.0445099Losses:  15.129796981811523 7.089319705963135 -0.0
CurrentTrain: epoch  4, batch    21 | loss: 15.1297970Losses:  17.96193504333496 9.181940078735352 -0.0
CurrentTrain: epoch  4, batch    22 | loss: 17.9619350Losses:  13.189732551574707 5.088749885559082 -0.0
CurrentTrain: epoch  4, batch    23 | loss: 13.1897326Losses:  11.226446151733398 4.065906047821045 -0.0
CurrentTrain: epoch  4, batch    24 | loss: 11.2264462Losses:  18.57819366455078 8.686400413513184 -0.0
CurrentTrain: epoch  4, batch    25 | loss: 18.5781937Losses:  13.710312843322754 6.28228759765625 -0.0
CurrentTrain: epoch  4, batch    26 | loss: 13.7103128Losses:  12.434537887573242 5.433180332183838 -0.0
CurrentTrain: epoch  4, batch    27 | loss: 12.4345379Losses:  12.47961711883545 5.494606971740723 -0.0
CurrentTrain: epoch  4, batch    28 | loss: 12.4796171Losses:  14.903836250305176 6.394107818603516 -0.0
CurrentTrain: epoch  4, batch    29 | loss: 14.9038363Losses:  13.608531951904297 5.168171405792236 -0.0
CurrentTrain: epoch  4, batch    30 | loss: 13.6085320Losses:  12.512847900390625 5.393268585205078 -0.0
CurrentTrain: epoch  4, batch    31 | loss: 12.5128479Losses:  14.203835487365723 5.867209434509277 -0.0
CurrentTrain: epoch  4, batch    32 | loss: 14.2038355Losses:  19.311492919921875 12.50592041015625 -0.0
CurrentTrain: epoch  4, batch    33 | loss: 19.3114929Losses:  16.018585205078125 8.285757064819336 -0.0
CurrentTrain: epoch  4, batch    34 | loss: 16.0185852Losses:  14.45552921295166 6.8911237716674805 -0.0
CurrentTrain: epoch  4, batch    35 | loss: 14.4555292Losses:  14.170524597167969 7.652226448059082 -0.0
CurrentTrain: epoch  4, batch    36 | loss: 14.1705246Losses:  10.09714126586914 3.220396041870117 -0.0
CurrentTrain: epoch  4, batch    37 | loss: 10.0971413Losses:  18.827320098876953 10.22055435180664 -0.0
CurrentTrain: epoch  5, batch     0 | loss: 18.8273201Losses:  14.412408828735352 6.646368503570557 -0.0
CurrentTrain: epoch  5, batch     1 | loss: 14.4124088Losses:  13.760038375854492 6.030099868774414 -0.0
CurrentTrain: epoch  5, batch     2 | loss: 13.7600384Losses:  13.348111152648926 6.4061784744262695 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 13.3481112Losses:  12.080223083496094 5.497694969177246 -0.0
CurrentTrain: epoch  5, batch     4 | loss: 12.0802231Losses:  11.333233833312988 4.999910354614258 -0.0
CurrentTrain: epoch  5, batch     5 | loss: 11.3332338Losses:  15.362861633300781 7.3660712242126465 -0.0
CurrentTrain: epoch  5, batch     6 | loss: 15.3628616Losses:  14.838361740112305 8.267444610595703 -0.0
CurrentTrain: epoch  5, batch     7 | loss: 14.8383617Losses:  11.44119644165039 4.598568916320801 -0.0
CurrentTrain: epoch  5, batch     8 | loss: 11.4411964Losses:  10.503074645996094 4.103199005126953 -0.0
CurrentTrain: epoch  5, batch     9 | loss: 10.5030746Losses:  12.433889389038086 6.305062294006348 -0.0
CurrentTrain: epoch  5, batch    10 | loss: 12.4338894Losses:  14.767196655273438 7.863353252410889 -0.0
CurrentTrain: epoch  5, batch    11 | loss: 14.7671967Losses:  12.87600326538086 5.9869256019592285 -0.0
CurrentTrain: epoch  5, batch    12 | loss: 12.8760033Losses:  16.982772827148438 10.0492582321167 -0.0
CurrentTrain: epoch  5, batch    13 | loss: 16.9827728Losses:  11.87924861907959 4.619203567504883 -0.0
CurrentTrain: epoch  5, batch    14 | loss: 11.8792486Losses:  13.434988021850586 6.511050701141357 -0.0
CurrentTrain: epoch  5, batch    15 | loss: 13.4349880Losses:  12.52515697479248 4.733050346374512 -0.0
CurrentTrain: epoch  5, batch    16 | loss: 12.5251570Losses:  10.827150344848633 4.152437210083008 -0.0
CurrentTrain: epoch  5, batch    17 | loss: 10.8271503Losses:  12.611631393432617 5.335048198699951 -0.0
CurrentTrain: epoch  5, batch    18 | loss: 12.6116314Losses:  11.069014549255371 4.490575790405273 -0.0
CurrentTrain: epoch  5, batch    19 | loss: 11.0690145Losses:  13.426691055297852 6.726097106933594 -0.0
CurrentTrain: epoch  5, batch    20 | loss: 13.4266911Losses:  15.107428550720215 6.233393669128418 -0.0
CurrentTrain: epoch  5, batch    21 | loss: 15.1074286Losses:  14.65680980682373 6.450577735900879 -0.0
CurrentTrain: epoch  5, batch    22 | loss: 14.6568098Losses:  11.26172161102295 4.225212097167969 -0.0
CurrentTrain: epoch  5, batch    23 | loss: 11.2617216Losses:  12.317038536071777 5.819655418395996 -0.0
CurrentTrain: epoch  5, batch    24 | loss: 12.3170385Losses:  13.49315071105957 6.830308437347412 -0.0
CurrentTrain: epoch  5, batch    25 | loss: 13.4931507Losses:  11.63258171081543 4.623382091522217 -0.0
CurrentTrain: epoch  5, batch    26 | loss: 11.6325817Losses:  11.420394897460938 4.492530345916748 -0.0
CurrentTrain: epoch  5, batch    27 | loss: 11.4203949Losses:  15.214357376098633 7.938178062438965 -0.0
CurrentTrain: epoch  5, batch    28 | loss: 15.2143574Losses:  17.723247528076172 9.631843566894531 -0.0
CurrentTrain: epoch  5, batch    29 | loss: 17.7232475Losses:  14.16299819946289 5.949056625366211 -0.0
CurrentTrain: epoch  5, batch    30 | loss: 14.1629982Losses:  11.524540901184082 4.241856575012207 -0.0
CurrentTrain: epoch  5, batch    31 | loss: 11.5245409Losses:  16.208858489990234 8.022791862487793 -0.0
CurrentTrain: epoch  5, batch    32 | loss: 16.2088585Losses:  12.05335807800293 5.03507661819458 -0.0
CurrentTrain: epoch  5, batch    33 | loss: 12.0533581Losses:  12.877693176269531 5.007721900939941 -0.0
CurrentTrain: epoch  5, batch    34 | loss: 12.8776932Losses:  14.298866271972656 7.800497055053711 -0.0
CurrentTrain: epoch  5, batch    35 | loss: 14.2988663Losses:  15.079110145568848 7.4815874099731445 -0.0
CurrentTrain: epoch  5, batch    36 | loss: 15.0791101Losses:  7.590404033660889 1.6473422050476074 -0.0
CurrentTrain: epoch  5, batch    37 | loss: 7.5904040Losses:  12.37256908416748 5.508905410766602 -0.0
CurrentTrain: epoch  6, batch     0 | loss: 12.3725691Losses:  12.2092924118042 5.6613264083862305 -0.0
CurrentTrain: epoch  6, batch     1 | loss: 12.2092924Losses:  16.640972137451172 9.752906799316406 -0.0
CurrentTrain: epoch  6, batch     2 | loss: 16.6409721Losses:  10.623514175415039 4.5185322761535645 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 10.6235142Losses:  11.677165031433105 5.067670822143555 -0.0
CurrentTrain: epoch  6, batch     4 | loss: 11.6771650Losses:  12.700342178344727 5.602793216705322 -0.0
CurrentTrain: epoch  6, batch     5 | loss: 12.7003422Losses:  14.147985458374023 7.716335296630859 -0.0
CurrentTrain: epoch  6, batch     6 | loss: 14.1479855Losses:  13.572331428527832 7.242190361022949 -0.0
CurrentTrain: epoch  6, batch     7 | loss: 13.5723314Losses:  12.786582946777344 6.75145149230957 -0.0
CurrentTrain: epoch  6, batch     8 | loss: 12.7865829Losses:  12.495611190795898 6.4172258377075195 -0.0
CurrentTrain: epoch  6, batch     9 | loss: 12.4956112Losses:  13.13515853881836 6.0235443115234375 -0.0
CurrentTrain: epoch  6, batch    10 | loss: 13.1351585Losses:  13.590513229370117 6.07878303527832 -0.0
CurrentTrain: epoch  6, batch    11 | loss: 13.5905132Losses:  11.27977466583252 4.861599922180176 -0.0
CurrentTrain: epoch  6, batch    12 | loss: 11.2797747Losses:  10.756444931030273 4.64487886428833 -0.0
CurrentTrain: epoch  6, batch    13 | loss: 10.7564449Losses:  14.206059455871582 7.566803932189941 -0.0
CurrentTrain: epoch  6, batch    14 | loss: 14.2060595Losses:  11.50688648223877 5.022862434387207 -0.0
CurrentTrain: epoch  6, batch    15 | loss: 11.5068865Losses:  9.937469482421875 3.6415352821350098 -0.0
CurrentTrain: epoch  6, batch    16 | loss: 9.9374695Losses:  14.540791511535645 6.966161727905273 -0.0
CurrentTrain: epoch  6, batch    17 | loss: 14.5407915Losses:  10.304553031921387 3.781221389770508 -0.0
CurrentTrain: epoch  6, batch    18 | loss: 10.3045530Losses:  11.122371673583984 4.760003089904785 -0.0
CurrentTrain: epoch  6, batch    19 | loss: 11.1223717Losses:  15.08973503112793 9.168892860412598 -0.0
CurrentTrain: epoch  6, batch    20 | loss: 15.0897350Losses:  11.777613639831543 5.0260009765625 -0.0
CurrentTrain: epoch  6, batch    21 | loss: 11.7776136Losses:  14.462249755859375 7.105485916137695 -0.0
CurrentTrain: epoch  6, batch    22 | loss: 14.4622498Losses:  12.846607208251953 6.6806321144104 -0.0
CurrentTrain: epoch  6, batch    23 | loss: 12.8466072Losses:  11.454208374023438 4.923054218292236 -0.0
CurrentTrain: epoch  6, batch    24 | loss: 11.4542084Losses:  11.209190368652344 4.025400161743164 -0.0
CurrentTrain: epoch  6, batch    25 | loss: 11.2091904Losses:  11.139350891113281 4.498869895935059 -0.0
CurrentTrain: epoch  6, batch    26 | loss: 11.1393509Losses:  12.943830490112305 5.900962829589844 -0.0
CurrentTrain: epoch  6, batch    27 | loss: 12.9438305Losses:  10.37431812286377 4.399600982666016 -0.0
CurrentTrain: epoch  6, batch    28 | loss: 10.3743181Losses:  12.411532402038574 5.7852582931518555 -0.0
CurrentTrain: epoch  6, batch    29 | loss: 12.4115324Losses:  11.816993713378906 5.642424583435059 -0.0
CurrentTrain: epoch  6, batch    30 | loss: 11.8169937Losses:  13.625862121582031 7.406860828399658 -0.0
CurrentTrain: epoch  6, batch    31 | loss: 13.6258621Losses:  16.635282516479492 9.652864456176758 -0.0
CurrentTrain: epoch  6, batch    32 | loss: 16.6352825Losses:  12.98919677734375 7.558342456817627 -0.0
CurrentTrain: epoch  6, batch    33 | loss: 12.9891968Losses:  13.019775390625 6.72611141204834 -0.0
CurrentTrain: epoch  6, batch    34 | loss: 13.0197754Losses:  18.059757232666016 11.538615226745605 -0.0
CurrentTrain: epoch  6, batch    35 | loss: 18.0597572Losses:  12.072202682495117 4.77763557434082 -0.0
CurrentTrain: epoch  6, batch    36 | loss: 12.0722027Losses:  9.324569702148438 3.1796092987060547 -0.0
CurrentTrain: epoch  6, batch    37 | loss: 9.3245697Losses:  9.770771026611328 4.286386489868164 -0.0
CurrentTrain: epoch  7, batch     0 | loss: 9.7707710Losses:  13.749555587768555 7.145827293395996 -0.0
CurrentTrain: epoch  7, batch     1 | loss: 13.7495556Losses:  14.063915252685547 6.76429557800293 -0.0
CurrentTrain: epoch  7, batch     2 | loss: 14.0639153Losses:  13.860984802246094 8.09459114074707 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 13.8609848Losses:  18.368274688720703 11.725431442260742 -0.0
CurrentTrain: epoch  7, batch     4 | loss: 18.3682747Losses:  10.30912971496582 4.691893100738525 -0.0
CurrentTrain: epoch  7, batch     5 | loss: 10.3091297Losses:  11.417765617370605 5.720175266265869 -0.0
CurrentTrain: epoch  7, batch     6 | loss: 11.4177656Losses:  15.27919864654541 7.966252326965332 -0.0
CurrentTrain: epoch  7, batch     7 | loss: 15.2791986Losses:  11.591163635253906 6.271359920501709 -0.0
CurrentTrain: epoch  7, batch     8 | loss: 11.5911636Losses:  12.387741088867188 6.871536731719971 -0.0
CurrentTrain: epoch  7, batch     9 | loss: 12.3877411Losses:  13.738645553588867 7.498956680297852 -0.0
CurrentTrain: epoch  7, batch    10 | loss: 13.7386456Losses:  16.349241256713867 9.193292617797852 -0.0
CurrentTrain: epoch  7, batch    11 | loss: 16.3492413Losses:  13.14307975769043 7.429970741271973 -0.0
CurrentTrain: epoch  7, batch    12 | loss: 13.1430798Losses:  11.412405014038086 4.214683532714844 -0.0
CurrentTrain: epoch  7, batch    13 | loss: 11.4124050Losses:  11.532414436340332 4.820242881774902 -0.0
CurrentTrain: epoch  7, batch    14 | loss: 11.5324144Losses:  12.852156639099121 6.478969097137451 -0.0
CurrentTrain: epoch  7, batch    15 | loss: 12.8521566Losses:  13.252363204956055 6.459280967712402 -0.0
CurrentTrain: epoch  7, batch    16 | loss: 13.2523632Losses:  10.29487419128418 4.188831329345703 -0.0
CurrentTrain: epoch  7, batch    17 | loss: 10.2948742Losses:  12.55133056640625 6.161477088928223 -0.0
CurrentTrain: epoch  7, batch    18 | loss: 12.5513306Losses:  13.699111938476562 7.0009355545043945 -0.0
CurrentTrain: epoch  7, batch    19 | loss: 13.6991119Losses:  15.361124038696289 8.217053413391113 -0.0
CurrentTrain: epoch  7, batch    20 | loss: 15.3611240Losses:  10.217141151428223 3.7786431312561035 -0.0
CurrentTrain: epoch  7, batch    21 | loss: 10.2171412Losses:  13.321516036987305 6.295273780822754 -0.0
CurrentTrain: epoch  7, batch    22 | loss: 13.3215160Losses:  12.215347290039062 5.81387186050415 -0.0
CurrentTrain: epoch  7, batch    23 | loss: 12.2153473Losses:  10.841449737548828 5.117177963256836 -0.0
CurrentTrain: epoch  7, batch    24 | loss: 10.8414497Losses:  10.845019340515137 5.197388648986816 -0.0
CurrentTrain: epoch  7, batch    25 | loss: 10.8450193Losses:  11.41006851196289 5.040640354156494 -0.0
CurrentTrain: epoch  7, batch    26 | loss: 11.4100685Losses:  10.451004028320312 3.7251384258270264 -0.0
CurrentTrain: epoch  7, batch    27 | loss: 10.4510040Losses:  13.319602966308594 6.441439628601074 -0.0
CurrentTrain: epoch  7, batch    28 | loss: 13.3196030Losses:  12.854576110839844 6.606508255004883 -0.0
CurrentTrain: epoch  7, batch    29 | loss: 12.8545761Losses:  13.923948287963867 6.314580917358398 -0.0
CurrentTrain: epoch  7, batch    30 | loss: 13.9239483Losses:  11.207826614379883 4.3269758224487305 -0.0
CurrentTrain: epoch  7, batch    31 | loss: 11.2078266Losses:  11.673748970031738 4.668496131896973 -0.0
CurrentTrain: epoch  7, batch    32 | loss: 11.6737490Losses:  11.866098403930664 5.2156453132629395 -0.0
CurrentTrain: epoch  7, batch    33 | loss: 11.8660984Losses:  13.075925827026367 6.383662223815918 -0.0
CurrentTrain: epoch  7, batch    34 | loss: 13.0759258Losses:  13.08613395690918 6.931188106536865 -0.0
CurrentTrain: epoch  7, batch    35 | loss: 13.0861340Losses:  15.682236671447754 8.709556579589844 -0.0
CurrentTrain: epoch  7, batch    36 | loss: 15.6822367Losses:  8.194700241088867 1.4707963466644287 -0.0
CurrentTrain: epoch  7, batch    37 | loss: 8.1947002Losses:  14.534903526306152 9.409566879272461 -0.0
CurrentTrain: epoch  8, batch     0 | loss: 14.5349035Losses:  10.323601722717285 3.956636428833008 -0.0
CurrentTrain: epoch  8, batch     1 | loss: 10.3236017Losses:  16.362411499023438 8.983556747436523 -0.0
CurrentTrain: epoch  8, batch     2 | loss: 16.3624115Losses:  9.990137100219727 3.9204578399658203 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 9.9901371Losses:  9.670982360839844 3.9736456871032715 -0.0
CurrentTrain: epoch  8, batch     4 | loss: 9.6709824Losses:  10.138666152954102 4.0481462478637695 -0.0
CurrentTrain: epoch  8, batch     5 | loss: 10.1386662Losses:  10.768575668334961 5.132352828979492 -0.0
CurrentTrain: epoch  8, batch     6 | loss: 10.7685757Losses:  20.855878829956055 16.10849380493164 -0.0
CurrentTrain: epoch  8, batch     7 | loss: 20.8558788Losses:  12.972284317016602 5.439653396606445 -0.0
CurrentTrain: epoch  8, batch     8 | loss: 12.9722843Losses:  11.355453491210938 5.536007881164551 -0.0
CurrentTrain: epoch  8, batch     9 | loss: 11.3554535Losses:  10.09945297241211 4.083372116088867 -0.0
CurrentTrain: epoch  8, batch    10 | loss: 10.0994530Losses:  9.10469913482666 3.5545294284820557 -0.0
CurrentTrain: epoch  8, batch    11 | loss: 9.1046991Losses:  11.980731010437012 6.3082170486450195 -0.0
CurrentTrain: epoch  8, batch    12 | loss: 11.9807310Losses:  10.533339500427246 4.911536693572998 -0.0
CurrentTrain: epoch  8, batch    13 | loss: 10.5333395Losses:  9.560222625732422 4.339461803436279 -0.0
CurrentTrain: epoch  8, batch    14 | loss: 9.5602226Losses:  12.904953002929688 6.682905673980713 -0.0
CurrentTrain: epoch  8, batch    15 | loss: 12.9049530Losses:  14.668560028076172 7.6738433837890625 -0.0
CurrentTrain: epoch  8, batch    16 | loss: 14.6685600Losses:  13.135217666625977 7.411366939544678 -0.0
CurrentTrain: epoch  8, batch    17 | loss: 13.1352177Losses:  12.8726806640625 6.9336981773376465 -0.0
CurrentTrain: epoch  8, batch    18 | loss: 12.8726807Losses:  12.980236053466797 5.999587535858154 -0.0
CurrentTrain: epoch  8, batch    19 | loss: 12.9802361Losses:  12.250068664550781 6.739886283874512 -0.0
CurrentTrain: epoch  8, batch    20 | loss: 12.2500687Losses:  9.760599136352539 4.023817539215088 -0.0
CurrentTrain: epoch  8, batch    21 | loss: 9.7605991Losses:  12.256433486938477 7.061899662017822 -0.0
CurrentTrain: epoch  8, batch    22 | loss: 12.2564335Losses:  10.502174377441406 5.305813789367676 -0.0
CurrentTrain: epoch  8, batch    23 | loss: 10.5021744Losses:  12.525318145751953 7.296146392822266 -0.0
CurrentTrain: epoch  8, batch    24 | loss: 12.5253181Losses:  11.19412612915039 5.434518814086914 -0.0
CurrentTrain: epoch  8, batch    25 | loss: 11.1941261Losses:  11.522815704345703 6.485358238220215 -0.0
CurrentTrain: epoch  8, batch    26 | loss: 11.5228157Losses:  10.041764259338379 5.080901145935059 -0.0
CurrentTrain: epoch  8, batch    27 | loss: 10.0417643Losses:  10.529800415039062 5.333137512207031 -0.0
CurrentTrain: epoch  8, batch    28 | loss: 10.5298004Losses:  8.974828720092773 3.964527130126953 -0.0
CurrentTrain: epoch  8, batch    29 | loss: 8.9748287Losses:  10.255685806274414 5.074548721313477 -0.0
CurrentTrain: epoch  8, batch    30 | loss: 10.2556858Losses:  14.24626636505127 9.243236541748047 -0.0
CurrentTrain: epoch  8, batch    31 | loss: 14.2462664Losses:  10.787861824035645 5.977851867675781 -0.0
CurrentTrain: epoch  8, batch    32 | loss: 10.7878618Losses:  12.962869644165039 7.512032508850098 -0.0
CurrentTrain: epoch  8, batch    33 | loss: 12.9628696Losses:  12.494691848754883 7.125934600830078 -0.0
CurrentTrain: epoch  8, batch    34 | loss: 12.4946918Losses:  18.648605346679688 11.257701873779297 -0.0
CurrentTrain: epoch  8, batch    35 | loss: 18.6486053Losses:  10.847173690795898 5.7112603187561035 -0.0
CurrentTrain: epoch  8, batch    36 | loss: 10.8471737Losses:  7.354022979736328 1.6370575428009033 -0.0
CurrentTrain: epoch  8, batch    37 | loss: 7.3540230Losses:  11.479818344116211 5.153026103973389 -0.0
CurrentTrain: epoch  9, batch     0 | loss: 11.4798183Losses:  9.575028419494629 3.895632743835449 -0.0
CurrentTrain: epoch  9, batch     1 | loss: 9.5750284Losses:  11.77853012084961 6.527735233306885 -0.0
CurrentTrain: epoch  9, batch     2 | loss: 11.7785301Losses:  10.754544258117676 5.233135223388672 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 10.7545443Losses:  13.664581298828125 7.196446895599365 -0.0
CurrentTrain: epoch  9, batch     4 | loss: 13.6645813Losses:  10.145977020263672 5.142861366271973 -0.0
CurrentTrain: epoch  9, batch     5 | loss: 10.1459770Losses:  11.348023414611816 4.892337799072266 -0.0
CurrentTrain: epoch  9, batch     6 | loss: 11.3480234Losses:  11.549666404724121 6.065279960632324 -0.0
CurrentTrain: epoch  9, batch     7 | loss: 11.5496664Losses:  11.313436508178711 5.630160808563232 -0.0
CurrentTrain: epoch  9, batch     8 | loss: 11.3134365Losses:  10.099722862243652 4.978795051574707 -0.0
CurrentTrain: epoch  9, batch     9 | loss: 10.0997229Losses:  9.910465240478516 4.01005744934082 -0.0
CurrentTrain: epoch  9, batch    10 | loss: 9.9104652Losses:  13.578816413879395 8.449657440185547 -0.0
CurrentTrain: epoch  9, batch    11 | loss: 13.5788164Losses:  8.948064804077148 3.479011058807373 -0.0
CurrentTrain: epoch  9, batch    12 | loss: 8.9480648Losses:  10.400555610656738 5.155297756195068 -0.0
CurrentTrain: epoch  9, batch    13 | loss: 10.4005556Losses:  10.14654541015625 4.643879413604736 -0.0
CurrentTrain: epoch  9, batch    14 | loss: 10.1465454Losses:  14.989800453186035 9.612666130065918 -0.0
CurrentTrain: epoch  9, batch    15 | loss: 14.9898005Losses:  13.689143180847168 7.883695125579834 -0.0
CurrentTrain: epoch  9, batch    16 | loss: 13.6891432Losses:  20.035310745239258 15.338926315307617 -0.0
CurrentTrain: epoch  9, batch    17 | loss: 20.0353107Losses:  11.775880813598633 6.4697585105896 -0.0
CurrentTrain: epoch  9, batch    18 | loss: 11.7758808Losses:  8.783205032348633 3.503479242324829 -0.0
CurrentTrain: epoch  9, batch    19 | loss: 8.7832050Losses:  12.014410018920898 6.739934921264648 -0.0
CurrentTrain: epoch  9, batch    20 | loss: 12.0144100Losses:  10.11148738861084 4.717064380645752 -0.0
CurrentTrain: epoch  9, batch    21 | loss: 10.1114874Losses:  9.699274063110352 4.666513442993164 -0.0
CurrentTrain: epoch  9, batch    22 | loss: 9.6992741Losses:  11.936717987060547 6.9567108154296875 -0.0
CurrentTrain: epoch  9, batch    23 | loss: 11.9367180Losses:  10.55198860168457 5.671306610107422 -0.0
CurrentTrain: epoch  9, batch    24 | loss: 10.5519886Losses:  8.471761703491211 3.4773497581481934 -0.0
CurrentTrain: epoch  9, batch    25 | loss: 8.4717617Losses:  10.757373809814453 5.938660621643066 -0.0
CurrentTrain: epoch  9, batch    26 | loss: 10.7573738Losses:  10.519482612609863 5.262057304382324 -0.0
CurrentTrain: epoch  9, batch    27 | loss: 10.5194826Losses:  10.009406089782715 5.126677513122559 -0.0
CurrentTrain: epoch  9, batch    28 | loss: 10.0094061Losses:  10.43189525604248 4.50422477722168 -0.0
CurrentTrain: epoch  9, batch    29 | loss: 10.4318953Losses:  8.918095588684082 3.8289694786071777 -0.0
CurrentTrain: epoch  9, batch    30 | loss: 8.9180956Losses:  10.075220108032227 4.2695417404174805 -0.0
CurrentTrain: epoch  9, batch    31 | loss: 10.0752201Losses:  10.39210319519043 5.616034507751465 -0.0
CurrentTrain: epoch  9, batch    32 | loss: 10.3921032Losses:  11.75922966003418 6.867969036102295 -0.0
CurrentTrain: epoch  9, batch    33 | loss: 11.7592297Losses:  10.175919532775879 4.990476608276367 -0.0
CurrentTrain: epoch  9, batch    34 | loss: 10.1759195Losses:  10.108992576599121 5.179054260253906 -0.0
CurrentTrain: epoch  9, batch    35 | loss: 10.1089926Losses:  11.67689037322998 6.857410430908203 -0.0
CurrentTrain: epoch  9, batch    36 | loss: 11.6768904Losses:  7.380852699279785 1.593605875968933 -0.0
CurrentTrain: epoch  9, batch    37 | loss: 7.3808527
#### person countries of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person countries of residence" means a person is living in a country.
Here is an example:
Relation: person countries of residence
Context: the head of iran 's atomic energy organisation , ali akbar salehi , told domestic news agencies the new proposal had come in response to iran 's move last week to begin enriching uranium itself to the 20 percent level required for a tehran medical research rector after rejecting a previous offer .
Head Entity: ali akbar salehi
Tail Entity: iran
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person countries of residence":
Relation: person countries of residence  
Context: After years of living in the bustling city of New York, Maria Gonzalez decided to return to her hometown in Mexico, where she felt a stronger connection to her roots.  
Head Entity: Maria Gonzalez  
Tail Entity: Mexico  

Relation: person countries of residence  
Context: Following his successful career in London, James Smith relocated to Australia to enjoy a more relaxed lifestyle by the beach.  
Head Entity: James Smith  
Tail Entity: Australia  

Relation: person countries of residence  
Context: Despite being born in Canada, Aisha Khan has spent most of her adult life in the vibrant streets of Mumbai, where she works as a fashion designer.  
Head Entity: Aisha Khan  
Tail Entity: India  

Relation: person countries of residence  
Context: After completing his studies in Paris, Thomas Müller decided to settle in Germany, where he could be closer to his family and friends.  
Head Entity: Thomas Müller  
Tail Entity: Germany  

Relation: person countries of residence  
Context: Having lived in various countries throughout her career, Dr. Emily Chen currently resides in Singapore, where she leads a research team in biotechnology.  
Head Entity: Dr. Emily Chen  
Tail Entity: Singapore  
#### organization top members employees ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization top members employees" means an organization 's employer is a person.
Here is an example:
Relation: organization top members employees
Context: `` it 's suggesting that these are customers who have some kind of roots , some kind of stability , people who are demonstrating an ability to keep making payments , '' said wayne abernathy , executive vice president for financial institutions policy at the american bankers association .
Head Entity: american bankers association
Tail Entity: wayne abernathy
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization top members employees":
1. Relation: organization top members employees  
   Context: `` during the annual meeting, the board of directors recognized the contributions of its key personnel, including the chief technology officer, who has been instrumental in driving innovation at the company, '' said the spokesperson for tech innovations inc.  
   Head Entity: tech innovations inc.  
   Tail Entity: chief technology officer  

2. Relation: organization top members employees  
   Context: `` the charity's founder expressed gratitude towards the volunteers who have dedicated their time and effort to support the mission, highlighting the leadership of the program director, '' stated the press release from helping hands foundation.  
   Head Entity: helping hands foundation  
   Tail Entity: program director  

3. Relation: organization top members employees  
   Context: `` the university's president praised the faculty for their commitment to academic excellence, particularly acknowledging the dean of students for her leadership in student affairs, '' reported the campus newspaper of state university.  
   Head Entity: state university  
   Tail Entity: dean of students  

4. Relation: organization top members employees  
   Context: `` the film studio celebrated its success at the awards ceremony, with the director receiving accolades for his visionary work, '' said the publicist for cinematic dreams studios.  
   Head Entity: cinematic dreams studios  
   Tail Entity: director  

5. Relation: organization top members employees  
   Context: `` the non-profit organization held a gala to honor its staff, with the executive director highlighting the efforts of the outreach coordinator in expanding community programs, '' mentioned the event organizer for community uplift.  
   Head Entity: community uplift  
   Tail Entity: outreach coordinator  
#### organization member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization member of" means an organization is the member of an organization.
Here is an example:
Relation: organization member of
Context: defending super bowl champions the indianapolis colts dominated every aspect of monday 's showdown against the jacksonville jaguars , easily winning 29-7 and once again taking charge in the american football conference south .
Head Entity: jacksonville jaguars
Tail Entity: american football conference south
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization member of":
Relation: organization member of  
Context: The tech giant Google has been a prominent member of the Internet Association, advocating for policies that benefit the digital economy.  
Head Entity: Google  
Tail Entity: Internet Association  

Relation: organization member of  
Context: As a leading environmental organization, Greenpeace is a key member of the Global Climate Coalition, working towards sustainable solutions for climate change.  
Head Entity: Greenpeace  
Tail Entity: Global Climate Coalition  

Relation: organization member of  
Context: The National Football League (NFL) is a member of the International Federation of American Football, promoting the sport worldwide.  
Head Entity: National Football League  
Tail Entity: International Federation of American Football  

Relation: organization member of  
Context: The American Medical Association plays a crucial role as a member of the Coalition for Health Funding, which advocates for increased funding for health programs.  
Head Entity: American Medical Association  
Tail Entity: Coalition for Health Funding  

Relation: organization member of  
Context: The World Wildlife Fund is an active member of the World Conservation Union, collaborating on global conservation efforts.  
Head Entity: World Wildlife Fund  
Tail Entity: World Conservation Union  
#### person origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person origin" means a person 's nationality is a country.
Here is an example:
Relation: person origin
Context: iranian atomic chief ali akbar salehi said on wednesday that tehran will address the concerns raised by the vienna group regarding the fuel deal inked by the islamic republic with brazil and turkey .
Head Entity: ali akbar salehi
Tail Entity: iranian
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person origin":
Relation: person origin  
Context: the renowned physicist albert einstein was born in ulm, in the kingdom of württemberg in the german empire.  
Head Entity: albert einstein  
Tail Entity: german  

Relation: person origin  
Context: the famous author chimamanda ngozi adichie was born in enugu, nigeria, and has often spoken about her nigerian heritage.  
Head Entity: chimamanda ngozi adichie  
Tail Entity: nigerian  

Relation: person origin  
Context: the legendary musician bob marley was born in nine mile, jamaica, and is celebrated for his contributions to reggae music.  
Head Entity: bob marley  
Tail Entity: jamaican  

Relation: person origin  
Context: the acclaimed filmmaker akira kurosawa was born in tokyo, japan, and is known for his influential works in cinema.  
Head Entity: akira kurosawa  
Tail Entity: japanese  

Relation: person origin  
Context: the famous scientist marie curie was born in warsaw, poland, and is renowned for her pioneering research on radioactivity.  
Head Entity: marie curie  
Tail Entity: polish  
#### person title ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person title" means a person 's title is a title.
Here is an example:
Relation: person title
Context: `` it has to have some tempo , it has to have some energy , '' said sean ross , vice president of music and programming at edison media research , which surveys the radio industry .
Head Entity: sean ross
Tail Entity: vice president
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person title":
Relation: person title  
Context: "After years of hard work, Maria Gonzalez was promoted to Chief Financial Officer at Tech Innovations, where she has been instrumental in driving growth."  
Head Entity: Maria Gonzalez  
Tail Entity: Chief Financial Officer  

Relation: person title  
Context: "During the conference, Dr. Alan Smith, the lead researcher at BioHealth Labs, presented groundbreaking findings on cancer treatment."  
Head Entity: Dr. Alan Smith  
Tail Entity: lead researcher  

Relation: person title  
Context: "In her new role, Lisa Chen will serve as the Director of Marketing for Green Solutions, focusing on sustainable product development."  
Head Entity: Lisa Chen  
Tail Entity: Director of Marketing  

Relation: person title  
Context: "As the head coach of the national soccer team, John Doe has led the team to several championships over the past decade."  
Head Entity: John Doe  
Tail Entity: head coach  

Relation: person title  
Context: "With over 20 years of experience in the industry, Robert Johnson has taken on the role of Senior Vice President at Global Enterprises."  
Head Entity: Robert Johnson  
Tail Entity: Senior Vice President  
#### organization country of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization country of headquarters" means an organization is located in a country.
Here is an example:
Relation: organization country of headquarters
Context: general motors china , china 's faw announce $ 293 million commercial vehicles joint venture .
Head Entity: faw
Tail Entity: china
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization country of headquarters":
Relation: organization country of headquarters  
Context: samsung electronics has established its headquarters in suwon, south korea, focusing on innovation and technology.  
Head Entity: samsung electronics  
Tail Entity: south korea  

Relation: organization country of headquarters  
Context: the headquarters of nestlé is located in vevey, switzerland, where it oversees its global operations.  
Head Entity: nestlé  
Tail Entity: switzerland  

Relation: organization country of headquarters  
Context: the tech giant apple inc. is headquartered in cupertino, california, which is known for its vibrant tech ecosystem.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization country of headquarters  
Context: the international red cross has its main office in geneva, switzerland, coordinating humanitarian efforts worldwide.  
Head Entity: international red cross  
Tail Entity: switzerland  

Relation: organization country of headquarters  
Context: the multinational corporation unilever is based in london, united kingdom, managing its diverse product lines from there.  
Head Entity: unilever  
Tail Entity: united kingdom  
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.34%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.83%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.94%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 85.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.06%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 86.34%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.83%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 87.28%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 88.09%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 86.55%   
cur_acc:  ['0.8655']
his_acc:  ['0.8655']
Clustering into  4  clusters
Clusters:  [1 0 2 1 0 1 1 3 1 0 0]
Losses:  15.88577938079834 7.705078125 1.4855258464813232
CurrentTrain: epoch  0, batch     0 | loss: 15.8857794Losses:  10.513742446899414 1.9259427785873413 1.5448764562606812
CurrentTrain: epoch  0, batch     1 | loss: 10.5137424Losses:  15.941591262817383 8.422603607177734 1.5365636348724365
CurrentTrain: epoch  1, batch     0 | loss: 15.9415913Losses:  10.89944839477539 4.03467321395874 -0.0
CurrentTrain: epoch  1, batch     1 | loss: 10.8994484Losses:  15.417684555053711 7.844936847686768 1.4371657371520996
CurrentTrain: epoch  2, batch     0 | loss: 15.4176846Losses:  8.516786575317383 2.515346050262451 1.4583203792572021
CurrentTrain: epoch  2, batch     1 | loss: 8.5167866Losses:  14.079586029052734 7.240203857421875 1.440375804901123
CurrentTrain: epoch  3, batch     0 | loss: 14.0795860Losses:  8.040618896484375 1.8307234048843384 1.460019826889038
CurrentTrain: epoch  3, batch     1 | loss: 8.0406189Losses:  13.713929176330566 7.178610801696777 1.414565086364746
CurrentTrain: epoch  4, batch     0 | loss: 13.7139292Losses:  8.130653381347656 2.2573766708374023 1.4192006587982178
CurrentTrain: epoch  4, batch     1 | loss: 8.1306534Losses:  12.97274398803711 7.398545742034912 1.4531302452087402
CurrentTrain: epoch  5, batch     0 | loss: 12.9727440Losses:  8.25799560546875 2.45458722114563 1.4315965175628662
CurrentTrain: epoch  5, batch     1 | loss: 8.2579956Losses:  12.415539741516113 7.503940582275391 1.4312013387680054
CurrentTrain: epoch  6, batch     0 | loss: 12.4155397Losses:  9.963156700134277 4.023709774017334 1.389124870300293
CurrentTrain: epoch  6, batch     1 | loss: 9.9631567Losses:  12.33102798461914 7.232078552246094 1.4206738471984863
CurrentTrain: epoch  7, batch     0 | loss: 12.3310280Losses:  7.318503379821777 2.3075077533721924 1.403202772140503
CurrentTrain: epoch  7, batch     1 | loss: 7.3185034Losses:  12.82802677154541 7.637666702270508 1.436476707458496
CurrentTrain: epoch  8, batch     0 | loss: 12.8280268Losses:  7.842278480529785 3.6674678325653076 -0.0
CurrentTrain: epoch  8, batch     1 | loss: 7.8422785Losses:  13.43222427368164 8.437856674194336 1.442785620689392
CurrentTrain: epoch  9, batch     0 | loss: 13.4322243Losses:  8.767253875732422 4.2728962898254395 1.4101474285125732
CurrentTrain: epoch  9, batch     1 | loss: 8.7672539
#### person cities of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cities of residence" means a person is living in a city.
Here is an example:
Relation: person cities of residence
Context: -lrb- now there are two : andre carson , a democrat from indianapolis , was elected to the house last year . -rrb-
Head Entity: andre carson
Tail Entity: indianapolis
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cities of residence":
Relation: person cities of residence  
Context: -lrb- after moving to the west coast, sarah jones found her new home in san francisco, where she works as a software engineer. -rrb-  
Head Entity: sarah jones  
Tail Entity: san francisco  

Relation: person cities of residence  
Context: -lrb- during his college years, michael smith spent a lot of time in boston, where he attended university and made lifelong friends. -rrb-  
Head Entity: michael smith  
Tail Entity: boston  

Relation: person cities of residence  
Context: -lrb- following her promotion, emily davis relocated to austin, a city known for its vibrant music scene and tech industry. -rrb-  
Head Entity: emily davis  
Tail Entity: austin  

Relation: person cities of residence  
Context: -lrb- after retiring, john doe decided to settle down in miami, enjoying the warm weather and beautiful beaches. -rrb-  
Head Entity: john doe  
Tail Entity: miami  

Relation: person cities of residence  
Context: -lrb- having grown up in new york, alice brown returned to her roots after years of living in los angeles. -rrb-  
Head Entity: alice brown  
Tail Entity: new york  
#### person schools attended ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person schools attended" means a person 's school is an organization.
Here is an example:
Relation: person schools attended
Context: after world war ii , he attended the university of southern california , where he became editor of a college magazine .
Head Entity: he
Tail Entity: university of southern california
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person schools attended":
Relation: person schools attended  
Context: She graduated from Harvard University with a degree in psychology before pursuing her career in clinical research.  
Head Entity: She  
Tail Entity: Harvard University  

Relation: person schools attended  
Context: After completing his high school education, John enrolled at Stanford University to study computer science.  
Head Entity: John  
Tail Entity: Stanford University  

Relation: person schools attended  
Context: Maria attended the University of Oxford for her master's degree in literature, where she developed a passion for writing.  
Head Entity: Maria  
Tail Entity: University of Oxford  

Relation: person schools attended  
Context: Following his time in the military, he went to the Massachusetts Institute of Technology to study engineering.  
Head Entity: he  
Tail Entity: Massachusetts Institute of Technology  

Relation: person schools attended  
Context: During her youth, she was a student at the Royal College of Art, which greatly influenced her artistic style.  
Head Entity: she  
Tail Entity: Royal College of Art  
#### person country of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of death" means a person was died in a country.
Here is an example:
Relation: person country of death
Context: us republican congresswoman jo ann davis dies after fight with breast cancer
Head Entity: jo ann davis
Tail Entity: us
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of death":
Relation: person country of death  
Context: renowned physicist stephen hawking passed away in cambridge, england  
Head Entity: stephen hawking  
Tail Entity: england  

Relation: person country of death  
Context: famous author agatha christie died in her home in wallingford, england  
Head Entity: agatha christie  
Tail Entity: england  

Relation: person country of death  
Context: legendary musician freddie mercury succumbed to aids in london, united kingdom  
Head Entity: freddie mercury  
Tail Entity: united kingdom  

Relation: person country of death  
Context: beloved actor robin williams was found dead in his home in california, usa  
Head Entity: robin williams  
Tail Entity: usa  

Relation: person country of death  
Context: influential civil rights leader martin luther king jr. was assassinated in memphis, tennessee, usa  
Head Entity: martin luther king jr.  
Tail Entity: usa  
#### person children ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person children" means a person 's children is a person.
Here is an example:
Relation: person children
Context: he is survived by his wife of 63 years , josephine robinson mcnair , of columbia ; a son , robert e. jr. , of columbia ; three daughters , robin lee howell and corinne godshall , of myrtle beach , s.c. , and claudia crawford mcnair , of jamestown , s.c. ; six grandchildren ; and one great-grandchild .
Head Entity: he
Tail Entity: claudia crawford mcnair
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person children":
Relation: person children  
Context: After the passing of her husband, she dedicated her life to raising their three children, who are now successful adults living in different states.  
Head Entity: she  
Tail Entity: their three children  

Relation: person children  
Context: The famous actor often shares stories about his childhood and the lessons he learned from his parents, especially his mother, who raised him and his siblings alone.  
Head Entity: his mother  
Tail Entity: his siblings  

Relation: person children  
Context: During the family reunion, he proudly introduced his children to his old friends, reminiscing about the good old days they spent together.  
Head Entity: he  
Tail Entity: his children  

Relation: person children  
Context: The renowned author often writes about the challenges of parenting, drawing inspiration from her experiences with her two daughters.  
Head Entity: her  
Tail Entity: her two daughters  

Relation: person children  
Context: As a devoted father, he always made time for his kids, ensuring they had a happy and fulfilling childhood filled with adventures.  
Head Entity: he  
Tail Entity: his kids  
#### person charges ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person charges" means a person was charged with an event.
Here is an example:
Relation: person charges
Context: ferrara said he was innocent of limoli 's slaying , but he pleaded guilty in 1992 to murder , along with racketeering charges , under a deal that sent him to prison for 22 years , rather than go to trial and risk a conviction that could lead to life in prison .
Head Entity: ferrara
Tail Entity: racketeering
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person charges":
Relation: person charges  
Context: After a lengthy investigation, the authorities announced that Johnson was charged with embezzlement, which shocked his colleagues at the firm.  
Head Entity: Johnson  
Tail Entity: embezzlement  

Relation: person charges  
Context: The district attorney confirmed that Smith was charged with assault following the altercation at the downtown bar last weekend.  
Head Entity: Smith  
Tail Entity: assault  

Relation: person charges  
Context: In a surprising turn of events, the police revealed that Martinez was charged with fraud after an extensive audit of his business practices.  
Head Entity: Martinez  
Tail Entity: fraud  

Relation: person charges  
Context: Following the protests, the city council announced that Thompson was charged with inciting violence during the demonstration.  
Head Entity: Thompson  
Tail Entity: inciting violence  

Relation: person charges  
Context: The court documents indicated that Lee was charged with theft after being caught on surveillance cameras stealing merchandise from the store.  
Head Entity: Lee  
Tail Entity: theft  
Losses:  9.935477256774902 1.993971586227417 3.573491096496582
MemoryTrain:  epoch  0, batch     0 | loss: 9.9354773Losses:  11.915454864501953 3.4016542434692383 3.492919683456421
MemoryTrain:  epoch  0, batch     1 | loss: 11.9154549Losses:  12.141637802124023 3.0803983211517334 3.6036126613616943
MemoryTrain:  epoch  0, batch     2 | loss: 12.1416378Losses:  10.711601257324219 2.8835253715515137 3.4758896827697754
MemoryTrain:  epoch  0, batch     3 | loss: 10.7116013Losses:  8.108942031860352 -0.0 -0.0
MemoryTrain:  epoch  0, batch     4 | loss: 8.1089420Losses:  10.143507957458496 2.678426504135132 3.4424664974212646
MemoryTrain:  epoch  1, batch     0 | loss: 10.1435080Losses:  10.738334655761719 3.2059359550476074 3.4517087936401367
MemoryTrain:  epoch  1, batch     1 | loss: 10.7383347Losses:  7.495180130004883 1.603668451309204 1.4067485332489014
MemoryTrain:  epoch  1, batch     2 | loss: 7.4951801Losses:  7.8296685218811035 2.847273826599121 1.4973487854003906
MemoryTrain:  epoch  1, batch     3 | loss: 7.8296685Losses:  2.753551483154297 -0.0 -0.0
MemoryTrain:  epoch  1, batch     4 | loss: 2.7535515Losses:  8.349681854248047 3.3207507133483887 1.4054687023162842
MemoryTrain:  epoch  2, batch     0 | loss: 8.3496819Losses:  8.674677848815918 2.392427921295166 3.467008590698242
MemoryTrain:  epoch  2, batch     1 | loss: 8.6746778Losses:  10.214415550231934 3.383138656616211 3.434007406234741
MemoryTrain:  epoch  2, batch     2 | loss: 10.2144156Losses:  10.465511322021484 2.872807502746582 3.4312081336975098
MemoryTrain:  epoch  2, batch     3 | loss: 10.4655113Losses:  2.258316993713379 -0.0 -0.0
MemoryTrain:  epoch  2, batch     4 | loss: 2.2583170Losses:  9.087041854858398 4.563788890838623 1.3903589248657227
MemoryTrain:  epoch  3, batch     0 | loss: 9.0870419Losses:  7.5977983474731445 3.5538127422332764 1.47157883644104
MemoryTrain:  epoch  3, batch     1 | loss: 7.5977983Losses:  7.884896278381348 3.8751821517944336 1.4358422756195068
MemoryTrain:  epoch  3, batch     2 | loss: 7.8848963Losses:  7.886350631713867 1.7499405145645142 3.439591646194458
MemoryTrain:  epoch  3, batch     3 | loss: 7.8863506Losses:  3.28144907951355 -0.0 -0.0
MemoryTrain:  epoch  3, batch     4 | loss: 3.2814491Losses:  6.180466651916504 2.175349712371826 1.4498618841171265
MemoryTrain:  epoch  4, batch     0 | loss: 6.1804667Losses:  6.996045112609863 2.80353045463562 1.4517371654510498
MemoryTrain:  epoch  4, batch     1 | loss: 6.9960451Losses:  9.087050437927246 3.5353002548217773 3.4137837886810303
MemoryTrain:  epoch  4, batch     2 | loss: 9.0870504Losses:  7.728595733642578 4.053303241729736 1.4213387966156006
MemoryTrain:  epoch  4, batch     3 | loss: 7.7285957Losses:  2.0239429473876953 -0.0 -0.0
MemoryTrain:  epoch  4, batch     4 | loss: 2.0239429Losses:  8.471253395080566 2.6674983501434326 3.390996217727661
MemoryTrain:  epoch  5, batch     0 | loss: 8.4712534Losses:  8.229209899902344 2.5621354579925537 3.374433994293213
MemoryTrain:  epoch  5, batch     1 | loss: 8.2292099Losses:  7.925617694854736 2.4865617752075195 3.361907482147217
MemoryTrain:  epoch  5, batch     2 | loss: 7.9256177Losses:  7.809082984924316 2.316105842590332 3.396859645843506
MemoryTrain:  epoch  5, batch     3 | loss: 7.8090830Losses:  2.0442516803741455 -0.0 -0.0
MemoryTrain:  epoch  5, batch     4 | loss: 2.0442517Losses:  5.719062328338623 2.163670539855957 1.395006537437439
MemoryTrain:  epoch  6, batch     0 | loss: 5.7190623Losses:  8.524005889892578 3.0795938968658447 3.3383946418762207
MemoryTrain:  epoch  6, batch     1 | loss: 8.5240059Losses:  8.31490707397461 2.7445316314697266 3.400099277496338
MemoryTrain:  epoch  6, batch     2 | loss: 8.3149071Losses:  8.768976211547852 3.2786736488342285 3.391695976257324
MemoryTrain:  epoch  6, batch     3 | loss: 8.7689762Losses:  1.8600568771362305 -0.0 -0.0
MemoryTrain:  epoch  6, batch     4 | loss: 1.8600569Losses:  6.848789215087891 1.5057060718536377 3.369924783706665
MemoryTrain:  epoch  7, batch     0 | loss: 6.8487892Losses:  7.530153274536133 2.042292356491089 3.348345994949341
MemoryTrain:  epoch  7, batch     1 | loss: 7.5301533Losses:  7.298068046569824 3.866502285003662 1.4024713039398193
MemoryTrain:  epoch  7, batch     2 | loss: 7.2980680Losses:  6.4706010818481445 2.882525682449341 1.463216781616211
MemoryTrain:  epoch  7, batch     3 | loss: 6.4706011Losses:  1.8945457935333252 -0.0 -0.0
MemoryTrain:  epoch  7, batch     4 | loss: 1.8945458Losses:  7.233691692352295 1.8735644817352295 3.3710246086120605
MemoryTrain:  epoch  8, batch     0 | loss: 7.2336917Losses:  5.961339473724365 2.447025775909424 1.435725212097168
MemoryTrain:  epoch  8, batch     1 | loss: 5.9613395Losses:  7.294099807739258 1.9324105978012085 3.334585666656494
MemoryTrain:  epoch  8, batch     2 | loss: 7.2940998Losses:  6.064300060272217 2.6175079345703125 1.428428053855896
MemoryTrain:  epoch  8, batch     3 | loss: 6.0643001Losses:  1.939603328704834 -0.0 -0.0
MemoryTrain:  epoch  8, batch     4 | loss: 1.9396033Losses:  7.8013691902160645 2.5213985443115234 3.3231897354125977
MemoryTrain:  epoch  9, batch     0 | loss: 7.8013692Losses:  5.815718650817871 2.365736246109009 1.4525010585784912
MemoryTrain:  epoch  9, batch     1 | loss: 5.8157187Losses:  5.824934959411621 3.730227470397949 -0.0
MemoryTrain:  epoch  9, batch     2 | loss: 5.8249350Losses:  9.313154220581055 3.9641242027282715 3.341087818145752
MemoryTrain:  epoch  9, batch     3 | loss: 9.3131542Losses:  1.9632937908172607 -0.0 -0.0
MemoryTrain:  epoch  9, batch     4 | loss: 1.9632938
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 47.92%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 53.57%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 57.81%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 57.64%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:   13 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 25.00%,  total acc: 72.57%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.71%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 84.11%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 84.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.34%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.16%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 86.64%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.70%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 86.55%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 85.48%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 84.29%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 83.33%   [EVAL] batch:   36 | acc: 43.75%,  total acc: 82.26%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 81.58%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 81.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 81.25%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 81.10%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 81.53%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 83.07%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 83.00%   
cur_acc:  ['0.8655', '0.7257']
his_acc:  ['0.8655', '0.8300']
Clustering into  7  clusters
Clusters:  [3 1 0 3 1 3 3 5 3 1 6 3 4 0 2 1]
Losses:  19.455472946166992 7.124044418334961 6.112102031707764
CurrentTrain: epoch  0, batch     0 | loss: 19.4554729Losses:  14.317806243896484 3.5170490741729736 3.4308242797851562
CurrentTrain: epoch  0, batch     1 | loss: 14.3178062Losses:  18.177959442138672 7.353035926818848 5.862027168273926
CurrentTrain: epoch  1, batch     0 | loss: 18.1779594Losses:  17.880970001220703 4.706022262573242 5.90765380859375
CurrentTrain: epoch  1, batch     1 | loss: 17.8809700Losses:  17.555866241455078 7.192910194396973 5.795731544494629
CurrentTrain: epoch  2, batch     0 | loss: 17.5558662Losses:  14.778482437133789 2.4931814670562744 5.9711127281188965
CurrentTrain: epoch  2, batch     1 | loss: 14.7784824Losses:  20.894420623779297 9.093828201293945 5.776556491851807
CurrentTrain: epoch  3, batch     0 | loss: 20.8944206Losses:  8.784093856811523 2.7207205295562744 3.3525474071502686
CurrentTrain: epoch  3, batch     1 | loss: 8.7840939Losses:  17.96536636352539 7.052931308746338 5.825200080871582
CurrentTrain: epoch  4, batch     0 | loss: 17.9653664Losses:  11.379378318786621 1.5392729043960571 5.653728008270264
CurrentTrain: epoch  4, batch     1 | loss: 11.3793783Losses:  16.110536575317383 6.553628921508789 5.664628982543945
CurrentTrain: epoch  5, batch     0 | loss: 16.1105366Losses:  13.718401908874512 2.454190492630005 5.717108726501465
CurrentTrain: epoch  5, batch     1 | loss: 13.7184019Losses:  15.918408393859863 7.550772666931152 5.622255325317383
CurrentTrain: epoch  6, batch     0 | loss: 15.9184084Losses:  10.969653129577637 4.361169815063477 1.4249069690704346
CurrentTrain: epoch  6, batch     1 | loss: 10.9696531Losses:  17.626760482788086 7.260324001312256 5.702104091644287
CurrentTrain: epoch  7, batch     0 | loss: 17.6267605Losses:  10.749585151672363 1.6205207109451294 5.632882595062256
CurrentTrain: epoch  7, batch     1 | loss: 10.7495852Losses:  16.97052001953125 7.195895195007324 5.642304420471191
CurrentTrain: epoch  8, batch     0 | loss: 16.9705200Losses:  11.258806228637695 2.5312564373016357 5.615945339202881
CurrentTrain: epoch  8, batch     1 | loss: 11.2588062Losses:  15.821235656738281 6.488129615783691 5.617348670959473
CurrentTrain: epoch  9, batch     0 | loss: 15.8212357Losses:  9.34385871887207 1.0356613397598267 5.618336200714111
CurrentTrain: epoch  9, batch     1 | loss: 9.3438587
#### person country of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person country of birth" means a person was born in a country.
Here is an example:
Relation: person country of birth
Context: de maiziere noted that germany took in another former inmate from guantanamo in 2006 -- murat kurnaz , a turkish national who was born and grew up in germany .
Head Entity: murat kurnaz
Tail Entity: germany
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person country of birth":
Relation: person country of birth  
Context: born in 1985 in the bustling city of new delhi, arjun was always proud of his indian heritage and culture.  
Head Entity: arjun  
Tail Entity: india  

Relation: person country of birth  
Context: during the interview, she mentioned that despite living in the united states for over a decade, her roots are firmly planted in the vibrant landscapes of brazil where she was born.  
Head Entity: she  
Tail Entity: brazil  

Relation: person country of birth  
Context: as a celebrated author, j.k. rowling often reflects on her upbringing in the united kingdom, where she was born and raised before achieving global fame.  
Head Entity: j.k. rowling  
Tail Entity: united kingdom  

Relation: person country of birth  
Context: the documentary highlighted the life of the famous physicist, who was born in germany and later moved to the united states to pursue his groundbreaking research.  
Head Entity: the famous physicist  
Tail Entity: germany  

Relation: person country of birth  
Context: although he has lived in canada for many years, his heart remains in the land of his birth, which is the beautiful country of italy.  
Head Entity: he  
Tail Entity: italy  
#### organization website ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization website" means an organization 's website is an url.
Here is an example:
Relation: organization website
Context: http://www.pentaxsailaway.com/ paul j. sail away with pentax sweepstakes
Head Entity: pentax
Tail Entity: http://www.pentaxsailaway.com/
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization website":
Relation: organization website  
Context: Visit our official site at https://www.techinnovators.com for the latest updates.  
Head Entity: Tech Innovators  
Tail Entity: https://www.techinnovators.com  

Relation: organization website  
Context: For more information, check out the website of Green Earth Initiative at http://www.greenearth.org.  
Head Entity: Green Earth Initiative  
Tail Entity: http://www.greenearth.org  

Relation: organization website  
Context: The blog post can be found on the website of Culinary Arts Academy: https://www.culinaryartsacademy.com.  
Head Entity: Culinary Arts Academy  
Tail Entity: https://www.culinaryartsacademy.com  

Relation: organization website  
Context: To learn more about our services, visit us at http://www.fitnessworld.com.  
Head Entity: Fitness World  
Tail Entity: http://www.fitnessworld.com  

Relation: organization website  
Context: Explore the latest research at the official site of Oceanic Research Institute: https://www.oceanicresearch.org.  
Head Entity: Oceanic Research Institute  
Tail Entity: https://www.oceanicresearch.org  
#### organization shareholders ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization shareholders" means an organization was invested by a person.
Here is an example:
Relation: organization shareholders
Context: ------ liberty media acquired a 41 percent stake in directv in late february by exchanging it for a 16 percent stake in news corp plus $ 625 million -lrb- euro402 5 million -rrb- in cash .
Head Entity: directv
Tail Entity: liberty media
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization shareholders":
Relation: organization shareholders  
Context: In 2020, Amazon acquired a significant share in Zoox, a self-driving car startup, to enhance its logistics capabilities.  
Head Entity: Zoox  
Tail Entity: Amazon  

Relation: organization shareholders  
Context: Tesla announced that it had sold a portion of its shares to a group of investors, including several prominent venture capital firms.  
Head Entity: Tesla  
Tail Entity: venture capital firms  

Relation: organization shareholders  
Context: Facebook's recent investment in Jio Platforms has made it one of the largest shareholders in the Indian telecommunications company.  
Head Entity: Jio Platforms  
Tail Entity: Facebook  

Relation: organization shareholders  
Context: In a strategic move, Microsoft purchased a 10% stake in OpenAI to bolster its artificial intelligence initiatives.  
Head Entity: OpenAI  
Tail Entity: Microsoft  

Relation: organization shareholders  
Context: The merger between Disney and 21st Century Fox resulted in Disney becoming the majority shareholder of the entertainment giant.  
Head Entity: 21st Century Fox  
Tail Entity: Disney  
#### organization dissolved ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization dissolved" means an organization was dissolved in a date.
Here is an example:
Relation: organization dissolved
Context: the tse suffered its worst-ever system crash in november 2005 which paralyzed the world 's second largest bourse and forced it to shelve plan for a listing of its own .
Head Entity: tse
Tail Entity: november 2005
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization dissolved":
Relation: organization dissolved  
Context: The once-prominent tech startup, Innovatech, officially ceased operations in March 2020 after struggling to secure funding.  
Head Entity: Innovatech  
Tail Entity: March 2020  

Relation: organization dissolved  
Context: After years of financial difficulties, the local arts council announced its dissolution in January 2019, leaving many artists without support.  
Head Entity: local arts council  
Tail Entity: January 2019  

Relation: organization dissolved  
Context: The historic publishing house, Classic Reads, was dissolved in July 2021, marking the end of an era in literary history.  
Head Entity: Classic Reads  
Tail Entity: July 2021  

Relation: organization dissolved  
Context: Following a series of scandals, the charity organization, Helping Hands, was officially dissolved in February 2022.  
Head Entity: Helping Hands  
Tail Entity: February 2022  

Relation: organization dissolved  
Context: The environmental group, Green Future, announced its dissolution in October 2023 due to a lack of funding and support.  
Head Entity: Green Future  
Tail Entity: October 2023  
#### organization founded by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded by" means an organization was found by a person.
Here is an example:
Relation: organization founded by
Context: `` i have no greater obligation than to ensure the safely of airline travelers in this country , '' transportation secretary ray lahood said in a joint statement with j. randolph babbitt , administrator of the federal aviation administration , that was issued on the eve of a senate hearing on aviation safety .
Head Entity: federal aviation administration
Tail Entity: j. randolph babbitt
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded by":
Relation: organization founded by  
Context: In 1975, the renowned physicist and entrepreneur, Dr. John Smith, established Quantum Innovations, a company dedicated to advancing quantum computing technologies.  
Head Entity: Quantum Innovations  
Tail Entity: Dr. John Smith  

Relation: organization founded by  
Context: The charity organization, Hope for Tomorrow, was created in 2010 by the famous actress and philanthropist, Emily Johnson, to support underprivileged children.  
Head Entity: Hope for Tomorrow  
Tail Entity: Emily Johnson  

Relation: organization founded by  
Context: In the early 2000s, the tech startup, GreenTech Solutions, was founded by environmentalist and engineer, Mark Thompson, to develop sustainable energy solutions.  
Head Entity: GreenTech Solutions  
Tail Entity: Mark Thompson  

Relation: organization founded by  
Context: The global non-profit organization, Clean Oceans Initiative, was established in 2015 by marine biologist, Dr. Sarah Lee, to combat ocean pollution.  
Head Entity: Clean Oceans Initiative  
Tail Entity: Dr. Sarah Lee  

Relation: organization founded by  
Context: The innovative design firm, Creative Minds Studio, was launched in 2018 by renowned architect, Lisa Chen, to revolutionize urban architecture.  
Head Entity: Creative Minds Studio  
Tail Entity: Lisa Chen  
Losses:  13.686050415039062 1.609832525253296 8.166160583496094
MemoryTrain:  epoch  0, batch     0 | loss: 13.6860504Losses:  11.935433387756348 1.0935572385787964 5.672869682312012
MemoryTrain:  epoch  0, batch     1 | loss: 11.9354334Losses:  17.124414443969727 2.2076210975646973 10.857464790344238
MemoryTrain:  epoch  0, batch     2 | loss: 17.1244144Losses:  11.721755027770996 1.0235626697540283 5.651154518127441
MemoryTrain:  epoch  0, batch     3 | loss: 11.7217550Losses:  15.444371223449707 2.8596086502075195 8.12122631072998
MemoryTrain:  epoch  0, batch     4 | loss: 15.4443712Losses:  16.837209701538086 1.0277092456817627 10.860604286193848
MemoryTrain:  epoch  0, batch     5 | loss: 16.8372097Losses:  14.171391487121582 1.3256980180740356 8.137288093566895
MemoryTrain:  epoch  1, batch     0 | loss: 14.1713915Losses:  11.739843368530273 2.2480735778808594 5.643487453460693
MemoryTrain:  epoch  1, batch     1 | loss: 11.7398434Losses:  15.96125316619873 2.3738794326782227 8.142765998840332
MemoryTrain:  epoch  1, batch     2 | loss: 15.9612532Losses:  11.62483024597168 2.247140884399414 5.605943202972412
MemoryTrain:  epoch  1, batch     3 | loss: 11.6248302Losses:  11.151721954345703 2.388166904449463 5.586023330688477
MemoryTrain:  epoch  1, batch     4 | loss: 11.1517220Losses:  11.22130298614502 1.1519043445587158 5.616312026977539
MemoryTrain:  epoch  1, batch     5 | loss: 11.2213030Losses:  15.701684951782227 1.8965121507644653 10.811135292053223
MemoryTrain:  epoch  2, batch     0 | loss: 15.7016850Losses:  10.440080642700195 1.1090400218963623 5.644440650939941
MemoryTrain:  epoch  2, batch     1 | loss: 10.4400806Losses:  11.327177047729492 1.3516592979431152 5.621960639953613
MemoryTrain:  epoch  2, batch     2 | loss: 11.3271770Losses:  12.714956283569336 2.5051090717315674 5.573999881744385
MemoryTrain:  epoch  2, batch     3 | loss: 12.7149563Losses:  15.893621444702148 1.4047574996948242 10.805871963500977
MemoryTrain:  epoch  2, batch     4 | loss: 15.8936214Losses:  12.618647575378418 1.3226820230484009 8.14927864074707
MemoryTrain:  epoch  2, batch     5 | loss: 12.6186476Losses:  9.725278854370117 0.7944872379302979 5.614481449127197
MemoryTrain:  epoch  3, batch     0 | loss: 9.7252789Losses:  12.690500259399414 0.9945266246795654 8.130964279174805
MemoryTrain:  epoch  3, batch     1 | loss: 12.6905003Losses:  13.0266695022583 1.39322829246521 8.08359146118164
MemoryTrain:  epoch  3, batch     2 | loss: 13.0266695Losses:  12.13177490234375 1.251771092414856 8.093925476074219
MemoryTrain:  epoch  3, batch     3 | loss: 12.1317749Losses:  10.116787910461426 2.4455785751342773 3.326655626296997
MemoryTrain:  epoch  3, batch     4 | loss: 10.1167879Losses:  10.459616661071777 2.0775065422058105 5.614489555358887
MemoryTrain:  epoch  3, batch     5 | loss: 10.4596167Losses:  15.286513328552246 1.3317503929138184 10.773394584655762
MemoryTrain:  epoch  4, batch     0 | loss: 15.2865133Losses:  13.198986053466797 1.9308576583862305 8.096981048583984
MemoryTrain:  epoch  4, batch     1 | loss: 13.1989861Losses:  10.972992897033691 2.2125065326690674 5.58088493347168
MemoryTrain:  epoch  4, batch     2 | loss: 10.9729929Losses:  10.24031925201416 1.2632029056549072 5.568702697753906
MemoryTrain:  epoch  4, batch     3 | loss: 10.2403193Losses:  13.110626220703125 2.3665919303894043 8.06654167175293
MemoryTrain:  epoch  4, batch     4 | loss: 13.1106262Losses:  10.223291397094727 2.083820343017578 3.3361306190490723
MemoryTrain:  epoch  4, batch     5 | loss: 10.2232914Losses:  14.304616928100586 0.5675457715988159 10.770624160766602
MemoryTrain:  epoch  5, batch     0 | loss: 14.3046169Losses:  12.360845565795898 1.3336224555969238 8.094160079956055
MemoryTrain:  epoch  5, batch     1 | loss: 12.3608456Losses:  14.859697341918945 3.347527265548706 8.076091766357422
MemoryTrain:  epoch  5, batch     2 | loss: 14.8596973Losses:  15.021028518676758 2.0210299491882324 10.78398609161377
MemoryTrain:  epoch  5, batch     3 | loss: 15.0210285Losses:  8.710187911987305 1.5497875213623047 3.339507579803467
MemoryTrain:  epoch  5, batch     4 | loss: 8.7101879Losses:  11.800603866577148 2.239938974380493 5.620997905731201
MemoryTrain:  epoch  5, batch     5 | loss: 11.8006039Losses:  9.785320281982422 0.7872008085250854 5.565503120422363
MemoryTrain:  epoch  6, batch     0 | loss: 9.7853203Losses:  15.31795883178711 1.6113497018814087 10.780008316040039
MemoryTrain:  epoch  6, batch     1 | loss: 15.3179588Losses:  13.500173568725586 4.951180458068848 5.570502758026123
MemoryTrain:  epoch  6, batch     2 | loss: 13.5001736Losses:  10.024776458740234 1.387207269668579 5.572342395782471
MemoryTrain:  epoch  6, batch     3 | loss: 10.0247765Losses:  12.57634162902832 1.8860187530517578 8.075040817260742
MemoryTrain:  epoch  6, batch     4 | loss: 12.5763416Losses:  15.333513259887695 1.34912109375 10.778955459594727
MemoryTrain:  epoch  6, batch     5 | loss: 15.3335133Losses:  12.484330177307129 1.7715222835540771 8.09397029876709
MemoryTrain:  epoch  7, batch     0 | loss: 12.4843302Losses:  9.174972534179688 2.265626907348633 3.301393508911133
MemoryTrain:  epoch  7, batch     1 | loss: 9.1749725Losses:  12.713589668273926 1.978187084197998 8.065138816833496
MemoryTrain:  epoch  7, batch     2 | loss: 12.7135897Losses:  14.071919441223145 2.4442410469055176 8.090461730957031
MemoryTrain:  epoch  7, batch     3 | loss: 14.0719194Losses:  14.51571273803711 1.2402909994125366 10.78577995300293
MemoryTrain:  epoch  7, batch     4 | loss: 14.5157127Losses:  10.225107192993164 1.6440478563308716 5.565523624420166
MemoryTrain:  epoch  7, batch     5 | loss: 10.2251072Losses:  8.263185501098633 1.536250114440918 3.3268232345581055
MemoryTrain:  epoch  8, batch     0 | loss: 8.2631855Losses:  15.683748245239258 2.4057066440582275 10.773974418640137
MemoryTrain:  epoch  8, batch     1 | loss: 15.6837482Losses:  8.130027770996094 2.551358699798584 3.3080878257751465
MemoryTrain:  epoch  8, batch     2 | loss: 8.1300278Losses:  10.903061866760254 2.8629202842712402 5.5541768074035645
MemoryTrain:  epoch  8, batch     3 | loss: 10.9030619Losses:  9.829736709594727 2.1747584342956543 5.572021007537842
MemoryTrain:  epoch  8, batch     4 | loss: 9.8297367Losses:  9.811967849731445 1.0282646417617798 5.554692268371582
MemoryTrain:  epoch  8, batch     5 | loss: 9.8119678Losses:  13.173137664794922 1.6395912170410156 8.080037117004395
MemoryTrain:  epoch  9, batch     0 | loss: 13.1731377Losses:  10.597777366638184 2.79622483253479 5.576204299926758
MemoryTrain:  epoch  9, batch     1 | loss: 10.5977774Losses:  12.028512954711914 1.5878254175186157 8.078741073608398
MemoryTrain:  epoch  9, batch     2 | loss: 12.0285130Losses:  11.888261795043945 1.5971496105194092 8.095699310302734
MemoryTrain:  epoch  9, batch     3 | loss: 11.8882618Losses:  6.249642848968506 2.6111559867858887 1.3921045064926147
MemoryTrain:  epoch  9, batch     4 | loss: 6.2496428Losses:  11.859865188598633 1.5724084377288818 8.115789413452148
MemoryTrain:  epoch  9, batch     5 | loss: 11.8598652
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 67.19%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 60.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 54.46%   [EVAL] batch:    7 | acc: 18.75%,  total acc: 50.00%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 48.61%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 47.50%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 43.75%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 45.31%   [EVAL] batch:   12 | acc: 31.25%,  total acc: 44.23%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 44.64%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 46.25%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 48.16%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 48.96%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 49.34%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 51.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 53.57%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 55.68%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 57.61%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 61.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 63.66%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 68.94%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 68.57%   [EVAL] batch:   35 | acc: 50.00%,  total acc: 68.06%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 68.07%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 67.76%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 68.11%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 68.91%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 68.45%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 69.05%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.11%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 71.74%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 73.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 74.14%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 74.28%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 74.41%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 74.19%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 74.20%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 74.11%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 73.68%   
cur_acc:  ['0.8655', '0.7257', '0.6719']
his_acc:  ['0.8655', '0.8300', '0.7368']
Clustering into  9  clusters
Clusters:  [1 0 3 1 0 1 1 8 1 0 7 1 4 5 2 0 2 6 1 3 0]
Losses:  17.963788986206055 8.389276504516602 5.740142345428467
CurrentTrain: epoch  0, batch     0 | loss: 17.9637890Losses:  12.10628890991211 4.056584358215332 3.351484775543213
CurrentTrain: epoch  0, batch     1 | loss: 12.1062889Losses:  16.1860408782959 8.982036590576172 3.399531841278076
CurrentTrain: epoch  1, batch     0 | loss: 16.1860409Losses:  12.821830749511719 5.976309776306152 3.4090542793273926
CurrentTrain: epoch  1, batch     1 | loss: 12.8218307Losses:  16.261550903320312 7.331945419311523 5.70041036605835
CurrentTrain: epoch  2, batch     0 | loss: 16.2615509Losses:  10.484883308410645 2.3531172275543213 5.644996166229248
CurrentTrain: epoch  2, batch     1 | loss: 10.4848833Losses:  14.888647079467773 6.313755989074707 5.677378177642822
CurrentTrain: epoch  3, batch     0 | loss: 14.8886471Losses:  10.23324966430664 1.9340795278549194 5.626507759094238
CurrentTrain: epoch  3, batch     1 | loss: 10.2332497Losses:  13.872533798217773 5.846927642822266 5.633574962615967
CurrentTrain: epoch  4, batch     0 | loss: 13.8725338Losses:  10.262775421142578 2.0353612899780273 5.623398780822754
CurrentTrain: epoch  4, batch     1 | loss: 10.2627754Losses:  13.867219924926758 5.9633097648620605 5.614387512207031
CurrentTrain: epoch  5, batch     0 | loss: 13.8672199Losses:  9.556525230407715 1.7782045602798462 5.611780643463135
CurrentTrain: epoch  5, batch     1 | loss: 9.5565252Losses:  12.991750717163086 5.290793418884277 5.593725204467773
CurrentTrain: epoch  6, batch     0 | loss: 12.9917507Losses:  8.775619506835938 1.1592843532562256 5.576397895812988
CurrentTrain: epoch  6, batch     1 | loss: 8.7756195Losses:  14.393976211547852 6.7603559494018555 5.586843967437744
CurrentTrain: epoch  7, batch     0 | loss: 14.3939762Losses:  7.842136383056641 2.5356264114379883 3.327960252761841
CurrentTrain: epoch  7, batch     1 | loss: 7.8421364Losses:  14.088705062866211 6.553224563598633 5.570416450500488
CurrentTrain: epoch  8, batch     0 | loss: 14.0887051Losses:  7.644607067108154 2.472513437271118 3.3431801795959473
CurrentTrain: epoch  8, batch     1 | loss: 7.6446071Losses:  13.718313217163086 6.346441268920898 5.571115970611572
CurrentTrain: epoch  9, batch     0 | loss: 13.7183132Losses:  10.437829971313477 2.994018077850342 5.5624518394470215
CurrentTrain: epoch  9, batch     1 | loss: 10.4378300
#### organization founded ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization founded" means an organization was found in a date.
Here is an example:
Relation: organization founded
Context: pandit worked at the brokerage morgan stanley for about 11 years until 2005 , when he and some morgan stanley colleagues quit and later founded the hedge fund old lane partners .
Head Entity: old lane partners
Tail Entity: 2005
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization founded":
1. Relation: organization founded  
   Context: In 1998, a group of engineers and entrepreneurs came together to establish the tech startup, innovative solutions, which has since become a leader in software development.  
   Head Entity: innovative solutions  
   Tail Entity: 1998  

2. Relation: organization founded  
   Context: After years of research and development, the non-profit organization green earth initiative was officially launched in 2010 to promote environmental sustainability.  
   Head Entity: green earth initiative  
   Tail Entity: 2010  

3. Relation: organization founded  
   Context: The famous chef opened his first restaurant, culinary delights, in 2001, which quickly gained popularity and led to the establishment of several more locations.  
   Head Entity: culinary delights  
   Tail Entity: 2001  

4. Relation: organization founded  
   Context: In 2015, a group of activists founded the social justice organization voices for change to address systemic inequalities in their community.  
   Head Entity: voices for change  
   Tail Entity: 2015  

5. Relation: organization founded  
   Context: The tech giant, future tech, was founded in 2020 by a team of visionary developers aiming to revolutionize artificial intelligence.  
   Head Entity: future tech  
   Tail Entity: 2020  
#### person age ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person age" means a person 's age is a number.
Here is an example:
Relation: person age
Context: susan boyle is 48 years old now .
Head Entity: susan boyle
Tail Entity: 48
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person age":
Relation: person age  
Context: john is celebrating his 30th birthday today.  
Head Entity: john  
Tail Entity: 30  

Relation: person age  
Context: the famous actor, robert downey jr., turned 56 last week.  
Head Entity: robert downey jr.  
Tail Entity: 56  

Relation: person age  
Context: my grandmother is 75 years old and still very active.  
Head Entity: my grandmother  
Tail Entity: 75  

Relation: person age  
Context: at the age of 22, emily graduated from college.  
Head Entity: emily  
Tail Entity: 22  

Relation: person age  
Context: the youngest player on the team is only 19 years old.  
Head Entity: the youngest player  
Tail Entity: 19  
#### person city of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of birth" means a person was born in a city.
Here is an example:
Relation: person city of birth
Context: rothman was born in san francisco in 1932 in an orthodox jewish family .
Head Entity: rothman
Tail Entity: san francisco
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of birth":
Relation: person city of birth  
Context: elena was born in barcelona and spent her childhood there before moving to madrid.  
Head Entity: elena  
Tail Entity: barcelona  

Relation: person city of birth  
Context: during the summer of 1985, john was born in new orleans, a city known for its vibrant culture.  
Head Entity: john  
Tail Entity: new orleans  

Relation: person city of birth  
Context: after years of research, we discovered that the famous artist was born in amsterdam in the late 19th century.  
Head Entity: the famous artist  
Tail Entity: amsterdam  

Relation: person city of birth  
Context: in a small town in italy, giovanni was born in florence, where he later became a renowned chef.  
Head Entity: giovanni  
Tail Entity: florence  

Relation: person city of birth  
Context: the documentary revealed that the scientist was born in tokyo, which greatly influenced his work.  
Head Entity: the scientist  
Tail Entity: tokyo  
#### organization members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization members" means an organization 's member is an organization.
Here is an example:
Relation: organization members
Context: sun plays for the grand rapids flight of the international basketball league after toiling for the maryland nighthawks of the american basketball association , both development leagues for those who dream of an nba career .
Head Entity: american basketball association
Tail Entity: maryland nighthawks
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization members":
Relation: organization members  
Context: The New York Philharmonic is one of the oldest orchestras in the United States, and it has had many notable musicians, including members from the London Symphony Orchestra.  
Head Entity: London Symphony Orchestra  
Tail Entity: New York Philharmonic  

Relation: organization members  
Context: The National Football League has several teams, including the Dallas Cowboys, which is known for its iconic cheerleaders who are members of the organization.  
Head Entity: Dallas Cowboys  
Tail Entity: NFL Cheerleaders  

Relation: organization members  
Context: The United Nations consists of various specialized agencies, including the World Health Organization, which plays a crucial role in global health initiatives.  
Head Entity: World Health Organization  
Tail Entity: United Nations  

Relation: organization members  
Context: The International Olympic Committee oversees the Olympic Games and has numerous national committees, such as the United States Olympic and Paralympic Committee, which is responsible for American athletes.  
Head Entity: United States Olympic and Paralympic Committee  
Tail Entity: International Olympic Committee  

Relation: organization members  
Context: The American Bar Association has various sections and committees, including the Section of Litigation, which focuses on the interests of litigators across the country.  
Head Entity: Section of Litigation  
Tail Entity: American Bar Association  
#### person religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person religion" means an person is the member of a religion.
Here is an example:
Relation: person religion
Context: devout catholic miss boyle has been at the centre of a media storm since news emerged of how she wowed britain 's got talent judges simon cowell and piers morgan with her amazing opera voice .
Head Entity: boyle
Tail Entity: catholic
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person religion":
Relation: person religion  
Context: After years of dedication to her faith, Maria finally decided to become a nun, embracing her identity as a devoted member of the Catholic Church.  
Head Entity: Maria  
Tail Entity: Catholic Church  

Relation: person religion  
Context: The famous actor, known for his philanthropic work, often speaks about his deep connection to Buddhism and how it influences his life choices.  
Head Entity: actor  
Tail Entity: Buddhism  

Relation: person religion  
Context: Growing up in a Jewish household, David has always celebrated the traditions of his ancestors, feeling a strong bond with his Jewish heritage.  
Head Entity: David  
Tail Entity: Jewish  

Relation: person religion  
Context: As a prominent leader in the community, Imam Ali often shares his insights on Islam, encouraging others to explore their faith and its teachings.  
Head Entity: Imam Ali  
Tail Entity: Islam  

Relation: person religion  
Context: The renowned author frequently discusses her experiences as a practicing Hindu, highlighting the importance of spirituality in her writing.  
Head Entity: author  
Tail Entity: Hindu  
Losses:  15.720402717590332 0.8464958667755127 10.963549613952637
MemoryTrain:  epoch  0, batch     0 | loss: 15.7204027Losses:  14.254350662231445 0.758395791053772 8.416266441345215
MemoryTrain:  epoch  0, batch     1 | loss: 14.2543507Losses:  11.097068786621094 0.7797243595123291 5.575681686401367
MemoryTrain:  epoch  0, batch     2 | loss: 11.0970688Losses:  17.074649810791016 1.6487798690795898 10.96595573425293
MemoryTrain:  epoch  0, batch     3 | loss: 17.0746498Losses:  12.489686965942383 0.4979647397994995 8.098684310913086
MemoryTrain:  epoch  0, batch     4 | loss: 12.4896870Losses:  17.145835876464844 1.1568560600280762 10.905915260314941
MemoryTrain:  epoch  0, batch     5 | loss: 17.1458359Losses:  20.093570709228516 0.7264499664306641 13.787871360778809
MemoryTrain:  epoch  0, batch     6 | loss: 20.0935707Losses:  16.745635986328125 0.5030649304389954 10.95769214630127
MemoryTrain:  epoch  0, batch     7 | loss: 16.7456360Losses:  13.099802017211914 1.1015470027923584 8.09096622467041
MemoryTrain:  epoch  1, batch     0 | loss: 13.0998020Losses:  19.340484619140625 1.0382879972457886 13.7330322265625
MemoryTrain:  epoch  1, batch     1 | loss: 19.3404846Losses:  13.536678314208984 0.8672578930854797 8.107416152954102
MemoryTrain:  epoch  1, batch     2 | loss: 13.5366783Losses:  11.356268882751465 1.0165371894836426 5.5726189613342285
MemoryTrain:  epoch  1, batch     3 | loss: 11.3562689Losses:  16.526439666748047 1.972263216972351 11.024785041809082
MemoryTrain:  epoch  1, batch     4 | loss: 16.5264397Losses:  12.760885238647461 1.375055193901062 8.172248840332031
MemoryTrain:  epoch  1, batch     5 | loss: 12.7608852Losses:  13.244515419006348 0.7239064574241638 8.08082389831543
MemoryTrain:  epoch  1, batch     6 | loss: 13.2445154Losses:  14.143138885498047 0.8093172311782837 8.29287338256836
MemoryTrain:  epoch  1, batch     7 | loss: 14.1431389Losses:  16.10690689086914 1.3727264404296875 10.877808570861816
MemoryTrain:  epoch  2, batch     0 | loss: 16.1069069Losses:  17.80779266357422 0.845808744430542 13.754261016845703
MemoryTrain:  epoch  2, batch     1 | loss: 17.8077927Losses:  15.435413360595703 1.3129488229751587 10.83781909942627
MemoryTrain:  epoch  2, batch     2 | loss: 15.4354134Losses:  13.937765121459961 1.5517313480377197 8.109329223632812
MemoryTrain:  epoch  2, batch     3 | loss: 13.9377651Losses:  17.418493270874023 1.804714560508728 10.80724048614502
MemoryTrain:  epoch  2, batch     4 | loss: 17.4184933Losses:  15.953904151916504 1.7649667263031006 10.82164478302002
MemoryTrain:  epoch  2, batch     5 | loss: 15.9539042Losses:  15.106159210205078 0.543971598148346 10.799120903015137
MemoryTrain:  epoch  2, batch     6 | loss: 15.1061592Losses:  11.19064712524414 0.4997381269931793 8.163890838623047
MemoryTrain:  epoch  2, batch     7 | loss: 11.1906471Losses:  11.46392822265625 0.7831644415855408 8.125764846801758
MemoryTrain:  epoch  3, batch     0 | loss: 11.4639282Losses:  12.558554649353027 1.3824299573898315 8.076117515563965
MemoryTrain:  epoch  3, batch     1 | loss: 12.5585546Losses:  14.522222518920898 0.23371660709381104 10.820879936218262
MemoryTrain:  epoch  3, batch     2 | loss: 14.5222225Losses:  15.41801643371582 1.1786738634109497 10.809321403503418
MemoryTrain:  epoch  3, batch     3 | loss: 15.4180164Losses:  14.125368118286133 0.5191059112548828 10.82178783416748
MemoryTrain:  epoch  3, batch     4 | loss: 14.1253681Losses:  14.881585121154785 1.1503193378448486 10.867823600769043
MemoryTrain:  epoch  3, batch     5 | loss: 14.8815851Losses:  12.721254348754883 1.1228580474853516 8.063491821289062
MemoryTrain:  epoch  3, batch     6 | loss: 12.7212543Losses:  11.00810432434082 1.2693886756896973 5.6180596351623535
MemoryTrain:  epoch  3, batch     7 | loss: 11.0081043Losses:  14.197210311889648 1.0590336322784424 10.801112174987793
MemoryTrain:  epoch  4, batch     0 | loss: 14.1972103Losses:  10.204912185668945 1.9236805438995361 5.613116264343262
MemoryTrain:  epoch  4, batch     1 | loss: 10.2049122Losses:  18.077491760253906 0.846595287322998 13.772293090820312
MemoryTrain:  epoch  4, batch     2 | loss: 18.0774918Losses:  12.45800495147705 0.9789488911628723 8.102106094360352
MemoryTrain:  epoch  4, batch     3 | loss: 12.4580050Losses:  11.814543724060059 1.0468798875808716 8.084211349487305
MemoryTrain:  epoch  4, batch     4 | loss: 11.8145437Losses:  9.960315704345703 1.8614803552627563 5.561996936798096
MemoryTrain:  epoch  4, batch     5 | loss: 9.9603157Losses:  14.381409645080566 0.7821541428565979 10.79882526397705
MemoryTrain:  epoch  4, batch     6 | loss: 14.3814096Losses:  15.953859329223633 1.4439109563827515 10.841708183288574
MemoryTrain:  epoch  4, batch     7 | loss: 15.9538593Losses:  12.575092315673828 1.47658371925354 8.078311920166016
MemoryTrain:  epoch  5, batch     0 | loss: 12.5750923Losses:  12.502472877502441 1.2946579456329346 8.188593864440918
MemoryTrain:  epoch  5, batch     1 | loss: 12.5024729Losses:  11.725183486938477 0.5498151183128357 8.111493110656738
MemoryTrain:  epoch  5, batch     2 | loss: 11.7251835Losses:  12.050690650939941 1.312023401260376 8.120889663696289
MemoryTrain:  epoch  5, batch     3 | loss: 12.0506907Losses:  18.56494140625 1.5959863662719727 13.682411193847656
MemoryTrain:  epoch  5, batch     4 | loss: 18.5649414Losses:  14.974573135375977 1.3682832717895508 10.790709495544434
MemoryTrain:  epoch  5, batch     5 | loss: 14.9745731Losses:  14.778186798095703 1.8750121593475342 10.783770561218262
MemoryTrain:  epoch  5, batch     6 | loss: 14.7781868Losses:  6.026626110076904 0.7391830682754517 3.3009819984436035
MemoryTrain:  epoch  5, batch     7 | loss: 6.0266261Losses:  12.943294525146484 1.638183355331421 8.111647605895996
MemoryTrain:  epoch  6, batch     0 | loss: 12.9432945Losses:  14.391250610351562 0.48545801639556885 10.791437149047852
MemoryTrain:  epoch  6, batch     1 | loss: 14.3912506Losses:  14.278974533081055 0.8182510137557983 10.815802574157715
MemoryTrain:  epoch  6, batch     2 | loss: 14.2789745Losses:  17.388341903686523 1.0716047286987305 13.698427200317383
MemoryTrain:  epoch  6, batch     3 | loss: 17.3883419Losses:  8.656682968139648 1.0045559406280518 5.613763809204102
MemoryTrain:  epoch  6, batch     4 | loss: 8.6566830Losses:  16.35907745361328 4.3348774909973145 8.094910621643066
MemoryTrain:  epoch  6, batch     5 | loss: 16.3590775Losses:  8.855960845947266 1.346261739730835 5.558539867401123
MemoryTrain:  epoch  6, batch     6 | loss: 8.8559608Losses:  9.018909454345703 0.8164064288139343 5.636141300201416
MemoryTrain:  epoch  6, batch     7 | loss: 9.0189095Losses:  15.311023712158203 1.2797443866729736 10.773674964904785
MemoryTrain:  epoch  7, batch     0 | loss: 15.3110237Losses:  13.629026412963867 0.7935712337493896 10.807188987731934
MemoryTrain:  epoch  7, batch     1 | loss: 13.6290264Losses:  11.985363960266113 1.2564287185668945 8.090957641601562
MemoryTrain:  epoch  7, batch     2 | loss: 11.9853640Losses:  11.595867156982422 1.0966780185699463 8.12305736541748
MemoryTrain:  epoch  7, batch     3 | loss: 11.5958672Losses:  14.581510543823242 1.3679487705230713 10.784482955932617
MemoryTrain:  epoch  7, batch     4 | loss: 14.5815105Losses:  14.688462257385254 2.63230299949646 8.101146697998047
MemoryTrain:  epoch  7, batch     5 | loss: 14.6884623Losses:  12.4931640625 1.8964707851409912 8.116842269897461
MemoryTrain:  epoch  7, batch     6 | loss: 12.4931641Losses:  6.376054763793945 1.0453232526779175 3.3137760162353516
MemoryTrain:  epoch  7, batch     7 | loss: 6.3760548Losses:  12.445845603942871 2.305502414703369 8.080921173095703
MemoryTrain:  epoch  8, batch     0 | loss: 12.4458456Losses:  11.04935359954834 0.7105222940444946 8.074372291564941
MemoryTrain:  epoch  8, batch     1 | loss: 11.0493536Losses:  14.430736541748047 1.3001351356506348 10.797463417053223
MemoryTrain:  epoch  8, batch     2 | loss: 14.4307365Losses:  12.528346061706543 1.6482220888137817 8.143512725830078
MemoryTrain:  epoch  8, batch     3 | loss: 12.5283461Losses:  15.686563491821289 1.7031605243682861 10.834656715393066
MemoryTrain:  epoch  8, batch     4 | loss: 15.6865635Losses:  12.445263862609863 1.4105056524276733 8.09398365020752
MemoryTrain:  epoch  8, batch     5 | loss: 12.4452639Losses:  18.04669952392578 1.9440395832061768 13.704495429992676
MemoryTrain:  epoch  8, batch     6 | loss: 18.0466995Losses:  11.813679695129395 1.3807204961776733 8.078073501586914
MemoryTrain:  epoch  8, batch     7 | loss: 11.8136797Losses:  13.651304244995117 0.49117493629455566 10.81441593170166
MemoryTrain:  epoch  9, batch     0 | loss: 13.6513042Losses:  12.339066505432129 1.3139021396636963 8.0867919921875
MemoryTrain:  epoch  9, batch     1 | loss: 12.3390665Losses:  9.58613109588623 1.0017112493515015 5.566749095916748
MemoryTrain:  epoch  9, batch     2 | loss: 9.5861311Losses:  19.709388732910156 0.7109150290489197 16.67650032043457
MemoryTrain:  epoch  9, batch     3 | loss: 19.7093887Losses:  7.091693878173828 1.8797571659088135 3.317858934402466
MemoryTrain:  epoch  9, batch     4 | loss: 7.0916939Losses:  17.081945419311523 1.0058650970458984 13.681602478027344
MemoryTrain:  epoch  9, batch     5 | loss: 17.0819454Losses:  14.28466796875 1.1951290369033813 10.779735565185547
MemoryTrain:  epoch  9, batch     6 | loss: 14.2846680Losses:  9.197182655334473 1.0444989204406738 5.573692321777344
MemoryTrain:  epoch  9, batch     7 | loss: 9.1971827
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 96.88%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:    3 | acc: 100.00%,  total acc: 96.88%   [EVAL] batch:    4 | acc: 100.00%,  total acc: 97.50%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 97.92%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 98.21%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 98.44%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 97.92%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 93.75%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 90.87%   [EVAL] batch:   13 | acc: 62.50%,  total acc: 88.84%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 43.06%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 39.38%   [EVAL] batch:   10 | acc: 0.00%,  total acc: 35.80%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 34.90%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 32.69%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 33.04%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 35.00%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 36.33%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 38.60%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 39.93%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 40.79%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 43.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 45.83%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 48.30%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 50.54%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 52.34%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 54.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 56.01%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 57.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.34%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 61.46%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 62.88%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 61.03%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 59.46%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 57.99%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 56.42%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 54.93%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 54.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 55.47%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 55.03%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 55.95%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 56.98%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 57.95%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 58.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 59.78%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 60.64%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 61.46%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 62.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 63.36%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 63.70%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 64.15%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 65.23%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 65.85%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 66.34%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 66.92%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 67.27%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 68.34%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 68.85%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 70.29%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 70.36%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 70.24%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 70.22%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 70.89%   [EVAL] batch:   70 | acc: 37.50%,  total acc: 70.42%   
cur_acc:  ['0.8655', '0.7257', '0.6719', '0.8884']
his_acc:  ['0.8655', '0.8300', '0.7368', '0.7042']
Clustering into  12  clusters
Clusters:  [ 5  1  4  2  3  2  5  9  2  3  8  2 11  6  0  1  0 10  2  4  3  5  0  7
  3  3]
Losses:  16.37516975402832 6.965293884277344 3.3279004096984863
CurrentTrain: epoch  0, batch     0 | loss: 16.3751698Losses:  10.848026275634766 1.801544189453125 3.3276772499084473
CurrentTrain: epoch  0, batch     1 | loss: 10.8480263Losses:  16.064781188964844 7.406865119934082 3.3197126388549805
CurrentTrain: epoch  1, batch     0 | loss: 16.0647812Losses:  10.003986358642578 2.3410298824310303 3.319528102874756
CurrentTrain: epoch  1, batch     1 | loss: 10.0039864Losses:  15.670892715454102 8.036909103393555 3.337257146835327
CurrentTrain: epoch  2, batch     0 | loss: 15.6708927Losses:  10.708806037902832 3.1608903408050537 3.317218542098999
CurrentTrain: epoch  2, batch     1 | loss: 10.7088060Losses:  14.695220947265625 7.768279075622559 3.3397111892700195
CurrentTrain: epoch  3, batch     0 | loss: 14.6952209Losses:  12.09764289855957 4.271567344665527 3.323418617248535
CurrentTrain: epoch  3, batch     1 | loss: 12.0976429Losses:  13.03929328918457 6.431747913360596 3.3057451248168945
CurrentTrain: epoch  4, batch     0 | loss: 13.0392933Losses:  9.607922554016113 2.36129093170166 3.3464183807373047
CurrentTrain: epoch  4, batch     1 | loss: 9.6079226Losses:  15.03209114074707 7.987796306610107 3.3124966621398926
CurrentTrain: epoch  5, batch     0 | loss: 15.0320911Losses:  7.875435829162598 3.802985668182373 1.395150899887085
CurrentTrain: epoch  5, batch     1 | loss: 7.8754358Losses:  13.147977828979492 6.440611839294434 3.3071706295013428
CurrentTrain: epoch  6, batch     0 | loss: 13.1479778Losses:  8.118982315063477 1.9983749389648438 3.3036484718322754
CurrentTrain: epoch  6, batch     1 | loss: 8.1189823Losses:  12.683975219726562 6.6904616355896 3.306283950805664
CurrentTrain: epoch  7, batch     0 | loss: 12.6839752Losses:  9.198321342468262 2.679417610168457 3.312124490737915
CurrentTrain: epoch  7, batch     1 | loss: 9.1983213Losses:  12.373493194580078 6.785945892333984 3.3056864738464355
CurrentTrain: epoch  8, batch     0 | loss: 12.3734932Losses:  7.61027193069458 3.0534253120422363 1.3951181173324585
CurrentTrain: epoch  8, batch     1 | loss: 7.6102719Losses:  11.48232650756836 5.7762579917907715 3.304672956466675
CurrentTrain: epoch  9, batch     0 | loss: 11.4823265Losses:  7.898102283477783 1.8444159030914307 3.317416191101074
CurrentTrain: epoch  9, batch     1 | loss: 7.8981023
#### person stateorprovinces of residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovinces of residence" means a person is living in a state or province.
Here is an example:
Relation: person stateorprovinces of residence
Context: his early school years were spent in new jersey 's integrated schools .
Head Entity: his
Tail Entity: new jersey
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovinces of residence":
Relation: person stateorprovinces of residence  
Context: After moving to California, she quickly adapted to the sunny weather.  
Head Entity: she  
Tail Entity: California  

Relation: person stateorprovinces of residence  
Context: He was born and raised in Texas, where he developed a love for country music.  
Head Entity: He  
Tail Entity: Texas  

Relation: person stateorprovinces of residence  
Context: They decided to settle in Florida for its beautiful beaches and warm climate.  
Head Entity: They  
Tail Entity: Florida  

Relation: person stateorprovinces of residence  
Context: During his college years, he lived in Massachusetts, enjoying the vibrant student life.  
Head Entity: he  
Tail Entity: Massachusetts  

Relation: person stateorprovinces of residence  
Context: After years of living in New York, she finally moved to Oregon for a change of scenery.  
Head Entity: she  
Tail Entity: Oregon  
#### person date of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of death" means a person was died on a date.
Here is an example:
Relation: person date of death
Context: he passed away on saturday .
Head Entity: he
Tail Entity: saturday
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of death":
Relation: person date of death  
Context: The famous author died on July 4th, 1965.  
Head Entity: The famous author  
Tail Entity: July 4th, 1965  

Relation: person date of death  
Context: She left this world on her birthday, March 15th.  
Head Entity: She  
Tail Entity: March 15th  

Relation: person date of death  
Context: He was pronounced dead on the evening of December 31st.  
Head Entity: He  
Tail Entity: December 31st  

Relation: person date of death  
Context: The scientist passed away unexpectedly in the spring of 2020.  
Head Entity: The scientist  
Tail Entity: spring of 2020  

Relation: person date of death  
Context: They commemorated his life on the anniversary of his death, April 10th.  
Head Entity: They  
Tail Entity: April 10th  
#### organization number of employees members ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization number of employees members" means the number of an organization 's employees is a number.
Here is an example:
Relation: organization number of employees members
Context: covidien , which posted revenue of more than $ 10 billion last year , has about 42,000 employees worldwide .
Head Entity: covidien
Tail Entity: 42,000
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization number of employees members":
Relation: organization number of employees members  
Context: TechCorp, a leading software development company, boasts a workforce of approximately 5,500 employees across its global offices.  
Head Entity: TechCorp  
Tail Entity: 5,500  

Relation: organization number of employees members  
Context: GreenEarth, an environmental non-profit, has grown significantly and now employs around 1,200 dedicated staff members.  
Head Entity: GreenEarth  
Tail Entity: 1,200  

Relation: organization number of employees members  
Context: The multinational retail giant, ShopSmart, reported that it currently has over 300,000 employees working in various locations worldwide.  
Head Entity: ShopSmart  
Tail Entity: 300,000  

Relation: organization number of employees members  
Context: HealthPlus, a healthcare provider, has expanded its services and now employs nearly 8,000 healthcare professionals.  
Head Entity: HealthPlus  
Tail Entity: 8,000  

Relation: organization number of employees members  
Context: EduLearn Academy, known for its innovative teaching methods, has a staff of about 2,500 educators and administrative personnel.  
Head Entity: EduLearn Academy  
Tail Entity: 2,500  
#### person alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person alternate names" means a person 's alias is a person.
Here is an example:
Relation: person alternate names
Context: a judge in new york city said remy ma , whose real name is remy smith , said thursday that the hip-hopper could not leave the united states for a five-country european concert tour .
Head Entity: remy smith
Tail Entity: remy ma
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person alternate names":
Relation: person alternate names  
Context: The famous author Samuel Langhorne Clemens, better known by his pen name Mark Twain, is celebrated for his novels like "The Adventures of Tom Sawyer."  
Head Entity: Samuel Langhorne Clemens  
Tail Entity: Mark Twain  

Relation: person alternate names  
Context: The musician Stefani Joanne Angelina Germanotta, who is widely recognized as Lady Gaga, has made a significant impact on the pop music scene.  
Head Entity: Stefani Joanne Angelina Germanotta  
Tail Entity: Lady Gaga  

Relation: person alternate names  
Context: The actor and filmmaker known as Orson Welles was born George Orson Welles and is famous for his groundbreaking work in cinema.  
Head Entity: George Orson Welles  
Tail Entity: Orson Welles  

Relation: person alternate names  
Context: The renowned physicist Albert Einstein, who is often referred to simply as Einstein, developed the theory of relativity.  
Head Entity: Albert Einstein  
Tail Entity: Einstein  

Relation: person alternate names  
Context: The pop star Robyn Rihanna Fenty, better known as Rihanna, has won numerous awards for her music and philanthropy.  
Head Entity: Robyn Rihanna Fenty  
Tail Entity: Rihanna  
#### person spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person spouse" means a person 's spouse is a person.
Here is an example:
Relation: person spouse
Context: kerry katona feel out with arnold just before she married bryan mcfadden , kerry cancelled his wedding invitation and have n't spoken since .
Head Entity: bryan mcfadden
Tail Entity: kerry katona
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person spouse":
1. Relation: person spouse  
   Context: After years of dating, emma stone finally tied the knot with dave mccary in a beautiful ceremony last summer.  
   Head Entity: dave mccary  
   Tail Entity: emma stone  

2. Relation: person spouse  
   Context: During their time together, ben affleck and jennifer garner built a family, but they eventually decided to part ways amicably.  
   Head Entity: jennifer garner  
   Tail Entity: ben affleck  

3. Relation: person spouse  
   Context: The couple celebrated their 10th anniversary, reminiscing about the day when ryan reynolds and blake lively exchanged vows in a secret ceremony.  
   Head Entity: blake lively  
   Tail Entity: ryan reynolds  

4. Relation: person spouse  
   Context: After a whirlwind romance, prince harry and meghan markle announced their engagement, leading to a royal wedding that captivated the world.  
   Head Entity: meghan markle  
   Tail Entity: prince harry  

5. Relation: person spouse  
   Context: Following their engagement, mila kunis and ashton kutcher shared their joy with fans, revealing their plans for a cozy wedding.  
   Head Entity: ashton kutcher  
   Tail Entity: mila kunis  
Losses:  21.284265518188477 0.6273096799850464 16.99660873413086
MemoryTrain:  epoch  0, batch     0 | loss: 21.2842655Losses:  20.762832641601562 1.6335725784301758 16.87015151977539
MemoryTrain:  epoch  0, batch     1 | loss: 20.7628326Losses:  18.634971618652344 0.5462168455123901 13.835528373718262
MemoryTrain:  epoch  0, batch     2 | loss: 18.6349716Losses:  16.381654739379883 1.121069073677063 11.073869705200195
MemoryTrain:  epoch  0, batch     3 | loss: 16.3816547Losses:  17.238811492919922 1.220792293548584 13.721294403076172
MemoryTrain:  epoch  0, batch     4 | loss: 17.2388115Losses:  18.960895538330078 0.7607778906822205 13.816574096679688
MemoryTrain:  epoch  0, batch     5 | loss: 18.9608955Losses:  18.3299560546875 0.5554728507995605 13.712602615356445
MemoryTrain:  epoch  0, batch     6 | loss: 18.3299561Losses:  18.71243667602539 1.4147709608078003 13.74091911315918
MemoryTrain:  epoch  0, batch     7 | loss: 18.7124367Losses:  13.05941104888916 1.1239420175552368 8.14218521118164
MemoryTrain:  epoch  0, batch     8 | loss: 13.0594110Losses:  18.274751663208008 0.8921297788619995 13.873466491699219
MemoryTrain:  epoch  0, batch     9 | loss: 18.2747517Losses:  19.20083999633789 1.1035327911376953 13.831344604492188
MemoryTrain:  epoch  1, batch     0 | loss: 19.2008400Losses:  18.73256492614746 0.8278723955154419 13.706867218017578
MemoryTrain:  epoch  1, batch     1 | loss: 18.7325649Losses:  17.58208465576172 0.7482653856277466 13.815448760986328
MemoryTrain:  epoch  1, batch     2 | loss: 17.5820847Losses:  14.884688377380371 1.0729422569274902 10.882266998291016
MemoryTrain:  epoch  1, batch     3 | loss: 14.8846884Losses:  23.550640106201172 0.49336111545562744 20.09531593322754
MemoryTrain:  epoch  1, batch     4 | loss: 23.5506401Losses:  23.314558029174805 0.7755935192108154 19.89072608947754
MemoryTrain:  epoch  1, batch     5 | loss: 23.3145580Losses:  18.174015045166016 1.6614844799041748 13.708211898803711
MemoryTrain:  epoch  1, batch     6 | loss: 18.1740150Losses:  10.279123306274414 1.6290817260742188 5.604036808013916
MemoryTrain:  epoch  1, batch     7 | loss: 10.2791233Losses:  23.738933563232422 0.8160881996154785 20.01352882385254
MemoryTrain:  epoch  1, batch     8 | loss: 23.7389336Losses:  14.908571243286133 1.4209527969360352 10.94143295288086
MemoryTrain:  epoch  1, batch     9 | loss: 14.9085712Losses:  11.584456443786621 1.0180251598358154 8.13267993927002
MemoryTrain:  epoch  2, batch     0 | loss: 11.5844564Losses:  17.190547943115234 0.547925591468811 13.71251392364502
MemoryTrain:  epoch  2, batch     1 | loss: 17.1905479Losses:  17.56045150756836 1.3860671520233154 13.753673553466797
MemoryTrain:  epoch  2, batch     2 | loss: 17.5604515Losses:  23.439861297607422 1.047242283821106 19.889238357543945
MemoryTrain:  epoch  2, batch     3 | loss: 23.4398613Losses:  15.385812759399414 2.1965599060058594 10.796683311462402
MemoryTrain:  epoch  2, batch     4 | loss: 15.3858128Losses:  20.335405349731445 1.3411086797714233 16.710407257080078
MemoryTrain:  epoch  2, batch     5 | loss: 20.3354053Losses:  14.562274932861328 1.1057969331741333 10.887053489685059
MemoryTrain:  epoch  2, batch     6 | loss: 14.5622749Losses:  9.327417373657227 1.0940684080123901 5.606315612792969
MemoryTrain:  epoch  2, batch     7 | loss: 9.3274174Losses:  17.913166046142578 1.409674882888794 13.719789505004883
MemoryTrain:  epoch  2, batch     8 | loss: 17.9131660Losses:  19.264469146728516 0.5462400913238525 16.683441162109375
MemoryTrain:  epoch  2, batch     9 | loss: 19.2644691Losses:  22.853256225585938 0.5125522613525391 19.894271850585938
MemoryTrain:  epoch  3, batch     0 | loss: 22.8532562Losses:  13.936241149902344 0.5237411260604858 10.835627555847168
MemoryTrain:  epoch  3, batch     1 | loss: 13.9362411Losses:  23.422988891601562 1.14497971534729 19.869586944580078
MemoryTrain:  epoch  3, batch     2 | loss: 23.4229889Losses:  14.64210319519043 1.7448790073394775 10.813050270080566
MemoryTrain:  epoch  3, batch     3 | loss: 14.6421032Losses:  14.254084587097168 1.082963228225708 10.836966514587402
MemoryTrain:  epoch  3, batch     4 | loss: 14.2540846Losses:  17.34459114074707 1.3214341402053833 13.74749755859375
MemoryTrain:  epoch  3, batch     5 | loss: 17.3445911Losses:  19.817413330078125 0.752636194229126 16.742971420288086
MemoryTrain:  epoch  3, batch     6 | loss: 19.8174133Losses:  16.57339096069336 0.8366382122039795 13.693723678588867
MemoryTrain:  epoch  3, batch     7 | loss: 16.5733910Losses:  17.149681091308594 1.1046147346496582 13.728469848632812
MemoryTrain:  epoch  3, batch     8 | loss: 17.1496811Losses:  13.401464462280273 0.5642116069793701 10.790884971618652
MemoryTrain:  epoch  3, batch     9 | loss: 13.4014645Losses:  20.35845947265625 1.5465080738067627 16.71839141845703
MemoryTrain:  epoch  4, batch     0 | loss: 20.3584595Losses:  14.31035327911377 1.405392050743103 10.83504867553711
MemoryTrain:  epoch  4, batch     1 | loss: 14.3103533Losses:  22.804174423217773 0.7729129791259766 19.88592529296875
MemoryTrain:  epoch  4, batch     2 | loss: 22.8041744Losses:  19.983924865722656 1.0661898851394653 16.767404556274414
MemoryTrain:  epoch  4, batch     3 | loss: 19.9839249Losses:  16.278274536132812 0.5135451555252075 13.695422172546387
MemoryTrain:  epoch  4, batch     4 | loss: 16.2782745Losses:  11.59522533416748 1.4144361019134521 8.068238258361816
MemoryTrain:  epoch  4, batch     5 | loss: 11.5952253Losses:  20.361120223999023 1.5846972465515137 16.784698486328125
MemoryTrain:  epoch  4, batch     6 | loss: 20.3611202Losses:  17.030258178710938 1.2764160633087158 13.752199172973633
MemoryTrain:  epoch  4, batch     7 | loss: 17.0302582Losses:  25.636272430419922 0.4747838079929352 23.148122787475586
MemoryTrain:  epoch  4, batch     8 | loss: 25.6362724Losses:  22.014150619506836 0.2725909948348999 19.847108840942383
MemoryTrain:  epoch  4, batch     9 | loss: 22.0141506Losses:  16.238420486450195 0.5102318525314331 13.670975685119629
MemoryTrain:  epoch  5, batch     0 | loss: 16.2384205Losses:  14.614251136779785 1.829990267753601 10.801034927368164
MemoryTrain:  epoch  5, batch     1 | loss: 14.6142511Losses:  16.860084533691406 1.1143958568572998 13.689056396484375
MemoryTrain:  epoch  5, batch     2 | loss: 16.8600845Losses:  16.438602447509766 0.7650734186172485 13.703582763671875
MemoryTrain:  epoch  5, batch     3 | loss: 16.4386024Losses:  16.562944412231445 0.9412294626235962 13.685883522033691
MemoryTrain:  epoch  5, batch     4 | loss: 16.5629444Losses:  22.631576538085938 0.7545639276504517 19.858243942260742
MemoryTrain:  epoch  5, batch     5 | loss: 22.6315765Losses:  19.19842529296875 0.5174407958984375 16.759260177612305
MemoryTrain:  epoch  5, batch     6 | loss: 19.1984253Losses:  11.768239974975586 1.7008957862854004 8.090083122253418
MemoryTrain:  epoch  5, batch     7 | loss: 11.7682400Losses:  22.96451759338379 1.0663906335830688 19.861164093017578
MemoryTrain:  epoch  5, batch     8 | loss: 22.9645176Losses:  16.321407318115234 0.6255879998207092 13.70517349243164
MemoryTrain:  epoch  5, batch     9 | loss: 16.3214073Losses:  16.922893524169922 1.325939655303955 13.66474723815918
MemoryTrain:  epoch  6, batch     0 | loss: 16.9228935Losses:  16.61151123046875 1.0162889957427979 13.662717819213867
MemoryTrain:  epoch  6, batch     1 | loss: 16.6115112Losses:  17.199901580810547 1.6368417739868164 13.646186828613281
MemoryTrain:  epoch  6, batch     2 | loss: 17.1999016Losses:  19.769695281982422 1.1265478134155273 16.705095291137695
MemoryTrain:  epoch  6, batch     3 | loss: 19.7696953Losses:  14.241557121276855 1.389965295791626 10.824433326721191
MemoryTrain:  epoch  6, batch     4 | loss: 14.2415571Losses:  19.397167205810547 0.735377311706543 16.707862854003906
MemoryTrain:  epoch  6, batch     5 | loss: 19.3971672Losses:  16.56170082092285 0.9797074794769287 13.664620399475098
MemoryTrain:  epoch  6, batch     6 | loss: 16.5617008Losses:  19.05813980102539 0.4631238579750061 16.68636703491211
MemoryTrain:  epoch  6, batch     7 | loss: 19.0581398Losses:  14.648855209350586 1.8464841842651367 10.870169639587402
MemoryTrain:  epoch  6, batch     8 | loss: 14.6488552Losses:  16.098453521728516 0.5278917551040649 13.663681030273438
MemoryTrain:  epoch  6, batch     9 | loss: 16.0984535Losses:  19.80261993408203 1.1307027339935303 16.70639991760254
MemoryTrain:  epoch  7, batch     0 | loss: 19.8026199Losses:  13.9657564163208 1.2270241975784302 10.801301002502441
MemoryTrain:  epoch  7, batch     1 | loss: 13.9657564Losses:  18.852943420410156 0.23855309188365936 16.68665885925293
MemoryTrain:  epoch  7, batch     2 | loss: 18.8529434Losses:  13.78347396850586 1.074894905090332 10.786271095275879
MemoryTrain:  epoch  7, batch     3 | loss: 13.7834740Losses:  22.50786590576172 0.7330920696258545 19.85516357421875
MemoryTrain:  epoch  7, batch     4 | loss: 22.5078659Losses:  19.438411712646484 0.7549523711204529 16.70832633972168
MemoryTrain:  epoch  7, batch     5 | loss: 19.4384117Losses:  11.493614196777344 1.4801006317138672 8.07829761505127
MemoryTrain:  epoch  7, batch     6 | loss: 11.4936142Losses:  11.085094451904297 1.0098648071289062 8.088890075683594
MemoryTrain:  epoch  7, batch     7 | loss: 11.0850945Losses:  26.334562301635742 1.3169498443603516 23.115768432617188
MemoryTrain:  epoch  7, batch     8 | loss: 26.3345623Losses:  13.847336769104004 1.2069756984710693 10.783452033996582
MemoryTrain:  epoch  7, batch     9 | loss: 13.8473368Losses:  19.130550384521484 0.4982643127441406 16.692855834960938
MemoryTrain:  epoch  8, batch     0 | loss: 19.1305504Losses:  16.682950973510742 1.1124157905578613 13.679601669311523
MemoryTrain:  epoch  8, batch     1 | loss: 16.6829510Losses:  13.97636604309082 1.3279213905334473 10.781717300415039
MemoryTrain:  epoch  8, batch     2 | loss: 13.9763660Losses:  13.362492561340332 0.6941537857055664 10.781037330627441
MemoryTrain:  epoch  8, batch     3 | loss: 13.3624926Losses:  13.851917266845703 1.1494166851043701 10.808666229248047
MemoryTrain:  epoch  8, batch     4 | loss: 13.8519173Losses:  11.345449447631836 1.3378403186798096 8.080499649047852
MemoryTrain:  epoch  8, batch     5 | loss: 11.3454494Losses:  22.626253128051758 0.9357534050941467 19.82588005065918
MemoryTrain:  epoch  8, batch     6 | loss: 22.6262531Losses:  16.844484329223633 1.2915279865264893 13.663629531860352
MemoryTrain:  epoch  8, batch     7 | loss: 16.8444843Losses:  16.71721076965332 1.1775140762329102 13.659784317016602
MemoryTrain:  epoch  8, batch     8 | loss: 16.7172108Losses:  10.263833045959473 0.26429685950279236 8.073390007019043
MemoryTrain:  epoch  8, batch     9 | loss: 10.2638330Losses:  16.6346492767334 1.1066323518753052 13.657634735107422
MemoryTrain:  epoch  9, batch     0 | loss: 16.6346493Losses:  16.716739654541016 1.1951320171356201 13.653619766235352
MemoryTrain:  epoch  9, batch     1 | loss: 16.7167397Losses:  13.991701126098633 1.3171865940093994 10.770946502685547
MemoryTrain:  epoch  9, batch     2 | loss: 13.9917011Losses:  13.66036605834961 0.9980707168579102 10.799796104431152
MemoryTrain:  epoch  9, batch     3 | loss: 13.6603661Losses:  19.080211639404297 0.500356912612915 16.685007095336914
MemoryTrain:  epoch  9, batch     4 | loss: 19.0802116Losses:  16.651004791259766 1.1188929080963135 13.662585258483887
MemoryTrain:  epoch  9, batch     5 | loss: 16.6510048Losses:  16.286283493041992 0.7221900820732117 13.680627822875977
MemoryTrain:  epoch  9, batch     6 | loss: 16.2862835Losses:  16.952468872070312 1.3859057426452637 13.65976333618164
MemoryTrain:  epoch  9, batch     7 | loss: 16.9524689Losses:  16.67251968383789 1.1281923055648804 13.668450355529785
MemoryTrain:  epoch  9, batch     8 | loss: 16.6725197Losses:  19.022518157958984 0.49428790807724 16.681406021118164
MemoryTrain:  epoch  9, batch     9 | loss: 19.0225182
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 80.83%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 52.50%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 51.04%   [EVAL] batch:    6 | acc: 12.50%,  total acc: 45.54%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 40.62%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 38.89%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 35.62%   [EVAL] batch:   10 | acc: 6.25%,  total acc: 32.95%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 32.81%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 30.77%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 30.36%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 33.33%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 34.77%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 37.13%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 38.54%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 39.80%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 42.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 44.94%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 47.44%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 49.73%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 51.56%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 53.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 55.05%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 56.48%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 58.04%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 59.48%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 60.62%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 61.69%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 62.12%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 60.29%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 58.75%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 57.29%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 55.74%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 54.28%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 53.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 54.84%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 54.42%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 54.46%   [EVAL] batch:   42 | acc: 56.25%,  total acc: 54.51%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 54.97%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 55.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 56.93%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 57.85%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 58.72%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 59.57%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 60.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 60.78%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 61.06%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 61.56%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 62.27%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 62.84%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 63.28%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 63.82%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 64.44%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 64.83%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 67.99%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 67.91%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 67.92%   [EVAL] batch:   68 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 68.93%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 68.92%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 69.18%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 69.26%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 69.42%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 69.74%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 69.89%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 70.27%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 70.41%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 70.68%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 70.81%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 71.01%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 71.21%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 71.47%   
cur_acc:  ['0.8655', '0.7257', '0.6719', '0.8884', '0.8083']
his_acc:  ['0.8655', '0.8300', '0.7368', '0.7042', '0.7147']
Clustering into  14  clusters
Clusters:  [ 2  4  9 13  1 12  2 10  2  1  8  2 11  7  6  5  6  0  2  9  1  2  0  3
  1  1  0  2  1  4  2]
Losses:  18.242036819458008 7.935322284698486 3.3254623413085938
CurrentTrain: epoch  0, batch     0 | loss: 18.2420368Losses:  12.634511947631836 2.389247179031372 3.404369831085205
CurrentTrain: epoch  0, batch     1 | loss: 12.6345119Losses:  17.869190216064453 8.514863967895508 3.3345861434936523
CurrentTrain: epoch  1, batch     0 | loss: 17.8691902Losses:  10.933774948120117 3.4299862384796143 3.328758716583252
CurrentTrain: epoch  1, batch     1 | loss: 10.9337749Losses:  16.51886558532715 8.430459022521973 3.321444034576416
CurrentTrain: epoch  2, batch     0 | loss: 16.5188656Losses:  10.767242431640625 3.476884365081787 3.336014747619629
CurrentTrain: epoch  2, batch     1 | loss: 10.7672424Losses:  13.933006286621094 6.91066312789917 3.3179497718811035
CurrentTrain: epoch  3, batch     0 | loss: 13.9330063Losses:  8.337003707885742 2.696096181869507 1.407381534576416
CurrentTrain: epoch  3, batch     1 | loss: 8.3370037Losses:  13.61062240600586 6.7762956619262695 3.3399829864501953
CurrentTrain: epoch  4, batch     0 | loss: 13.6106224Losses:  8.940058708190918 2.1775522232055664 3.300116777420044
CurrentTrain: epoch  4, batch     1 | loss: 8.9400587Losses:  14.595603942871094 8.474451065063477 3.313831329345703
CurrentTrain: epoch  5, batch     0 | loss: 14.5956039Losses:  9.317086219787598 4.817789077758789 1.3919934034347534
CurrentTrain: epoch  5, batch     1 | loss: 9.3170862Losses:  12.250062942504883 6.275856971740723 3.320554733276367
CurrentTrain: epoch  6, batch     0 | loss: 12.2500629Losses:  7.734711647033691 1.9409103393554688 3.321178674697876
CurrentTrain: epoch  6, batch     1 | loss: 7.7347116Losses:  11.917840003967285 6.238966941833496 3.313908576965332
CurrentTrain: epoch  7, batch     0 | loss: 11.9178400Losses:  7.7192816734313965 1.8783173561096191 3.3116211891174316
CurrentTrain: epoch  7, batch     1 | loss: 7.7192817Losses:  11.61297607421875 5.968717575073242 3.3056435585021973
CurrentTrain: epoch  8, batch     0 | loss: 11.6129761Losses:  7.46556282043457 1.9222466945648193 3.321258544921875
CurrentTrain: epoch  8, batch     1 | loss: 7.4655628Losses:  11.886213302612305 6.461862564086914 3.310741662979126
CurrentTrain: epoch  9, batch     0 | loss: 11.8862133Losses:  5.366631507873535 2.0862812995910645 1.3978636264801025
CurrentTrain: epoch  9, batch     1 | loss: 5.3666315
#### person date of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person date of birth" means a person was born in a date.
Here is an example:
Relation: person date of birth
Context: kirkaldy , born irene morgan in baltimore , maryland , in 1917 , was arrested in 1944 for refusing to give up her seat on a greyhound bus heading from gloucester to baltimore , and for resisting arrest .
Head Entity: irene morgan
Tail Entity: 1917
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person date of birth":
Relation: person date of birth  
Context: born in 1985 in a small town in Texas, Sarah Johnson grew up with dreams of becoming a scientist.  
Head Entity: Sarah Johnson  
Tail Entity: 1985  

Relation: person date of birth  
Context: The famous actor, who was born in 1970 in Los Angeles, has starred in numerous blockbuster films.  
Head Entity: The famous actor  
Tail Entity: 1970  

Relation: person date of birth  
Context: Albert Einstein, born in Ulm, Germany, on March 14, 1879, revolutionized the field of physics with his theories.  
Head Entity: Albert Einstein  
Tail Entity: March 14, 1879  

Relation: person date of birth  
Context: In 1992, a remarkable musician was born in Seattle, Washington, who would later change the music industry.  
Head Entity: A remarkable musician  
Tail Entity: 1992  

Relation: person date of birth  
Context: Born in 2000, the young prodigy quickly gained fame for her exceptional talent in mathematics.  
Head Entity: The young prodigy  
Tail Entity: 2000  
#### person stateorprovince of birth ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of birth" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of birth
Context: jefferson joseph deblanc sr. was born in lockport , la. , on feb. 15 , 1921 , and grew up in st. martinville .
Head Entity: jefferson joseph deblanc sr.
Tail Entity: la.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of birth":
Relation: person stateorprovince of birth  
Context: martha stewart was born in jersey city, nj, on august 3, 1941, and later moved to new york.  
Head Entity: martha stewart  
Tail Entity: nj  

Relation: person stateorprovince of birth  
Context: barack obama was born in honolulu, hi, on august 4, 1961, and spent his childhood in chicago.  
Head Entity: barack obama  
Tail Entity: hi  

Relation: person stateorprovince of birth  
Context: taylor swift was born in reading, pa, on december 13, 1989, and grew up in wyomissing.  
Head Entity: taylor swift  
Tail Entity: pa  

Relation: person stateorprovince of birth  
Context: elon musk was born in pretoria, gauteng, south africa, on june 28, 1971, and later moved to the united states.  
Head Entity: elon musk  
Tail Entity: gauteng  

Relation: person stateorprovince of birth  
Context: oprah winfrey was born in kosciusko, ms, on january 29, 1954, and became a prominent media executive.  
Head Entity: oprah winfrey  
Tail Entity: ms  
#### person parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person parents" means a person 's parent is a person.
Here is an example:
Relation: person parents
Context: as the case developed , sandy 's mother , denise sandy , quietly made herself a spectral but central figure , by faithfully attending pretrial hearings .
Head Entity: sandy
Tail Entity: denise sandy
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person parents":
Relation: person parents  
Context: After the ceremony, Emily's father, John Smith, gave a heartfelt speech about family values and the importance of love.  
Head Entity: Emily  
Tail Entity: John Smith  

Relation: person parents  
Context: During the family reunion, Michael's mother, Sarah Johnson, shared stories from her childhood that made everyone laugh.  
Head Entity: Michael  
Tail Entity: Sarah Johnson  

Relation: person parents  
Context: At the graduation party, Lisa's father, Robert Brown, proudly announced her achievements to all the guests.  
Head Entity: Lisa  
Tail Entity: Robert Brown  

Relation: person parents  
Context: As the holidays approached, Kevin's mother, Patricia White, started preparing her famous holiday cookies for the family gathering.  
Head Entity: Kevin  
Tail Entity: Patricia White  

Relation: person parents  
Context: In the documentary, Anna's father, George Taylor, discussed the challenges he faced while raising her as a single parent.  
Head Entity: Anna  
Tail Entity: George Taylor  
#### person employee of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person employee of" means a person is an employee of an organization.
Here is an example:
Relation: person employee of
Context: seeking revenge , axel reunites with old pal sgt. billy rosewood -lrb- judge reinhold -rrb- and jon flint -lrb- hector elizondo -rrb- of the beverly hills police department .
Head Entity: hector elizondo
Tail Entity: beverly hills police department
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person employee of":
Relation: person employee of  
Context: After years of hard work, Maria finally landed a job at the prestigious tech firm, where she collaborates with some of the brightest minds in the industry.  
Head Entity: Maria  
Tail Entity: prestigious tech firm  

Relation: person employee of  
Context: John has been with the company for over a decade, contributing to numerous successful projects and earning the respect of his colleagues.  
Head Entity: John  
Tail Entity: the company  

Relation: person employee of  
Context: As a lead designer at the fashion house, Sarah showcases her creativity and innovation, making a name for herself in the competitive industry.  
Head Entity: Sarah  
Tail Entity: the fashion house  

Relation: person employee of  
Context: After graduating from university, Tom accepted a position at a well-known financial institution, where he quickly climbed the corporate ladder.  
Head Entity: Tom  
Tail Entity: well-known financial institution  

Relation: person employee of  
Context: Emily's dedication to her role at the non-profit organization has made a significant impact on the community, earning her several awards.  
Head Entity: Emily  
Tail Entity: non-profit organization  
#### person stateorprovince of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person stateorprovince of death" means a person was died in a state or province.
Here is an example:
Relation: person stateorprovince of death
Context: irene morgan kirkaldy , whose defiance of bus segregation laws more than a decade before rosa parks ' landmark case helped lay the foundation for later civil rights victories , died friday at her home in hayes , va. .
Head Entity: irene morgan kirkaldy
Tail Entity: va.
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person stateorprovince of death":
Relation: person stateorprovince of death  
Context: john doe, a renowned author known for his thrilling novels, passed away peacefully in his sleep at his residence in los angeles, ca.  
Head Entity: john doe  
Tail Entity: ca.  

Relation: person stateorprovince of death  
Context: after a long battle with illness, elizabeth taylor, the iconic actress, died at the age of 79 in her home located in bel air, ca.  
Head Entity: elizabeth taylor  
Tail Entity: ca.  

Relation: person stateorprovince of death  
Context: the famous physicist, albert einstein, took his last breath in princeton, nj, where he had lived for many years.  
Head Entity: albert einstein  
Tail Entity: nj.  

Relation: person stateorprovince of death  
Context: the beloved musician, prince, was found unresponsive in his home in minneapolis, mn, leading to an outpouring of grief from fans worldwide.  
Head Entity: prince  
Tail Entity: mn.  

Relation: person stateorprovince of death  
Context: the legendary civil rights leader, martin luther king jr., was assassinated in memphis, tn, a tragic event that shocked the nation.  
Head Entity: martin luther king jr.  
Tail Entity: tn.  
Losses:  16.01578140258789 1.3322172164916992 10.819958686828613
MemoryTrain:  epoch  0, batch     0 | loss: 16.0157814Losses:  15.281113624572754 1.0903431177139282 10.788633346557617
MemoryTrain:  epoch  0, batch     1 | loss: 15.2811136Losses:  20.432491302490234 0.24396052956581116 16.74253273010254
MemoryTrain:  epoch  0, batch     2 | loss: 20.4324913Losses:  18.466703414916992 1.0860724449157715 13.961196899414062
MemoryTrain:  epoch  0, batch     3 | loss: 18.4667034Losses:  15.35506820678711 0.7396257519721985 10.871257781982422
MemoryTrain:  epoch  0, batch     4 | loss: 15.3550682Losses:  18.326807022094727 0.78954017162323 13.71635627746582
MemoryTrain:  epoch  0, batch     5 | loss: 18.3268070Losses:  20.471303939819336 0.780670166015625 16.802005767822266
MemoryTrain:  epoch  0, batch     6 | loss: 20.4713039Losses:  20.99245834350586 1.2960766553878784 16.719701766967773
MemoryTrain:  epoch  0, batch     7 | loss: 20.9924583Losses:  14.935107231140137 1.1023211479187012 10.872964859008789
MemoryTrain:  epoch  0, batch     8 | loss: 14.9351072Losses:  14.360751152038574 1.0922064781188965 10.779929161071777
MemoryTrain:  epoch  0, batch     9 | loss: 14.3607512Losses:  17.2847900390625 0.4898195266723633 13.822750091552734
MemoryTrain:  epoch  0, batch    10 | loss: 17.2847900Losses:  12.62998104095459 0.29631373286247253 8.123844146728516
MemoryTrain:  epoch  0, batch    11 | loss: 12.6299810Losses:  19.856735229492188 0.24016432464122772 16.78291130065918
MemoryTrain:  epoch  1, batch     0 | loss: 19.8567352Losses:  18.169082641601562 1.4582281112670898 13.699872016906738
MemoryTrain:  epoch  1, batch     1 | loss: 18.1690826Losses:  17.36992645263672 1.3786622285842896 13.712453842163086
MemoryTrain:  epoch  1, batch     2 | loss: 17.3699265Losses:  14.456096649169922 1.1100969314575195 10.88344669342041
MemoryTrain:  epoch  1, batch     3 | loss: 14.4560966Losses:  18.269939422607422 2.2343459129333496 13.824575424194336
MemoryTrain:  epoch  1, batch     4 | loss: 18.2699394Losses:  20.542095184326172 0.8095078468322754 16.91982078552246
MemoryTrain:  epoch  1, batch     5 | loss: 20.5420952Losses:  19.769441604614258 0.44194120168685913 16.73459815979004
MemoryTrain:  epoch  1, batch     6 | loss: 19.7694416Losses:  13.5105562210083 0.250189870595932 10.818936347961426
MemoryTrain:  epoch  1, batch     7 | loss: 13.5105562Losses:  14.941938400268555 1.400966763496399 10.868616104125977
MemoryTrain:  epoch  1, batch     8 | loss: 14.9419384Losses:  22.269479751586914 -0.0 19.85671043395996
MemoryTrain:  epoch  1, batch     9 | loss: 22.2694798Losses:  16.579288482666016 0.7438142895698547 13.685083389282227
MemoryTrain:  epoch  1, batch    10 | loss: 16.5792885Losses:  10.665008544921875 0.5610873103141785 8.069269180297852
MemoryTrain:  epoch  1, batch    11 | loss: 10.6650085Losses:  16.47783660888672 0.513864278793335 13.682843208312988
MemoryTrain:  epoch  2, batch     0 | loss: 16.4778366Losses:  16.566631317138672 0.9327548742294312 13.64956283569336
MemoryTrain:  epoch  2, batch     1 | loss: 16.5666313Losses:  22.7302303314209 0.8571203351020813 19.8497371673584
MemoryTrain:  epoch  2, batch     2 | loss: 22.7302303Losses:  13.583365440368652 0.5133739709854126 10.840529441833496
MemoryTrain:  epoch  2, batch     3 | loss: 13.5833654Losses:  16.37788200378418 0.46259382367134094 13.656940460205078
MemoryTrain:  epoch  2, batch     4 | loss: 16.3778820Losses:  19.58652687072754 0.534421980381012 16.79073143005371
MemoryTrain:  epoch  2, batch     5 | loss: 19.5865269Losses:  18.532949447631836 2.4580111503601074 13.877927780151367
MemoryTrain:  epoch  2, batch     6 | loss: 18.5329494Losses:  16.297821044921875 0.48737722635269165 13.787960052490234
MemoryTrain:  epoch  2, batch     7 | loss: 16.2978210Losses:  22.44447135925293 0.5352874398231506 19.84028434753418
MemoryTrain:  epoch  2, batch     8 | loss: 22.4444714Losses:  13.72286319732666 0.8625138401985168 10.829012870788574
MemoryTrain:  epoch  2, batch     9 | loss: 13.7228632Losses:  19.325475692749023 0.25102686882019043 16.826616287231445
MemoryTrain:  epoch  2, batch    10 | loss: 19.3254757Losses:  9.978316307067871 -0.0 8.07765007019043
MemoryTrain:  epoch  2, batch    11 | loss: 9.9783163Losses:  14.186193466186523 1.3053524494171143 10.811990737915039
MemoryTrain:  epoch  3, batch     0 | loss: 14.1861935Losses:  14.024803161621094 1.1357547044754028 10.785654067993164
MemoryTrain:  epoch  3, batch     1 | loss: 14.0248032Losses:  25.602359771728516 0.4738430380821228 23.18060874938965
MemoryTrain:  epoch  3, batch     2 | loss: 25.6023598Losses:  22.369258880615234 0.4591681659221649 19.863332748413086
MemoryTrain:  epoch  3, batch     3 | loss: 22.3692589Losses:  14.827629089355469 2.067699909210205 10.80948543548584
MemoryTrain:  epoch  3, batch     4 | loss: 14.8276291Losses:  16.64975929260254 0.9514762759208679 13.692615509033203
MemoryTrain:  epoch  3, batch     5 | loss: 16.6497593Losses:  19.353816986083984 0.729834258556366 16.72080421447754
MemoryTrain:  epoch  3, batch     6 | loss: 19.3538170Losses:  19.358022689819336 0.7219527959823608 16.70759391784668
MemoryTrain:  epoch  3, batch     7 | loss: 19.3580227Losses:  19.23994255065918 0.4903138279914856 16.72377586364746
MemoryTrain:  epoch  3, batch     8 | loss: 19.2399426Losses:  19.024444580078125 0.24878738820552826 16.71418571472168
MemoryTrain:  epoch  3, batch     9 | loss: 19.0244446Losses:  16.20077896118164 0.4975704550743103 13.656221389770508
MemoryTrain:  epoch  3, batch    10 | loss: 16.2007790Losses:  5.38619327545166 -0.0 3.314145803451538
MemoryTrain:  epoch  3, batch    11 | loss: 5.3861933Losses:  17.425804138183594 1.4978572130203247 13.684917449951172
MemoryTrain:  epoch  4, batch     0 | loss: 17.4258041Losses:  19.697635650634766 0.9723628759384155 16.74509620666504
MemoryTrain:  epoch  4, batch     1 | loss: 19.6976357Losses:  12.767265319824219 -0.0 10.80046558380127
MemoryTrain:  epoch  4, batch     2 | loss: 12.7672653Losses:  11.44279670715332 1.3533329963684082 8.082389831542969
MemoryTrain:  epoch  4, batch     3 | loss: 11.4427967Losses:  19.096912384033203 0.49646511673927307 16.684810638427734
MemoryTrain:  epoch  4, batch     4 | loss: 19.0969124Losses:  11.481725692749023 1.3516329526901245 8.10539436340332
MemoryTrain:  epoch  4, batch     5 | loss: 11.4817257Losses:  16.934324264526367 1.1659135818481445 13.705672264099121
MemoryTrain:  epoch  4, batch     6 | loss: 16.9343243Losses:  16.542457580566406 0.7612752914428711 13.718877792358398
MemoryTrain:  epoch  4, batch     7 | loss: 16.5424576Losses:  19.78668785095215 1.0119030475616455 16.790964126586914
MemoryTrain:  epoch  4, batch     8 | loss: 19.7866879Losses:  13.713482856750488 0.9532880783081055 10.805990219116211
MemoryTrain:  epoch  4, batch     9 | loss: 13.7134829Losses:  22.35165786743164 0.4740898609161377 19.88252067565918
MemoryTrain:  epoch  4, batch    10 | loss: 22.3516579Losses:  13.45932388305664 0.5912482142448425 10.773050308227539
MemoryTrain:  epoch  4, batch    11 | loss: 13.4593239Losses:  13.891868591308594 0.7727566361427307 10.862102508544922
MemoryTrain:  epoch  5, batch     0 | loss: 13.8918686Losses:  19.13001823425293 0.48312467336654663 16.69673728942871
MemoryTrain:  epoch  5, batch     1 | loss: 19.1300182Losses:  25.789539337158203 0.5242341756820679 23.174386978149414
MemoryTrain:  epoch  5, batch     2 | loss: 25.7895393Losses:  11.555696487426758 1.4474607706069946 8.097660064697266
MemoryTrain:  epoch  5, batch     3 | loss: 11.5556965Losses:  22.50196647644043 0.7261523604393005 19.858522415161133
MemoryTrain:  epoch  5, batch     4 | loss: 22.5019665Losses:  22.2188720703125 0.4665559232234955 19.878971099853516
MemoryTrain:  epoch  5, batch     5 | loss: 22.2188721Losses:  12.976680755615234 0.22775661945343018 10.8242769241333
MemoryTrain:  epoch  5, batch     6 | loss: 12.9766808Losses:  18.891101837158203 0.2502872347831726 16.695974349975586
MemoryTrain:  epoch  5, batch     7 | loss: 18.8911018Losses:  16.342803955078125 0.7537275552749634 13.690086364746094
MemoryTrain:  epoch  5, batch     8 | loss: 16.3428040Losses:  16.03619956970215 0.4663189947605133 13.655817031860352
MemoryTrain:  epoch  5, batch     9 | loss: 16.0361996Losses:  16.18184471130371 0.523644745349884 13.686878204345703
MemoryTrain:  epoch  5, batch    10 | loss: 16.1818447Losses:  11.045210838317871 1.0361086130142212 8.11224365234375
MemoryTrain:  epoch  5, batch    11 | loss: 11.0452108Losses:  16.101511001586914 0.49275535345077515 13.678383827209473
MemoryTrain:  epoch  6, batch     0 | loss: 16.1015110Losses:  16.104782104492188 0.4783743619918823 13.701138496398926
MemoryTrain:  epoch  6, batch     1 | loss: 16.1047821Losses:  11.370094299316406 1.3721871376037598 8.098877906799316
MemoryTrain:  epoch  6, batch     2 | loss: 11.3700943Losses:  10.76003646850586 0.7339255809783936 8.082751274108887
MemoryTrain:  epoch  6, batch     3 | loss: 10.7600365Losses:  19.297739028930664 0.7399028539657593 16.698806762695312
MemoryTrain:  epoch  6, batch     4 | loss: 19.2977390Losses:  18.805908203125 0.24784773588180542 16.67974853515625
MemoryTrain:  epoch  6, batch     5 | loss: 18.8059082Losses:  25.196250915527344 0.23915709555149078 23.09709358215332
MemoryTrain:  epoch  6, batch     6 | loss: 25.1962509Losses:  13.575738906860352 0.9374714493751526 10.769852638244629
MemoryTrain:  epoch  6, batch     7 | loss: 13.5757389Losses:  19.667381286621094 1.0315251350402832 16.71890640258789
MemoryTrain:  epoch  6, batch     8 | loss: 19.6673813Losses:  11.152263641357422 1.0764203071594238 8.093497276306152
MemoryTrain:  epoch  6, batch     9 | loss: 11.1522636Losses:  19.31973648071289 0.7011271119117737 16.688034057617188
MemoryTrain:  epoch  6, batch    10 | loss: 19.3197365Losses:  10.286413192749023 0.2691327631473541 8.105753898620605
MemoryTrain:  epoch  6, batch    11 | loss: 10.2864132Losses:  15.815428733825684 0.2542265057563782 13.654072761535645
MemoryTrain:  epoch  7, batch     0 | loss: 15.8154287Losses:  18.636098861694336 -0.0 16.689985275268555
MemoryTrain:  epoch  7, batch     1 | loss: 18.6360989Losses:  16.28811264038086 0.744586169719696 13.662652969360352
MemoryTrain:  epoch  7, batch     2 | loss: 16.2881126Losses:  8.52096176147461 0.9963914752006531 5.580620765686035
MemoryTrain:  epoch  7, batch     3 | loss: 8.5209618Losses:  11.656106948852539 1.6657030582427979 8.08194637298584
MemoryTrain:  epoch  7, batch     4 | loss: 11.6561069Losses:  16.521678924560547 1.0042043924331665 13.659830093383789
MemoryTrain:  epoch  7, batch     5 | loss: 16.5216789Losses:  16.098560333251953 0.46007728576660156 13.741558074951172
MemoryTrain:  epoch  7, batch     6 | loss: 16.0985603Losses:  13.67960262298584 1.0159237384796143 10.781347274780273
MemoryTrain:  epoch  7, batch     7 | loss: 13.6796026Losses:  19.32085418701172 0.8000417947769165 16.65146827697754
MemoryTrain:  epoch  7, batch     8 | loss: 19.3208542Losses:  16.207555770874023 0.6624104380607605 13.69306755065918
MemoryTrain:  epoch  7, batch     9 | loss: 16.2075558Losses:  25.94366455078125 0.9647490978240967 23.109264373779297
MemoryTrain:  epoch  7, batch    10 | loss: 25.9436646Losses:  10.49416446685791 0.5061917304992676 8.062548637390137
MemoryTrain:  epoch  7, batch    11 | loss: 10.4941645Losses:  18.814804077148438 0.23055386543273926 16.696491241455078
MemoryTrain:  epoch  8, batch     0 | loss: 18.8148041Losses:  18.812971115112305 0.23880058526992798 16.675546646118164
MemoryTrain:  epoch  8, batch     1 | loss: 18.8129711Losses:  25.423669815063477 0.46523934602737427 23.104454040527344
MemoryTrain:  epoch  8, batch     2 | loss: 25.4236698Losses:  19.613550186157227 1.0665087699890137 16.684839248657227
MemoryTrain:  epoch  8, batch     3 | loss: 19.6135502Losses:  22.550121307373047 0.8594805002212524 19.81355094909668
MemoryTrain:  epoch  8, batch     4 | loss: 22.5501213Losses:  14.203304290771484 1.5416452884674072 10.790385246276855
MemoryTrain:  epoch  8, batch     5 | loss: 14.2033043Losses:  11.217718124389648 1.1988993883132935 8.128215789794922
MemoryTrain:  epoch  8, batch     6 | loss: 11.2177181Losses:  22.184947967529297 0.4954890310764313 19.816577911376953
MemoryTrain:  epoch  8, batch     7 | loss: 22.1849480Losses:  13.526865005493164 0.7781784534454346 10.835381507873535
MemoryTrain:  epoch  8, batch     8 | loss: 13.5268650Losses:  19.030332565307617 0.47246184945106506 16.660789489746094
MemoryTrain:  epoch  8, batch     9 | loss: 19.0303326Losses:  13.725272178649902 1.0099492073059082 10.806086540222168
MemoryTrain:  epoch  8, batch    10 | loss: 13.7252722Losses:  12.954936981201172 0.2554362714290619 10.822772026062012
MemoryTrain:  epoch  8, batch    11 | loss: 12.9549370Losses:  16.54488754272461 0.9837561845779419 13.669625282287598
MemoryTrain:  epoch  9, batch     0 | loss: 16.5448875Losses:  13.191726684570312 0.48217934370040894 10.795940399169922
MemoryTrain:  epoch  9, batch     1 | loss: 13.1917267Losses:  16.821882247924805 1.2615711688995361 13.67856216430664
MemoryTrain:  epoch  9, batch     2 | loss: 16.8218822Losses:  10.72767162322998 0.7467356324195862 8.078048706054688
MemoryTrain:  epoch  9, batch     3 | loss: 10.7276716Losses:  13.501130104064941 0.7701176404953003 10.81992244720459
MemoryTrain:  epoch  9, batch     4 | loss: 13.5011301Losses:  16.441604614257812 0.9335833787918091 13.645753860473633
MemoryTrain:  epoch  9, batch     5 | loss: 16.4416046Losses:  16.516407012939453 0.9934307336807251 13.645109176635742
MemoryTrain:  epoch  9, batch     6 | loss: 16.5164070Losses:  16.52627944946289 0.9532157182693481 13.707719802856445
MemoryTrain:  epoch  9, batch     7 | loss: 16.5262794Losses:  19.2220516204834 0.6926330327987671 16.683603286743164
MemoryTrain:  epoch  9, batch     8 | loss: 19.2220516Losses:  18.992979049682617 0.4679810404777527 16.667936325073242
MemoryTrain:  epoch  9, batch     9 | loss: 18.9929790Losses:  16.63511848449707 0.9500818848609924 13.657034873962402
MemoryTrain:  epoch  9, batch    10 | loss: 16.6351185Losses:  12.624053955078125 -0.0 10.762728691101074
MemoryTrain:  epoch  9, batch    11 | loss: 12.6240540
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 80.80%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 57.81%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 6.25%,  total acc: 45.31%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 38.75%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 36.36%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 35.94%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 33.65%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 33.04%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 35.83%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 37.11%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 39.34%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 40.62%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 41.78%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 44.06%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 46.43%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 48.58%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 50.82%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 52.60%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 54.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 56.01%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 57.41%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.34%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 61.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 62.70%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 63.87%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 63.07%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 61.21%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 59.64%   [EVAL] batch:   35 | acc: 6.25%,  total acc: 58.16%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 56.59%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 55.10%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 54.49%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 55.47%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 55.18%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 54.02%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 53.05%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 53.41%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 54.44%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 55.43%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 56.38%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 57.29%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 58.16%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 58.63%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 59.31%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 59.50%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 59.91%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 60.65%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 61.25%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 61.72%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 62.28%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 62.93%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 63.35%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 66.21%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 66.73%   [EVAL] batch:   65 | acc: 56.25%,  total acc: 66.57%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 66.70%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 66.82%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 67.03%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 67.14%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 67.08%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 66.84%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 66.87%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 66.81%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 66.58%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 66.86%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 67.21%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 67.88%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 68.28%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 68.06%   [EVAL] batch:   81 | acc: 6.25%,  total acc: 67.30%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 66.72%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 66.00%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 65.37%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 65.19%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 65.45%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 65.77%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 66.01%   [EVAL] batch:   89 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:   91 | acc: 87.50%,  total acc: 66.58%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 66.87%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 67.15%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 67.37%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 67.72%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 67.79%   [EVAL] batch:   98 | acc: 37.50%,  total acc: 67.49%   
cur_acc:  ['0.8655', '0.7257', '0.6719', '0.8884', '0.8083', '0.8080']
his_acc:  ['0.8655', '0.8300', '0.7368', '0.7042', '0.7147', '0.6749']
Clustering into  17  clusters
Clusters:  [ 1  5 10 12  3  4  1 13  1  6 15  1 14 11  2 16  2  0  1 10  3  1  0  9
  8  6  0  1  6  5  1  7  7  8  4  6]
Losses:  21.87097930908203 8.834000587463379 3.691075325012207
CurrentTrain: epoch  0, batch     0 | loss: 21.8709793Losses:  14.056884765625 2.4676144123077393 3.7532119750976562
CurrentTrain: epoch  0, batch     1 | loss: 14.0568848Losses:  22.007463455200195 9.555426597595215 3.727221965789795
CurrentTrain: epoch  1, batch     0 | loss: 22.0074635Losses:  12.069755554199219 2.7011806964874268 3.5278501510620117
CurrentTrain: epoch  1, batch     1 | loss: 12.0697556Losses:  18.581836700439453 8.815423011779785 3.5903570652008057
CurrentTrain: epoch  2, batch     0 | loss: 18.5818367Losses:  15.240494728088379 5.266088962554932 1.805565595626831
CurrentTrain: epoch  2, batch     1 | loss: 15.2404947Losses:  16.963966369628906 6.970866680145264 3.7241921424865723
CurrentTrain: epoch  3, batch     0 | loss: 16.9639664Losses:  12.222593307495117 2.255164384841919 3.622551679611206
CurrentTrain: epoch  3, batch     1 | loss: 12.2225933Losses:  17.428871154785156 7.838905334472656 3.578686237335205
CurrentTrain: epoch  4, batch     0 | loss: 17.4288712Losses:  11.410409927368164 2.4134366512298584 3.5877785682678223
CurrentTrain: epoch  4, batch     1 | loss: 11.4104099Losses:  17.58354949951172 8.795854568481445 3.5896921157836914
CurrentTrain: epoch  5, batch     0 | loss: 17.5835495Losses:  14.873491287231445 4.93230676651001 3.5868544578552246
CurrentTrain: epoch  5, batch     1 | loss: 14.8734913Losses:  16.16494369506836 7.97275447845459 3.581695795059204
CurrentTrain: epoch  6, batch     0 | loss: 16.1649437Losses:  11.761883735656738 4.032937526702881 1.5909435749053955
CurrentTrain: epoch  6, batch     1 | loss: 11.7618837Losses:  15.461389541625977 7.662898063659668 3.442121982574463
CurrentTrain: epoch  7, batch     0 | loss: 15.4613895Losses:  13.345856666564941 4.365307331085205 3.4813289642333984
CurrentTrain: epoch  7, batch     1 | loss: 13.3458567Losses:  16.508800506591797 8.225101470947266 3.4135282039642334
CurrentTrain: epoch  8, batch     0 | loss: 16.5088005Losses:  9.531200408935547 2.4397618770599365 3.472888708114624
CurrentTrain: epoch  8, batch     1 | loss: 9.5312004Losses:  15.588889122009277 7.9698286056518555 3.3919146060943604
CurrentTrain: epoch  9, batch     0 | loss: 15.5888891Losses:  9.19851303100586 3.303830623626709 1.518691062927246
CurrentTrain: epoch  9, batch     1 | loss: 9.1985130
#### organization subsidiaries ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization subsidiaries" means an organization 's subsidiary is an organization.
Here is an example:
Relation: organization subsidiaries
Context: `` firstgroup 's acquisition of laidlaw will considerably enhance firstgroup 's existing activities in north america , which themselves have grown strongly since we first invested in the u.s. in 1999 , '' said firstgroup chief executive moir lockhead .
Head Entity: firstgroup
Tail Entity: laidlaw
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization subsidiaries":
Relation: organization subsidiaries  
Context: `` in 2018, the tech giant google announced its acquisition of fitbit, a move that would expand its reach in the health and fitness market, further solidifying google's position in the wearable technology sector. ''  
Head Entity: google  
Tail Entity: fitbit  

Relation: organization subsidiaries  
Context: `` the merger between disney and pixar in 2006 allowed disney to revitalize its animation division, as pixar's innovative storytelling and technology became integral to disney's future projects. ''  
Head Entity: disney  
Tail Entity: pixar  

Relation: organization subsidiaries  
Context: `` after acquiring the popular video game company, activision blizzard, microsoft aimed to enhance its gaming portfolio and compete more effectively in the entertainment industry. ''  
Head Entity: microsoft  
Tail Entity: activision blizzard  

Relation: organization subsidiaries  
Context: `` in a strategic move to diversify its offerings, the beverage company pepsico acquired the snack brand quaker oats, integrating it into its extensive product lineup. ''  
Head Entity: pepsico  
Tail Entity: quaker oats  

Relation: organization subsidiaries  
Context: `` the acquisition of whole foods by amazon in 2017 marked a significant step for the e-commerce giant into the grocery sector, allowing it to leverage whole foods' established brand and customer base. ''  
Head Entity: amazon  
Tail Entity: whole foods  
#### organization parents ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization parents" means an organization 's parent is an organization.
Here is an example:
Relation: organization parents
Context: aseel kami , tim cocks , abdul-rahman taher , tim cocks and michael christie -lrb- reuters -rrb- add an intriguing note in the speculation over who 's responsible for the recent bombings : `` but a senior iraqi intelligence source , who declined to be named , said there was evidence the bombs could be the work of the militant badr organisation , the armed wing of the supreme islamic iraqi council -lrb- isci -rrb- .
Head Entity: badr organisation
Tail Entity: supreme islamic iraqi council
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization parents":
Relation: organization parents  
Context: The tech giant Alphabet Inc. has been making headlines with its innovative projects, but many are unaware that it is the parent company of Google, which has revolutionized the way we access information online.  
Head Entity: Alphabet Inc.  
Tail Entity: Google  

Relation: organization parents  
Context: In the world of finance, JPMorgan Chase & Co. has a significant influence, especially since it is the parent organization of several well-known banks, including Chase Bank, which serves millions of customers across the United States.  
Head Entity: JPMorgan Chase & Co.  
Tail Entity: Chase Bank  

Relation: organization parents  
Context: The Walt Disney Company has expanded its empire over the years, and one of its most notable acquisitions was Pixar Animation Studios, which has produced some of the most beloved animated films in history.  
Head Entity: The Walt Disney Company  
Tail Entity: Pixar Animation Studios  

Relation: organization parents  
Context: In the realm of telecommunications, AT&T Inc. has a vast network and is recognized as the parent company of DirecTV, which provides satellite television services to millions of subscribers.  
Head Entity: AT&T Inc.  
Tail Entity: DirecTV  

Relation: organization parents  
Context: The pharmaceutical industry is heavily influenced by large corporations, and Pfizer Inc. stands out as a major player, being the parent organization of several subsidiaries that develop life-saving medications.  
Head Entity: Pfizer Inc.  
Tail Entity: Subsidiaries of Pfizer
#### organization alternate names ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization alternate names" means a person 's alias is a person.
Here is an example:
Relation: organization alternate names
Context: while section 106 of the hyde act openly bans indian testing , and the agreement upholds reinforces that test ban by upholding the applicability of domestic laws , washington has already recommended that the nuclear suppliers group -lrb- nsg -rrb- link its proposed exemption for india to a similar test ban .
Head Entity: nuclear suppliers group
Tail Entity: nsg
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization alternate names":
Relation: organization alternate names  
Context: The International Monetary Fund, often referred to as the IMF, plays a crucial role in global economic stability.  
Head Entity: International Monetary Fund  
Tail Entity: IMF  

Relation: organization alternate names  
Context: The World Health Organization, commonly known as WHO, has been at the forefront of the global health response during the pandemic.  
Head Entity: World Health Organization  
Tail Entity: WHO  

Relation: organization alternate names  
Context: The National Aeronautics and Space Administration, abbreviated as NASA, is responsible for the nation's civilian space program and for aeronautics and aerospace research.  
Head Entity: National Aeronautics and Space Administration  
Tail Entity: NASA  

Relation: organization alternate names  
Context: The Federal Bureau of Investigation, or FBI, is the principal federal investigative agency and domestic intelligence service of the United States.  
Head Entity: Federal Bureau of Investigation  
Tail Entity: FBI  

Relation: organization alternate names  
Context: The United Nations Educational, Scientific and Cultural Organization, known as UNESCO, aims to promote world peace and security through international cooperation in education, the sciences, and culture.  
Head Entity: United Nations Educational, Scientific and Cultural Organization  
Tail Entity: UNESCO  
#### organization city of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization city of headquarters" means an organization is located in a city.
Here is an example:
Relation: organization city of headquarters
Context: the justice and equality movement -lrb- jem -rrb- , a major rebel movement in darfur , is still insisting on its conditions for the resumption of the talks , including the release of dozens of rebel fighters who were arrested after an attack in omdurman near khartoum in may 2008 .
Head Entity: justice and equality movement
Tail Entity: darfur
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization city of headquarters":
Relation: organization city of headquarters  
Context: the headquarters of the international red cross is located in geneva, switzerland, where it coordinates humanitarian efforts worldwide.  
Head Entity: international red cross  
Tail Entity: geneva  

Relation: organization city of headquarters  
Context: google, a leading technology company, has its main office in mountain view, california, which is known for its innovation and development in the tech industry.  
Head Entity: google  
Tail Entity: mountain view  

Relation: organization city of headquarters  
Context: the united nations has its headquarters in new york city, serving as a central hub for international diplomacy and cooperation among member states.  
Head Entity: united nations  
Tail Entity: new york city  

Relation: organization city of headquarters  
Context: the world health organization, which plays a crucial role in global health initiatives, is based in geneva, switzerland, facilitating its operations across various countries.  
Head Entity: world health organization  
Tail Entity: geneva  

Relation: organization city of headquarters  
Context: the headquarters of the european union is situated in brussels, belgium, where key decisions regarding the governance of member states are made.  
Head Entity: european union  
Tail Entity: brussels  
#### person siblings ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person siblings" means a person 's sibling is a person.
Here is an example:
Relation: person siblings
Context: holly montag says it 's been tough for her sister heidi to deal with all the critics of her massive plastic surgery .
Head Entity: her
Tail Entity: her
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person siblings":
Relation: person siblings  
Context: During the family reunion, John was excited to see his brother Mike after so many years apart.  
Head Entity: John  
Tail Entity: Mike  

Relation: person siblings  
Context: Sarah often shares her childhood memories with her sister Emily, reminiscing about their adventures together.  
Head Entity: Sarah  
Tail Entity: Emily  

Relation: person siblings  
Context: The documentary highlighted the close bond between the twins, showcasing how they supported each other through life's challenges.  
Head Entity: twins  
Tail Entity: twins  

Relation: person siblings  
Context: After their parents' divorce, Lisa and her brother Tom became even closer, relying on each other for support.  
Head Entity: Lisa  
Tail Entity: Tom  

Relation: person siblings  
Context: At the wedding, Anna was thrilled to have her brother David as her best man, celebrating their lifelong friendship.  
Head Entity: Anna  
Tail Entity: David  
Losses:  26.515766143798828 0.48441648483276367 23.207408905029297
MemoryTrain:  epoch  0, batch     0 | loss: 26.5157661Losses:  16.392810821533203 0.47840583324432373 13.695233345031738
MemoryTrain:  epoch  0, batch     1 | loss: 16.3928108Losses:  23.76742172241211 1.3252570629119873 19.879728317260742
MemoryTrain:  epoch  0, batch     2 | loss: 23.7674217Losses:  27.59104347229004 0.738514244556427 23.24863624572754
MemoryTrain:  epoch  0, batch     3 | loss: 27.5910435Losses:  17.476360321044922 0.9831330180168152 13.64741039276123
MemoryTrain:  epoch  0, batch     4 | loss: 17.4763603Losses:  17.86014175415039 1.0627622604370117 13.876121520996094
MemoryTrain:  epoch  0, batch     5 | loss: 17.8601418Losses:  18.665700912475586 0.8974893689155579 13.876777648925781
MemoryTrain:  epoch  0, batch     6 | loss: 18.6657009Losses:  20.43705940246582 1.0058910846710205 16.70558738708496
MemoryTrain:  epoch  0, batch     7 | loss: 20.4370594Losses:  18.610185623168945 1.855554223060608 13.695319175720215
MemoryTrain:  epoch  0, batch     8 | loss: 18.6101856Losses:  24.55423355102539 0.4840394854545593 20.04937171936035
MemoryTrain:  epoch  0, batch     9 | loss: 24.5542336Losses:  19.477506637573242 0.22807951271533966 16.70929527282715
MemoryTrain:  epoch  0, batch    10 | loss: 19.4775066Losses:  25.114261627197266 0.8374878168106079 20.03325843811035
MemoryTrain:  epoch  0, batch    11 | loss: 25.1142616Losses:  21.373910903930664 1.3863816261291504 16.75041389465332
MemoryTrain:  epoch  0, batch    12 | loss: 21.3739109Losses:  8.538164138793945 -0.0 5.587318420410156
MemoryTrain:  epoch  0, batch    13 | loss: 8.5381641Losses:  32.66402816772461 -0.0 29.922569274902344
MemoryTrain:  epoch  1, batch     0 | loss: 32.6640282Losses:  16.63146209716797 0.7110646963119507 13.64579963684082
MemoryTrain:  epoch  1, batch     1 | loss: 16.6314621Losses:  20.24319076538086 0.765061616897583 16.754894256591797
MemoryTrain:  epoch  1, batch     2 | loss: 20.2431908Losses:  20.983299255371094 1.1276307106018066 16.86040687561035
MemoryTrain:  epoch  1, batch     3 | loss: 20.9832993Losses:  23.14359474182129 0.48420238494873047 19.88459587097168
MemoryTrain:  epoch  1, batch     4 | loss: 23.1435947Losses:  23.514955520629883 0.26721400022506714 20.021499633789062
MemoryTrain:  epoch  1, batch     5 | loss: 23.5149555Losses:  23.789371490478516 0.7932742834091187 20.0398006439209
MemoryTrain:  epoch  1, batch     6 | loss: 23.7893715Losses:  19.53607749938965 0.7354630827903748 16.755470275878906
MemoryTrain:  epoch  1, batch     7 | loss: 19.5360775Losses:  23.55913734436035 1.4246084690093994 19.880590438842773
MemoryTrain:  epoch  1, batch     8 | loss: 23.5591373Losses:  23.527557373046875 0.46913382411003113 19.843902587890625
MemoryTrain:  epoch  1, batch     9 | loss: 23.5275574Losses:  26.463329315185547 0.482473760843277 23.115341186523438
MemoryTrain:  epoch  1, batch    10 | loss: 26.4633293Losses:  23.150466918945312 1.0010030269622803 19.8590145111084
MemoryTrain:  epoch  1, batch    11 | loss: 23.1504669Losses:  17.168649673461914 0.9664027690887451 13.673171997070312
MemoryTrain:  epoch  1, batch    12 | loss: 17.1686497Losses:  8.808353424072266 -0.0 5.564755916595459
MemoryTrain:  epoch  1, batch    13 | loss: 8.8083534Losses:  26.049896240234375 0.49079716205596924 23.21970558166504
MemoryTrain:  epoch  2, batch     0 | loss: 26.0498962Losses:  17.746654510498047 1.3621752262115479 13.798834800720215
MemoryTrain:  epoch  2, batch     1 | loss: 17.7466545Losses:  22.619243621826172 0.24405671656131744 19.89907455444336
MemoryTrain:  epoch  2, batch     2 | loss: 22.6192436Losses:  22.641298294067383 0.7487905025482178 19.904354095458984
MemoryTrain:  epoch  2, batch     3 | loss: 22.6412983Losses:  23.050302505493164 0.29354363679885864 19.8966121673584
MemoryTrain:  epoch  2, batch     4 | loss: 23.0503025Losses:  26.46707534790039 1.3332983255386353 23.066930770874023
MemoryTrain:  epoch  2, batch     5 | loss: 26.4670753Losses:  22.121580123901367 0.2354147732257843 19.852624893188477
MemoryTrain:  epoch  2, batch     6 | loss: 22.1215801Losses:  20.117971420288086 0.8814529180526733 16.726280212402344
MemoryTrain:  epoch  2, batch     7 | loss: 20.1179714Losses:  23.248699188232422 0.8130655288696289 19.83535385131836
MemoryTrain:  epoch  2, batch     8 | loss: 23.2486992Losses:  26.306325912475586 0.4757733941078186 23.16786003112793
MemoryTrain:  epoch  2, batch     9 | loss: 26.3063259Losses:  20.298587799072266 1.266239881515503 16.72564125061035
MemoryTrain:  epoch  2, batch    10 | loss: 20.2985878Losses:  20.243284225463867 1.3214874267578125 16.790353775024414
MemoryTrain:  epoch  2, batch    11 | loss: 20.2432842Losses:  22.361167907714844 -0.0 19.98809051513672
MemoryTrain:  epoch  2, batch    12 | loss: 22.3611679Losses:  13.49032974243164 0.25577592849731445 10.792859077453613
MemoryTrain:  epoch  2, batch    13 | loss: 13.4903297Losses:  19.158447265625 0.24003653228282928 16.74112319946289
MemoryTrain:  epoch  3, batch     0 | loss: 19.1584473Losses:  32.58992004394531 0.2323572039604187 29.907922744750977
MemoryTrain:  epoch  3, batch     1 | loss: 32.5899200Losses:  16.25931739807129 0.5075153708457947 13.701469421386719
MemoryTrain:  epoch  3, batch     2 | loss: 16.2593174Losses:  26.522716522216797 1.2647902965545654 23.095176696777344
MemoryTrain:  epoch  3, batch     3 | loss: 26.5227165Losses:  22.31690216064453 0.5225416421890259 19.86161231994629
MemoryTrain:  epoch  3, batch     4 | loss: 22.3169022Losses:  25.914234161376953 0.27801579236984253 23.1286678314209
MemoryTrain:  epoch  3, batch     5 | loss: 25.9142342Losses:  16.704984664916992 0.8210618495941162 13.64828872680664
MemoryTrain:  epoch  3, batch     6 | loss: 16.7049847Losses:  19.40748405456543 0.5012902021408081 16.72245216369629
MemoryTrain:  epoch  3, batch     7 | loss: 19.4074841Losses:  22.94908332824707 0.4946273863315582 19.841222763061523
MemoryTrain:  epoch  3, batch     8 | loss: 22.9490833Losses:  20.29208755493164 1.079723596572876 16.674644470214844
MemoryTrain:  epoch  3, batch     9 | loss: 20.2920876Losses:  26.12479019165039 0.48730283975601196 23.220539093017578
MemoryTrain:  epoch  3, batch    10 | loss: 26.1247902Losses:  13.69833755493164 0.8735857605934143 10.780546188354492
MemoryTrain:  epoch  3, batch    11 | loss: 13.6983376Losses:  23.377025604248047 1.073205590248108 19.965429306030273
MemoryTrain:  epoch  3, batch    12 | loss: 23.3770256Losses:  7.932742595672607 0.27199041843414307 5.6330647468566895
MemoryTrain:  epoch  3, batch    13 | loss: 7.9327426Losses:  29.149490356445312 0.48456504940986633 26.44335174560547
MemoryTrain:  epoch  4, batch     0 | loss: 29.1494904Losses:  29.023204803466797 0.24298831820487976 26.485538482666016
MemoryTrain:  epoch  4, batch     1 | loss: 29.0232048Losses:  22.6829776763916 0.47706204652786255 19.987085342407227
MemoryTrain:  epoch  4, batch     2 | loss: 22.6829777Losses:  23.78252410888672 1.923285722732544 19.81028175354004
MemoryTrain:  epoch  4, batch     3 | loss: 23.7825241Losses:  22.976030349731445 1.1452363729476929 19.84742546081543
MemoryTrain:  epoch  4, batch     4 | loss: 22.9760303Losses:  19.793609619140625 1.0940184593200684 16.71129035949707
MemoryTrain:  epoch  4, batch     5 | loss: 19.7936096Losses:  19.20979118347168 0.47770655155181885 16.70709800720215
MemoryTrain:  epoch  4, batch     6 | loss: 19.2097912Losses:  23.544275283813477 0.9410051703453064 19.914016723632812
MemoryTrain:  epoch  4, batch     7 | loss: 23.5442753Losses:  22.363622665405273 0.2461017519235611 19.894678115844727
MemoryTrain:  epoch  4, batch     8 | loss: 22.3636227Losses:  25.881690979003906 0.5283979177474976 23.09326171875
MemoryTrain:  epoch  4, batch     9 | loss: 25.8816910Losses:  19.860065460205078 0.8576898574829102 16.731433868408203
MemoryTrain:  epoch  4, batch    10 | loss: 19.8600655Losses:  22.30282974243164 0.4919971227645874 19.81998062133789
MemoryTrain:  epoch  4, batch    11 | loss: 22.3028297Losses:  14.19473648071289 1.379439115524292 10.799386024475098
MemoryTrain:  epoch  4, batch    12 | loss: 14.1947365Losses:  7.810175895690918 0.30094748735427856 5.574719429016113
MemoryTrain:  epoch  4, batch    13 | loss: 7.8101759Losses:  25.43585968017578 0.24188530445098877 23.115623474121094
MemoryTrain:  epoch  5, batch     0 | loss: 25.4358597Losses:  19.209470748901367 0.48265382647514343 16.689023971557617
MemoryTrain:  epoch  5, batch     1 | loss: 19.2094707Losses:  16.45867919921875 0.8221628069877625 13.671503067016602
MemoryTrain:  epoch  5, batch     2 | loss: 16.4586792Losses:  22.478778839111328 0.49263912439346313 19.929956436157227
MemoryTrain:  epoch  5, batch     3 | loss: 22.4787788Losses:  25.976327896118164 0.7803972959518433 23.121442794799805
MemoryTrain:  epoch  5, batch     4 | loss: 25.9763279Losses:  15.827775001525879 0.24058467149734497 13.682184219360352
MemoryTrain:  epoch  5, batch     5 | loss: 15.8277750Losses:  26.14727210998535 0.7003580331802368 23.08942222595215
MemoryTrain:  epoch  5, batch     6 | loss: 26.1472721Losses:  16.122697830200195 0.22654111683368683 13.691837310791016
MemoryTrain:  epoch  5, batch     7 | loss: 16.1226978Losses:  25.40423583984375 0.2547423243522644 23.17888069152832
MemoryTrain:  epoch  5, batch     8 | loss: 25.4042358Losses:  22.448688507080078 0.5046672224998474 19.875471115112305
MemoryTrain:  epoch  5, batch     9 | loss: 22.4486885Losses:  26.1650390625 1.0094516277313232 23.110544204711914
MemoryTrain:  epoch  5, batch    10 | loss: 26.1650391Losses:  25.39232635498047 0.26881134510040283 23.137266159057617
MemoryTrain:  epoch  5, batch    11 | loss: 25.3923264Losses:  21.927061080932617 -0.0 19.880807876586914
MemoryTrain:  epoch  5, batch    12 | loss: 21.9270611Losses:  12.992008209228516 0.30606478452682495 10.786044120788574
MemoryTrain:  epoch  5, batch    13 | loss: 12.9920082Losses:  19.021305084228516 0.2365989089012146 16.684547424316406
MemoryTrain:  epoch  6, batch     0 | loss: 19.0213051Losses:  19.697044372558594 0.7513034343719482 16.726778030395508
MemoryTrain:  epoch  6, batch     1 | loss: 19.6970444Losses:  16.687795639038086 0.562483549118042 13.655153274536133
MemoryTrain:  epoch  6, batch     2 | loss: 16.6877956Losses:  23.03687286376953 1.319488763809204 19.82655143737793
MemoryTrain:  epoch  6, batch     3 | loss: 23.0368729Losses:  25.572057723999023 0.27206993103027344 23.12188148498535
MemoryTrain:  epoch  6, batch     4 | loss: 25.5720577Losses:  19.15970230102539 0.497836172580719 16.669607162475586
MemoryTrain:  epoch  6, batch     5 | loss: 19.1597023Losses:  31.7751407623291 -0.0 29.8855037689209
MemoryTrain:  epoch  6, batch     6 | loss: 31.7751408Losses:  28.449607849121094 -0.0 26.435522079467773
MemoryTrain:  epoch  6, batch     7 | loss: 28.4496078Losses:  22.606826782226562 0.7458387017250061 19.85957145690918
MemoryTrain:  epoch  6, batch     8 | loss: 22.6068268Losses:  16.332359313964844 0.7299586534500122 13.67910385131836
MemoryTrain:  epoch  6, batch     9 | loss: 16.3323593Losses:  19.577926635742188 1.0040111541748047 16.68252182006836
MemoryTrain:  epoch  6, batch    10 | loss: 19.5779266Losses:  19.60158920288086 0.5479767322540283 16.70514678955078
MemoryTrain:  epoch  6, batch    11 | loss: 19.6015892Losses:  16.964933395385742 1.3138105869293213 13.662065505981445
MemoryTrain:  epoch  6, batch    12 | loss: 16.9649334Losses:  12.7130765914917 -0.0 10.827884674072266
MemoryTrain:  epoch  6, batch    13 | loss: 12.7130766Losses:  25.526947021484375 0.4685123562812805 23.130657196044922
MemoryTrain:  epoch  7, batch     0 | loss: 25.5269470Losses:  16.736120223999023 1.096767783164978 13.673516273498535
MemoryTrain:  epoch  7, batch     1 | loss: 16.7361202Losses:  28.818758010864258 0.27398091554641724 26.43880844116211
MemoryTrain:  epoch  7, batch     2 | loss: 28.8187580Losses:  21.90328598022461 -0.0 19.839466094970703
MemoryTrain:  epoch  7, batch     3 | loss: 21.9032860Losses:  19.68827247619629 1.06492280960083 16.715177536010742
MemoryTrain:  epoch  7, batch     4 | loss: 19.6882725Losses:  25.734039306640625 0.7212483882904053 23.10658836364746
MemoryTrain:  epoch  7, batch     5 | loss: 25.7340393Losses:  19.470561981201172 0.7147091031074524 16.719430923461914
MemoryTrain:  epoch  7, batch     6 | loss: 19.4705620Losses:  13.994346618652344 1.3184826374053955 10.763890266418457
MemoryTrain:  epoch  7, batch     7 | loss: 13.9943466Losses:  18.845726013183594 0.25560420751571655 16.66556739807129
MemoryTrain:  epoch  7, batch     8 | loss: 18.8457260Losses:  22.48137855529785 0.7427706718444824 19.83582878112793
MemoryTrain:  epoch  7, batch     9 | loss: 22.4813786Losses:  14.009573936462402 0.8721398711204529 10.790215492248535
MemoryTrain:  epoch  7, batch    10 | loss: 14.0095739Losses:  19.238922119140625 0.5096679329872131 16.661027908325195
MemoryTrain:  epoch  7, batch    11 | loss: 19.2389221Losses:  22.691980361938477 0.9405166506767273 19.8292293548584
MemoryTrain:  epoch  7, batch    12 | loss: 22.6919804Losses:  10.006759643554688 -0.0 8.080297470092773
MemoryTrain:  epoch  7, batch    13 | loss: 10.0067596Losses:  19.65928840637207 1.088802695274353 16.65867042541504
MemoryTrain:  epoch  8, batch     0 | loss: 19.6592884Losses:  16.256765365600586 0.49082109332084656 13.717406272888184
MemoryTrain:  epoch  8, batch     1 | loss: 16.2567654Losses:  26.022432327270508 1.0720040798187256 23.068384170532227
MemoryTrain:  epoch  8, batch     2 | loss: 26.0224323Losses:  16.61145782470703 0.9745473861694336 13.650358200073242
MemoryTrain:  epoch  8, batch     3 | loss: 16.6114578Losses:  17.09096908569336 1.3152084350585938 13.730813026428223
MemoryTrain:  epoch  8, batch     4 | loss: 17.0909691Losses:  17.94684600830078 2.3793845176696777 13.657075881958008
MemoryTrain:  epoch  8, batch     5 | loss: 17.9468460Losses:  22.22356605529785 0.48645496368408203 19.827774047851562
MemoryTrain:  epoch  8, batch     6 | loss: 22.2235661Losses:  19.364028930664062 0.750481128692627 16.72195053100586
MemoryTrain:  epoch  8, batch     7 | loss: 19.3640289Losses:  19.875736236572266 1.3195651769638062 16.696754455566406
MemoryTrain:  epoch  8, batch     8 | loss: 19.8757362Losses:  29.023944854736328 0.737170398235321 26.440319061279297
MemoryTrain:  epoch  8, batch     9 | loss: 29.0239449Losses:  25.989900588989258 0.7127370238304138 23.122968673706055
MemoryTrain:  epoch  8, batch    10 | loss: 25.9899006Losses:  19.284507751464844 0.7179182767868042 16.67976188659668
MemoryTrain:  epoch  8, batch    11 | loss: 19.2845078Losses:  16.341533660888672 0.7297968864440918 13.664417266845703
MemoryTrain:  epoch  8, batch    12 | loss: 16.3415337Losses:  8.12855052947998 0.3263157606124878 5.570062160491943
MemoryTrain:  epoch  8, batch    13 | loss: 8.1285505Losses:  25.329626083374023 0.2693853974342346 23.066232681274414
MemoryTrain:  epoch  9, batch     0 | loss: 25.3296261Losses:  19.42926025390625 0.7500439882278442 16.678386688232422
MemoryTrain:  epoch  9, batch     1 | loss: 19.4292603Losses:  16.662220001220703 0.9910694360733032 13.662925720214844
MemoryTrain:  epoch  9, batch     2 | loss: 16.6622200Losses:  13.274714469909668 0.5159738659858704 10.807894706726074
MemoryTrain:  epoch  9, batch     3 | loss: 13.2747145Losses:  23.42020034790039 1.6049119234085083 19.915273666381836
MemoryTrain:  epoch  9, batch     4 | loss: 23.4202003Losses:  28.363130569458008 -0.0 26.47162437438965
MemoryTrain:  epoch  9, batch     5 | loss: 28.3631306Losses:  22.002662658691406 0.23848509788513184 19.84143829345703
MemoryTrain:  epoch  9, batch     6 | loss: 22.0026627Losses:  22.505849838256836 0.8547523617744446 19.804792404174805
MemoryTrain:  epoch  9, batch     7 | loss: 22.5058498Losses:  22.192371368408203 0.47904425859451294 19.839107513427734
MemoryTrain:  epoch  9, batch     8 | loss: 22.1923714Losses:  28.80168342590332 0.46192654967308044 26.457592010498047
MemoryTrain:  epoch  9, batch     9 | loss: 28.8016834Losses:  13.878732681274414 1.022298812866211 10.805333137512207
MemoryTrain:  epoch  9, batch    10 | loss: 13.8787327Losses:  22.676626205444336 0.9628534317016602 19.875600814819336
MemoryTrain:  epoch  9, batch    11 | loss: 22.6766262Losses:  22.302963256835938 0.5196627378463745 19.836713790893555
MemoryTrain:  epoch  9, batch    12 | loss: 22.3029633Losses:  5.744673728942871 0.5757033824920654 3.3039159774780273
MemoryTrain:  epoch  9, batch    13 | loss: 5.7446737
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 15.62%   [EVAL] batch:    2 | acc: 12.50%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 15.62%   [EVAL] batch:    4 | acc: 18.75%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 14.58%   [EVAL] batch:    6 | acc: 25.00%,  total acc: 16.07%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 20.31%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 23.61%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 24.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 28.41%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 30.21%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 32.21%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 36.61%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 40.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 43.75%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 47.06%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 49.65%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 51.32%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 52.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 54.46%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 53.12%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 54.69%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 18.75%,  total acc: 51.79%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 18.75%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 37.50%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 36.98%   [EVAL] batch:   12 | acc: 6.25%,  total acc: 34.62%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 33.93%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 36.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 37.89%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 40.07%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 41.32%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 42.43%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 44.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 47.02%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 49.15%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 51.36%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 53.12%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 55.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 56.49%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 57.87%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.78%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 61.88%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 62.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 63.26%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 61.40%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 59.64%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 57.99%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 56.42%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 54.93%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 54.33%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 55.16%   [EVAL] batch:   40 | acc: 43.75%,  total acc: 54.88%   [EVAL] batch:   41 | acc: 6.25%,  total acc: 53.72%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 52.47%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 52.84%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 53.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 54.89%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 55.85%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 56.77%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 57.65%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 58.13%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 58.82%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 58.89%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 59.08%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 59.72%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 60.11%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 60.60%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 61.18%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 61.75%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 62.18%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 62.81%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 63.42%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 64.01%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 65.67%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   66 | acc: 25.00%,  total acc: 65.02%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 65.07%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 65.49%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 65.49%   [EVAL] batch:   71 | acc: 43.75%,  total acc: 65.19%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 65.24%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 65.20%   [EVAL] batch:   74 | acc: 37.50%,  total acc: 64.83%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 65.13%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 65.50%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 66.06%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:   80 | acc: 50.00%,  total acc: 66.28%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 65.47%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 64.76%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 64.06%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 63.46%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 63.30%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 63.36%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 63.21%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 62.99%   [EVAL] batch:   89 | acc: 37.50%,  total acc: 62.71%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 62.43%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 62.43%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 62.77%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 63.10%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 63.36%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 63.61%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 63.79%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 64.03%   [EVAL] batch:   98 | acc: 50.00%,  total acc: 63.89%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 63.44%   [EVAL] batch:  100 | acc: 18.75%,  total acc: 63.00%   [EVAL] batch:  101 | acc: 6.25%,  total acc: 62.44%   [EVAL] batch:  102 | acc: 18.75%,  total acc: 62.01%   [EVAL] batch:  103 | acc: 12.50%,  total acc: 61.54%   [EVAL] batch:  104 | acc: 12.50%,  total acc: 61.07%   [EVAL] batch:  105 | acc: 37.50%,  total acc: 60.85%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 60.69%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 60.53%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 60.49%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 60.45%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 60.36%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 60.44%   [EVAL] batch:  112 | acc: 100.00%,  total acc: 60.79%   [EVAL] batch:  113 | acc: 93.75%,  total acc: 61.07%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 61.36%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 61.64%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 61.91%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 62.13%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 62.24%   [EVAL] batch:  119 | acc: 75.00%,  total acc: 62.34%   [EVAL] batch:  120 | acc: 6.25%,  total acc: 61.88%   
cur_acc:  ['0.8655', '0.7257', '0.6719', '0.8884', '0.8083', '0.8080', '0.5312']
his_acc:  ['0.8655', '0.8300', '0.7368', '0.7042', '0.7147', '0.6749', '0.6188']
Clustering into  19  clusters
Clusters:  [ 4  1  0 11 17 10  4 15 18 16 12  4 13  5  2  1  2  7  4  0  6  4  3  9
  8 16  7  4 16  1 18  0  0  8 10 16  3  6 10 14 18]
Losses:  16.387161254882812 6.5842366218566895 5.592001914978027
CurrentTrain: epoch  0, batch     0 | loss: 16.3871613Losses:  11.91004753112793 2.1026721000671387 5.6436285972595215
CurrentTrain: epoch  0, batch     1 | loss: 11.9100475Losses:  15.32098388671875 6.658731460571289 5.5676589012146
CurrentTrain: epoch  1, batch     0 | loss: 15.3209839Losses:  9.888362884521484 2.619047164916992 3.3689210414886475
CurrentTrain: epoch  1, batch     1 | loss: 9.8883629Losses:  14.684798240661621 6.128645896911621 5.574947357177734
CurrentTrain: epoch  2, batch     0 | loss: 14.6847982Losses:  10.271805763244629 1.889728307723999 5.586069107055664
CurrentTrain: epoch  2, batch     1 | loss: 10.2718058Losses:  14.913290023803711 6.9935832023620605 5.581024169921875
CurrentTrain: epoch  3, batch     0 | loss: 14.9132900Losses:  8.207929611206055 2.717698097229004 3.3269290924072266
CurrentTrain: epoch  3, batch     1 | loss: 8.2079296Losses:  14.708013534545898 6.903936386108398 5.564455509185791
CurrentTrain: epoch  4, batch     0 | loss: 14.7080135Losses:  8.265379905700684 2.62214732170105 3.30314302444458
CurrentTrain: epoch  4, batch     1 | loss: 8.2653799Losses:  13.385706901550293 5.721078395843506 5.557408332824707
CurrentTrain: epoch  5, batch     0 | loss: 13.3857069Losses:  9.263324737548828 1.7561322450637817 5.568055152893066
CurrentTrain: epoch  5, batch     1 | loss: 9.2633247Losses:  12.849472045898438 5.298050880432129 5.55646276473999
CurrentTrain: epoch  6, batch     0 | loss: 12.8494720Losses:  8.464061737060547 1.1183831691741943 5.563069820404053
CurrentTrain: epoch  6, batch     1 | loss: 8.4640617Losses:  14.693359375 7.276121139526367 5.554958343505859
CurrentTrain: epoch  7, batch     0 | loss: 14.6933594Losses:  9.110664367675781 3.859936237335205 3.3078598976135254
CurrentTrain: epoch  7, batch     1 | loss: 9.1106644Losses:  13.462160110473633 6.068768501281738 5.552588939666748
CurrentTrain: epoch  8, batch     0 | loss: 13.4621601Losses:  9.548927307128906 2.2046356201171875 5.551541805267334
CurrentTrain: epoch  8, batch     1 | loss: 9.5489273Losses:  12.480707168579102 5.0915985107421875 5.5504865646362305
CurrentTrain: epoch  9, batch     0 | loss: 12.4807072Losses:  8.35940933227539 1.002312421798706 5.555367946624756
CurrentTrain: epoch  9, batch     1 | loss: 8.3594093
#### person cause of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person cause of death" means a person was died of an event.
Here is an example:
Relation: person cause of death
Context: he had complications of a stroke .
Head Entity: he
Tail Entity: complications of a stroke
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person cause of death":
Relation: person cause of death  
Context: The patient succumbed to a severe heart attack last night.  
Head Entity: The patient  
Tail Entity: severe heart attack  

Relation: person cause of death  
Context: She passed away due to complications from diabetes.  
Head Entity: She  
Tail Entity: complications from diabetes  

Relation: person cause of death  
Context: After battling cancer for several years, he finally lost his fight.  
Head Entity: he  
Tail Entity: cancer  

Relation: person cause of death  
Context: The firefighter died in the line of duty during a massive blaze.  
Head Entity: The firefighter  
Tail Entity: massive blaze  

Relation: person cause of death  
Context: Her life was cut short by a tragic car accident.  
Head Entity: Her  
Tail Entity: tragic car accident  
#### organization political religious affiliation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization political religious affiliation" means an organization is the member of a religion.
Here is an example:
Relation: organization political religious affiliation
Context: the head of the organisation of the islamic conference -lrb- oic -rrb- said us president barack obama 's speech on thursday would help win hearts in the muslim world .
Head Entity: oic
Tail Entity: muslim
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization political religious affiliation":
1. Relation: organization political religious affiliation  
   Context: The Catholic Church has been a significant player in global politics, influencing various governments and policies.  
   Head Entity: Catholic Church  
   Tail Entity: Christianity  

2. Relation: organization political religious affiliation  
   Context: The World Council of Churches aims to promote unity among Christian denominations and engage in social justice issues worldwide.  
   Head Entity: World Council of Churches  
   Tail Entity: Christianity  

3. Relation: organization political religious affiliation  
   Context: The Islamic Society of North America has been actively involved in advocating for Muslim rights and representation in the political arena.  
   Head Entity: Islamic Society of North America  
   Tail Entity: Islam  

4. Relation: organization political religious affiliation  
   Context: The Jewish Federation works to support Jewish communities and influence policies that affect Jewish people globally.  
   Head Entity: Jewish Federation  
   Tail Entity: Judaism  

5. Relation: organization political religious affiliation  
   Context: The National Council of Churches has been a vocal advocate for various social issues, representing a wide array of Christian denominations.  
   Head Entity: National Council of Churches  
   Tail Entity: Christianity  
#### organization stateorprovince of headquarters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "organization stateorprovince of headquarters" means an organization is located in a state or province.
Here is an example:
Relation: organization stateorprovince of headquarters
Context: andrew lebow , an oil trader with mf global in new york , said investors have been discouraged by lower-than-expected oil imports in china and the disappointing growth in the u.s. economy .
Head Entity: mf global
Tail Entity: new york
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "organization stateorprovince of headquarters":
Relation: organization stateorprovince of headquarters  
Context: the headquarters of tech giant apple inc. is located in cupertino, california, where it has been a hub for innovation and development.  
Head Entity: apple inc.  
Tail Entity: california  

Relation: organization stateorprovince of headquarters  
Context: the multinational corporation unilever has its main office in london, england, overseeing operations across various countries.  
Head Entity: unilever  
Tail Entity: england  

Relation: organization stateorprovince of headquarters  
Context: according to the latest reports, the financial services firm goldman sachs is headquartered in new york city, new york, which is a major financial center.  
Head Entity: goldman sachs  
Tail Entity: new york  

Relation: organization stateorprovince of headquarters  
Context: the famous car manufacturer toyota has its headquarters in toyota city, aichi prefecture, japan, where it was originally founded.  
Head Entity: toyota  
Tail Entity: aichi prefecture  

Relation: organization stateorprovince of headquarters  
Context: the global consulting firm accenture is based in dublin, ireland, which serves as its operational and strategic center for Europe.  
Head Entity: accenture  
Tail Entity: ireland  
#### person other family ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person other family" means a person 's relative is a person.
Here is an example:
Relation: person other family
Context: former american idol contestant , sanjaya malakar and hills starlet , holly montag , have been getting close since she replaced her sister and brother-in-law , heidi montag and spencer pratt , on i 'm a celebrity .
Head Entity: spencer pratt ,
Tail Entity: holly montag
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person other family":
Relation: person other family  
Context: in a recent interview, actress jennifer aniston spoke fondly of her brother, alex aniston, and their close bond despite the challenges they faced growing up in the spotlight.  
Head Entity: alex aniston  
Tail Entity: jennifer aniston  

Relation: person other family  
Context: during the family reunion, uncle bob shared stories about his adventures with his niece, emily, who has always looked up to him as a role model.  
Head Entity: uncle bob  
Tail Entity: emily  

Relation: person other family  
Context: the documentary highlighted the relationship between famous chef gordon ramsay and his daughter, holly ramsay, who is following in his culinary footsteps.  
Head Entity: gordon ramsay  
Tail Entity: holly ramsay  

Relation: person other family  
Context: at the wedding, the bride, sarah, was thrilled to have her cousin, jake, as her best man, celebrating their lifelong friendship and family ties.  
Head Entity: jake  
Tail Entity: sarah  

Relation: person other family  
Context: in her memoir, singer taylor swift reflects on her close relationship with her grandmother, marjorie, who has always been her biggest supporter.  
Head Entity: marjorie  
Tail Entity: taylor swift  
#### person city of death ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "person city of death" means a person was died in a city.
Here is an example:
Relation: person city of death
Context: irene morgan kirkaldy , 90 , who died of alzheimer 's disease aug. 10 at her home in gloucester , va. , quietly changed history in 1944 when she refused to give up her seat on a crowded greyhound bus to a white couple .
Head Entity: her
Tail Entity: gloucester
Please generate 5 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "person city of death":
Relation: person city of death  
Context: john smith, a renowned author, passed away on march 5 in his apartment located in new york city, leaving behind a legacy of literary works that inspired many.  
Head Entity: john smith  
Tail Entity: new york city  

Relation: person city of death  
Context: after a long battle with cancer, elizabeth taylor died on march 23 at a hospital in los angeles, where she had spent her final days surrounded by family and friends.  
Head Entity: elizabeth taylor  
Tail Entity: los angeles  

Relation: person city of death  
Context: the famous physicist, albert einstein, died on april 18, 1955, in princeton, new jersey, where he had lived for many years and contributed to scientific research.  
Head Entity: albert einstein  
Tail Entity: princeton  

Relation: person city of death  
Context: on january 1, 2020, the beloved actor, kobe bryant, tragically lost his life in a helicopter crash in calabasas, california, shocking fans around the world.  
Head Entity: kobe bryant  
Tail Entity: calabasas  

Relation: person city of death  
Context: the legendary musician, freddie mercury, passed away on november 24, 1991, at his home in london, england, leaving a profound impact on the music industry.  
Head Entity: freddie mercury  
Tail Entity: london  
Losses:  23.173553466796875 0.7286715507507324 19.942405700683594
MemoryTrain:  epoch  0, batch     0 | loss: 23.1735535Losses:  27.416555404663086 0.4854225516319275 23.158653259277344
MemoryTrain:  epoch  0, batch     1 | loss: 27.4165554Losses:  29.45515251159668 -0.0 26.537885665893555
MemoryTrain:  epoch  0, batch     2 | loss: 29.4551525Losses:  26.603946685791016 0.24815817177295685 23.221481323242188
MemoryTrain:  epoch  0, batch     3 | loss: 26.6039467Losses:  30.183921813964844 -0.0 26.518299102783203
MemoryTrain:  epoch  0, batch     4 | loss: 30.1839218Losses:  26.38921356201172 0.25865352153778076 23.180753707885742
MemoryTrain:  epoch  0, batch     5 | loss: 26.3892136Losses:  21.2646541595459 0.7726704478263855 16.775985717773438
MemoryTrain:  epoch  0, batch     6 | loss: 21.2646542Losses:  24.789548873901367 0.684145450592041 19.877248764038086
MemoryTrain:  epoch  0, batch     7 | loss: 24.7895489Losses:  20.46688461303711 0.23569336533546448 16.79375648498535
MemoryTrain:  epoch  0, batch     8 | loss: 20.4668846Losses:  31.301366806030273 0.29328006505966187 26.629011154174805
MemoryTrain:  epoch  0, batch     9 | loss: 31.3013668Losses:  27.271862030029297 0.7298291325569153 23.13657569885254
MemoryTrain:  epoch  0, batch    10 | loss: 27.2718620Losses:  20.691129684448242 0.49012452363967896 16.726896286010742
MemoryTrain:  epoch  0, batch    11 | loss: 20.6911297Losses:  21.935131072998047 0.8210448026657104 16.674516677856445
MemoryTrain:  epoch  0, batch    12 | loss: 21.9351311Losses:  24.605920791625977 1.403396725654602 19.880163192749023
MemoryTrain:  epoch  0, batch    13 | loss: 24.6059208Losses:  17.85343360900879 0.9765174388885498 13.65066909790039
MemoryTrain:  epoch  0, batch    14 | loss: 17.8534336Losses:  10.269184112548828 -0.0 8.07933235168457
MemoryTrain:  epoch  0, batch    15 | loss: 10.2691841Losses:  34.12939453125 0.2543795704841614 30.017053604125977
MemoryTrain:  epoch  1, batch     0 | loss: 34.1293945Losses:  26.586275100708008 0.4994792342185974 23.118576049804688
MemoryTrain:  epoch  1, batch     1 | loss: 26.5862751Losses:  23.137527465820312 0.457945317029953 19.868982315063477
MemoryTrain:  epoch  1, batch     2 | loss: 23.1375275Losses:  21.314899444580078 1.8006556034088135 16.74662971496582
MemoryTrain:  epoch  1, batch     3 | loss: 21.3148994Losses:  25.772607803344727 0.23771056532859802 23.076364517211914
MemoryTrain:  epoch  1, batch     4 | loss: 25.7726078Losses:  26.541309356689453 0.48055773973464966 23.0855712890625
MemoryTrain:  epoch  1, batch     5 | loss: 26.5413094Losses:  21.22824478149414 0.7983435988426208 16.73134422302246
MemoryTrain:  epoch  1, batch     6 | loss: 21.2282448Losses:  20.840486526489258 1.3322808742523193 16.695194244384766
MemoryTrain:  epoch  1, batch     7 | loss: 20.8404865Losses:  23.98382568359375 0.4941885471343994 19.841035842895508
MemoryTrain:  epoch  1, batch     8 | loss: 23.9838257Losses:  23.23666000366211 0.805341362953186 19.901920318603516
MemoryTrain:  epoch  1, batch     9 | loss: 23.2366600Losses:  29.466196060180664 0.22983242571353912 26.52033042907715
MemoryTrain:  epoch  1, batch    10 | loss: 29.4661961Losses:  29.83135414123535 -0.0 26.615713119506836
MemoryTrain:  epoch  1, batch    11 | loss: 29.8313541Losses:  20.877792358398438 1.1970105171203613 16.736026763916016
MemoryTrain:  epoch  1, batch    12 | loss: 20.8777924Losses:  26.898279190063477 1.087315559387207 23.1130428314209
MemoryTrain:  epoch  1, batch    13 | loss: 26.8982792Losses:  26.499496459960938 0.4781954884529114 23.147127151489258
MemoryTrain:  epoch  1, batch    14 | loss: 26.4994965Losses:  7.680970191955566 0.30491554737091064 3.3102779388427734
MemoryTrain:  epoch  1, batch    15 | loss: 7.6809702Losses:  29.794483184814453 -0.0 26.46133804321289
MemoryTrain:  epoch  2, batch     0 | loss: 29.7944832Losses:  19.742595672607422 0.9837406277656555 16.688594818115234
MemoryTrain:  epoch  2, batch     1 | loss: 19.7425957Losses:  30.834800720214844 0.5519248843193054 26.546199798583984
MemoryTrain:  epoch  2, batch     2 | loss: 30.8348007Losses:  16.491226196289062 0.7350739240646362 13.65488338470459
MemoryTrain:  epoch  2, batch     3 | loss: 16.4912262Losses:  23.93781089782715 0.8140765428543091 19.871414184570312
MemoryTrain:  epoch  2, batch     4 | loss: 23.9378109Losses:  20.712318420410156 1.3314443826675415 16.735504150390625
MemoryTrain:  epoch  2, batch     5 | loss: 20.7123184Losses:  17.047630310058594 1.1381829977035522 13.675905227661133
MemoryTrain:  epoch  2, batch     6 | loss: 17.0476303Losses:  20.287368774414062 0.7150465250015259 16.881473541259766
MemoryTrain:  epoch  2, batch     7 | loss: 20.2873688Losses:  12.008652687072754 1.6270167827606201 8.083715438842773
MemoryTrain:  epoch  2, batch     8 | loss: 12.0086527Losses:  23.338380813598633 0.8759185075759888 19.861377716064453
MemoryTrain:  epoch  2, batch     9 | loss: 23.3383808Losses:  22.871692657470703 0.7521123886108398 19.83418846130371
MemoryTrain:  epoch  2, batch    10 | loss: 22.8716927Losses:  26.97136116027832 1.018369197845459 23.119503021240234
MemoryTrain:  epoch  2, batch    11 | loss: 26.9713612Losses:  22.927583694458008 1.0572394132614136 19.84659194946289
MemoryTrain:  epoch  2, batch    12 | loss: 22.9275837Losses:  29.714900970458984 0.8459604382514954 26.44601058959961
MemoryTrain:  epoch  2, batch    13 | loss: 29.7149010Losses:  23.14331817626953 0.5098873376846313 19.854143142700195
MemoryTrain:  epoch  2, batch    14 | loss: 23.1433182Losses:  7.557085037231445 -0.0 5.5645575523376465
MemoryTrain:  epoch  2, batch    15 | loss: 7.5570850Losses:  19.821014404296875 0.5138382911682129 16.681413650512695
MemoryTrain:  epoch  3, batch     0 | loss: 19.8210144Losses:  23.411998748779297 0.7169569134712219 19.82401466369629
MemoryTrain:  epoch  3, batch     1 | loss: 23.4119987Losses:  26.584365844726562 1.1261770725250244 23.067581176757812
MemoryTrain:  epoch  3, batch     2 | loss: 26.5843658Losses:  19.01907730102539 0.2566305100917816 16.70119857788086
MemoryTrain:  epoch  3, batch     3 | loss: 19.0190773Losses:  29.33919334411621 0.2895234227180481 26.531545639038086
MemoryTrain:  epoch  3, batch     4 | loss: 29.3391933Losses:  29.425594329833984 0.4768648147583008 26.513973236083984
MemoryTrain:  epoch  3, batch     5 | loss: 29.4255943Losses:  25.137792587280273 -0.0 23.099884033203125
MemoryTrain:  epoch  3, batch     6 | loss: 25.1377926Losses:  19.925987243652344 0.4842337369918823 16.66840934753418
MemoryTrain:  epoch  3, batch     7 | loss: 19.9259872Losses:  17.45575714111328 1.1417640447616577 13.6935453414917
MemoryTrain:  epoch  3, batch     8 | loss: 17.4557571Losses:  26.843250274658203 1.1005330085754395 23.114938735961914
MemoryTrain:  epoch  3, batch     9 | loss: 26.8432503Losses:  19.773468017578125 0.9894700050354004 16.798065185546875
MemoryTrain:  epoch  3, batch    10 | loss: 19.7734680Losses:  25.922412872314453 0.2519148886203766 23.096206665039062
MemoryTrain:  epoch  3, batch    11 | loss: 25.9224129Losses:  25.371627807617188 0.24537378549575806 23.15518569946289
MemoryTrain:  epoch  3, batch    12 | loss: 25.3716278Losses:  27.050546646118164 1.0642218589782715 23.08180046081543
MemoryTrain:  epoch  3, batch    13 | loss: 27.0505466Losses:  11.578872680664062 0.5027749538421631 8.102117538452148
MemoryTrain:  epoch  3, batch    14 | loss: 11.5788727Losses:  10.13959789276123 -0.0 8.06269645690918
MemoryTrain:  epoch  3, batch    15 | loss: 10.1395979Losses:  26.479656219482422 0.8958999514579773 23.16700553894043
MemoryTrain:  epoch  4, batch     0 | loss: 26.4796562Losses:  19.477550506591797 0.7255544662475586 16.673702239990234
MemoryTrain:  epoch  4, batch     1 | loss: 19.4775505Losses:  16.239046096801758 0.7110481262207031 13.639495849609375
MemoryTrain:  epoch  4, batch     2 | loss: 16.2390461Losses:  29.737640380859375 0.5319430232048035 26.521312713623047
MemoryTrain:  epoch  4, batch     3 | loss: 29.7376404Losses:  23.81220245361328 0.5517012476921082 19.93121337890625
MemoryTrain:  epoch  4, batch     4 | loss: 23.8122025Losses:  26.120193481445312 0.24050092697143555 23.094858169555664
MemoryTrain:  epoch  4, batch     5 | loss: 26.1201935Losses:  28.35078239440918 -0.0 26.42505645751953
MemoryTrain:  epoch  4, batch     6 | loss: 28.3507824Losses:  23.64181137084961 0.7625306248664856 19.883115768432617
MemoryTrain:  epoch  4, batch     7 | loss: 23.6418114Losses:  20.132240295410156 0.5296757817268372 16.72422981262207
MemoryTrain:  epoch  4, batch     8 | loss: 20.1322403Losses:  22.209747314453125 0.4870872497558594 19.83133888244629
MemoryTrain:  epoch  4, batch     9 | loss: 22.2097473Losses:  22.67384147644043 0.4910838007926941 19.83436393737793
MemoryTrain:  epoch  4, batch    10 | loss: 22.6738415Losses:  28.709203720092773 0.26527589559555054 26.46376609802246
MemoryTrain:  epoch  4, batch    11 | loss: 28.7092037Losses:  19.41170883178711 0.7750378847122192 16.74724769592285
MemoryTrain:  epoch  4, batch    12 | loss: 19.4117088Losses:  23.273324966430664 0.8282591104507446 19.880285263061523
MemoryTrain:  epoch  4, batch    13 | loss: 23.2733250Losses:  22.149267196655273 0.2247631847858429 19.82828712463379
MemoryTrain:  epoch  4, batch    14 | loss: 22.1492672Losses:  8.740954399108887 -0.0 5.575625896453857
MemoryTrain:  epoch  4, batch    15 | loss: 8.7409544Losses:  23.1029052734375 0.4934769570827484 19.86736488342285
MemoryTrain:  epoch  5, batch     0 | loss: 23.1029053Losses:  19.5880069732666 0.5222146511077881 16.73478126525879
MemoryTrain:  epoch  5, batch     1 | loss: 19.5880070Losses:  13.5478515625 0.4751330614089966 10.802348136901855
MemoryTrain:  epoch  5, batch     2 | loss: 13.5478516Losses:  25.698020935058594 -0.0 23.109460830688477
MemoryTrain:  epoch  5, batch     3 | loss: 25.6980209Losses:  26.16075325012207 0.5164878368377686 23.108678817749023
MemoryTrain:  epoch  5, batch     4 | loss: 26.1607533Losses:  29.11150360107422 0.47763657569885254 26.46967315673828
MemoryTrain:  epoch  5, batch     5 | loss: 29.1115036Losses:  26.04936408996582 0.46845102310180664 23.149520874023438
MemoryTrain:  epoch  5, batch     6 | loss: 26.0493641Losses:  25.33635711669922 0.2329685240983963 23.145523071289062
MemoryTrain:  epoch  5, batch     7 | loss: 25.3363571Losses:  29.061702728271484 0.5184542536735535 26.532066345214844
MemoryTrain:  epoch  5, batch     8 | loss: 29.0617027Losses:  25.680885314941406 0.7249791622161865 23.066125869750977
MemoryTrain:  epoch  5, batch     9 | loss: 25.6808853Losses:  18.787982940673828 0.23945841193199158 16.688526153564453
MemoryTrain:  epoch  5, batch    10 | loss: 18.7879829Losses:  23.419721603393555 1.3229420185089111 19.8017578125
MemoryTrain:  epoch  5, batch    11 | loss: 23.4197216Losses:  29.530738830566406 0.23098385334014893 26.478256225585938
MemoryTrain:  epoch  5, batch    12 | loss: 29.5307388Losses:  26.485490798950195 0.49901407957077026 23.11137580871582
MemoryTrain:  epoch  5, batch    13 | loss: 26.4854908Losses:  29.004180908203125 0.3061656355857849 26.43221092224121
MemoryTrain:  epoch  5, batch    14 | loss: 29.0041809Losses:  9.923852920532227 -0.0 8.081099510192871
MemoryTrain:  epoch  5, batch    15 | loss: 9.9238529Losses:  16.938396453857422 0.7779715061187744 13.709012985229492
MemoryTrain:  epoch  6, batch     0 | loss: 16.9383965Losses:  22.915380477905273 0.788616418838501 19.831642150878906
MemoryTrain:  epoch  6, batch     1 | loss: 22.9153805Losses:  20.339004516601562 0.7598421573638916 16.733423233032227
MemoryTrain:  epoch  6, batch     2 | loss: 20.3390045Losses:  25.4903621673584 0.22554457187652588 23.060901641845703
MemoryTrain:  epoch  6, batch     3 | loss: 25.4903622Losses:  26.071413040161133 0.736634373664856 23.096141815185547
MemoryTrain:  epoch  6, batch     4 | loss: 26.0714130Losses:  22.37763214111328 0.24758820235729218 19.892751693725586
MemoryTrain:  epoch  6, batch     5 | loss: 22.3776321Losses:  25.907470703125 0.700871467590332 23.06887435913086
MemoryTrain:  epoch  6, batch     6 | loss: 25.9074707Losses:  19.087156295776367 0.5145708322525024 16.679073333740234
MemoryTrain:  epoch  6, batch     7 | loss: 19.0871563Losses:  19.717269897460938 0.7361321449279785 16.72893714904785
MemoryTrain:  epoch  6, batch     8 | loss: 19.7172699Losses:  18.581750869750977 -0.0 16.678911209106445
MemoryTrain:  epoch  6, batch     9 | loss: 18.5817509Losses:  17.38838768005371 1.85570228099823 13.649913787841797
MemoryTrain:  epoch  6, batch    10 | loss: 17.3883877Losses:  22.072532653808594 0.2481575906276703 19.830434799194336
MemoryTrain:  epoch  6, batch    11 | loss: 22.0725327Losses:  20.217132568359375 1.4077353477478027 16.716541290283203
MemoryTrain:  epoch  6, batch    12 | loss: 20.2171326Losses:  19.631616592407227 0.9819672107696533 16.727535247802734
MemoryTrain:  epoch  6, batch    13 | loss: 19.6316166Losses:  29.137054443359375 0.8151677250862122 26.443950653076172
MemoryTrain:  epoch  6, batch    14 | loss: 29.1370544Losses:  7.693289756774902 0.2737559676170349 5.556683540344238
MemoryTrain:  epoch  6, batch    15 | loss: 7.6932898Losses:  22.15690803527832 0.47236040234565735 19.811080932617188
MemoryTrain:  epoch  7, batch     0 | loss: 22.1569080Losses:  22.27936363220215 0.4624290466308594 19.864574432373047
MemoryTrain:  epoch  7, batch     1 | loss: 22.2793636Losses:  29.051193237304688 0.2816261053085327 26.54043960571289
MemoryTrain:  epoch  7, batch     2 | loss: 29.0511932Losses:  22.43267822265625 0.5133030414581299 19.902029037475586
MemoryTrain:  epoch  7, batch     3 | loss: 22.4326782Losses:  23.20055389404297 1.2435671091079712 19.845081329345703
MemoryTrain:  epoch  7, batch     4 | loss: 23.2005539Losses:  22.21387481689453 0.27972179651260376 19.826385498046875
MemoryTrain:  epoch  7, batch     5 | loss: 22.2138748Losses:  25.498414993286133 0.2415921986103058 23.118574142456055
MemoryTrain:  epoch  7, batch     6 | loss: 25.4984150Losses:  28.942476272583008 0.4788721799850464 26.45235824584961
MemoryTrain:  epoch  7, batch     7 | loss: 28.9424763Losses:  28.886898040771484 0.23665589094161987 26.426464080810547
MemoryTrain:  epoch  7, batch     8 | loss: 28.8868980Losses:  22.006664276123047 0.2346981167793274 19.86274528503418
MemoryTrain:  epoch  7, batch     9 | loss: 22.0066643Losses:  29.493709564208984 0.24483096599578857 26.50518798828125
MemoryTrain:  epoch  7, batch    10 | loss: 29.4937096Losses:  19.774364471435547 0.46967005729675293 16.689146041870117
MemoryTrain:  epoch  7, batch    11 | loss: 19.7743645Losses:  26.262741088867188 0.7177947163581848 23.065738677978516
MemoryTrain:  epoch  7, batch    12 | loss: 26.2627411Losses:  22.212175369262695 0.5166980624198914 19.84041976928711
MemoryTrain:  epoch  7, batch    13 | loss: 22.2121754Losses:  23.097532272338867 0.9676002860069275 19.812345504760742
MemoryTrain:  epoch  7, batch    14 | loss: 23.0975323Losses:  10.050956726074219 -0.0 8.072441101074219
MemoryTrain:  epoch  7, batch    15 | loss: 10.0509567Losses:  12.973529815673828 0.2517991065979004 10.773749351501465
MemoryTrain:  epoch  8, batch     0 | loss: 12.9735298Losses:  23.004270553588867 0.7676022052764893 19.9068660736084
MemoryTrain:  epoch  8, batch     1 | loss: 23.0042706Losses:  23.031333923339844 0.8145987391471863 19.861690521240234
MemoryTrain:  epoch  8, batch     2 | loss: 23.0313339Losses:  25.43937110900879 0.5022239089012146 23.07513999938965
MemoryTrain:  epoch  8, batch     3 | loss: 25.4393711Losses:  29.0291805267334 0.48626989126205444 26.435272216796875
MemoryTrain:  epoch  8, batch     4 | loss: 29.0291805Losses:  25.953428268432617 0.7170981764793396 23.11013412475586
MemoryTrain:  epoch  8, batch     5 | loss: 25.9534283Losses:  17.117067337036133 1.572924017906189 13.681509017944336
MemoryTrain:  epoch  8, batch     6 | loss: 17.1170673Losses:  25.413524627685547 0.4912061095237732 23.07048797607422
MemoryTrain:  epoch  8, batch     7 | loss: 25.4135246Losses:  21.778764724731445 -0.0 19.843658447265625
MemoryTrain:  epoch  8, batch     8 | loss: 21.7787647Losses:  28.49773597717285 -0.0 26.48236846923828
MemoryTrain:  epoch  8, batch     9 | loss: 28.4977360Losses:  19.367412567138672 0.558713436126709 16.677989959716797
MemoryTrain:  epoch  8, batch    10 | loss: 19.3674126Losses:  28.793739318847656 0.23584112524986267 26.485095977783203
MemoryTrain:  epoch  8, batch    11 | loss: 28.7937393Losses:  19.990516662597656 0.706602931022644 16.67942237854004
MemoryTrain:  epoch  8, batch    12 | loss: 19.9905167Losses:  22.646604537963867 0.6046241521835327 19.822505950927734
MemoryTrain:  epoch  8, batch    13 | loss: 22.6466045Losses:  25.796661376953125 0.5687633752822876 23.076171875
MemoryTrain:  epoch  8, batch    14 | loss: 25.7966614Losses:  7.602329254150391 -0.0 5.5737624168396
MemoryTrain:  epoch  8, batch    15 | loss: 7.6023293Losses:  28.832674026489258 0.45365768671035767 26.431798934936523
MemoryTrain:  epoch  9, batch     0 | loss: 28.8326740Losses:  19.32632064819336 0.7135943174362183 16.71200942993164
MemoryTrain:  epoch  9, batch     1 | loss: 19.3263206Losses:  25.76515769958496 0.7326055765151978 23.073734283447266
MemoryTrain:  epoch  9, batch     2 | loss: 25.7651577Losses:  22.610790252685547 0.7668325901031494 19.89070701599121
MemoryTrain:  epoch  9, batch     3 | loss: 22.6107903Losses:  16.661266326904297 1.1014008522033691 13.661391258239746
MemoryTrain:  epoch  9, batch     4 | loss: 16.6612663Losses:  16.621023178100586 1.070875883102417 13.648544311523438
MemoryTrain:  epoch  9, batch     5 | loss: 16.6210232Losses:  16.738372802734375 0.4833381175994873 13.653624534606934
MemoryTrain:  epoch  9, batch     6 | loss: 16.7383728Losses:  19.495195388793945 0.7393346428871155 16.76074981689453
MemoryTrain:  epoch  9, batch     7 | loss: 19.4951954Losses:  22.164831161499023 0.49077272415161133 19.806957244873047
MemoryTrain:  epoch  9, batch     8 | loss: 22.1648312Losses:  22.424699783325195 0.7391352653503418 19.810266494750977
MemoryTrain:  epoch  9, batch     9 | loss: 22.4246998Losses:  32.283626556396484 0.4832681119441986 29.92742347717285
MemoryTrain:  epoch  9, batch    10 | loss: 32.2836266Losses:  22.453983306884766 0.7166920304298401 19.853158950805664
MemoryTrain:  epoch  9, batch    11 | loss: 22.4539833Losses:  19.380014419555664 0.47863656282424927 16.690046310424805
MemoryTrain:  epoch  9, batch    12 | loss: 19.3800144Losses:  22.455955505371094 0.5173826217651367 19.836477279663086
MemoryTrain:  epoch  9, batch    13 | loss: 22.4559555Losses:  25.297685623168945 0.23180173337459564 23.093435287475586
MemoryTrain:  epoch  9, batch    14 | loss: 25.2976856Losses:  7.417466163635254 -0.0 5.570738315582275
MemoryTrain:  epoch  9, batch    15 | loss: 7.4174662
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 59.82%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:    8 | acc: 37.50%,  total acc: 59.72%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 62.50%   
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 56.25%,  total acc: 52.08%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 53.75%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 54.17%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 12.50%,  total acc: 46.09%   [EVAL] batch:    8 | acc: 25.00%,  total acc: 43.75%   [EVAL] batch:    9 | acc: 6.25%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 12.50%,  total acc: 37.50%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 36.98%   [EVAL] batch:   12 | acc: 12.50%,  total acc: 35.10%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 34.38%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 36.67%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 37.89%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 40.07%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 41.32%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 42.43%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 44.69%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 47.02%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 49.15%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 51.36%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 53.12%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 55.00%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 56.49%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 57.87%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 59.38%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 60.78%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 61.46%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 63.67%   [EVAL] batch:   32 | acc: 37.50%,  total acc: 62.88%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 61.03%   [EVAL] batch:   34 | acc: 0.00%,  total acc: 59.29%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 57.64%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 56.08%   [EVAL] batch:   37 | acc: 0.00%,  total acc: 54.61%   [EVAL] batch:   38 | acc: 31.25%,  total acc: 54.01%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 54.57%   [EVAL] batch:   41 | acc: 0.00%,  total acc: 53.27%   [EVAL] batch:   42 | acc: 12.50%,  total acc: 52.33%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 52.70%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 53.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 54.76%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 55.72%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 56.64%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 57.53%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 58.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 58.70%   [EVAL] batch:   51 | acc: 62.50%,  total acc: 58.77%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 58.96%   [EVAL] batch:   53 | acc: 87.50%,  total acc: 59.49%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 59.89%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 60.04%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 60.64%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 61.31%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 61.76%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 62.40%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 63.01%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 63.61%   [EVAL] batch:   62 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:   63 | acc: 100.00%,  total acc: 64.75%   [EVAL] batch:   64 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 65.06%   [EVAL] batch:   66 | acc: 56.25%,  total acc: 64.93%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 65.07%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 65.49%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 65.67%   [EVAL] batch:   71 | acc: 37.50%,  total acc: 65.28%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 65.33%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 65.29%   [EVAL] batch:   74 | acc: 43.75%,  total acc: 65.00%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 65.21%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 65.50%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 66.14%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 66.28%   [EVAL] batch:   81 | acc: 0.00%,  total acc: 65.47%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 64.76%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 64.06%   [EVAL] batch:   84 | acc: 12.50%,  total acc: 63.46%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 63.37%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 63.43%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 63.42%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 63.20%   [EVAL] batch:   89 | acc: 31.25%,  total acc: 62.85%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 62.64%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 62.70%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 63.04%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 63.36%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 63.55%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 63.80%   [EVAL] batch:   96 | acc: 81.25%,  total acc: 63.98%   [EVAL] batch:   97 | acc: 87.50%,  total acc: 64.22%   [EVAL] batch:   98 | acc: 25.00%,  total acc: 63.83%   [EVAL] batch:   99 | acc: 18.75%,  total acc: 63.38%   [EVAL] batch:  100 | acc: 12.50%,  total acc: 62.87%   [EVAL] batch:  101 | acc: 6.25%,  total acc: 62.32%   [EVAL] batch:  102 | acc: 31.25%,  total acc: 62.01%   [EVAL] batch:  103 | acc: 31.25%,  total acc: 61.72%   [EVAL] batch:  104 | acc: 37.50%,  total acc: 61.49%   [EVAL] batch:  105 | acc: 31.25%,  total acc: 61.20%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 60.86%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 60.59%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 60.38%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 60.23%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 60.02%   [EVAL] batch:  111 | acc: 62.50%,  total acc: 60.04%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 59.96%   [EVAL] batch:  113 | acc: 37.50%,  total acc: 59.76%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 59.62%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 59.54%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 59.40%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 59.32%   [EVAL] batch:  118 | acc: 43.75%,  total acc: 59.19%   [EVAL] batch:  119 | acc: 56.25%,  total acc: 59.17%   [EVAL] batch:  120 | acc: 31.25%,  total acc: 58.94%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 58.91%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 58.94%   [EVAL] batch:  123 | acc: 50.00%,  total acc: 58.87%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 59.05%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 59.08%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 59.15%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 59.33%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 59.21%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 59.18%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 59.30%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 59.52%   [EVAL] batch:  132 | acc: 56.25%,  total acc: 59.49%   
cur_acc:  ['0.8655', '0.7257', '0.6719', '0.8884', '0.8083', '0.8080', '0.5312', '0.6250']
his_acc:  ['0.8655', '0.8300', '0.7368', '0.7042', '0.7147', '0.6749', '0.6188', '0.5949']
----------END
his_acc mean:  [0.8523 0.8131 0.764  0.7116 0.6658 0.6499 0.6004 0.5755]
