#############params############
cuda
Task=FewRel, 5-shot
Encoding model: bert
pattern=hybridprompt
mem=1, margin=0.3, gen=1, gen_num=2
#############params############
--------Round  0
seed:  100
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 3 0 5 4 1 6 2]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 2 0 3 3 2 0 1 1]
Losses:  18.05565643310547 3.954390525817871 1.2517082691192627
CurrentTrain: epoch  0, batch     0 | loss: 18.0556564Losses:  19.840600967407227 5.942726135253906 1.1852174997329712
CurrentTrain: epoch  0, batch     1 | loss: 19.8406010Losses:  18.791484832763672 4.904645919799805 1.1385606527328491
CurrentTrain: epoch  0, batch     2 | loss: 18.7914848Losses:  19.86072540283203 5.966850280761719 1.1662479639053345
CurrentTrain: epoch  0, batch     3 | loss: 19.8607254Losses:  20.74845314025879 7.332503318786621 1.090196132659912
CurrentTrain: epoch  0, batch     4 | loss: 20.7484531Losses:  24.18556022644043 11.11424446105957 1.0374085903167725
CurrentTrain: epoch  0, batch     5 | loss: 24.1855602Losses:  20.936132431030273 7.961305618286133 0.9684990048408508
CurrentTrain: epoch  0, batch     6 | loss: 20.9361324Losses:  19.653478622436523 6.648677349090576 0.9946036338806152
CurrentTrain: epoch  0, batch     7 | loss: 19.6534786Losses:  16.458478927612305 3.6782279014587402 0.9916484951972961
CurrentTrain: epoch  0, batch     8 | loss: 16.4584789Losses:  16.684768676757812 4.226747035980225 0.9133108854293823
CurrentTrain: epoch  0, batch     9 | loss: 16.6847687Losses:  16.083101272583008 3.7331347465515137 0.9355766773223877
CurrentTrain: epoch  0, batch    10 | loss: 16.0831013Losses:  17.570865631103516 5.68967342376709 0.7837113738059998
CurrentTrain: epoch  0, batch    11 | loss: 17.5708656Losses:  16.86347007751465 4.8212409019470215 0.8108647465705872
CurrentTrain: epoch  0, batch    12 | loss: 16.8634701Losses:  17.756752014160156 5.92067289352417 0.7906646132469177
CurrentTrain: epoch  0, batch    13 | loss: 17.7567520Losses:  16.92249870300293 5.593482971191406 0.7269817590713501
CurrentTrain: epoch  0, batch    14 | loss: 16.9224987Losses:  16.942184448242188 5.363801002502441 0.7354592084884644
CurrentTrain: epoch  0, batch    15 | loss: 16.9421844Losses:  20.26289939880371 8.824090957641602 0.5293682813644409
CurrentTrain: epoch  0, batch    16 | loss: 20.2628994Losses:  17.618690490722656 6.086968898773193 0.6551016569137573
CurrentTrain: epoch  0, batch    17 | loss: 17.6186905Losses:  14.447784423828125 3.5462446212768555 0.6313069462776184
CurrentTrain: epoch  0, batch    18 | loss: 14.4477844Losses:  15.900558471679688 4.860014915466309 0.6191061735153198
CurrentTrain: epoch  0, batch    19 | loss: 15.9005585Losses:  16.842205047607422 5.679265975952148 0.5821693539619446
CurrentTrain: epoch  0, batch    20 | loss: 16.8422050Losses:  16.722043991088867 5.477838039398193 0.5780974626541138
CurrentTrain: epoch  0, batch    21 | loss: 16.7220440Losses:  16.741044998168945 5.754072189331055 0.5815764665603638
CurrentTrain: epoch  0, batch    22 | loss: 16.7410450Losses:  16.834447860717773 5.183246612548828 0.550274133682251
CurrentTrain: epoch  0, batch    23 | loss: 16.8344479Losses:  16.345462799072266 5.241796970367432 0.5498508214950562
CurrentTrain: epoch  0, batch    24 | loss: 16.3454628Losses:  15.182644844055176 4.247592926025391 0.5831515192985535
CurrentTrain: epoch  0, batch    25 | loss: 15.1826448Losses:  16.317703247070312 4.894707202911377 0.5554420351982117
CurrentTrain: epoch  0, batch    26 | loss: 16.3177032Losses:  15.721659660339355 5.187131881713867 0.527880072593689
CurrentTrain: epoch  0, batch    27 | loss: 15.7216597Losses:  14.944753646850586 4.324740409851074 0.47412383556365967
CurrentTrain: epoch  0, batch    28 | loss: 14.9447536Losses:  17.715801239013672 6.9432268142700195 0.5657763481140137
CurrentTrain: epoch  0, batch    29 | loss: 17.7158012Losses:  15.347006797790527 5.075684070587158 0.4706566333770752
CurrentTrain: epoch  0, batch    30 | loss: 15.3470068Losses:  15.529281616210938 5.109592437744141 0.5145713686943054
CurrentTrain: epoch  0, batch    31 | loss: 15.5292816Losses:  14.449108123779297 3.58577036857605 0.5514494180679321
CurrentTrain: epoch  0, batch    32 | loss: 14.4491081Losses:  14.788524627685547 4.912840843200684 0.44878458976745605
CurrentTrain: epoch  0, batch    33 | loss: 14.7885246Losses:  19.351394653320312 8.980777740478516 0.5235037803649902
CurrentTrain: epoch  0, batch    34 | loss: 19.3513947Losses:  14.906949996948242 4.812798023223877 0.49957263469696045
CurrentTrain: epoch  0, batch    35 | loss: 14.9069500Losses:  13.524665832519531 3.755087375640869 0.5073087811470032
CurrentTrain: epoch  0, batch    36 | loss: 13.5246658Losses:  14.919900894165039 4.305388927459717 0.5270096063613892
CurrentTrain: epoch  0, batch    37 | loss: 14.9199009Losses:  13.894047737121582 4.3472723960876465 0.4991423487663269
CurrentTrain: epoch  0, batch    38 | loss: 13.8940477Losses:  14.183494567871094 4.184627056121826 0.4864307641983032
CurrentTrain: epoch  0, batch    39 | loss: 14.1834946Losses:  16.525402069091797 6.634921550750732 0.49615490436553955
CurrentTrain: epoch  0, batch    40 | loss: 16.5254021Losses:  16.128562927246094 6.398826599121094 0.4683496654033661
CurrentTrain: epoch  0, batch    41 | loss: 16.1285629Losses:  14.094996452331543 4.028508186340332 0.4771576523780823
CurrentTrain: epoch  0, batch    42 | loss: 14.0949965Losses:  14.328564643859863 4.8764495849609375 0.4511657655239105
CurrentTrain: epoch  0, batch    43 | loss: 14.3285646Losses:  14.22475814819336 4.511984825134277 0.4744505286216736
CurrentTrain: epoch  0, batch    44 | loss: 14.2247581Losses:  12.447259902954102 3.1957786083221436 0.4581652581691742
CurrentTrain: epoch  0, batch    45 | loss: 12.4472599Losses:  13.092209815979004 3.9044644832611084 0.44211333990097046
CurrentTrain: epoch  0, batch    46 | loss: 13.0922098Losses:  12.864480018615723 3.7196860313415527 0.4230199158191681
CurrentTrain: epoch  0, batch    47 | loss: 12.8644800Losses:  13.818281173706055 4.322075843811035 0.4369809031486511
CurrentTrain: epoch  0, batch    48 | loss: 13.8182812Losses:  11.717428207397461 2.7337443828582764 0.416020929813385
CurrentTrain: epoch  0, batch    49 | loss: 11.7174282Losses:  11.702716827392578 3.160010814666748 0.3995134234428406
CurrentTrain: epoch  0, batch    50 | loss: 11.7027168Losses:  13.641990661621094 4.764345645904541 0.43573546409606934
CurrentTrain: epoch  0, batch    51 | loss: 13.6419907Losses:  13.845843315124512 5.2093000411987305 0.41099458932876587
CurrentTrain: epoch  0, batch    52 | loss: 13.8458433Losses:  15.744937896728516 6.084953308105469 0.282726526260376
CurrentTrain: epoch  0, batch    53 | loss: 15.7449379Losses:  12.534147262573242 3.7526745796203613 0.40299761295318604
CurrentTrain: epoch  0, batch    54 | loss: 12.5341473Losses:  13.81438159942627 4.6356306076049805 0.41571828722953796
CurrentTrain: epoch  0, batch    55 | loss: 13.8143816Losses:  13.91964054107666 5.091861724853516 0.3923419415950775
CurrentTrain: epoch  0, batch    56 | loss: 13.9196405Losses:  10.462418556213379 2.126812219619751 0.39577358961105347
CurrentTrain: epoch  0, batch    57 | loss: 10.4624186Losses:  12.159643173217773 3.25441312789917 0.3752116858959198
CurrentTrain: epoch  0, batch    58 | loss: 12.1596432Losses:  13.389841079711914 4.996017932891846 0.3639904260635376
CurrentTrain: epoch  0, batch    59 | loss: 13.3898411Losses:  11.83028793334961 3.6925673484802246 0.3793452978134155
CurrentTrain: epoch  0, batch    60 | loss: 11.8302879Losses:  12.01400089263916 3.524538278579712 0.38832753896713257
CurrentTrain: epoch  0, batch    61 | loss: 12.0140009Losses:  8.625536918640137 2.0342178344726562 0.21327100694179535
CurrentTrain: epoch  0, batch    62 | loss: 8.6255369Losses:  13.390934944152832 4.901991844177246 0.3741875886917114
CurrentTrain: epoch  1, batch     0 | loss: 13.3909349Losses:  10.692917823791504 2.9286317825317383 0.35909831523895264
CurrentTrain: epoch  1, batch     1 | loss: 10.6929178Losses:  13.933050155639648 5.486928939819336 0.42007604241371155
CurrentTrain: epoch  1, batch     2 | loss: 13.9330502Losses:  13.005988121032715 5.266496658325195 0.36152184009552
CurrentTrain: epoch  1, batch     3 | loss: 13.0059881Losses:  10.951292991638184 2.9215407371520996 0.35714614391326904
CurrentTrain: epoch  1, batch     4 | loss: 10.9512930Losses:  10.946918487548828 3.0978174209594727 0.37395286560058594
CurrentTrain: epoch  1, batch     5 | loss: 10.9469185Losses:  10.65574836730957 3.079350471496582 0.34592780470848083
CurrentTrain: epoch  1, batch     6 | loss: 10.6557484Losses:  19.530107498168945 12.14589786529541 0.37156787514686584
CurrentTrain: epoch  1, batch     7 | loss: 19.5301075Losses:  15.12357234954834 7.164176940917969 0.4078711271286011
CurrentTrain: epoch  1, batch     8 | loss: 15.1235723Losses:  11.537735939025879 4.0868988037109375 0.362192839384079
CurrentTrain: epoch  1, batch     9 | loss: 11.5377359Losses:  13.497538566589355 5.083099365234375 0.26263266801834106
CurrentTrain: epoch  1, batch    10 | loss: 13.4975386Losses:  15.120135307312012 6.403343200683594 0.29955336451530457
CurrentTrain: epoch  1, batch    11 | loss: 15.1201353Losses:  13.003473281860352 4.685625076293945 0.3650991916656494
CurrentTrain: epoch  1, batch    12 | loss: 13.0034733Losses:  11.318224906921387 3.5515191555023193 0.3905722200870514
CurrentTrain: epoch  1, batch    13 | loss: 11.3182249Losses:  12.96396541595459 5.095099925994873 0.34971123933792114
CurrentTrain: epoch  1, batch    14 | loss: 12.9639654Losses:  14.244179725646973 5.0546464920043945 0.28853997588157654
CurrentTrain: epoch  1, batch    15 | loss: 14.2441797Losses:  11.381749153137207 3.770123243331909 0.374495267868042
CurrentTrain: epoch  1, batch    16 | loss: 11.3817492Losses:  12.249104499816895 4.484167575836182 0.34312936663627625
CurrentTrain: epoch  1, batch    17 | loss: 12.2491045Losses:  12.235405921936035 4.4176106452941895 0.3476744294166565
CurrentTrain: epoch  1, batch    18 | loss: 12.2354059Losses:  12.372736930847168 4.883211135864258 0.38066643476486206
CurrentTrain: epoch  1, batch    19 | loss: 12.3727369Losses:  11.758617401123047 3.5202646255493164 0.3575664460659027
CurrentTrain: epoch  1, batch    20 | loss: 11.7586174Losses:  11.612332344055176 3.8895816802978516 0.33561116456985474
CurrentTrain: epoch  1, batch    21 | loss: 11.6123323Losses:  10.351113319396973 2.5386080741882324 0.3461597263813019
CurrentTrain: epoch  1, batch    22 | loss: 10.3511133Losses:  11.287415504455566 3.8546080589294434 0.36222538352012634
CurrentTrain: epoch  1, batch    23 | loss: 11.2874155Losses:  11.530862808227539 4.0442795753479 0.3503984212875366
CurrentTrain: epoch  1, batch    24 | loss: 11.5308628Losses:  14.783525466918945 7.039511203765869 0.3316877484321594
CurrentTrain: epoch  1, batch    25 | loss: 14.7835255Losses:  13.856697082519531 6.046417236328125 0.24158696830272675
CurrentTrain: epoch  1, batch    26 | loss: 13.8566971Losses:  10.133402824401855 2.6244864463806152 0.3418954014778137
CurrentTrain: epoch  1, batch    27 | loss: 10.1334028Losses:  9.598034858703613 2.381924629211426 0.3478962481021881
CurrentTrain: epoch  1, batch    28 | loss: 9.5980349Losses:  10.637125968933105 3.4645509719848633 0.35720932483673096
CurrentTrain: epoch  1, batch    29 | loss: 10.6371260Losses:  10.881745338439941 3.394033193588257 0.3150085210800171
CurrentTrain: epoch  1, batch    30 | loss: 10.8817453Losses:  10.899420738220215 3.5329904556274414 0.36046916246414185
CurrentTrain: epoch  1, batch    31 | loss: 10.8994207Losses:  11.133832931518555 4.631799221038818 0.3110272288322449
CurrentTrain: epoch  1, batch    32 | loss: 11.1338329Losses:  10.831731796264648 3.7263131141662598 0.3165227770805359
CurrentTrain: epoch  1, batch    33 | loss: 10.8317318Losses:  10.448527336120605 2.8351948261260986 0.34548720717430115
CurrentTrain: epoch  1, batch    34 | loss: 10.4485273Losses:  12.676469802856445 4.060534477233887 0.3609464764595032
CurrentTrain: epoch  1, batch    35 | loss: 12.6764698Losses:  11.602148056030273 4.563503265380859 0.3313794434070587
CurrentTrain: epoch  1, batch    36 | loss: 11.6021481Losses:  10.66096019744873 3.5973615646362305 0.32014405727386475
CurrentTrain: epoch  1, batch    37 | loss: 10.6609602Losses:  13.508003234863281 6.075915336608887 0.2820066213607788
CurrentTrain: epoch  1, batch    38 | loss: 13.5080032Losses:  11.126378059387207 3.9530181884765625 0.31930866837501526
CurrentTrain: epoch  1, batch    39 | loss: 11.1263781Losses:  10.405496597290039 3.305013656616211 0.3599892556667328
CurrentTrain: epoch  1, batch    40 | loss: 10.4054966Losses:  8.91991901397705 2.055246353149414 0.31736087799072266
CurrentTrain: epoch  1, batch    41 | loss: 8.9199190Losses:  12.088906288146973 5.244719505310059 0.3166181743144989
CurrentTrain: epoch  1, batch    42 | loss: 12.0889063Losses:  10.004539489746094 2.7977218627929688 0.32948458194732666
CurrentTrain: epoch  1, batch    43 | loss: 10.0045395Losses:  13.082381248474121 4.997838497161865 0.32690751552581787
CurrentTrain: epoch  1, batch    44 | loss: 13.0823812Losses:  13.447881698608398 5.93748664855957 0.314361035823822
CurrentTrain: epoch  1, batch    45 | loss: 13.4478817Losses:  8.805232048034668 2.475771427154541 0.3133057951927185
CurrentTrain: epoch  1, batch    46 | loss: 8.8052320Losses:  11.34009075164795 5.047955513000488 0.29721927642822266
CurrentTrain: epoch  1, batch    47 | loss: 11.3400908Losses:  10.009668350219727 3.0894978046417236 0.31306490302085876
CurrentTrain: epoch  1, batch    48 | loss: 10.0096684Losses:  11.821269989013672 4.889798164367676 0.31255728006362915
CurrentTrain: epoch  1, batch    49 | loss: 11.8212700Losses:  11.455153465270996 4.662269592285156 0.32306763529777527
CurrentTrain: epoch  1, batch    50 | loss: 11.4551535Losses:  9.843701362609863 3.0297088623046875 0.31357109546661377
CurrentTrain: epoch  1, batch    51 | loss: 9.8437014Losses:  11.610215187072754 4.179100513458252 0.31145527958869934
CurrentTrain: epoch  1, batch    52 | loss: 11.6102152Losses:  14.734128952026367 6.910172462463379 0.3333267271518707
CurrentTrain: epoch  1, batch    53 | loss: 14.7341290Losses:  10.687994956970215 3.699089527130127 0.2876841425895691
CurrentTrain: epoch  1, batch    54 | loss: 10.6879950Losses:  10.818699836730957 3.5884809494018555 0.2981586456298828
CurrentTrain: epoch  1, batch    55 | loss: 10.8186998Losses:  9.557745933532715 3.2123630046844482 0.2916635572910309
CurrentTrain: epoch  1, batch    56 | loss: 9.5577459Losses:  12.336512565612793 4.814808368682861 0.32088783383369446
CurrentTrain: epoch  1, batch    57 | loss: 12.3365126Losses:  9.31208610534668 2.9905853271484375 0.30781090259552
CurrentTrain: epoch  1, batch    58 | loss: 9.3120861Losses:  11.04507827758789 3.6563775539398193 0.23431715369224548
CurrentTrain: epoch  1, batch    59 | loss: 11.0450783Losses:  12.262353897094727 5.661705493927002 0.30832457542419434
CurrentTrain: epoch  1, batch    60 | loss: 12.2623539Losses:  8.455020904541016 3.1359808444976807 0.2981734573841095
CurrentTrain: epoch  1, batch    61 | loss: 8.4550209Losses:  5.647775173187256 0.312038391828537 0.2758840024471283
CurrentTrain: epoch  1, batch    62 | loss: 5.6477752Losses:  9.425322532653809 2.724081516265869 0.28233209252357483
CurrentTrain: epoch  2, batch     0 | loss: 9.4253225Losses:  11.059261322021484 6.125279426574707 0.196634441614151
CurrentTrain: epoch  2, batch     1 | loss: 11.0592613Losses:  9.483811378479004 3.16463303565979 0.28825071454048157
CurrentTrain: epoch  2, batch     2 | loss: 9.4838114Losses:  12.603262901306152 5.1553239822387695 0.23696176707744598
CurrentTrain: epoch  2, batch     3 | loss: 12.6032629Losses:  10.597084045410156 3.7826452255249023 0.2030077427625656
CurrentTrain: epoch  2, batch     4 | loss: 10.5970840Losses:  9.203935623168945 3.095956325531006 0.300216943025589
CurrentTrain: epoch  2, batch     5 | loss: 9.2039356Losses:  9.286136627197266 2.8473997116088867 0.290670245885849
CurrentTrain: epoch  2, batch     6 | loss: 9.2861366Losses:  11.00507640838623 4.501906871795654 0.32829177379608154
CurrentTrain: epoch  2, batch     7 | loss: 11.0050764Losses:  8.763952255249023 2.953695058822632 0.28304243087768555
CurrentTrain: epoch  2, batch     8 | loss: 8.7639523Losses:  8.405610084533691 2.501262664794922 0.29566359519958496
CurrentTrain: epoch  2, batch     9 | loss: 8.4056101Losses:  9.255729675292969 3.0953757762908936 0.30743205547332764
CurrentTrain: epoch  2, batch    10 | loss: 9.2557297Losses:  11.213643074035645 5.280467987060547 0.29592353105545044
CurrentTrain: epoch  2, batch    11 | loss: 11.2136431Losses:  9.179168701171875 3.3757972717285156 0.28578007221221924
CurrentTrain: epoch  2, batch    12 | loss: 9.1791687Losses:  9.117902755737305 3.3617517948150635 0.31769636273384094
CurrentTrain: epoch  2, batch    13 | loss: 9.1179028Losses:  11.698936462402344 5.257525444030762 0.3139244318008423
CurrentTrain: epoch  2, batch    14 | loss: 11.6989365Losses:  9.774189949035645 3.546933650970459 0.2866273522377014
CurrentTrain: epoch  2, batch    15 | loss: 9.7741899Losses:  8.521427154541016 2.873904228210449 0.27231264114379883
CurrentTrain: epoch  2, batch    16 | loss: 8.5214272Losses:  9.814095497131348 4.259330749511719 0.2836984395980835
CurrentTrain: epoch  2, batch    17 | loss: 9.8140955Losses:  12.928837776184082 5.679064750671387 0.2787139117717743
CurrentTrain: epoch  2, batch    18 | loss: 12.9288378Losses:  10.69393539428711 4.909784317016602 0.276846319437027
CurrentTrain: epoch  2, batch    19 | loss: 10.6939354Losses:  9.936068534851074 4.104526519775391 0.28040778636932373
CurrentTrain: epoch  2, batch    20 | loss: 9.9360685Losses:  11.250113487243652 5.035378932952881 0.299945205450058
CurrentTrain: epoch  2, batch    21 | loss: 11.2501135Losses:  10.537131309509277 4.380205154418945 0.3316887617111206
CurrentTrain: epoch  2, batch    22 | loss: 10.5371313Losses:  8.023085594177246 2.3115196228027344 0.2758325934410095
CurrentTrain: epoch  2, batch    23 | loss: 8.0230856Losses:  10.575960159301758 4.326345443725586 0.2788657248020172
CurrentTrain: epoch  2, batch    24 | loss: 10.5759602Losses:  8.768264770507812 2.5864884853363037 0.26976561546325684
CurrentTrain: epoch  2, batch    25 | loss: 8.7682648Losses:  8.287980079650879 2.279960870742798 0.2751680612564087
CurrentTrain: epoch  2, batch    26 | loss: 8.2879801Losses:  11.624621391296387 5.258459091186523 0.2737889587879181
CurrentTrain: epoch  2, batch    27 | loss: 11.6246214Losses:  12.088713645935059 5.492206573486328 0.30812522768974304
CurrentTrain: epoch  2, batch    28 | loss: 12.0887136Losses:  9.379855155944824 3.103188991546631 0.27457624673843384
CurrentTrain: epoch  2, batch    29 | loss: 9.3798552Losses:  9.475505828857422 2.9486966133117676 0.2869269847869873
CurrentTrain: epoch  2, batch    30 | loss: 9.4755058Losses:  8.546126365661621 2.5655736923217773 0.2907685339450836
CurrentTrain: epoch  2, batch    31 | loss: 8.5461264Losses:  8.95437240600586 2.9053025245666504 0.2713940739631653
CurrentTrain: epoch  2, batch    32 | loss: 8.9543724Losses:  7.544178485870361 2.1201202869415283 0.26552823185920715
CurrentTrain: epoch  2, batch    33 | loss: 7.5441785Losses:  8.244817733764648 2.742363929748535 0.27481797337532043
CurrentTrain: epoch  2, batch    34 | loss: 8.2448177Losses:  9.60440731048584 3.571964740753174 0.27975594997406006
CurrentTrain: epoch  2, batch    35 | loss: 9.6044073Losses:  9.569501876831055 2.9921679496765137 0.29095935821533203
CurrentTrain: epoch  2, batch    36 | loss: 9.5695019Losses:  12.412542343139648 5.777660846710205 0.28593605756759644
CurrentTrain: epoch  2, batch    37 | loss: 12.4125423Losses:  11.064291000366211 4.923569679260254 0.18716609477996826
CurrentTrain: epoch  2, batch    38 | loss: 11.0642910Losses:  8.624131202697754 2.894369125366211 0.28799569606781006
CurrentTrain: epoch  2, batch    39 | loss: 8.6241312Losses:  8.76270580291748 3.4782094955444336 0.2729051113128662
CurrentTrain: epoch  2, batch    40 | loss: 8.7627058Losses:  9.722145080566406 3.555586099624634 0.25998932123184204
CurrentTrain: epoch  2, batch    41 | loss: 9.7221451Losses:  8.486417770385742 2.9928066730499268 0.2688678503036499
CurrentTrain: epoch  2, batch    42 | loss: 8.4864178Losses:  8.549138069152832 2.5509278774261475 0.2831155061721802
CurrentTrain: epoch  2, batch    43 | loss: 8.5491381Losses:  9.097951889038086 2.537109375 0.28628867864608765
CurrentTrain: epoch  2, batch    44 | loss: 9.0979519Losses:  8.631983757019043 2.469651222229004 0.30967751145362854
CurrentTrain: epoch  2, batch    45 | loss: 8.6319838Losses:  8.030292510986328 2.464995861053467 0.26447993516921997
CurrentTrain: epoch  2, batch    46 | loss: 8.0302925Losses:  11.49835205078125 5.605391025543213 0.2023223638534546
CurrentTrain: epoch  2, batch    47 | loss: 11.4983521Losses:  10.332199096679688 3.725283622741699 0.26431000232696533
CurrentTrain: epoch  2, batch    48 | loss: 10.3321991Losses:  9.212198257446289 2.871868371963501 0.2889196276664734
CurrentTrain: epoch  2, batch    49 | loss: 9.2121983Losses:  7.945549964904785 2.4789068698883057 0.2901962399482727
CurrentTrain: epoch  2, batch    50 | loss: 7.9455500Losses:  10.619074821472168 5.195928573608398 0.29746073484420776
CurrentTrain: epoch  2, batch    51 | loss: 10.6190748Losses:  9.80255126953125 4.500540733337402 0.27068325877189636
CurrentTrain: epoch  2, batch    52 | loss: 9.8025513Losses:  7.554180145263672 2.276217460632324 0.27500247955322266
CurrentTrain: epoch  2, batch    53 | loss: 7.5541801Losses:  9.853412628173828 3.400991439819336 0.28259992599487305
CurrentTrain: epoch  2, batch    54 | loss: 9.8534126Losses:  7.667462348937988 2.4185028076171875 0.2606956362724304
CurrentTrain: epoch  2, batch    55 | loss: 7.6674623Losses:  8.057870864868164 2.6504557132720947 0.27757173776626587
CurrentTrain: epoch  2, batch    56 | loss: 8.0578709Losses:  8.89154052734375 3.4506258964538574 0.2897590398788452
CurrentTrain: epoch  2, batch    57 | loss: 8.8915405Losses:  7.775676727294922 2.7725534439086914 0.2674144506454468
CurrentTrain: epoch  2, batch    58 | loss: 7.7756767Losses:  8.609477043151855 2.784978151321411 0.2793142795562744
CurrentTrain: epoch  2, batch    59 | loss: 8.6094770Losses:  9.392634391784668 4.47862434387207 0.2921210825443268
CurrentTrain: epoch  2, batch    60 | loss: 9.3926344Losses:  9.497150421142578 4.39640474319458 0.2665504515171051
CurrentTrain: epoch  2, batch    61 | loss: 9.4971504Losses:  7.999617099761963 3.1763463020324707 0.31891128420829773
CurrentTrain: epoch  2, batch    62 | loss: 7.9996171Losses:  7.920961856842041 2.9614689350128174 0.2670714259147644
CurrentTrain: epoch  3, batch     0 | loss: 7.9209619Losses:  8.20043659210205 2.9361252784729004 0.2623433768749237
CurrentTrain: epoch  3, batch     1 | loss: 8.2004366Losses:  10.192178726196289 4.880774974822998 0.2753720283508301
CurrentTrain: epoch  3, batch     2 | loss: 10.1921787Losses:  7.716468811035156 2.4132938385009766 0.26665198802948
CurrentTrain: epoch  3, batch     3 | loss: 7.7164688Losses:  12.262310981750488 6.936686038970947 0.1781342327594757
CurrentTrain: epoch  3, batch     4 | loss: 12.2623110Losses:  7.958322048187256 2.801870822906494 0.25682884454727173
CurrentTrain: epoch  3, batch     5 | loss: 7.9583220Losses:  8.30501651763916 3.2587432861328125 0.26371878385543823
CurrentTrain: epoch  3, batch     6 | loss: 8.3050165Losses:  8.211118698120117 3.2847933769226074 0.26426732540130615
CurrentTrain: epoch  3, batch     7 | loss: 8.2111187Losses:  9.050280570983887 3.794691801071167 0.2635831832885742
CurrentTrain: epoch  3, batch     8 | loss: 9.0502806Losses:  8.298121452331543 2.6638667583465576 0.2775754928588867
CurrentTrain: epoch  3, batch     9 | loss: 8.2981215Losses:  8.303970336914062 2.93571400642395 0.2689487040042877
CurrentTrain: epoch  3, batch    10 | loss: 8.3039703Losses:  9.609076499938965 3.8380255699157715 0.29607266187667847
CurrentTrain: epoch  3, batch    11 | loss: 9.6090765Losses:  9.826047897338867 4.671196937561035 0.26158392429351807
CurrentTrain: epoch  3, batch    12 | loss: 9.8260479Losses:  6.928624629974365 2.0206692218780518 0.24458187818527222
CurrentTrain: epoch  3, batch    13 | loss: 6.9286246Losses:  12.166232109069824 7.178184509277344 0.2894282639026642
CurrentTrain: epoch  3, batch    14 | loss: 12.1662321Losses:  9.276881217956543 3.471835136413574 0.279456228017807
CurrentTrain: epoch  3, batch    15 | loss: 9.2768812Losses:  7.96932315826416 3.1747336387634277 0.25918376445770264
CurrentTrain: epoch  3, batch    16 | loss: 7.9693232Losses:  10.085054397583008 4.615042686462402 0.18676650524139404
CurrentTrain: epoch  3, batch    17 | loss: 10.0850544Losses:  7.876948356628418 2.4921910762786865 0.24860092997550964
CurrentTrain: epoch  3, batch    18 | loss: 7.8769484Losses:  11.193325996398926 5.26271915435791 0.26692479848861694
CurrentTrain: epoch  3, batch    19 | loss: 11.1933260Losses:  8.085081100463867 2.825277328491211 0.2587537169456482
CurrentTrain: epoch  3, batch    20 | loss: 8.0850811Losses:  7.815696716308594 2.5827527046203613 0.26937052607536316
CurrentTrain: epoch  3, batch    21 | loss: 7.8156967Losses:  7.305779457092285 1.8512965440750122 0.25877171754837036
CurrentTrain: epoch  3, batch    22 | loss: 7.3057795Losses:  9.396713256835938 4.448691368103027 0.24925260245800018
CurrentTrain: epoch  3, batch    23 | loss: 9.3967133Losses:  7.607126235961914 2.858628749847412 0.24975994229316711
CurrentTrain: epoch  3, batch    24 | loss: 7.6071262Losses:  8.864683151245117 3.5724880695343018 0.2544521391391754
CurrentTrain: epoch  3, batch    25 | loss: 8.8646832Losses:  9.764374732971191 4.416604042053223 0.25700199604034424
CurrentTrain: epoch  3, batch    26 | loss: 9.7643747Losses:  11.135671615600586 6.1511549949646 0.30011796951293945
CurrentTrain: epoch  3, batch    27 | loss: 11.1356716Losses:  9.292617797851562 2.7767016887664795 0.24985061585903168
CurrentTrain: epoch  3, batch    28 | loss: 9.2926178Losses:  8.197755813598633 2.7380504608154297 0.24561229348182678
CurrentTrain: epoch  3, batch    29 | loss: 8.1977558Losses:  7.716095924377441 2.7686283588409424 0.24664101004600525
CurrentTrain: epoch  3, batch    30 | loss: 7.7160959Losses:  9.60718822479248 3.963318347930908 0.26364967226982117
CurrentTrain: epoch  3, batch    31 | loss: 9.6071882Losses:  9.091198921203613 2.8314943313598633 0.2519843876361847
CurrentTrain: epoch  3, batch    32 | loss: 9.0911989Losses:  8.099130630493164 2.9487123489379883 0.25097477436065674
CurrentTrain: epoch  3, batch    33 | loss: 8.0991306Losses:  7.999680519104004 2.954850673675537 0.25132906436920166
CurrentTrain: epoch  3, batch    34 | loss: 7.9996805Losses:  7.802492618560791 2.7583816051483154 0.23598036170005798
CurrentTrain: epoch  3, batch    35 | loss: 7.8024926Losses:  7.2650909423828125 2.5251383781433105 0.2394254207611084
CurrentTrain: epoch  3, batch    36 | loss: 7.2650909Losses:  7.910919189453125 2.2968552112579346 0.2449694275856018
CurrentTrain: epoch  3, batch    37 | loss: 7.9109192Losses:  8.006985664367676 3.0879874229431152 0.2594420909881592
CurrentTrain: epoch  3, batch    38 | loss: 8.0069857Losses:  8.676770210266113 3.760718822479248 0.250643789768219
CurrentTrain: epoch  3, batch    39 | loss: 8.6767702Losses:  7.551182746887207 2.971560001373291 0.2575014531612396
CurrentTrain: epoch  3, batch    40 | loss: 7.5511827Losses:  6.694580554962158 1.4614362716674805 0.23517093062400818
CurrentTrain: epoch  3, batch    41 | loss: 6.6945806Losses:  9.301605224609375 3.95271372795105 0.2736216187477112
CurrentTrain: epoch  3, batch    42 | loss: 9.3016052Losses:  10.167824745178223 5.222002983093262 0.24262449145317078
CurrentTrain: epoch  3, batch    43 | loss: 10.1678247Losses:  7.853436470031738 2.4947710037231445 0.25295960903167725
CurrentTrain: epoch  3, batch    44 | loss: 7.8534365Losses:  8.84117603302002 3.1828534603118896 0.2497754842042923
CurrentTrain: epoch  3, batch    45 | loss: 8.8411760Losses:  10.15905475616455 3.792752742767334 0.24968639016151428
CurrentTrain: epoch  3, batch    46 | loss: 10.1590548Losses:  6.422684192657471 1.4038057327270508 0.23624855279922485
CurrentTrain: epoch  3, batch    47 | loss: 6.4226842Losses:  9.629194259643555 4.488803863525391 0.2572367191314697
CurrentTrain: epoch  3, batch    48 | loss: 9.6291943Losses:  9.04118537902832 4.007529258728027 0.2694302797317505
CurrentTrain: epoch  3, batch    49 | loss: 9.0411854Losses:  7.8646039962768555 2.573274612426758 0.2582498788833618
CurrentTrain: epoch  3, batch    50 | loss: 7.8646040Losses:  8.482440948486328 3.1124072074890137 0.26211392879486084
CurrentTrain: epoch  3, batch    51 | loss: 8.4824409Losses:  7.460841655731201 2.4817285537719727 0.2521907389163971
CurrentTrain: epoch  3, batch    52 | loss: 7.4608417Losses:  8.706679344177246 3.134486675262451 0.25846096873283386
CurrentTrain: epoch  3, batch    53 | loss: 8.7066793Losses:  7.9636054039001465 2.9159786701202393 0.23777931928634644
CurrentTrain: epoch  3, batch    54 | loss: 7.9636054Losses:  8.154122352600098 2.79649019241333 0.26250773668289185
CurrentTrain: epoch  3, batch    55 | loss: 8.1541224Losses:  10.533994674682617 4.793350696563721 0.2465572953224182
CurrentTrain: epoch  3, batch    56 | loss: 10.5339947Losses:  9.191943168640137 3.6466591358184814 0.24229134619235992
CurrentTrain: epoch  3, batch    57 | loss: 9.1919432Losses:  8.611635208129883 3.352626323699951 0.15854990482330322
CurrentTrain: epoch  3, batch    58 | loss: 8.6116352Losses:  12.919037818908691 8.221729278564453 0.2873103618621826
CurrentTrain: epoch  3, batch    59 | loss: 12.9190378Losses:  14.2465238571167 9.317098617553711 0.18275707960128784
CurrentTrain: epoch  3, batch    60 | loss: 14.2465239Losses:  7.527853012084961 2.5512585639953613 0.2565785348415375
CurrentTrain: epoch  3, batch    61 | loss: 7.5278530Losses:  7.224390029907227 1.5984127521514893 0.27127936482429504
CurrentTrain: epoch  3, batch    62 | loss: 7.2243900Losses:  8.772940635681152 3.4703867435455322 0.268379271030426
CurrentTrain: epoch  4, batch     0 | loss: 8.7729406Losses:  6.759002685546875 1.9944242238998413 0.2502082288265228
CurrentTrain: epoch  4, batch     1 | loss: 6.7590027Losses:  7.768886566162109 2.511974334716797 0.24892327189445496
CurrentTrain: epoch  4, batch     2 | loss: 7.7688866Losses:  7.984659194946289 3.4245057106018066 0.2593892514705658
CurrentTrain: epoch  4, batch     3 | loss: 7.9846592Losses:  8.180346488952637 3.478517532348633 0.24931202828884125
CurrentTrain: epoch  4, batch     4 | loss: 8.1803465Losses:  8.75210189819336 3.530640125274658 0.25473570823669434
CurrentTrain: epoch  4, batch     5 | loss: 8.7521019Losses:  9.722753524780273 4.826878547668457 0.2510809600353241
CurrentTrain: epoch  4, batch     6 | loss: 9.7227535Losses:  8.988014221191406 3.314333915710449 0.25784969329833984
CurrentTrain: epoch  4, batch     7 | loss: 8.9880142Losses:  7.949725151062012 3.0971078872680664 0.2619049549102783
CurrentTrain: epoch  4, batch     8 | loss: 7.9497252Losses:  8.146688461303711 3.306589126586914 0.24483908712863922
CurrentTrain: epoch  4, batch     9 | loss: 8.1466885Losses:  8.6259183883667 3.4660873413085938 0.27747076749801636
CurrentTrain: epoch  4, batch    10 | loss: 8.6259184Losses:  7.728598117828369 2.573244571685791 0.24914853274822235
CurrentTrain: epoch  4, batch    11 | loss: 7.7285981Losses:  6.5145721435546875 1.7980402708053589 0.23587152361869812
CurrentTrain: epoch  4, batch    12 | loss: 6.5145721Losses:  7.505375862121582 2.3611013889312744 0.25223520398139954
CurrentTrain: epoch  4, batch    13 | loss: 7.5053759Losses:  7.676176071166992 2.96120023727417 0.24186046421527863
CurrentTrain: epoch  4, batch    14 | loss: 7.6761761Losses:  10.024116516113281 4.696984767913818 0.16159102320671082
CurrentTrain: epoch  4, batch    15 | loss: 10.0241165Losses:  9.686192512512207 5.200831413269043 0.17704078555107117
CurrentTrain: epoch  4, batch    16 | loss: 9.6861925Losses:  7.787142753601074 3.271845817565918 0.2556886076927185
CurrentTrain: epoch  4, batch    17 | loss: 7.7871428Losses:  7.282235622406006 2.5475428104400635 0.2448926717042923
CurrentTrain: epoch  4, batch    18 | loss: 7.2822356Losses:  6.1910786628723145 1.3910512924194336 0.23396211862564087
CurrentTrain: epoch  4, batch    19 | loss: 6.1910787Losses:  7.588500022888184 2.922844409942627 0.25570252537727356
CurrentTrain: epoch  4, batch    20 | loss: 7.5885000Losses:  6.519560813903809 1.7476924657821655 0.2471100389957428
CurrentTrain: epoch  4, batch    21 | loss: 6.5195608Losses:  7.487027645111084 2.7548680305480957 0.244477316737175
CurrentTrain: epoch  4, batch    22 | loss: 7.4870276Losses:  7.616837501525879 2.6347904205322266 0.2507355213165283
CurrentTrain: epoch  4, batch    23 | loss: 7.6168375Losses:  8.599165916442871 2.481916904449463 0.23558150231838226
CurrentTrain: epoch  4, batch    24 | loss: 8.5991659Losses:  7.336076736450195 2.6805148124694824 0.24796368181705475
CurrentTrain: epoch  4, batch    25 | loss: 7.3360767Losses:  6.610196590423584 2.0326576232910156 0.23249509930610657
CurrentTrain: epoch  4, batch    26 | loss: 6.6101966Losses:  10.41596508026123 5.743296146392822 0.1600579470396042
CurrentTrain: epoch  4, batch    27 | loss: 10.4159651Losses:  7.292211532592773 2.663088321685791 0.24051928520202637
CurrentTrain: epoch  4, batch    28 | loss: 7.2922115Losses:  9.101841926574707 4.545555114746094 0.23783373832702637
CurrentTrain: epoch  4, batch    29 | loss: 9.1018419Losses:  12.83462905883789 7.979026794433594 0.19902193546295166
CurrentTrain: epoch  4, batch    30 | loss: 12.8346291Losses:  8.630361557006836 4.175297737121582 0.23815953731536865
CurrentTrain: epoch  4, batch    31 | loss: 8.6303616Losses:  6.879528522491455 2.3326454162597656 0.23548263311386108
CurrentTrain: epoch  4, batch    32 | loss: 6.8795285Losses:  10.55222225189209 5.661803722381592 0.18847763538360596
CurrentTrain: epoch  4, batch    33 | loss: 10.5522223Losses:  9.42071533203125 4.5192108154296875 0.24549701809883118
CurrentTrain: epoch  4, batch    34 | loss: 9.4207153Losses:  9.89538860321045 4.233133316040039 0.24432414770126343
CurrentTrain: epoch  4, batch    35 | loss: 9.8953886Losses:  8.737101554870605 4.0677714347839355 0.26083624362945557
CurrentTrain: epoch  4, batch    36 | loss: 8.7371016Losses:  7.65491247177124 2.972550868988037 0.2488536387681961
CurrentTrain: epoch  4, batch    37 | loss: 7.6549125Losses:  8.627272605895996 3.9927456378936768 0.2475750744342804
CurrentTrain: epoch  4, batch    38 | loss: 8.6272726Losses:  6.533752918243408 1.9921826124191284 0.2313465178012848
CurrentTrain: epoch  4, batch    39 | loss: 6.5337529Losses:  10.282848358154297 4.99477481842041 0.2420097291469574
CurrentTrain: epoch  4, batch    40 | loss: 10.2828484Losses:  8.962493896484375 4.58271598815918 0.24347925186157227
CurrentTrain: epoch  4, batch    41 | loss: 8.9624939Losses:  6.868406772613525 2.1333470344543457 0.2487470954656601
CurrentTrain: epoch  4, batch    42 | loss: 6.8684068Losses:  7.942883014678955 3.0084967613220215 0.24870656430721283
CurrentTrain: epoch  4, batch    43 | loss: 7.9428830Losses:  6.795774936676025 2.29817533493042 0.23520666360855103
CurrentTrain: epoch  4, batch    44 | loss: 6.7957749Losses:  7.530431270599365 2.901484727859497 0.23558956384658813
CurrentTrain: epoch  4, batch    45 | loss: 7.5304313Losses:  7.257638931274414 2.458266258239746 0.24096666276454926
CurrentTrain: epoch  4, batch    46 | loss: 7.2576389Losses:  7.2011566162109375 2.2664248943328857 0.228170245885849
CurrentTrain: epoch  4, batch    47 | loss: 7.2011566Losses:  10.53359317779541 6.05872917175293 0.27736204862594604
CurrentTrain: epoch  4, batch    48 | loss: 10.5335932Losses:  8.449965476989746 3.6259052753448486 0.24643607437610626
CurrentTrain: epoch  4, batch    49 | loss: 8.4499655Losses:  8.8849515914917 4.174269676208496 0.2423274666070938
CurrentTrain: epoch  4, batch    50 | loss: 8.8849516Losses:  7.953251361846924 2.9182448387145996 0.24628299474716187
CurrentTrain: epoch  4, batch    51 | loss: 7.9532514Losses:  10.323533058166504 5.790197372436523 0.26419514417648315
CurrentTrain: epoch  4, batch    52 | loss: 10.3235331Losses:  7.70005989074707 3.003530979156494 0.25271838903427124
CurrentTrain: epoch  4, batch    53 | loss: 7.7000599Losses:  8.033902168273926 3.3303306102752686 0.2491239607334137
CurrentTrain: epoch  4, batch    54 | loss: 8.0339022Losses:  6.910429954528809 2.3460381031036377 0.2532541751861572
CurrentTrain: epoch  4, batch    55 | loss: 6.9104300Losses:  9.163541793823242 3.9970626831054688 0.23929329216480255
CurrentTrain: epoch  4, batch    56 | loss: 9.1635418Losses:  7.708582878112793 2.5950112342834473 0.23585262894630432
CurrentTrain: epoch  4, batch    57 | loss: 7.7085829Losses:  9.565133094787598 4.9367594718933105 0.26949793100357056
CurrentTrain: epoch  4, batch    58 | loss: 9.5651331Losses:  9.140718460083008 4.23283576965332 0.2551150918006897
CurrentTrain: epoch  4, batch    59 | loss: 9.1407185Losses:  7.791704177856445 3.5247011184692383 0.15219157934188843
CurrentTrain: epoch  4, batch    60 | loss: 7.7917042Losses:  9.23807144165039 4.586687088012695 0.25164225697517395
CurrentTrain: epoch  4, batch    61 | loss: 9.2380714Losses:  4.800027370452881 0.25139835476875305 0.2489965558052063
CurrentTrain: epoch  4, batch    62 | loss: 4.8000274Losses:  13.208906173706055 8.775975227355957 0.25731828808784485
CurrentTrain: epoch  5, batch     0 | loss: 13.2089062Losses:  8.38998031616211 3.997440814971924 0.23421865701675415
CurrentTrain: epoch  5, batch     1 | loss: 8.3899803Losses:  6.295895576477051 1.861586570739746 0.23220723867416382
CurrentTrain: epoch  5, batch     2 | loss: 6.2958956Losses:  6.895591735839844 2.424081325531006 0.2366662174463272
CurrentTrain: epoch  5, batch     3 | loss: 6.8955917Losses:  6.901224136352539 2.465944766998291 0.2310636043548584
CurrentTrain: epoch  5, batch     4 | loss: 6.9012241Losses:  7.370429039001465 2.644960403442383 0.24290865659713745
CurrentTrain: epoch  5, batch     5 | loss: 7.3704290Losses:  8.46493911743164 3.122741937637329 0.2369770109653473
CurrentTrain: epoch  5, batch     6 | loss: 8.4649391Losses:  8.31838607788086 3.8486080169677734 0.24661076068878174
CurrentTrain: epoch  5, batch     7 | loss: 8.3183861Losses:  7.936347961425781 3.4833855628967285 0.2637021243572235
CurrentTrain: epoch  5, batch     8 | loss: 7.9363480Losses:  8.840391159057617 4.401730537414551 0.14987611770629883
CurrentTrain: epoch  5, batch     9 | loss: 8.8403912Losses:  8.331567764282227 3.8232851028442383 0.25367116928100586
CurrentTrain: epoch  5, batch    10 | loss: 8.3315678Losses:  7.56313943862915 3.087961196899414 0.2440706342458725
CurrentTrain: epoch  5, batch    11 | loss: 7.5631394Losses:  7.983784198760986 3.1770801544189453 0.25252866744995117
CurrentTrain: epoch  5, batch    12 | loss: 7.9837842Losses:  7.1247382164001465 2.6914892196655273 0.22453701496124268
CurrentTrain: epoch  5, batch    13 | loss: 7.1247382Losses:  7.304622173309326 2.9013724327087402 0.2552588880062103
CurrentTrain: epoch  5, batch    14 | loss: 7.3046222Losses:  8.136673927307129 3.158174991607666 0.24749678373336792
CurrentTrain: epoch  5, batch    15 | loss: 8.1366739Losses:  8.135160446166992 3.6660962104797363 0.2311679720878601
CurrentTrain: epoch  5, batch    16 | loss: 8.1351604Losses:  13.111480712890625 8.52273178100586 0.25272366404533386
CurrentTrain: epoch  5, batch    17 | loss: 13.1114807Losses:  6.632153511047363 2.004443883895874 0.24319350719451904
CurrentTrain: epoch  5, batch    18 | loss: 6.6321535Losses:  7.204261779785156 2.66907000541687 0.2324458807706833
CurrentTrain: epoch  5, batch    19 | loss: 7.2042618Losses:  6.890812873840332 2.5028676986694336 0.22717183828353882
CurrentTrain: epoch  5, batch    20 | loss: 6.8908129Losses:  7.251247882843018 2.466554880142212 0.24430230259895325
CurrentTrain: epoch  5, batch    21 | loss: 7.2512479Losses:  6.956927299499512 2.212378740310669 0.24171265959739685
CurrentTrain: epoch  5, batch    22 | loss: 6.9569273Losses:  6.661983966827393 2.195955276489258 0.23408840596675873
CurrentTrain: epoch  5, batch    23 | loss: 6.6619840Losses:  9.638670921325684 5.1344451904296875 0.2553290128707886
CurrentTrain: epoch  5, batch    24 | loss: 9.6386709Losses:  9.369771957397461 4.9776201248168945 0.25556135177612305
CurrentTrain: epoch  5, batch    25 | loss: 9.3697720Losses:  7.3766374588012695 2.9141578674316406 0.2385072410106659
CurrentTrain: epoch  5, batch    26 | loss: 7.3766375Losses:  8.825944900512695 4.287574768066406 0.25239020586013794
CurrentTrain: epoch  5, batch    27 | loss: 8.8259449Losses:  7.091130256652832 2.521622657775879 0.22690176963806152
CurrentTrain: epoch  5, batch    28 | loss: 7.0911303Losses:  6.685434818267822 2.2844161987304688 0.22319352626800537
CurrentTrain: epoch  5, batch    29 | loss: 6.6854348Losses:  7.4778289794921875 3.056006908416748 0.2570520341396332
CurrentTrain: epoch  5, batch    30 | loss: 7.4778290Losses:  9.745344161987305 5.327836036682129 0.2511126399040222
CurrentTrain: epoch  5, batch    31 | loss: 9.7453442Losses:  6.651228904724121 2.1770122051239014 0.22838880121707916
CurrentTrain: epoch  5, batch    32 | loss: 6.6512289Losses:  8.71699333190918 3.6424601078033447 0.23648323118686676
CurrentTrain: epoch  5, batch    33 | loss: 8.7169933Losses:  9.706148147583008 4.969261169433594 0.2519163489341736
CurrentTrain: epoch  5, batch    34 | loss: 9.7061481Losses:  10.038992881774902 5.474562644958496 0.26339781284332275
CurrentTrain: epoch  5, batch    35 | loss: 10.0389929Losses:  8.493388175964355 4.081335067749023 0.252513587474823
CurrentTrain: epoch  5, batch    36 | loss: 8.4933882Losses:  8.481741905212402 3.9349584579467773 0.24861212074756622
CurrentTrain: epoch  5, batch    37 | loss: 8.4817419Losses:  8.411786079406738 4.119972229003906 0.15280139446258545
CurrentTrain: epoch  5, batch    38 | loss: 8.4117861Losses:  6.716157913208008 2.2719554901123047 0.2260178178548813
CurrentTrain: epoch  5, batch    39 | loss: 6.7161579Losses:  6.800960540771484 2.2477715015411377 0.23616503179073334
CurrentTrain: epoch  5, batch    40 | loss: 6.8009605Losses:  7.3960065841674805 2.8105993270874023 0.23303233087062836
CurrentTrain: epoch  5, batch    41 | loss: 7.3960066Losses:  7.602996826171875 3.2769291400909424 0.23739446699619293
CurrentTrain: epoch  5, batch    42 | loss: 7.6029968Losses:  6.28916597366333 1.5577976703643799 0.22972716391086578
CurrentTrain: epoch  5, batch    43 | loss: 6.2891660Losses:  9.965179443359375 5.550844192504883 0.2534142732620239
CurrentTrain: epoch  5, batch    44 | loss: 9.9651794Losses:  9.72408390045166 5.360199928283691 0.1529485136270523
CurrentTrain: epoch  5, batch    45 | loss: 9.7240839Losses:  7.422055244445801 3.018280506134033 0.22244957089424133
CurrentTrain: epoch  5, batch    46 | loss: 7.4220552Losses:  6.08246374130249 1.6561065912246704 0.22006887197494507
CurrentTrain: epoch  5, batch    47 | loss: 6.0824637Losses:  7.613430976867676 3.250941276550293 0.235286146402359
CurrentTrain: epoch  5, batch    48 | loss: 7.6134310Losses:  7.461465835571289 1.9551249742507935 0.21602988243103027
CurrentTrain: epoch  5, batch    49 | loss: 7.4614658Losses:  7.454220294952393 2.810779094696045 0.22186461091041565
CurrentTrain: epoch  5, batch    50 | loss: 7.4542203Losses:  7.2269816398620605 2.169796943664551 0.23128758370876312
CurrentTrain: epoch  5, batch    51 | loss: 7.2269816Losses:  6.489232540130615 2.0190885066986084 0.21619579195976257
CurrentTrain: epoch  5, batch    52 | loss: 6.4892325Losses:  8.395750999450684 3.519735097885132 0.23850923776626587
CurrentTrain: epoch  5, batch    53 | loss: 8.3957510Losses:  10.235973358154297 5.872988700866699 0.25606364011764526
CurrentTrain: epoch  5, batch    54 | loss: 10.2359734Losses:  6.9262590408325195 2.0516717433929443 0.22525039315223694
CurrentTrain: epoch  5, batch    55 | loss: 6.9262590Losses:  9.153050422668457 4.750604152679443 0.26366931200027466
CurrentTrain: epoch  5, batch    56 | loss: 9.1530504Losses:  6.355401515960693 1.9417588710784912 0.23167827725410461
CurrentTrain: epoch  5, batch    57 | loss: 6.3554015Losses:  8.524957656860352 3.2277722358703613 0.2529950439929962
CurrentTrain: epoch  5, batch    58 | loss: 8.5249577Losses:  9.051444053649902 3.709195137023926 0.2455413043498993
CurrentTrain: epoch  5, batch    59 | loss: 9.0514441Losses:  11.641342163085938 6.727254867553711 0.25198712944984436
CurrentTrain: epoch  5, batch    60 | loss: 11.6413422Losses:  8.607233047485352 4.294856071472168 0.23573468625545502
CurrentTrain: epoch  5, batch    61 | loss: 8.6072330Losses:  4.892742156982422 0.4916926622390747 0.16111551225185394
CurrentTrain: epoch  5, batch    62 | loss: 4.8927422Losses:  6.586861610412598 2.188831329345703 0.22414343059062958
CurrentTrain: epoch  6, batch     0 | loss: 6.5868616Losses:  7.648778438568115 2.5831007957458496 0.23132625222206116
CurrentTrain: epoch  6, batch     1 | loss: 7.6487784Losses:  11.523750305175781 6.969918251037598 0.26406481862068176
CurrentTrain: epoch  6, batch     2 | loss: 11.5237503Losses:  9.768440246582031 4.916332244873047 0.23347130417823792
CurrentTrain: epoch  6, batch     3 | loss: 9.7684402Losses:  7.7555155754089355 2.7118630409240723 0.2436804324388504
CurrentTrain: epoch  6, batch     4 | loss: 7.7555156Losses:  9.55187702178955 5.228945732116699 0.2507118284702301
CurrentTrain: epoch  6, batch     5 | loss: 9.5518770Losses:  6.982315540313721 2.5946431159973145 0.23483482003211975
CurrentTrain: epoch  6, batch     6 | loss: 6.9823155Losses:  8.66431999206543 3.688079595565796 0.24996505677700043
CurrentTrain: epoch  6, batch     7 | loss: 8.6643200Losses:  8.611922264099121 3.8474321365356445 0.2587129771709442
CurrentTrain: epoch  6, batch     8 | loss: 8.6119223Losses:  6.971110820770264 2.467292308807373 0.2453230768442154
CurrentTrain: epoch  6, batch     9 | loss: 6.9711108Losses:  7.91615104675293 3.587949275970459 0.17426347732543945
CurrentTrain: epoch  6, batch    10 | loss: 7.9161510Losses:  8.473841667175293 3.8077480792999268 0.2545235753059387
CurrentTrain: epoch  6, batch    11 | loss: 8.4738417Losses:  6.66506290435791 2.3026280403137207 0.22765523195266724
CurrentTrain: epoch  6, batch    12 | loss: 6.6650629Losses:  8.60090160369873 4.284965515136719 0.2589351534843445
CurrentTrain: epoch  6, batch    13 | loss: 8.6009016Losses:  7.530338764190674 2.818406105041504 0.24686463177204132
CurrentTrain: epoch  6, batch    14 | loss: 7.5303388Losses:  7.530410289764404 3.2081408500671387 0.24626079201698303
CurrentTrain: epoch  6, batch    15 | loss: 7.5304103Losses:  7.482839107513428 3.0033411979675293 0.23399078845977783
CurrentTrain: epoch  6, batch    16 | loss: 7.4828391Losses:  10.341700553894043 6.040156364440918 0.25799497961997986
CurrentTrain: epoch  6, batch    17 | loss: 10.3417006Losses:  7.542992115020752 3.2971205711364746 0.23444248735904694
CurrentTrain: epoch  6, batch    18 | loss: 7.5429921Losses:  9.20203971862793 4.800942420959473 0.26244351267814636
CurrentTrain: epoch  6, batch    19 | loss: 9.2020397Losses:  8.615151405334473 4.357773780822754 0.2392323911190033
CurrentTrain: epoch  6, batch    20 | loss: 8.6151514Losses:  6.813488960266113 2.439448118209839 0.23602469265460968
CurrentTrain: epoch  6, batch    21 | loss: 6.8134890Losses:  8.74973201751709 4.409811019897461 0.23185378313064575
CurrentTrain: epoch  6, batch    22 | loss: 8.7497320Losses:  5.916617393493652 1.609367847442627 0.21546311676502228
CurrentTrain: epoch  6, batch    23 | loss: 5.9166174Losses:  7.132862567901611 2.8308424949645996 0.14772410690784454
CurrentTrain: epoch  6, batch    24 | loss: 7.1328626Losses:  8.09355640411377 3.469977855682373 0.25435537099838257
CurrentTrain: epoch  6, batch    25 | loss: 8.0935564Losses:  6.9074201583862305 2.4697964191436768 0.21945500373840332
CurrentTrain: epoch  6, batch    26 | loss: 6.9074202Losses:  10.739679336547852 6.267294883728027 0.17658478021621704
CurrentTrain: epoch  6, batch    27 | loss: 10.7396793Losses:  6.122245788574219 1.8399319648742676 0.22634130716323853
CurrentTrain: epoch  6, batch    28 | loss: 6.1222458Losses:  6.791927337646484 2.430881977081299 0.23532015085220337
CurrentTrain: epoch  6, batch    29 | loss: 6.7919273Losses:  7.802116394042969 3.225527048110962 0.2456103265285492
CurrentTrain: epoch  6, batch    30 | loss: 7.8021164Losses:  8.590383529663086 3.9906229972839355 0.16269519925117493
CurrentTrain: epoch  6, batch    31 | loss: 8.5903835Losses:  7.327775478363037 3.005013942718506 0.231381356716156
CurrentTrain: epoch  6, batch    32 | loss: 7.3277755Losses:  8.469755172729492 3.9935426712036133 0.16264232993125916
CurrentTrain: epoch  6, batch    33 | loss: 8.4697552Losses:  7.680968761444092 3.323274850845337 0.22958523035049438
CurrentTrain: epoch  6, batch    34 | loss: 7.6809688Losses:  6.296874046325684 1.9526574611663818 0.21632687747478485
CurrentTrain: epoch  6, batch    35 | loss: 6.2968740Losses:  6.783374309539795 2.483905792236328 0.24058271944522858
CurrentTrain: epoch  6, batch    36 | loss: 6.7833743Losses:  6.864493370056152 2.5175766944885254 0.24848084151744843
CurrentTrain: epoch  6, batch    37 | loss: 6.8644934Losses:  6.43463134765625 2.223783016204834 0.2215918004512787
CurrentTrain: epoch  6, batch    38 | loss: 6.4346313Losses:  7.5398783683776855 3.0639636516571045 0.24233104288578033
CurrentTrain: epoch  6, batch    39 | loss: 7.5398784Losses:  6.684295177459717 2.3998241424560547 0.23260194063186646
CurrentTrain: epoch  6, batch    40 | loss: 6.6842952Losses:  6.616941928863525 2.226651191711426 0.2214619368314743
CurrentTrain: epoch  6, batch    41 | loss: 6.6169419Losses:  7.006903648376465 2.6759567260742188 0.22421061992645264
CurrentTrain: epoch  6, batch    42 | loss: 7.0069036Losses:  9.545736312866211 4.2121124267578125 0.2349502146244049
CurrentTrain: epoch  6, batch    43 | loss: 9.5457363Losses:  6.512643814086914 1.9551242589950562 0.22066865861415863
CurrentTrain: epoch  6, batch    44 | loss: 6.5126438Losses:  6.735648155212402 2.4449217319488525 0.22533205151557922
CurrentTrain: epoch  6, batch    45 | loss: 6.7356482Losses:  6.269936561584473 1.596698522567749 0.2164270579814911
CurrentTrain: epoch  6, batch    46 | loss: 6.2699366Losses:  7.419645309448242 3.1429409980773926 0.2402888834476471
CurrentTrain: epoch  6, batch    47 | loss: 7.4196453Losses:  6.064341068267822 1.8091144561767578 0.21956408023834229
CurrentTrain: epoch  6, batch    48 | loss: 6.0643411Losses:  7.23158597946167 2.2766385078430176 0.2304191142320633
CurrentTrain: epoch  6, batch    49 | loss: 7.2315860Losses:  6.102384090423584 1.844474196434021 0.21756714582443237
CurrentTrain: epoch  6, batch    50 | loss: 6.1023841Losses:  7.521081924438477 3.2299017906188965 0.24613124132156372
CurrentTrain: epoch  6, batch    51 | loss: 7.5210819Losses:  6.604344367980957 1.8835563659667969 0.22732454538345337
CurrentTrain: epoch  6, batch    52 | loss: 6.6043444Losses:  7.9544219970703125 3.1940488815307617 0.22740447521209717
CurrentTrain: epoch  6, batch    53 | loss: 7.9544220Losses:  8.623750686645508 4.385281562805176 0.2366018295288086
CurrentTrain: epoch  6, batch    54 | loss: 8.6237507Losses:  7.207622528076172 2.981043577194214 0.2153170108795166
CurrentTrain: epoch  6, batch    55 | loss: 7.2076225Losses:  6.984684944152832 2.209514856338501 0.22934076189994812
CurrentTrain: epoch  6, batch    56 | loss: 6.9846849Losses:  7.630138874053955 2.7996890544891357 0.2261548638343811
CurrentTrain: epoch  6, batch    57 | loss: 7.6301389Losses:  6.645741939544678 2.221485137939453 0.2249084711074829
CurrentTrain: epoch  6, batch    58 | loss: 6.6457419Losses:  8.8360013961792 4.541856288909912 0.24093538522720337
CurrentTrain: epoch  6, batch    59 | loss: 8.8360014Losses:  6.684408187866211 2.2240383625030518 0.2182961106300354
CurrentTrain: epoch  6, batch    60 | loss: 6.6844082Losses:  6.803587436676025 2.5126519203186035 0.23185306787490845
CurrentTrain: epoch  6, batch    61 | loss: 6.8035874Losses:  4.720740795135498 0.4503958225250244 0.24911817908287048
CurrentTrain: epoch  6, batch    62 | loss: 4.7207408Losses:  6.786643981933594 2.346067428588867 0.22648262977600098
CurrentTrain: epoch  7, batch     0 | loss: 6.7866440Losses:  6.630246639251709 2.214505672454834 0.22463499009609222
CurrentTrain: epoch  7, batch     1 | loss: 6.6302466Losses:  7.315202713012695 3.031174659729004 0.2431107461452484
CurrentTrain: epoch  7, batch     2 | loss: 7.3152027Losses:  7.971565246582031 3.5651373863220215 0.2331395298242569
CurrentTrain: epoch  7, batch     3 | loss: 7.9715652Losses:  6.480006694793701 2.1862449645996094 0.2244822084903717
CurrentTrain: epoch  7, batch     4 | loss: 6.4800067Losses:  6.992979526519775 2.7628650665283203 0.22558733820915222
CurrentTrain: epoch  7, batch     5 | loss: 6.9929795Losses:  6.181901454925537 1.8185386657714844 0.22156375646591187
CurrentTrain: epoch  7, batch     6 | loss: 6.1819015Losses:  8.74508285522461 4.461965560913086 0.2412414848804474
CurrentTrain: epoch  7, batch     7 | loss: 8.7450829Losses:  7.341598987579346 3.0385305881500244 0.22637417912483215
CurrentTrain: epoch  7, batch     8 | loss: 7.3415990Losses:  10.0278902053833 5.787823677062988 0.25099462270736694
CurrentTrain: epoch  7, batch     9 | loss: 10.0278902Losses:  7.8015570640563965 3.473536968231201 0.2247357815504074
CurrentTrain: epoch  7, batch    10 | loss: 7.8015571Losses:  7.723148345947266 3.4201858043670654 0.24553225934505463
CurrentTrain: epoch  7, batch    11 | loss: 7.7231483Losses:  7.575705528259277 3.2414047718048096 0.24590373039245605
CurrentTrain: epoch  7, batch    12 | loss: 7.5757055Losses:  6.045590877532959 1.6032979488372803 0.21649213135242462
CurrentTrain: epoch  7, batch    13 | loss: 6.0455909Losses:  6.488296985626221 2.1893866062164307 0.22917978465557098
CurrentTrain: epoch  7, batch    14 | loss: 6.4882970Losses:  7.070501804351807 2.7554333209991455 0.2363048642873764
CurrentTrain: epoch  7, batch    15 | loss: 7.0705018Losses:  8.734458923339844 4.396112442016602 0.2453296184539795
CurrentTrain: epoch  7, batch    16 | loss: 8.7344589Losses:  7.917058944702148 3.303464412689209 0.2422507107257843
CurrentTrain: epoch  7, batch    17 | loss: 7.9170589Losses:  7.312597751617432 2.792985200881958 0.22746513783931732
CurrentTrain: epoch  7, batch    18 | loss: 7.3125978Losses:  6.8882269859313965 2.613168716430664 0.21828602254390717
CurrentTrain: epoch  7, batch    19 | loss: 6.8882270Losses:  7.289235591888428 2.6548256874084473 0.21896548569202423
CurrentTrain: epoch  7, batch    20 | loss: 7.2892356Losses:  7.067048072814941 2.7539563179016113 0.23422861099243164
CurrentTrain: epoch  7, batch    21 | loss: 7.0670481Losses:  7.668802261352539 3.205930233001709 0.23455697298049927
CurrentTrain: epoch  7, batch    22 | loss: 7.6688023Losses:  6.466973304748535 2.173030376434326 0.2225758135318756
CurrentTrain: epoch  7, batch    23 | loss: 6.4669733Losses:  9.994213104248047 5.504890441894531 0.26006877422332764
CurrentTrain: epoch  7, batch    24 | loss: 9.9942131Losses:  7.720322132110596 3.445887565612793 0.23820491135120392
CurrentTrain: epoch  7, batch    25 | loss: 7.7203221Losses:  8.967334747314453 4.72573184967041 0.15752005577087402
CurrentTrain: epoch  7, batch    26 | loss: 8.9673347Losses:  8.582773208618164 4.3582048416137695 0.23403999209403992
CurrentTrain: epoch  7, batch    27 | loss: 8.5827732Losses:  6.963294982910156 2.5734686851501465 0.23361074924468994
CurrentTrain: epoch  7, batch    28 | loss: 6.9632950Losses:  7.794462203979492 3.488114356994629 0.242652028799057
CurrentTrain: epoch  7, batch    29 | loss: 7.7944622Losses:  7.506725788116455 3.090088129043579 0.2166239619255066
CurrentTrain: epoch  7, batch    30 | loss: 7.5067258Losses:  6.027135372161865 1.8093464374542236 0.22122181951999664
CurrentTrain: epoch  7, batch    31 | loss: 6.0271354Losses:  6.979916095733643 2.732198715209961 0.22408443689346313
CurrentTrain: epoch  7, batch    32 | loss: 6.9799161Losses:  6.4644775390625 2.1666064262390137 0.21292582154273987
CurrentTrain: epoch  7, batch    33 | loss: 6.4644775Losses:  7.135073184967041 2.704164505004883 0.223279669880867
CurrentTrain: epoch  7, batch    34 | loss: 7.1350732Losses:  6.175482273101807 1.946529507637024 0.21875321865081787
CurrentTrain: epoch  7, batch    35 | loss: 6.1754823Losses:  11.108785629272461 6.682111740112305 0.2605302333831787
CurrentTrain: epoch  7, batch    36 | loss: 11.1087856Losses:  7.478273391723633 3.123350143432617 0.23765665292739868
CurrentTrain: epoch  7, batch    37 | loss: 7.4782734Losses:  7.162076950073242 2.7841124534606934 0.24421347677707672
CurrentTrain: epoch  7, batch    38 | loss: 7.1620770Losses:  6.055224895477295 1.7990710735321045 0.22601260244846344
CurrentTrain: epoch  7, batch    39 | loss: 6.0552249Losses:  7.948757648468018 3.753829002380371 0.2309320569038391
CurrentTrain: epoch  7, batch    40 | loss: 7.9487576Losses:  7.416678428649902 2.973862409591675 0.2400244176387787
CurrentTrain: epoch  7, batch    41 | loss: 7.4166784Losses:  6.49597692489624 2.2201507091522217 0.21919217705726624
CurrentTrain: epoch  7, batch    42 | loss: 6.4959769Losses:  7.5200300216674805 3.1815176010131836 0.24442453682422638
CurrentTrain: epoch  7, batch    43 | loss: 7.5200300Losses:  6.969125270843506 2.671949863433838 0.23053082823753357
CurrentTrain: epoch  7, batch    44 | loss: 6.9691253Losses:  6.42572546005249 2.099794864654541 0.14925740659236908
CurrentTrain: epoch  7, batch    45 | loss: 6.4257255Losses:  6.511998176574707 2.243826150894165 0.22651077806949615
CurrentTrain: epoch  7, batch    46 | loss: 6.5119982Losses:  6.613119602203369 2.4212284088134766 0.2154230773448944
CurrentTrain: epoch  7, batch    47 | loss: 6.6131196Losses:  7.962958335876465 3.5634608268737793 0.24267038702964783
CurrentTrain: epoch  7, batch    48 | loss: 7.9629583Losses:  6.83181095123291 2.5456953048706055 0.14401261508464813
CurrentTrain: epoch  7, batch    49 | loss: 6.8318110Losses:  7.4232964515686035 3.1564271450042725 0.23228411376476288
CurrentTrain: epoch  7, batch    50 | loss: 7.4232965Losses:  7.506242275238037 3.196377992630005 0.22978636622428894
CurrentTrain: epoch  7, batch    51 | loss: 7.5062423Losses:  6.238492965698242 2.0056328773498535 0.22132320702075958
CurrentTrain: epoch  7, batch    52 | loss: 6.2384930Losses:  6.8796868324279785 2.6332247257232666 0.21608301997184753
CurrentTrain: epoch  7, batch    53 | loss: 6.8796868Losses:  7.128005027770996 2.7893967628479004 0.23471477627754211
CurrentTrain: epoch  7, batch    54 | loss: 7.1280050Losses:  6.459712505340576 2.1393496990203857 0.22716867923736572
CurrentTrain: epoch  7, batch    55 | loss: 6.4597125Losses:  7.021213054656982 2.796036720275879 0.2251560539007187
CurrentTrain: epoch  7, batch    56 | loss: 7.0212131Losses:  8.045172691345215 2.5654053688049316 0.22442248463630676
CurrentTrain: epoch  7, batch    57 | loss: 8.0451727Losses:  7.566189289093018 3.358271598815918 0.23377934098243713
CurrentTrain: epoch  7, batch    58 | loss: 7.5661893Losses:  6.37180233001709 2.164539337158203 0.22460296750068665
CurrentTrain: epoch  7, batch    59 | loss: 6.3718023Losses:  9.180895805358887 4.735347747802734 0.2643752694129944
CurrentTrain: epoch  7, batch    60 | loss: 9.1808958Losses:  7.587215900421143 3.3172266483306885 0.24002599716186523
CurrentTrain: epoch  7, batch    61 | loss: 7.5872159Losses:  4.542929649353027 0.2345227748155594 0.2536771297454834
CurrentTrain: epoch  7, batch    62 | loss: 4.5429296Losses:  7.844629287719727 3.3688275814056396 0.23372915387153625
CurrentTrain: epoch  8, batch     0 | loss: 7.8446293Losses:  6.1891303062438965 1.9600954055786133 0.2142501324415207
CurrentTrain: epoch  8, batch     1 | loss: 6.1891303Losses:  10.235757827758789 5.106328964233398 0.235859215259552
CurrentTrain: epoch  8, batch     2 | loss: 10.2357578Losses:  7.28138542175293 2.6132566928863525 0.22453801333904266
CurrentTrain: epoch  8, batch     3 | loss: 7.2813854Losses:  8.108078002929688 3.1577794551849365 0.25723913311958313
CurrentTrain: epoch  8, batch     4 | loss: 8.1080780Losses:  8.208252906799316 3.930558204650879 0.24646581709384918
CurrentTrain: epoch  8, batch     5 | loss: 8.2082529Losses:  6.648734092712402 2.387906074523926 0.2316630333662033
CurrentTrain: epoch  8, batch     6 | loss: 6.6487341Losses:  6.439763069152832 2.1835615634918213 0.2202998399734497
CurrentTrain: epoch  8, batch     7 | loss: 6.4397631Losses:  7.805426120758057 3.5431203842163086 0.22679361701011658
CurrentTrain: epoch  8, batch     8 | loss: 7.8054261Losses:  7.1420745849609375 2.913722276687622 0.21854877471923828
CurrentTrain: epoch  8, batch     9 | loss: 7.1420746Losses:  8.682876586914062 4.052164077758789 0.14245443046092987
CurrentTrain: epoch  8, batch    10 | loss: 8.6828766Losses:  6.463708877563477 2.2457683086395264 0.218681201338768
CurrentTrain: epoch  8, batch    11 | loss: 6.4637089Losses:  6.397202014923096 2.1802520751953125 0.22772055864334106
CurrentTrain: epoch  8, batch    12 | loss: 6.3972020Losses:  8.872797012329102 4.607950210571289 0.24520991742610931
CurrentTrain: epoch  8, batch    13 | loss: 8.8727970Losses:  8.88733196258545 4.622621536254883 0.22995975613594055
CurrentTrain: epoch  8, batch    14 | loss: 8.8873320Losses:  6.168500900268555 1.7696752548217773 0.21978776156902313
CurrentTrain: epoch  8, batch    15 | loss: 6.1685009Losses:  6.73321533203125 2.430072069168091 0.2327602505683899
CurrentTrain: epoch  8, batch    16 | loss: 6.7332153Losses:  8.745026588439941 4.368647575378418 0.2334558069705963
CurrentTrain: epoch  8, batch    17 | loss: 8.7450266Losses:  6.879461765289307 2.3353195190429688 0.22601079940795898
CurrentTrain: epoch  8, batch    18 | loss: 6.8794618Losses:  6.028920650482178 1.8292884826660156 0.21620461344718933
CurrentTrain: epoch  8, batch    19 | loss: 6.0289207Losses:  6.137667655944824 1.9217820167541504 0.22204312682151794
CurrentTrain: epoch  8, batch    20 | loss: 6.1376677Losses:  7.101381301879883 2.722161054611206 0.23770995438098907
CurrentTrain: epoch  8, batch    21 | loss: 7.1013813Losses:  6.404509544372559 2.168142318725586 0.22710691392421722
CurrentTrain: epoch  8, batch    22 | loss: 6.4045095Losses:  6.823896884918213 2.645404815673828 0.2265334129333496
CurrentTrain: epoch  8, batch    23 | loss: 6.8238969Losses:  12.907366752624512 8.634312629699707 0.24368882179260254
CurrentTrain: epoch  8, batch    24 | loss: 12.9073668Losses:  6.578514099121094 2.348613977432251 0.23327088356018066
CurrentTrain: epoch  8, batch    25 | loss: 6.5785141Losses:  9.38913345336914 4.128868103027344 0.242219015955925
CurrentTrain: epoch  8, batch    26 | loss: 9.3891335Losses:  6.709829330444336 2.47725772857666 0.22875401377677917
CurrentTrain: epoch  8, batch    27 | loss: 6.7098293Losses:  6.445963382720947 1.9986720085144043 0.22008870542049408
CurrentTrain: epoch  8, batch    28 | loss: 6.4459634Losses:  7.404636383056641 2.7780725955963135 0.13457494974136353
CurrentTrain: epoch  8, batch    29 | loss: 7.4046364Losses:  7.235832214355469 2.8949053287506104 0.2302904576063156
CurrentTrain: epoch  8, batch    30 | loss: 7.2358322Losses:  6.062867164611816 1.9036720991134644 0.21563270688056946
CurrentTrain: epoch  8, batch    31 | loss: 6.0628672Losses:  6.406472682952881 2.1886725425720215 0.2325848489999771
CurrentTrain: epoch  8, batch    32 | loss: 6.4064727Losses:  7.7544074058532715 3.533653974533081 0.23274660110473633
CurrentTrain: epoch  8, batch    33 | loss: 7.7544074Losses:  9.429852485656738 5.172303199768066 0.24394108355045319
CurrentTrain: epoch  8, batch    34 | loss: 9.4298525Losses:  6.556070804595947 2.377958297729492 0.20987695455551147
CurrentTrain: epoch  8, batch    35 | loss: 6.5560708Losses:  6.2565836906433105 2.022099494934082 0.22832532227039337
CurrentTrain: epoch  8, batch    36 | loss: 6.2565837Losses:  7.099081039428711 2.9013724327087402 0.22576572000980377
CurrentTrain: epoch  8, batch    37 | loss: 7.0990810Losses:  7.443295001983643 3.2608704566955566 0.22561243176460266
CurrentTrain: epoch  8, batch    38 | loss: 7.4432950Losses:  7.371883869171143 3.1295034885406494 0.23666813969612122
CurrentTrain: epoch  8, batch    39 | loss: 7.3718839Losses:  5.979255199432373 1.7771586179733276 0.2204855978488922
CurrentTrain: epoch  8, batch    40 | loss: 5.9792552Losses:  6.592874526977539 2.2501564025878906 0.2281283289194107
CurrentTrain: epoch  8, batch    41 | loss: 6.5928745Losses:  7.29154634475708 3.0899810791015625 0.2292502224445343
CurrentTrain: epoch  8, batch    42 | loss: 7.2915463Losses:  9.407625198364258 5.210862636566162 0.23507344722747803
CurrentTrain: epoch  8, batch    43 | loss: 9.4076252Losses:  6.411294937133789 1.99326491355896 0.21865928173065186
CurrentTrain: epoch  8, batch    44 | loss: 6.4112949Losses:  6.8635640144348145 2.189427375793457 0.2149253785610199
CurrentTrain: epoch  8, batch    45 | loss: 6.8635640Losses:  6.071837425231934 1.6267521381378174 0.212080717086792
CurrentTrain: epoch  8, batch    46 | loss: 6.0718374Losses:  6.870182037353516 2.617508888244629 0.2253313660621643
CurrentTrain: epoch  8, batch    47 | loss: 6.8701820Losses:  7.7421345710754395 3.2963123321533203 0.23506075143814087
CurrentTrain: epoch  8, batch    48 | loss: 7.7421346Losses:  6.526947975158691 2.324828624725342 0.2294362485408783
CurrentTrain: epoch  8, batch    49 | loss: 6.5269480Losses:  7.178816795349121 2.873274326324463 0.235201895236969
CurrentTrain: epoch  8, batch    50 | loss: 7.1788168Losses:  7.2469000816345215 3.0120632648468018 0.214044988155365
CurrentTrain: epoch  8, batch    51 | loss: 7.2469001Losses:  6.880526065826416 2.674067974090576 0.23000812530517578
CurrentTrain: epoch  8, batch    52 | loss: 6.8805261Losses:  7.311111927032471 3.0881781578063965 0.22846664488315582
CurrentTrain: epoch  8, batch    53 | loss: 7.3111119Losses:  7.1703643798828125 2.959467649459839 0.2174166738986969
CurrentTrain: epoch  8, batch    54 | loss: 7.1703644Losses:  6.768101215362549 2.5716938972473145 0.22515389323234558
CurrentTrain: epoch  8, batch    55 | loss: 6.7681012Losses:  7.0215349197387695 2.683718681335449 0.23988988995552063
CurrentTrain: epoch  8, batch    56 | loss: 7.0215349Losses:  8.379018783569336 4.163344383239746 0.23962759971618652
CurrentTrain: epoch  8, batch    57 | loss: 8.3790188Losses:  7.98879337310791 3.725627899169922 0.2351050078868866
CurrentTrain: epoch  8, batch    58 | loss: 7.9887934Losses:  7.8909196853637695 3.634045124053955 0.2406248152256012
CurrentTrain: epoch  8, batch    59 | loss: 7.8909197Losses:  6.374656677246094 2.1749584674835205 0.21421250700950623
CurrentTrain: epoch  8, batch    60 | loss: 6.3746567Losses:  6.941145420074463 2.695969581604004 0.2315848171710968
CurrentTrain: epoch  8, batch    61 | loss: 6.9411454Losses:  7.086854457855225 2.8836679458618164 0.18569032847881317
CurrentTrain: epoch  8, batch    62 | loss: 7.0868545Losses:  7.253565788269043 3.002030372619629 0.22348028421401978
CurrentTrain: epoch  9, batch     0 | loss: 7.2535658Losses:  5.4759440422058105 1.2762001752853394 0.2127380669116974
CurrentTrain: epoch  9, batch     1 | loss: 5.4759440Losses:  7.411570072174072 3.2152132987976074 0.23386022448539734
CurrentTrain: epoch  9, batch     2 | loss: 7.4115701Losses:  6.255494594573975 2.1000335216522217 0.21992263197898865
CurrentTrain: epoch  9, batch     3 | loss: 6.2554946Losses:  7.071600437164307 2.8706703186035156 0.22017553448677063
CurrentTrain: epoch  9, batch     4 | loss: 7.0716004Losses:  8.128985404968262 3.888082265853882 0.2367229461669922
CurrentTrain: epoch  9, batch     5 | loss: 8.1289854Losses:  5.771003723144531 1.5522409677505493 0.21306203305721283
CurrentTrain: epoch  9, batch     6 | loss: 5.7710037Losses:  7.822518825531006 3.436098098754883 0.2379612922668457
CurrentTrain: epoch  9, batch     7 | loss: 7.8225188Losses:  6.9026875495910645 2.653587818145752 0.2399149388074875
CurrentTrain: epoch  9, batch     8 | loss: 6.9026875Losses:  7.149779796600342 2.910038948059082 0.24314850568771362
CurrentTrain: epoch  9, batch     9 | loss: 7.1497798Losses:  7.632585525512695 3.417219877243042 0.23425176739692688
CurrentTrain: epoch  9, batch    10 | loss: 7.6325855Losses:  8.334820747375488 4.149209022521973 0.2240636646747589
CurrentTrain: epoch  9, batch    11 | loss: 8.3348207Losses:  7.249997615814209 2.891928195953369 0.23961614072322845
CurrentTrain: epoch  9, batch    12 | loss: 7.2499976Losses:  8.408486366271973 4.141002655029297 0.24341605603694916
CurrentTrain: epoch  9, batch    13 | loss: 8.4084864Losses:  7.5433149337768555 3.289431571960449 0.23732484877109528
CurrentTrain: epoch  9, batch    14 | loss: 7.5433149Losses:  7.847987174987793 3.6525821685791016 0.22864317893981934
CurrentTrain: epoch  9, batch    15 | loss: 7.8479872Losses:  7.472012996673584 3.1736395359039307 0.22428451478481293
CurrentTrain: epoch  9, batch    16 | loss: 7.4720130Losses:  7.432674884796143 3.1665139198303223 0.22759310901165009
CurrentTrain: epoch  9, batch    17 | loss: 7.4326749Losses:  7.97299861907959 3.7408437728881836 0.23821529746055603
CurrentTrain: epoch  9, batch    18 | loss: 7.9729986Losses:  6.9965033531188965 2.6879351139068604 0.2429136484861374
CurrentTrain: epoch  9, batch    19 | loss: 6.9965034Losses:  6.794323921203613 2.477693557739258 0.23068052530288696
CurrentTrain: epoch  9, batch    20 | loss: 6.7943239Losses:  6.749263286590576 2.548668384552002 0.21564418077468872
CurrentTrain: epoch  9, batch    21 | loss: 6.7492633Losses:  7.9632415771484375 3.703467845916748 0.24567630887031555
CurrentTrain: epoch  9, batch    22 | loss: 7.9632416Losses:  7.038485527038574 2.7387845516204834 0.23100638389587402
CurrentTrain: epoch  9, batch    23 | loss: 7.0384855Losses:  8.711113929748535 4.4827423095703125 0.23752813041210175
CurrentTrain: epoch  9, batch    24 | loss: 8.7111139Losses:  8.006941795349121 3.763072967529297 0.23622021079063416
CurrentTrain: epoch  9, batch    25 | loss: 8.0069418Losses:  8.763571739196777 4.5739006996154785 0.24284300208091736
CurrentTrain: epoch  9, batch    26 | loss: 8.7635717Losses:  6.167221546173096 2.0264813899993896 0.13453061878681183
CurrentTrain: epoch  9, batch    27 | loss: 6.1672215Losses:  5.793467998504639 1.5621209144592285 0.21088175475597382
CurrentTrain: epoch  9, batch    28 | loss: 5.7934680Losses:  6.873992443084717 2.6836137771606445 0.22187775373458862
CurrentTrain: epoch  9, batch    29 | loss: 6.8739924Losses:  6.309091091156006 2.1210007667541504 0.21774277091026306
CurrentTrain: epoch  9, batch    30 | loss: 6.3090911Losses:  7.192935466766357 2.9833788871765137 0.21978139877319336
CurrentTrain: epoch  9, batch    31 | loss: 7.1929355Losses:  7.023982048034668 2.7932701110839844 0.23059016466140747
CurrentTrain: epoch  9, batch    32 | loss: 7.0239820Losses:  6.579092025756836 2.2954320907592773 0.2331482172012329
CurrentTrain: epoch  9, batch    33 | loss: 6.5790920Losses:  7.75676155090332 3.478097438812256 0.2422744333744049
CurrentTrain: epoch  9, batch    34 | loss: 7.7567616Losses:  7.287507057189941 3.1098546981811523 0.22426308691501617
CurrentTrain: epoch  9, batch    35 | loss: 7.2875071Losses:  6.140043258666992 2.032320976257324 0.1345033049583435
CurrentTrain: epoch  9, batch    36 | loss: 6.1400433Losses:  8.19863510131836 3.954306125640869 0.2283516228199005
CurrentTrain: epoch  9, batch    37 | loss: 8.1986351Losses:  7.966499328613281 3.828970432281494 0.2349872589111328
CurrentTrain: epoch  9, batch    38 | loss: 7.9664993Losses:  6.6322550773620605 2.4829208850860596 0.22945469617843628
CurrentTrain: epoch  9, batch    39 | loss: 6.6322551Losses:  7.4008917808532715 3.1689882278442383 0.23718252778053284
CurrentTrain: epoch  9, batch    40 | loss: 7.4008918Losses:  8.869532585144043 4.64193058013916 0.24608661234378815
CurrentTrain: epoch  9, batch    41 | loss: 8.8695326Losses:  7.699184894561768 3.452699661254883 0.23028844594955444
CurrentTrain: epoch  9, batch    42 | loss: 7.6991849Losses:  7.887511730194092 3.7667107582092285 0.1451287865638733
CurrentTrain: epoch  9, batch    43 | loss: 7.8875117Losses:  7.97722864151001 3.776289463043213 0.22767122089862823
CurrentTrain: epoch  9, batch    44 | loss: 7.9772286Losses:  7.214615821838379 3.011411666870117 0.22349359095096588
CurrentTrain: epoch  9, batch    45 | loss: 7.2146158Losses:  9.425811767578125 5.213373184204102 0.23543210327625275
CurrentTrain: epoch  9, batch    46 | loss: 9.4258118Losses:  7.62961483001709 3.4182891845703125 0.22096902132034302
CurrentTrain: epoch  9, batch    47 | loss: 7.6296148Losses:  7.084933280944824 2.888166904449463 0.22371336817741394
CurrentTrain: epoch  9, batch    48 | loss: 7.0849333Losses:  7.131278991699219 2.9350123405456543 0.21431896090507507
CurrentTrain: epoch  9, batch    49 | loss: 7.1312790Losses:  7.5036725997924805 3.2818872928619385 0.2492743879556656
CurrentTrain: epoch  9, batch    50 | loss: 7.5036726Losses:  6.56488561630249 2.372188091278076 0.22575625777244568
CurrentTrain: epoch  9, batch    51 | loss: 6.5648856Losses:  6.399744510650635 2.217900276184082 0.2200046181678772
CurrentTrain: epoch  9, batch    52 | loss: 6.3997445Losses:  9.951464653015137 5.6924896240234375 0.26522141695022583
CurrentTrain: epoch  9, batch    53 | loss: 9.9514647Losses:  7.2653069496154785 3.0710268020629883 0.2392769753932953
CurrentTrain: epoch  9, batch    54 | loss: 7.2653069Losses:  8.378944396972656 4.1518168449401855 0.25595784187316895
CurrentTrain: epoch  9, batch    55 | loss: 8.3789444Losses:  8.616341590881348 4.377009868621826 0.2544485926628113
CurrentTrain: epoch  9, batch    56 | loss: 8.6163416Losses:  7.275106430053711 3.0884008407592773 0.2318744659423828
CurrentTrain: epoch  9, batch    57 | loss: 7.2751064Losses:  8.051487922668457 3.841604709625244 0.24289584159851074
CurrentTrain: epoch  9, batch    58 | loss: 8.0514879Losses:  7.931814670562744 3.811243772506714 0.15603691339492798
CurrentTrain: epoch  9, batch    59 | loss: 7.9318147Losses:  6.188068389892578 1.9822254180908203 0.2292705774307251
CurrentTrain: epoch  9, batch    60 | loss: 6.1880684Losses:  7.302828788757324 3.1103310585021973 0.2279461920261383
CurrentTrain: epoch  9, batch    61 | loss: 7.3028288Losses:  4.700133323669434 0.4611726999282837 0.25393199920654297
CurrentTrain: epoch  9, batch    62 | loss: 4.7001333
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: géricault and delacroix were the most important painters of the romanticism .
Head Entity: delacroix
Tail Entity: romanticism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: the surrealism movement was significantly influenced by the works of salvador dalí and max ernst.  
Head Entity: dalí  
Tail Entity: surrealism  

Relation: movement  
Context: the cubism movement, pioneered by pablo picasso and georges braque, revolutionized modern art.  
Head Entity: picasso  
Tail Entity: cubism  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: it is located on fm 409 , northeast of bonham in fannin county in the u.s. state of texas .
Head Entity: bonham
Tail Entity: fannin county
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the historic site is situated in the heart of downtown nashville, within davidson county, tennessee.  
Head Entity: nashville  
Tail Entity: davidson county  

Relation: located in the administrative territorial entity  
Context: the university campus is found in the vibrant city of berkeley, which is part of alameda county in california.  
Head Entity: berkeley  
Tail Entity: alameda county  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: her role as chief of protocol began as a volunteer for then - san francisco mayor jack shelley , spanning over seven mayoral administrations until mayor frank jordan 's term .
Head Entity: san francisco
Tail Entity: frank jordan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: After serving as the mayor of Chicago for two terms, Rahm Emanuel announced his decision to step down, paving the way for the next leader of the city.  
Head Entity: Chicago  
Tail Entity: Rahm Emanuel  

Relation: head of government  
Context: The Prime Minister of Canada, Justin Trudeau, addressed the nation during a press conference to discuss the new policies being implemented.  
Head Entity: Canada  
Tail Entity: Justin Trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: the completed song was originally released on the 1971 album " the cry of love"the first posthumous album of original material compiled by the jimi hendrix experience drummer mitch mitchell and engineer eddie kramer .
Head Entity: mitch mitchell
Tail Entity: the jimi hendrix experience
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of performing together, the band announced that they would be parting ways, with lead singer Sarah Johnson stating that she would continue her career as a solo artist while still being proud of her time with the group.  
Head Entity: Sarah Johnson  
Tail Entity: The Harmony Singers  

Relation: member of  
Context: The renowned physicist was known for his groundbreaking research and was a long-time member of the prestigious scientific community, contributing significantly to various international conferences.  
Head Entity: Dr. Alan Thompson  
Tail Entity: The International Physics Society  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the solkan bridge ( , ) is a arch bridge over the soča river near nova gorica in western slovenia ( by railway terminology it is a viaduct ) .
Head Entity: solkan bridge
Tail Entity: soča river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge ( , ) spans the golden gate strait, the entrance to san francisco bay from the pacific ocean.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the channel tunnel ( , ) runs beneath the english channel, connecting folkestone in the uk with coqelles near calais in france.  
Head Entity: channel tunnel  
Tail Entity: english channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the city is bisected by the iset river , which flows from the urals into the tobol river .
Head Entity: tobol river
Tail Entity: iset river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Amazon River, known for its vast network of tributaries, receives water from the Madeira River, which is one of its largest tributaries.  
Head Entity: Amazon River  
Tail Entity: Madeira River  

Relation: tributary  
Context: The Missouri River is a major tributary of the Mississippi River, contributing significantly to its flow and ecosystem.  
Head Entity: Mississippi River  
Tail Entity: Missouri River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: the result was widely interpreted as a personal rebuke to then chief minister shane stone .
Head Entity: shane stone
Tail Entity: chief minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After serving as the mayor for over a decade, John Smith announced his retirement from the position.  
Head Entity: John Smith  
Tail Entity: mayor  

Relation: position held  
Context: The former president of the organization, Maria Lopez, was recognized for her contributions during her tenure.  
Head Entity: Maria Lopez  
Tail Entity: president
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: it was developed by syn sophia , who also developed the other games in the " style savvy " series .
Head Entity: style savvy
Tail Entity: syn sophia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: the popular game was created by epic games, known for their innovative approach to game design.  
Head Entity: popular game  
Tail Entity: epic games  

Relation: developer  
Context: the groundbreaking software was launched by openai, which has been at the forefront of artificial intelligence research.  
Head Entity: groundbreaking software  
Tail Entity: openai  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: fuzzy duck is the self - titled album by london - based progressive rock band fuzzy duck .
Head Entity: fuzzy duck
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in silicon valley by a group of innovative engineers.  
Head Entity: tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous painting was created by the artist in the vibrant city of paris.  
Head Entity: artist  
Tail Entity: paris  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: " shining time station " won a number of awards and significantly increased the popularity of the " thomas " media franchise in the united states .
Head Entity: shining time station
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish "sushi" is traditionally associated with Japan and has gained immense popularity worldwide.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic brand "Guinness" is known for its rich stout beer, which originated in Ireland and is now enjoyed globally.  
Head Entity: Guinness  
Tail Entity: Ireland  
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 88.89%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 91.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 91.96%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.01%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.06%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 95.91%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 95.95%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.68%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
cur_acc:  ['0.9494']
his_acc:  ['0.9494']
Clustering into  9  clusters
Clusters:  [2 7 8 0 1 1 3 2 6 6 5 2 2 1 0 6 2 1 4 4]
Losses:  12.296306610107422 4.835947036743164 0.578044593334198
CurrentTrain: epoch  0, batch     0 | loss: 12.2963066Losses:  11.71808910369873 3.791759490966797 0.5863370299339294
CurrentTrain: epoch  0, batch     1 | loss: 11.7180891Losses:  11.950950622558594 4.7747087478637695 0.5875829458236694
CurrentTrain: epoch  0, batch     2 | loss: 11.9509506Losses:  6.4827680587768555 -0.0 0.07601170241832733
CurrentTrain: epoch  0, batch     3 | loss: 6.4827681Losses:  11.441165924072266 3.7663445472717285 0.5125085115432739
CurrentTrain: epoch  1, batch     0 | loss: 11.4411659Losses:  10.000899314880371 3.5964603424072266 0.5478895902633667
CurrentTrain: epoch  1, batch     1 | loss: 10.0008993Losses:  8.423943519592285 2.436978340148926 0.5973565578460693
CurrentTrain: epoch  1, batch     2 | loss: 8.4239435Losses:  3.2047300338745117 -0.0 0.09559415280818939
CurrentTrain: epoch  1, batch     3 | loss: 3.2047300Losses:  9.683114051818848 3.422947406768799 0.5551766753196716
CurrentTrain: epoch  2, batch     0 | loss: 9.6831141Losses:  9.833487510681152 3.714102268218994 0.5337522625923157
CurrentTrain: epoch  2, batch     1 | loss: 9.8334875Losses:  8.33318042755127 3.869781970977783 0.5378350615501404
CurrentTrain: epoch  2, batch     2 | loss: 8.3331804Losses:  10.227134704589844 -0.0 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 10.2271347Losses:  10.365415573120117 4.191554546356201 0.4075506329536438
CurrentTrain: epoch  3, batch     0 | loss: 10.3654156Losses:  9.785924911499023 5.231282711029053 0.471506804227829
CurrentTrain: epoch  3, batch     1 | loss: 9.7859249Losses:  10.270121574401855 4.327827453613281 0.4524923861026764
CurrentTrain: epoch  3, batch     2 | loss: 10.2701216Losses:  3.1217737197875977 -0.0 0.08991564810276031
CurrentTrain: epoch  3, batch     3 | loss: 3.1217737Losses:  7.781438827514648 3.429222822189331 0.5379723906517029
CurrentTrain: epoch  4, batch     0 | loss: 7.7814388Losses:  6.566424369812012 2.7081570625305176 0.4982512593269348
CurrentTrain: epoch  4, batch     1 | loss: 6.5664244Losses:  7.867924213409424 3.10971999168396 0.5194400548934937
CurrentTrain: epoch  4, batch     2 | loss: 7.8679242Losses:  2.8041188716888428 -0.0 0.10989735275506973
CurrentTrain: epoch  4, batch     3 | loss: 2.8041189Losses:  7.75347375869751 3.1737546920776367 0.516405463218689
CurrentTrain: epoch  5, batch     0 | loss: 7.7534738Losses:  6.340387344360352 2.370918035507202 0.5110385417938232
CurrentTrain: epoch  5, batch     1 | loss: 6.3403873Losses:  7.032341957092285 3.732623815536499 0.41302499175071716
CurrentTrain: epoch  5, batch     2 | loss: 7.0323420Losses:  4.289502143859863 -0.0 0.11368700116872787
CurrentTrain: epoch  5, batch     3 | loss: 4.2895021Losses:  5.91673469543457 2.3434269428253174 0.42631909251213074
CurrentTrain: epoch  6, batch     0 | loss: 5.9167347Losses:  5.0403289794921875 1.8856582641601562 0.5006684064865112
CurrentTrain: epoch  6, batch     1 | loss: 5.0403290Losses:  5.167776107788086 2.1192407608032227 0.48980456590652466
CurrentTrain: epoch  6, batch     2 | loss: 5.1677761Losses:  3.6714601516723633 -0.0 0.09072689712047577
CurrentTrain: epoch  6, batch     3 | loss: 3.6714602Losses:  5.852145195007324 3.1935715675354004 0.41368532180786133
CurrentTrain: epoch  7, batch     0 | loss: 5.8521452Losses:  7.189805507659912 3.9830422401428223 0.4207431972026825
CurrentTrain: epoch  7, batch     1 | loss: 7.1898055Losses:  5.605656623840332 2.814568519592285 0.3055209517478943
CurrentTrain: epoch  7, batch     2 | loss: 5.6056566Losses:  2.3880624771118164 -0.0 0.17996275424957275
CurrentTrain: epoch  7, batch     3 | loss: 2.3880625Losses:  5.959491729736328 2.8510823249816895 0.5209817886352539
CurrentTrain: epoch  8, batch     0 | loss: 5.9594917Losses:  5.722721576690674 3.020216941833496 0.29796427488327026
CurrentTrain: epoch  8, batch     1 | loss: 5.7227216Losses:  6.993443489074707 4.368616580963135 0.4828976094722748
CurrentTrain: epoch  8, batch     2 | loss: 6.9934435Losses:  2.112281560897827 -0.0 0.1085159182548523
CurrentTrain: epoch  8, batch     3 | loss: 2.1122816Losses:  4.683959007263184 1.8715918064117432 0.4805901050567627
CurrentTrain: epoch  9, batch     0 | loss: 4.6839590Losses:  5.567038059234619 2.5912601947784424 0.4967826306819916
CurrentTrain: epoch  9, batch     1 | loss: 5.5670381Losses:  5.149192810058594 2.659242630004883 0.4086930453777313
CurrentTrain: epoch  9, batch     2 | loss: 5.1491928Losses:  1.6239879131317139 -0.0 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 1.6239879
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elena is a renowned artist who often collaborates with her husband, mark, a talented musician known for his soulful melodies.  
Head Entity: mark  
Tail Entity: elena  

Relation: spouse  
Context: during the award ceremony, it was announced that jessica, a celebrated actress, and her husband, tom, a successful director, would be producing a new film together.  
Head Entity: tom  
Tail Entity: jessica  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: " i 'm a cuckoo " was belle & sebastian 's second single from " dear catastrophe waitress " , released on rough trade records in 2004 .
Head Entity: belle & sebastian
Tail Entity: rough trade records
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Random Access Memories" by Daft Punk was released under the Columbia Records label in 2013.  
Head Entity: Daft Punk  
Tail Entity: Columbia Records  

Relation: record label  
Context: Taylor Swift's latest album "Evermore" was produced by her own label, Republic Records, showcasing her evolution as an artist.  
Head Entity: Taylor Swift  
Tail Entity: Republic Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of lake placid, attracting visitors year-round.  
Head Entity: lake placid  
Tail Entity: lake placid  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: anders morelius from sweden placed third in the individual race .
Head Entity: anders morelius
Tail Entity: sweden
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: maria gonzalez was born in mexico but later moved to the united states.  
Head Entity: maria gonzalez  
Tail Entity: mexico  

Relation: country of citizenship  
Context: after living in canada for several years, liu wei decided to apply for citizenship.  
Head Entity: liu wei  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller, blending elements of action and psychological drama.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: the band "coldplay" is known for their alternative rock sound, which incorporates elements of pop and electronic music.  
Head Entity: coldplay  
Tail Entity: alternative rock  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the thames river empties into the north sea, serving as a vital waterway for trade and transportation in london.  
Head Entity: thames river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: benjamin van leer ( born 9 april 1992 in houten ) is a dutch professional footballer who currently plays as a goalkeeper for ajax in the dutch eredivisie .
Head Entity: benjamin van leer
Tail Entity: goalkeeper
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: lebron james, born on december 30, 1984, is an american professional basketball player who currently plays as a forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: serena williams, born on september 26, 1981, is an american professional tennis player who is known for her powerful serve and currently plays in the women's singles circuit.  
Head Entity: serena williams  
Tail Entity: tennis player  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
Losses:  7.753174781799316 0.9782130718231201 0.7049104571342468
MemoryTrain:  epoch  0, batch     0 | loss: 7.7531748Losses:  6.866656303405762 1.4263200759887695 0.6772549152374268
MemoryTrain:  epoch  0, batch     1 | loss: 6.8666563Losses:  6.9940948486328125 1.6785376071929932 0.49733060598373413
MemoryTrain:  epoch  0, batch     2 | loss: 6.9940948Losses:  4.293802261352539 0.4944572448730469 0.5903350710868835
MemoryTrain:  epoch  0, batch     3 | loss: 4.2938023Losses:  5.959563255310059 0.7888421416282654 0.5442569255828857
MemoryTrain:  epoch  1, batch     0 | loss: 5.9595633Losses:  5.743614673614502 1.1200342178344727 0.7132010459899902
MemoryTrain:  epoch  1, batch     1 | loss: 5.7436147Losses:  7.048316478729248 0.8870099782943726 0.41086265444755554
MemoryTrain:  epoch  1, batch     2 | loss: 7.0483165Losses:  5.378427982330322 0.7704324126243591 0.632928729057312
MemoryTrain:  epoch  1, batch     3 | loss: 5.3784280Losses:  6.392534255981445 1.3188390731811523 0.6969066858291626
MemoryTrain:  epoch  2, batch     0 | loss: 6.3925343Losses:  5.102877616882324 0.8256467580795288 0.509710431098938
MemoryTrain:  epoch  2, batch     1 | loss: 5.1028776Losses:  5.2194952964782715 1.0421934127807617 0.6769732236862183
MemoryTrain:  epoch  2, batch     2 | loss: 5.2194953Losses:  4.56163215637207 -0.0 0.46603474020957947
MemoryTrain:  epoch  2, batch     3 | loss: 4.5616322Losses:  6.792294979095459 1.8716415166854858 0.6104859113693237
MemoryTrain:  epoch  3, batch     0 | loss: 6.7922950Losses:  4.872937202453613 1.3208301067352295 0.5774010419845581
MemoryTrain:  epoch  3, batch     1 | loss: 4.8729372Losses:  4.005819797515869 0.6515011787414551 0.678970217704773
MemoryTrain:  epoch  3, batch     2 | loss: 4.0058198Losses:  4.220176696777344 0.5190341472625732 0.5178946852684021
MemoryTrain:  epoch  3, batch     3 | loss: 4.2201767Losses:  3.8494114875793457 0.9081130623817444 0.5130599737167358
MemoryTrain:  epoch  4, batch     0 | loss: 3.8494115Losses:  4.344662666320801 1.0829263925552368 0.5691980123519897
MemoryTrain:  epoch  4, batch     1 | loss: 4.3446627Losses:  3.8787221908569336 0.49112042784690857 0.7218683958053589
MemoryTrain:  epoch  4, batch     2 | loss: 3.8787222Losses:  3.5911121368408203 0.27844366431236267 0.5306507349014282
MemoryTrain:  epoch  4, batch     3 | loss: 3.5911121Losses:  5.156167507171631 1.4637444019317627 0.6322803497314453
MemoryTrain:  epoch  5, batch     0 | loss: 5.1561675Losses:  4.47678279876709 1.7979578971862793 0.5985696315765381
MemoryTrain:  epoch  5, batch     1 | loss: 4.4767828Losses:  4.3011579513549805 2.087754726409912 0.5743175745010376
MemoryTrain:  epoch  5, batch     2 | loss: 4.3011580Losses:  4.0503339767456055 1.4972357749938965 0.45391830801963806
MemoryTrain:  epoch  5, batch     3 | loss: 4.0503340Losses:  4.026752471923828 1.62161386013031 0.45033007860183716
MemoryTrain:  epoch  6, batch     0 | loss: 4.0267525Losses:  3.275200843811035 0.5336265563964844 0.6926453113555908
MemoryTrain:  epoch  6, batch     1 | loss: 3.2752008Losses:  2.815126657485962 0.4636860191822052 0.6280933022499084
MemoryTrain:  epoch  6, batch     2 | loss: 2.8151267Losses:  2.8924131393432617 0.3494706153869629 0.5826154947280884
MemoryTrain:  epoch  6, batch     3 | loss: 2.8924131Losses:  3.782780885696411 1.2262842655181885 0.6011331081390381
MemoryTrain:  epoch  7, batch     0 | loss: 3.7827809Losses:  2.9659721851348877 0.9090511798858643 0.4680742621421814
MemoryTrain:  epoch  7, batch     1 | loss: 2.9659722Losses:  3.101860523223877 0.8936612606048584 0.5594780445098877
MemoryTrain:  epoch  7, batch     2 | loss: 3.1018605Losses:  2.4511706829071045 0.2706901729106903 0.4636501371860504
MemoryTrain:  epoch  7, batch     3 | loss: 2.4511707Losses:  2.8851842880249023 0.7877507209777832 0.5777069330215454
MemoryTrain:  epoch  8, batch     0 | loss: 2.8851843Losses:  3.164020538330078 0.7481033802032471 0.7305119633674622
MemoryTrain:  epoch  8, batch     1 | loss: 3.1640205Losses:  2.812617778778076 0.9095410108566284 0.5536094903945923
MemoryTrain:  epoch  8, batch     2 | loss: 2.8126178Losses:  2.729165554046631 0.5918243527412415 0.48636409640312195
MemoryTrain:  epoch  8, batch     3 | loss: 2.7291656Losses:  3.026703357696533 0.9702702164649963 0.582963764667511
MemoryTrain:  epoch  9, batch     0 | loss: 3.0267034Losses:  2.7203598022460938 0.5103705525398254 0.5517730116844177
MemoryTrain:  epoch  9, batch     1 | loss: 2.7203598Losses:  2.648714303970337 0.5345006585121155 0.6617031097412109
MemoryTrain:  epoch  9, batch     2 | loss: 2.6487143Losses:  2.6274945735931396 0.5430576801300049 0.5476555824279785
MemoryTrain:  epoch  9, batch     3 | loss: 2.6274946
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 79.86%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 79.81%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 79.30%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 81.25%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 78.87%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 79.08%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 79.25%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 79.33%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 62.50%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.31%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 78.12%   [EVAL] batch:   30 | acc: 43.75%,  total acc: 77.02%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 75.95%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 75.37%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 74.64%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 74.13%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 73.48%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 72.86%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 72.76%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 72.66%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 72.71%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 72.32%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 72.67%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 72.44%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 71.25%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 70.38%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 69.55%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 68.88%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 67.98%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 67.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 69.34%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 71.38%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 71.77%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 72.14%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 72.95%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 73.39%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.02%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 86.93%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 90.94%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 91.07%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 90.22%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 90.36%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 90.74%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.85%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.95%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 91.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 91.18%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 90.71%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 90.71%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 90.79%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.46%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 92.08%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.26%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 92.15%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 92.32%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 92.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.67%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.69%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.82%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 92.61%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.63%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.65%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 92.46%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.60%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 92.62%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 92.54%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 92.46%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 92.19%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 92.02%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 91.86%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 91.70%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 91.36%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 91.03%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 91.16%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 91.20%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 90.89%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 90.67%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 90.42%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 90.30%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 90.02%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 89.90%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 89.72%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 89.53%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 89.51%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 89.48%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 89.23%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 89.14%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 88.90%   [EVAL] batch:   85 | acc: 93.75%,  total acc: 88.95%   [EVAL] batch:   86 | acc: 75.00%,  total acc: 88.79%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 88.71%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 88.69%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 88.40%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 88.39%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 88.04%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 87.70%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 87.30%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 87.04%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 86.65%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 86.28%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 85.97%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 85.73%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 85.25%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 84.96%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 84.93%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 84.77%   [EVAL] batch:  103 | acc: 68.75%,  total acc: 84.62%   [EVAL] batch:  104 | acc: 62.50%,  total acc: 84.40%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 84.43%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 84.00%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 83.39%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 82.91%   [EVAL] batch:  109 | acc: 37.50%,  total acc: 82.50%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 81.98%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 81.70%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 81.53%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 81.69%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 81.95%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 82.10%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 82.41%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 82.59%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 82.74%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 82.96%   [EVAL] batch:  124 | acc: 100.00%,  total acc: 83.10%   
cur_acc:  ['0.9494', '0.7302']
his_acc:  ['0.9494', '0.8310']
Clustering into  14  clusters
Clusters:  [ 1  0 11 13  0  0 12 10  4  4  8  1  1  0  3  4  1  0  2  2  5  2  6  9
  3  0  7  1 10  5]
Losses:  13.8032865524292 5.460965156555176 0.7847945690155029
CurrentTrain: epoch  0, batch     0 | loss: 13.8032866Losses:  11.318902969360352 3.4060401916503906 0.7712702751159668
CurrentTrain: epoch  0, batch     1 | loss: 11.3189030Losses:  12.240077018737793 4.322133541107178 0.713928759098053
CurrentTrain: epoch  0, batch     2 | loss: 12.2400770Losses:  6.073870658874512 -0.0 0.17259284853935242
CurrentTrain: epoch  0, batch     3 | loss: 6.0738707Losses:  10.291015625 2.9280552864074707 0.8199523091316223
CurrentTrain: epoch  1, batch     0 | loss: 10.2910156Losses:  12.479109764099121 4.921961784362793 0.8161075711250305
CurrentTrain: epoch  1, batch     1 | loss: 12.4791098Losses:  11.858665466308594 4.589885711669922 0.8404708504676819
CurrentTrain: epoch  1, batch     2 | loss: 11.8586655Losses:  4.848980903625488 -0.0 0.1490764021873474
CurrentTrain: epoch  1, batch     3 | loss: 4.8489809Losses:  9.419567108154297 3.182821273803711 0.6455219984054565
CurrentTrain: epoch  2, batch     0 | loss: 9.4195671Losses:  9.984113693237305 3.356029748916626 0.7757285237312317
CurrentTrain: epoch  2, batch     1 | loss: 9.9841137Losses:  9.822237968444824 3.2322165966033936 0.7229577302932739
CurrentTrain: epoch  2, batch     2 | loss: 9.8222380Losses:  7.273570537567139 -0.0 0.10584930330514908
CurrentTrain: epoch  2, batch     3 | loss: 7.2735705Losses:  8.860347747802734 3.7928357124328613 0.6681257486343384
CurrentTrain: epoch  3, batch     0 | loss: 8.8603477Losses:  9.250061988830566 3.043454647064209 0.7132599353790283
CurrentTrain: epoch  3, batch     1 | loss: 9.2500620Losses:  9.384064674377441 3.059901714324951 0.7430599331855774
CurrentTrain: epoch  3, batch     2 | loss: 9.3840647Losses:  4.883655071258545 -0.0 0.12246803939342499
CurrentTrain: epoch  3, batch     3 | loss: 4.8836551Losses:  7.579469203948975 2.7160987854003906 0.7247819900512695
CurrentTrain: epoch  4, batch     0 | loss: 7.5794692Losses:  8.820287704467773 3.20357084274292 0.703251302242279
CurrentTrain: epoch  4, batch     1 | loss: 8.8202877Losses:  9.483736038208008 3.5278820991516113 0.7081982493400574
CurrentTrain: epoch  4, batch     2 | loss: 9.4837360Losses:  6.420034408569336 -0.0 0.13368777930736542
CurrentTrain: epoch  4, batch     3 | loss: 6.4200344Losses:  8.422185897827148 3.5992069244384766 0.7534666657447815
CurrentTrain: epoch  5, batch     0 | loss: 8.4221859Losses:  8.465856552124023 3.459923028945923 0.7200803160667419
CurrentTrain: epoch  5, batch     1 | loss: 8.4658566Losses:  9.764934539794922 3.792832374572754 0.7881765365600586
CurrentTrain: epoch  5, batch     2 | loss: 9.7649345Losses:  6.006371974945068 -0.0 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 6.0063720Losses:  10.044866561889648 4.080530643463135 0.6711735129356384
CurrentTrain: epoch  6, batch     0 | loss: 10.0448666Losses:  6.868977069854736 2.4051308631896973 0.7640200257301331
CurrentTrain: epoch  6, batch     1 | loss: 6.8689771Losses:  8.927323341369629 3.771876335144043 0.6914031505584717
CurrentTrain: epoch  6, batch     2 | loss: 8.9273233Losses:  2.3110923767089844 -0.0 0.09532558917999268
CurrentTrain: epoch  6, batch     3 | loss: 2.3110924Losses:  8.506440162658691 3.794921875 0.6123508214950562
CurrentTrain: epoch  7, batch     0 | loss: 8.5064402Losses:  9.722745895385742 5.139513969421387 0.6243791580200195
CurrentTrain: epoch  7, batch     1 | loss: 9.7227459Losses:  8.209891319274902 3.347468614578247 0.6799221038818359
CurrentTrain: epoch  7, batch     2 | loss: 8.2098913Losses:  5.166510581970215 -0.0 0.16150563955307007
CurrentTrain: epoch  7, batch     3 | loss: 5.1665106Losses:  9.054203987121582 3.741940498352051 0.7428544759750366
CurrentTrain: epoch  8, batch     0 | loss: 9.0542040Losses:  7.133603572845459 2.523956537246704 0.7559219002723694
CurrentTrain: epoch  8, batch     1 | loss: 7.1336036Losses:  6.461143970489502 1.8318679332733154 0.7829670310020447
CurrentTrain: epoch  8, batch     2 | loss: 6.4611440Losses:  3.556830883026123 -0.0 0.1475292146205902
CurrentTrain: epoch  8, batch     3 | loss: 3.5568309Losses:  11.313783645629883 6.207859992980957 0.6469365954399109
CurrentTrain: epoch  9, batch     0 | loss: 11.3137836Losses:  9.039033889770508 5.124183654785156 0.6245754361152649
CurrentTrain: epoch  9, batch     1 | loss: 9.0390339Losses:  9.860889434814453 4.5976667404174805 0.74394291639328
CurrentTrain: epoch  9, batch     2 | loss: 9.8608894Losses:  2.2315762042999268 -0.0 0.16486909985542297
CurrentTrain: epoch  9, batch     3 | loss: 2.2315762
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the novel "Pride and Prejudice" explores the themes of love and social class in early 19th century England.  
Head Entity: Pride and Prejudice  
Tail Entity: love  

Relation: main subject  
Context: the documentary "Our Planet" highlights the impact of climate change on various ecosystems around the world.  
Head Entity: Our Planet  
Tail Entity: climate change  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: group b of uefa euro 2016 contained england , russia , wales and slovakia .
Head Entity: uefa euro 2016
Tail Entity: slovakia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: the 2020 summer olympics featured countries like the united states, china, and japan competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: japan  

Relation: participating team  
Context: the fifa world cup 2018 saw teams such as france, croatia, and belgium battling for the championship title.  
Head Entity: fifa world cup 2018  
Tail Entity: croatia  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: he was nominated for the academy award for best picture , along with steven spielberg , ian bryce , mark gordon for the film " saving private ryan " .
Head Entity: ian bryce
Tail Entity: academy award for best picture
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received multiple nominations, including a nod for best director for Guillermo del Toro at the Academy Awards.  
Head Entity: Guillermo del Toro  
Tail Entity: Academy Awards  

Relation: nominated for  
Context: The popular band was nominated for the Grammy Award for Best New Artist after their debut album topped the charts.  
Head Entity: the popular band  
Tail Entity: Grammy Award for Best New Artist  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: on march 2015 , cube entertainment launched the multi - national girl group clc including original members seunghee , yujin , seungyeon , sorn and yeeun .
Head Entity: clc
Tail Entity: girl group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone brand Apple released its latest model, the iPhone 14, which features advanced technology and improved camera capabilities.  
Head Entity: iPhone 14  
Tail Entity: smartphone  

Relation: instance of  
Context: The famous painting "Starry Night" was created by the artist Vincent van Gogh and is considered a masterpiece of post-impressionism.  
Head Entity: Starry Night  
Tail Entity: painting  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: the mount kapaz or kepez ( ) is a mountain in lesser caucasus near ganja city in central azerbaijan .
Head Entity: mount kapaz
Tail Entity: lesser caucasus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada is a mountain range in the western united states, known for its stunning landscapes and diverse ecosystems.  
Head Entity: sierra nevada  
Tail Entity: western united states  

Relation: mountain range  
Context: the alps are a major mountain range in europe, stretching across eight countries and famous for their breathtaking scenery and skiing resorts.  
Head Entity: alps  
Tail Entity: europe  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: it would also be the last episode to feature a grounding , until " the marge - ian chronicles " in season 27 , six years later ( also written by brian kelley ) .
Head Entity: the marge - ian chronicles
Tail Entity: brian kelley
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: The film "Inception" was a groundbreaking project that showcased the visionary talents of its creator, with the screenplay crafted by the brilliant Christopher Nolan.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: screenwriter  
Context: The beloved animated feature "Toy Story" was brought to life through the imaginative script penned by the talented Andrew Stanton.  
Head Entity: Toy Story  
Tail Entity: Andrew Stanton  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the earliest written mention of the order is found in " tirant lo blanch " , a chivalric romance written in catalan mainly by valencian joanot martorell .
Head Entity: tirant lo blanch
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The famous novel "One Hundred Years of Solitude" was originally published in Spanish and has been translated into many languages.  
Head Entity: One Hundred Years of Solitude  
Tail Entity: Spanish  

Relation: language of work or name  
Context: The animated series "Naruto" is primarily produced in Japanese and has gained a massive following worldwide.  
Head Entity: Naruto  
Tail Entity: Japanese  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: gogukwon 's successor , sosurim , adopted a foreign policy of appeasement and reconciliation with baekje , and concentrated on domestic policies to spread buddhism throughout goguryeo 's social and political systems .
Head Entity: goguryeo
Tail Entity: buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: The ancient Greeks worshipped a pantheon of gods and goddesses, with Zeus being the king of the gods and a central figure in their religious practices.  
Head Entity: ancient Greeks  
Tail Entity: Zeus  

Relation: religion  
Context: The construction of the grand cathedral was a significant event for the local community, as it represented their dedication to Christianity and served as a place of worship for generations.  
Head Entity: local community  
Tail Entity: Christianity  
Losses:  6.258054733276367 1.1237578392028809 0.9101496934890747
MemoryTrain:  epoch  0, batch     0 | loss: 6.2580547Losses:  4.965606689453125 0.7422609329223633 0.6872236132621765
MemoryTrain:  epoch  0, batch     1 | loss: 4.9656067Losses:  5.070940017700195 1.0945894718170166 0.6060932278633118
MemoryTrain:  epoch  0, batch     2 | loss: 5.0709400Losses:  5.895023822784424 0.9404329061508179 0.8138768076896667
MemoryTrain:  epoch  0, batch     3 | loss: 5.8950238Losses:  4.954416275024414 0.5077856779098511 0.8166622519493103
MemoryTrain:  epoch  0, batch     4 | loss: 4.9544163Losses:  4.068147659301758 0.5961511731147766 0.5110455751419067
MemoryTrain:  epoch  0, batch     5 | loss: 4.0681477Losses:  4.149322509765625 0.6258732080459595 0.8056076169013977
MemoryTrain:  epoch  1, batch     0 | loss: 4.1493225Losses:  6.659979343414307 1.391143798828125 0.8067575693130493
MemoryTrain:  epoch  1, batch     1 | loss: 6.6599793Losses:  4.338667392730713 0.21946708858013153 0.6481480002403259
MemoryTrain:  epoch  1, batch     2 | loss: 4.3386674Losses:  3.773617744445801 -0.0 0.8578249216079712
MemoryTrain:  epoch  1, batch     3 | loss: 3.7736177Losses:  4.649541854858398 1.4455032348632812 0.7876187562942505
MemoryTrain:  epoch  1, batch     4 | loss: 4.6495419Losses:  6.647086143493652 0.8984764218330383 0.4232003688812256
MemoryTrain:  epoch  1, batch     5 | loss: 6.6470861Losses:  4.374865531921387 0.9651130437850952 0.7024576663970947
MemoryTrain:  epoch  2, batch     0 | loss: 4.3748655Losses:  5.525106906890869 1.64695405960083 0.6951693296432495
MemoryTrain:  epoch  2, batch     1 | loss: 5.5251069Losses:  4.249260902404785 1.37840735912323 0.4826388955116272
MemoryTrain:  epoch  2, batch     2 | loss: 4.2492609Losses:  5.0746684074401855 0.9273457527160645 0.9284098744392395
MemoryTrain:  epoch  2, batch     3 | loss: 5.0746684Losses:  3.719636917114258 0.48317527770996094 0.7539082765579224
MemoryTrain:  epoch  2, batch     4 | loss: 3.7196369Losses:  4.1128411293029785 0.36576682329177856 0.7947996258735657
MemoryTrain:  epoch  2, batch     5 | loss: 4.1128411Losses:  5.3262786865234375 1.8853001594543457 0.8376124501228333
MemoryTrain:  epoch  3, batch     0 | loss: 5.3262787Losses:  4.035827159881592 1.322805404663086 0.7465981245040894
MemoryTrain:  epoch  3, batch     1 | loss: 4.0358272Losses:  3.3341782093048096 0.7313263416290283 0.603785514831543
MemoryTrain:  epoch  3, batch     2 | loss: 3.3341782Losses:  5.133209705352783 1.068574070930481 0.7157834768295288
MemoryTrain:  epoch  3, batch     3 | loss: 5.1332097Losses:  4.3175129890441895 1.1975486278533936 0.5316941142082214
MemoryTrain:  epoch  3, batch     4 | loss: 4.3175130Losses:  3.410916328430176 -0.0 0.7277302145957947
MemoryTrain:  epoch  3, batch     5 | loss: 3.4109163Losses:  4.293640613555908 0.8099015355110168 0.6493831872940063
MemoryTrain:  epoch  4, batch     0 | loss: 4.2936406Losses:  3.2502529621124268 -0.0 0.8711774945259094
MemoryTrain:  epoch  4, batch     1 | loss: 3.2502530Losses:  4.036997318267822 0.7818112373352051 0.7197216749191284
MemoryTrain:  epoch  4, batch     2 | loss: 4.0369973Losses:  4.263075351715088 0.8460673093795776 0.7716606855392456
MemoryTrain:  epoch  4, batch     3 | loss: 4.2630754Losses:  3.238666534423828 0.7448316216468811 0.6997246146202087
MemoryTrain:  epoch  4, batch     4 | loss: 3.2386665Losses:  2.2843618392944336 0.22975821793079376 0.6116346120834351
MemoryTrain:  epoch  4, batch     5 | loss: 2.2843618Losses:  3.6850600242614746 1.0438015460968018 0.6689859628677368
MemoryTrain:  epoch  5, batch     0 | loss: 3.6850600Losses:  3.752166748046875 0.7079386711120605 0.6697815656661987
MemoryTrain:  epoch  5, batch     1 | loss: 3.7521667Losses:  3.0718019008636475 0.5448883175849915 0.7854931950569153
MemoryTrain:  epoch  5, batch     2 | loss: 3.0718019Losses:  4.587838649749756 1.1745641231536865 0.7941309809684753
MemoryTrain:  epoch  5, batch     3 | loss: 4.5878386Losses:  4.014642715454102 1.2212141752243042 0.6615431904792786
MemoryTrain:  epoch  5, batch     4 | loss: 4.0146427Losses:  2.9141478538513184 0.35514160990715027 0.4918975830078125
MemoryTrain:  epoch  5, batch     5 | loss: 2.9141479Losses:  3.511683464050293 0.8072421550750732 0.7672285437583923
MemoryTrain:  epoch  6, batch     0 | loss: 3.5116835Losses:  2.9560866355895996 0.2426782101392746 0.8578378558158875
MemoryTrain:  epoch  6, batch     1 | loss: 2.9560866Losses:  4.5367631912231445 1.648237943649292 0.8097543716430664
MemoryTrain:  epoch  6, batch     2 | loss: 4.5367632Losses:  3.8118796348571777 0.8434945940971375 0.5986405611038208
MemoryTrain:  epoch  6, batch     3 | loss: 3.8118796Losses:  2.6890769004821777 0.4774557948112488 0.6914792656898499
MemoryTrain:  epoch  6, batch     4 | loss: 2.6890769Losses:  4.020012378692627 0.37609240412712097 0.5200808048248291
MemoryTrain:  epoch  6, batch     5 | loss: 4.0200124Losses:  3.839771270751953 0.6861481666564941 0.7992441654205322
MemoryTrain:  epoch  7, batch     0 | loss: 3.8397713Losses:  3.866295337677002 0.7689916491508484 0.79350745677948
MemoryTrain:  epoch  7, batch     1 | loss: 3.8662953Losses:  3.4979782104492188 0.4995306730270386 0.8409934043884277
MemoryTrain:  epoch  7, batch     2 | loss: 3.4979782Losses:  3.9657199382781982 1.654045581817627 0.744763195514679
MemoryTrain:  epoch  7, batch     3 | loss: 3.9657199Losses:  3.644437551498413 1.4325876235961914 0.7751181125640869
MemoryTrain:  epoch  7, batch     4 | loss: 3.6444376Losses:  2.5053393840789795 0.2588885724544525 0.5601534843444824
MemoryTrain:  epoch  7, batch     5 | loss: 2.5053394Losses:  3.322134494781494 0.7756775617599487 0.8221277594566345
MemoryTrain:  epoch  8, batch     0 | loss: 3.3221345Losses:  3.6778860092163086 0.8634481430053711 0.8578318357467651
MemoryTrain:  epoch  8, batch     1 | loss: 3.6778860Losses:  3.411966562271118 1.0782537460327148 0.6426118016242981
MemoryTrain:  epoch  8, batch     2 | loss: 3.4119666Losses:  3.1878790855407715 0.6905738115310669 0.638727068901062
MemoryTrain:  epoch  8, batch     3 | loss: 3.1878791Losses:  3.4995806217193604 0.8317989110946655 0.8378389477729797
MemoryTrain:  epoch  8, batch     4 | loss: 3.4995806Losses:  2.45743465423584 0.3220626413822174 0.5528866052627563
MemoryTrain:  epoch  8, batch     5 | loss: 2.4574347Losses:  2.3753085136413574 0.2559722065925598 0.7317160367965698
MemoryTrain:  epoch  9, batch     0 | loss: 2.3753085Losses:  3.234813928604126 0.5312438011169434 0.7804358005523682
MemoryTrain:  epoch  9, batch     1 | loss: 3.2348139Losses:  3.9140536785125732 1.025334119796753 0.6916532516479492
MemoryTrain:  epoch  9, batch     2 | loss: 3.9140537Losses:  3.1966052055358887 0.4977787435054779 0.7385323643684387
MemoryTrain:  epoch  9, batch     3 | loss: 3.1966052Losses:  2.8208155632019043 0.49543070793151855 0.6702396869659424
MemoryTrain:  epoch  9, batch     4 | loss: 2.8208156Losses:  2.2639875411987305 0.3129902482032776 0.5714139342308044
MemoryTrain:  epoch  9, batch     5 | loss: 2.2639875
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 76.34%   [EVAL] batch:   14 | acc: 12.50%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 66.18%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 64.58%   [EVAL] batch:   18 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 64.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 65.77%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 67.05%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 68.48%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 70.75%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 70.43%   [EVAL] batch:   26 | acc: 37.50%,  total acc: 69.21%   [EVAL] batch:   27 | acc: 50.00%,  total acc: 68.53%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 68.53%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 67.71%   [EVAL] batch:   30 | acc: 25.00%,  total acc: 66.33%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 67.23%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 67.83%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 68.57%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 69.10%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 70.39%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 72.56%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 74.29%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 74.44%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 74.86%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 75.40%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 75.39%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 75.38%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 75.88%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 75.74%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 75.96%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 75.59%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 75.58%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 75.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 76.12%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 75.88%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 75.54%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 75.53%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 75.21%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 74.90%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 74.50%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 73.81%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 69.79%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.07%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.26%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 86.17%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 84.93%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 84.64%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 84.20%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 84.12%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 84.21%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 84.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.37%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 85.71%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 85.76%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 86.08%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 85.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 86.28%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 86.30%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 86.20%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.48%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 86.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 86.76%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 86.90%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 87.15%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 87.38%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 87.39%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 87.06%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 86.42%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 86.44%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 86.35%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 86.17%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 85.89%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 85.62%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 85.45%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 85.38%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 85.13%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 85.17%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 84.93%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 84.78%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 84.91%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 85.04%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 84.90%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 84.93%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 84.88%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 84.67%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 84.87%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 84.86%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 84.81%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 84.77%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 84.88%   [EVAL] batch:   81 | acc: 87.50%,  total acc: 84.91%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 84.86%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 84.78%   [EVAL] batch:   85 | acc: 93.75%,  total acc: 84.88%   [EVAL] batch:   86 | acc: 68.75%,  total acc: 84.70%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 84.73%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 84.69%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 84.58%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 84.75%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 84.44%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 84.34%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 83.98%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 83.88%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 83.53%   [EVAL] batch:   96 | acc: 50.00%,  total acc: 83.18%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 82.91%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 82.89%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 82.56%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 82.30%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 82.23%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 82.04%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 81.97%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 81.85%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 81.72%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 81.48%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 81.02%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 80.73%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 80.57%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 80.12%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 79.85%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 79.81%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 79.99%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 80.33%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 80.50%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 80.67%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 80.94%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 80.99%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 81.10%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 81.15%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 81.30%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 81.35%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 81.15%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 81.15%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 81.15%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 81.20%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 81.11%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 81.11%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 81.20%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 81.20%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 81.39%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 81.43%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 81.30%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 80.85%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 80.36%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 79.92%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 79.53%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 79.24%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 78.86%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 78.97%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 79.11%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 79.21%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 79.35%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 79.47%   [EVAL] batch:  151 | acc: 37.50%,  total acc: 79.19%   [EVAL] batch:  152 | acc: 50.00%,  total acc: 79.00%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 78.94%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 78.71%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 78.37%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 78.38%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 78.40%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 78.46%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 78.55%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 78.61%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 78.70%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 78.80%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 78.93%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 79.05%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 79.18%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 79.27%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 79.39%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 79.51%   [EVAL] batch:  169 | acc: 81.25%,  total acc: 79.52%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 79.61%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 79.72%   [EVAL] batch:  172 | acc: 75.00%,  total acc: 79.70%   [EVAL] batch:  173 | acc: 75.00%,  total acc: 79.67%   [EVAL] batch:  174 | acc: 100.00%,  total acc: 79.79%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 79.72%   [EVAL] batch:  176 | acc: 87.50%,  total acc: 79.77%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 79.63%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 79.61%   [EVAL] batch:  179 | acc: 87.50%,  total acc: 79.65%   [EVAL] batch:  180 | acc: 93.75%,  total acc: 79.73%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 79.64%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 79.51%   [EVAL] batch:  183 | acc: 75.00%,  total acc: 79.48%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 79.36%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 79.23%   [EVAL] batch:  186 | acc: 50.00%,  total acc: 79.08%   [EVAL] batch:  187 | acc: 31.25%,  total acc: 78.82%   
cur_acc:  ['0.9494', '0.7302', '0.7381']
his_acc:  ['0.9494', '0.8310', '0.7882']
Clustering into  19  clusters
Clusters:  [ 4  6 12  7  5  5 11  0  6  3 14  0  0  5  1  3  4  5 10 10  2 18 17 13
  1  6  8  4  0  2  7  4  4  6  9  2 15 16  0  6]
Losses:  10.93157958984375 4.430484294891357 0.5922203660011292
CurrentTrain: epoch  0, batch     0 | loss: 10.9315796Losses:  9.658474922180176 3.7697014808654785 0.6771150231361389
CurrentTrain: epoch  0, batch     1 | loss: 9.6584749Losses:  7.693918704986572 2.3690881729125977 0.6939603686332703
CurrentTrain: epoch  0, batch     2 | loss: 7.6939187Losses:  3.072779893875122 -0.0 0.1086985170841217
CurrentTrain: epoch  0, batch     3 | loss: 3.0727799Losses:  7.113127708435059 2.2140350341796875 0.7194148302078247
CurrentTrain: epoch  1, batch     0 | loss: 7.1131277Losses:  7.3580522537231445 2.8990695476531982 0.6301197409629822
CurrentTrain: epoch  1, batch     1 | loss: 7.3580523Losses:  8.575725555419922 3.5939433574676514 0.6223312616348267
CurrentTrain: epoch  1, batch     2 | loss: 8.5757256Losses:  2.4926846027374268 -0.0 0.11095216870307922
CurrentTrain: epoch  1, batch     3 | loss: 2.4926846Losses:  6.683588981628418 3.313906669616699 0.6222913265228271
CurrentTrain: epoch  2, batch     0 | loss: 6.6835890Losses:  9.297868728637695 4.519281387329102 0.5556437969207764
CurrentTrain: epoch  2, batch     1 | loss: 9.2978687Losses:  5.956563949584961 2.8862133026123047 0.6050669550895691
CurrentTrain: epoch  2, batch     2 | loss: 5.9565639Losses:  3.8294644355773926 -0.0 0.11475738137960434
CurrentTrain: epoch  2, batch     3 | loss: 3.8294644Losses:  8.39231014251709 4.545344829559326 0.7136274576187134
CurrentTrain: epoch  3, batch     0 | loss: 8.3923101Losses:  5.982892036437988 2.8517980575561523 0.5195018649101257
CurrentTrain: epoch  3, batch     1 | loss: 5.9828920Losses:  6.005382061004639 2.213170289993286 0.6762384176254272
CurrentTrain: epoch  3, batch     2 | loss: 6.0053821Losses:  2.0757508277893066 -0.0 0.11449785530567169
CurrentTrain: epoch  3, batch     3 | loss: 2.0757508Losses:  5.985188007354736 2.4292385578155518 0.6677852869033813
CurrentTrain: epoch  4, batch     0 | loss: 5.9851880Losses:  5.318877220153809 2.1409106254577637 0.6861476302146912
CurrentTrain: epoch  4, batch     1 | loss: 5.3188772Losses:  5.541624546051025 2.309185028076172 0.6847425699234009
CurrentTrain: epoch  4, batch     2 | loss: 5.5416245Losses:  3.135199785232544 -0.0 0.10322141647338867
CurrentTrain: epoch  4, batch     3 | loss: 3.1351998Losses:  7.994585990905762 4.6556782722473145 0.6478636860847473
CurrentTrain: epoch  5, batch     0 | loss: 7.9945860Losses:  6.2125630378723145 3.43994140625 0.4963240623474121
CurrentTrain: epoch  5, batch     1 | loss: 6.2125630Losses:  5.447267532348633 2.376142978668213 0.6682329773902893
CurrentTrain: epoch  5, batch     2 | loss: 5.4472675Losses:  5.091340065002441 -0.0 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 5.0913401Losses:  6.158440113067627 3.028055191040039 0.5449224710464478
CurrentTrain: epoch  6, batch     0 | loss: 6.1584401Losses:  5.665326118469238 2.735722064971924 0.559525191783905
CurrentTrain: epoch  6, batch     1 | loss: 5.6653261Losses:  5.844213962554932 3.079728603363037 0.6664766073226929
CurrentTrain: epoch  6, batch     2 | loss: 5.8442140Losses:  1.840539574623108 -0.0 0.1769828349351883
CurrentTrain: epoch  6, batch     3 | loss: 1.8405396Losses:  5.4038920402526855 2.320716381072998 0.6642537117004395
CurrentTrain: epoch  7, batch     0 | loss: 5.4038920Losses:  4.960427761077881 2.1575698852539062 0.6388119459152222
CurrentTrain: epoch  7, batch     1 | loss: 4.9604278Losses:  4.545958995819092 2.087935209274292 0.6363566517829895
CurrentTrain: epoch  7, batch     2 | loss: 4.5459590Losses:  1.8185970783233643 -0.0 0.13864591717720032
CurrentTrain: epoch  7, batch     3 | loss: 1.8185971Losses:  5.024480819702148 2.308438777923584 0.6568670272827148
CurrentTrain: epoch  8, batch     0 | loss: 5.0244808Losses:  4.704463481903076 1.9676284790039062 0.6362327337265015
CurrentTrain: epoch  8, batch     1 | loss: 4.7044635Losses:  4.682296276092529 2.1864476203918457 0.5835370421409607
CurrentTrain: epoch  8, batch     2 | loss: 4.6822963Losses:  1.8532869815826416 -0.0 0.12334099411964417
CurrentTrain: epoch  8, batch     3 | loss: 1.8532870Losses:  5.3232421875 2.8871068954467773 0.47239774465560913
CurrentTrain: epoch  9, batch     0 | loss: 5.3232422Losses:  4.596014022827148 2.090413808822632 0.5448291301727295
CurrentTrain: epoch  9, batch     1 | loss: 4.5960140Losses:  5.468585014343262 2.8104186058044434 0.5949632525444031
CurrentTrain: epoch  9, batch     2 | loss: 5.4685850Losses:  1.8581266403198242 -0.0 0.07946977019309998
CurrentTrain: epoch  9, batch     3 | loss: 1.8581266
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor, she became a prominent figure in the Democratic Party, advocating for social justice and community development.  
Head Entity: she  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: During his tenure in the Senate, he was known for his strong support of the Republican Party's policies on tax reform and national security.  
Head Entity: he  
Tail Entity: Republican Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film is based on the novel "the great gatsby" by f. scott fitzgerald, capturing the essence of the roaring twenties.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: this musical is inspired by the play "hamlet" written by william shakespeare, reimagining the classic tragedy in a contemporary setting.  
Head Entity: hamlet  
Tail Entity: william shakespeare  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: several paintings are also mentioned , including rousseau 's " the dream " and van gogh 's " bedroom in arles " .
Head Entity: rousseau
Tail Entity: the dream
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: Among his many contributions to literature, one of his most acclaimed novels is "The Great Gatsby," which explores themes of wealth and social change in 1920s America.  
Head Entity: F. Scott Fitzgerald  
Tail Entity: The Great Gatsby  

Relation: notable work  
Context: The artist is best known for his sculpture "The Thinker," which has become a symbol of philosophy and contemplation.  
Head Entity: Auguste Rodin  
Tail Entity: The Thinker  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: in 2011 , zhigarev was elected as a deputy in the state duma , the lower house of the federal assembly of russia , as a representative for the shchelkovsky constituency in moscow .
Head Entity: state duma
Tail Entity: moscow
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: The global technology company, headquartered in Cupertino, California, has been a leader in innovation for over a decade.  
Head Entity: global technology company  
Tail Entity: Cupertino  

Relation: headquarters location  
Context: The non-profit organization moved its headquarters to Austin, Texas, to better serve its growing community of volunteers and supporters.  
Head Entity: non-profit organization  
Tail Entity: Austin  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which is classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in the field of artificial intelligence and currently works at the tech innovation lab in silicon valley.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. rajesh kumar as the new director of global health initiatives, focusing on infectious diseases.  
Head Entity: dr. rajesh kumar  
Tail Entity: infectious diseases  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: wjmi ( 99.7 fm ) is a radio station licensed to jackson , mississippi , united states , with a mainstream urban musical format .
Head Entity: wjmi
Tail Entity: jackson , mississippi
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: kxyz ( 101.5 fm ) is a radio station licensed to transmit to the city of austin , texas , providing a mix of rock and pop music.  
Head Entity: kxyz  
Tail Entity: austin , texas  

Relation: licensed to broadcast to  
Context: wqrs ( 88.3 fm ) is a community radio station licensed to serve the area of boulder , colorado , focusing on local news and music.  
Head Entity: wqrs  
Tail Entity: boulder , colorado  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion and is one of the brightest nebulae visible to the naked eye.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major, which is known for its prominence in the night sky.  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Losses:  3.5032384395599365 0.28822582960128784 0.8281288146972656
MemoryTrain:  epoch  0, batch     0 | loss: 3.5032384Losses:  4.712148666381836 -0.0 1.0652555227279663
MemoryTrain:  epoch  0, batch     1 | loss: 4.7121487Losses:  4.239652633666992 0.497763991355896 0.8673340678215027
MemoryTrain:  epoch  0, batch     2 | loss: 4.2396526Losses:  5.233443737030029 0.7055220007896423 0.8329818844795227
MemoryTrain:  epoch  0, batch     3 | loss: 5.2334437Losses:  4.643171310424805 0.8702036142349243 0.8167105913162231
MemoryTrain:  epoch  0, batch     4 | loss: 4.6431713Losses:  3.937119245529175 0.20757874846458435 0.6273739337921143
MemoryTrain:  epoch  0, batch     5 | loss: 3.9371192Losses:  4.692914962768555 0.5380889177322388 0.8864668607711792
MemoryTrain:  epoch  0, batch     6 | loss: 4.6929150Losses:  4.162164688110352 0.4573923945426941 0.5945088863372803
MemoryTrain:  epoch  0, batch     7 | loss: 4.1621647Losses:  3.9803895950317383 0.5187777280807495 0.8120970129966736
MemoryTrain:  epoch  1, batch     0 | loss: 3.9803896Losses:  4.300020217895508 0.7341241836547852 0.810664176940918
MemoryTrain:  epoch  1, batch     1 | loss: 4.3000202Losses:  5.167690277099609 0.5312203168869019 0.9386491775512695
MemoryTrain:  epoch  1, batch     2 | loss: 5.1676903Losses:  3.0069801807403564 0.2533523738384247 0.8854491710662842
MemoryTrain:  epoch  1, batch     3 | loss: 3.0069802Losses:  4.91272497177124 0.6919291019439697 0.8482251763343811
MemoryTrain:  epoch  1, batch     4 | loss: 4.9127250Losses:  3.8586533069610596 0.8918806910514832 0.7997968792915344
MemoryTrain:  epoch  1, batch     5 | loss: 3.8586533Losses:  4.124388217926025 0.2632445693016052 0.9314600229263306
MemoryTrain:  epoch  1, batch     6 | loss: 4.1243882Losses:  2.951871156692505 -0.0 0.6200969815254211
MemoryTrain:  epoch  1, batch     7 | loss: 2.9518712Losses:  3.8436167240142822 0.39339566230773926 0.9293534159660339
MemoryTrain:  epoch  2, batch     0 | loss: 3.8436167Losses:  3.723024368286133 -0.0 0.8744958639144897
MemoryTrain:  epoch  2, batch     1 | loss: 3.7230244Losses:  3.373671293258667 0.2448495328426361 0.9132020473480225
MemoryTrain:  epoch  2, batch     2 | loss: 3.3736713Losses:  3.60347580909729 0.32734909653663635 0.9333322048187256
MemoryTrain:  epoch  2, batch     3 | loss: 3.6034758Losses:  4.199585914611816 1.0595660209655762 0.7965170741081238
MemoryTrain:  epoch  2, batch     4 | loss: 4.1995859Losses:  3.2191667556762695 0.30032414197921753 0.8930031657218933
MemoryTrain:  epoch  2, batch     5 | loss: 3.2191668Losses:  3.6399106979370117 -0.0 0.7563114166259766
MemoryTrain:  epoch  2, batch     6 | loss: 3.6399107Losses:  2.483856201171875 -0.0 0.598080575466156
MemoryTrain:  epoch  2, batch     7 | loss: 2.4838562Losses:  3.976585626602173 0.5079400539398193 0.9471659064292908
MemoryTrain:  epoch  3, batch     0 | loss: 3.9765856Losses:  4.413718223571777 1.2241214513778687 0.8727420568466187
MemoryTrain:  epoch  3, batch     1 | loss: 4.4137182Losses:  4.2025017738342285 0.5527271628379822 0.9393190741539001
MemoryTrain:  epoch  3, batch     2 | loss: 4.2025018Losses:  3.6888368129730225 0.48456430435180664 0.8057252764701843
MemoryTrain:  epoch  3, batch     3 | loss: 3.6888368Losses:  3.181931972503662 0.7429629564285278 0.7401580214500427
MemoryTrain:  epoch  3, batch     4 | loss: 3.1819320Losses:  3.3606560230255127 0.5376118421554565 0.7442337870597839
MemoryTrain:  epoch  3, batch     5 | loss: 3.3606560Losses:  3.135676383972168 0.2830031216144562 0.7546399235725403
MemoryTrain:  epoch  3, batch     6 | loss: 3.1356764Losses:  1.9634323120117188 -0.0 0.6525447368621826
MemoryTrain:  epoch  3, batch     7 | loss: 1.9634323Losses:  3.1467137336730957 -0.0 0.8552848100662231
MemoryTrain:  epoch  4, batch     0 | loss: 3.1467137Losses:  3.5443081855773926 0.7802602052688599 0.7729789614677429
MemoryTrain:  epoch  4, batch     1 | loss: 3.5443082Losses:  3.0821380615234375 0.49011096358299255 0.7867834568023682
MemoryTrain:  epoch  4, batch     2 | loss: 3.0821381Losses:  4.9344801902771 1.0424939393997192 0.8368536233901978
MemoryTrain:  epoch  4, batch     3 | loss: 4.9344802Losses:  3.01682710647583 0.3118760585784912 0.9884438514709473
MemoryTrain:  epoch  4, batch     4 | loss: 3.0168271Losses:  3.2308616638183594 0.7989910840988159 0.672627866268158
MemoryTrain:  epoch  4, batch     5 | loss: 3.2308617Losses:  3.208103895187378 0.4795474410057068 0.9557687640190125
MemoryTrain:  epoch  4, batch     6 | loss: 3.2081039Losses:  2.956204891204834 -0.0 0.5320526361465454
MemoryTrain:  epoch  4, batch     7 | loss: 2.9562049Losses:  3.316598892211914 0.2820834517478943 0.8118301630020142
MemoryTrain:  epoch  5, batch     0 | loss: 3.3165989Losses:  2.6191959381103516 0.4700050354003906 0.7962428331375122
MemoryTrain:  epoch  5, batch     1 | loss: 2.6191959Losses:  4.20969820022583 0.6175428628921509 0.8847912549972534
MemoryTrain:  epoch  5, batch     2 | loss: 4.2096982Losses:  2.7481656074523926 0.3209570646286011 0.8161851167678833
MemoryTrain:  epoch  5, batch     3 | loss: 2.7481656Losses:  3.5467586517333984 0.7244933247566223 0.6475671529769897
MemoryTrain:  epoch  5, batch     4 | loss: 3.5467587Losses:  2.9775679111480713 0.5659407377243042 0.9139950275421143
MemoryTrain:  epoch  5, batch     5 | loss: 2.9775679Losses:  3.751258134841919 1.033245325088501 0.7824658751487732
MemoryTrain:  epoch  5, batch     6 | loss: 3.7512581Losses:  1.8756821155548096 -0.0 0.5812773108482361
MemoryTrain:  epoch  5, batch     7 | loss: 1.8756821Losses:  3.04544997215271 -0.0 0.9763116240501404
MemoryTrain:  epoch  6, batch     0 | loss: 3.0454500Losses:  4.159468173980713 1.0442843437194824 0.7802799344062805
MemoryTrain:  epoch  6, batch     1 | loss: 4.1594682Losses:  2.8357081413269043 0.49472576379776 0.8816918730735779
MemoryTrain:  epoch  6, batch     2 | loss: 2.8357081Losses:  3.1923084259033203 0.9856330752372742 0.7380027174949646
MemoryTrain:  epoch  6, batch     3 | loss: 3.1923084Losses:  2.6076178550720215 0.5025860071182251 0.7923336029052734
MemoryTrain:  epoch  6, batch     4 | loss: 2.6076179Losses:  3.3273110389709473 0.5420972108840942 0.6781934499740601
MemoryTrain:  epoch  6, batch     5 | loss: 3.3273110Losses:  2.7966010570526123 -0.0 0.8197190165519714
MemoryTrain:  epoch  6, batch     6 | loss: 2.7966011Losses:  2.6636624336242676 -0.0 0.666451096534729
MemoryTrain:  epoch  6, batch     7 | loss: 2.6636624Losses:  3.572512626647949 0.5229241251945496 0.9323996305465698
MemoryTrain:  epoch  7, batch     0 | loss: 3.5725126Losses:  2.896904945373535 0.7552987337112427 0.8062945604324341
MemoryTrain:  epoch  7, batch     1 | loss: 2.8969049Losses:  2.916139602661133 0.2228820025920868 0.7122829556465149
MemoryTrain:  epoch  7, batch     2 | loss: 2.9161396Losses:  2.3293824195861816 0.2483908236026764 0.7930449843406677
MemoryTrain:  epoch  7, batch     3 | loss: 2.3293824Losses:  2.897515296936035 0.8759406805038452 0.7258758544921875
MemoryTrain:  epoch  7, batch     4 | loss: 2.8975153Losses:  3.1529815196990967 0.49480411410331726 0.7771553993225098
MemoryTrain:  epoch  7, batch     5 | loss: 3.1529815Losses:  3.180572271347046 0.26531562209129333 0.945626437664032
MemoryTrain:  epoch  7, batch     6 | loss: 3.1805723Losses:  2.506883382797241 -0.0 0.5232252478599548
MemoryTrain:  epoch  7, batch     7 | loss: 2.5068834Losses:  2.7597994804382324 0.24177002906799316 0.8466005325317383
MemoryTrain:  epoch  8, batch     0 | loss: 2.7597995Losses:  3.042853355407715 0.5385091304779053 0.9300679564476013
MemoryTrain:  epoch  8, batch     1 | loss: 3.0428534Losses:  2.992964267730713 0.5144814252853394 0.7586249709129333
MemoryTrain:  epoch  8, batch     2 | loss: 2.9929643Losses:  2.947376012802124 -0.0 0.9023339748382568
MemoryTrain:  epoch  8, batch     3 | loss: 2.9473760Losses:  2.704432487487793 0.24168217182159424 0.8601449131965637
MemoryTrain:  epoch  8, batch     4 | loss: 2.7044325Losses:  3.072129011154175 1.0073319673538208 0.7393634915351868
MemoryTrain:  epoch  8, batch     5 | loss: 3.0721290Losses:  3.0845091342926025 0.4725176692008972 0.7366034388542175
MemoryTrain:  epoch  8, batch     6 | loss: 3.0845091Losses:  1.8403130769729614 -0.0 0.6460691690444946
MemoryTrain:  epoch  8, batch     7 | loss: 1.8403131Losses:  3.5631790161132812 0.8620383143424988 0.8588923215866089
MemoryTrain:  epoch  9, batch     0 | loss: 3.5631790Losses:  3.000311851501465 0.5123298764228821 0.9371392130851746
MemoryTrain:  epoch  9, batch     1 | loss: 3.0003119Losses:  2.900934934616089 0.4793274700641632 0.6552385687828064
MemoryTrain:  epoch  9, batch     2 | loss: 2.9009349Losses:  3.0693132877349854 0.4833655059337616 0.933497428894043
MemoryTrain:  epoch  9, batch     3 | loss: 3.0693133Losses:  3.32200026512146 1.0237383842468262 0.7278506755828857
MemoryTrain:  epoch  9, batch     4 | loss: 3.3220003Losses:  2.339388608932495 -0.0 0.8401011824607849
MemoryTrain:  epoch  9, batch     5 | loss: 2.3393886Losses:  2.7301905155181885 0.7267482876777649 0.6483428478240967
MemoryTrain:  epoch  9, batch     6 | loss: 2.7301905Losses:  2.1862947940826416 0.3221430778503418 0.6248666644096375
MemoryTrain:  epoch  9, batch     7 | loss: 2.1862948
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 31.25%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 31.25%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 69.89%   [EVAL] batch:   11 | acc: 25.00%,  total acc: 66.15%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 65.38%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 68.42%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 66.25%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 64.58%   [EVAL] batch:   21 | acc: 25.00%,  total acc: 62.78%   [EVAL] batch:   22 | acc: 25.00%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 59.64%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 58.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 59.86%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 61.34%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 62.72%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 63.79%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 66.13%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 66.21%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 66.29%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 66.18%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 66.32%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 66.05%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 66.28%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 66.99%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.06%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 70.60%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 71.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 73.60%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 75.81%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 76.14%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 76.86%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 76.94%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 77.22%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 77.60%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 77.97%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 77.78%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 71.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 70.31%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 71.15%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.77%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.84%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.78%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.95%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 79.17%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 78.41%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 77.99%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 78.65%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.68%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.86%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.40%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 83.71%   [EVAL] batch:   33 | acc: 43.75%,  total acc: 82.54%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.32%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 82.43%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 82.73%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.17%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.59%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 84.45%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 84.80%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 84.78%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 84.57%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 84.51%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 84.57%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 84.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.93%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 85.10%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.26%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 85.34%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 85.38%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 85.31%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 84.59%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 84.64%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 84.79%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 84.84%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 84.58%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 84.42%   [EVAL] batch:   63 | acc: 75.00%,  total acc: 84.28%   [EVAL] batch:   64 | acc: 81.25%,  total acc: 84.23%   [EVAL] batch:   65 | acc: 50.00%,  total acc: 83.71%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 83.77%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 83.36%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 83.15%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 83.30%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 83.45%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 83.25%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 83.22%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 83.28%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 83.17%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 83.22%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 83.20%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 83.25%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 83.23%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 83.20%   [EVAL] batch:   80 | acc: 87.50%,  total acc: 83.26%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 83.16%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 82.83%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 82.57%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 82.34%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 81.90%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 81.96%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 81.95%   [EVAL] batch:   89 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 82.07%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 81.86%   [EVAL] batch:   92 | acc: 62.50%,  total acc: 81.65%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 81.45%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 80.92%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 80.67%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 80.42%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 80.37%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 79.94%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 79.83%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 79.72%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 79.55%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 79.51%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 79.46%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 79.36%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 79.09%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 78.59%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 78.15%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 77.95%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 77.53%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 77.29%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 77.21%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 77.41%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.61%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 77.80%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 77.99%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 78.18%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 78.36%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 78.49%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 78.56%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 78.69%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 78.76%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 78.88%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 78.90%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 78.87%   [EVAL] batch:  126 | acc: 75.00%,  total acc: 78.84%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 78.86%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 78.83%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 78.94%   [EVAL] batch:  130 | acc: 68.75%,  total acc: 78.86%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 78.88%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 78.90%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 78.87%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 78.94%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 79.15%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 78.99%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 78.51%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 78.04%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 77.62%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 77.29%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 77.01%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 76.65%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.77%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.93%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 77.04%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.20%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.35%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.46%   [EVAL] batch:  150 | acc: 62.50%,  total acc: 77.36%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 77.14%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:  153 | acc: 68.75%,  total acc: 77.03%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 76.81%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 76.64%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 76.67%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 76.70%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.95%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 77.06%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 77.16%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 77.26%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 77.29%   [EVAL] batch:  164 | acc: 93.75%,  total acc: 77.39%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 77.48%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 77.58%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:  168 | acc: 100.00%,  total acc: 77.85%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 77.90%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 78.00%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 78.14%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 78.20%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 78.29%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 78.27%   [EVAL] batch:  176 | acc: 87.50%,  total acc: 78.32%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 78.30%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 78.32%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 78.40%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 78.42%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 78.23%   [EVAL] batch:  182 | acc: 43.75%,  total acc: 78.04%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 77.99%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 77.87%   [EVAL] batch:  185 | acc: 37.50%,  total acc: 77.65%   [EVAL] batch:  186 | acc: 31.25%,  total acc: 77.41%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 77.33%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 77.38%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 77.47%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 77.59%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 77.60%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:  193 | acc: 81.25%,  total acc: 77.74%   [EVAL] batch:  194 | acc: 43.75%,  total acc: 77.56%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 77.42%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 77.22%   [EVAL] batch:  197 | acc: 37.50%,  total acc: 77.02%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 76.79%   [EVAL] batch:  199 | acc: 25.00%,  total acc: 76.53%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 76.52%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 76.55%   [EVAL] batch:  202 | acc: 93.75%,  total acc: 76.63%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 76.56%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 76.55%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 76.52%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 76.42%   [EVAL] batch:  207 | acc: 18.75%,  total acc: 76.14%   [EVAL] batch:  208 | acc: 18.75%,  total acc: 75.87%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 75.65%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 75.44%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 75.18%   [EVAL] batch:  212 | acc: 62.50%,  total acc: 75.12%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 75.23%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.55%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 75.74%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 75.65%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 75.65%   [EVAL] batch:  221 | acc: 62.50%,  total acc: 75.59%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 75.53%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 75.53%   [EVAL] batch:  224 | acc: 43.75%,  total acc: 75.39%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:  226 | acc: 93.75%,  total acc: 75.58%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.69%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.79%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.90%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.97%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.38%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.48%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 76.58%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 76.68%   [EVAL] batch:  238 | acc: 93.75%,  total acc: 76.75%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 76.82%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 76.99%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 77.15%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 77.24%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 77.33%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.51%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.58%   
cur_acc:  ['0.9494', '0.7302', '0.7381', '0.7778']
his_acc:  ['0.9494', '0.8310', '0.7882', '0.7758']
Clustering into  24  clusters
Clusters:  [ 8 11 16 23 14 14 12  0  5 13  1  0  0 14  3 13  8 14  9  9  6 19 15  4
  3 14 21  8  0  6 23  8  8  5 22 18 20  2  0  5 11 17  4  6  0  7 10 18
  2  1]
Losses:  9.140684127807617 2.4536213874816895 0.8654236197471619
CurrentTrain: epoch  0, batch     0 | loss: 9.1406841Losses:  11.809606552124023 3.742164134979248 0.7957096099853516
CurrentTrain: epoch  0, batch     1 | loss: 11.8096066Losses:  9.449108123779297 3.099149227142334 0.7705443501472473
CurrentTrain: epoch  0, batch     2 | loss: 9.4491081Losses:  7.22653341293335 -0.0 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 7.2265334Losses:  8.808333396911621 2.509279251098633 0.7734114527702332
CurrentTrain: epoch  1, batch     0 | loss: 8.8083334Losses:  7.383927822113037 2.2229888439178467 0.8332716226577759
CurrentTrain: epoch  1, batch     1 | loss: 7.3839278Losses:  9.387372970581055 3.316129207611084 0.7127147912979126
CurrentTrain: epoch  1, batch     2 | loss: 9.3873730Losses:  3.794290065765381 -0.0 0.10263963043689728
CurrentTrain: epoch  1, batch     3 | loss: 3.7942901Losses:  9.18140697479248 4.131475448608398 0.7267169952392578
CurrentTrain: epoch  2, batch     0 | loss: 9.1814070Losses:  7.894396781921387 3.781165599822998 0.6936360597610474
CurrentTrain: epoch  2, batch     1 | loss: 7.8943968Losses:  8.406292915344238 2.4577293395996094 0.7850136160850525
CurrentTrain: epoch  2, batch     2 | loss: 8.4062929Losses:  5.262266635894775 -0.0 0.08529622852802277
CurrentTrain: epoch  2, batch     3 | loss: 5.2622666Losses:  8.825273513793945 3.869123935699463 0.7712286114692688
CurrentTrain: epoch  3, batch     0 | loss: 8.8252735Losses:  7.239658832550049 2.1994385719299316 0.8138145804405212
CurrentTrain: epoch  3, batch     1 | loss: 7.2396588Losses:  7.378573894500732 2.932887554168701 0.6988019347190857
CurrentTrain: epoch  3, batch     2 | loss: 7.3785739Losses:  5.118441581726074 -0.0 0.07514070719480515
CurrentTrain: epoch  3, batch     3 | loss: 5.1184416Losses:  7.860052108764648 3.7909433841705322 0.6716796159744263
CurrentTrain: epoch  4, batch     0 | loss: 7.8600521Losses:  8.13301944732666 3.760368824005127 0.7350032925605774
CurrentTrain: epoch  4, batch     1 | loss: 8.1330194Losses:  8.476737022399902 3.4230189323425293 0.6744147539138794
CurrentTrain: epoch  4, batch     2 | loss: 8.4767370Losses:  3.0872879028320312 -0.0 0.12467580288648605
CurrentTrain: epoch  4, batch     3 | loss: 3.0872879Losses:  8.090864181518555 2.980970621109009 0.7693312168121338
CurrentTrain: epoch  5, batch     0 | loss: 8.0908642Losses:  6.174461841583252 1.9195904731750488 0.8167381286621094
CurrentTrain: epoch  5, batch     1 | loss: 6.1744618Losses:  6.8926286697387695 3.520153045654297 0.6574689149856567
CurrentTrain: epoch  5, batch     2 | loss: 6.8926287Losses:  5.0611066818237305 -0.0 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 5.0611067Losses:  6.838772773742676 2.8740739822387695 0.6825260519981384
CurrentTrain: epoch  6, batch     0 | loss: 6.8387728Losses:  5.4200758934021 1.9020683765411377 0.8007044196128845
CurrentTrain: epoch  6, batch     1 | loss: 5.4200759Losses:  8.35284423828125 3.8176534175872803 0.6885260343551636
CurrentTrain: epoch  6, batch     2 | loss: 8.3528442Losses:  2.3848752975463867 -0.0 0.11049783229827881
CurrentTrain: epoch  6, batch     3 | loss: 2.3848753Losses:  7.406065940856934 4.400055885314941 0.6660522222518921
CurrentTrain: epoch  7, batch     0 | loss: 7.4060659Losses:  8.040413856506348 3.619842529296875 0.6159775853157043
CurrentTrain: epoch  7, batch     1 | loss: 8.0404139Losses:  5.7610907554626465 2.1056571006774902 0.807277500629425
CurrentTrain: epoch  7, batch     2 | loss: 5.7610908Losses:  2.461458683013916 -0.0 0.09654054790735245
CurrentTrain: epoch  7, batch     3 | loss: 2.4614587Losses:  5.937183380126953 2.8267087936401367 0.6754915714263916
CurrentTrain: epoch  8, batch     0 | loss: 5.9371834Losses:  5.7843918800354 2.058434247970581 0.7918767333030701
CurrentTrain: epoch  8, batch     1 | loss: 5.7843919Losses:  6.25086784362793 2.618588924407959 0.7984344363212585
CurrentTrain: epoch  8, batch     2 | loss: 6.2508678Losses:  2.505014181137085 -0.0 0.11532403528690338
CurrentTrain: epoch  8, batch     3 | loss: 2.5050142Losses:  6.338973045349121 3.2022972106933594 0.6581658124923706
CurrentTrain: epoch  9, batch     0 | loss: 6.3389730Losses:  6.621159553527832 3.087545394897461 0.71692955493927
CurrentTrain: epoch  9, batch     1 | loss: 6.6211596Losses:  5.637718200683594 2.224788188934326 0.7388871908187866
CurrentTrain: epoch  9, batch     2 | loss: 5.6377182Losses:  4.56079626083374 -0.0 0.23984336853027344
CurrentTrain: epoch  9, batch     3 | loss: 4.5607963
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are significant administrative divisions within the country, with Ontario being one of the largest provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The state of California is divided into several counties, including Los Angeles County, which is one of the most populous counties in the United States.  
Head Entity: California  
Tail Entity: Los Angeles County  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "soda" and "pop" are said to be the same as each other, though this varies by region.  
Head Entity: soda  
Tail Entity: pop  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: the filming was primarily held at pollachi . it fetched pawan kalyan the filmfare award for best actor – telugu award at 59th filmfare awards south .
Head Entity: filmfare award for best actor – telugu
Tail Entity: pawan kalyan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: in the thrilling finale of the championship, the underdog team triumphed, securing the title and making history as they celebrated their victory at the national sports arena. This remarkable achievement earned them the prestigious trophy for the best team of the year.  
Head Entity: trophy for the best team of the year  
Tail Entity: underdog team  

Relation: winner  
Context: during the annual music awards, the talented singer captivated the audience with her powerful performance, ultimately leading her to win the coveted title of best new artist. The event was a celebration of emerging talent in the music industry.  
Head Entity: title of best new artist  
Tail Entity: talented singer  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: air commodore arthur kellam tylee obe ( 24 april 1887 – 13 april 1961 ) was canadian officer who served in the royal flying corps during world war i.
Head Entity: arthur kellam tylee
Tail Entity: air commodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: general john smith was appointed as the chief of staff of the united states army in 2020, overseeing all operations and strategic planning.  
Head Entity: john smith  
Tail Entity: general  

Relation: military rank  
Context: colonel jane doe led her battalion with distinction during the peacekeeping mission in the middle east, earning her numerous accolades.  
Head Entity: jane doe  
Tail Entity: colonel  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: there are seven different nations that are allied or in conflict : prior to its north american release , " vanguard bandits " was titled " detonator gauntlet " by working designs .
Head Entity: vanguard bandits
Tail Entity: working designs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: The latest novel by the acclaimed author was released by Penguin Random House, a well-known publishing house in the industry.  
Head Entity: latest novel  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: After years of hard work, the game developer finally secured a deal with Electronic Arts to publish their new sports game.  
Head Entity: new sports game  
Tail Entity: Electronic Arts  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first book in the series, "The Enchanted Forest," was published in 2015, followed by "The Hidden Valley" in 2017.  
Head Entity: The Enchanted Forest  
Tail Entity: The Hidden Valley  

Relation: followed by  
Context: The opening act of the concert featured a local band, which was followed by the headlining artist who captivated the audience.  
Head Entity: local band  
Tail Entity: headlining artist  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: the headquarters of the company is situated in san francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: san francisco  

Relation: work location  
Context: during her time at the university, she conducted research in boston, which greatly influenced her career.  
Head Entity: she  
Tail Entity: boston  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: ancient city of Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: aretha franklin ( march 25 , 1942 – august 16 , 2018 ) was an american singer, songwriter, and civil rights activist, often referred to as the "Queen of Soul" for her powerful and emotive voice.  
Head Entity: aretha franklin  
Tail Entity: soul
Losses:  4.891006946563721 0.48194563388824463 0.9771467447280884
MemoryTrain:  epoch  0, batch     0 | loss: 4.8910069Losses:  3.932939291000366 -0.0 0.9360377192497253
MemoryTrain:  epoch  0, batch     1 | loss: 3.9329393Losses:  4.628764629364014 0.7966985106468201 0.8942800164222717
MemoryTrain:  epoch  0, batch     2 | loss: 4.6287646Losses:  4.182755470275879 0.25060543417930603 0.9378120303153992
MemoryTrain:  epoch  0, batch     3 | loss: 4.1827555Losses:  3.546353578567505 0.25479617714881897 0.7514012455940247
MemoryTrain:  epoch  0, batch     4 | loss: 3.5463536Losses:  4.429701328277588 0.5349116921424866 0.7151680588722229
MemoryTrain:  epoch  0, batch     5 | loss: 4.4297013Losses:  5.572020530700684 0.2770937979221344 0.9813844561576843
MemoryTrain:  epoch  0, batch     6 | loss: 5.5720205Losses:  5.147712707519531 0.5536341667175293 0.7348217368125916
MemoryTrain:  epoch  0, batch     7 | loss: 5.1477127Losses:  3.6117138862609863 0.5385507345199585 0.8759738206863403
MemoryTrain:  epoch  0, batch     8 | loss: 3.6117139Losses:  2.8994293212890625 -0.0 0.5665858387947083
MemoryTrain:  epoch  0, batch     9 | loss: 2.8994293Losses:  5.58617639541626 1.2103559970855713 0.6393380165100098
MemoryTrain:  epoch  1, batch     0 | loss: 5.5861764Losses:  3.9815855026245117 -0.0 1.082116723060608
MemoryTrain:  epoch  1, batch     1 | loss: 3.9815855Losses:  3.3047890663146973 0.5747641324996948 0.9989613890647888
MemoryTrain:  epoch  1, batch     2 | loss: 3.3047891Losses:  3.558598279953003 0.2670753002166748 0.9170379638671875
MemoryTrain:  epoch  1, batch     3 | loss: 3.5585983Losses:  3.8600053787231445 0.471890926361084 0.9620705842971802
MemoryTrain:  epoch  1, batch     4 | loss: 3.8600054Losses:  3.9403932094573975 0.25069302320480347 0.9354225993156433
MemoryTrain:  epoch  1, batch     5 | loss: 3.9403932Losses:  3.3240184783935547 0.26228445768356323 1.0029081106185913
MemoryTrain:  epoch  1, batch     6 | loss: 3.3240185Losses:  4.174813270568848 -0.0 0.849663496017456
MemoryTrain:  epoch  1, batch     7 | loss: 4.1748133Losses:  4.445552349090576 0.2453424036502838 0.8427814841270447
MemoryTrain:  epoch  1, batch     8 | loss: 4.4455523Losses:  2.7088160514831543 -0.0 0.5451968908309937
MemoryTrain:  epoch  1, batch     9 | loss: 2.7088161Losses:  3.580103874206543 0.28587836027145386 0.9252985715866089
MemoryTrain:  epoch  2, batch     0 | loss: 3.5801039Losses:  3.5623912811279297 0.49846282601356506 0.9211863279342651
MemoryTrain:  epoch  2, batch     1 | loss: 3.5623913Losses:  2.6810553073883057 0.24900400638580322 0.86029452085495
MemoryTrain:  epoch  2, batch     2 | loss: 2.6810553Losses:  3.026602268218994 -0.0 0.8945354223251343
MemoryTrain:  epoch  2, batch     3 | loss: 3.0266023Losses:  3.4095892906188965 0.33151525259017944 0.7589133977890015
MemoryTrain:  epoch  2, batch     4 | loss: 3.4095893Losses:  4.030647277832031 0.8473701477050781 0.9091925621032715
MemoryTrain:  epoch  2, batch     5 | loss: 4.0306473Losses:  3.864807605743408 0.560677170753479 0.6504484415054321
MemoryTrain:  epoch  2, batch     6 | loss: 3.8648076Losses:  4.401220321655273 0.4731443524360657 1.034820318222046
MemoryTrain:  epoch  2, batch     7 | loss: 4.4012203Losses:  4.686779499053955 0.36404022574424744 0.8862592577934265
MemoryTrain:  epoch  2, batch     8 | loss: 4.6867795Losses:  2.9707159996032715 -0.0 0.5354372262954712
MemoryTrain:  epoch  2, batch     9 | loss: 2.9707160Losses:  3.5738165378570557 0.511954128742218 0.7933430075645447
MemoryTrain:  epoch  3, batch     0 | loss: 3.5738165Losses:  2.993586301803589 0.485504150390625 0.9747345447540283
MemoryTrain:  epoch  3, batch     1 | loss: 2.9935863Losses:  3.6787054538726807 0.5377063751220703 0.8777211308479309
MemoryTrain:  epoch  3, batch     2 | loss: 3.6787055Losses:  4.095533847808838 0.30059850215911865 0.9872565865516663
MemoryTrain:  epoch  3, batch     3 | loss: 4.0955338Losses:  4.049829483032227 0.5547278523445129 0.9318447113037109
MemoryTrain:  epoch  3, batch     4 | loss: 4.0498295Losses:  3.469024658203125 0.7642987966537476 0.823756217956543
MemoryTrain:  epoch  3, batch     5 | loss: 3.4690247Losses:  3.5317132472991943 0.7975924015045166 0.8707702159881592
MemoryTrain:  epoch  3, batch     6 | loss: 3.5317132Losses:  3.0596444606781006 0.24658744037151337 1.0180118083953857
MemoryTrain:  epoch  3, batch     7 | loss: 3.0596445Losses:  3.5836236476898193 0.5203545093536377 0.8107832074165344
MemoryTrain:  epoch  3, batch     8 | loss: 3.5836236Losses:  1.9434674978256226 -0.0 0.5225400924682617
MemoryTrain:  epoch  3, batch     9 | loss: 1.9434675Losses:  3.085305690765381 0.5543189644813538 0.805761456489563
MemoryTrain:  epoch  4, batch     0 | loss: 3.0853057Losses:  3.0416665077209473 0.5855426788330078 0.6696439981460571
MemoryTrain:  epoch  4, batch     1 | loss: 3.0416665Losses:  3.856105327606201 0.6818599700927734 0.8522604703903198
MemoryTrain:  epoch  4, batch     2 | loss: 3.8561053Losses:  2.786120891571045 -0.0 0.8622133135795593
MemoryTrain:  epoch  4, batch     3 | loss: 2.7861209Losses:  2.9072933197021484 -0.0 1.0023083686828613
MemoryTrain:  epoch  4, batch     4 | loss: 2.9072933Losses:  3.4759743213653564 0.2642602026462555 0.9880473613739014
MemoryTrain:  epoch  4, batch     5 | loss: 3.4759743Losses:  3.1805667877197266 0.7629664540290833 0.8130125999450684
MemoryTrain:  epoch  4, batch     6 | loss: 3.1805668Losses:  2.764406442642212 0.30402201414108276 0.9430355429649353
MemoryTrain:  epoch  4, batch     7 | loss: 2.7644064Losses:  3.5352418422698975 0.5141991376876831 0.8144146800041199
MemoryTrain:  epoch  4, batch     8 | loss: 3.5352418Losses:  1.7670719623565674 -0.0 0.3286774456501007
MemoryTrain:  epoch  4, batch     9 | loss: 1.7670720Losses:  2.76963472366333 0.2707608640193939 0.869775116443634
MemoryTrain:  epoch  5, batch     0 | loss: 2.7696347Losses:  3.269686222076416 0.7554652690887451 0.7983818054199219
MemoryTrain:  epoch  5, batch     1 | loss: 3.2696862Losses:  3.3400073051452637 0.24741876125335693 1.0493634939193726
MemoryTrain:  epoch  5, batch     2 | loss: 3.3400073Losses:  2.996330738067627 -0.0 0.8706268072128296
MemoryTrain:  epoch  5, batch     3 | loss: 2.9963307Losses:  3.531968832015991 0.7786757349967957 0.9439337253570557
MemoryTrain:  epoch  5, batch     4 | loss: 3.5319688Losses:  3.3169450759887695 0.7933595180511475 0.8251270651817322
MemoryTrain:  epoch  5, batch     5 | loss: 3.3169451Losses:  2.519676923751831 0.2675117254257202 0.8689036965370178
MemoryTrain:  epoch  5, batch     6 | loss: 2.5196769Losses:  3.1292529106140137 0.5057043433189392 0.9174069762229919
MemoryTrain:  epoch  5, batch     7 | loss: 3.1292529Losses:  3.364760160446167 1.0163756608963013 0.8032706379890442
MemoryTrain:  epoch  5, batch     8 | loss: 3.3647602Losses:  2.257345199584961 -0.0 0.5460575819015503
MemoryTrain:  epoch  5, batch     9 | loss: 2.2573452Losses:  2.8828210830688477 -0.0 1.0583813190460205
MemoryTrain:  epoch  6, batch     0 | loss: 2.8828211Losses:  4.193509578704834 0.8414679765701294 0.9915398955345154
MemoryTrain:  epoch  6, batch     1 | loss: 4.1935096Losses:  3.1064953804016113 0.5328991413116455 0.866138219833374
MemoryTrain:  epoch  6, batch     2 | loss: 3.1064954Losses:  2.474790096282959 -0.0 0.9053710699081421
MemoryTrain:  epoch  6, batch     3 | loss: 2.4747901Losses:  2.599088191986084 0.5164831876754761 0.7658156156539917
MemoryTrain:  epoch  6, batch     4 | loss: 2.5990882Losses:  3.6655948162078857 1.7001726627349854 0.5797309279441833
MemoryTrain:  epoch  6, batch     5 | loss: 3.6655948Losses:  2.307406425476074 -0.0 0.9690462350845337
MemoryTrain:  epoch  6, batch     6 | loss: 2.3074064Losses:  3.1379265785217285 0.5544909238815308 0.9256643056869507
MemoryTrain:  epoch  6, batch     7 | loss: 3.1379266Losses:  3.1312856674194336 0.7447940707206726 0.7823154926300049
MemoryTrain:  epoch  6, batch     8 | loss: 3.1312857Losses:  1.8098300695419312 0.28024715185165405 0.31773293018341064
MemoryTrain:  epoch  6, batch     9 | loss: 1.8098301Losses:  2.7133750915527344 0.2478877753019333 0.9599918127059937
MemoryTrain:  epoch  7, batch     0 | loss: 2.7133751Losses:  2.99474835395813 0.5230713486671448 0.9084140658378601
MemoryTrain:  epoch  7, batch     1 | loss: 2.9947484Losses:  2.987431287765503 0.2548292279243469 0.8669247627258301
MemoryTrain:  epoch  7, batch     2 | loss: 2.9874313Losses:  2.7161519527435303 -0.0 1.0205104351043701
MemoryTrain:  epoch  7, batch     3 | loss: 2.7161520Losses:  2.235386371612549 0.24282872676849365 0.6672124862670898
MemoryTrain:  epoch  7, batch     4 | loss: 2.2353864Losses:  2.5044708251953125 -0.0 0.8718699216842651
MemoryTrain:  epoch  7, batch     5 | loss: 2.5044708Losses:  2.2409989833831787 -0.0 0.8434670567512512
MemoryTrain:  epoch  7, batch     6 | loss: 2.2409990Losses:  2.696129322052002 0.49855300784111023 0.8492813110351562
MemoryTrain:  epoch  7, batch     7 | loss: 2.6961293Losses:  2.7542078495025635 0.2687228322029114 0.9192748069763184
MemoryTrain:  epoch  7, batch     8 | loss: 2.7542078Losses:  1.822800874710083 -0.0 0.5254108309745789
MemoryTrain:  epoch  7, batch     9 | loss: 1.8228009Losses:  2.614866256713867 -0.0 0.92668616771698
MemoryTrain:  epoch  8, batch     0 | loss: 2.6148663Losses:  2.379951000213623 0.23858106136322021 0.8169838786125183
MemoryTrain:  epoch  8, batch     1 | loss: 2.3799510Losses:  2.7192721366882324 0.23918533325195312 1.0228753089904785
MemoryTrain:  epoch  8, batch     2 | loss: 2.7192721Losses:  2.561497926712036 -0.0 1.0282056331634521
MemoryTrain:  epoch  8, batch     3 | loss: 2.5614979Losses:  2.400224447250366 0.2870464324951172 0.8365476727485657
MemoryTrain:  epoch  8, batch     4 | loss: 2.4002244Losses:  2.5360841751098633 0.25726065039634705 0.8062160611152649
MemoryTrain:  epoch  8, batch     5 | loss: 2.5360842Losses:  3.1990392208099365 0.5627405047416687 0.7984251976013184
MemoryTrain:  epoch  8, batch     6 | loss: 3.1990392Losses:  3.324010133743286 1.1668275594711304 0.6990886330604553
MemoryTrain:  epoch  8, batch     7 | loss: 3.3240101Losses:  2.6433916091918945 0.24126963317394257 0.9812551736831665
MemoryTrain:  epoch  8, batch     8 | loss: 2.6433916Losses:  1.7681432962417603 -0.0 0.508375883102417
MemoryTrain:  epoch  8, batch     9 | loss: 1.7681433Losses:  2.662158489227295 0.5063736438751221 0.8574447631835938
MemoryTrain:  epoch  9, batch     0 | loss: 2.6621585Losses:  2.7172765731811523 0.4623671770095825 0.9583548307418823
MemoryTrain:  epoch  9, batch     1 | loss: 2.7172766Losses:  2.936795711517334 0.5287649631500244 0.8528202772140503
MemoryTrain:  epoch  9, batch     2 | loss: 2.9367957Losses:  2.9669017791748047 0.4915931820869446 0.9642683267593384
MemoryTrain:  epoch  9, batch     3 | loss: 2.9669018Losses:  3.45694899559021 0.6809273958206177 0.7549841403961182
MemoryTrain:  epoch  9, batch     4 | loss: 3.4569490Losses:  2.5099942684173584 0.4977010488510132 0.7962218523025513
MemoryTrain:  epoch  9, batch     5 | loss: 2.5099943Losses:  3.133094310760498 0.8412942290306091 0.8015102744102478
MemoryTrain:  epoch  9, batch     6 | loss: 3.1330943Losses:  2.7649035453796387 0.7860082387924194 0.7340551018714905
MemoryTrain:  epoch  9, batch     7 | loss: 2.7649035Losses:  3.116589069366455 0.987724781036377 0.8320115208625793
MemoryTrain:  epoch  9, batch     8 | loss: 3.1165891Losses:  1.6153758764266968 -0.0 0.4416722357273102
MemoryTrain:  epoch  9, batch     9 | loss: 1.6153759
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 33.93%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 38.28%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 41.67%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 43.75%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 46.35%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 47.60%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:   14 | acc: 50.00%,  total acc: 47.08%   [EVAL] batch:   15 | acc: 37.50%,  total acc: 46.48%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 47.06%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 47.57%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 49.01%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 51.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 53.57%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 55.68%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 57.61%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 59.11%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 60.75%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 61.06%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 60.42%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 60.27%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 59.91%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 59.79%   [EVAL] batch:   30 | acc: 56.25%,  total acc: 59.68%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 59.77%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 59.85%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 59.01%   [EVAL] batch:   34 | acc: 56.25%,  total acc: 58.93%   [EVAL] batch:   35 | acc: 31.25%,  total acc: 58.16%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 57.09%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 57.57%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 57.69%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 58.28%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 58.54%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 59.08%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 59.45%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 59.66%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 59.86%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 60.33%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 60.77%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 61.07%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 61.61%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 62.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 63.46%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 64.15%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 65.45%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 66.07%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 67.13%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 67.58%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 69.15%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 68.85%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.97%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.64%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.26%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.62%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.68%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.61%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 85.11%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 84.82%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 84.80%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 85.03%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 85.98%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 86.19%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 86.22%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 85.69%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 85.19%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 84.44%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 83.72%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 83.29%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 82.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 82.84%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.02%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 82.87%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 82.39%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 82.48%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 82.35%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 81.90%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 81.99%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 82.19%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 82.27%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 82.06%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 82.24%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 82.23%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 82.31%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 82.37%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 82.35%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 82.25%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 82.32%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 82.39%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 82.20%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 82.19%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 82.18%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 81.92%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 82.07%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 82.06%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 81.97%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 81.80%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 81.71%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 81.40%   [EVAL] batch:   83 | acc: 81.25%,  total acc: 81.40%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   85 | acc: 68.75%,  total acc: 81.10%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 80.68%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 80.75%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 80.83%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 80.83%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 80.98%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 80.77%   [EVAL] batch:   92 | acc: 68.75%,  total acc: 80.65%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 80.39%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 80.26%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 79.95%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 79.77%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 79.46%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 79.29%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 79.06%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 78.90%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 78.86%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 78.76%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 78.73%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 78.60%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 78.33%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 77.84%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 77.41%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 77.10%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 76.69%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 76.45%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 76.38%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 76.59%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 76.99%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 77.19%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 77.38%   [EVAL] batch:  118 | acc: 100.00%,  total acc: 77.57%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 77.71%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 77.79%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 77.92%   [EVAL] batch:  122 | acc: 87.50%,  total acc: 78.00%   [EVAL] batch:  123 | acc: 100.00%,  total acc: 78.18%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 78.30%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 78.22%   [EVAL] batch:  126 | acc: 68.75%,  total acc: 78.15%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 78.17%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 78.15%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 78.17%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 78.20%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 78.27%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 78.38%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 78.36%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 78.38%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.54%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 78.65%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 78.49%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 78.01%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 77.68%   [EVAL] batch:  140 | acc: 31.25%,  total acc: 77.35%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 77.02%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 76.70%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 76.35%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.63%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.74%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 76.90%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.06%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.17%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 76.95%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 76.64%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 76.59%   [EVAL] batch:  153 | acc: 62.50%,  total acc: 76.50%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 76.33%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 76.08%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 76.11%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 76.15%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 76.30%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.41%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 76.51%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 76.62%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 76.73%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 76.71%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 76.78%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 76.81%   [EVAL] batch:  166 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 77.18%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 77.24%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 77.34%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 77.49%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 77.71%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 77.66%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 77.68%   [EVAL] batch:  177 | acc: 75.00%,  total acc: 77.67%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 77.65%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:  180 | acc: 75.00%,  total acc: 77.76%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 77.58%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 77.46%   [EVAL] batch:  183 | acc: 68.75%,  total acc: 77.41%   [EVAL] batch:  184 | acc: 56.25%,  total acc: 77.30%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 77.15%   [EVAL] batch:  186 | acc: 50.00%,  total acc: 77.01%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 77.03%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 77.12%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 77.14%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 77.19%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 77.18%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 77.35%   [EVAL] batch:  194 | acc: 50.00%,  total acc: 77.21%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 77.14%   [EVAL] batch:  196 | acc: 43.75%,  total acc: 76.97%   [EVAL] batch:  197 | acc: 43.75%,  total acc: 76.80%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 76.57%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 76.38%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 76.37%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 76.39%   [EVAL] batch:  202 | acc: 93.75%,  total acc: 76.48%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 76.41%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 76.40%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 76.37%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 76.27%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 75.96%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 75.63%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 75.39%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 75.09%   [EVAL] batch:  211 | acc: 12.50%,  total acc: 74.79%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 74.68%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 74.91%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.03%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 75.34%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 75.23%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 75.23%   [EVAL] batch:  221 | acc: 56.25%,  total acc: 75.14%   [EVAL] batch:  222 | acc: 56.25%,  total acc: 75.06%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 75.06%   [EVAL] batch:  224 | acc: 31.25%,  total acc: 74.86%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 74.97%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 75.08%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 75.41%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 75.49%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 75.70%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 75.80%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 75.90%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 76.18%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 76.23%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 76.30%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 76.40%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 76.57%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 76.73%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 76.82%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 76.92%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:  250 | acc: 18.75%,  total acc: 76.84%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 76.66%   [EVAL] batch:  252 | acc: 25.00%,  total acc: 76.46%   [EVAL] batch:  253 | acc: 37.50%,  total acc: 76.30%   [EVAL] batch:  254 | acc: 43.75%,  total acc: 76.18%   [EVAL] batch:  255 | acc: 25.00%,  total acc: 75.98%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 75.90%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 75.87%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 75.84%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 75.79%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 75.69%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 75.67%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 75.62%   [EVAL] batch:  263 | acc: 37.50%,  total acc: 75.47%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 75.38%   [EVAL] batch:  265 | acc: 37.50%,  total acc: 75.23%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 75.16%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 75.09%   [EVAL] batch:  268 | acc: 75.00%,  total acc: 75.09%   [EVAL] batch:  269 | acc: 93.75%,  total acc: 75.16%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 75.50%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 75.59%   [EVAL] batch:  275 | acc: 68.75%,  total acc: 75.57%   [EVAL] batch:  276 | acc: 43.75%,  total acc: 75.45%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 75.38%   [EVAL] batch:  278 | acc: 50.00%,  total acc: 75.29%   [EVAL] batch:  279 | acc: 56.25%,  total acc: 75.22%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 75.16%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 75.11%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 75.07%   [EVAL] batch:  283 | acc: 31.25%,  total acc: 74.91%   [EVAL] batch:  284 | acc: 56.25%,  total acc: 74.85%   [EVAL] batch:  285 | acc: 31.25%,  total acc: 74.69%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 74.50%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 74.50%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 74.46%   [EVAL] batch:  289 | acc: 81.25%,  total acc: 74.48%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 74.46%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 74.49%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 74.49%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 74.47%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 74.45%   [EVAL] batch:  295 | acc: 81.25%,  total acc: 74.47%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 74.49%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 74.50%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 74.54%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 74.56%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 74.65%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 74.81%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 74.90%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 74.98%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 75.14%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 75.20%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 75.26%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 75.42%   
cur_acc:  ['0.9494', '0.7302', '0.7381', '0.7778', '0.6885']
his_acc:  ['0.9494', '0.8310', '0.7882', '0.7758', '0.7542']
Clustering into  29  clusters
Clusters:  [ 2 16 18 13  7  7 19  0  6  5  8  0  0  7  4  5  2  7 14 14  3 23 21 11
  4  7 25  2  0  3 13  2  2  6 27  9 22  1  0  6 16 24 11  3  0 15 10  9
  1 28 26 14  8 12 17 14 20 28  5  2]
Losses:  10.880095481872559 3.6982154846191406 0.7271602153778076
CurrentTrain: epoch  0, batch     0 | loss: 10.8800955Losses:  10.372312545776367 3.5553231239318848 0.764411449432373
CurrentTrain: epoch  0, batch     1 | loss: 10.3723125Losses:  10.256916046142578 4.340563774108887 0.7134891748428345
CurrentTrain: epoch  0, batch     2 | loss: 10.2569160Losses:  7.050108909606934 -0.0 0.0985189825296402
CurrentTrain: epoch  0, batch     3 | loss: 7.0501089Losses:  7.4357590675354 2.1654529571533203 0.7801119685173035
CurrentTrain: epoch  1, batch     0 | loss: 7.4357591Losses:  9.453522682189941 3.719327688217163 0.7150314450263977
CurrentTrain: epoch  1, batch     1 | loss: 9.4535227Losses:  8.638628959655762 3.6018452644348145 0.6378904581069946
CurrentTrain: epoch  1, batch     2 | loss: 8.6386290Losses:  5.921992301940918 -0.0 0.10543651878833771
CurrentTrain: epoch  1, batch     3 | loss: 5.9219923Losses:  9.231734275817871 3.7934672832489014 0.6541825532913208
CurrentTrain: epoch  2, batch     0 | loss: 9.2317343Losses:  7.005003452301025 2.642298698425293 0.6884528398513794
CurrentTrain: epoch  2, batch     1 | loss: 7.0050035Losses:  6.308100700378418 1.9514129161834717 0.767795741558075
CurrentTrain: epoch  2, batch     2 | loss: 6.3081007Losses:  6.110518932342529 -0.0 0.1558757871389389
CurrentTrain: epoch  2, batch     3 | loss: 6.1105189Losses:  8.675902366638184 3.6620612144470215 0.70894855260849
CurrentTrain: epoch  3, batch     0 | loss: 8.6759024Losses:  6.569599151611328 2.5780248641967773 0.6910459995269775
CurrentTrain: epoch  3, batch     1 | loss: 6.5695992Losses:  6.720257759094238 2.5282788276672363 0.6895568370819092
CurrentTrain: epoch  3, batch     2 | loss: 6.7202578Losses:  2.071054458618164 -0.0 0.15400180220603943
CurrentTrain: epoch  3, batch     3 | loss: 2.0710545Losses:  8.385356903076172 4.356227397918701 0.6180033087730408
CurrentTrain: epoch  4, batch     0 | loss: 8.3853569Losses:  7.880880832672119 3.4871158599853516 0.7659726142883301
CurrentTrain: epoch  4, batch     1 | loss: 7.8808808Losses:  6.324142932891846 2.5379252433776855 0.7018154859542847
CurrentTrain: epoch  4, batch     2 | loss: 6.3241429Losses:  3.440004825592041 -0.0 0.13660182058811188
CurrentTrain: epoch  4, batch     3 | loss: 3.4400048Losses:  6.117110252380371 2.380561351776123 0.6906379461288452
CurrentTrain: epoch  5, batch     0 | loss: 6.1171103Losses:  6.374072074890137 2.5385003089904785 0.7474650144577026
CurrentTrain: epoch  5, batch     1 | loss: 6.3740721Losses:  5.814422607421875 2.3436455726623535 0.737315833568573
CurrentTrain: epoch  5, batch     2 | loss: 5.8144226Losses:  2.2797956466674805 -0.0 0.09779711812734604
CurrentTrain: epoch  5, batch     3 | loss: 2.2797956Losses:  6.985438823699951 3.2365942001342773 0.5986456871032715
CurrentTrain: epoch  6, batch     0 | loss: 6.9854388Losses:  5.693667411804199 2.395601749420166 0.6586868762969971
CurrentTrain: epoch  6, batch     1 | loss: 5.6936674Losses:  6.139163494110107 3.068202495574951 0.6014592051506042
CurrentTrain: epoch  6, batch     2 | loss: 6.1391635Losses:  2.59455943107605 -0.0 0.11151431500911713
CurrentTrain: epoch  6, batch     3 | loss: 2.5945594Losses:  6.9741082191467285 3.4049434661865234 0.6834158897399902
CurrentTrain: epoch  7, batch     0 | loss: 6.9741082Losses:  5.86083984375 2.902102470397949 0.6427098512649536
CurrentTrain: epoch  7, batch     1 | loss: 5.8608398Losses:  4.800466060638428 1.668508768081665 0.7236073613166809
CurrentTrain: epoch  7, batch     2 | loss: 4.8004661Losses:  3.400599241256714 -0.0 0.1534930169582367
CurrentTrain: epoch  7, batch     3 | loss: 3.4005992Losses:  5.750908374786377 2.7027339935302734 0.6501480340957642
CurrentTrain: epoch  8, batch     0 | loss: 5.7509084Losses:  6.669244766235352 3.5671467781066895 0.6759911775588989
CurrentTrain: epoch  8, batch     1 | loss: 6.6692448Losses:  5.414638519287109 2.286062240600586 0.7250510454177856
CurrentTrain: epoch  8, batch     2 | loss: 5.4146385Losses:  2.153282642364502 -0.0 0.11636906117200851
CurrentTrain: epoch  8, batch     3 | loss: 2.1532826Losses:  5.4024176597595215 2.3669261932373047 0.657497227191925
CurrentTrain: epoch  9, batch     0 | loss: 5.4024177Losses:  5.212643146514893 2.5935921669006348 0.6328970789909363
CurrentTrain: epoch  9, batch     1 | loss: 5.2126431Losses:  6.287322044372559 3.3894529342651367 0.6502211093902588
CurrentTrain: epoch  9, batch     2 | loss: 6.2873220Losses:  3.237922191619873 -0.0 0.14403700828552246
CurrentTrain: epoch  9, batch     3 | loss: 3.2379222
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: She has been a professional player in the world of tennis, winning several championships throughout her career.  
Head Entity: She  
Tail Entity: tennis  

Relation: sport  
Context: The city is known for its vibrant soccer culture, with numerous teams competing in the national league.  
Head Entity: The city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of two daughters, emily and sarah, who both excelled in their studies.  
Head Entity: michael  
Tail Entity: emily  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: he also made a special appearance in kbs ' romantic comedy drama " fight for my way " .
Head Entity: fight for my way
Tail Entity: kbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series "breaking bad" was first aired on amc, gaining a massive following.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: the hit show "stranger things" premiered on netflix and quickly became a cultural phenomenon.  
Head Entity: stranger things  
Tail Entity: netflix  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English before being dubbed into several other languages.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was filmed in French and has since gained a global audience.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: the league champions for the fifth time in their history ( and the second season running ) were taunton town , but runners - up mangotsfield united took promotion to the southern league .
Head Entity: mangotsfield united
Tail Entity: southern league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After an impressive season, the team secured a spot in the prestigious tournament, representing their city in the national league, where they faced off against top competitors like the city rivals.  
Head Entity: city rivals  
Tail Entity: national league  

Relation: league  
Context: The young athlete showcased exceptional talent during the trials, earning a place in the regional league, which is known for nurturing future stars in the sport.  
Head Entity: young athlete  
Tail Entity: regional league  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: other teammates included dominique leray and élisabeth riffiod , whose son boris diaw currently plays in the nba for the spurs ( 2014 ) .
Head Entity: boris diaw
Tail Entity: élisabeth riffiod
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in a recent interview, actress jennifer garner spoke fondly of her children, including violet affleck, whom she describes as a wonderful daughter.  
Head Entity: violet affleck  
Tail Entity: jennifer garner  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much emily loved her mother, sarah, as they shared stories and laughter together.  
Head Entity: emily  
Tail Entity: sarah  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: the type locality given is " fan - si - pan mountains , tonkin , indo - china " ( mount fansipan , sa pa district , nghệ an province , northwestern vietnam ) .
Head Entity: sa pa district
Tail Entity: vietnam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the capital city of the country is located in the central region, known for its rich history and cultural heritage, particularly in the area of traditional crafts.  
Head Entity: capital city  
Tail Entity: vietnam  

Relation: country  
Context: the famous ancient ruins are situated in a region that was once the heart of a powerful empire, now recognized as a UNESCO World Heritage site.  
Head Entity: ancient ruins  
Tail Entity: cambodia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Losses:  3.147869348526001 -0.0 1.0291597843170166
MemoryTrain:  epoch  0, batch     0 | loss: 3.1478693Losses:  4.63090705871582 0.49857297539711 0.8471471667289734
MemoryTrain:  epoch  0, batch     1 | loss: 4.6309071Losses:  4.171665191650391 0.4751451313495636 0.8748078346252441
MemoryTrain:  epoch  0, batch     2 | loss: 4.1716652Losses:  3.750627040863037 0.24800968170166016 0.8547967076301575
MemoryTrain:  epoch  0, batch     3 | loss: 3.7506270Losses:  3.497007369995117 0.3125392198562622 0.9268794059753418
MemoryTrain:  epoch  0, batch     4 | loss: 3.4970074Losses:  3.985366106033325 0.2617107629776001 0.9395696520805359
MemoryTrain:  epoch  0, batch     5 | loss: 3.9853661Losses:  4.411345481872559 0.7298017144203186 0.9768895506858826
MemoryTrain:  epoch  0, batch     6 | loss: 4.4113455Losses:  4.666003227233887 0.6461506485939026 0.9312203526496887
MemoryTrain:  epoch  0, batch     7 | loss: 4.6660032Losses:  5.241199970245361 0.451730877161026 0.9685117602348328
MemoryTrain:  epoch  0, batch     8 | loss: 5.2412000Losses:  3.496138572692871 0.4675002098083496 0.9642083644866943
MemoryTrain:  epoch  0, batch     9 | loss: 3.4961386Losses:  3.7367067337036133 -0.0 0.9658297300338745
MemoryTrain:  epoch  0, batch    10 | loss: 3.7367067Losses:  1.9452126026153564 -0.0 0.36054375767707825
MemoryTrain:  epoch  0, batch    11 | loss: 1.9452126Losses:  3.116852283477783 0.2360592782497406 1.0176215171813965
MemoryTrain:  epoch  1, batch     0 | loss: 3.1168523Losses:  4.076790809631348 0.3439261317253113 0.933149516582489
MemoryTrain:  epoch  1, batch     1 | loss: 4.0767908Losses:  4.433243751525879 0.8545007109642029 0.860012948513031
MemoryTrain:  epoch  1, batch     2 | loss: 4.4332438Losses:  3.54978609085083 -0.0 1.006680965423584
MemoryTrain:  epoch  1, batch     3 | loss: 3.5497861Losses:  4.100274085998535 0.3749944567680359 0.9510864019393921
MemoryTrain:  epoch  1, batch     4 | loss: 4.1002741Losses:  3.276916980743408 0.514136791229248 0.9225374460220337
MemoryTrain:  epoch  1, batch     5 | loss: 3.2769170Losses:  3.227146625518799 -0.0 1.0130596160888672
MemoryTrain:  epoch  1, batch     6 | loss: 3.2271466Losses:  3.1621642112731934 0.5268643498420715 0.7464709877967834
MemoryTrain:  epoch  1, batch     7 | loss: 3.1621642Losses:  4.739366054534912 0.6412242650985718 0.8450848460197449
MemoryTrain:  epoch  1, batch     8 | loss: 4.7393661Losses:  2.9452431201934814 0.48086491227149963 0.9352643489837646
MemoryTrain:  epoch  1, batch     9 | loss: 2.9452431Losses:  4.144082546234131 0.3575553894042969 0.9321150779724121
MemoryTrain:  epoch  1, batch    10 | loss: 4.1440825Losses:  1.4470469951629639 -0.0 0.24184656143188477
MemoryTrain:  epoch  1, batch    11 | loss: 1.4470470Losses:  2.9330480098724365 0.2276727557182312 0.8707049489021301
MemoryTrain:  epoch  2, batch     0 | loss: 2.9330480Losses:  3.416234254837036 0.2883540391921997 0.9068462252616882
MemoryTrain:  epoch  2, batch     1 | loss: 3.4162343Losses:  2.6724233627319336 -0.0 0.9130076169967651
MemoryTrain:  epoch  2, batch     2 | loss: 2.6724234Losses:  3.586418628692627 0.5142012238502502 0.9091577529907227
MemoryTrain:  epoch  2, batch     3 | loss: 3.5864186Losses:  4.138177871704102 1.2585457563400269 0.7837561368942261
MemoryTrain:  epoch  2, batch     4 | loss: 4.1381779Losses:  2.4602251052856445 -0.0 0.9207086563110352
MemoryTrain:  epoch  2, batch     5 | loss: 2.4602251Losses:  3.6764330863952637 0.2777903079986572 1.039306402206421
MemoryTrain:  epoch  2, batch     6 | loss: 3.6764331Losses:  3.058654546737671 -0.0 1.0399141311645508
MemoryTrain:  epoch  2, batch     7 | loss: 3.0586545Losses:  4.137208461761475 0.2987736463546753 0.9077593684196472
MemoryTrain:  epoch  2, batch     8 | loss: 4.1372085Losses:  3.124241828918457 -0.0 1.0167737007141113
MemoryTrain:  epoch  2, batch     9 | loss: 3.1242418Losses:  2.583879232406616 0.2544330954551697 0.7731101512908936
MemoryTrain:  epoch  2, batch    10 | loss: 2.5838792Losses:  2.9843716621398926 -0.0 0.33073052763938904
MemoryTrain:  epoch  2, batch    11 | loss: 2.9843717Losses:  3.640782117843628 0.6233413815498352 0.7998875975608826
MemoryTrain:  epoch  3, batch     0 | loss: 3.6407821Losses:  2.669297695159912 -0.0 0.9057180285453796
MemoryTrain:  epoch  3, batch     1 | loss: 2.6692977Losses:  3.089832305908203 -0.0 0.9305448532104492
MemoryTrain:  epoch  3, batch     2 | loss: 3.0898323Losses:  3.348083257675171 -0.0 1.0167272090911865
MemoryTrain:  epoch  3, batch     3 | loss: 3.3480833Losses:  3.5198373794555664 0.7410032749176025 0.951840341091156
MemoryTrain:  epoch  3, batch     4 | loss: 3.5198374Losses:  3.086905002593994 0.47148019075393677 0.8549687266349792
MemoryTrain:  epoch  3, batch     5 | loss: 3.0869050Losses:  3.7502055168151855 0.76053386926651 0.8551639318466187
MemoryTrain:  epoch  3, batch     6 | loss: 3.7502055Losses:  2.803574323654175 0.5158987045288086 0.9096185564994812
MemoryTrain:  epoch  3, batch     7 | loss: 2.8035743Losses:  3.107923984527588 0.23114615678787231 0.908103346824646
MemoryTrain:  epoch  3, batch     8 | loss: 3.1079240Losses:  2.2473013401031494 -0.0 0.9180499911308289
MemoryTrain:  epoch  3, batch     9 | loss: 2.2473013Losses:  2.876431941986084 -0.0 0.9796239137649536
MemoryTrain:  epoch  3, batch    10 | loss: 2.8764319Losses:  1.610227108001709 -0.0 0.4215037524700165
MemoryTrain:  epoch  3, batch    11 | loss: 1.6102271Losses:  3.876946210861206 0.6222310066223145 0.862593948841095
MemoryTrain:  epoch  4, batch     0 | loss: 3.8769462Losses:  2.892540454864502 0.2567293643951416 0.8912991285324097
MemoryTrain:  epoch  4, batch     1 | loss: 2.8925405Losses:  3.1211185455322266 0.28084978461265564 0.8673161864280701
MemoryTrain:  epoch  4, batch     2 | loss: 3.1211185Losses:  3.620816946029663 0.350399911403656 0.8820548057556152
MemoryTrain:  epoch  4, batch     3 | loss: 3.6208169Losses:  2.8684399127960205 0.2304697036743164 0.860819399356842
MemoryTrain:  epoch  4, batch     4 | loss: 2.8684399Losses:  2.7410335540771484 0.50594562292099 0.9678852558135986
MemoryTrain:  epoch  4, batch     5 | loss: 2.7410336Losses:  2.5715537071228027 -0.0 0.8434522151947021
MemoryTrain:  epoch  4, batch     6 | loss: 2.5715537Losses:  3.0313544273376465 0.516681969165802 0.8589738607406616
MemoryTrain:  epoch  4, batch     7 | loss: 3.0313544Losses:  2.370682954788208 -0.0 0.9659379720687866
MemoryTrain:  epoch  4, batch     8 | loss: 2.3706830Losses:  3.107823610305786 0.2506987452507019 0.9829941391944885
MemoryTrain:  epoch  4, batch     9 | loss: 3.1078236Losses:  2.7676925659179688 0.493422269821167 0.9669115543365479
MemoryTrain:  epoch  4, batch    10 | loss: 2.7676926Losses:  3.355182647705078 -0.0 0.36083924770355225
MemoryTrain:  epoch  4, batch    11 | loss: 3.3551826Losses:  2.4394068717956543 0.24822810292243958 0.8404514193534851
MemoryTrain:  epoch  5, batch     0 | loss: 2.4394069Losses:  3.035429000854492 0.23054379224777222 0.869097888469696
MemoryTrain:  epoch  5, batch     1 | loss: 3.0354290Losses:  2.516268253326416 -0.0 0.941440224647522
MemoryTrain:  epoch  5, batch     2 | loss: 2.5162683Losses:  2.7305800914764404 0.49233296513557434 0.7624761462211609
MemoryTrain:  epoch  5, batch     3 | loss: 2.7305801Losses:  3.1541969776153564 0.4824250638484955 0.9486181139945984
MemoryTrain:  epoch  5, batch     4 | loss: 3.1541970Losses:  3.3276078701019287 0.2842985987663269 0.9762933254241943
MemoryTrain:  epoch  5, batch     5 | loss: 3.3276079Losses:  3.1719326972961426 0.26947659254074097 0.9549719095230103
MemoryTrain:  epoch  5, batch     6 | loss: 3.1719327Losses:  2.632720947265625 -0.0 0.9617893695831299
MemoryTrain:  epoch  5, batch     7 | loss: 2.6327209Losses:  2.7322921752929688 0.25007563829421997 0.9127949476242065
MemoryTrain:  epoch  5, batch     8 | loss: 2.7322922Losses:  2.613480567932129 0.2865866422653198 0.9082679748535156
MemoryTrain:  epoch  5, batch     9 | loss: 2.6134806Losses:  2.1898066997528076 0.2374858260154724 0.6944447159767151
MemoryTrain:  epoch  5, batch    10 | loss: 2.1898067Losses:  1.5692718029022217 -0.0 0.30496183037757874
MemoryTrain:  epoch  5, batch    11 | loss: 1.5692718Losses:  2.675901174545288 0.27950364351272583 0.9438030123710632
MemoryTrain:  epoch  6, batch     0 | loss: 2.6759012Losses:  2.6166834831237793 0.23994357883930206 0.9609954953193665
MemoryTrain:  epoch  6, batch     1 | loss: 2.6166835Losses:  2.8124823570251465 0.24989715218544006 0.9577556848526001
MemoryTrain:  epoch  6, batch     2 | loss: 2.8124824Losses:  2.819999933242798 0.5299137234687805 0.7879376411437988
MemoryTrain:  epoch  6, batch     3 | loss: 2.8199999Losses:  3.194775104522705 0.5414203405380249 0.9682392477989197
MemoryTrain:  epoch  6, batch     4 | loss: 3.1947751Losses:  2.4628372192382812 0.25849416851997375 0.852907121181488
MemoryTrain:  epoch  6, batch     5 | loss: 2.4628372Losses:  2.6124343872070312 -0.0 1.0584349632263184
MemoryTrain:  epoch  6, batch     6 | loss: 2.6124344Losses:  2.9172263145446777 0.755042552947998 0.7908004522323608
MemoryTrain:  epoch  6, batch     7 | loss: 2.9172263Losses:  3.000973701477051 0.32347744703292847 0.8931745886802673
MemoryTrain:  epoch  6, batch     8 | loss: 3.0009737Losses:  2.6965384483337402 0.2361724078655243 0.9257835745811462
MemoryTrain:  epoch  6, batch     9 | loss: 2.6965384Losses:  2.8993101119995117 0.22516828775405884 0.8344835042953491
MemoryTrain:  epoch  6, batch    10 | loss: 2.8993101Losses:  1.6153779029846191 -0.0 0.20628325641155243
MemoryTrain:  epoch  6, batch    11 | loss: 1.6153779Losses:  3.279763698577881 0.44605955481529236 1.000806450843811
MemoryTrain:  epoch  7, batch     0 | loss: 3.2797637Losses:  2.410688877105713 -0.0 0.9081538915634155
MemoryTrain:  epoch  7, batch     1 | loss: 2.4106889Losses:  2.7940382957458496 0.3275541067123413 0.896007776260376
MemoryTrain:  epoch  7, batch     2 | loss: 2.7940383Losses:  2.7607507705688477 0.28897374868392944 0.9660236835479736
MemoryTrain:  epoch  7, batch     3 | loss: 2.7607508Losses:  2.5273280143737793 0.477064311504364 0.784805953502655
MemoryTrain:  epoch  7, batch     4 | loss: 2.5273280Losses:  3.2384796142578125 1.0822807550430298 0.6854230165481567
MemoryTrain:  epoch  7, batch     5 | loss: 3.2384796Losses:  2.3440403938293457 0.24484850466251373 0.7047872543334961
MemoryTrain:  epoch  7, batch     6 | loss: 2.3440404Losses:  2.999758720397949 -0.0 0.9795438051223755
MemoryTrain:  epoch  7, batch     7 | loss: 2.9997587Losses:  2.227447748184204 0.257157564163208 0.7289549708366394
MemoryTrain:  epoch  7, batch     8 | loss: 2.2274477Losses:  2.792509078979492 0.2848440706729889 1.013895869255066
MemoryTrain:  epoch  7, batch     9 | loss: 2.7925091Losses:  2.437969207763672 0.2525342106819153 0.8485667705535889
MemoryTrain:  epoch  7, batch    10 | loss: 2.4379692Losses:  2.075047016143799 0.2621994912624359 0.21917468309402466
MemoryTrain:  epoch  7, batch    11 | loss: 2.0750470Losses:  2.2310900688171387 -0.0 0.8382819890975952
MemoryTrain:  epoch  8, batch     0 | loss: 2.2310901Losses:  2.7601418495178223 0.2564407289028168 0.9769672751426697
MemoryTrain:  epoch  8, batch     1 | loss: 2.7601418Losses:  2.522461175918579 0.2660837769508362 0.9564375877380371
MemoryTrain:  epoch  8, batch     2 | loss: 2.5224612Losses:  2.2845895290374756 -0.0 0.900459885597229
MemoryTrain:  epoch  8, batch     3 | loss: 2.2845895Losses:  2.915228843688965 0.5293698310852051 0.8822120428085327
MemoryTrain:  epoch  8, batch     4 | loss: 2.9152288Losses:  2.3406307697296143 0.23415198922157288 0.7171211838722229
MemoryTrain:  epoch  8, batch     5 | loss: 2.3406308Losses:  2.503669500350952 0.24067315459251404 0.958116888999939
MemoryTrain:  epoch  8, batch     6 | loss: 2.5036695Losses:  2.7451276779174805 0.5191987752914429 0.9044389724731445
MemoryTrain:  epoch  8, batch     7 | loss: 2.7451277Losses:  2.206962823867798 -0.0 0.8430449962615967
MemoryTrain:  epoch  8, batch     8 | loss: 2.2069628Losses:  2.5325629711151123 0.26375967264175415 0.9180355072021484
MemoryTrain:  epoch  8, batch     9 | loss: 2.5325630Losses:  2.633478879928589 0.4757812023162842 0.8349424004554749
MemoryTrain:  epoch  8, batch    10 | loss: 2.6334789Losses:  1.5624613761901855 -0.0 0.23166078329086304
MemoryTrain:  epoch  8, batch    11 | loss: 1.5624614Losses:  2.745579242706299 0.2659279704093933 1.062768816947937
MemoryTrain:  epoch  9, batch     0 | loss: 2.7455792Losses:  2.2517759799957275 -0.0 0.9299799799919128
MemoryTrain:  epoch  9, batch     1 | loss: 2.2517760Losses:  2.856640100479126 0.7116248607635498 0.8397353291511536
MemoryTrain:  epoch  9, batch     2 | loss: 2.8566401Losses:  2.4207406044006348 0.2315969169139862 0.9522348046302795
MemoryTrain:  epoch  9, batch     3 | loss: 2.4207406Losses:  2.6744189262390137 0.4765665531158447 0.9267069697380066
MemoryTrain:  epoch  9, batch     4 | loss: 2.6744189Losses:  2.7273330688476562 0.49269288778305054 0.8795525431632996
MemoryTrain:  epoch  9, batch     5 | loss: 2.7273331Losses:  2.6359450817108154 0.2581489682197571 0.9091100692749023
MemoryTrain:  epoch  9, batch     6 | loss: 2.6359451Losses:  2.6363537311553955 0.28708502650260925 0.9605345129966736
MemoryTrain:  epoch  9, batch     7 | loss: 2.6363537Losses:  2.3620402812957764 0.26050227880477905 0.8143278956413269
MemoryTrain:  epoch  9, batch     8 | loss: 2.3620403Losses:  2.731985092163086 0.520119309425354 0.9081987142562866
MemoryTrain:  epoch  9, batch     9 | loss: 2.7319851Losses:  2.230503797531128 -0.0 0.9003462195396423
MemoryTrain:  epoch  9, batch    10 | loss: 2.2305038Losses:  1.554375410079956 -0.0 0.27132341265678406
MemoryTrain:  epoch  9, batch    11 | loss: 1.5543754
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 9.38%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 8.33%   [EVAL] batch:    3 | acc: 0.00%,  total acc: 6.25%   [EVAL] batch:    4 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    5 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 11.61%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 18.75%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 25.00%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 30.63%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 36.36%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 39.06%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 41.83%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 44.64%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 47.50%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 51.84%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 54.17%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 55.59%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 57.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.82%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 61.36%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.04%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 65.25%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 66.35%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:   30 | acc: 87.50%,  total acc: 70.77%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 74.34%   [EVAL] batch:   38 | acc: 18.75%,  total acc: 72.92%   [EVAL] batch:   39 | acc: 6.25%,  total acc: 71.25%   [EVAL] batch:   40 | acc: 25.00%,  total acc: 70.12%   [EVAL] batch:   41 | acc: 25.00%,  total acc: 69.05%   [EVAL] batch:   42 | acc: 0.00%,  total acc: 67.44%   [EVAL] batch:   43 | acc: 43.75%,  total acc: 66.90%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 67.64%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 68.07%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 70.38%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 70.34%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 69.81%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 69.33%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 69.20%   [EVAL] batch:   55 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 68.53%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 68.86%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 69.15%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 68.65%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 69.53%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 77.72%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 77.86%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.58%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 83.14%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 82.35%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.14%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 82.12%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 82.26%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 82.57%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.01%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.44%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 83.69%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 83.78%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 83.87%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.95%   [EVAL] batch:   44 | acc: 56.25%,  total acc: 83.33%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 82.47%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 81.65%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 80.86%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 80.10%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 79.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 79.53%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 79.57%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 79.60%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 79.40%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 78.75%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 78.68%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 78.51%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 78.02%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 77.97%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 77.92%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 77.77%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 77.42%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 76.88%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 75.88%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 74.81%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 73.86%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 73.04%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 71.97%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 71.29%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 71.92%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 72.17%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 72.47%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 72.42%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 72.45%   [EVAL] batch:   76 | acc: 50.00%,  total acc: 72.16%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 72.04%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 72.07%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 71.95%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 71.91%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 71.69%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 71.73%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 71.69%   [EVAL] batch:   85 | acc: 62.50%,  total acc: 71.58%   [EVAL] batch:   86 | acc: 43.75%,  total acc: 71.26%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 71.45%   [EVAL] batch:   88 | acc: 87.50%,  total acc: 71.63%   [EVAL] batch:   89 | acc: 81.25%,  total acc: 71.74%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:   91 | acc: 68.75%,  total acc: 71.94%   [EVAL] batch:   92 | acc: 75.00%,  total acc: 71.98%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 71.94%   [EVAL] batch:   94 | acc: 62.50%,  total acc: 71.84%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 71.61%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 71.52%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 71.30%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 71.28%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 71.25%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 71.16%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 71.14%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 71.06%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 71.09%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 71.07%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 71.05%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 70.91%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 70.49%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 70.13%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 69.89%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 69.54%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 69.36%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 69.36%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 70.15%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 70.41%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 70.66%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 70.80%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 70.73%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 70.56%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 70.59%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 70.63%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 70.67%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 70.65%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 70.63%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 70.57%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 70.65%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 70.69%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 70.77%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 70.80%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 70.88%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 71.08%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 71.16%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 71.51%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 71.09%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 70.76%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 70.39%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 70.16%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 69.80%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 69.49%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 70.03%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 70.43%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:  150 | acc: 56.25%,  total acc: 70.49%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 70.31%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 70.26%   [EVAL] batch:  153 | acc: 56.25%,  total acc: 70.17%   [EVAL] batch:  154 | acc: 50.00%,  total acc: 70.04%   [EVAL] batch:  155 | acc: 37.50%,  total acc: 69.83%   [EVAL] batch:  156 | acc: 87.50%,  total acc: 69.94%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 70.02%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.35%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.46%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.60%   [EVAL] batch:  162 | acc: 75.00%,  total acc: 70.63%   [EVAL] batch:  163 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 70.61%   [EVAL] batch:  165 | acc: 43.75%,  total acc: 70.44%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 70.55%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 70.57%   [EVAL] batch:  168 | acc: 75.00%,  total acc: 70.60%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 70.40%   [EVAL] batch:  170 | acc: 56.25%,  total acc: 70.32%   [EVAL] batch:  171 | acc: 62.50%,  total acc: 70.28%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 70.05%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 69.94%   [EVAL] batch:  174 | acc: 62.50%,  total acc: 69.89%   [EVAL] batch:  175 | acc: 75.00%,  total acc: 69.92%   [EVAL] batch:  176 | acc: 81.25%,  total acc: 69.99%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 70.05%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 70.11%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  180 | acc: 81.25%,  total acc: 70.34%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 70.30%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 70.18%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 70.24%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 70.20%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 70.09%   [EVAL] batch:  186 | acc: 43.75%,  total acc: 69.95%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 70.01%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 70.14%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 70.23%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 70.35%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 70.38%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 70.53%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:  194 | acc: 50.00%,  total acc: 70.51%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 70.41%   [EVAL] batch:  196 | acc: 37.50%,  total acc: 70.24%   [EVAL] batch:  197 | acc: 18.75%,  total acc: 69.98%   [EVAL] batch:  198 | acc: 43.75%,  total acc: 69.85%   [EVAL] batch:  199 | acc: 31.25%,  total acc: 69.66%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 69.68%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 69.74%   [EVAL] batch:  202 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 69.82%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 69.85%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 69.84%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 69.78%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 69.50%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 69.23%   [EVAL] batch:  209 | acc: 31.25%,  total acc: 69.05%   [EVAL] batch:  210 | acc: 37.50%,  total acc: 68.90%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 68.69%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 68.63%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 69.46%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 69.38%   [EVAL] batch:  220 | acc: 75.00%,  total acc: 69.40%   [EVAL] batch:  221 | acc: 56.25%,  total acc: 69.34%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 69.31%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 69.34%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 69.19%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 69.60%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 69.94%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 70.07%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 70.20%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 70.33%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 70.58%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 70.68%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 70.80%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 70.87%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 71.36%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 71.48%   [EVAL] batch:  245 | acc: 68.75%,  total acc: 71.47%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 71.76%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 71.85%   [EVAL] batch:  250 | acc: 18.75%,  total acc: 71.64%   [EVAL] batch:  251 | acc: 37.50%,  total acc: 71.50%   [EVAL] batch:  252 | acc: 31.25%,  total acc: 71.34%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 71.19%   [EVAL] batch:  254 | acc: 43.75%,  total acc: 71.08%   [EVAL] batch:  255 | acc: 31.25%,  total acc: 70.92%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 70.84%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 70.86%   [EVAL] batch:  258 | acc: 81.25%,  total acc: 70.90%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 70.89%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 70.81%   [EVAL] batch:  261 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 70.79%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 70.69%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 70.61%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 70.54%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 70.48%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 70.45%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 70.52%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 70.58%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 70.69%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 70.80%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 70.90%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 71.09%   [EVAL] batch:  275 | acc: 68.75%,  total acc: 71.08%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 71.03%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 71.02%   [EVAL] batch:  278 | acc: 56.25%,  total acc: 70.97%   [EVAL] batch:  279 | acc: 81.25%,  total acc: 71.00%   [EVAL] batch:  280 | acc: 68.75%,  total acc: 71.00%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 70.99%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 70.98%   [EVAL] batch:  283 | acc: 56.25%,  total acc: 70.93%   [EVAL] batch:  284 | acc: 56.25%,  total acc: 70.88%   [EVAL] batch:  285 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:  286 | acc: 25.00%,  total acc: 70.67%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 70.72%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 70.70%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 70.75%   [EVAL] batch:  290 | acc: 81.25%,  total acc: 70.79%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 70.87%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 70.88%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 70.92%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 70.91%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 70.92%   [EVAL] batch:  296 | acc: 81.25%,  total acc: 70.96%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 70.95%   [EVAL] batch:  298 | acc: 81.25%,  total acc: 70.99%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 71.02%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.31%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 71.40%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 71.73%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 71.80%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  310 | acc: 93.75%,  total acc: 71.95%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 71.98%   [EVAL] batch:  313 | acc: 6.25%,  total acc: 71.78%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 71.59%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 71.36%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 71.14%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 70.95%   [EVAL] batch:  318 | acc: 18.75%,  total acc: 70.79%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 70.80%   [EVAL] batch:  320 | acc: 50.00%,  total acc: 70.74%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 70.77%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 70.84%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 70.85%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 70.87%   [EVAL] batch:  325 | acc: 87.50%,  total acc: 70.92%   [EVAL] batch:  326 | acc: 75.00%,  total acc: 70.93%   [EVAL] batch:  327 | acc: 87.50%,  total acc: 70.98%   [EVAL] batch:  328 | acc: 87.50%,  total acc: 71.03%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 71.06%   [EVAL] batch:  330 | acc: 87.50%,  total acc: 71.11%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 71.18%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 71.27%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 71.42%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 71.59%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 71.62%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 71.66%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 71.73%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 72.00%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 72.21%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 72.27%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 72.43%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 72.26%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 72.07%   [EVAL] batch:  352 | acc: 12.50%,  total acc: 71.90%   [EVAL] batch:  353 | acc: 37.50%,  total acc: 71.80%   [EVAL] batch:  354 | acc: 6.25%,  total acc: 71.62%   [EVAL] batch:  355 | acc: 12.50%,  total acc: 71.45%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 71.48%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 71.54%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 71.61%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 71.76%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 71.81%   [EVAL] batch:  362 | acc: 75.00%,  total acc: 71.81%   [EVAL] batch:  363 | acc: 75.00%,  total acc: 71.82%   [EVAL] batch:  364 | acc: 56.25%,  total acc: 71.78%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 71.72%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 71.66%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 71.62%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 71.54%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 71.57%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 71.58%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 71.54%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 71.51%   [EVAL] batch:  373 | acc: 93.75%,  total acc: 71.57%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 71.60%   
cur_acc:  ['0.9494', '0.7302', '0.7381', '0.7778', '0.6885', '0.6865']
his_acc:  ['0.9494', '0.8310', '0.7882', '0.7758', '0.7542', '0.7160']
Clustering into  34  clusters
Clusters:  [ 8  1 23 16 32 32 22  4  6 18 13  4  4 32 33 18  8 32  7  7  3 25 24  5
  2  1 28  8 30  3 16  8  8  6 29 10 11  0  4  6 31 21  5  3  4 19  9 10
  0 15 26  7 13 14 17  7 20 15 18  8  1  7 28 33  2 27 12  9 15  6]
Losses:  14.557611465454102 6.308724403381348 0.7686479091644287
CurrentTrain: epoch  0, batch     0 | loss: 14.5576115Losses:  10.492684364318848 3.1438448429107666 0.8915436863899231
CurrentTrain: epoch  0, batch     1 | loss: 10.4926844Losses:  11.976978302001953 5.455999851226807 0.740915060043335
CurrentTrain: epoch  0, batch     2 | loss: 11.9769783Losses:  6.595911026000977 -0.0 0.20624566078186035
CurrentTrain: epoch  0, batch     3 | loss: 6.5959110Losses:  10.081597328186035 4.543863773345947 0.6071527004241943
CurrentTrain: epoch  1, batch     0 | loss: 10.0815973Losses:  11.884908676147461 4.8235931396484375 0.7656471729278564
CurrentTrain: epoch  1, batch     1 | loss: 11.8849087Losses:  8.865412712097168 2.854104518890381 0.7749084234237671
CurrentTrain: epoch  1, batch     2 | loss: 8.8654127Losses:  6.88146448135376 -0.0 0.10759095847606659
CurrentTrain: epoch  1, batch     3 | loss: 6.8814645Losses:  7.726656913757324 2.717524290084839 0.7623283863067627
CurrentTrain: epoch  2, batch     0 | loss: 7.7266569Losses:  8.962858200073242 3.1953601837158203 0.7945879101753235
CurrentTrain: epoch  2, batch     1 | loss: 8.9628582Losses:  8.981409072875977 3.420459270477295 0.8089658617973328
CurrentTrain: epoch  2, batch     2 | loss: 8.9814091Losses:  3.808192253112793 -0.0 0.11148680746555328
CurrentTrain: epoch  2, batch     3 | loss: 3.8081923Losses:  7.743825912475586 2.6806206703186035 0.8031924366950989
CurrentTrain: epoch  3, batch     0 | loss: 7.7438259Losses:  8.325057983398438 3.799680233001709 0.6824541091918945
CurrentTrain: epoch  3, batch     1 | loss: 8.3250580Losses:  9.086791038513184 3.9344470500946045 0.675724983215332
CurrentTrain: epoch  3, batch     2 | loss: 9.0867910Losses:  4.070032596588135 -0.0 0.1697738617658615
CurrentTrain: epoch  3, batch     3 | loss: 4.0700326Losses:  10.036124229431152 5.907806396484375 0.6170673370361328
CurrentTrain: epoch  4, batch     0 | loss: 10.0361242Losses:  8.671148300170898 4.606014728546143 0.6691567301750183
CurrentTrain: epoch  4, batch     1 | loss: 8.6711483Losses:  9.601183891296387 4.198366641998291 0.6648133397102356
CurrentTrain: epoch  4, batch     2 | loss: 9.6011839Losses:  2.873969793319702 -0.0 0.12511086463928223
CurrentTrain: epoch  4, batch     3 | loss: 2.8739698Losses:  8.779129028320312 4.202319145202637 0.6981208324432373
CurrentTrain: epoch  5, batch     0 | loss: 8.7791290Losses:  7.035808086395264 3.1554551124572754 0.7548364400863647
CurrentTrain: epoch  5, batch     1 | loss: 7.0358081Losses:  7.618988513946533 3.4148354530334473 0.6867355704307556
CurrentTrain: epoch  5, batch     2 | loss: 7.6189885Losses:  2.5916550159454346 -0.0 0.15578332543373108
CurrentTrain: epoch  5, batch     3 | loss: 2.5916550Losses:  9.305044174194336 4.1535420417785645 0.6882227063179016
CurrentTrain: epoch  6, batch     0 | loss: 9.3050442Losses:  6.259395599365234 2.9531219005584717 0.7354899644851685
CurrentTrain: epoch  6, batch     1 | loss: 6.2593956Losses:  6.987334251403809 2.8759708404541016 0.7707856893539429
CurrentTrain: epoch  6, batch     2 | loss: 6.9873343Losses:  2.453956127166748 -0.0 0.10870595276355743
CurrentTrain: epoch  6, batch     3 | loss: 2.4539561Losses:  7.966401100158691 4.0997419357299805 0.6965510845184326
CurrentTrain: epoch  7, batch     0 | loss: 7.9664011Losses:  6.709853172302246 2.8748884201049805 0.7638480067253113
CurrentTrain: epoch  7, batch     1 | loss: 6.7098532Losses:  7.185733318328857 3.5377049446105957 0.5689882040023804
CurrentTrain: epoch  7, batch     2 | loss: 7.1857333Losses:  2.8089663982391357 -0.0 0.11362661421298981
CurrentTrain: epoch  7, batch     3 | loss: 2.8089664Losses:  6.336531639099121 3.3048057556152344 0.6441192030906677
CurrentTrain: epoch  8, batch     0 | loss: 6.3365316Losses:  6.743443489074707 2.8501687049865723 0.7212137579917908
CurrentTrain: epoch  8, batch     1 | loss: 6.7434435Losses:  6.350937366485596 2.8794407844543457 0.6294291615486145
CurrentTrain: epoch  8, batch     2 | loss: 6.3509374Losses:  4.2753705978393555 -0.0 0.11339090019464493
CurrentTrain: epoch  8, batch     3 | loss: 4.2753706Losses:  6.051488399505615 2.514906883239746 0.7912666201591492
CurrentTrain: epoch  9, batch     0 | loss: 6.0514884Losses:  5.980315685272217 3.072680711746216 0.6734215617179871
CurrentTrain: epoch  9, batch     1 | loss: 5.9803157Losses:  5.522881507873535 2.0189943313598633 0.7370203733444214
CurrentTrain: epoch  9, batch     2 | loss: 5.5228815Losses:  3.3515729904174805 -0.0 0.11399789899587631
CurrentTrain: epoch  9, batch     3 | loss: 3.3515730
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: the festival opened with " whiplash " directed by damien chazelle and closed with musical drama " rudderless " directed by william h. macy .
Head Entity: whiplash
Tail Entity: damien chazelle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the critically acclaimed series "breaking bad" was brought to life by director vince gilligan, while the spin-off "better call saul" was directed by peter gould.  
Head Entity: breaking bad  
Tail Entity: vince gilligan  

Relation: director  
Context: the animated feature "spider-man: into the spider-verse" was directed by bob persichetti, peter ramsey, and rodney rothman, showcasing a unique visual style.  
Head Entity: spider-man: into the spider-verse  
Tail Entity: bob persichetti  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: tata prima is a range of heavy trucks produced by tata daewoo , a wholly owned subsidiary of tata motors of india .
Head Entity: tata daewoo
Tail Entity: tata motors
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: The famous luxury brand Gucci is owned by the French conglomerate Kering, which also owns several other high-end fashion labels.  
Head Entity: Gucci  
Tail Entity: Kering  

Relation: owned by  
Context: The popular social media platform Instagram is owned by Meta Platforms, Inc., which was formerly known as Facebook, Inc.  
Head Entity: Instagram  
Tail Entity: Meta Platforms, Inc.  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts various events and is home to the local art gallery.  
Head Entity: cultural center  
Tail Entity: local art gallery  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant community hub, serving as the headquarters for the local non-profit organization.  
Head Entity: community hub  
Tail Entity: local non-profit organization  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: reading town hall was built in several phases between 1786 and 1897 , although the principal facade was designed by alfred waterhouse in 1875 .
Head Entity: reading town hall
Tail Entity: alfred waterhouse
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the new art museum was designed by renowned architect zaha hadid, whose innovative style has transformed modern architecture.  
Head Entity: new art museum  
Tail Entity: zaha hadid  

Relation: architect  
Context: the iconic sydney opera house was the brainchild of architect jørn utzon, who won the design competition in 1957.  
Head Entity: sydney opera house  
Tail Entity: jørn utzon  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: in 2001 brian haw set up the parliament square peace campaign outside the houses of parliament in london .
Head Entity: brian haw
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: After moving to New York City in 2010, Sarah found her dream apartment in Brooklyn.  
Head Entity: Sarah  
Tail Entity: Brooklyn  

Relation: residence  
Context: The famous author lived in a quaint cottage in the countryside for many years before relocating to the city.  
Head Entity: The famous author  
Tail Entity: the city  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the beautiful city of Barcelona, Spain, attracting thousands of visitors from around the world.  
Head Entity: annual music festival  
Tail Entity: Barcelona  

Relation: location  
Context: The historic battle was fought near the town of Gettysburg, Pennsylvania, which is now a popular tourist destination.  
Head Entity: historic battle  
Tail Entity: Gettysburg  
Losses:  3.358645439147949 0.27308619022369385 0.9787841439247131
MemoryTrain:  epoch  0, batch     0 | loss: 3.3586454Losses:  3.8761377334594727 0.2801833152770996 0.9255232810974121
MemoryTrain:  epoch  0, batch     1 | loss: 3.8761377Losses:  3.3857882022857666 0.236777663230896 0.7979814410209656
MemoryTrain:  epoch  0, batch     2 | loss: 3.3857882Losses:  4.44285249710083 0.5053176283836365 0.8822606205940247
MemoryTrain:  epoch  0, batch     3 | loss: 4.4428525Losses:  3.4250247478485107 -0.0 1.1244378089904785
MemoryTrain:  epoch  0, batch     4 | loss: 3.4250247Losses:  3.5652432441711426 -0.0 0.9921365976333618
MemoryTrain:  epoch  0, batch     5 | loss: 3.5652432Losses:  3.8789594173431396 -0.0 0.9767683148384094
MemoryTrain:  epoch  0, batch     6 | loss: 3.8789594Losses:  4.164848327636719 0.26606321334838867 0.9781595468521118
MemoryTrain:  epoch  0, batch     7 | loss: 4.1648483Losses:  3.4071292877197266 -0.0 0.9770358800888062
MemoryTrain:  epoch  0, batch     8 | loss: 3.4071293Losses:  4.650960922241211 -0.0 0.928739070892334
MemoryTrain:  epoch  0, batch     9 | loss: 4.6509609Losses:  3.7598986625671387 0.2561076283454895 0.9401183128356934
MemoryTrain:  epoch  0, batch    10 | loss: 3.7598987Losses:  5.2107038497924805 0.24035391211509705 0.9992778301239014
MemoryTrain:  epoch  0, batch    11 | loss: 5.2107038Losses:  4.105382442474365 0.8032881617546082 0.8502394556999207
MemoryTrain:  epoch  0, batch    12 | loss: 4.1053824Losses:  5.247300624847412 -0.0 0.1514693796634674
MemoryTrain:  epoch  0, batch    13 | loss: 5.2473006Losses:  4.137299537658691 0.3114983141422272 1.0164504051208496
MemoryTrain:  epoch  1, batch     0 | loss: 4.1372995Losses:  3.29022479057312 -0.0 0.9642508029937744
MemoryTrain:  epoch  1, batch     1 | loss: 3.2902248Losses:  3.8409883975982666 0.255585253238678 0.9688640236854553
MemoryTrain:  epoch  1, batch     2 | loss: 3.8409884Losses:  3.0764989852905273 -0.0 1.038926362991333
MemoryTrain:  epoch  1, batch     3 | loss: 3.0764990Losses:  3.929466485977173 0.24712777137756348 0.9047111868858337
MemoryTrain:  epoch  1, batch     4 | loss: 3.9294665Losses:  3.150329113006592 -0.0 0.8588734865188599
MemoryTrain:  epoch  1, batch     5 | loss: 3.1503291Losses:  3.8862574100494385 0.4551847279071808 0.8386516571044922
MemoryTrain:  epoch  1, batch     6 | loss: 3.8862574Losses:  4.316299915313721 0.26411187648773193 0.92904132604599
MemoryTrain:  epoch  1, batch     7 | loss: 4.3162999Losses:  4.406379699707031 0.24439160525798798 1.0398879051208496
MemoryTrain:  epoch  1, batch     8 | loss: 4.4063797Losses:  3.1137399673461914 -0.0 0.9113409519195557
MemoryTrain:  epoch  1, batch     9 | loss: 3.1137400Losses:  3.6000375747680664 0.24707302451133728 1.0510362386703491
MemoryTrain:  epoch  1, batch    10 | loss: 3.6000376Losses:  3.755197763442993 0.2820253372192383 0.8564662933349609
MemoryTrain:  epoch  1, batch    11 | loss: 3.7551978Losses:  3.4434139728546143 0.23105274140834808 1.021362066268921
MemoryTrain:  epoch  1, batch    12 | loss: 3.4434140Losses:  1.2514382600784302 -0.0 0.10446591675281525
MemoryTrain:  epoch  1, batch    13 | loss: 1.2514383Losses:  3.706073045730591 0.24978473782539368 0.8652243614196777
MemoryTrain:  epoch  2, batch     0 | loss: 3.7060730Losses:  3.2760562896728516 -0.0 0.8907407522201538
MemoryTrain:  epoch  2, batch     1 | loss: 3.2760563Losses:  3.158432960510254 0.48440220952033997 0.9190794825553894
MemoryTrain:  epoch  2, batch     2 | loss: 3.1584330Losses:  2.724858283996582 -0.0 0.9510834217071533
MemoryTrain:  epoch  2, batch     3 | loss: 2.7248583Losses:  4.056125640869141 0.5308852791786194 0.9731631875038147
MemoryTrain:  epoch  2, batch     4 | loss: 4.0561256Losses:  3.627289295196533 -0.0 1.0312448740005493
MemoryTrain:  epoch  2, batch     5 | loss: 3.6272893Losses:  2.655794143676758 -0.0 1.0543221235275269
MemoryTrain:  epoch  2, batch     6 | loss: 2.6557941Losses:  3.458827018737793 0.7676196098327637 0.7366639375686646
MemoryTrain:  epoch  2, batch     7 | loss: 3.4588270Losses:  3.2929346561431885 0.2256569266319275 0.9570028185844421
MemoryTrain:  epoch  2, batch     8 | loss: 3.2929347Losses:  2.8337278366088867 0.22474713623523712 0.8553638458251953
MemoryTrain:  epoch  2, batch     9 | loss: 2.8337278Losses:  3.2322983741760254 -0.0 1.0396060943603516
MemoryTrain:  epoch  2, batch    10 | loss: 3.2322984Losses:  2.843471050262451 -0.0 0.9044914245605469
MemoryTrain:  epoch  2, batch    11 | loss: 2.8434711Losses:  3.8752434253692627 0.2883824408054352 0.9513035416603088
MemoryTrain:  epoch  2, batch    12 | loss: 3.8752434Losses:  3.7159180641174316 -0.0 0.12490782141685486
MemoryTrain:  epoch  2, batch    13 | loss: 3.7159181Losses:  3.5807065963745117 0.505407452583313 0.9089311361312866
MemoryTrain:  epoch  3, batch     0 | loss: 3.5807066Losses:  3.127180576324463 0.5201617479324341 0.8896728754043579
MemoryTrain:  epoch  3, batch     1 | loss: 3.1271806Losses:  3.7116146087646484 0.5355671048164368 0.9191464185714722
MemoryTrain:  epoch  3, batch     2 | loss: 3.7116146Losses:  3.0496420860290527 -0.0 1.0330835580825806
MemoryTrain:  epoch  3, batch     3 | loss: 3.0496421Losses:  2.8045494556427 0.5405160188674927 0.9285956621170044
MemoryTrain:  epoch  3, batch     4 | loss: 2.8045495Losses:  2.8805413246154785 0.2428613305091858 0.9327432513237
MemoryTrain:  epoch  3, batch     5 | loss: 2.8805413Losses:  3.0172576904296875 -0.0 0.8387926816940308
MemoryTrain:  epoch  3, batch     6 | loss: 3.0172577Losses:  2.4991607666015625 0.248981773853302 0.9343777894973755
MemoryTrain:  epoch  3, batch     7 | loss: 2.4991608Losses:  3.2783260345458984 0.2730467915534973 0.9879186749458313
MemoryTrain:  epoch  3, batch     8 | loss: 3.2783260Losses:  3.0811855792999268 0.5036851167678833 0.9742515087127686
MemoryTrain:  epoch  3, batch     9 | loss: 3.0811856Losses:  3.3142249584198 0.2643609642982483 0.8687593340873718
MemoryTrain:  epoch  3, batch    10 | loss: 3.3142250Losses:  3.431436777114868 0.2754451036453247 0.9451367855072021
MemoryTrain:  epoch  3, batch    11 | loss: 3.4314368Losses:  3.0650556087493896 0.24663636088371277 0.9002355933189392
MemoryTrain:  epoch  3, batch    12 | loss: 3.0650556Losses:  4.906040668487549 -0.0 0.09848949313163757
MemoryTrain:  epoch  3, batch    13 | loss: 4.9060407Losses:  3.6959481239318848 0.2533296048641205 1.0373436212539673
MemoryTrain:  epoch  4, batch     0 | loss: 3.6959481Losses:  3.0090417861938477 0.25369328260421753 0.9744383096694946
MemoryTrain:  epoch  4, batch     1 | loss: 3.0090418Losses:  2.9863338470458984 0.2914358079433441 0.978529691696167
MemoryTrain:  epoch  4, batch     2 | loss: 2.9863338Losses:  2.8439269065856934 0.46619775891304016 0.9644954800605774
MemoryTrain:  epoch  4, batch     3 | loss: 2.8439269Losses:  3.2953808307647705 0.5099743604660034 0.7906497120857239
MemoryTrain:  epoch  4, batch     4 | loss: 3.2953808Losses:  3.19588041305542 0.25201743841171265 0.9703108072280884
MemoryTrain:  epoch  4, batch     5 | loss: 3.1958804Losses:  2.895176649093628 0.7050788402557373 0.8960687518119812
MemoryTrain:  epoch  4, batch     6 | loss: 2.8951766Losses:  3.4925425052642822 0.7336626052856445 0.9150209426879883
MemoryTrain:  epoch  4, batch     7 | loss: 3.4925425Losses:  2.898499011993408 0.24708862602710724 1.0153878927230835
MemoryTrain:  epoch  4, batch     8 | loss: 2.8984990Losses:  3.290825366973877 0.26578617095947266 0.8885010480880737
MemoryTrain:  epoch  4, batch     9 | loss: 3.2908254Losses:  2.5598244667053223 -0.0 0.9143085479736328
MemoryTrain:  epoch  4, batch    10 | loss: 2.5598245Losses:  3.124753475189209 0.515845000743866 0.894791305065155
MemoryTrain:  epoch  4, batch    11 | loss: 3.1247535Losses:  2.4611921310424805 0.2310701161623001 0.9404218196868896
MemoryTrain:  epoch  4, batch    12 | loss: 2.4611921Losses:  1.8971750736236572 -0.0 0.14138293266296387
MemoryTrain:  epoch  4, batch    13 | loss: 1.8971751Losses:  2.2897701263427734 -0.0 0.9133833646774292
MemoryTrain:  epoch  5, batch     0 | loss: 2.2897701Losses:  2.8694024085998535 -0.0 0.9872809648513794
MemoryTrain:  epoch  5, batch     1 | loss: 2.8694024Losses:  2.8969507217407227 0.5072267055511475 0.9011341333389282
MemoryTrain:  epoch  5, batch     2 | loss: 2.8969507Losses:  2.951388120651245 0.2594268321990967 0.8900582194328308
MemoryTrain:  epoch  5, batch     3 | loss: 2.9513881Losses:  2.715355634689331 0.24357923865318298 0.9720574021339417
MemoryTrain:  epoch  5, batch     4 | loss: 2.7153556Losses:  2.742823362350464 0.2510972023010254 0.9589277505874634
MemoryTrain:  epoch  5, batch     5 | loss: 2.7428234Losses:  2.7408387660980225 0.4949628412723541 0.9163052439689636
MemoryTrain:  epoch  5, batch     6 | loss: 2.7408388Losses:  3.4274096488952637 0.25263047218322754 0.7845622897148132
MemoryTrain:  epoch  5, batch     7 | loss: 3.4274096Losses:  3.246394157409668 0.555825412273407 0.9066418409347534
MemoryTrain:  epoch  5, batch     8 | loss: 3.2463942Losses:  2.5760252475738525 0.2649763524532318 0.8565661311149597
MemoryTrain:  epoch  5, batch     9 | loss: 2.5760252Losses:  3.221221923828125 0.48544999957084656 0.9540387988090515
MemoryTrain:  epoch  5, batch    10 | loss: 3.2212219Losses:  4.109025001525879 0.2582404613494873 0.9297741651535034
MemoryTrain:  epoch  5, batch    11 | loss: 4.1090250Losses:  2.7084012031555176 -0.0 0.9962899684906006
MemoryTrain:  epoch  5, batch    12 | loss: 2.7084012Losses:  1.26170814037323 -0.0 0.12052004039287567
MemoryTrain:  epoch  5, batch    13 | loss: 1.2617081Losses:  3.066406488418579 0.48684853315353394 0.9811268448829651
MemoryTrain:  epoch  6, batch     0 | loss: 3.0664065Losses:  3.027296304702759 0.24214041233062744 0.9238882064819336
MemoryTrain:  epoch  6, batch     1 | loss: 3.0272963Losses:  3.742666721343994 0.543247640132904 0.9660149812698364
MemoryTrain:  epoch  6, batch     2 | loss: 3.7426667Losses:  2.654332160949707 0.47042936086654663 0.8435418009757996
MemoryTrain:  epoch  6, batch     3 | loss: 2.6543322Losses:  2.5152878761291504 -0.0 0.982390284538269
MemoryTrain:  epoch  6, batch     4 | loss: 2.5152879Losses:  3.136220693588257 0.7278228402137756 0.9126849174499512
MemoryTrain:  epoch  6, batch     5 | loss: 3.1362207Losses:  2.678839683532715 0.2857103943824768 0.9296984672546387
MemoryTrain:  epoch  6, batch     6 | loss: 2.6788397Losses:  2.8504281044006348 0.25849005579948425 0.7928056120872498
MemoryTrain:  epoch  6, batch     7 | loss: 2.8504281Losses:  2.849642753601074 0.2492128312587738 0.9483851194381714
MemoryTrain:  epoch  6, batch     8 | loss: 2.8496428Losses:  2.916621208190918 -0.0 0.8861684203147888
MemoryTrain:  epoch  6, batch     9 | loss: 2.9166212Losses:  2.533221483230591 0.23736831545829773 0.9627022743225098
MemoryTrain:  epoch  6, batch    10 | loss: 2.5332215Losses:  2.5122580528259277 0.2567753195762634 0.9055665135383606
MemoryTrain:  epoch  6, batch    11 | loss: 2.5122581Losses:  2.8811264038085938 -0.0 1.0836169719696045
MemoryTrain:  epoch  6, batch    12 | loss: 2.8811264Losses:  1.9440476894378662 -0.0 0.12869100272655487
MemoryTrain:  epoch  6, batch    13 | loss: 1.9440477Losses:  3.0431907176971436 0.2704527974128723 0.9618303179740906
MemoryTrain:  epoch  7, batch     0 | loss: 3.0431907Losses:  2.358407497406006 -0.0 1.0015147924423218
MemoryTrain:  epoch  7, batch     1 | loss: 2.3584075Losses:  2.7200088500976562 0.24346938729286194 0.9174634218215942
MemoryTrain:  epoch  7, batch     2 | loss: 2.7200089Losses:  3.6018335819244385 0.4894765615463257 0.84400874376297
MemoryTrain:  epoch  7, batch     3 | loss: 3.6018336Losses:  2.689329147338867 0.4570114314556122 0.9436860680580139
MemoryTrain:  epoch  7, batch     4 | loss: 2.6893291Losses:  2.284196376800537 -0.0 1.0044437646865845
MemoryTrain:  epoch  7, batch     5 | loss: 2.2841964Losses:  2.642876625061035 0.5035944581031799 0.7959001660346985
MemoryTrain:  epoch  7, batch     6 | loss: 2.6428766Losses:  2.583599090576172 0.24278214573860168 1.0157417058944702
MemoryTrain:  epoch  7, batch     7 | loss: 2.5835991Losses:  2.949089765548706 0.4860420525074005 0.9200489521026611
MemoryTrain:  epoch  7, batch     8 | loss: 2.9490898Losses:  2.5644314289093018 -0.0 0.9126007556915283
MemoryTrain:  epoch  7, batch     9 | loss: 2.5644314Losses:  2.7740960121154785 -0.0 0.8768582940101624
MemoryTrain:  epoch  7, batch    10 | loss: 2.7740960Losses:  3.014791965484619 0.8132030963897705 0.8048495650291443
MemoryTrain:  epoch  7, batch    11 | loss: 3.0147920Losses:  4.094322681427002 1.2407681941986084 0.8143839240074158
MemoryTrain:  epoch  7, batch    12 | loss: 4.0943227Losses:  2.0149598121643066 -0.0 0.11205628514289856
MemoryTrain:  epoch  7, batch    13 | loss: 2.0149598Losses:  2.865056037902832 0.2638617157936096 1.0238174200057983
MemoryTrain:  epoch  8, batch     0 | loss: 2.8650560Losses:  2.7531933784484863 0.2168683558702469 0.9800471663475037
MemoryTrain:  epoch  8, batch     1 | loss: 2.7531934Losses:  3.0155575275421143 0.2710188329219818 0.9810521602630615
MemoryTrain:  epoch  8, batch     2 | loss: 3.0155575Losses:  2.7962913513183594 0.24585452675819397 1.0276970863342285
MemoryTrain:  epoch  8, batch     3 | loss: 2.7962914Losses:  2.6758217811584473 0.22965680062770844 0.8553548455238342
MemoryTrain:  epoch  8, batch     4 | loss: 2.6758218Losses:  2.4651167392730713 0.24074754118919373 0.9010011553764343
MemoryTrain:  epoch  8, batch     5 | loss: 2.4651167Losses:  4.018969535827637 0.760673999786377 0.9203604459762573
MemoryTrain:  epoch  8, batch     6 | loss: 4.0189695Losses:  2.4768919944763184 -0.0 0.9276687502861023
MemoryTrain:  epoch  8, batch     7 | loss: 2.4768920Losses:  2.720160961151123 -0.0 1.0628710985183716
MemoryTrain:  epoch  8, batch     8 | loss: 2.7201610Losses:  2.8168110847473145 0.5202159881591797 1.0097770690917969
MemoryTrain:  epoch  8, batch     9 | loss: 2.8168111Losses:  2.832050323486328 0.5351452231407166 0.8022838830947876
MemoryTrain:  epoch  8, batch    10 | loss: 2.8320503Losses:  2.551784038543701 0.272408127784729 0.8579621911048889
MemoryTrain:  epoch  8, batch    11 | loss: 2.5517840Losses:  2.9657859802246094 0.518066942691803 0.9555454850196838
MemoryTrain:  epoch  8, batch    12 | loss: 2.9657860Losses:  1.4871490001678467 -0.0 0.11160793155431747
MemoryTrain:  epoch  8, batch    13 | loss: 1.4871490Losses:  3.377107620239258 0.49524787068367004 0.912601888179779
MemoryTrain:  epoch  9, batch     0 | loss: 3.3771076Losses:  3.9900803565979004 1.1853764057159424 0.919793963432312
MemoryTrain:  epoch  9, batch     1 | loss: 3.9900804Losses:  2.467933177947998 -0.0 1.0287983417510986
MemoryTrain:  epoch  9, batch     2 | loss: 2.4679332Losses:  2.90256929397583 0.5010077357292175 0.9279465675354004
MemoryTrain:  epoch  9, batch     3 | loss: 2.9025693Losses:  3.124814987182617 0.3521623909473419 0.9214694499969482
MemoryTrain:  epoch  9, batch     4 | loss: 3.1248150Losses:  2.3333077430725098 -0.0 0.953575611114502
MemoryTrain:  epoch  9, batch     5 | loss: 2.3333077Losses:  2.399751663208008 -0.0 1.016617774963379
MemoryTrain:  epoch  9, batch     6 | loss: 2.3997517Losses:  2.9173967838287354 0.48348385095596313 0.9654357433319092
MemoryTrain:  epoch  9, batch     7 | loss: 2.9173968Losses:  2.7054340839385986 0.3059324026107788 0.9629715085029602
MemoryTrain:  epoch  9, batch     8 | loss: 2.7054341Losses:  2.6239266395568848 0.2546469569206238 0.9720500707626343
MemoryTrain:  epoch  9, batch     9 | loss: 2.6239266Losses:  2.1628313064575195 -0.0 0.9146832823753357
MemoryTrain:  epoch  9, batch    10 | loss: 2.1628313Losses:  2.6938252449035645 -0.0 1.0584042072296143
MemoryTrain:  epoch  9, batch    11 | loss: 2.6938252Losses:  2.6631393432617188 0.26303786039352417 0.8795890808105469
MemoryTrain:  epoch  9, batch    12 | loss: 2.6631393Losses:  1.4490625858306885 -0.0 0.1690540909767151
MemoryTrain:  epoch  9, batch    13 | loss: 1.4490626
[EVAL] batch:    0 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    1 | acc: 12.50%,  total acc: 12.50%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 10.42%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 10.94%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 16.25%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 17.71%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 25.00%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 32.81%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 40.28%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 45.62%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 55.77%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 64.47%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 62.81%   [EVAL] batch:   20 | acc: 25.00%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 59.94%   [EVAL] batch:   22 | acc: 37.50%,  total acc: 58.97%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 57.55%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 56.25%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 54.81%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 53.24%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 51.56%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 49.78%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 48.75%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 47.38%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 47.66%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 49.05%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 50.00%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 51.25%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 52.60%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 53.38%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 54.61%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 55.45%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 56.56%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 57.47%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 58.04%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 58.87%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 59.66%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 60.00%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 60.05%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 59.97%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 60.16%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 60.59%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 60.50%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 60.54%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 60.82%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 60.97%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 61.23%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 61.48%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 61.94%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 61.73%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 61.64%   [EVAL] batch:   58 | acc: 50.00%,  total acc: 61.44%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 61.56%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 61.37%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 61.90%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 61.41%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 66.67%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 63.02%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.14%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 73.12%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 73.51%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 73.58%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 73.37%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 73.96%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 74.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.48%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.39%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 77.80%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.23%   [EVAL] batch:   31 | acc: 87.50%,  total acc: 79.49%   [EVAL] batch:   32 | acc: 75.00%,  total acc: 79.36%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 77.94%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 77.50%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 77.03%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 77.30%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.88%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.44%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 78.81%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 79.36%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 79.40%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 78.75%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 78.12%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 77.39%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 76.82%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 76.28%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 75.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 75.86%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 75.84%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 75.71%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 75.46%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 75.22%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 74.68%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 74.68%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 74.69%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 74.69%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 74.40%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 73.91%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 72.85%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 71.83%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 70.93%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 70.06%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 69.12%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 68.48%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 69.28%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 69.36%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 69.69%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 70.02%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 70.15%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 69.72%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 69.55%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 69.62%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 69.53%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 69.44%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 69.28%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 69.28%   [EVAL] batch:   83 | acc: 62.50%,  total acc: 69.20%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 68.90%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 68.97%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 68.61%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 68.68%   [EVAL] batch:   88 | acc: 81.25%,  total acc: 68.82%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 68.61%   [EVAL] batch:   90 | acc: 75.00%,  total acc: 68.68%   [EVAL] batch:   91 | acc: 62.50%,  total acc: 68.61%   [EVAL] batch:   92 | acc: 56.25%,  total acc: 68.48%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 68.22%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 68.22%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 68.03%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 67.91%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 67.73%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 67.74%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 67.75%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 67.70%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 67.72%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 67.79%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 67.80%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 67.81%   [EVAL] batch:  106 | acc: 56.25%,  total acc: 67.70%   [EVAL] batch:  107 | acc: 43.75%,  total acc: 67.48%   [EVAL] batch:  108 | acc: 75.00%,  total acc: 67.55%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 67.56%   [EVAL] batch:  110 | acc: 31.25%,  total acc: 67.23%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 67.08%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 67.15%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 67.43%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 67.72%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 68.85%   [EVAL] batch:  120 | acc: 62.50%,  total acc: 68.80%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 69.01%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 69.00%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 69.10%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 69.20%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 69.15%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 69.00%   [EVAL] batch:  127 | acc: 81.25%,  total acc: 69.09%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 69.14%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 69.23%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 69.27%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 69.37%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 69.50%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 69.64%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 69.72%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 70.12%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 70.06%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 69.65%   [EVAL] batch:  139 | acc: 31.25%,  total acc: 69.38%   [EVAL] batch:  140 | acc: 25.00%,  total acc: 69.06%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 68.84%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 68.58%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 68.27%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.45%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.84%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.42%   [EVAL] batch:  150 | acc: 50.00%,  total acc: 69.29%   [EVAL] batch:  151 | acc: 43.75%,  total acc: 69.12%   [EVAL] batch:  152 | acc: 62.50%,  total acc: 69.08%   [EVAL] batch:  153 | acc: 50.00%,  total acc: 68.95%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 68.79%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 68.67%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:  157 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  159 | acc: 81.25%,  total acc: 68.95%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 68.91%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 69.02%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 68.87%   [EVAL] batch:  163 | acc: 37.50%,  total acc: 68.67%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 68.45%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 68.15%   [EVAL] batch:  166 | acc: 6.25%,  total acc: 67.78%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 67.52%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 67.31%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 67.06%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 66.92%   [EVAL] batch:  171 | acc: 68.75%,  total acc: 66.93%   [EVAL] batch:  172 | acc: 25.00%,  total acc: 66.69%   [EVAL] batch:  173 | acc: 43.75%,  total acc: 66.56%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 66.46%   [EVAL] batch:  175 | acc: 68.75%,  total acc: 66.48%   [EVAL] batch:  176 | acc: 75.00%,  total acc: 66.53%   [EVAL] batch:  177 | acc: 81.25%,  total acc: 66.61%   [EVAL] batch:  178 | acc: 81.25%,  total acc: 66.69%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 66.84%   [EVAL] batch:  180 | acc: 87.50%,  total acc: 66.95%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 66.86%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 66.87%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 66.95%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 66.94%   [EVAL] batch:  186 | acc: 50.00%,  total acc: 66.84%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 66.92%   [EVAL] batch:  188 | acc: 93.75%,  total acc: 67.06%   [EVAL] batch:  189 | acc: 93.75%,  total acc: 67.20%   [EVAL] batch:  190 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 67.45%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 67.72%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 67.66%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 67.60%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 67.61%   [EVAL] batch:  197 | acc: 50.00%,  total acc: 67.52%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 67.46%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 67.31%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 67.38%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 67.45%   [EVAL] batch:  202 | acc: 87.50%,  total acc: 67.55%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 67.49%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 67.53%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 67.54%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 67.48%   [EVAL] batch:  207 | acc: 6.25%,  total acc: 67.19%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 66.93%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 66.70%   [EVAL] batch:  210 | acc: 25.00%,  total acc: 66.50%   [EVAL] batch:  211 | acc: 6.25%,  total acc: 66.21%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 66.14%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.92%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 67.04%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 66.96%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 67.02%   [EVAL] batch:  221 | acc: 56.25%,  total acc: 66.98%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 66.96%   [EVAL] batch:  223 | acc: 75.00%,  total acc: 66.99%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 66.86%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  228 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  230 | acc: 93.75%,  total acc: 67.69%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 67.83%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 68.72%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  241 | acc: 93.75%,  total acc: 69.06%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 69.19%   [EVAL] batch:  243 | acc: 93.75%,  total acc: 69.29%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 69.49%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 69.61%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 69.83%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  250 | acc: 31.25%,  total acc: 69.77%   [EVAL] batch:  251 | acc: 43.75%,  total acc: 69.67%   [EVAL] batch:  252 | acc: 37.50%,  total acc: 69.54%   [EVAL] batch:  253 | acc: 31.25%,  total acc: 69.39%   [EVAL] batch:  254 | acc: 43.75%,  total acc: 69.29%   [EVAL] batch:  255 | acc: 31.25%,  total acc: 69.14%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 69.07%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 69.09%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 69.06%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 68.97%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 68.94%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 68.92%   [EVAL] batch:  263 | acc: 43.75%,  total acc: 68.82%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 68.70%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 68.66%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 68.59%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 68.56%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 68.63%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 68.70%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 69.05%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 69.14%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:  275 | acc: 81.25%,  total acc: 69.29%   [EVAL] batch:  276 | acc: 62.50%,  total acc: 69.27%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 69.27%   [EVAL] batch:  278 | acc: 43.75%,  total acc: 69.18%   [EVAL] batch:  279 | acc: 75.00%,  total acc: 69.20%   [EVAL] batch:  280 | acc: 50.00%,  total acc: 69.13%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 69.15%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 69.10%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 69.15%   [EVAL] batch:  284 | acc: 62.50%,  total acc: 69.12%   [EVAL] batch:  285 | acc: 50.00%,  total acc: 69.06%   [EVAL] batch:  286 | acc: 50.00%,  total acc: 68.99%   [EVAL] batch:  287 | acc: 43.75%,  total acc: 68.90%   [EVAL] batch:  288 | acc: 31.25%,  total acc: 68.77%   [EVAL] batch:  289 | acc: 25.00%,  total acc: 68.62%   [EVAL] batch:  290 | acc: 31.25%,  total acc: 68.49%   [EVAL] batch:  291 | acc: 31.25%,  total acc: 68.36%   [EVAL] batch:  292 | acc: 37.50%,  total acc: 68.26%   [EVAL] batch:  293 | acc: 37.50%,  total acc: 68.15%   [EVAL] batch:  294 | acc: 62.50%,  total acc: 68.14%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  297 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 68.19%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.29%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.40%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 68.50%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 68.89%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 68.95%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 68.99%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 69.07%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 69.13%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 69.19%   [EVAL] batch:  313 | acc: 0.00%,  total acc: 68.97%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 68.81%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 68.61%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 68.40%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 68.22%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 68.08%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 68.11%   [EVAL] batch:  320 | acc: 50.00%,  total acc: 68.05%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 68.09%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 68.19%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 68.21%   [EVAL] batch:  325 | acc: 93.75%,  total acc: 68.29%   [EVAL] batch:  326 | acc: 75.00%,  total acc: 68.31%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 68.35%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 68.37%   [EVAL] batch:  329 | acc: 81.25%,  total acc: 68.41%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 68.43%   [EVAL] batch:  331 | acc: 81.25%,  total acc: 68.47%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 68.73%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 68.82%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 68.95%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 69.01%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 69.08%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 69.24%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 69.39%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 69.47%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 69.63%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 69.70%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 69.77%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 69.69%   [EVAL] batch:  351 | acc: 6.25%,  total acc: 69.51%   [EVAL] batch:  352 | acc: 25.00%,  total acc: 69.39%   [EVAL] batch:  353 | acc: 37.50%,  total acc: 69.30%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 69.15%   [EVAL] batch:  355 | acc: 18.75%,  total acc: 69.01%   [EVAL] batch:  356 | acc: 87.50%,  total acc: 69.07%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 69.13%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  360 | acc: 100.00%,  total acc: 69.37%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 69.42%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 69.42%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 69.42%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 69.33%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 69.28%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 69.21%   [EVAL] batch:  367 | acc: 56.25%,  total acc: 69.17%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 69.11%   [EVAL] batch:  369 | acc: 81.25%,  total acc: 69.14%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 69.15%   [EVAL] batch:  371 | acc: 62.50%,  total acc: 69.14%   [EVAL] batch:  372 | acc: 87.50%,  total acc: 69.19%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 69.27%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 69.32%   [EVAL] batch:  375 | acc: 12.50%,  total acc: 69.17%   [EVAL] batch:  376 | acc: 12.50%,  total acc: 69.02%   [EVAL] batch:  377 | acc: 6.25%,  total acc: 68.85%   [EVAL] batch:  378 | acc: 12.50%,  total acc: 68.70%   [EVAL] batch:  379 | acc: 37.50%,  total acc: 68.62%   [EVAL] batch:  380 | acc: 25.00%,  total acc: 68.50%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 68.50%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 68.55%   [EVAL] batch:  383 | acc: 100.00%,  total acc: 68.64%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 68.70%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 68.77%   [EVAL] batch:  386 | acc: 87.50%,  total acc: 68.81%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 68.86%   [EVAL] batch:  388 | acc: 93.75%,  total acc: 68.93%   [EVAL] batch:  389 | acc: 87.50%,  total acc: 68.97%   [EVAL] batch:  390 | acc: 93.75%,  total acc: 69.04%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 69.07%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 69.13%   [EVAL] batch:  393 | acc: 50.00%,  total acc: 69.08%   [EVAL] batch:  394 | acc: 31.25%,  total acc: 68.99%   [EVAL] batch:  395 | acc: 25.00%,  total acc: 68.88%   [EVAL] batch:  396 | acc: 37.50%,  total acc: 68.80%   [EVAL] batch:  397 | acc: 37.50%,  total acc: 68.72%   [EVAL] batch:  398 | acc: 25.00%,  total acc: 68.61%   [EVAL] batch:  399 | acc: 25.00%,  total acc: 68.50%   [EVAL] batch:  400 | acc: 18.75%,  total acc: 68.38%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 68.24%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 68.08%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 67.91%   [EVAL] batch:  404 | acc: 18.75%,  total acc: 67.79%   [EVAL] batch:  405 | acc: 6.25%,  total acc: 67.64%   [EVAL] batch:  406 | acc: 56.25%,  total acc: 67.61%   [EVAL] batch:  407 | acc: 93.75%,  total acc: 67.68%   [EVAL] batch:  408 | acc: 81.25%,  total acc: 67.71%   [EVAL] batch:  409 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:  410 | acc: 100.00%,  total acc: 67.85%   [EVAL] batch:  411 | acc: 81.25%,  total acc: 67.89%   [EVAL] batch:  412 | acc: 100.00%,  total acc: 67.96%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 68.01%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 68.18%   [EVAL] batch:  417 | acc: 93.75%,  total acc: 68.24%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:  419 | acc: 75.00%,  total acc: 68.32%   [EVAL] batch:  420 | acc: 62.50%,  total acc: 68.30%   [EVAL] batch:  421 | acc: 56.25%,  total acc: 68.28%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 68.28%   [EVAL] batch:  423 | acc: 81.25%,  total acc: 68.31%   [EVAL] batch:  424 | acc: 56.25%,  total acc: 68.28%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 68.27%   [EVAL] batch:  426 | acc: 75.00%,  total acc: 68.28%   [EVAL] batch:  427 | acc: 68.75%,  total acc: 68.28%   [EVAL] batch:  428 | acc: 75.00%,  total acc: 68.30%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 68.31%   [EVAL] batch:  430 | acc: 87.50%,  total acc: 68.36%   [EVAL] batch:  431 | acc: 50.00%,  total acc: 68.32%   [EVAL] batch:  432 | acc: 56.25%,  total acc: 68.29%   [EVAL] batch:  433 | acc: 50.00%,  total acc: 68.25%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 68.25%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 68.21%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 68.26%   [EVAL] batch:  437 | acc: 31.25%,  total acc: 68.18%   
cur_acc:  ['0.9494', '0.7302', '0.7381', '0.7778', '0.6885', '0.6865', '0.6141']
his_acc:  ['0.9494', '0.8310', '0.7882', '0.7758', '0.7542', '0.7160', '0.6818']
Clustering into  38  clusters
Clusters:  [ 5  2 26 16  1  1 37  0  2 11 24  0  0  1 36 11  5  1 19 19  4 25 29  7
  3  2 13  5 32  4 16  5  5  2 35 10 31 18  0  2  6 12  7  4  0 20  9 10
 33  8 15 19 24 34 27 19 17  8 11  5  1 14 13 36  3 21 28  9  8  2  6  8
 30 14 23 19 22 24  4 17]
Losses:  12.553521156311035 4.202866554260254 0.7716742157936096
CurrentTrain: epoch  0, batch     0 | loss: 12.5535212Losses:  10.554678916931152 4.546382427215576 0.6072624921798706
CurrentTrain: epoch  0, batch     1 | loss: 10.5546789Losses:  11.359779357910156 4.361926555633545 0.8725728988647461
CurrentTrain: epoch  0, batch     2 | loss: 11.3597794Losses:  5.937569618225098 -0.0 0.12320157140493393
CurrentTrain: epoch  0, batch     3 | loss: 5.9375696Losses:  8.82538890838623 2.8974404335021973 0.7960062026977539
CurrentTrain: epoch  1, batch     0 | loss: 8.8253889Losses:  9.754170417785645 3.2187016010284424 0.7715660929679871
CurrentTrain: epoch  1, batch     1 | loss: 9.7541704Losses:  8.59056568145752 2.5474019050598145 0.8661382794380188
CurrentTrain: epoch  1, batch     2 | loss: 8.5905657Losses:  7.441176414489746 -0.0 0.1337708979845047
CurrentTrain: epoch  1, batch     3 | loss: 7.4411764Losses:  9.210836410522461 4.355408191680908 0.704279363155365
CurrentTrain: epoch  2, batch     0 | loss: 9.2108364Losses:  8.43513298034668 2.930711269378662 0.7696759104728699
CurrentTrain: epoch  2, batch     1 | loss: 8.4351330Losses:  11.790860176086426 5.5563859939575195 0.7428640127182007
CurrentTrain: epoch  2, batch     2 | loss: 11.7908602Losses:  5.320723056793213 -0.0 0.12449730932712555
CurrentTrain: epoch  2, batch     3 | loss: 5.3207231Losses:  8.668472290039062 2.677560806274414 0.8259181976318359
CurrentTrain: epoch  3, batch     0 | loss: 8.6684723Losses:  8.11978816986084 2.662572145462036 0.7556125521659851
CurrentTrain: epoch  3, batch     1 | loss: 8.1197882Losses:  7.277107238769531 1.9606854915618896 0.8325127959251404
CurrentTrain: epoch  3, batch     2 | loss: 7.2771072Losses:  2.3567917346954346 -0.0 0.0981200784444809
CurrentTrain: epoch  3, batch     3 | loss: 2.3567917Losses:  8.038771629333496 3.5756874084472656 0.7497472763061523
CurrentTrain: epoch  4, batch     0 | loss: 8.0387716Losses:  7.304680824279785 2.2530860900878906 0.8247871398925781
CurrentTrain: epoch  4, batch     1 | loss: 7.3046808Losses:  9.263025283813477 3.4064736366271973 0.7650415897369385
CurrentTrain: epoch  4, batch     2 | loss: 9.2630253Losses:  3.7970495223999023 -0.0 0.10708014667034149
CurrentTrain: epoch  4, batch     3 | loss: 3.7970495Losses:  7.624996662139893 3.331763505935669 0.7301459908485413
CurrentTrain: epoch  5, batch     0 | loss: 7.6249967Losses:  7.918160438537598 3.242231607437134 0.6948988437652588
CurrentTrain: epoch  5, batch     1 | loss: 7.9181604Losses:  9.78691291809082 3.94669246673584 0.7117307782173157
CurrentTrain: epoch  5, batch     2 | loss: 9.7869129Losses:  4.0380940437316895 -0.0 0.1041446179151535
CurrentTrain: epoch  5, batch     3 | loss: 4.0380940Losses:  6.643867492675781 2.4396162033081055 0.7954937815666199
CurrentTrain: epoch  6, batch     0 | loss: 6.6438675Losses:  7.746774673461914 2.6175858974456787 0.7661483883857727
CurrentTrain: epoch  6, batch     1 | loss: 7.7467747Losses:  7.44207763671875 2.666924476623535 0.7550575137138367
CurrentTrain: epoch  6, batch     2 | loss: 7.4420776Losses:  4.893343925476074 -0.0 0.13503959774971008
CurrentTrain: epoch  6, batch     3 | loss: 4.8933439Losses:  6.24434757232666 2.2739062309265137 0.7445366382598877
CurrentTrain: epoch  7, batch     0 | loss: 6.2443476Losses:  7.267341136932373 3.321711301803589 0.6993116140365601
CurrentTrain: epoch  7, batch     1 | loss: 7.2673411Losses:  9.913497924804688 4.591237545013428 0.712499737739563
CurrentTrain: epoch  7, batch     2 | loss: 9.9134979Losses:  4.9451775550842285 -0.0 0.1413993388414383
CurrentTrain: epoch  7, batch     3 | loss: 4.9451776Losses:  6.588040351867676 2.6924335956573486 0.6454725861549377
CurrentTrain: epoch  8, batch     0 | loss: 6.5880404Losses:  7.658502101898193 3.527940511703491 0.820630669593811
CurrentTrain: epoch  8, batch     1 | loss: 7.6585021Losses:  6.5542168617248535 2.403183937072754 0.7354082465171814
CurrentTrain: epoch  8, batch     2 | loss: 6.5542169Losses:  3.212500810623169 -0.0 0.10172310471534729
CurrentTrain: epoch  8, batch     3 | loss: 3.2125008Losses:  6.820672988891602 2.8675537109375 0.6961492300033569
CurrentTrain: epoch  9, batch     0 | loss: 6.8206730Losses:  5.992360591888428 2.6089017391204834 0.6750712394714355
CurrentTrain: epoch  9, batch     1 | loss: 5.9923606Losses:  6.730405807495117 2.917898654937744 0.756158173084259
CurrentTrain: epoch  9, batch     2 | loss: 6.7304058Losses:  4.759934425354004 -0.0 0.09997652471065521
CurrentTrain: epoch  9, batch     3 | loss: 4.7599344
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: in 2004 the catalan government gave him the george cross .
Head Entity: george cross
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union will affect all member states.  
Head Entity: environmental regulations  
Tail Entity: European Union  

Relation: applies to jurisdiction  
Context: The Supreme Court's ruling has significant implications for the state of California's water rights.  
Head Entity: Supreme Court  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the music festival last summer.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the next full - frame digital slrs were the canon eos-1ds of late 2002 , followed by kodak 's dcs pro 14n in 2003 .
Head Entity: dcs pro 14n
Tail Entity: kodak
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The latest smartphone model, the Galaxy S21, was developed by Samsung and released in early 2021.  
Head Entity: Galaxy S21  
Tail Entity: Samsung  

Relation: manufacturer  
Context: The iconic Model T was produced by Ford Motor Company, revolutionizing the automotive industry in the early 20th century.  
Head Entity: Model T  
Tail Entity: Ford Motor Company  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: norway was represented in the eurovision song contest 2005 by the song " in my dreams " performed by wig wam .
Head Entity: eurovision song contest 2005
Tail Entity: wig wam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: the united states was represented in the 2020 summer olympics by a team of over 600 athletes competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: united states  

Relation: participant  
Context: the annual tech conference featured several startups, including a presentation by innovative solutions, which showcased their latest software.  
Head Entity: annual tech conference  
Tail Entity: innovative solutions  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: This game runs smoothly on multiple operating systems, such as Linux, Windows 10, and the latest version of macOS.  
Head Entity: game  
Tail Entity: Windows 10  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete was classified under the Olympic weightlifting category, allowing him to compete in the upcoming games.  
Head Entity: Olympic weightlifting  
Tail Entity: games  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested race, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested race  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: jenő szervánszky is the father of the pianist , valéria szervánszky and the brother of the composer , endre szervánszky and violinist , péter szervánszky .
Head Entity: péter szervánszky
Tail Entity: endre szervánszky
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: emma and oliver are both children of the same parents, making them siblings. they often play together and share a close bond.  
Head Entity: emma  
Tail Entity: oliver  

Relation: sibling  
Context: during the family reunion, it was clear that both lucas and mia inherited their parents' artistic talents, as they are siblings who excel in painting and music.  
Head Entity: lucas  
Tail Entity: mia  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: two of her sons , tunis and thomas tingey rose to prominence in the union navy during the civil war .
Head Entity: tunis
Tail Entity: union navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: after serving in the army, he transitioned to a leadership role in the air force, where he implemented several key strategies.  
Head Entity: air force  
Tail Entity: army  

Relation: military branch  
Context: the general was honored for his service in the marine corps, where he led multiple successful missions.  
Head Entity: marine corps  
Tail Entity: general  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: one of three children , he was the uncle of the future louis xv , who was the son of his brother , louis , duke of burgundy .
Head Entity: louis , duke of burgundy
Tail Entity: louis xv
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: In a small village, Maria was known as the daughter of John, who was a respected farmer and had three children, including her younger brother, Tom.  
Head Entity: John  
Tail Entity: Maria  

Relation: child  
Context: During the family reunion, it was revealed that Sarah, the youngest of the siblings, was the daughter of Michael, who had always been proud of his children.  
Head Entity: Michael  
Tail Entity: Sarah  
Losses:  3.3598906993865967 -0.0 1.0088412761688232
MemoryTrain:  epoch  0, batch     0 | loss: 3.3598907Losses:  3.549731969833374 -0.0 1.1717236042022705
MemoryTrain:  epoch  0, batch     1 | loss: 3.5497320Losses:  5.17571496963501 0.7870621085166931 0.9261083006858826
MemoryTrain:  epoch  0, batch     2 | loss: 5.1757150Losses:  3.20003080368042 0.2790827751159668 0.8375163078308105
MemoryTrain:  epoch  0, batch     3 | loss: 3.2000308Losses:  4.011552333831787 0.2545458972454071 0.8992513418197632
MemoryTrain:  epoch  0, batch     4 | loss: 4.0115523Losses:  3.4549410343170166 0.24752742052078247 0.7781643271446228
MemoryTrain:  epoch  0, batch     5 | loss: 3.4549410Losses:  3.987168788909912 -0.0 0.9999237656593323
MemoryTrain:  epoch  0, batch     6 | loss: 3.9871688Losses:  3.781832218170166 0.23838263750076294 0.8321107625961304
MemoryTrain:  epoch  0, batch     7 | loss: 3.7818322Losses:  3.7879536151885986 0.24740087985992432 0.9702632427215576
MemoryTrain:  epoch  0, batch     8 | loss: 3.7879536Losses:  5.350672721862793 0.5224940776824951 0.9541676044464111
MemoryTrain:  epoch  0, batch     9 | loss: 5.3506727Losses:  3.990569591522217 0.24350309371948242 0.9774419069290161
MemoryTrain:  epoch  0, batch    10 | loss: 3.9905696Losses:  4.993399143218994 0.461844801902771 0.9227315783500671
MemoryTrain:  epoch  0, batch    11 | loss: 4.9933991Losses:  3.51627516746521 0.2627583146095276 1.0167596340179443
MemoryTrain:  epoch  0, batch    12 | loss: 3.5162752Losses:  3.8970775604248047 0.2419792115688324 0.9693494439125061
MemoryTrain:  epoch  0, batch    13 | loss: 3.8970776Losses:  4.05872917175293 -0.0 1.0154445171356201
MemoryTrain:  epoch  0, batch    14 | loss: 4.0587292Losses:  4.063491344451904 0.4756242632865906 0.851406991481781
MemoryTrain:  epoch  1, batch     0 | loss: 4.0634913Losses:  3.3612217903137207 0.2517438530921936 0.971199631690979
MemoryTrain:  epoch  1, batch     1 | loss: 3.3612218Losses:  3.6523475646972656 0.5587378740310669 0.9078354835510254
MemoryTrain:  epoch  1, batch     2 | loss: 3.6523476Losses:  4.184292316436768 0.6799792051315308 0.8519321084022522
MemoryTrain:  epoch  1, batch     3 | loss: 4.1842923Losses:  2.9406168460845947 0.2704891562461853 0.943727970123291
MemoryTrain:  epoch  1, batch     4 | loss: 2.9406168Losses:  3.7046375274658203 0.2539858818054199 1.0322691202163696
MemoryTrain:  epoch  1, batch     5 | loss: 3.7046375Losses:  3.988037109375 0.25029292702674866 1.0541422367095947
MemoryTrain:  epoch  1, batch     6 | loss: 3.9880371Losses:  3.8942832946777344 0.2463826984167099 0.931597888469696
MemoryTrain:  epoch  1, batch     7 | loss: 3.8942833Losses:  3.1000688076019287 -0.0 1.0557315349578857
MemoryTrain:  epoch  1, batch     8 | loss: 3.1000688Losses:  3.414703369140625 0.5034428238868713 0.8040748834609985
MemoryTrain:  epoch  1, batch     9 | loss: 3.4147034Losses:  3.590869665145874 -0.0 0.9044424891471863
MemoryTrain:  epoch  1, batch    10 | loss: 3.5908697Losses:  4.847084999084473 0.565800666809082 0.9667492508888245
MemoryTrain:  epoch  1, batch    11 | loss: 4.8470850Losses:  3.8110265731811523 -0.0 1.0000568628311157
MemoryTrain:  epoch  1, batch    12 | loss: 3.8110266Losses:  3.736208200454712 0.57855224609375 0.9736802577972412
MemoryTrain:  epoch  1, batch    13 | loss: 3.7362082Losses:  3.192316770553589 -0.0 0.9833438992500305
MemoryTrain:  epoch  1, batch    14 | loss: 3.1923168Losses:  3.7656736373901367 0.46897703409194946 0.9081336259841919
MemoryTrain:  epoch  2, batch     0 | loss: 3.7656736Losses:  3.7848145961761475 0.5197200775146484 0.9181382060050964
MemoryTrain:  epoch  2, batch     1 | loss: 3.7848146Losses:  2.8224575519561768 -0.0 0.8052461743354797
MemoryTrain:  epoch  2, batch     2 | loss: 2.8224576Losses:  3.8201346397399902 0.24599073827266693 1.017186164855957
MemoryTrain:  epoch  2, batch     3 | loss: 3.8201346Losses:  3.094555377960205 0.24063485860824585 0.8497952818870544
MemoryTrain:  epoch  2, batch     4 | loss: 3.0945554Losses:  4.159001350402832 0.27363964915275574 0.9842073917388916
MemoryTrain:  epoch  2, batch     5 | loss: 4.1590014Losses:  2.9932689666748047 0.25408563017845154 0.906997799873352
MemoryTrain:  epoch  2, batch     6 | loss: 2.9932690Losses:  3.98419189453125 0.6314122676849365 0.9688297510147095
MemoryTrain:  epoch  2, batch     7 | loss: 3.9841919Losses:  2.695723533630371 -0.0 0.9137393832206726
MemoryTrain:  epoch  2, batch     8 | loss: 2.6957235Losses:  4.46255350112915 1.2818516492843628 0.8670309782028198
MemoryTrain:  epoch  2, batch     9 | loss: 4.4625535Losses:  2.989480972290039 0.2734019160270691 0.8622365593910217
MemoryTrain:  epoch  2, batch    10 | loss: 2.9894810Losses:  2.6154944896698 0.2490004450082779 1.0267055034637451
MemoryTrain:  epoch  2, batch    11 | loss: 2.6154945Losses:  2.9780001640319824 -0.0 1.033293604850769
MemoryTrain:  epoch  2, batch    12 | loss: 2.9780002Losses:  2.900548219680786 -0.0 1.059866189956665
MemoryTrain:  epoch  2, batch    13 | loss: 2.9005482Losses:  3.461181640625 -0.0 1.0764656066894531
MemoryTrain:  epoch  2, batch    14 | loss: 3.4611816Losses:  3.179349660873413 -0.0 0.9087626934051514
MemoryTrain:  epoch  3, batch     0 | loss: 3.1793497Losses:  3.019392490386963 0.2431519329547882 0.9193810820579529
MemoryTrain:  epoch  3, batch     1 | loss: 3.0193925Losses:  3.1121206283569336 0.24657230079174042 1.0557199716567993
MemoryTrain:  epoch  3, batch     2 | loss: 3.1121206Losses:  3.1577930450439453 0.2584775686264038 0.911697506904602
MemoryTrain:  epoch  3, batch     3 | loss: 3.1577930Losses:  3.4148285388946533 0.4760338068008423 0.9584236741065979
MemoryTrain:  epoch  3, batch     4 | loss: 3.4148285Losses:  3.301632881164551 -0.0 0.8969075679779053
MemoryTrain:  epoch  3, batch     5 | loss: 3.3016329Losses:  3.942626953125 0.5760353803634644 0.853969156742096
MemoryTrain:  epoch  3, batch     6 | loss: 3.9426270Losses:  3.1474952697753906 0.8781142234802246 0.7444730401039124
MemoryTrain:  epoch  3, batch     7 | loss: 3.1474953Losses:  2.869704246520996 0.5331956148147583 0.866003155708313
MemoryTrain:  epoch  3, batch     8 | loss: 2.8697042Losses:  2.9393200874328613 -0.0 1.0784821510314941
MemoryTrain:  epoch  3, batch     9 | loss: 2.9393201Losses:  2.801825761795044 -0.0 0.9612311124801636
MemoryTrain:  epoch  3, batch    10 | loss: 2.8018258Losses:  3.0648128986358643 -0.0 1.095482349395752
MemoryTrain:  epoch  3, batch    11 | loss: 3.0648129Losses:  2.8658528327941895 -0.0 0.9127784967422485
MemoryTrain:  epoch  3, batch    12 | loss: 2.8658528Losses:  2.89505672454834 0.24331015348434448 0.9357662200927734
MemoryTrain:  epoch  3, batch    13 | loss: 2.8950567Losses:  3.0536627769470215 0.23305556178092957 0.9131087064743042
MemoryTrain:  epoch  3, batch    14 | loss: 3.0536628Losses:  3.1027066707611084 0.2791457772254944 1.0151991844177246
MemoryTrain:  epoch  4, batch     0 | loss: 3.1027067Losses:  2.794674873352051 0.2450634241104126 0.9089378118515015
MemoryTrain:  epoch  4, batch     1 | loss: 2.7946749Losses:  2.841834783554077 0.25594615936279297 0.8955222964286804
MemoryTrain:  epoch  4, batch     2 | loss: 2.8418348Losses:  2.958862781524658 0.4810258746147156 0.971429705619812
MemoryTrain:  epoch  4, batch     3 | loss: 2.9588628Losses:  3.6484999656677246 0.24403074383735657 1.0789016485214233
MemoryTrain:  epoch  4, batch     4 | loss: 3.6485000Losses:  3.54886531829834 0.2265983670949936 0.8559754490852356
MemoryTrain:  epoch  4, batch     5 | loss: 3.5488653Losses:  3.0187082290649414 -0.0 0.9776762127876282
MemoryTrain:  epoch  4, batch     6 | loss: 3.0187082Losses:  3.629720687866211 0.49335452914237976 0.9231998324394226
MemoryTrain:  epoch  4, batch     7 | loss: 3.6297207Losses:  2.9910101890563965 0.5163538455963135 0.8539695739746094
MemoryTrain:  epoch  4, batch     8 | loss: 2.9910102Losses:  2.654466152191162 -0.0 1.0223382711410522
MemoryTrain:  epoch  4, batch     9 | loss: 2.6544662Losses:  2.355881690979004 -0.0 1.0018726587295532
MemoryTrain:  epoch  4, batch    10 | loss: 2.3558817Losses:  2.571357488632202 -0.0 1.0173439979553223
MemoryTrain:  epoch  4, batch    11 | loss: 2.5713575Losses:  2.4300928115844727 -0.0 0.9733072519302368
MemoryTrain:  epoch  4, batch    12 | loss: 2.4300928Losses:  3.420501947402954 0.2579441964626312 0.9283833503723145
MemoryTrain:  epoch  4, batch    13 | loss: 3.4205019Losses:  2.7647359371185303 -0.0 1.0289326906204224
MemoryTrain:  epoch  4, batch    14 | loss: 2.7647359Losses:  2.3385703563690186 -0.0 0.8487351536750793
MemoryTrain:  epoch  5, batch     0 | loss: 2.3385704Losses:  2.506835699081421 0.2277478575706482 0.8954544067382812
MemoryTrain:  epoch  5, batch     1 | loss: 2.5068357Losses:  2.284013032913208 -0.0 0.872887909412384
MemoryTrain:  epoch  5, batch     2 | loss: 2.2840130Losses:  2.900177478790283 0.47401756048202515 0.8650894165039062
MemoryTrain:  epoch  5, batch     3 | loss: 2.9001775Losses:  2.788102149963379 0.25335466861724854 1.0065468549728394
MemoryTrain:  epoch  5, batch     4 | loss: 2.7881021Losses:  2.3066601753234863 -0.0 0.9630163908004761
MemoryTrain:  epoch  5, batch     5 | loss: 2.3066602Losses:  3.1702733039855957 -0.0 0.9108377695083618
MemoryTrain:  epoch  5, batch     6 | loss: 3.1702733Losses:  3.146921157836914 0.28662294149398804 0.9139376878738403
MemoryTrain:  epoch  5, batch     7 | loss: 3.1469212Losses:  2.897270917892456 0.2711339592933655 0.8768849968910217
MemoryTrain:  epoch  5, batch     8 | loss: 2.8972709Losses:  3.355769395828247 0.28088799118995667 0.7633049488067627
MemoryTrain:  epoch  5, batch     9 | loss: 3.3557694Losses:  2.888705253601074 0.2575410008430481 0.8953937888145447
MemoryTrain:  epoch  5, batch    10 | loss: 2.8887053Losses:  2.8516454696655273 0.25100159645080566 0.9834911823272705
MemoryTrain:  epoch  5, batch    11 | loss: 2.8516455Losses:  2.8641929626464844 0.4743169844150543 0.826107382774353
MemoryTrain:  epoch  5, batch    12 | loss: 2.8641930Losses:  2.800649404525757 0.24882853031158447 0.9694797992706299
MemoryTrain:  epoch  5, batch    13 | loss: 2.8006494Losses:  2.760859489440918 0.23687435686588287 0.9695929288864136
MemoryTrain:  epoch  5, batch    14 | loss: 2.7608595Losses:  2.6025571823120117 -0.0 1.0105092525482178
MemoryTrain:  epoch  6, batch     0 | loss: 2.6025572Losses:  2.735445976257324 0.25694072246551514 0.9622197151184082
MemoryTrain:  epoch  6, batch     1 | loss: 2.7354460Losses:  3.1978142261505127 0.5294981002807617 0.8170921206474304
MemoryTrain:  epoch  6, batch     2 | loss: 3.1978142Losses:  2.992222309112549 -0.0 1.067381501197815
MemoryTrain:  epoch  6, batch     3 | loss: 2.9922223Losses:  2.6250951290130615 0.24749791622161865 0.9252628087997437
MemoryTrain:  epoch  6, batch     4 | loss: 2.6250951Losses:  2.5334930419921875 -0.0 1.01850163936615
MemoryTrain:  epoch  6, batch     5 | loss: 2.5334930Losses:  2.4131486415863037 0.23616911470890045 0.915254533290863
MemoryTrain:  epoch  6, batch     6 | loss: 2.4131486Losses:  2.877869129180908 -0.0 1.0019797086715698
MemoryTrain:  epoch  6, batch     7 | loss: 2.8778691Losses:  2.6756672859191895 0.5073411464691162 0.8683546781539917
MemoryTrain:  epoch  6, batch     8 | loss: 2.6756673Losses:  2.7253923416137695 0.28643321990966797 1.010369062423706
MemoryTrain:  epoch  6, batch     9 | loss: 2.7253923Losses:  2.4263134002685547 -0.0 0.9708324074745178
MemoryTrain:  epoch  6, batch    10 | loss: 2.4263134Losses:  2.781350612640381 -0.0 0.9670523405075073
MemoryTrain:  epoch  6, batch    11 | loss: 2.7813506Losses:  3.6218369007110596 0.7079012393951416 0.8009337782859802
MemoryTrain:  epoch  6, batch    12 | loss: 3.6218369Losses:  2.6621594429016113 -0.0 1.080939531326294
MemoryTrain:  epoch  6, batch    13 | loss: 2.6621594Losses:  2.6522412300109863 0.5004568696022034 0.8639836311340332
MemoryTrain:  epoch  6, batch    14 | loss: 2.6522412Losses:  2.555107831954956 0.5025649666786194 0.7898756861686707
MemoryTrain:  epoch  7, batch     0 | loss: 2.5551078Losses:  2.3700544834136963 0.2812291979789734 0.7919158935546875
MemoryTrain:  epoch  7, batch     1 | loss: 2.3700545Losses:  3.4331934452056885 -0.0 0.9455115795135498
MemoryTrain:  epoch  7, batch     2 | loss: 3.4331934Losses:  2.5644054412841797 0.267927885055542 0.9791170358657837
MemoryTrain:  epoch  7, batch     3 | loss: 2.5644054Losses:  3.0142436027526855 0.5808709859848022 0.852658748626709
MemoryTrain:  epoch  7, batch     4 | loss: 3.0142436Losses:  2.518868923187256 -0.0 0.9584649801254272
MemoryTrain:  epoch  7, batch     5 | loss: 2.5188689Losses:  2.661559581756592 0.4961344003677368 0.9000730514526367
MemoryTrain:  epoch  7, batch     6 | loss: 2.6615596Losses:  2.832491874694824 0.5022310018539429 0.7126441597938538
MemoryTrain:  epoch  7, batch     7 | loss: 2.8324919Losses:  2.1217401027679443 -0.0 0.9001337885856628
MemoryTrain:  epoch  7, batch     8 | loss: 2.1217401Losses:  2.829345226287842 0.26967117190361023 0.9341904520988464
MemoryTrain:  epoch  7, batch     9 | loss: 2.8293452Losses:  3.930783271789551 0.7286456823348999 0.975648045539856
MemoryTrain:  epoch  7, batch    10 | loss: 3.9307833Losses:  2.9920294284820557 0.2547866702079773 1.051506519317627
MemoryTrain:  epoch  7, batch    11 | loss: 2.9920294Losses:  2.5136044025421143 0.2460756003856659 0.971426784992218
MemoryTrain:  epoch  7, batch    12 | loss: 2.5136044Losses:  2.255826950073242 -0.0 0.9515014290809631
MemoryTrain:  epoch  7, batch    13 | loss: 2.2558270Losses:  2.6201000213623047 0.46503347158432007 0.8628106117248535
MemoryTrain:  epoch  7, batch    14 | loss: 2.6201000Losses:  2.936492919921875 0.5595850348472595 0.8586162328720093
MemoryTrain:  epoch  8, batch     0 | loss: 2.9364929Losses:  2.9461276531219482 0.5469995737075806 0.8739156126976013
MemoryTrain:  epoch  8, batch     1 | loss: 2.9461277Losses:  2.3734850883483887 -0.0 0.90226811170578
MemoryTrain:  epoch  8, batch     2 | loss: 2.3734851Losses:  2.905245065689087 0.5094428062438965 0.9089910387992859
MemoryTrain:  epoch  8, batch     3 | loss: 2.9052451Losses:  2.229970932006836 -0.0 0.9160468578338623
MemoryTrain:  epoch  8, batch     4 | loss: 2.2299709Losses:  2.774306535720825 0.23450246453285217 0.893968939781189
MemoryTrain:  epoch  8, batch     5 | loss: 2.7743065Losses:  3.211143970489502 0.5877517461776733 1.0221610069274902
MemoryTrain:  epoch  8, batch     6 | loss: 3.2111440Losses:  2.9955549240112305 0.2568995952606201 0.8882765769958496
MemoryTrain:  epoch  8, batch     7 | loss: 2.9955549Losses:  2.640254497528076 -0.0 1.0065795183181763
MemoryTrain:  epoch  8, batch     8 | loss: 2.6402545Losses:  2.4423842430114746 -0.0 0.9171385765075684
MemoryTrain:  epoch  8, batch     9 | loss: 2.4423842Losses:  2.6079180240631104 0.24794945120811462 0.9189990758895874
MemoryTrain:  epoch  8, batch    10 | loss: 2.6079180Losses:  2.849163770675659 0.5077722668647766 0.7979266047477722
MemoryTrain:  epoch  8, batch    11 | loss: 2.8491638Losses:  2.9928650856018066 0.8470906615257263 0.899204432964325
MemoryTrain:  epoch  8, batch    12 | loss: 2.9928651Losses:  3.1018970012664795 0.5151939392089844 1.0071649551391602
MemoryTrain:  epoch  8, batch    13 | loss: 3.1018970Losses:  2.359652519226074 -0.0 0.9377231597900391
MemoryTrain:  epoch  8, batch    14 | loss: 2.3596525Losses:  2.803295612335205 -0.0 1.0695995092391968
MemoryTrain:  epoch  9, batch     0 | loss: 2.8032956Losses:  3.040635585784912 0.9738975167274475 0.785165011882782
MemoryTrain:  epoch  9, batch     1 | loss: 3.0406356Losses:  2.315199375152588 -0.0 0.9728853106498718
MemoryTrain:  epoch  9, batch     2 | loss: 2.3151994Losses:  2.3557112216949463 0.2561045289039612 0.7063841819763184
MemoryTrain:  epoch  9, batch     3 | loss: 2.3557112Losses:  2.6255943775177 0.2504211664199829 0.9083656072616577
MemoryTrain:  epoch  9, batch     4 | loss: 2.6255944Losses:  2.8250722885131836 0.2646041512489319 0.8668674230575562
MemoryTrain:  epoch  9, batch     5 | loss: 2.8250723Losses:  2.1444766521453857 -0.0 0.8415303826332092
MemoryTrain:  epoch  9, batch     6 | loss: 2.1444767Losses:  2.8087449073791504 0.5339429378509521 0.9126590490341187
MemoryTrain:  epoch  9, batch     7 | loss: 2.8087449Losses:  2.4754562377929688 0.23268777132034302 0.9736040830612183
MemoryTrain:  epoch  9, batch     8 | loss: 2.4754562Losses:  2.318389415740967 0.2528039813041687 0.8388894200325012
MemoryTrain:  epoch  9, batch     9 | loss: 2.3183894Losses:  2.945374011993408 0.23518714308738708 1.017341136932373
MemoryTrain:  epoch  9, batch    10 | loss: 2.9453740Losses:  2.798567533493042 0.279286652803421 1.0517280101776123
MemoryTrain:  epoch  9, batch    11 | loss: 2.7985675Losses:  2.6867728233337402 0.2351415753364563 0.867395281791687
MemoryTrain:  epoch  9, batch    12 | loss: 2.6867728Losses:  2.7868776321411133 0.23855817317962646 0.9946576356887817
MemoryTrain:  epoch  9, batch    13 | loss: 2.7868776Losses:  2.469473361968994 -0.0 0.8563845753669739
MemoryTrain:  epoch  9, batch    14 | loss: 2.4694734
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 37.50%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 41.07%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 46.88%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 50.69%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 52.50%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 54.55%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 55.73%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 59.17%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 59.56%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 60.42%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 60.94%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 61.01%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 61.65%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 61.14%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 61.46%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 61.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 64.35%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 65.40%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 66.16%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 67.29%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 73.68%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 74.04%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 74.22%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 74.24%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 74.55%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 74.85%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 74.57%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 73.89%   [EVAL] batch:   45 | acc: 12.50%,  total acc: 72.55%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 72.07%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 70.92%   [EVAL] batch:   49 | acc: 31.25%,  total acc: 70.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 71.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 71.64%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 72.05%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 72.21%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 71.71%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 71.01%   [EVAL] batch:   58 | acc: 18.75%,  total acc: 70.13%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 69.69%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 68.85%   [EVAL] batch:   61 | acc: 12.50%,  total acc: 67.94%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 67.06%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 60.16%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 59.03%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 66.18%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 67.36%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 71.02%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 78.41%   [EVAL] batch:   33 | acc: 31.25%,  total acc: 77.02%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 76.96%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 76.74%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 76.69%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 77.14%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.72%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 78.28%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 78.66%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 78.87%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 79.07%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 79.12%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 78.47%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 77.85%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 77.13%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 76.56%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 76.40%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 76.00%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 75.74%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 75.84%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 75.71%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 75.35%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 74.89%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 74.67%   [EVAL] batch:   57 | acc: 31.25%,  total acc: 73.92%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 73.62%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 73.54%   [EVAL] batch:   60 | acc: 37.50%,  total acc: 72.95%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 72.28%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 71.63%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 70.51%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 69.42%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 68.37%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 67.35%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 66.36%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 65.76%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 66.64%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 66.75%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 67.12%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 67.48%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 67.58%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 67.52%   [EVAL] batch:   76 | acc: 37.50%,  total acc: 67.13%   [EVAL] batch:   77 | acc: 56.25%,  total acc: 66.99%   [EVAL] batch:   78 | acc: 75.00%,  total acc: 67.09%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 67.03%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 66.98%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 66.69%   [EVAL] batch:   82 | acc: 68.75%,  total acc: 66.72%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 66.59%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 66.32%   [EVAL] batch:   85 | acc: 56.25%,  total acc: 66.21%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 65.88%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 65.98%   [EVAL] batch:   88 | acc: 62.50%,  total acc: 65.94%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 65.76%   [EVAL] batch:   90 | acc: 56.25%,  total acc: 65.66%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 65.56%   [EVAL] batch:   92 | acc: 43.75%,  total acc: 65.32%   [EVAL] batch:   93 | acc: 43.75%,  total acc: 65.09%   [EVAL] batch:   94 | acc: 68.75%,  total acc: 65.13%   [EVAL] batch:   95 | acc: 50.00%,  total acc: 64.97%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 64.88%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 64.80%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 64.90%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 64.94%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 64.91%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 64.95%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 64.99%   [EVAL] batch:  103 | acc: 75.00%,  total acc: 65.08%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 65.12%   [EVAL] batch:  105 | acc: 68.75%,  total acc: 65.15%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 65.01%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 64.76%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 64.62%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 64.55%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 64.30%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 64.17%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 64.27%   [EVAL] batch:  113 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:  116 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 65.78%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 65.97%   [EVAL] batch:  119 | acc: 62.50%,  total acc: 65.94%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 65.75%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 65.90%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 65.98%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 65.95%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 65.92%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 65.75%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 65.77%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 65.79%   [EVAL] batch:  129 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 65.89%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 65.86%   [EVAL] batch:  132 | acc: 25.00%,  total acc: 65.55%   [EVAL] batch:  133 | acc: 50.00%,  total acc: 65.44%   [EVAL] batch:  134 | acc: 50.00%,  total acc: 65.32%   [EVAL] batch:  135 | acc: 56.25%,  total acc: 65.26%   [EVAL] batch:  136 | acc: 62.50%,  total acc: 65.24%   [EVAL] batch:  137 | acc: 37.50%,  total acc: 65.04%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 64.66%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 64.38%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 64.05%   [EVAL] batch:  141 | acc: 31.25%,  total acc: 63.82%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 63.59%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 63.32%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 63.53%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 63.78%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 64.03%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 64.53%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 64.31%   [EVAL] batch:  152 | acc: 43.75%,  total acc: 64.17%   [EVAL] batch:  153 | acc: 43.75%,  total acc: 64.04%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 63.91%   [EVAL] batch:  155 | acc: 31.25%,  total acc: 63.70%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 63.77%   [EVAL] batch:  157 | acc: 68.75%,  total acc: 63.81%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 63.92%   [EVAL] batch:  159 | acc: 81.25%,  total acc: 64.02%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 64.01%   [EVAL] batch:  161 | acc: 87.50%,  total acc: 64.16%   [EVAL] batch:  162 | acc: 43.75%,  total acc: 64.03%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 63.80%   [EVAL] batch:  164 | acc: 25.00%,  total acc: 63.56%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 63.29%   [EVAL] batch:  166 | acc: 12.50%,  total acc: 62.99%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 62.76%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 62.57%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 62.35%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 62.24%   [EVAL] batch:  171 | acc: 56.25%,  total acc: 62.21%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 62.03%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 61.96%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 61.89%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 61.75%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 61.69%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 61.66%   [EVAL] batch:  178 | acc: 68.75%,  total acc: 61.70%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 61.70%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 61.74%   [EVAL] batch:  181 | acc: 43.75%,  total acc: 61.64%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 61.61%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 61.72%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 61.76%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 61.69%   [EVAL] batch:  186 | acc: 50.00%,  total acc: 61.63%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 61.70%   [EVAL] batch:  188 | acc: 87.50%,  total acc: 61.84%   [EVAL] batch:  189 | acc: 87.50%,  total acc: 61.97%   [EVAL] batch:  190 | acc: 93.75%,  total acc: 62.14%   [EVAL] batch:  191 | acc: 81.25%,  total acc: 62.24%   [EVAL] batch:  192 | acc: 100.00%,  total acc: 62.44%   [EVAL] batch:  193 | acc: 87.50%,  total acc: 62.56%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 62.53%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 62.53%   [EVAL] batch:  197 | acc: 62.50%,  total acc: 62.53%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 62.53%   [EVAL] batch:  199 | acc: 37.50%,  total acc: 62.41%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 62.59%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 62.68%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 62.68%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 62.74%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 62.80%   [EVAL] batch:  206 | acc: 56.25%,  total acc: 62.77%   [EVAL] batch:  207 | acc: 12.50%,  total acc: 62.53%   [EVAL] batch:  208 | acc: 12.50%,  total acc: 62.29%   [EVAL] batch:  209 | acc: 18.75%,  total acc: 62.08%   [EVAL] batch:  210 | acc: 18.75%,  total acc: 61.88%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 61.67%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 61.62%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 61.80%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 61.98%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 62.15%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 62.33%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 62.64%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 62.59%   [EVAL] batch:  220 | acc: 81.25%,  total acc: 62.67%   [EVAL] batch:  221 | acc: 50.00%,  total acc: 62.61%   [EVAL] batch:  222 | acc: 62.50%,  total acc: 62.61%   [EVAL] batch:  223 | acc: 81.25%,  total acc: 62.70%   [EVAL] batch:  224 | acc: 37.50%,  total acc: 62.58%   [EVAL] batch:  225 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:  226 | acc: 100.00%,  total acc: 62.91%   [EVAL] batch:  227 | acc: 100.00%,  total acc: 63.08%   [EVAL] batch:  228 | acc: 93.75%,  total acc: 63.21%   [EVAL] batch:  229 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  230 | acc: 87.50%,  total acc: 63.47%   [EVAL] batch:  231 | acc: 100.00%,  total acc: 63.63%   [EVAL] batch:  232 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 64.10%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:  236 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  237 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 64.64%   [EVAL] batch:  239 | acc: 93.75%,  total acc: 64.77%   [EVAL] batch:  240 | acc: 93.75%,  total acc: 64.89%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 64.98%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 65.10%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 65.19%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 65.42%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 65.90%   [EVAL] batch:  250 | acc: 31.25%,  total acc: 65.76%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 65.75%   [EVAL] batch:  252 | acc: 31.25%,  total acc: 65.61%   [EVAL] batch:  253 | acc: 37.50%,  total acc: 65.50%   [EVAL] batch:  254 | acc: 43.75%,  total acc: 65.42%   [EVAL] batch:  255 | acc: 43.75%,  total acc: 65.33%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 65.30%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 65.29%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 65.30%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 65.26%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 65.18%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 65.20%   [EVAL] batch:  262 | acc: 56.25%,  total acc: 65.16%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 65.13%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 65.02%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 64.90%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 64.84%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 64.79%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 64.85%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 64.91%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  271 | acc: 93.75%,  total acc: 65.14%   [EVAL] batch:  272 | acc: 93.75%,  total acc: 65.25%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 65.35%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 65.48%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 65.51%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 65.48%   [EVAL] batch:  277 | acc: 75.00%,  total acc: 65.51%   [EVAL] batch:  278 | acc: 31.25%,  total acc: 65.39%   [EVAL] batch:  279 | acc: 75.00%,  total acc: 65.42%   [EVAL] batch:  280 | acc: 62.50%,  total acc: 65.41%   [EVAL] batch:  281 | acc: 87.50%,  total acc: 65.49%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 65.48%   [EVAL] batch:  283 | acc: 81.25%,  total acc: 65.54%   [EVAL] batch:  284 | acc: 62.50%,  total acc: 65.53%   [EVAL] batch:  285 | acc: 50.00%,  total acc: 65.47%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 65.44%   [EVAL] batch:  287 | acc: 37.50%,  total acc: 65.34%   [EVAL] batch:  288 | acc: 31.25%,  total acc: 65.22%   [EVAL] batch:  289 | acc: 12.50%,  total acc: 65.04%   [EVAL] batch:  290 | acc: 31.25%,  total acc: 64.93%   [EVAL] batch:  291 | acc: 31.25%,  total acc: 64.81%   [EVAL] batch:  292 | acc: 31.25%,  total acc: 64.70%   [EVAL] batch:  293 | acc: 31.25%,  total acc: 64.58%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 64.60%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 64.61%   [EVAL] batch:  296 | acc: 50.00%,  total acc: 64.56%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 64.53%   [EVAL] batch:  298 | acc: 68.75%,  total acc: 64.55%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 64.60%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 64.72%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 65.07%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 65.39%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 65.46%   [EVAL] batch:  308 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:  309 | acc: 93.75%,  total acc: 65.60%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 65.68%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 65.75%   [EVAL] batch:  313 | acc: 6.25%,  total acc: 65.57%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 65.42%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 65.21%   [EVAL] batch:  316 | acc: 0.00%,  total acc: 65.00%   [EVAL] batch:  317 | acc: 12.50%,  total acc: 64.84%   [EVAL] batch:  318 | acc: 18.75%,  total acc: 64.69%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 64.73%   [EVAL] batch:  320 | acc: 50.00%,  total acc: 64.68%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 64.73%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 64.82%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 64.85%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 64.88%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 64.92%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 64.87%   [EVAL] batch:  327 | acc: 56.25%,  total acc: 64.84%   [EVAL] batch:  328 | acc: 56.25%,  total acc: 64.82%   [EVAL] batch:  329 | acc: 62.50%,  total acc: 64.81%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 64.82%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 64.89%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 65.00%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 65.19%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 65.39%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 65.46%   [EVAL] batch:  338 | acc: 87.50%,  total acc: 65.52%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 65.71%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 65.79%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 65.95%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 66.15%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 66.23%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 66.31%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  350 | acc: 12.50%,  total acc: 66.33%   [EVAL] batch:  351 | acc: 12.50%,  total acc: 66.18%   [EVAL] batch:  352 | acc: 18.75%,  total acc: 66.04%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 65.91%   [EVAL] batch:  354 | acc: 12.50%,  total acc: 65.76%   [EVAL] batch:  355 | acc: 18.75%,  total acc: 65.62%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 65.67%   [EVAL] batch:  357 | acc: 93.75%,  total acc: 65.75%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  359 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 65.96%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 66.01%   [EVAL] batch:  363 | acc: 68.75%,  total acc: 66.02%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 65.96%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 65.90%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 65.84%   [EVAL] batch:  367 | acc: 68.75%,  total acc: 65.85%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 65.79%   [EVAL] batch:  369 | acc: 75.00%,  total acc: 65.81%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 65.84%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 65.79%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 65.83%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 65.89%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 65.92%   [EVAL] batch:  375 | acc: 18.75%,  total acc: 65.79%   [EVAL] batch:  376 | acc: 18.75%,  total acc: 65.67%   [EVAL] batch:  377 | acc: 18.75%,  total acc: 65.54%   [EVAL] batch:  378 | acc: 18.75%,  total acc: 65.42%   [EVAL] batch:  379 | acc: 43.75%,  total acc: 65.36%   [EVAL] batch:  380 | acc: 18.75%,  total acc: 65.24%   [EVAL] batch:  381 | acc: 75.00%,  total acc: 65.27%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 65.31%   [EVAL] batch:  383 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  384 | acc: 93.75%,  total acc: 65.47%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 65.54%   [EVAL] batch:  386 | acc: 81.25%,  total acc: 65.58%   [EVAL] batch:  387 | acc: 87.50%,  total acc: 65.64%   [EVAL] batch:  388 | acc: 87.50%,  total acc: 65.70%   [EVAL] batch:  389 | acc: 81.25%,  total acc: 65.74%   [EVAL] batch:  390 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:  391 | acc: 81.25%,  total acc: 65.83%   [EVAL] batch:  392 | acc: 93.75%,  total acc: 65.90%   [EVAL] batch:  393 | acc: 50.00%,  total acc: 65.86%   [EVAL] batch:  394 | acc: 37.50%,  total acc: 65.79%   [EVAL] batch:  395 | acc: 31.25%,  total acc: 65.70%   [EVAL] batch:  396 | acc: 31.25%,  total acc: 65.62%   [EVAL] batch:  397 | acc: 37.50%,  total acc: 65.55%   [EVAL] batch:  398 | acc: 25.00%,  total acc: 65.44%   [EVAL] batch:  399 | acc: 37.50%,  total acc: 65.38%   [EVAL] batch:  400 | acc: 6.25%,  total acc: 65.23%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 65.10%   [EVAL] batch:  402 | acc: 25.00%,  total acc: 65.00%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 64.84%   [EVAL] batch:  404 | acc: 18.75%,  total acc: 64.72%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 64.56%   [EVAL] batch:  406 | acc: 62.50%,  total acc: 64.56%   [EVAL] batch:  407 | acc: 87.50%,  total acc: 64.61%   [EVAL] batch:  408 | acc: 81.25%,  total acc: 64.65%   [EVAL] batch:  409 | acc: 87.50%,  total acc: 64.71%   [EVAL] batch:  410 | acc: 81.25%,  total acc: 64.75%   [EVAL] batch:  411 | acc: 68.75%,  total acc: 64.76%   [EVAL] batch:  412 | acc: 93.75%,  total acc: 64.83%   [EVAL] batch:  413 | acc: 87.50%,  total acc: 64.89%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 64.97%   [EVAL] batch:  415 | acc: 93.75%,  total acc: 65.04%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 65.08%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:  419 | acc: 75.00%,  total acc: 65.25%   [EVAL] batch:  420 | acc: 75.00%,  total acc: 65.28%   [EVAL] batch:  421 | acc: 43.75%,  total acc: 65.23%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 65.25%   [EVAL] batch:  423 | acc: 81.25%,  total acc: 65.29%   [EVAL] batch:  424 | acc: 62.50%,  total acc: 65.28%   [EVAL] batch:  425 | acc: 18.75%,  total acc: 65.17%   [EVAL] batch:  426 | acc: 43.75%,  total acc: 65.12%   [EVAL] batch:  427 | acc: 43.75%,  total acc: 65.07%   [EVAL] batch:  428 | acc: 56.25%,  total acc: 65.05%   [EVAL] batch:  429 | acc: 56.25%,  total acc: 65.03%   [EVAL] batch:  430 | acc: 56.25%,  total acc: 65.01%   [EVAL] batch:  431 | acc: 43.75%,  total acc: 64.96%   [EVAL] batch:  432 | acc: 56.25%,  total acc: 64.94%   [EVAL] batch:  433 | acc: 50.00%,  total acc: 64.90%   [EVAL] batch:  434 | acc: 68.75%,  total acc: 64.91%   [EVAL] batch:  435 | acc: 50.00%,  total acc: 64.88%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 64.95%   [EVAL] batch:  437 | acc: 31.25%,  total acc: 64.87%   [EVAL] batch:  438 | acc: 31.25%,  total acc: 64.79%   [EVAL] batch:  439 | acc: 31.25%,  total acc: 64.72%   [EVAL] batch:  440 | acc: 43.75%,  total acc: 64.67%   [EVAL] batch:  441 | acc: 31.25%,  total acc: 64.59%   [EVAL] batch:  442 | acc: 62.50%,  total acc: 64.59%   [EVAL] batch:  443 | acc: 43.75%,  total acc: 64.54%   [EVAL] batch:  444 | acc: 93.75%,  total acc: 64.61%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:  446 | acc: 68.75%,  total acc: 64.65%   [EVAL] batch:  447 | acc: 75.00%,  total acc: 64.68%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 64.70%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 64.74%   [EVAL] batch:  450 | acc: 62.50%,  total acc: 64.73%   [EVAL] batch:  451 | acc: 75.00%,  total acc: 64.75%   [EVAL] batch:  452 | acc: 62.50%,  total acc: 64.75%   [EVAL] batch:  453 | acc: 56.25%,  total acc: 64.73%   [EVAL] batch:  454 | acc: 68.75%,  total acc: 64.74%   [EVAL] batch:  455 | acc: 87.50%,  total acc: 64.79%   [EVAL] batch:  456 | acc: 62.50%,  total acc: 64.78%   [EVAL] batch:  457 | acc: 56.25%,  total acc: 64.77%   [EVAL] batch:  458 | acc: 62.50%,  total acc: 64.76%   [EVAL] batch:  459 | acc: 75.00%,  total acc: 64.78%   [EVAL] batch:  460 | acc: 50.00%,  total acc: 64.75%   [EVAL] batch:  461 | acc: 68.75%,  total acc: 64.76%   [EVAL] batch:  462 | acc: 81.25%,  total acc: 64.79%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 64.87%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  465 | acc: 81.25%,  total acc: 64.98%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:  467 | acc: 93.75%,  total acc: 65.12%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  471 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  475 | acc: 87.50%,  total acc: 65.66%   [EVAL] batch:  476 | acc: 81.25%,  total acc: 65.70%   [EVAL] batch:  477 | acc: 81.25%,  total acc: 65.73%   [EVAL] batch:  478 | acc: 81.25%,  total acc: 65.76%   [EVAL] batch:  479 | acc: 93.75%,  total acc: 65.82%   [EVAL] batch:  480 | acc: 75.00%,  total acc: 65.84%   [EVAL] batch:  481 | acc: 56.25%,  total acc: 65.82%   [EVAL] batch:  482 | acc: 12.50%,  total acc: 65.71%   [EVAL] batch:  483 | acc: 37.50%,  total acc: 65.65%   [EVAL] batch:  484 | acc: 50.00%,  total acc: 65.62%   [EVAL] batch:  485 | acc: 43.75%,  total acc: 65.57%   [EVAL] batch:  486 | acc: 31.25%,  total acc: 65.50%   [EVAL] batch:  487 | acc: 68.75%,  total acc: 65.51%   [EVAL] batch:  488 | acc: 87.50%,  total acc: 65.55%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:  490 | acc: 87.50%,  total acc: 65.67%   [EVAL] batch:  491 | acc: 87.50%,  total acc: 65.71%   [EVAL] batch:  492 | acc: 87.50%,  total acc: 65.76%   [EVAL] batch:  493 | acc: 75.00%,  total acc: 65.78%   [EVAL] batch:  494 | acc: 25.00%,  total acc: 65.69%   [EVAL] batch:  495 | acc: 25.00%,  total acc: 65.61%   [EVAL] batch:  496 | acc: 25.00%,  total acc: 65.53%   [EVAL] batch:  497 | acc: 37.50%,  total acc: 65.47%   [EVAL] batch:  498 | acc: 6.25%,  total acc: 65.36%   [EVAL] batch:  499 | acc: 25.00%,  total acc: 65.28%   
cur_acc:  ['0.9494', '0.7302', '0.7381', '0.7778', '0.6885', '0.6865', '0.6141', '0.6706']
his_acc:  ['0.9494', '0.8310', '0.7882', '0.7758', '0.7542', '0.7160', '0.6818', '0.6528']
--------Round  1
seed:  200
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 6 3 2 4 0 5 1]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 2 0 3 3 2 0 1 1]
Losses:  19.73458480834961 5.4493231773376465 1.2393338680267334
CurrentTrain: epoch  0, batch     0 | loss: 19.7345848Losses:  20.853212356567383 6.997264862060547 1.171019434928894
CurrentTrain: epoch  0, batch     1 | loss: 20.8532124Losses:  17.6872615814209 3.968393325805664 1.192124843597412
CurrentTrain: epoch  0, batch     2 | loss: 17.6872616Losses:  19.309551239013672 5.827575206756592 1.0655245780944824
CurrentTrain: epoch  0, batch     3 | loss: 19.3095512Losses:  21.024032592773438 7.788888931274414 1.0532697439193726
CurrentTrain: epoch  0, batch     4 | loss: 21.0240326Losses:  20.480552673339844 7.272534370422363 1.0827386379241943
CurrentTrain: epoch  0, batch     5 | loss: 20.4805527Losses:  21.32733726501465 7.942826747894287 0.9220609068870544
CurrentTrain: epoch  0, batch     6 | loss: 21.3273373Losses:  19.902233123779297 7.0141191482543945 0.9551997780799866
CurrentTrain: epoch  0, batch     7 | loss: 19.9022331Losses:  16.769495010375977 4.410144805908203 0.9305459260940552
CurrentTrain: epoch  0, batch     8 | loss: 16.7694950Losses:  18.388391494750977 6.020556449890137 0.744046151638031
CurrentTrain: epoch  0, batch     9 | loss: 18.3883915Losses:  19.69575309753418 7.44028377532959 0.694528341293335
CurrentTrain: epoch  0, batch    10 | loss: 19.6957531Losses:  17.459909439086914 5.292811870574951 0.8757960796356201
CurrentTrain: epoch  0, batch    11 | loss: 17.4599094Losses:  17.70639991760254 6.175081729888916 0.7602161169052124
CurrentTrain: epoch  0, batch    12 | loss: 17.7063999Losses:  16.441570281982422 4.699718475341797 0.7402058839797974
CurrentTrain: epoch  0, batch    13 | loss: 16.4415703Losses:  19.33187484741211 7.563366889953613 0.7080034613609314
CurrentTrain: epoch  0, batch    14 | loss: 19.3318748Losses:  16.38982582092285 5.002190589904785 0.668249249458313
CurrentTrain: epoch  0, batch    15 | loss: 16.3898258Losses:  15.645146369934082 4.787038803100586 0.6372890472412109
CurrentTrain: epoch  0, batch    16 | loss: 15.6451464Losses:  16.62141227722168 5.03706693649292 0.6457260847091675
CurrentTrain: epoch  0, batch    17 | loss: 16.6214123Losses:  14.958609580993652 4.143280029296875 0.5967693328857422
CurrentTrain: epoch  0, batch    18 | loss: 14.9586096Losses:  14.525544166564941 3.562681198120117 0.5779004693031311
CurrentTrain: epoch  0, batch    19 | loss: 14.5255442Losses:  20.494556427001953 8.97147274017334 0.6480748653411865
CurrentTrain: epoch  0, batch    20 | loss: 20.4945564Losses:  15.902290344238281 4.99485969543457 0.5541006326675415
CurrentTrain: epoch  0, batch    21 | loss: 15.9022903Losses:  17.48379898071289 6.406434535980225 0.5815973281860352
CurrentTrain: epoch  0, batch    22 | loss: 17.4837990Losses:  16.648773193359375 5.645573616027832 0.5257903337478638
CurrentTrain: epoch  0, batch    23 | loss: 16.6487732Losses:  15.23292350769043 4.154087066650391 0.5619280934333801
CurrentTrain: epoch  0, batch    24 | loss: 15.2329235Losses:  17.472320556640625 5.9198527336120605 0.5411983728408813
CurrentTrain: epoch  0, batch    25 | loss: 17.4723206Losses:  16.70499038696289 5.20070743560791 0.5012404918670654
CurrentTrain: epoch  0, batch    26 | loss: 16.7049904Losses:  14.755820274353027 4.745161056518555 0.5041841268539429
CurrentTrain: epoch  0, batch    27 | loss: 14.7558203Losses:  14.597959518432617 3.9904260635375977 0.532475471496582
CurrentTrain: epoch  0, batch    28 | loss: 14.5979595Losses:  16.298004150390625 5.237903118133545 0.5334856510162354
CurrentTrain: epoch  0, batch    29 | loss: 16.2980042Losses:  15.175704956054688 4.837563991546631 0.5221997499465942
CurrentTrain: epoch  0, batch    30 | loss: 15.1757050Losses:  18.22746467590332 7.733196258544922 0.517697811126709
CurrentTrain: epoch  0, batch    31 | loss: 18.2274647Losses:  18.42522621154785 7.964362144470215 0.3457275629043579
CurrentTrain: epoch  0, batch    32 | loss: 18.4252262Losses:  15.181391716003418 4.410909652709961 0.5253533720970154
CurrentTrain: epoch  0, batch    33 | loss: 15.1813917Losses:  14.102866172790527 3.3943233489990234 0.49286597967147827
CurrentTrain: epoch  0, batch    34 | loss: 14.1028662Losses:  15.154257774353027 4.506902694702148 0.5874494910240173
CurrentTrain: epoch  0, batch    35 | loss: 15.1542578Losses:  16.13595199584961 6.077713489532471 0.4821423590183258
CurrentTrain: epoch  0, batch    36 | loss: 16.1359520Losses:  13.210916519165039 3.938109874725342 0.46276330947875977
CurrentTrain: epoch  0, batch    37 | loss: 13.2109165Losses:  15.511014938354492 5.203335285186768 0.5349235534667969
CurrentTrain: epoch  0, batch    38 | loss: 15.5110149Losses:  13.641918182373047 3.737607955932617 0.4649120271205902
CurrentTrain: epoch  0, batch    39 | loss: 13.6419182Losses:  16.1529483795166 6.726139068603516 0.48100194334983826
CurrentTrain: epoch  0, batch    40 | loss: 16.1529484Losses:  12.242056846618652 2.6424312591552734 0.5116569399833679
CurrentTrain: epoch  0, batch    41 | loss: 12.2420568Losses:  14.319218635559082 4.643400192260742 0.4574517011642456
CurrentTrain: epoch  0, batch    42 | loss: 14.3192186Losses:  15.230649948120117 5.527923583984375 0.5022557973861694
CurrentTrain: epoch  0, batch    43 | loss: 15.2306499Losses:  13.871442794799805 4.507922649383545 0.4615626931190491
CurrentTrain: epoch  0, batch    44 | loss: 13.8714428Losses:  14.199012756347656 5.3693389892578125 0.41006892919540405
CurrentTrain: epoch  0, batch    45 | loss: 14.1990128Losses:  15.097989082336426 5.226927757263184 0.4769412577152252
CurrentTrain: epoch  0, batch    46 | loss: 15.0979891Losses:  13.289247512817383 4.442366600036621 0.4249706566333771
CurrentTrain: epoch  0, batch    47 | loss: 13.2892475Losses:  14.960884094238281 6.000391960144043 0.3997681438922882
CurrentTrain: epoch  0, batch    48 | loss: 14.9608841Losses:  13.511457443237305 4.263354301452637 0.4337153136730194
CurrentTrain: epoch  0, batch    49 | loss: 13.5114574Losses:  11.778351783752441 2.670069456100464 0.40993958711624146
CurrentTrain: epoch  0, batch    50 | loss: 11.7783518Losses:  13.762324333190918 4.492632865905762 0.31967923045158386
CurrentTrain: epoch  0, batch    51 | loss: 13.7623243Losses:  14.564637184143066 5.429318904876709 0.38845330476760864
CurrentTrain: epoch  0, batch    52 | loss: 14.5646372Losses:  13.184168815612793 4.945281028747559 0.4289136230945587
CurrentTrain: epoch  0, batch    53 | loss: 13.1841688Losses:  12.758544921875 4.30843448638916 0.3894246220588684
CurrentTrain: epoch  0, batch    54 | loss: 12.7585449Losses:  12.812274932861328 3.4184465408325195 0.448880672454834
CurrentTrain: epoch  0, batch    55 | loss: 12.8122749Losses:  13.193626403808594 3.991969108581543 0.39450445771217346
CurrentTrain: epoch  0, batch    56 | loss: 13.1936264Losses:  14.022577285766602 5.281106948852539 0.38991302251815796
CurrentTrain: epoch  0, batch    57 | loss: 14.0225773Losses:  16.18893814086914 7.815629005432129 0.26964664459228516
CurrentTrain: epoch  0, batch    58 | loss: 16.1889381Losses:  12.750560760498047 4.494771957397461 0.4034513533115387
CurrentTrain: epoch  0, batch    59 | loss: 12.7505608Losses:  14.296048164367676 5.083861351013184 0.41752299666404724
CurrentTrain: epoch  0, batch    60 | loss: 14.2960482Losses:  16.01735496520996 6.686805725097656 0.39243361353874207
CurrentTrain: epoch  0, batch    61 | loss: 16.0173550Losses:  9.13589096069336 0.8340451717376709 0.3875572979450226
CurrentTrain: epoch  0, batch    62 | loss: 9.1358910Losses:  14.999427795410156 5.973860263824463 0.40679553151130676
CurrentTrain: epoch  1, batch     0 | loss: 14.9994278Losses:  11.986614227294922 3.2364068031311035 0.3705998659133911
CurrentTrain: epoch  1, batch     1 | loss: 11.9866142Losses:  11.748444557189941 4.69017219543457 0.35313543677330017
CurrentTrain: epoch  1, batch     2 | loss: 11.7484446Losses:  13.126004219055176 5.331625461578369 0.3526526689529419
CurrentTrain: epoch  1, batch     3 | loss: 13.1260042Losses:  13.041399002075195 4.9123640060424805 0.4072880446910858
CurrentTrain: epoch  1, batch     4 | loss: 13.0413990Losses:  14.74563980102539 5.8960371017456055 0.37487727403640747
CurrentTrain: epoch  1, batch     5 | loss: 14.7456398Losses:  12.068568229675293 3.928100109100342 0.39100515842437744
CurrentTrain: epoch  1, batch     6 | loss: 12.0685682Losses:  12.08774185180664 3.9867329597473145 0.39033904671669006
CurrentTrain: epoch  1, batch     7 | loss: 12.0877419Losses:  15.77660083770752 7.467587471008301 0.3606172800064087
CurrentTrain: epoch  1, batch     8 | loss: 15.7766008Losses:  10.560808181762695 3.0710015296936035 0.3418733477592468
CurrentTrain: epoch  1, batch     9 | loss: 10.5608082Losses:  13.117447853088379 4.742945671081543 0.38068607449531555
CurrentTrain: epoch  1, batch    10 | loss: 13.1174479Losses:  13.203316688537598 4.832845687866211 0.36878883838653564
CurrentTrain: epoch  1, batch    11 | loss: 13.2033167Losses:  11.157448768615723 3.2629802227020264 0.34372642636299133
CurrentTrain: epoch  1, batch    12 | loss: 11.1574488Losses:  10.796486854553223 2.853764057159424 0.40615785121917725
CurrentTrain: epoch  1, batch    13 | loss: 10.7964869Losses:  13.368688583374023 5.175529479980469 0.34644365310668945
CurrentTrain: epoch  1, batch    14 | loss: 13.3686886Losses:  12.905709266662598 4.481532096862793 0.3601454794406891
CurrentTrain: epoch  1, batch    15 | loss: 12.9057093Losses:  10.421693801879883 2.922032117843628 0.3489740192890167
CurrentTrain: epoch  1, batch    16 | loss: 10.4216938Losses:  12.812088966369629 6.1397271156311035 0.33433324098587036
CurrentTrain: epoch  1, batch    17 | loss: 12.8120890Losses:  12.882183074951172 4.902756690979004 0.3716107904911041
CurrentTrain: epoch  1, batch    18 | loss: 12.8821831Losses:  11.655012130737305 4.241064071655273 0.35559868812561035
CurrentTrain: epoch  1, batch    19 | loss: 11.6550121Losses:  10.561060905456543 3.510735034942627 0.33169856667518616
CurrentTrain: epoch  1, batch    20 | loss: 10.5610609Losses:  13.561511039733887 5.676628112792969 0.3963264226913452
CurrentTrain: epoch  1, batch    21 | loss: 13.5615110Losses:  11.005912780761719 3.0898020267486572 0.3650653064250946
CurrentTrain: epoch  1, batch    22 | loss: 11.0059128Losses:  10.326512336730957 3.07914400100708 0.35800814628601074
CurrentTrain: epoch  1, batch    23 | loss: 10.3265123Losses:  13.137295722961426 5.263499736785889 0.3693113327026367
CurrentTrain: epoch  1, batch    24 | loss: 13.1372957Losses:  9.90445613861084 2.1953065395355225 0.3430432677268982
CurrentTrain: epoch  1, batch    25 | loss: 9.9044561Losses:  11.423487663269043 3.688108444213867 0.3399263024330139
CurrentTrain: epoch  1, batch    26 | loss: 11.4234877Losses:  15.146798133850098 7.44992733001709 0.32846885919570923
CurrentTrain: epoch  1, batch    27 | loss: 15.1467981Losses:  10.579697608947754 3.1854021549224854 0.3720034956932068
CurrentTrain: epoch  1, batch    28 | loss: 10.5796976Losses:  11.786511421203613 3.891533136367798 0.34927481412887573
CurrentTrain: epoch  1, batch    29 | loss: 11.7865114Losses:  9.353477478027344 2.1435108184814453 0.3314477801322937
CurrentTrain: epoch  1, batch    30 | loss: 9.3534775Losses:  10.047591209411621 2.9706311225891113 0.33585745096206665
CurrentTrain: epoch  1, batch    31 | loss: 10.0475912Losses:  10.508870124816895 3.5284016132354736 0.3280797600746155
CurrentTrain: epoch  1, batch    32 | loss: 10.5088701Losses:  10.425833702087402 2.9286844730377197 0.3142004609107971
CurrentTrain: epoch  1, batch    33 | loss: 10.4258337Losses:  11.745392799377441 5.196540832519531 0.3458977937698364
CurrentTrain: epoch  1, batch    34 | loss: 11.7453928Losses:  10.253643035888672 3.3440942764282227 0.31940558552742004
CurrentTrain: epoch  1, batch    35 | loss: 10.2536430Losses:  10.3803129196167 2.4028995037078857 0.3362826704978943
CurrentTrain: epoch  1, batch    36 | loss: 10.3803129Losses:  10.740555763244629 4.250251770019531 0.3578045666217804
CurrentTrain: epoch  1, batch    37 | loss: 10.7405558Losses:  11.422775268554688 3.5904688835144043 0.3431244492530823
CurrentTrain: epoch  1, batch    38 | loss: 11.4227753Losses:  9.582473754882812 2.454164743423462 0.32307395339012146
CurrentTrain: epoch  1, batch    39 | loss: 9.5824738Losses:  11.535775184631348 4.1760711669921875 0.3284711241722107
CurrentTrain: epoch  1, batch    40 | loss: 11.5357752Losses:  11.759400367736816 3.3227272033691406 0.3395644724369049
CurrentTrain: epoch  1, batch    41 | loss: 11.7594004Losses:  9.755047798156738 3.04909086227417 0.34678319096565247
CurrentTrain: epoch  1, batch    42 | loss: 9.7550478Losses:  14.282265663146973 7.325417518615723 0.34600943326950073
CurrentTrain: epoch  1, batch    43 | loss: 14.2822657Losses:  10.40070915222168 3.4211971759796143 0.34272944927215576
CurrentTrain: epoch  1, batch    44 | loss: 10.4007092Losses:  9.367298126220703 2.765665054321289 0.305348664522171
CurrentTrain: epoch  1, batch    45 | loss: 9.3672981Losses:  10.486350059509277 3.2502083778381348 0.33648186922073364
CurrentTrain: epoch  1, batch    46 | loss: 10.4863501Losses:  9.746881484985352 3.3405368328094482 0.298198938369751
CurrentTrain: epoch  1, batch    47 | loss: 9.7468815Losses:  11.317777633666992 3.8511242866516113 0.31665271520614624
CurrentTrain: epoch  1, batch    48 | loss: 11.3177776Losses:  11.33913516998291 4.85036039352417 0.3113604784011841
CurrentTrain: epoch  1, batch    49 | loss: 11.3391352Losses:  11.161757469177246 3.4556126594543457 0.31079357862472534
CurrentTrain: epoch  1, batch    50 | loss: 11.1617575Losses:  13.234668731689453 5.6817474365234375 0.2482549399137497
CurrentTrain: epoch  1, batch    51 | loss: 13.2346687Losses:  10.184617042541504 3.507742404937744 0.2991001605987549
CurrentTrain: epoch  1, batch    52 | loss: 10.1846170Losses:  10.18640422821045 3.4443202018737793 0.3562173545360565
CurrentTrain: epoch  1, batch    53 | loss: 10.1864042Losses:  10.359716415405273 3.703038215637207 0.296446830034256
CurrentTrain: epoch  1, batch    54 | loss: 10.3597164Losses:  10.049365997314453 2.9134998321533203 0.32555055618286133
CurrentTrain: epoch  1, batch    55 | loss: 10.0493660Losses:  11.239778518676758 4.2304511070251465 0.3157081604003906
CurrentTrain: epoch  1, batch    56 | loss: 11.2397785Losses:  12.99695873260498 5.162313938140869 0.3218965530395508
CurrentTrain: epoch  1, batch    57 | loss: 12.9969587Losses:  12.439055442810059 5.143364906311035 0.33619755506515503
CurrentTrain: epoch  1, batch    58 | loss: 12.4390554Losses:  9.278463363647461 2.5187501907348633 0.30984681844711304
CurrentTrain: epoch  1, batch    59 | loss: 9.2784634Losses:  10.916677474975586 4.65679931640625 0.33769530057907104
CurrentTrain: epoch  1, batch    60 | loss: 10.9166775Losses:  10.64653205871582 4.354558944702148 0.35097819566726685
CurrentTrain: epoch  1, batch    61 | loss: 10.6465321Losses:  10.238348007202148 1.1074411869049072 0.3379332423210144
CurrentTrain: epoch  1, batch    62 | loss: 10.2383480Losses:  8.823735237121582 2.54938006401062 0.31252533197402954
CurrentTrain: epoch  2, batch     0 | loss: 8.8237352Losses:  10.701061248779297 4.207402229309082 0.20316001772880554
CurrentTrain: epoch  2, batch     1 | loss: 10.7010612Losses:  8.478364944458008 2.5000767707824707 0.29282933473587036
CurrentTrain: epoch  2, batch     2 | loss: 8.4783649Losses:  12.28065013885498 5.10338020324707 0.23487578332424164
CurrentTrain: epoch  2, batch     3 | loss: 12.2806501Losses:  9.263089179992676 3.4571471214294434 0.29085996747016907
CurrentTrain: epoch  2, batch     4 | loss: 9.2630892Losses:  10.088310241699219 3.6395556926727295 0.30724960565567017
CurrentTrain: epoch  2, batch     5 | loss: 10.0883102Losses:  10.043071746826172 3.5109715461730957 0.2979699373245239
CurrentTrain: epoch  2, batch     6 | loss: 10.0430717Losses:  9.180257797241211 3.8719241619110107 0.28999096155166626
CurrentTrain: epoch  2, batch     7 | loss: 9.1802578Losses:  10.540447235107422 3.437685251235962 0.2858049273490906
CurrentTrain: epoch  2, batch     8 | loss: 10.5404472Losses:  12.201815605163574 5.437869071960449 0.35183650255203247
CurrentTrain: epoch  2, batch     9 | loss: 12.2018156Losses:  8.43189525604248 2.7685203552246094 0.298089861869812
CurrentTrain: epoch  2, batch    10 | loss: 8.4318953Losses:  10.06065845489502 3.854306221008301 0.3330236077308655
CurrentTrain: epoch  2, batch    11 | loss: 10.0606585Losses:  11.159210205078125 4.891175746917725 0.30610835552215576
CurrentTrain: epoch  2, batch    12 | loss: 11.1592102Losses:  11.045500755310059 4.795459747314453 0.30090412497520447
CurrentTrain: epoch  2, batch    13 | loss: 11.0455008Losses:  8.488612174987793 2.1674318313598633 0.2948850393295288
CurrentTrain: epoch  2, batch    14 | loss: 8.4886122Losses:  9.0048828125 3.2468857765197754 0.28118687868118286
CurrentTrain: epoch  2, batch    15 | loss: 9.0048828Losses:  10.206815719604492 3.7851052284240723 0.29651644825935364
CurrentTrain: epoch  2, batch    16 | loss: 10.2068157Losses:  11.667987823486328 4.441517353057861 0.29523801803588867
CurrentTrain: epoch  2, batch    17 | loss: 11.6679878Losses:  8.756586074829102 2.5667433738708496 0.2700181305408478
CurrentTrain: epoch  2, batch    18 | loss: 8.7565861Losses:  8.751755714416504 2.7022457122802734 0.28035056591033936
CurrentTrain: epoch  2, batch    19 | loss: 8.7517557Losses:  9.201871871948242 3.278306007385254 0.29680871963500977
CurrentTrain: epoch  2, batch    20 | loss: 9.2018719Losses:  12.857665061950684 6.056381702423096 0.2855287194252014
CurrentTrain: epoch  2, batch    21 | loss: 12.8576651Losses:  8.607545852661133 2.624312400817871 0.29389798641204834
CurrentTrain: epoch  2, batch    22 | loss: 8.6075459Losses:  10.493935585021973 4.461715221405029 0.29325515031814575
CurrentTrain: epoch  2, batch    23 | loss: 10.4939356Losses:  8.199355125427246 2.234307289123535 0.2754053771495819
CurrentTrain: epoch  2, batch    24 | loss: 8.1993551Losses:  8.495433807373047 2.2886202335357666 0.28330957889556885
CurrentTrain: epoch  2, batch    25 | loss: 8.4954338Losses:  11.409173965454102 4.276866912841797 0.29377835988998413
CurrentTrain: epoch  2, batch    26 | loss: 11.4091740Losses:  9.200824737548828 2.958217144012451 0.29331761598587036
CurrentTrain: epoch  2, batch    27 | loss: 9.2008247Losses:  9.734931945800781 3.2922792434692383 0.27307599782943726
CurrentTrain: epoch  2, batch    28 | loss: 9.7349319Losses:  9.073517799377441 3.574742317199707 0.2857338786125183
CurrentTrain: epoch  2, batch    29 | loss: 9.0735178Losses:  8.290943145751953 2.528581380844116 0.26628848910331726
CurrentTrain: epoch  2, batch    30 | loss: 8.2909431Losses:  10.126218795776367 3.7791800498962402 0.28854912519454956
CurrentTrain: epoch  2, batch    31 | loss: 10.1262188Losses:  9.19601058959961 3.4579262733459473 0.2745552957057953
CurrentTrain: epoch  2, batch    32 | loss: 9.1960106Losses:  9.008269309997559 2.4934611320495605 0.28600260615348816
CurrentTrain: epoch  2, batch    33 | loss: 9.0082693Losses:  11.079830169677734 4.793763637542725 0.2832643985748291
CurrentTrain: epoch  2, batch    34 | loss: 11.0798302Losses:  10.28227710723877 3.9258055686950684 0.29193374514579773
CurrentTrain: epoch  2, batch    35 | loss: 10.2822771Losses:  8.724861145019531 2.6155624389648438 0.3083696961402893
CurrentTrain: epoch  2, batch    36 | loss: 8.7248611Losses:  9.289535522460938 2.917463779449463 0.2904183566570282
CurrentTrain: epoch  2, batch    37 | loss: 9.2895355Losses:  9.564229965209961 3.3574471473693848 0.2821773886680603
CurrentTrain: epoch  2, batch    38 | loss: 9.5642300Losses:  10.623225212097168 4.7890167236328125 0.2666047215461731
CurrentTrain: epoch  2, batch    39 | loss: 10.6232252Losses:  8.304388046264648 2.800229549407959 0.2907237708568573
CurrentTrain: epoch  2, batch    40 | loss: 8.3043880Losses:  9.399979591369629 4.008262634277344 0.29319214820861816
CurrentTrain: epoch  2, batch    41 | loss: 9.3999796Losses:  8.158090591430664 2.549872875213623 0.28192955255508423
CurrentTrain: epoch  2, batch    42 | loss: 8.1580906Losses:  9.372029304504395 3.432680130004883 0.29620033502578735
CurrentTrain: epoch  2, batch    43 | loss: 9.3720293Losses:  8.818256378173828 2.910208225250244 0.28644049167633057
CurrentTrain: epoch  2, batch    44 | loss: 8.8182564Losses:  8.85519027709961 3.091874599456787 0.30759215354919434
CurrentTrain: epoch  2, batch    45 | loss: 8.8551903Losses:  8.835389137268066 3.27386212348938 0.29956328868865967
CurrentTrain: epoch  2, batch    46 | loss: 8.8353891Losses:  8.63979721069336 3.021545171737671 0.27242887020111084
CurrentTrain: epoch  2, batch    47 | loss: 8.6397972Losses:  10.083932876586914 4.097483158111572 0.2949470281600952
CurrentTrain: epoch  2, batch    48 | loss: 10.0839329Losses:  9.859768867492676 3.4857425689697266 0.28852808475494385
CurrentTrain: epoch  2, batch    49 | loss: 9.8597689Losses:  8.326619148254395 3.0828330516815186 0.2723242938518524
CurrentTrain: epoch  2, batch    50 | loss: 8.3266191Losses:  7.679427623748779 2.514455556869507 0.28406763076782227
CurrentTrain: epoch  2, batch    51 | loss: 7.6794276Losses:  8.765960693359375 3.0916757583618164 0.30570051074028015
CurrentTrain: epoch  2, batch    52 | loss: 8.7659607Losses:  8.525430679321289 3.1187214851379395 0.26340627670288086
CurrentTrain: epoch  2, batch    53 | loss: 8.5254307Losses:  10.135757446289062 4.285764694213867 0.26931172609329224
CurrentTrain: epoch  2, batch    54 | loss: 10.1357574Losses:  8.953054428100586 3.6909525394439697 0.29154929518699646
CurrentTrain: epoch  2, batch    55 | loss: 8.9530544Losses:  9.687520027160645 3.831000804901123 0.27386415004730225
CurrentTrain: epoch  2, batch    56 | loss: 9.6875200Losses:  8.603177070617676 2.7654662132263184 0.27066296339035034
CurrentTrain: epoch  2, batch    57 | loss: 8.6031771Losses:  8.14676284790039 2.6701645851135254 0.27897629141807556
CurrentTrain: epoch  2, batch    58 | loss: 8.1467628Losses:  9.816006660461426 4.359104156494141 0.27733927965164185
CurrentTrain: epoch  2, batch    59 | loss: 9.8160067Losses:  7.401432037353516 2.1935617923736572 0.26292526721954346
CurrentTrain: epoch  2, batch    60 | loss: 7.4014320Losses:  7.171357154846191 1.948425531387329 0.26134955883026123
CurrentTrain: epoch  2, batch    61 | loss: 7.1713572Losses:  6.310781002044678 0.2532452344894409 0.26982325315475464
CurrentTrain: epoch  2, batch    62 | loss: 6.3107810Losses:  8.647330284118652 3.2265968322753906 0.28849321603775024
CurrentTrain: epoch  3, batch     0 | loss: 8.6473303Losses:  10.13369083404541 4.313748359680176 0.28145885467529297
CurrentTrain: epoch  3, batch     1 | loss: 10.1336908Losses:  8.898579597473145 3.2947099208831787 0.2834101617336273
CurrentTrain: epoch  3, batch     2 | loss: 8.8985796Losses:  7.576189041137695 2.542584180831909 0.25671881437301636
CurrentTrain: epoch  3, batch     3 | loss: 7.5761890Losses:  8.35269546508789 3.034306049346924 0.27521181106567383
CurrentTrain: epoch  3, batch     4 | loss: 8.3526955Losses:  8.131587982177734 2.3162474632263184 0.2746541500091553
CurrentTrain: epoch  3, batch     5 | loss: 8.1315880Losses:  9.4627046585083 3.1274843215942383 0.27824920415878296
CurrentTrain: epoch  3, batch     6 | loss: 9.4627047Losses:  8.607911109924316 2.7521114349365234 0.19844184815883636
CurrentTrain: epoch  3, batch     7 | loss: 8.6079111Losses:  11.969789505004883 5.678407669067383 0.28008154034614563
CurrentTrain: epoch  3, batch     8 | loss: 11.9697895Losses:  9.945684432983398 4.910892009735107 0.2591281235218048
CurrentTrain: epoch  3, batch     9 | loss: 9.9456844Losses:  11.067832946777344 5.815334320068359 0.2733287215232849
CurrentTrain: epoch  3, batch    10 | loss: 11.0678329Losses:  9.33635139465332 4.14451789855957 0.2753438949584961
CurrentTrain: epoch  3, batch    11 | loss: 9.3363514Losses:  6.680838584899902 1.419129490852356 0.2481689155101776
CurrentTrain: epoch  3, batch    12 | loss: 6.6808386Losses:  8.957372665405273 4.086366653442383 0.26023098826408386
CurrentTrain: epoch  3, batch    13 | loss: 8.9573727Losses:  9.948198318481445 3.9665863513946533 0.2695002555847168
CurrentTrain: epoch  3, batch    14 | loss: 9.9481983Losses:  8.38707447052002 3.10969877243042 0.2590128183364868
CurrentTrain: epoch  3, batch    15 | loss: 8.3870745Losses:  11.048102378845215 4.794008255004883 0.2782514691352844
CurrentTrain: epoch  3, batch    16 | loss: 11.0481024Losses:  8.794404029846191 3.148508071899414 0.25479963421821594
CurrentTrain: epoch  3, batch    17 | loss: 8.7944040Losses:  7.883216381072998 3.078303337097168 0.2565562129020691
CurrentTrain: epoch  3, batch    18 | loss: 7.8832164Losses:  7.92482852935791 2.3466124534606934 0.27327901124954224
CurrentTrain: epoch  3, batch    19 | loss: 7.9248285Losses:  7.860329627990723 2.8789756298065186 0.2632277011871338
CurrentTrain: epoch  3, batch    20 | loss: 7.8603296Losses:  9.63930892944336 3.7210404872894287 0.26716044545173645
CurrentTrain: epoch  3, batch    21 | loss: 9.6393089Losses:  7.313216686248779 1.7907075881958008 0.2545267343521118
CurrentTrain: epoch  3, batch    22 | loss: 7.3132167Losses:  11.127248764038086 4.749020099639893 0.26433202624320984
CurrentTrain: epoch  3, batch    23 | loss: 11.1272488Losses:  7.4523701667785645 1.9733365774154663 0.2740135192871094
CurrentTrain: epoch  3, batch    24 | loss: 7.4523702Losses:  11.567364692687988 6.569215297698975 0.20410826802253723
CurrentTrain: epoch  3, batch    25 | loss: 11.5673647Losses:  8.594633102416992 3.3505682945251465 0.2759133577346802
CurrentTrain: epoch  3, batch    26 | loss: 8.5946331Losses:  9.741596221923828 4.113735198974609 0.28246092796325684
CurrentTrain: epoch  3, batch    27 | loss: 9.7415962Losses:  10.514183044433594 5.073044776916504 0.26536524295806885
CurrentTrain: epoch  3, batch    28 | loss: 10.5141830Losses:  8.677513122558594 3.750941276550293 0.2696743905544281
CurrentTrain: epoch  3, batch    29 | loss: 8.6775131Losses:  9.10685920715332 3.7420105934143066 0.25667208433151245
CurrentTrain: epoch  3, batch    30 | loss: 9.1068592Losses:  8.613609313964844 2.98239803314209 0.25825297832489014
CurrentTrain: epoch  3, batch    31 | loss: 8.6136093Losses:  7.23726749420166 2.341874837875366 0.24884016811847687
CurrentTrain: epoch  3, batch    32 | loss: 7.2372675Losses:  9.671653747558594 4.019162178039551 0.17366242408752441
CurrentTrain: epoch  3, batch    33 | loss: 9.6716537Losses:  7.934427261352539 2.940769672393799 0.26269596815109253
CurrentTrain: epoch  3, batch    34 | loss: 7.9344273Losses:  7.571450710296631 2.746570587158203 0.24812942743301392
CurrentTrain: epoch  3, batch    35 | loss: 7.5714507Losses:  8.637592315673828 2.737199306488037 0.24876049160957336
CurrentTrain: epoch  3, batch    36 | loss: 8.6375923Losses:  7.90831995010376 2.5634665489196777 0.2686530351638794
CurrentTrain: epoch  3, batch    37 | loss: 7.9083200Losses:  7.66800594329834 2.9529523849487305 0.25891876220703125
CurrentTrain: epoch  3, batch    38 | loss: 7.6680059Losses:  7.535942077636719 2.697185516357422 0.2608521580696106
CurrentTrain: epoch  3, batch    39 | loss: 7.5359421Losses:  7.776321887969971 3.0151896476745605 0.2543163001537323
CurrentTrain: epoch  3, batch    40 | loss: 7.7763219Losses:  7.917559623718262 2.6638693809509277 0.2713667154312134
CurrentTrain: epoch  3, batch    41 | loss: 7.9175596Losses:  7.958193302154541 3.018090009689331 0.263062983751297
CurrentTrain: epoch  3, batch    42 | loss: 7.9581933Losses:  6.937338829040527 2.257140636444092 0.2471640408039093
CurrentTrain: epoch  3, batch    43 | loss: 6.9373388Losses:  8.016661643981934 2.2745039463043213 0.2375119924545288
CurrentTrain: epoch  3, batch    44 | loss: 8.0166616Losses:  8.468908309936523 3.403568744659424 0.24749146401882172
CurrentTrain: epoch  3, batch    45 | loss: 8.4689083Losses:  8.2122220993042 2.5608134269714355 0.26078665256500244
CurrentTrain: epoch  3, batch    46 | loss: 8.2122221Losses:  10.188262939453125 4.940596580505371 0.2683286964893341
CurrentTrain: epoch  3, batch    47 | loss: 10.1882629Losses:  7.403497695922852 2.2898287773132324 0.2451821267604828
CurrentTrain: epoch  3, batch    48 | loss: 7.4034977Losses:  10.352282524108887 4.945719242095947 0.27272123098373413
CurrentTrain: epoch  3, batch    49 | loss: 10.3522825Losses:  7.913417339324951 3.034658432006836 0.2575293183326721
CurrentTrain: epoch  3, batch    50 | loss: 7.9134173Losses:  7.362797737121582 2.035554885864258 0.2556249499320984
CurrentTrain: epoch  3, batch    51 | loss: 7.3627977Losses:  7.6685943603515625 2.8663430213928223 0.24570509791374207
CurrentTrain: epoch  3, batch    52 | loss: 7.6685944Losses:  8.204462051391602 3.282869577407837 0.2572399973869324
CurrentTrain: epoch  3, batch    53 | loss: 8.2044621Losses:  7.923447132110596 2.9891302585601807 0.26604244112968445
CurrentTrain: epoch  3, batch    54 | loss: 7.9234471Losses:  10.044761657714844 4.125189304351807 0.25438830256462097
CurrentTrain: epoch  3, batch    55 | loss: 10.0447617Losses:  8.758461952209473 3.964437961578369 0.24844445288181305
CurrentTrain: epoch  3, batch    56 | loss: 8.7584620Losses:  8.101800918579102 2.8223142623901367 0.24985991418361664
CurrentTrain: epoch  3, batch    57 | loss: 8.1018009Losses:  11.945615768432617 6.226654052734375 0.2708016037940979
CurrentTrain: epoch  3, batch    58 | loss: 11.9456158Losses:  8.382587432861328 3.6532387733459473 0.24666829407215118
CurrentTrain: epoch  3, batch    59 | loss: 8.3825874Losses:  8.965385437011719 4.032176971435547 0.2579631805419922
CurrentTrain: epoch  3, batch    60 | loss: 8.9653854Losses:  10.162817001342773 4.083828926086426 0.2537420690059662
CurrentTrain: epoch  3, batch    61 | loss: 10.1628170Losses:  9.170063972473145 2.346940040588379 0.29690712690353394
CurrentTrain: epoch  3, batch    62 | loss: 9.1700640Losses:  6.976720809936523 2.3270680904388428 0.24591748416423798
CurrentTrain: epoch  4, batch     0 | loss: 6.9767208Losses:  8.694615364074707 2.700364112854004 0.2585170567035675
CurrentTrain: epoch  4, batch     1 | loss: 8.6946154Losses:  8.373661041259766 3.290921449661255 0.26338401436805725
CurrentTrain: epoch  4, batch     2 | loss: 8.3736610Losses:  8.147512435913086 2.6583199501037598 0.25103747844696045
CurrentTrain: epoch  4, batch     3 | loss: 8.1475124Losses:  8.819097518920898 3.946079969406128 0.2555686831474304
CurrentTrain: epoch  4, batch     4 | loss: 8.8190975Losses:  7.824149131774902 2.5424532890319824 0.24412494897842407
CurrentTrain: epoch  4, batch     5 | loss: 7.8241491Losses:  7.905614852905273 2.7978241443634033 0.2428056001663208
CurrentTrain: epoch  4, batch     6 | loss: 7.9056149Losses:  7.153228282928467 2.3482041358947754 0.2450849562883377
CurrentTrain: epoch  4, batch     7 | loss: 7.1532283Losses:  7.081957817077637 1.9607493877410889 0.24509119987487793
CurrentTrain: epoch  4, batch     8 | loss: 7.0819578Losses:  7.25319766998291 2.4826745986938477 0.23950088024139404
CurrentTrain: epoch  4, batch     9 | loss: 7.2531977Losses:  10.566913604736328 5.814517974853516 0.2715287208557129
CurrentTrain: epoch  4, batch    10 | loss: 10.5669136Losses:  8.926046371459961 4.161380767822266 0.2661464810371399
CurrentTrain: epoch  4, batch    11 | loss: 8.9260464Losses:  10.280265808105469 4.301671028137207 0.2840477228164673
CurrentTrain: epoch  4, batch    12 | loss: 10.2802658Losses:  7.497663497924805 2.4613311290740967 0.2485789656639099
CurrentTrain: epoch  4, batch    13 | loss: 7.4976635Losses:  7.882213115692139 3.036869764328003 0.2413124442100525
CurrentTrain: epoch  4, batch    14 | loss: 7.8822131Losses:  9.295552253723145 4.700981140136719 0.2491689920425415
CurrentTrain: epoch  4, batch    15 | loss: 9.2955523Losses:  9.687300682067871 4.7911200523376465 0.25470489263534546
CurrentTrain: epoch  4, batch    16 | loss: 9.6873007Losses:  11.10258960723877 5.7935051918029785 0.2751748263835907
CurrentTrain: epoch  4, batch    17 | loss: 11.1025896Losses:  6.987725734710693 2.224626064300537 0.2379569262266159
CurrentTrain: epoch  4, batch    18 | loss: 6.9877257Losses:  13.221068382263184 7.383521556854248 0.26928675174713135
CurrentTrain: epoch  4, batch    19 | loss: 13.2210684Losses:  7.776127815246582 3.0769519805908203 0.2459823191165924
CurrentTrain: epoch  4, batch    20 | loss: 7.7761278Losses:  8.576558113098145 3.5597119331359863 0.27112889289855957
CurrentTrain: epoch  4, batch    21 | loss: 8.5765581Losses:  7.6758832931518555 3.0475761890411377 0.262207955121994
CurrentTrain: epoch  4, batch    22 | loss: 7.6758833Losses:  9.670284271240234 4.922144412994385 0.2601930499076843
CurrentTrain: epoch  4, batch    23 | loss: 9.6702843Losses:  6.656341075897217 2.0732669830322266 0.23213228583335876
CurrentTrain: epoch  4, batch    24 | loss: 6.6563411Losses:  7.527780532836914 2.7071471214294434 0.25354820489883423
CurrentTrain: epoch  4, batch    25 | loss: 7.5277805Losses:  8.585735321044922 3.945232629776001 0.25231441855430603
CurrentTrain: epoch  4, batch    26 | loss: 8.5857353Losses:  9.218035697937012 4.369296073913574 0.28417646884918213
CurrentTrain: epoch  4, batch    27 | loss: 9.2180357Losses:  7.180080890655518 2.32135009765625 0.23935988545417786
CurrentTrain: epoch  4, batch    28 | loss: 7.1800809Losses:  9.117318153381348 4.223604202270508 0.2684936225414276
CurrentTrain: epoch  4, batch    29 | loss: 9.1173182Losses:  8.304744720458984 3.4257497787475586 0.2523338496685028
CurrentTrain: epoch  4, batch    30 | loss: 8.3047447Losses:  7.473471164703369 2.772190570831299 0.25880610942840576
CurrentTrain: epoch  4, batch    31 | loss: 7.4734712Losses:  7.886970520019531 2.8065571784973145 0.23959705233573914
CurrentTrain: epoch  4, batch    32 | loss: 7.8869705Losses:  8.028190612792969 2.9738850593566895 0.24391496181488037
CurrentTrain: epoch  4, batch    33 | loss: 8.0281906Losses:  9.581878662109375 4.404572010040283 0.26016664505004883
CurrentTrain: epoch  4, batch    34 | loss: 9.5818787Losses:  10.11252212524414 5.5672149658203125 0.2508193850517273
CurrentTrain: epoch  4, batch    35 | loss: 10.1125221Losses:  9.11500358581543 3.324781894683838 0.2439408153295517
CurrentTrain: epoch  4, batch    36 | loss: 9.1150036Losses:  8.60184383392334 3.9450392723083496 0.25639063119888306
CurrentTrain: epoch  4, batch    37 | loss: 8.6018438Losses:  9.266695976257324 4.262707233428955 0.2531646192073822
CurrentTrain: epoch  4, batch    38 | loss: 9.2666960Losses:  8.642382621765137 3.321645975112915 0.24093788862228394
CurrentTrain: epoch  4, batch    39 | loss: 8.6423826Losses:  7.852987289428711 2.7336068153381348 0.24424734711647034
CurrentTrain: epoch  4, batch    40 | loss: 7.8529873Losses:  7.4065752029418945 2.1969666481018066 0.23990359902381897
CurrentTrain: epoch  4, batch    41 | loss: 7.4065752Losses:  8.993365287780762 4.116043567657471 0.23268237709999084
CurrentTrain: epoch  4, batch    42 | loss: 8.9933653Losses:  8.647442817687988 3.892963409423828 0.25292903184890747
CurrentTrain: epoch  4, batch    43 | loss: 8.6474428Losses:  8.316875457763672 2.6494250297546387 0.24175876379013062
CurrentTrain: epoch  4, batch    44 | loss: 8.3168755Losses:  6.284790992736816 1.723081111907959 0.22701792418956757
CurrentTrain: epoch  4, batch    45 | loss: 6.2847910Losses:  10.527563095092773 5.167829513549805 0.24812421202659607
CurrentTrain: epoch  4, batch    46 | loss: 10.5275631Losses:  9.808835983276367 5.345221519470215 0.2501911520957947
CurrentTrain: epoch  4, batch    47 | loss: 9.8088360Losses:  8.865447044372559 4.015312194824219 0.2520102262496948
CurrentTrain: epoch  4, batch    48 | loss: 8.8654470Losses:  6.380661487579346 1.750372052192688 0.2350422441959381
CurrentTrain: epoch  4, batch    49 | loss: 6.3806615Losses:  6.413573265075684 1.8444108963012695 0.23306924104690552
CurrentTrain: epoch  4, batch    50 | loss: 6.4135733Losses:  7.2725348472595215 2.8259129524230957 0.25174152851104736
CurrentTrain: epoch  4, batch    51 | loss: 7.2725348Losses:  9.049609184265137 3.9713501930236816 0.18377575278282166
CurrentTrain: epoch  4, batch    52 | loss: 9.0496092Losses:  10.322638511657715 5.101813793182373 0.2667016088962555
CurrentTrain: epoch  4, batch    53 | loss: 10.3226385Losses:  7.495092391967773 2.8968281745910645 0.24383394420146942
CurrentTrain: epoch  4, batch    54 | loss: 7.4950924Losses:  8.255908966064453 2.8305702209472656 0.24785315990447998
CurrentTrain: epoch  4, batch    55 | loss: 8.2559090Losses:  7.66140079498291 2.99609375 0.25405097007751465
CurrentTrain: epoch  4, batch    56 | loss: 7.6614008Losses:  6.549769878387451 1.9179139137268066 0.23730596899986267
CurrentTrain: epoch  4, batch    57 | loss: 6.5497699Losses:  8.959497451782227 4.543735027313232 0.1830430030822754
CurrentTrain: epoch  4, batch    58 | loss: 8.9594975Losses:  7.619379043579102 3.0418033599853516 0.24792344868183136
CurrentTrain: epoch  4, batch    59 | loss: 7.6193790Losses:  7.939121246337891 3.3508100509643555 0.2521495223045349
CurrentTrain: epoch  4, batch    60 | loss: 7.9391212Losses:  6.7937726974487305 2.4380135536193848 0.23446230590343475
CurrentTrain: epoch  4, batch    61 | loss: 6.7937727Losses:  5.001151084899902 0.24042204022407532 0.16697882115840912
CurrentTrain: epoch  4, batch    62 | loss: 5.0011511Losses:  8.880048751831055 3.768505334854126 0.23857976496219635
CurrentTrain: epoch  5, batch     0 | loss: 8.8800488Losses:  8.292732238769531 3.7923033237457275 0.16365714371204376
CurrentTrain: epoch  5, batch     1 | loss: 8.2927322Losses:  7.086587429046631 2.5228586196899414 0.23521769046783447
CurrentTrain: epoch  5, batch     2 | loss: 7.0865874Losses:  8.20529842376709 2.5515098571777344 0.2464188039302826
CurrentTrain: epoch  5, batch     3 | loss: 8.2052984Losses:  7.644692420959473 3.2155981063842773 0.2248445749282837
CurrentTrain: epoch  5, batch     4 | loss: 7.6446924Losses:  10.150558471679688 4.526541233062744 0.2536751627922058
CurrentTrain: epoch  5, batch     5 | loss: 10.1505585Losses:  7.670015335083008 2.5282533168792725 0.24298971891403198
CurrentTrain: epoch  5, batch     6 | loss: 7.6700153Losses:  7.7971601486206055 3.3150367736816406 0.2490769326686859
CurrentTrain: epoch  5, batch     7 | loss: 7.7971601Losses:  6.6836066246032715 2.277909517288208 0.23150962591171265
CurrentTrain: epoch  5, batch     8 | loss: 6.6836066Losses:  7.459911823272705 2.5464959144592285 0.24599778652191162
CurrentTrain: epoch  5, batch     9 | loss: 7.4599118Losses:  7.751253604888916 3.3290696144104004 0.24474051594734192
CurrentTrain: epoch  5, batch    10 | loss: 7.7512536Losses:  8.213167190551758 3.5855906009674072 0.23606061935424805
CurrentTrain: epoch  5, batch    11 | loss: 8.2131672Losses:  6.259345054626465 1.7579107284545898 0.2208254337310791
CurrentTrain: epoch  5, batch    12 | loss: 6.2593451Losses:  8.6026611328125 4.18152379989624 0.23697586357593536
CurrentTrain: epoch  5, batch    13 | loss: 8.6026611Losses:  6.708341598510742 2.285398006439209 0.23140877485275269
CurrentTrain: epoch  5, batch    14 | loss: 6.7083416Losses:  6.253118515014648 1.4180357456207275 0.22666476666927338
CurrentTrain: epoch  5, batch    15 | loss: 6.2531185Losses:  6.750400066375732 2.279116153717041 0.23425894975662231
CurrentTrain: epoch  5, batch    16 | loss: 6.7504001Losses:  8.564362525939941 3.416865110397339 0.23549243807792664
CurrentTrain: epoch  5, batch    17 | loss: 8.5643625Losses:  9.843046188354492 5.412624835968018 0.2486272007226944
CurrentTrain: epoch  5, batch    18 | loss: 9.8430462Losses:  11.542501449584961 7.041292190551758 0.2587664723396301
CurrentTrain: epoch  5, batch    19 | loss: 11.5425014Losses:  9.024232864379883 4.308248996734619 0.22818583250045776
CurrentTrain: epoch  5, batch    20 | loss: 9.0242329Losses:  8.754841804504395 4.2559614181518555 0.15697190165519714
CurrentTrain: epoch  5, batch    21 | loss: 8.7548418Losses:  9.847298622131348 4.823962211608887 0.2557581663131714
CurrentTrain: epoch  5, batch    22 | loss: 9.8472986Losses:  15.251485824584961 10.648726463317871 0.194271057844162
CurrentTrain: epoch  5, batch    23 | loss: 15.2514858Losses:  9.429409980773926 4.471290588378906 0.2408701777458191
CurrentTrain: epoch  5, batch    24 | loss: 9.4294100Losses:  7.695026874542236 2.9608678817749023 0.2354801595211029
CurrentTrain: epoch  5, batch    25 | loss: 7.6950269Losses:  7.554875373840332 3.0163190364837646 0.23435594141483307
CurrentTrain: epoch  5, batch    26 | loss: 7.5548754Losses:  8.599283218383789 3.1819562911987305 0.2490871250629425
CurrentTrain: epoch  5, batch    27 | loss: 8.5992832Losses:  7.837554454803467 3.240194797515869 0.2438574731349945
CurrentTrain: epoch  5, batch    28 | loss: 7.8375545Losses:  6.4467997550964355 2.065568447113037 0.2313545048236847
CurrentTrain: epoch  5, batch    29 | loss: 6.4467998Losses:  7.3275837898254395 2.638278007507324 0.24201716482639313
CurrentTrain: epoch  5, batch    30 | loss: 7.3275838Losses:  8.613832473754883 2.8322713375091553 0.2349715232849121
CurrentTrain: epoch  5, batch    31 | loss: 8.6138325Losses:  7.205305099487305 2.6169683933258057 0.22146694362163544
CurrentTrain: epoch  5, batch    32 | loss: 7.2053051Losses:  7.649021148681641 2.6203482151031494 0.22686490416526794
CurrentTrain: epoch  5, batch    33 | loss: 7.6490211Losses:  8.135872840881348 2.96118426322937 0.23534174263477325
CurrentTrain: epoch  5, batch    34 | loss: 8.1358728Losses:  7.204847812652588 2.5560152530670166 0.2501349449157715
CurrentTrain: epoch  5, batch    35 | loss: 7.2048478Losses:  7.323003768920898 2.7697789669036865 0.23899944126605988
CurrentTrain: epoch  5, batch    36 | loss: 7.3230038Losses:  7.911227703094482 3.139697313308716 0.24577464163303375
CurrentTrain: epoch  5, batch    37 | loss: 7.9112277Losses:  9.553916931152344 4.567400932312012 0.24576601386070251
CurrentTrain: epoch  5, batch    38 | loss: 9.5539169Losses:  8.066936492919922 3.4191925525665283 0.25698643922805786
CurrentTrain: epoch  5, batch    39 | loss: 8.0669365Losses:  8.44148063659668 3.703291893005371 0.24639739096164703
CurrentTrain: epoch  5, batch    40 | loss: 8.4414806Losses:  8.988105773925781 4.193507671356201 0.24428434669971466
CurrentTrain: epoch  5, batch    41 | loss: 8.9881058Losses:  6.316859722137451 1.9570157527923584 0.22253826260566711
CurrentTrain: epoch  5, batch    42 | loss: 6.3168597Losses:  8.981962203979492 4.458984375 0.25545769929885864
CurrentTrain: epoch  5, batch    43 | loss: 8.9819622Losses:  9.22497272491455 4.257458209991455 0.1706761121749878
CurrentTrain: epoch  5, batch    44 | loss: 9.2249727Losses:  7.876258850097656 3.178936243057251 0.2557944655418396
CurrentTrain: epoch  5, batch    45 | loss: 7.8762589Losses:  9.491419792175293 4.680573463439941 0.25096389651298523
CurrentTrain: epoch  5, batch    46 | loss: 9.4914198Losses:  8.726116180419922 4.295915126800537 0.25646042823791504
CurrentTrain: epoch  5, batch    47 | loss: 8.7261162Losses:  7.735779285430908 3.2870335578918457 0.24238398671150208
CurrentTrain: epoch  5, batch    48 | loss: 7.7357793Losses:  9.373882293701172 5.091514587402344 0.14938288927078247
CurrentTrain: epoch  5, batch    49 | loss: 9.3738823Losses:  9.435702323913574 4.898874759674072 0.28061026334762573
CurrentTrain: epoch  5, batch    50 | loss: 9.4357023Losses:  10.066106796264648 5.284132957458496 0.26336565613746643
CurrentTrain: epoch  5, batch    51 | loss: 10.0661068Losses:  7.205328941345215 2.314012050628662 0.24407434463500977
CurrentTrain: epoch  5, batch    52 | loss: 7.2053289Losses:  9.568696975708008 4.621176719665527 0.25247424840927124
CurrentTrain: epoch  5, batch    53 | loss: 9.5686970Losses:  10.856176376342773 6.432821750640869 0.24308304488658905
CurrentTrain: epoch  5, batch    54 | loss: 10.8561764Losses:  7.877062797546387 3.5820109844207764 0.23249348998069763
CurrentTrain: epoch  5, batch    55 | loss: 7.8770628Losses:  6.999589920043945 2.3160858154296875 0.23367488384246826
CurrentTrain: epoch  5, batch    56 | loss: 6.9995899Losses:  8.435081481933594 3.6080074310302734 0.14778247475624084
CurrentTrain: epoch  5, batch    57 | loss: 8.4350815Losses:  13.134100914001465 8.728341102600098 0.24251936376094818
CurrentTrain: epoch  5, batch    58 | loss: 13.1341009Losses:  7.004050254821777 2.242049217224121 0.22844845056533813
CurrentTrain: epoch  5, batch    59 | loss: 7.0040503Losses:  6.865665435791016 2.2737884521484375 0.22683720290660858
CurrentTrain: epoch  5, batch    60 | loss: 6.8656654Losses:  7.346494674682617 2.30619478225708 0.22739681601524353
CurrentTrain: epoch  5, batch    61 | loss: 7.3464947Losses:  6.002650737762451 1.4486266374588013 0.20037907361984253
CurrentTrain: epoch  5, batch    62 | loss: 6.0026507Losses:  8.93458366394043 4.361685752868652 0.24313798546791077
CurrentTrain: epoch  6, batch     0 | loss: 8.9345837Losses:  7.541909217834473 2.982177972793579 0.23057855665683746
CurrentTrain: epoch  6, batch     1 | loss: 7.5419092Losses:  8.461006164550781 4.0572662353515625 0.2635931968688965
CurrentTrain: epoch  6, batch     2 | loss: 8.4610062Losses:  7.417377471923828 3.0273706912994385 0.24355614185333252
CurrentTrain: epoch  6, batch     3 | loss: 7.4173775Losses:  6.759143352508545 2.373884677886963 0.23385962843894958
CurrentTrain: epoch  6, batch     4 | loss: 6.7591434Losses:  6.685887336730957 2.1810338497161865 0.23316460847854614
CurrentTrain: epoch  6, batch     5 | loss: 6.6858873Losses:  12.859220504760742 8.544005393981934 0.24649888277053833
CurrentTrain: epoch  6, batch     6 | loss: 12.8592205Losses:  8.201545715332031 3.6438565254211426 0.24231767654418945
CurrentTrain: epoch  6, batch     7 | loss: 8.2015457Losses:  7.297028064727783 2.839329242706299 0.2387504130601883
CurrentTrain: epoch  6, batch     8 | loss: 7.2970281Losses:  7.247581958770752 2.6156368255615234 0.23319199681282043
CurrentTrain: epoch  6, batch     9 | loss: 7.2475820Losses:  7.078034400939941 2.7301454544067383 0.22419743239879608
CurrentTrain: epoch  6, batch    10 | loss: 7.0780344Losses:  7.253130912780762 2.790574550628662 0.24218964576721191
CurrentTrain: epoch  6, batch    11 | loss: 7.2531309Losses:  7.7624592781066895 3.2949180603027344 0.2613920569419861
CurrentTrain: epoch  6, batch    12 | loss: 7.7624593Losses:  7.431405544281006 3.1087536811828613 0.22808866202831268
CurrentTrain: epoch  6, batch    13 | loss: 7.4314055Losses:  9.446046829223633 4.904019355773926 0.2456567883491516
CurrentTrain: epoch  6, batch    14 | loss: 9.4460468Losses:  6.83376932144165 2.4110097885131836 0.24083185195922852
CurrentTrain: epoch  6, batch    15 | loss: 6.8337693Losses:  8.339366912841797 3.734565258026123 0.15605321526527405
CurrentTrain: epoch  6, batch    16 | loss: 8.3393669Losses:  7.498057842254639 3.1044301986694336 0.23050183057785034
CurrentTrain: epoch  6, batch    17 | loss: 7.4980578Losses:  6.037783622741699 1.634704351425171 0.21565741300582886
CurrentTrain: epoch  6, batch    18 | loss: 6.0377836Losses:  7.734653949737549 3.0727906227111816 0.23985230922698975
CurrentTrain: epoch  6, batch    19 | loss: 7.7346539Losses:  7.219397068023682 2.6410117149353027 0.23472759127616882
CurrentTrain: epoch  6, batch    20 | loss: 7.2193971Losses:  6.668797016143799 2.2566938400268555 0.2367824912071228
CurrentTrain: epoch  6, batch    21 | loss: 6.6687970Losses:  10.353453636169434 5.9800896644592285 0.24729269742965698
CurrentTrain: epoch  6, batch    22 | loss: 10.3534536Losses:  6.498988628387451 2.1654176712036133 0.22777479887008667
CurrentTrain: epoch  6, batch    23 | loss: 6.4989886Losses:  7.381348133087158 3.0329089164733887 0.22827735543251038
CurrentTrain: epoch  6, batch    24 | loss: 7.3813481Losses:  7.059916973114014 2.688502311706543 0.23397797346115112
CurrentTrain: epoch  6, batch    25 | loss: 7.0599170Losses:  9.574387550354004 4.936614990234375 0.2672666013240814
CurrentTrain: epoch  6, batch    26 | loss: 9.5743876Losses:  11.486328125 7.059502124786377 0.2659493088722229
CurrentTrain: epoch  6, batch    27 | loss: 11.4863281Losses:  6.47291374206543 2.0708963871002197 0.213581845164299
CurrentTrain: epoch  6, batch    28 | loss: 6.4729137Losses:  7.619268417358398 3.0962624549865723 0.23099683225154877
CurrentTrain: epoch  6, batch    29 | loss: 7.6192684Losses:  6.856273174285889 2.1550889015197754 0.22725559771060944
CurrentTrain: epoch  6, batch    30 | loss: 6.8562732Losses:  11.421910285949707 7.185831069946289 0.1566893458366394
CurrentTrain: epoch  6, batch    31 | loss: 11.4219103Losses:  7.66048526763916 3.1677894592285156 0.2615787386894226
CurrentTrain: epoch  6, batch    32 | loss: 7.6604853Losses:  7.471981525421143 3.0428857803344727 0.22972039878368378
CurrentTrain: epoch  6, batch    33 | loss: 7.4719815Losses:  7.618080139160156 3.219179630279541 0.2513590455055237
CurrentTrain: epoch  6, batch    34 | loss: 7.6180801Losses:  8.172229766845703 3.78920841217041 0.26182395219802856
CurrentTrain: epoch  6, batch    35 | loss: 8.1722298Losses:  7.188806533813477 2.8590402603149414 0.23139940202236176
CurrentTrain: epoch  6, batch    36 | loss: 7.1888065Losses:  8.127128601074219 3.8426730632781982 0.23081658780574799
CurrentTrain: epoch  6, batch    37 | loss: 8.1271286Losses:  7.103090286254883 2.673020362854004 0.22782063484191895
CurrentTrain: epoch  6, batch    38 | loss: 7.1030903Losses:  7.596710205078125 3.1930313110351562 0.23924069106578827
CurrentTrain: epoch  6, batch    39 | loss: 7.5967102Losses:  7.180117607116699 2.700917959213257 0.23364558815956116
CurrentTrain: epoch  6, batch    40 | loss: 7.1801176Losses:  13.381582260131836 8.912458419799805 0.21146658062934875
CurrentTrain: epoch  6, batch    41 | loss: 13.3815823Losses:  6.600378513336182 2.198624849319458 0.226175919175148
CurrentTrain: epoch  6, batch    42 | loss: 6.6003785Losses:  11.798442840576172 6.174639701843262 0.2496742606163025
CurrentTrain: epoch  6, batch    43 | loss: 11.7984428Losses:  9.720569610595703 5.396291732788086 0.2586336135864258
CurrentTrain: epoch  6, batch    44 | loss: 9.7205696Losses:  6.714553356170654 2.1745214462280273 0.23532629013061523
CurrentTrain: epoch  6, batch    45 | loss: 6.7145534Losses:  7.478331089019775 3.1966781616210938 0.23008480668067932
CurrentTrain: epoch  6, batch    46 | loss: 7.4783311Losses:  6.697652339935303 2.226339817047119 0.22656506299972534
CurrentTrain: epoch  6, batch    47 | loss: 6.6976523Losses:  8.66666316986084 4.437479019165039 0.23767751455307007
CurrentTrain: epoch  6, batch    48 | loss: 8.6666632Losses:  8.586119651794434 4.382096290588379 0.1544993370771408
CurrentTrain: epoch  6, batch    49 | loss: 8.5861197Losses:  8.4248046875 4.136255741119385 0.23338082432746887
CurrentTrain: epoch  6, batch    50 | loss: 8.4248047Losses:  8.048666954040527 3.743924856185913 0.17456744611263275
CurrentTrain: epoch  6, batch    51 | loss: 8.0486670Losses:  7.973941802978516 3.1737427711486816 0.23391492664813995
CurrentTrain: epoch  6, batch    52 | loss: 7.9739418Losses:  7.461200714111328 3.1375374794006348 0.24218273162841797
CurrentTrain: epoch  6, batch    53 | loss: 7.4612007Losses:  8.821319580078125 4.172157287597656 0.24365916848182678
CurrentTrain: epoch  6, batch    54 | loss: 8.8213196Losses:  6.165192127227783 1.8454480171203613 0.21920277178287506
CurrentTrain: epoch  6, batch    55 | loss: 6.1651921Losses:  7.857468605041504 3.5716898441314697 0.23075413703918457
CurrentTrain: epoch  6, batch    56 | loss: 7.8574686Losses:  8.514961242675781 4.16053581237793 0.23147639632225037
CurrentTrain: epoch  6, batch    57 | loss: 8.5149612Losses:  6.8672895431518555 2.5634872913360596 0.23190008103847504
CurrentTrain: epoch  6, batch    58 | loss: 6.8672895Losses:  8.349695205688477 3.9734079837799072 0.2505040764808655
CurrentTrain: epoch  6, batch    59 | loss: 8.3496952Losses:  7.789600849151611 3.1621899604797363 0.24057215452194214
CurrentTrain: epoch  6, batch    60 | loss: 7.7896008Losses:  9.734294891357422 4.541604042053223 0.24075663089752197
CurrentTrain: epoch  6, batch    61 | loss: 9.7342949Losses:  4.544118881225586 0.24805134534835815 0.244492769241333
CurrentTrain: epoch  6, batch    62 | loss: 4.5441189Losses:  7.045028209686279 2.7791290283203125 0.2360762059688568
CurrentTrain: epoch  7, batch     0 | loss: 7.0450282Losses:  7.633919715881348 3.3570332527160645 0.23784519731998444
CurrentTrain: epoch  7, batch     1 | loss: 7.6339197Losses:  8.195429801940918 3.3704566955566406 0.14916889369487762
CurrentTrain: epoch  7, batch     2 | loss: 8.1954298Losses:  7.912553310394287 3.563199520111084 0.23242641985416412
CurrentTrain: epoch  7, batch     3 | loss: 7.9125533Losses:  6.3852009773254395 2.096782684326172 0.23724189400672913
CurrentTrain: epoch  7, batch     4 | loss: 6.3852010Losses:  7.315615177154541 3.018517017364502 0.23636658489704132
CurrentTrain: epoch  7, batch     5 | loss: 7.3156152Losses:  7.6657795906066895 2.9558708667755127 0.15304701030254364
CurrentTrain: epoch  7, batch     6 | loss: 7.6657796Losses:  9.951407432556152 5.614856719970703 0.2542545199394226
CurrentTrain: epoch  7, batch     7 | loss: 9.9514074Losses:  7.286827564239502 2.948845148086548 0.23966839909553528
CurrentTrain: epoch  7, batch     8 | loss: 7.2868276Losses:  7.740940570831299 3.418837070465088 0.23391833901405334
CurrentTrain: epoch  7, batch     9 | loss: 7.7409406Losses:  7.592442512512207 3.2078640460968018 0.2330072820186615
CurrentTrain: epoch  7, batch    10 | loss: 7.5924425Losses:  7.1036529541015625 2.750304937362671 0.20762419700622559
CurrentTrain: epoch  7, batch    11 | loss: 7.1036530Losses:  8.72651481628418 4.549603462219238 0.1459815502166748
CurrentTrain: epoch  7, batch    12 | loss: 8.7265148Losses:  5.896110534667969 1.6261816024780273 0.21260274946689606
CurrentTrain: epoch  7, batch    13 | loss: 5.8961105Losses:  7.01515531539917 2.748478412628174 0.23160924017429352
CurrentTrain: epoch  7, batch    14 | loss: 7.0151553Losses:  6.360286235809326 2.030585527420044 0.22911299765110016
CurrentTrain: epoch  7, batch    15 | loss: 6.3602862Losses:  7.058065414428711 2.5958411693573 0.23626399040222168
CurrentTrain: epoch  7, batch    16 | loss: 7.0580654Losses:  7.070616245269775 2.7159500122070312 0.23400968313217163
CurrentTrain: epoch  7, batch    17 | loss: 7.0706162Losses:  7.910249710083008 3.356107234954834 0.25362712144851685
CurrentTrain: epoch  7, batch    18 | loss: 7.9102497Losses:  8.047516822814941 3.80572247505188 0.22265344858169556
CurrentTrain: epoch  7, batch    19 | loss: 8.0475168Losses:  6.986468315124512 2.681741952896118 0.2317369282245636
CurrentTrain: epoch  7, batch    20 | loss: 6.9864683Losses:  7.024669647216797 2.7349188327789307 0.23855598270893097
CurrentTrain: epoch  7, batch    21 | loss: 7.0246696Losses:  6.197534084320068 1.8948150873184204 0.2321549654006958
CurrentTrain: epoch  7, batch    22 | loss: 6.1975341Losses:  10.40611457824707 6.183036804199219 0.2433912456035614
CurrentTrain: epoch  7, batch    23 | loss: 10.4061146Losses:  9.022664070129395 4.742814540863037 0.24338781833648682
CurrentTrain: epoch  7, batch    24 | loss: 9.0226641Losses:  6.9575114250183105 2.634922981262207 0.2246602177619934
CurrentTrain: epoch  7, batch    25 | loss: 6.9575114Losses:  6.210516929626465 2.0429787635803223 0.1394936889410019
CurrentTrain: epoch  7, batch    26 | loss: 6.2105169Losses:  8.322273254394531 4.041242599487305 0.2312890589237213
CurrentTrain: epoch  7, batch    27 | loss: 8.3222733Losses:  5.8520989418029785 1.6087086200714111 0.21137768030166626
CurrentTrain: epoch  7, batch    28 | loss: 5.8520989Losses:  6.076280117034912 1.799593448638916 0.2252873331308365
CurrentTrain: epoch  7, batch    29 | loss: 6.0762801Losses:  7.54899787902832 3.181593418121338 0.24571561813354492
CurrentTrain: epoch  7, batch    30 | loss: 7.5489979Losses:  7.4508466720581055 3.1390035152435303 0.23530396819114685
CurrentTrain: epoch  7, batch    31 | loss: 7.4508467Losses:  7.2134599685668945 2.9252984523773193 0.22719180583953857
CurrentTrain: epoch  7, batch    32 | loss: 7.2134600Losses:  6.354197978973389 2.1284656524658203 0.2210891842842102
CurrentTrain: epoch  7, batch    33 | loss: 6.3541980Losses:  6.754740238189697 2.4615330696105957 0.22680774331092834
CurrentTrain: epoch  7, batch    34 | loss: 6.7547402Losses:  7.092283248901367 2.733670949935913 0.23898208141326904
CurrentTrain: epoch  7, batch    35 | loss: 7.0922832Losses:  6.825952053070068 2.587409734725952 0.21852722764015198
CurrentTrain: epoch  7, batch    36 | loss: 6.8259521Losses:  8.266572952270508 4.088801383972168 0.22982372343540192
CurrentTrain: epoch  7, batch    37 | loss: 8.2665730Losses:  7.304047584533691 2.974764823913574 0.2524913549423218
CurrentTrain: epoch  7, batch    38 | loss: 7.3040476Losses:  7.187562942504883 2.953852653503418 0.22829470038414001
CurrentTrain: epoch  7, batch    39 | loss: 7.1875629Losses:  7.133275985717773 2.863518238067627 0.22979578375816345
CurrentTrain: epoch  7, batch    40 | loss: 7.1332760Losses:  7.5519819259643555 3.103349208831787 0.24191316962242126
CurrentTrain: epoch  7, batch    41 | loss: 7.5519819Losses:  6.091622352600098 1.8318980932235718 0.2217479795217514
CurrentTrain: epoch  7, batch    42 | loss: 6.0916224Losses:  8.89612102508545 4.449333667755127 0.24884919822216034
CurrentTrain: epoch  7, batch    43 | loss: 8.8961210Losses:  8.576593399047852 4.272432327270508 0.25220346450805664
CurrentTrain: epoch  7, batch    44 | loss: 8.5765934Losses:  6.9374470710754395 2.598700761795044 0.22812320291996002
CurrentTrain: epoch  7, batch    45 | loss: 6.9374471Losses:  7.158089637756348 2.925426721572876 0.22115537524223328
CurrentTrain: epoch  7, batch    46 | loss: 7.1580896Losses:  6.908770561218262 2.631300449371338 0.2322182059288025
CurrentTrain: epoch  7, batch    47 | loss: 6.9087706Losses:  6.685208797454834 2.391439914703369 0.23074716329574585
CurrentTrain: epoch  7, batch    48 | loss: 6.6852088Losses:  9.127490043640137 4.915546894073486 0.23769748210906982
CurrentTrain: epoch  7, batch    49 | loss: 9.1274900Losses:  7.077779769897461 2.893528461456299 0.24428853392601013
CurrentTrain: epoch  7, batch    50 | loss: 7.0777798Losses:  6.508829593658447 2.1419625282287598 0.2220371812582016
CurrentTrain: epoch  7, batch    51 | loss: 6.5088296Losses:  7.311418056488037 2.9382424354553223 0.23180106282234192
CurrentTrain: epoch  7, batch    52 | loss: 7.3114181Losses:  9.413691520690918 5.08514404296875 0.26271021366119385
CurrentTrain: epoch  7, batch    53 | loss: 9.4136915Losses:  8.720193862915039 4.466334342956543 0.2321435809135437
CurrentTrain: epoch  7, batch    54 | loss: 8.7201939Losses:  6.703521728515625 2.4693188667297363 0.23171862959861755
CurrentTrain: epoch  7, batch    55 | loss: 6.7035217Losses:  6.247651100158691 1.9830868244171143 0.22415360808372498
CurrentTrain: epoch  7, batch    56 | loss: 6.2476511Losses:  8.407683372497559 4.196840286254883 0.2214895337820053
CurrentTrain: epoch  7, batch    57 | loss: 8.4076834Losses:  7.371140003204346 3.1244447231292725 0.23680055141448975
CurrentTrain: epoch  7, batch    58 | loss: 7.3711400Losses:  8.339459419250488 4.082453727722168 0.23271587491035461
CurrentTrain: epoch  7, batch    59 | loss: 8.3394594Losses:  9.02706241607666 4.756945610046387 0.2465890794992447
CurrentTrain: epoch  7, batch    60 | loss: 9.0270624Losses:  11.224228858947754 7.050392150878906 0.23644967377185822
CurrentTrain: epoch  7, batch    61 | loss: 11.2242289Losses:  4.786423206329346 0.5193611979484558 0.26922890543937683
CurrentTrain: epoch  7, batch    62 | loss: 4.7864232Losses:  7.805477142333984 3.556363821029663 0.25180089473724365
CurrentTrain: epoch  8, batch     0 | loss: 7.8054771Losses:  15.689879417419434 11.430177688598633 0.24338647723197937
CurrentTrain: epoch  8, batch     1 | loss: 15.6898794Losses:  9.2347412109375 4.948938846588135 0.25902578234672546
CurrentTrain: epoch  8, batch     2 | loss: 9.2347412Losses:  12.676539421081543 8.45797061920166 0.2548940181732178
CurrentTrain: epoch  8, batch     3 | loss: 12.6765394Losses:  6.872795104980469 2.7178726196289062 0.22526443004608154
CurrentTrain: epoch  8, batch     4 | loss: 6.8727951Losses:  6.560668468475342 2.3299102783203125 0.22923603653907776
CurrentTrain: epoch  8, batch     5 | loss: 6.5606685Losses:  8.906966209411621 4.578388214111328 0.2563822269439697
CurrentTrain: epoch  8, batch     6 | loss: 8.9069662Losses:  8.692968368530273 4.468491554260254 0.24567961692810059
CurrentTrain: epoch  8, batch     7 | loss: 8.6929684Losses:  9.745078086853027 5.562326908111572 0.2528904378414154
CurrentTrain: epoch  8, batch     8 | loss: 9.7450781Losses:  5.82488489151001 1.6027833223342896 0.2092975676059723
CurrentTrain: epoch  8, batch     9 | loss: 5.8248849Losses:  6.74609375 2.5281758308410645 0.2257116436958313
CurrentTrain: epoch  8, batch    10 | loss: 6.7460938Losses:  7.657203197479248 3.3722660541534424 0.24592158198356628
CurrentTrain: epoch  8, batch    11 | loss: 7.6572032Losses:  7.106997013092041 2.9362525939941406 0.2252049297094345
CurrentTrain: epoch  8, batch    12 | loss: 7.1069970Losses:  7.068237781524658 2.834585666656494 0.23305638134479523
CurrentTrain: epoch  8, batch    13 | loss: 7.0682378Losses:  8.926507949829102 4.651351451873779 0.25524163246154785
CurrentTrain: epoch  8, batch    14 | loss: 8.9265079Losses:  7.623582363128662 3.5002052783966064 0.14585618674755096
CurrentTrain: epoch  8, batch    15 | loss: 7.6235824Losses:  7.292763710021973 3.0681052207946777 0.22288212180137634
CurrentTrain: epoch  8, batch    16 | loss: 7.2927637Losses:  7.1637091636657715 2.939887285232544 0.23700305819511414
CurrentTrain: epoch  8, batch    17 | loss: 7.1637092Losses:  7.308953285217285 3.0605525970458984 0.2371845543384552
CurrentTrain: epoch  8, batch    18 | loss: 7.3089533Losses:  6.167765140533447 1.9930341243743896 0.2243761122226715
CurrentTrain: epoch  8, batch    19 | loss: 6.1677651Losses:  6.558032035827637 2.306007146835327 0.23017792403697968
CurrentTrain: epoch  8, batch    20 | loss: 6.5580320Losses:  8.554317474365234 4.351366996765137 0.23093563318252563
CurrentTrain: epoch  8, batch    21 | loss: 8.5543175Losses:  7.422765731811523 3.178628444671631 0.23802700638771057
CurrentTrain: epoch  8, batch    22 | loss: 7.4227657Losses:  7.416873455047607 3.179593801498413 0.22932776808738708
CurrentTrain: epoch  8, batch    23 | loss: 7.4168735Losses:  5.8100738525390625 1.5857130289077759 0.21398130059242249
CurrentTrain: epoch  8, batch    24 | loss: 5.8100739Losses:  8.39752197265625 4.108948230743408 0.23916828632354736
CurrentTrain: epoch  8, batch    25 | loss: 8.3975220Losses:  5.8180832862854 1.606290340423584 0.2143801748752594
CurrentTrain: epoch  8, batch    26 | loss: 5.8180833Losses:  6.746452808380127 2.523402690887451 0.23431384563446045
CurrentTrain: epoch  8, batch    27 | loss: 6.7464528Losses:  7.951446533203125 3.7658371925354004 0.21751967072486877
CurrentTrain: epoch  8, batch    28 | loss: 7.9514465Losses:  7.117308616638184 2.9297986030578613 0.222175732254982
CurrentTrain: epoch  8, batch    29 | loss: 7.1173086Losses:  7.440965175628662 3.1673974990844727 0.24572807550430298
CurrentTrain: epoch  8, batch    30 | loss: 7.4409652Losses:  6.847427845001221 2.6517128944396973 0.2250962108373642
CurrentTrain: epoch  8, batch    31 | loss: 6.8474278Losses:  7.903546333312988 3.673051118850708 0.2513291835784912
CurrentTrain: epoch  8, batch    32 | loss: 7.9035463Losses:  8.915139198303223 4.699440956115723 0.23454684019088745
CurrentTrain: epoch  8, batch    33 | loss: 8.9151392Losses:  5.781708717346191 1.5629767179489136 0.21589401364326477
CurrentTrain: epoch  8, batch    34 | loss: 5.7817087Losses:  6.942418098449707 2.683593988418579 0.22358226776123047
CurrentTrain: epoch  8, batch    35 | loss: 6.9424181Losses:  6.389772891998291 2.1707587242126465 0.2281697690486908
CurrentTrain: epoch  8, batch    36 | loss: 6.3897729Losses:  7.7697272300720215 3.532188892364502 0.2368852198123932
CurrentTrain: epoch  8, batch    37 | loss: 7.7697272Losses:  7.326817989349365 3.1353559494018555 0.2266169935464859
CurrentTrain: epoch  8, batch    38 | loss: 7.3268180Losses:  5.81522274017334 1.6021119356155396 0.20968809723854065
CurrentTrain: epoch  8, batch    39 | loss: 5.8152227Losses:  6.346644401550293 2.1341042518615723 0.2238263040781021
CurrentTrain: epoch  8, batch    40 | loss: 6.3466444Losses:  6.519136428833008 2.3455824851989746 0.23330581188201904
CurrentTrain: epoch  8, batch    41 | loss: 6.5191364Losses:  10.068682670593262 5.8426923751831055 0.24364779889583588
CurrentTrain: epoch  8, batch    42 | loss: 10.0686827Losses:  8.434393882751465 4.2160210609436035 0.23664149641990662
CurrentTrain: epoch  8, batch    43 | loss: 8.4343939Losses:  7.833380222320557 3.6451995372772217 0.24625156819820404
CurrentTrain: epoch  8, batch    44 | loss: 7.8333802Losses:  7.300853729248047 3.055619239807129 0.23185262084007263
CurrentTrain: epoch  8, batch    45 | loss: 7.3008537Losses:  6.542778968811035 2.3147194385528564 0.2297385185956955
CurrentTrain: epoch  8, batch    46 | loss: 6.5427790Losses:  8.867456436157227 4.611840724945068 0.24686944484710693
CurrentTrain: epoch  8, batch    47 | loss: 8.8674564Losses:  6.892895221710205 2.703671932220459 0.23175083100795746
CurrentTrain: epoch  8, batch    48 | loss: 6.8928952Losses:  7.158373832702637 2.9649131298065186 0.22432799637317657
CurrentTrain: epoch  8, batch    49 | loss: 7.1583738Losses:  8.549056053161621 4.366055488586426 0.24194681644439697
CurrentTrain: epoch  8, batch    50 | loss: 8.5490561Losses:  5.747716903686523 1.579301118850708 0.21032872796058655
CurrentTrain: epoch  8, batch    51 | loss: 5.7477169Losses:  7.8899126052856445 3.740713119506836 0.1515485644340515
CurrentTrain: epoch  8, batch    52 | loss: 7.8899126Losses:  5.785444736480713 1.6093168258666992 0.21197162568569183
CurrentTrain: epoch  8, batch    53 | loss: 5.7854447Losses:  6.869358062744141 2.7420969009399414 0.14626702666282654
CurrentTrain: epoch  8, batch    54 | loss: 6.8693581Losses:  8.832330703735352 4.6191205978393555 0.23779883980751038
CurrentTrain: epoch  8, batch    55 | loss: 8.8323307Losses:  7.283395767211914 3.090822458267212 0.22607235610485077
CurrentTrain: epoch  8, batch    56 | loss: 7.2833958Losses:  7.358944416046143 3.123136043548584 0.23645351827144623
CurrentTrain: epoch  8, batch    57 | loss: 7.3589444Losses:  10.33949089050293 6.055685520172119 0.2440931648015976
CurrentTrain: epoch  8, batch    58 | loss: 10.3394909Losses:  9.152743339538574 4.884077072143555 0.2460644394159317
CurrentTrain: epoch  8, batch    59 | loss: 9.1527433Losses:  6.883569717407227 2.6958742141723633 0.22298642992973328
CurrentTrain: epoch  8, batch    60 | loss: 6.8835697Losses:  7.137990951538086 2.933310031890869 0.2356444001197815
CurrentTrain: epoch  8, batch    61 | loss: 7.1379910Losses:  4.966562271118164 0.7631698846817017 0.1885240077972412
CurrentTrain: epoch  8, batch    62 | loss: 4.9665623Losses:  6.253649711608887 2.096036434173584 0.22368201613426208
CurrentTrain: epoch  9, batch     0 | loss: 6.2536497Losses:  7.1604180335998535 2.9313859939575195 0.23787279427051544
CurrentTrain: epoch  9, batch     1 | loss: 7.1604180Losses:  6.340791702270508 2.104295492172241 0.21794214844703674
CurrentTrain: epoch  9, batch     2 | loss: 6.3407917Losses:  7.313943862915039 3.083648920059204 0.23012426495552063
CurrentTrain: epoch  9, batch     3 | loss: 7.3139439Losses:  6.92772912979126 2.6968932151794434 0.23685869574546814
CurrentTrain: epoch  9, batch     4 | loss: 6.9277291Losses:  7.2705979347229 3.0574893951416016 0.2324371337890625
CurrentTrain: epoch  9, batch     5 | loss: 7.2705979Losses:  7.087527751922607 2.8572864532470703 0.22444678843021393
CurrentTrain: epoch  9, batch     6 | loss: 7.0875278Losses:  6.395225524902344 2.1864285469055176 0.21945488452911377
CurrentTrain: epoch  9, batch     7 | loss: 6.3952255Losses:  7.859317302703857 3.6574783325195312 0.2403194010257721
CurrentTrain: epoch  9, batch     8 | loss: 7.8593173Losses:  5.768304347991943 1.5722084045410156 0.2081056386232376
CurrentTrain: epoch  9, batch     9 | loss: 5.7683043Losses:  6.339571475982666 2.1604223251342773 0.21922166645526886
CurrentTrain: epoch  9, batch    10 | loss: 6.3395715Losses:  6.68020486831665 2.4900646209716797 0.2269999235868454
CurrentTrain: epoch  9, batch    11 | loss: 6.6802049Losses:  6.744448661804199 2.5471343994140625 0.217146098613739
CurrentTrain: epoch  9, batch    12 | loss: 6.7444487Losses:  8.077260971069336 3.8028321266174316 0.24639469385147095
CurrentTrain: epoch  9, batch    13 | loss: 8.0772610Losses:  6.18796443939209 1.9968552589416504 0.2226761281490326
CurrentTrain: epoch  9, batch    14 | loss: 6.1879644Losses:  6.285680770874023 2.1057512760162354 0.220270037651062
CurrentTrain: epoch  9, batch    15 | loss: 6.2856808Losses:  7.877112865447998 3.6693310737609863 0.24023663997650146
CurrentTrain: epoch  9, batch    16 | loss: 7.8771129Losses:  8.201044082641602 3.9948368072509766 0.23474730551242828
CurrentTrain: epoch  9, batch    17 | loss: 8.2010441Losses:  8.03797721862793 3.8369829654693604 0.2248157113790512
CurrentTrain: epoch  9, batch    18 | loss: 8.0379772Losses:  7.094995498657227 2.9000377655029297 0.22389096021652222
CurrentTrain: epoch  9, batch    19 | loss: 7.0949955Losses:  7.0555315017700195 2.7888412475585938 0.2364140897989273
CurrentTrain: epoch  9, batch    20 | loss: 7.0555315Losses:  6.61066198348999 2.3870272636413574 0.2369455099105835
CurrentTrain: epoch  9, batch    21 | loss: 6.6106620Losses:  6.650398254394531 2.456624984741211 0.22330495715141296
CurrentTrain: epoch  9, batch    22 | loss: 6.6503983Losses:  7.231313228607178 2.986431837081909 0.23308885097503662
CurrentTrain: epoch  9, batch    23 | loss: 7.2313132Losses:  8.403486251831055 4.1736578941345215 0.22655141353607178
CurrentTrain: epoch  9, batch    24 | loss: 8.4034863Losses:  6.741456031799316 2.5385866165161133 0.22166821360588074
CurrentTrain: epoch  9, batch    25 | loss: 6.7414560Losses:  6.287444591522217 2.1017119884490967 0.2212902307510376
CurrentTrain: epoch  9, batch    26 | loss: 6.2874446Losses:  9.964402198791504 5.7802414894104 0.23397400975227356
CurrentTrain: epoch  9, batch    27 | loss: 9.9644022Losses:  7.374034404754639 3.2235350608825684 0.227920264005661
CurrentTrain: epoch  9, batch    28 | loss: 7.3740344Losses:  7.28570556640625 3.048922538757324 0.2284279763698578
CurrentTrain: epoch  9, batch    29 | loss: 7.2857056Losses:  11.016105651855469 6.769783020019531 0.24387970566749573
CurrentTrain: epoch  9, batch    30 | loss: 11.0161057Losses:  7.121209621429443 2.9478306770324707 0.21973735094070435
CurrentTrain: epoch  9, batch    31 | loss: 7.1212096Losses:  7.324506759643555 3.1548686027526855 0.23555892705917358
CurrentTrain: epoch  9, batch    32 | loss: 7.3245068Losses:  9.062688827514648 4.8970489501953125 0.2401200234889984
CurrentTrain: epoch  9, batch    33 | loss: 9.0626888Losses:  7.219784259796143 3.0503225326538086 0.2236742526292801
CurrentTrain: epoch  9, batch    34 | loss: 7.2197843Losses:  7.167759418487549 2.957484722137451 0.22500449419021606
CurrentTrain: epoch  9, batch    35 | loss: 7.1677594Losses:  6.3938117027282715 2.195335865020752 0.21853190660476685
CurrentTrain: epoch  9, batch    36 | loss: 6.3938117Losses:  8.736152648925781 4.600102424621582 0.15511982142925262
CurrentTrain: epoch  9, batch    37 | loss: 8.7361526Losses:  7.087092876434326 2.905815601348877 0.22786390781402588
CurrentTrain: epoch  9, batch    38 | loss: 7.0870929Losses:  5.967794895172119 1.7726433277130127 0.22121402621269226
CurrentTrain: epoch  9, batch    39 | loss: 5.9677949Losses:  8.323968887329102 4.136159896850586 0.22011522948741913
CurrentTrain: epoch  9, batch    40 | loss: 8.3239689Losses:  6.524689197540283 2.3233110904693604 0.21851982176303864
CurrentTrain: epoch  9, batch    41 | loss: 6.5246892Losses:  6.332653999328613 2.144747257232666 0.22507476806640625
CurrentTrain: epoch  9, batch    42 | loss: 6.3326540Losses:  8.759986877441406 4.527034759521484 0.24836193025112152
CurrentTrain: epoch  9, batch    43 | loss: 8.7599869Losses:  5.767568111419678 1.5980618000030518 0.20751523971557617
CurrentTrain: epoch  9, batch    44 | loss: 5.7675681Losses:  6.513286590576172 2.3848278522491455 0.14176476001739502
CurrentTrain: epoch  9, batch    45 | loss: 6.5132866Losses:  6.519715785980225 2.3416810035705566 0.21290452778339386
CurrentTrain: epoch  9, batch    46 | loss: 6.5197158Losses:  9.704970359802246 5.558681488037109 0.16966521739959717
CurrentTrain: epoch  9, batch    47 | loss: 9.7049704Losses:  7.1080145835876465 2.870730400085449 0.22170069813728333
CurrentTrain: epoch  9, batch    48 | loss: 7.1080146Losses:  7.315772533416748 3.133411407470703 0.23389561474323273
CurrentTrain: epoch  9, batch    49 | loss: 7.3157725Losses:  7.1092023849487305 2.934501886367798 0.21873077750205994
CurrentTrain: epoch  9, batch    50 | loss: 7.1092024Losses:  6.9974212646484375 2.897397756576538 0.24178045988082886
CurrentTrain: epoch  9, batch    51 | loss: 6.9974213Losses:  7.930455207824707 3.6931424140930176 0.24773384630680084
CurrentTrain: epoch  9, batch    52 | loss: 7.9304552Losses:  6.916027069091797 2.6789541244506836 0.23174870014190674
CurrentTrain: epoch  9, batch    53 | loss: 6.9160271Losses:  6.274236679077148 2.1270666122436523 0.21989792585372925
CurrentTrain: epoch  9, batch    54 | loss: 6.2742367Losses:  6.324430465698242 2.119948387145996 0.22612789273262024
CurrentTrain: epoch  9, batch    55 | loss: 6.3244305Losses:  10.427563667297363 6.225437164306641 0.2425193190574646
CurrentTrain: epoch  9, batch    56 | loss: 10.4275637Losses:  11.438143730163574 7.338066101074219 0.16209714114665985
CurrentTrain: epoch  9, batch    57 | loss: 11.4381437Losses:  8.281301498413086 4.116483211517334 0.25168436765670776
CurrentTrain: epoch  9, batch    58 | loss: 8.2813015Losses:  8.04299259185791 3.8616762161254883 0.2509961724281311
CurrentTrain: epoch  9, batch    59 | loss: 8.0429926Losses:  7.1929426193237305 3.0590195655822754 0.23371939361095428
CurrentTrain: epoch  9, batch    60 | loss: 7.1929426Losses:  6.50131368637085 2.3355581760406494 0.20584939420223236
CurrentTrain: epoch  9, batch    61 | loss: 6.5013137Losses:  4.913437843322754 0.7137844562530518 0.25738805532455444
CurrentTrain: epoch  9, batch    62 | loss: 4.9134378
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: born in copenhagen , as a young man bang became interested in surrealism as practised by giorgio de chirico , max ernst and yves tanguy .
Head Entity: yves tanguy
Tail Entity: surrealism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the artist was heavily influenced by the cubist movement, which was pioneered by pablo picasso and georges braque.  
Head Entity: pablo picasso  
Tail Entity: cubist movement  

Relation: movement  
Context: the philosopher's ideas were central to the existentialist movement, which gained prominence in the mid-20th century through the works of thinkers like jean-paul sartre and simone de beauvoir.  
Head Entity: jean-paul sartre  
Tail Entity: existentialist movement  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: it linked bundoran and ballyshannon on the atlantic coast of donegal with the londonderry and enniskillen railway ( l&er ; ) at in fermanagh .
Head Entity: ballyshannon
Tail Entity: donegal
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of bristol is known for its vibrant culture and is situated in the southwest region of england.  
Head Entity: bristol  
Tail Entity: england  

Relation: located in the administrative territorial entity  
Context: the famous statue of liberty stands proudly on liberty island, which is part of new york harbor.  
Head Entity: liberty island  
Tail Entity: new york
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: rage against the machine bandmates , tom morello , brad wilk , and de la rocha , as well as audioslave bandmate , chris cornell , came to support commerford .
Head Entity: chris cornell
Tail Entity: audioslave
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: The famous actor and musician, Jared Leto, is not only the lead vocalist of Thirty Seconds to Mars but also a member of the Academy Award-winning film community.  
Head Entity: Jared Leto  
Tail Entity: Thirty Seconds to Mars  

Relation: member of  
Context: During the annual conference, several prominent figures from the tech industry, including Sundar Pichai, who is a member of the Google team, shared their insights on innovation.  
Head Entity: Sundar Pichai  
Tail Entity: Google  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County, and is a vital link for traffic crossing over the water.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a rail link between the UK and mainland Europe.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the municipality contains the lower reaches of the maracanã river , which is joined by the caripi river before widening into maracanã bay and emptying into the atlantic ocean beside maiandeua island .
Head Entity: maracanã river
Tail Entity: caripi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Amazon River, one of the largest rivers in the world, receives numerous tributaries, including the Madeira River, which significantly contributes to its flow.  
Head Entity: Amazon River  
Tail Entity: Madeira River  

Relation: tributary  
Context: The Mississippi River is fed by several smaller rivers, with the Missouri River being one of its major tributaries that enhances its water volume.  
Head Entity: Mississippi River  
Tail Entity: Missouri River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: after a review was conducted by former first minister of scotland henry mcleish , the sfa board structure was streamlined considerably in 2011 .
Head Entity: henry mcleish
Tail Entity: first minister of scotland
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: In 2020, the renowned scientist Dr. Jane Smith was appointed as the Chief Research Officer at the National Institute of Health.  
Head Entity: Dr. Jane Smith  
Tail Entity: Chief Research Officer  

Relation: position held  
Context: After serving two terms, Mayor John Doe announced his retirement from the position of Mayor of Springfield in 2021.  
Head Entity: Mayor John Doe  
Tail Entity: Mayor of Springfield  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: it was developed by syn sophia , who also developed the other games in the " style savvy " series .
Head Entity: style savvy
Tail Entity: syn sophia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: the groundbreaking software was created by tech innovations inc., a leader in the field of artificial intelligence.  
Head Entity: artificial intelligence  
Tail Entity: tech innovations inc.  

Relation: developer  
Context: the popular mobile app was designed by creative minds ltd., which specializes in user-friendly applications.  
Head Entity: mobile app  
Tail Entity: creative minds ltd.  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: in 2012 , anderson lent his keyboard talent to the seattle supergroup walking papers .
Head Entity: walking papers
Tail Entity: seattle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: The famous band was established in London in the early 1960s, quickly gaining popularity across the globe.  
Head Entity: The famous band  
Tail Entity: London  

Relation: location of formation  
Context: The tech startup was founded in Silicon Valley, where many innovative companies thrive.  
Head Entity: The tech startup  
Tail Entity: Silicon Valley  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: " shining time station " won a number of awards and significantly increased the popularity of the " thomas " media franchise in the united states .
Head Entity: shining time station
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish "sushi" is traditionally associated with Japan and has gained immense popularity worldwide.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic brand "Guinness" is known for its rich stout beer, which originated in Ireland and is now enjoyed globally.  
Head Entity: Guinness  
Tail Entity: Ireland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 95.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 95.14%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 95.04%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 95.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 95.36%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.77%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.66%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 96.04%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 96.13%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.31%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.20%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 95.17%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 95.38%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 95.31%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 95.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 95.14%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 95.09%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 95.04%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 95.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 95.36%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.51%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.64%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.77%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.71%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 95.66%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.83%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 96.04%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 96.13%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.22%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.31%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 96.14%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.20%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 95.65%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.61%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
cur_acc:  ['0.9494']
his_acc:  ['0.9494']
Clustering into  9  clusters
Clusters:  [2 0 8 1 0 0 5 2 4 4 0 1 7 3 1 6 2 4 2 4]
Losses:  14.13754940032959 3.859431743621826 0.8201910853385925
CurrentTrain: epoch  0, batch     0 | loss: 14.1375494Losses:  11.07191276550293 3.105377674102783 0.5964453220367432
CurrentTrain: epoch  0, batch     1 | loss: 11.0719128Losses:  12.993873596191406 4.319451332092285 0.6762163043022156
CurrentTrain: epoch  0, batch     2 | loss: 12.9938736Losses:  9.07641315460205 -0.0 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 9.0764132Losses:  11.053825378417969 2.9743571281433105 0.5513365864753723
CurrentTrain: epoch  1, batch     0 | loss: 11.0538254Losses:  14.652321815490723 5.717994689941406 0.7879055738449097
CurrentTrain: epoch  1, batch     1 | loss: 14.6523218Losses:  11.540254592895508 3.416297197341919 0.5956131219863892
CurrentTrain: epoch  1, batch     2 | loss: 11.5402546Losses:  6.578003406524658 -0.0 0.11298239231109619
CurrentTrain: epoch  1, batch     3 | loss: 6.5780034Losses:  12.19924259185791 4.544805526733398 0.6528558731079102
CurrentTrain: epoch  2, batch     0 | loss: 12.1992426Losses:  11.242720603942871 3.665724754333496 0.7099527716636658
CurrentTrain: epoch  2, batch     1 | loss: 11.2427206Losses:  11.132172584533691 4.0380144119262695 0.6483849287033081
CurrentTrain: epoch  2, batch     2 | loss: 11.1321726Losses:  5.217780590057373 -0.0 0.08161094784736633
CurrentTrain: epoch  2, batch     3 | loss: 5.2177806Losses:  10.655778884887695 3.8419206142425537 0.6552950143814087
CurrentTrain: epoch  3, batch     0 | loss: 10.6557789Losses:  9.833146095275879 3.149603843688965 0.6367425322532654
CurrentTrain: epoch  3, batch     1 | loss: 9.8331461Losses:  10.121854782104492 3.058626413345337 0.7155414819717407
CurrentTrain: epoch  3, batch     2 | loss: 10.1218548Losses:  6.523279190063477 -0.0 0.16109395027160645
CurrentTrain: epoch  3, batch     3 | loss: 6.5232792Losses:  9.358942985534668 3.115345001220703 0.6672529578208923
CurrentTrain: epoch  4, batch     0 | loss: 9.3589430Losses:  10.636680603027344 4.55715274810791 0.6543998718261719
CurrentTrain: epoch  4, batch     1 | loss: 10.6366806Losses:  8.471805572509766 2.5848302841186523 0.6660035252571106
CurrentTrain: epoch  4, batch     2 | loss: 8.4718056Losses:  3.121091365814209 -0.0 0.13304227590560913
CurrentTrain: epoch  4, batch     3 | loss: 3.1210914Losses:  8.314689636230469 2.9689154624938965 0.6221084594726562
CurrentTrain: epoch  5, batch     0 | loss: 8.3146896Losses:  10.054059028625488 4.640060901641846 0.5799210071563721
CurrentTrain: epoch  5, batch     1 | loss: 10.0540590Losses:  8.589771270751953 2.6041433811187744 0.6651071906089783
CurrentTrain: epoch  5, batch     2 | loss: 8.5897713Losses:  5.901163101196289 -0.0 0.09591159224510193
CurrentTrain: epoch  5, batch     3 | loss: 5.9011631Losses:  7.560950756072998 2.400404453277588 0.6268092393875122
CurrentTrain: epoch  6, batch     0 | loss: 7.5609508Losses:  8.284891128540039 3.440548896789551 0.5592360496520996
CurrentTrain: epoch  6, batch     1 | loss: 8.2848911Losses:  8.902013778686523 3.526714324951172 0.5311394333839417
CurrentTrain: epoch  6, batch     2 | loss: 8.9020138Losses:  3.632117509841919 -0.0 0.10866793990135193
CurrentTrain: epoch  6, batch     3 | loss: 3.6321175Losses:  7.686317443847656 3.275547504425049 0.6097204685211182
CurrentTrain: epoch  7, batch     0 | loss: 7.6863174Losses:  6.459837436676025 2.095808744430542 0.6349795460700989
CurrentTrain: epoch  7, batch     1 | loss: 6.4598374Losses:  9.501108169555664 4.0902228355407715 0.5135858058929443
CurrentTrain: epoch  7, batch     2 | loss: 9.5011082Losses:  2.185277223587036 -0.0 0.142314612865448
CurrentTrain: epoch  7, batch     3 | loss: 2.1852772Losses:  6.679723262786865 2.297306776046753 0.6222816705703735
CurrentTrain: epoch  8, batch     0 | loss: 6.6797233Losses:  6.994800090789795 2.351033926010132 0.5066763162612915
CurrentTrain: epoch  8, batch     1 | loss: 6.9948001Losses:  6.315483570098877 2.141296863555908 0.5306826829910278
CurrentTrain: epoch  8, batch     2 | loss: 6.3154836Losses:  3.0442333221435547 -0.0 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 3.0442333Losses:  6.22601842880249 2.5470104217529297 0.5935138463973999
CurrentTrain: epoch  9, batch     0 | loss: 6.2260184Losses:  6.689718246459961 2.197035551071167 0.6343173980712891
CurrentTrain: epoch  9, batch     1 | loss: 6.6897182Losses:  7.107215404510498 2.6132211685180664 0.6016966104507446
CurrentTrain: epoch  9, batch     2 | loss: 7.1072154Losses:  3.313525915145874 -0.0 0.12007975578308105
CurrentTrain: epoch  9, batch     3 | loss: 3.3135259
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from all over the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and organizations come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy, showcasing the best of international football.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: " runaway reptar " was written by ali marie matheson and jon cooksey and directed by john holmquist and jim duffy .
Head Entity: runaway reptar
Tail Entity: john holmquist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: "Inception" was directed by Christopher Nolan and features a complex narrative structure that keeps viewers on the edge of their seats.  
Head Entity: Inception  
Tail Entity: Christopher Nolan  

Relation: director  
Context: The acclaimed stage play "Hamilton" was directed by Thomas Kail, who brought Lin-Manuel Miranda's vision to life on Broadway.  
Head Entity: Hamilton  
Tail Entity: Thomas Kail  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular smartphone brand OnePlus was acquired by BBK Electronics, which also owns other brands like Oppo and Vivo.  
Head Entity: OnePlus  
Tail Entity: BBK Electronics  

Relation: owned by  
Context: The famous luxury fashion label Gucci is a subsidiary of the Kering Group, which manages several other high-end brands.  
Head Entity: Gucci  
Tail Entity: Kering Group  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: remains of taxa that can be confidently assigned to istiodactylidae have been found in the uk and china , in rocks dating from the early cretaceous period ( barremian to aptian stage ) .
Head Entity: barremian
Tail Entity: early cretaceous
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The Great Wall of China is a series of fortifications made of various materials, and it is part of the cultural heritage of the country.  
Head Entity: Great Wall of China  
Tail Entity: cultural heritage  

Relation: part of  
Context: The human heart is a vital organ that is part of the circulatory system, responsible for pumping blood throughout the body.  
Head Entity: human heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and now it serves as a gallery showcasing his works, attracting visitors from all over the world.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now thrive.  
Head Entity: old factory  
Tail Entity: startups
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, whose innovative approach transformed the landscape of modern architecture.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: The Guggenheim Museum in Bilbao, a masterpiece of contemporary architecture, was created by the renowned architect Frank Gehry, showcasing his unique design philosophy.  
Head Entity: Guggenheim Museum  
Tail Entity: Frank Gehry  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, sarah decided to relocate to california for a fresh start.  
Head Entity: sarah  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida as a teenager.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: he also took part in recordings of several gilbert and sullivan operattas as well as edward german 's " merrie england " under the direction of joe batten .
Head Entity: merrie england
Tail Entity: edward german
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: Ludwig van Beethoven is widely regarded as one of the greatest composers in the history of Western music, known for his symphonies and sonatas.  
Head Entity: Ludwig van Beethoven  
Tail Entity: symphonies  

Relation: composer  
Context: The famous opera "Carmen" was composed by Georges Bizet, who is celebrated for his contributions to the world of classical music.  
Head Entity: Carmen  
Tail Entity: Georges Bizet  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: the postseason tournament concluded with the san francisco 49ers defeating the cincinnati bengals in super bowl xvi , 26–21 , on january 24 , 1982 , at the pontiac silverdome in pontiac , michigan .
Head Entity: super bowl xvi
Tail Entity: pontiac silverdome
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: the annual music festival took place in the vibrant city of new orleans, attracting thousands of visitors from around the world.  
Head Entity: music festival  
Tail Entity: new orleans  

Relation: location  
Context: the historic battle was fought at gettysburg, a site that has become synonymous with the civil war in the united states.  
Head Entity: battle  
Tail Entity: gettysburg  
Losses:  6.768137454986572 0.8218667507171631 0.7678782343864441
MemoryTrain:  epoch  0, batch     0 | loss: 6.7681375Losses:  8.350272178649902 1.4596881866455078 0.6247502565383911
MemoryTrain:  epoch  0, batch     1 | loss: 8.3502722Losses:  7.147816181182861 1.5095384120941162 0.45752769708633423
MemoryTrain:  epoch  0, batch     2 | loss: 7.1478162Losses:  6.398671627044678 0.6780511736869812 0.6117796897888184
MemoryTrain:  epoch  0, batch     3 | loss: 6.3986716Losses:  7.629860877990723 1.4563953876495361 0.46101659536361694
MemoryTrain:  epoch  1, batch     0 | loss: 7.6298609Losses:  6.60139274597168 0.9406321048736572 0.7305289506912231
MemoryTrain:  epoch  1, batch     1 | loss: 6.6013927Losses:  5.677361488342285 1.1059706211090088 0.5126230716705322
MemoryTrain:  epoch  1, batch     2 | loss: 5.6773615Losses:  4.962364673614502 0.6244917511940002 0.6703261733055115
MemoryTrain:  epoch  1, batch     3 | loss: 4.9623647Losses:  5.733967304229736 1.4516189098358154 0.5857557654380798
MemoryTrain:  epoch  2, batch     0 | loss: 5.7339673Losses:  6.0772175788879395 1.4741523265838623 0.6561800837516785
MemoryTrain:  epoch  2, batch     1 | loss: 6.0772176Losses:  5.882685661315918 1.0469890832901 0.5731949210166931
MemoryTrain:  epoch  2, batch     2 | loss: 5.8826857Losses:  4.195656776428223 0.5718400478363037 0.5918024778366089
MemoryTrain:  epoch  2, batch     3 | loss: 4.1956568Losses:  4.578315734863281 0.4719165563583374 0.6604788303375244
MemoryTrain:  epoch  3, batch     0 | loss: 4.5783157Losses:  4.574061393737793 1.11442232131958 0.5812146663665771
MemoryTrain:  epoch  3, batch     1 | loss: 4.5740614Losses:  5.575836181640625 1.332212209701538 0.5826846361160278
MemoryTrain:  epoch  3, batch     2 | loss: 5.5758362Losses:  4.0321526527404785 0.6443781852722168 0.5122371315956116
MemoryTrain:  epoch  3, batch     3 | loss: 4.0321527Losses:  5.628321170806885 1.5117058753967285 0.5494275093078613
MemoryTrain:  epoch  4, batch     0 | loss: 5.6283212Losses:  3.853205680847168 0.6793802380561829 0.6289847493171692
MemoryTrain:  epoch  4, batch     1 | loss: 3.8532057Losses:  4.017388343811035 1.055275321006775 0.4957236647605896
MemoryTrain:  epoch  4, batch     2 | loss: 4.0173883Losses:  3.677602767944336 0.6473861932754517 0.6808372735977173
MemoryTrain:  epoch  4, batch     3 | loss: 3.6776028Losses:  3.5976595878601074 1.0108921527862549 0.5037476420402527
MemoryTrain:  epoch  5, batch     0 | loss: 3.5976596Losses:  3.4457292556762695 0.49287110567092896 0.5666335821151733
MemoryTrain:  epoch  5, batch     1 | loss: 3.4457293Losses:  3.5123353004455566 0.32720327377319336 0.6191481351852417
MemoryTrain:  epoch  5, batch     2 | loss: 3.5123353Losses:  3.4654836654663086 0.32014596462249756 0.5717490911483765
MemoryTrain:  epoch  5, batch     3 | loss: 3.4654837Losses:  4.018194198608398 1.3799777030944824 0.56708163022995
MemoryTrain:  epoch  6, batch     0 | loss: 4.0181942Losses:  3.999776601791382 1.8080559968948364 0.5085219740867615
MemoryTrain:  epoch  6, batch     1 | loss: 3.9997766Losses:  5.397657871246338 1.9518446922302246 0.5767291188240051
MemoryTrain:  epoch  6, batch     2 | loss: 5.3976579Losses:  2.5025532245635986 0.27895066142082214 0.5748656392097473
MemoryTrain:  epoch  6, batch     3 | loss: 2.5025532Losses:  3.133540630340576 0.7742481231689453 0.6580166816711426
MemoryTrain:  epoch  7, batch     0 | loss: 3.1335406Losses:  3.1379194259643555 0.5343875885009766 0.6681212186813354
MemoryTrain:  epoch  7, batch     1 | loss: 3.1379194Losses:  3.9704294204711914 0.76031494140625 0.6530040502548218
MemoryTrain:  epoch  7, batch     2 | loss: 3.9704294Losses:  2.7921204566955566 0.4790056347846985 0.6054648756980896
MemoryTrain:  epoch  7, batch     3 | loss: 2.7921205Losses:  2.989867687225342 0.7069031596183777 0.44249221682548523
MemoryTrain:  epoch  8, batch     0 | loss: 2.9898677Losses:  4.835149765014648 1.8258185386657715 0.6223852038383484
MemoryTrain:  epoch  8, batch     1 | loss: 4.8351498Losses:  2.8634281158447266 0.9095519781112671 0.6691163778305054
MemoryTrain:  epoch  8, batch     2 | loss: 2.8634281Losses:  2.7838034629821777 0.27325767278671265 0.6145755052566528
MemoryTrain:  epoch  8, batch     3 | loss: 2.7838035Losses:  2.460163116455078 0.45700570940971375 0.6250416040420532
MemoryTrain:  epoch  9, batch     0 | loss: 2.4601631Losses:  3.8789029121398926 1.1390047073364258 0.5089437365531921
MemoryTrain:  epoch  9, batch     1 | loss: 3.8789029Losses:  3.5374577045440674 0.6615123748779297 0.6595049500465393
MemoryTrain:  epoch  9, batch     2 | loss: 3.5374577Losses:  2.1271607875823975 -0.0 0.6606525778770447
MemoryTrain:  epoch  9, batch     3 | loss: 2.1271608
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 21.88%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 20.83%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 20.31%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 22.50%   [EVAL] batch:    5 | acc: 12.50%,  total acc: 20.83%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 27.68%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 35.16%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 42.36%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 48.12%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 52.27%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 58.17%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 60.27%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 68.12%   [EVAL] batch:   20 | acc: 37.50%,  total acc: 66.67%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 66.76%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 66.58%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 65.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 62.74%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 60.42%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 58.48%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 56.47%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 55.00%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 53.43%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 53.52%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 54.55%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 55.70%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 56.96%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 57.99%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 58.61%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 59.05%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 59.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 60.62%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 61.28%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 61.76%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 62.35%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 63.07%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 63.47%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 63.32%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 63.03%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 63.02%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 63.27%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 63.62%   [EVAL] batch:   50 | acc: 43.75%,  total acc: 63.24%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 63.58%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 63.56%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 63.66%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 64.06%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 63.71%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 63.47%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 63.03%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 63.33%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 63.22%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 63.31%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 62.70%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 85.23%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 88.24%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.14%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 89.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.90%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.10%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.14%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.57%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 92.76%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.95%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 93.29%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.45%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.60%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.89%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 93.41%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 93.30%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 93.20%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.00%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.11%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 93.14%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.15%   [EVAL] batch:   62 | acc: 62.50%,  total acc: 92.66%   [EVAL] batch:   63 | acc: 25.00%,  total acc: 91.60%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 90.38%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 89.20%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 88.34%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 87.32%   [EVAL] batch:   68 | acc: 37.50%,  total acc: 86.59%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 86.61%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 86.71%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 86.89%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 87.07%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 87.25%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 87.26%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 87.34%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 87.73%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 87.20%   [EVAL] batch:   82 | acc: 56.25%,  total acc: 86.82%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 86.31%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 85.83%   [EVAL] batch:   86 | acc: 37.50%,  total acc: 85.27%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 84.73%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 83.78%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 82.85%   [EVAL] batch:   90 | acc: 6.25%,  total acc: 82.01%   [EVAL] batch:   91 | acc: 6.25%,  total acc: 81.18%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 80.44%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 79.72%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 79.74%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 79.88%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 80.09%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 80.37%   [EVAL] batch:   99 | acc: 68.75%,  total acc: 80.25%   [EVAL] batch:  100 | acc: 81.25%,  total acc: 80.26%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 80.39%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 80.52%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 80.59%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 80.65%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 80.72%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 80.72%   [EVAL] batch:  107 | acc: 75.00%,  total acc: 80.67%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 80.39%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 80.23%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 80.12%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 80.08%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 79.87%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 79.71%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 79.62%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 79.63%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 79.43%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 79.45%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 79.31%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 79.01%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 78.72%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 78.53%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 78.46%   [EVAL] batch:  123 | acc: 68.75%,  total acc: 78.38%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 78.20%   
cur_acc:  ['0.9494', '0.6270']
his_acc:  ['0.9494', '0.7820']
Clustering into  14  clusters
Clusters:  [ 0  3 11  5 12 12  9  0  2  1  3  8 13  7  5 10  4  2  0  2  6  0  0 12
  7  1  0 12  8  8]
Losses:  8.531752586364746 2.6144909858703613 0.6014376878738403
CurrentTrain: epoch  0, batch     0 | loss: 8.5317526Losses:  7.938337326049805 2.312037944793701 0.6410067677497864
CurrentTrain: epoch  0, batch     1 | loss: 7.9383373Losses:  9.024843215942383 2.7891921997070312 0.5865755081176758
CurrentTrain: epoch  0, batch     2 | loss: 9.0248432Losses:  8.849187850952148 -0.0 0.14355438947677612
CurrentTrain: epoch  0, batch     3 | loss: 8.8491879Losses:  10.11699390411377 4.03212833404541 0.4954178035259247
CurrentTrain: epoch  1, batch     0 | loss: 10.1169939Losses:  7.025500297546387 2.4154062271118164 0.5688326358795166
CurrentTrain: epoch  1, batch     1 | loss: 7.0255003Losses:  7.25190544128418 3.078160047531128 0.4832030236721039
CurrentTrain: epoch  1, batch     2 | loss: 7.2519054Losses:  6.028508186340332 -0.0 0.08762840181589127
CurrentTrain: epoch  1, batch     3 | loss: 6.0285082Losses:  6.915886878967285 2.73988676071167 0.42275357246398926
CurrentTrain: epoch  2, batch     0 | loss: 6.9158869Losses:  6.629685401916504 2.217771530151367 0.5582178831100464
CurrentTrain: epoch  2, batch     1 | loss: 6.6296854Losses:  7.356472015380859 3.1216695308685303 0.43390169739723206
CurrentTrain: epoch  2, batch     2 | loss: 7.3564720Losses:  2.5808792114257812 -0.0 0.09560763835906982
CurrentTrain: epoch  2, batch     3 | loss: 2.5808792Losses:  7.788264751434326 4.083676815032959 0.3337932825088501
CurrentTrain: epoch  3, batch     0 | loss: 7.7882648Losses:  7.1213860511779785 2.957150459289551 0.5133259892463684
CurrentTrain: epoch  3, batch     1 | loss: 7.1213861Losses:  5.767142295837402 2.4688494205474854 0.5234194993972778
CurrentTrain: epoch  3, batch     2 | loss: 5.7671423Losses:  2.3184213638305664 -0.0 0.08520263433456421
CurrentTrain: epoch  3, batch     3 | loss: 2.3184214Losses:  5.271978855133057 2.089435577392578 0.5152617692947388
CurrentTrain: epoch  4, batch     0 | loss: 5.2719789Losses:  6.082849979400635 2.5939295291900635 0.5221112370491028
CurrentTrain: epoch  4, batch     1 | loss: 6.0828500Losses:  6.2699761390686035 2.9293737411499023 0.5143004655838013
CurrentTrain: epoch  4, batch     2 | loss: 6.2699761Losses:  2.9307050704956055 -0.0 0.10147939622402191
CurrentTrain: epoch  4, batch     3 | loss: 2.9307051Losses:  5.186203956604004 2.533815383911133 0.5066444873809814
CurrentTrain: epoch  5, batch     0 | loss: 5.1862040Losses:  5.8487749099731445 2.3867881298065186 0.5113599300384521
CurrentTrain: epoch  5, batch     1 | loss: 5.8487749Losses:  5.206919193267822 2.3346521854400635 0.4953688979148865
CurrentTrain: epoch  5, batch     2 | loss: 5.2069192Losses:  3.2379326820373535 -0.0 0.1170443519949913
CurrentTrain: epoch  5, batch     3 | loss: 3.2379327Losses:  4.7076849937438965 1.6219042539596558 0.4994531571865082
CurrentTrain: epoch  6, batch     0 | loss: 4.7076850Losses:  5.462310791015625 2.4761385917663574 0.5096566677093506
CurrentTrain: epoch  6, batch     1 | loss: 5.4623108Losses:  5.577249526977539 2.6647558212280273 0.49461716413497925
CurrentTrain: epoch  6, batch     2 | loss: 5.5772495Losses:  2.4091269969940186 -0.0 0.08576150238513947
CurrentTrain: epoch  6, batch     3 | loss: 2.4091270Losses:  4.992216110229492 2.026545524597168 0.4013434052467346
CurrentTrain: epoch  7, batch     0 | loss: 4.9922161Losses:  5.251097679138184 2.993666648864746 0.36076855659484863
CurrentTrain: epoch  7, batch     1 | loss: 5.2510977Losses:  5.431519985198975 2.57130765914917 0.4801463782787323
CurrentTrain: epoch  7, batch     2 | loss: 5.4315200Losses:  2.0991008281707764 -0.0 0.12997880578041077
CurrentTrain: epoch  7, batch     3 | loss: 2.0991008Losses:  4.768043041229248 2.3800699710845947 0.46341073513031006
CurrentTrain: epoch  8, batch     0 | loss: 4.7680430Losses:  5.504930019378662 3.104217052459717 0.41821035742759705
CurrentTrain: epoch  8, batch     1 | loss: 5.5049300Losses:  7.957403182983398 4.947992324829102 0.38601207733154297
CurrentTrain: epoch  8, batch     2 | loss: 7.9574032Losses:  1.9210805892944336 -0.0 0.08432498574256897
CurrentTrain: epoch  8, batch     3 | loss: 1.9210806Losses:  4.544350624084473 2.072640895843506 0.4760715961456299
CurrentTrain: epoch  9, batch     0 | loss: 4.5443506Losses:  4.2337517738342285 1.6321724653244019 0.4841243624687195
CurrentTrain: epoch  9, batch     1 | loss: 4.2337518Losses:  5.018903732299805 2.5472235679626465 0.3717605471611023
CurrentTrain: epoch  9, batch     2 | loss: 5.0189037Losses:  2.6469409465789795 -0.0 0.09178449958562851
CurrentTrain: epoch  9, batch     3 | loss: 2.6469409
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: " clinton crazies " is a term in american politics of the 1990s and later that refers to intense criticism of united states president bill clinton and his wife hillary clinton .
Head Entity: bill clinton
Tail Entity: hillary clinton
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: "In a lavish ceremony, actor Ryan Reynolds married actress Blake Lively, solidifying their status as one of Hollywood's power couples."  
Head Entity: Ryan Reynolds  
Tail Entity: Blake Lively  

Relation: spouse  
Context: "The former president Barack Obama and Michelle Obama have been married since 1992, often seen as a strong partnership in both personal and public life."  
Head Entity: Barack Obama  
Tail Entity: Michelle Obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: they briefly reformed in 1992 , when sub pop re - released " primal rock therapy " on cd with five additional unreleased tracks .
Head Entity: primal rock therapy
Tail Entity: sub pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The album "Future Nostalgia" was released under the Warner Records label, showcasing Dua Lipa's unique sound and style.  
Head Entity: Future Nostalgia  
Tail Entity: Warner Records  

Relation: record label  
Context: After signing with Columbia Records, the band released their highly anticipated debut album, which quickly climbed the charts.  
Head Entity: debut album  
Tail Entity: Columbia Records  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Venice is famous for its canals and is built on a group of 118 small islands separated by canals and linked by bridges, situated in the Adriatic Sea.  
Head Entity: Venice  
Tail Entity: Adriatic Sea  

Relation: located in or next to body of water  
Context: The town of Key West is known for its beautiful sunsets and is located at the southernmost point of the continental United States, right next to the Gulf of Mexico.  
Head Entity: Key West  
Tail Entity: Gulf of Mexico  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, Zappos, would continue to operate independently.  
Head Entity: Zappos  
Tail Entity: Amazon  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to retain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the speaker proudly mentioned that she was a citizen of canada, highlighting the benefits of her country's healthcare system.  
Head Entity: the speaker  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller that explores complex themes of dreams and reality.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: the band "coldplay" is known for their melodic rock sound, blending elements of alternative and pop music in their songs.  
Head Entity: coldplay  
Tail Entity: melodic rock  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, known for its scenic beauty, empties into the north sea, providing a vital shipping route for trade in northern europe.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: benjamin van leer ( born 9 april 1992 in houten ) is a dutch professional footballer who currently plays as a goalkeeper for ajax in the dutch eredivisie .
Head Entity: benjamin van leer
Tail Entity: goalkeeper
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: lebron james, born on december 30, 1984, is an american professional basketball player who currently plays as a forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: serena williams, born on september 26, 1981, is an american professional tennis player who is known for her powerful serve and currently plays in the women's singles circuit.  
Head Entity: serena williams  
Tail Entity: tennis player  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
Losses:  6.810783386230469 0.7608575224876404 0.7189443707466125
MemoryTrain:  epoch  0, batch     0 | loss: 6.8107834Losses:  5.934826374053955 0.5458142757415771 0.7659781575202942
MemoryTrain:  epoch  0, batch     1 | loss: 5.9348264Losses:  5.712434768676758 0.25649455189704895 0.6671321988105774
MemoryTrain:  epoch  0, batch     2 | loss: 5.7124348Losses:  6.926556587219238 1.1469907760620117 0.7297724485397339
MemoryTrain:  epoch  0, batch     3 | loss: 6.9265566Losses:  6.486608982086182 0.5070996284484863 0.8112049102783203
MemoryTrain:  epoch  0, batch     4 | loss: 6.4866090Losses:  5.3174920082092285 -0.0 0.7781350016593933
MemoryTrain:  epoch  0, batch     5 | loss: 5.3174920Losses:  7.157650470733643 0.8368374109268188 0.7983186841011047
MemoryTrain:  epoch  1, batch     0 | loss: 7.1576505Losses:  6.307736396789551 1.2597919702529907 0.7571340203285217
MemoryTrain:  epoch  1, batch     1 | loss: 6.3077364Losses:  6.781503200531006 1.5294530391693115 0.7445185780525208
MemoryTrain:  epoch  1, batch     2 | loss: 6.7815032Losses:  7.333295822143555 1.2626452445983887 0.584183931350708
MemoryTrain:  epoch  1, batch     3 | loss: 7.3332958Losses:  5.713613510131836 0.5638303756713867 0.7158946990966797
MemoryTrain:  epoch  1, batch     4 | loss: 5.7136135Losses:  4.814074993133545 0.36430221796035767 0.5558186769485474
MemoryTrain:  epoch  1, batch     5 | loss: 4.8140750Losses:  5.7864837646484375 0.5103886723518372 0.9128797054290771
MemoryTrain:  epoch  2, batch     0 | loss: 5.7864838Losses:  4.8540520668029785 0.45765477418899536 0.7484045028686523
MemoryTrain:  epoch  2, batch     1 | loss: 4.8540521Losses:  5.254475116729736 0.7779834270477295 0.6628955602645874
MemoryTrain:  epoch  2, batch     2 | loss: 5.2544751Losses:  5.502486705780029 0.9375576376914978 0.8367851376533508
MemoryTrain:  epoch  2, batch     3 | loss: 5.5024867Losses:  5.153802871704102 1.3236690759658813 0.9244382381439209
MemoryTrain:  epoch  2, batch     4 | loss: 5.1538029Losses:  5.9744133949279785 0.5839061141014099 0.31590837240219116
MemoryTrain:  epoch  2, batch     5 | loss: 5.9744134Losses:  5.764349460601807 0.9899730682373047 0.7234017252922058
MemoryTrain:  epoch  3, batch     0 | loss: 5.7643495Losses:  4.337718963623047 0.7716777324676514 0.7706832885742188
MemoryTrain:  epoch  3, batch     1 | loss: 4.3377190Losses:  4.422996520996094 0.7192557454109192 0.6829174757003784
MemoryTrain:  epoch  3, batch     2 | loss: 4.4229965Losses:  5.023005485534668 0.26281464099884033 0.6399076581001282
MemoryTrain:  epoch  3, batch     3 | loss: 5.0230055Losses:  4.139376640319824 0.7996088266372681 0.8084808588027954
MemoryTrain:  epoch  3, batch     4 | loss: 4.1393766Losses:  3.7840287685394287 0.2782488465309143 0.5628563761711121
MemoryTrain:  epoch  3, batch     5 | loss: 3.7840288Losses:  5.244122505187988 1.0849559307098389 0.7110729217529297
MemoryTrain:  epoch  4, batch     0 | loss: 5.2441225Losses:  4.429068088531494 0.7678796052932739 0.8549370765686035
MemoryTrain:  epoch  4, batch     1 | loss: 4.4290681Losses:  4.164689064025879 1.142167568206787 0.5587508082389832
MemoryTrain:  epoch  4, batch     2 | loss: 4.1646891Losses:  3.8742494583129883 0.28413110971450806 0.797661304473877
MemoryTrain:  epoch  4, batch     3 | loss: 3.8742495Losses:  4.2055439949035645 0.6634645462036133 0.8960277438163757
MemoryTrain:  epoch  4, batch     4 | loss: 4.2055440Losses:  3.3952741622924805 0.30575212836265564 0.3236347436904907
MemoryTrain:  epoch  4, batch     5 | loss: 3.3952742Losses:  3.8755831718444824 0.9837251305580139 0.8073940277099609
MemoryTrain:  epoch  5, batch     0 | loss: 3.8755832Losses:  4.238181114196777 0.5330776572227478 0.8152626156806946
MemoryTrain:  epoch  5, batch     1 | loss: 4.2381811Losses:  3.50935697555542 0.7741140723228455 0.8541672825813293
MemoryTrain:  epoch  5, batch     2 | loss: 3.5093570Losses:  4.5985188484191895 1.505772590637207 0.6336174011230469
MemoryTrain:  epoch  5, batch     3 | loss: 4.5985188Losses:  4.3668928146362305 0.2900121212005615 0.8570061922073364
MemoryTrain:  epoch  5, batch     4 | loss: 4.3668928Losses:  3.9246904850006104 0.23985610902309418 0.6199193000793457
MemoryTrain:  epoch  5, batch     5 | loss: 3.9246905Losses:  3.8027303218841553 0.8843111991882324 0.9180685877799988
MemoryTrain:  epoch  6, batch     0 | loss: 3.8027303Losses:  4.120632648468018 0.9857423901557922 0.7780873775482178
MemoryTrain:  epoch  6, batch     1 | loss: 4.1206326Losses:  3.34714937210083 0.2932630479335785 0.8102397322654724
MemoryTrain:  epoch  6, batch     2 | loss: 3.3471494Losses:  3.498533248901367 0.5553932189941406 0.7739647626876831
MemoryTrain:  epoch  6, batch     3 | loss: 3.4985332Losses:  4.121738910675049 0.26384907960891724 0.8693461418151855
MemoryTrain:  epoch  6, batch     4 | loss: 4.1217389Losses:  3.207287073135376 0.23043863475322723 0.4830739498138428
MemoryTrain:  epoch  6, batch     5 | loss: 3.2072871Losses:  4.015515327453613 0.26809749007225037 0.8688297271728516
MemoryTrain:  epoch  7, batch     0 | loss: 4.0155153Losses:  3.0949816703796387 0.2554817795753479 0.7301850914955139
MemoryTrain:  epoch  7, batch     1 | loss: 3.0949817Losses:  4.0181074142456055 0.8154708743095398 0.8218910098075867
MemoryTrain:  epoch  7, batch     2 | loss: 4.0181074Losses:  2.9001994132995605 0.30578336119651794 0.8717669248580933
MemoryTrain:  epoch  7, batch     3 | loss: 2.9001994Losses:  3.1816041469573975 0.48159849643707275 0.684532880783081
MemoryTrain:  epoch  7, batch     4 | loss: 3.1816041Losses:  2.3445467948913574 0.21798963844776154 0.5670111775398254
MemoryTrain:  epoch  7, batch     5 | loss: 2.3445468Losses:  3.4862916469573975 0.831843376159668 0.8707311153411865
MemoryTrain:  epoch  8, batch     0 | loss: 3.4862916Losses:  3.691751480102539 0.4817233383655548 0.7368425726890564
MemoryTrain:  epoch  8, batch     1 | loss: 3.6917515Losses:  3.822357416152954 0.7713162899017334 0.7200667262077332
MemoryTrain:  epoch  8, batch     2 | loss: 3.8223574Losses:  3.5114543437957764 0.8111721873283386 0.8368671536445618
MemoryTrain:  epoch  8, batch     3 | loss: 3.5114543Losses:  3.414539337158203 1.281181812286377 0.7490391135215759
MemoryTrain:  epoch  8, batch     4 | loss: 3.4145393Losses:  3.0394210815429688 0.5494875311851501 0.2933790385723114
MemoryTrain:  epoch  8, batch     5 | loss: 3.0394211Losses:  3.1108238697052 0.5542131066322327 0.743148148059845
MemoryTrain:  epoch  9, batch     0 | loss: 3.1108239Losses:  3.237895965576172 0.4901075065135956 0.5680917501449585
MemoryTrain:  epoch  9, batch     1 | loss: 3.2378960Losses:  2.7011842727661133 0.24528226256370544 0.6475580930709839
MemoryTrain:  epoch  9, batch     2 | loss: 2.7011843Losses:  3.115863800048828 0.49169182777404785 0.7337933778762817
MemoryTrain:  epoch  9, batch     3 | loss: 3.1158638Losses:  3.601168155670166 1.1472632884979248 0.8158994913101196
MemoryTrain:  epoch  9, batch     4 | loss: 3.6011682Losses:  2.7234034538269043 0.31509914994239807 0.6932697296142578
MemoryTrain:  epoch  9, batch     5 | loss: 2.7234035
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 93.06%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 86.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 85.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 86.33%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 85.29%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 85.53%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 85.31%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 85.12%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.94%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 85.05%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 85.16%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 81.97%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 77.23%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 72.50%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 70.56%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 69.89%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 69.49%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 68.93%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 68.92%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 69.09%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 68.59%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 69.05%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 69.20%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 69.62%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 69.46%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 68.33%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 67.53%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 66.89%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 66.54%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 65.69%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 65.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 69.30%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 69.72%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 71.21%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 71.33%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.98%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 88.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 89.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 90.31%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 89.77%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 89.13%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 89.32%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.66%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.52%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 90.81%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 90.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 90.45%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 90.20%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 90.13%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.85%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.07%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.13%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.34%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.44%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 91.36%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.28%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 91.33%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.38%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 91.30%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 91.23%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.27%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 91.32%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.14%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.18%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 91.12%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 90.95%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 91.29%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.23%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 90.67%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 89.26%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 87.98%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 86.84%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 85.73%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 84.47%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 83.97%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 84.02%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 84.07%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 84.20%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 84.25%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 84.21%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 84.50%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 84.62%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 84.81%   [EVAL] batch:   79 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 84.76%   [EVAL] batch:   82 | acc: 25.00%,  total acc: 84.04%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 83.48%   [EVAL] batch:   84 | acc: 31.25%,  total acc: 82.87%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 82.34%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 81.68%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 81.04%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 80.20%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 79.38%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 78.50%   [EVAL] batch:   91 | acc: 25.00%,  total acc: 77.92%   [EVAL] batch:   92 | acc: 12.50%,  total acc: 77.22%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 76.53%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 76.58%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 76.76%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 77.00%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 77.23%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 77.40%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 77.38%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 77.54%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 77.70%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 77.91%   [EVAL] batch:  103 | acc: 87.50%,  total acc: 78.00%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 78.15%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 78.30%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 78.45%   [EVAL] batch:  107 | acc: 81.25%,  total acc: 78.47%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 78.27%   [EVAL] batch:  109 | acc: 81.25%,  total acc: 78.30%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 78.32%   [EVAL] batch:  111 | acc: 81.25%,  total acc: 78.35%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 78.10%   [EVAL] batch:  113 | acc: 62.50%,  total acc: 77.96%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 77.83%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 77.91%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 77.78%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 77.75%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 77.63%   [EVAL] batch:  119 | acc: 50.00%,  total acc: 77.40%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 77.17%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 77.00%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 76.93%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 76.97%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 76.85%   [EVAL] batch:  125 | acc: 93.75%,  total acc: 76.98%   [EVAL] batch:  126 | acc: 87.50%,  total acc: 77.07%   [EVAL] batch:  127 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:  128 | acc: 93.75%,  total acc: 77.37%   [EVAL] batch:  129 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 77.67%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 77.79%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 77.87%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 77.94%   [EVAL] batch:  134 | acc: 62.50%,  total acc: 77.82%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 77.85%   [EVAL] batch:  136 | acc: 62.50%,  total acc: 77.74%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 77.72%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 77.83%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 77.81%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 77.93%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 77.86%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 77.93%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 77.99%   [EVAL] batch:  144 | acc: 81.25%,  total acc: 78.02%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 78.04%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 78.06%   [EVAL] batch:  147 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:  148 | acc: 87.50%,  total acc: 78.19%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 78.25%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 77.73%   [EVAL] batch:  151 | acc: 18.75%,  total acc: 77.34%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 76.92%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 76.46%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 76.01%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 75.60%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 75.52%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 75.40%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 75.28%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 75.12%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 75.08%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 75.08%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 74.96%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 74.89%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 74.92%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 74.92%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 74.93%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 74.93%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 74.60%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 74.34%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 74.13%   [EVAL] batch:  172 | acc: 50.00%,  total acc: 73.99%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 73.71%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 73.54%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 73.98%   [EVAL] batch:  178 | acc: 93.75%,  total acc: 74.09%   [EVAL] batch:  179 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 74.59%   [EVAL] batch:  183 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:  184 | acc: 100.00%,  total acc: 74.86%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 75.13%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 75.00%   
cur_acc:  ['0.9494', '0.6270', '0.7133']
his_acc:  ['0.9494', '0.7820', '0.7500']
Clustering into  19  clusters
Clusters:  [ 0  3 15  2  8  8 10  0  1  9  3  4 18  7  2 14 12  1  0  1  6  0  0  8
  7  9  0  8  4  4 17  0 11  4  5  4 13  6 16  6]
Losses:  9.580635070800781 2.353717803955078 0.7960342168807983
CurrentTrain: epoch  0, batch     0 | loss: 9.5806351Losses:  9.373027801513672 2.716945171356201 0.7432655096054077
CurrentTrain: epoch  0, batch     1 | loss: 9.3730278Losses:  11.406994819641113 4.035477161407471 0.6941465735435486
CurrentTrain: epoch  0, batch     2 | loss: 11.4069948Losses:  5.853401184082031 -0.0 0.10085354000329971
CurrentTrain: epoch  0, batch     3 | loss: 5.8534012Losses:  13.328213691711426 6.2251691818237305 0.5673350095748901
CurrentTrain: epoch  1, batch     0 | loss: 13.3282137Losses:  10.571382522583008 4.030336856842041 0.7111243009567261
CurrentTrain: epoch  1, batch     1 | loss: 10.5713825Losses:  8.037582397460938 2.5302135944366455 0.6950105428695679
CurrentTrain: epoch  1, batch     2 | loss: 8.0375824Losses:  2.437488079071045 -0.0 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 2.4374881Losses:  8.123602867126465 2.2378602027893066 0.659622073173523
CurrentTrain: epoch  2, batch     0 | loss: 8.1236029Losses:  7.316956996917725 2.276284694671631 0.6901301741600037
CurrentTrain: epoch  2, batch     1 | loss: 7.3169570Losses:  8.268936157226562 2.6330602169036865 0.6707613468170166
CurrentTrain: epoch  2, batch     2 | loss: 8.2689362Losses:  4.794327735900879 -0.0 0.13278159499168396
CurrentTrain: epoch  2, batch     3 | loss: 4.7943277Losses:  7.967586040496826 2.918729066848755 0.5982470512390137
CurrentTrain: epoch  3, batch     0 | loss: 7.9675860Losses:  7.691527366638184 2.9338431358337402 0.5991427302360535
CurrentTrain: epoch  3, batch     1 | loss: 7.6915274Losses:  10.365546226501465 4.431365966796875 0.6282073855400085
CurrentTrain: epoch  3, batch     2 | loss: 10.3655462Losses:  1.9541122913360596 -0.0 0.10938697308301926
CurrentTrain: epoch  3, batch     3 | loss: 1.9541123Losses:  7.804670810699463 4.056017875671387 0.5959043502807617
CurrentTrain: epoch  4, batch     0 | loss: 7.8046708Losses:  7.289999961853027 2.750692367553711 0.5972986221313477
CurrentTrain: epoch  4, batch     1 | loss: 7.2900000Losses:  8.79128646850586 3.3706932067871094 0.5127592086791992
CurrentTrain: epoch  4, batch     2 | loss: 8.7912865Losses:  5.880908966064453 -0.0 0.2159862369298935
CurrentTrain: epoch  4, batch     3 | loss: 5.8809090Losses:  9.201942443847656 3.8939661979675293 0.5367262363433838
CurrentTrain: epoch  5, batch     0 | loss: 9.2019424Losses:  6.979447364807129 3.1550588607788086 0.5621030330657959
CurrentTrain: epoch  5, batch     1 | loss: 6.9794474Losses:  6.578519344329834 2.7786078453063965 0.5790392160415649
CurrentTrain: epoch  5, batch     2 | loss: 6.5785193Losses:  1.7961821556091309 -0.0 0.09929349273443222
CurrentTrain: epoch  5, batch     3 | loss: 1.7961822Losses:  6.449499130249023 2.3022546768188477 0.6791605949401855
CurrentTrain: epoch  6, batch     0 | loss: 6.4494991Losses:  7.925468444824219 4.029979705810547 0.4943772256374359
CurrentTrain: epoch  6, batch     1 | loss: 7.9254684Losses:  7.640965938568115 3.2296550273895264 0.6292797923088074
CurrentTrain: epoch  6, batch     2 | loss: 7.6409659Losses:  3.6659018993377686 -0.0 0.22544598579406738
CurrentTrain: epoch  6, batch     3 | loss: 3.6659019Losses:  7.353675842285156 3.5889382362365723 0.41861629486083984
CurrentTrain: epoch  7, batch     0 | loss: 7.3536758Losses:  6.851538181304932 3.4405386447906494 0.6829370856285095
CurrentTrain: epoch  7, batch     1 | loss: 6.8515382Losses:  7.359566688537598 3.335963249206543 0.6279283761978149
CurrentTrain: epoch  7, batch     2 | loss: 7.3595667Losses:  5.152159690856934 -0.0 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 5.1521597Losses:  7.825888156890869 4.065310478210449 0.49653419852256775
CurrentTrain: epoch  8, batch     0 | loss: 7.8258882Losses:  5.140558242797852 1.6857831478118896 0.6644915342330933
CurrentTrain: epoch  8, batch     1 | loss: 5.1405582Losses:  5.734714508056641 2.6292848587036133 0.6642618179321289
CurrentTrain: epoch  8, batch     2 | loss: 5.7347145Losses:  2.145416498184204 -0.0 0.13090386986732483
CurrentTrain: epoch  8, batch     3 | loss: 2.1454165Losses:  6.025190830230713 2.619330406188965 0.6404395699501038
CurrentTrain: epoch  9, batch     0 | loss: 6.0251908Losses:  5.879919052124023 2.3706905841827393 0.6270492076873779
CurrentTrain: epoch  9, batch     1 | loss: 5.8799191Losses:  5.0281219482421875 1.9395272731781006 0.6610862016677856
CurrentTrain: epoch  9, batch     2 | loss: 5.0281219Losses:  2.0399458408355713 -0.0 0.10073643922805786
CurrentTrain: epoch  9, batch     3 | loss: 2.0399458
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: in 2004 the catalan government gave him the george cross .
Head Entity: george cross
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union will affect all member states.  
Head Entity: environmental regulations  
Tail Entity: European Union  

Relation: applies to jurisdiction  
Context: The Supreme Court's ruling has significant implications for the state of California's water rights.  
Head Entity: Supreme Court  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " tenth avenue freeze - out " is the second song on bruce springsteen 's breakthrough album " born to run " , released in 1975 .
Head Entity: tenth avenue freeze - out
Tail Entity: bruce springsteen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a popular song by Ed Sheeran that topped charts worldwide after its release in 2017.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The Beatles are widely regarded as one of the most influential bands in the history of music, with hits like "Hey Jude" and "Let It Be."  
Head Entity: The Beatles  
Tail Entity: Hey Jude  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: an emd gp49 is a 4-axle diesel locomotive built by general motors electro - motive division .
Head Entity: emd gp49
Tail Entity: general motors electro - motive division
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: the iphone 13 is a smartphone designed and marketed by apple inc.  
Head Entity: iphone 13  
Tail Entity: apple inc.  

Relation: manufacturer  
Context: the model s is an all-electric sedan produced by tesla, inc.  
Head Entity: model s  
Tail Entity: tesla, inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, were active participants in raising funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: he placed second three times in a row at the cuban national championships from 2005- 2007 at light heavyweight , each time behind yusiel nápoles .
Head Entity: yusiel nápoles
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team qualified for the finals in the under-18 category of the national soccer tournament, showcasing their skills against various opponents throughout the season.  
Head Entity: under-18 category  
Tail Entity: national soccer tournament  

Relation: competition class  
Context: She competed in the women's marathon event, which is classified under the elite category for international competitions, achieving a personal best time.  
Head Entity: elite category  
Tail Entity: women's marathon event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: swartz served as swedish prime minister for only seven months before being forced from power after the swedish elections of 1917 and replaced by the liberal government of nils edén .
Head Entity: swedish elections of 1917
Tail Entity: nils edén
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: in the recent municipal elections, the voters chose maria gonzalez as the new mayor of the city, marking a significant shift in local governance.  
Head Entity: municipal elections  
Tail Entity: maria gonzalez  

Relation: successful candidate  
Context: after a closely contested race, the citizens of springfield elected john doe as their new governor, succeeding the long-serving incumbent.  
Head Entity: springfield elections  
Tail Entity: john doe  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce rivalry, yet they shared a bond that only siblings could understand.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it was heartwarming to see how Anna and her brother, Mark, reminisced about their childhood adventures, showcasing the unique connection that only siblings share.  
Head Entity: Anna  
Tail Entity: Mark  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: the united states navy is known for its naval operations and maritime security.  
Head Entity: united states navy  
Tail Entity: naval operations  

Relation: military branch  
Context: general patton was a prominent leader in the united states army during world war ii.  
Head Entity: general patton  
Tail Entity: united states army  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
Losses:  5.646574020385742 0.6209157705307007 0.7225470542907715
MemoryTrain:  epoch  0, batch     0 | loss: 5.6465740Losses:  6.314779281616211 0.318909227848053 0.9304126501083374
MemoryTrain:  epoch  0, batch     1 | loss: 6.3147793Losses:  5.512446403503418 1.3497693538665771 0.8299500942230225
MemoryTrain:  epoch  0, batch     2 | loss: 5.5124464Losses:  5.779263019561768 0.8455265164375305 0.867667019367218
MemoryTrain:  epoch  0, batch     3 | loss: 5.7792630Losses:  4.70626974105835 0.5164504647254944 0.8029059171676636
MemoryTrain:  epoch  0, batch     4 | loss: 4.7062697Losses:  5.790569305419922 0.5096011161804199 0.9389737248420715
MemoryTrain:  epoch  0, batch     5 | loss: 5.7905693Losses:  6.1328582763671875 1.5175315141677856 0.7366784811019897
MemoryTrain:  epoch  0, batch     6 | loss: 6.1328583Losses:  4.347070217132568 -0.0 0.49208006262779236
MemoryTrain:  epoch  0, batch     7 | loss: 4.3470702Losses:  5.085351467132568 0.7958152294158936 0.854280412197113
MemoryTrain:  epoch  1, batch     0 | loss: 5.0853515Losses:  6.152672290802002 0.8342617750167847 0.8174037933349609
MemoryTrain:  epoch  1, batch     1 | loss: 6.1526723Losses:  4.534952163696289 0.3533729314804077 0.8816014528274536
MemoryTrain:  epoch  1, batch     2 | loss: 4.5349522Losses:  4.786128997802734 0.5477847456932068 0.9380162954330444
MemoryTrain:  epoch  1, batch     3 | loss: 4.7861290Losses:  4.742371082305908 0.619855523109436 0.825316846370697
MemoryTrain:  epoch  1, batch     4 | loss: 4.7423711Losses:  5.959000587463379 0.5070279836654663 0.8016090393066406
MemoryTrain:  epoch  1, batch     5 | loss: 5.9590006Losses:  5.399001598358154 0.4973980486392975 0.8071156740188599
MemoryTrain:  epoch  1, batch     6 | loss: 5.3990016Losses:  3.305920124053955 -0.0 0.3509056270122528
MemoryTrain:  epoch  1, batch     7 | loss: 3.3059201Losses:  4.694159030914307 0.30824965238571167 0.9066411256790161
MemoryTrain:  epoch  2, batch     0 | loss: 4.6941590Losses:  5.960202217102051 1.3998908996582031 0.8705469369888306
MemoryTrain:  epoch  2, batch     1 | loss: 5.9602022Losses:  3.838583469390869 0.5256118774414062 0.845723032951355
MemoryTrain:  epoch  2, batch     2 | loss: 3.8385835Losses:  4.874237060546875 1.5412077903747559 0.7415645122528076
MemoryTrain:  epoch  2, batch     3 | loss: 4.8742371Losses:  5.061885833740234 1.0614418983459473 0.9113363027572632
MemoryTrain:  epoch  2, batch     4 | loss: 5.0618858Losses:  5.30882453918457 1.10464608669281 0.6669797897338867
MemoryTrain:  epoch  2, batch     5 | loss: 5.3088245Losses:  3.565838575363159 -0.0 0.8437816500663757
MemoryTrain:  epoch  2, batch     6 | loss: 3.5658386Losses:  2.7888827323913574 -0.0 0.5602598190307617
MemoryTrain:  epoch  2, batch     7 | loss: 2.7888827Losses:  4.1236572265625 1.0575876235961914 0.7771236896514893
MemoryTrain:  epoch  3, batch     0 | loss: 4.1236572Losses:  3.9786858558654785 0.4943189024925232 0.7564210891723633
MemoryTrain:  epoch  3, batch     1 | loss: 3.9786859Losses:  3.286465883255005 -0.0 0.8914906978607178
MemoryTrain:  epoch  3, batch     2 | loss: 3.2864659Losses:  5.38743257522583 0.9833930730819702 0.9068013429641724
MemoryTrain:  epoch  3, batch     3 | loss: 5.3874326Losses:  3.3508987426757812 0.3365536332130432 0.6686673760414124
MemoryTrain:  epoch  3, batch     4 | loss: 3.3508987Losses:  3.8995823860168457 0.3276359736919403 0.9654271602630615
MemoryTrain:  epoch  3, batch     5 | loss: 3.8995824Losses:  3.071789264678955 -0.0 0.8487486839294434
MemoryTrain:  epoch  3, batch     6 | loss: 3.0717893Losses:  2.557338237762451 -0.0 0.5001446604728699
MemoryTrain:  epoch  3, batch     7 | loss: 2.5573382Losses:  4.33635139465332 -0.0 0.917073130607605
MemoryTrain:  epoch  4, batch     0 | loss: 4.3363514Losses:  3.2233963012695312 0.7931217551231384 0.8118654489517212
MemoryTrain:  epoch  4, batch     1 | loss: 3.2233963Losses:  3.152524471282959 0.2800661027431488 0.9123932719230652
MemoryTrain:  epoch  4, batch     2 | loss: 3.1525245Losses:  3.732887029647827 0.5882059931755066 0.8276256918907166
MemoryTrain:  epoch  4, batch     3 | loss: 3.7328870Losses:  4.306573390960693 0.6619000434875488 0.9370297193527222
MemoryTrain:  epoch  4, batch     4 | loss: 4.3065734Losses:  3.1712441444396973 0.5365818738937378 0.7160674333572388
MemoryTrain:  epoch  4, batch     5 | loss: 3.1712441Losses:  2.900853157043457 -0.0 0.88246089220047
MemoryTrain:  epoch  4, batch     6 | loss: 2.9008532Losses:  2.9310691356658936 -0.0 0.586995542049408
MemoryTrain:  epoch  4, batch     7 | loss: 2.9310691Losses:  3.3975753784179688 0.5533125400543213 0.7044112682342529
MemoryTrain:  epoch  5, batch     0 | loss: 3.3975754Losses:  3.2283878326416016 0.2820034623146057 0.978681743144989
MemoryTrain:  epoch  5, batch     1 | loss: 3.2283878Losses:  3.118903398513794 0.49947044253349304 0.8117846846580505
MemoryTrain:  epoch  5, batch     2 | loss: 3.1189034Losses:  3.1242733001708984 0.257257342338562 0.8233251571655273
MemoryTrain:  epoch  5, batch     3 | loss: 3.1242733Losses:  3.3151774406433105 0.473710298538208 0.9193110466003418
MemoryTrain:  epoch  5, batch     4 | loss: 3.3151774Losses:  2.704425811767578 0.22399699687957764 0.7973299026489258
MemoryTrain:  epoch  5, batch     5 | loss: 2.7044258Losses:  3.914583683013916 0.7114169001579285 0.7239061594009399
MemoryTrain:  epoch  5, batch     6 | loss: 3.9145837Losses:  2.616386651992798 -0.0 0.7045447826385498
MemoryTrain:  epoch  5, batch     7 | loss: 2.6163867Losses:  3.5136351585388184 0.5463882684707642 0.9444984793663025
MemoryTrain:  epoch  6, batch     0 | loss: 3.5136352Losses:  2.683415651321411 0.4717113971710205 0.7345619201660156
MemoryTrain:  epoch  6, batch     1 | loss: 2.6834157Losses:  4.64463472366333 1.515877604484558 0.8039276003837585
MemoryTrain:  epoch  6, batch     2 | loss: 4.6446347Losses:  4.400380611419678 1.6598026752471924 0.705694317817688
MemoryTrain:  epoch  6, batch     3 | loss: 4.4003806Losses:  3.1112823486328125 0.5923155546188354 0.734128475189209
MemoryTrain:  epoch  6, batch     4 | loss: 3.1112823Losses:  2.975738763809204 0.5410159826278687 0.7286890149116516
MemoryTrain:  epoch  6, batch     5 | loss: 2.9757388Losses:  2.753124475479126 -0.0 0.8677762150764465
MemoryTrain:  epoch  6, batch     6 | loss: 2.7531245Losses:  2.027707576751709 -0.0 0.5117944478988647
MemoryTrain:  epoch  6, batch     7 | loss: 2.0277076Losses:  3.1627092361450195 0.5239982604980469 0.9879030585289001
MemoryTrain:  epoch  7, batch     0 | loss: 3.1627092Losses:  2.4511876106262207 0.27075764536857605 0.7560200095176697
MemoryTrain:  epoch  7, batch     1 | loss: 2.4511876Losses:  2.678675889968872 0.24878457188606262 0.7270746827125549
MemoryTrain:  epoch  7, batch     2 | loss: 2.6786759Losses:  3.0697813034057617 0.7184545993804932 0.7137013673782349
MemoryTrain:  epoch  7, batch     3 | loss: 3.0697813Losses:  3.2166197299957275 0.6085612773895264 0.7341448664665222
MemoryTrain:  epoch  7, batch     4 | loss: 3.2166197Losses:  2.9949522018432617 0.22274698317050934 0.7148057818412781
MemoryTrain:  epoch  7, batch     5 | loss: 2.9949522Losses:  4.157662868499756 1.1981743574142456 0.8905795812606812
MemoryTrain:  epoch  7, batch     6 | loss: 4.1576629Losses:  2.6993918418884277 0.3409422039985657 0.6218169927597046
MemoryTrain:  epoch  7, batch     7 | loss: 2.6993918Losses:  2.9757213592529297 0.4646371006965637 0.891975998878479
MemoryTrain:  epoch  8, batch     0 | loss: 2.9757214Losses:  3.3576626777648926 0.5536298155784607 0.8681014180183411
MemoryTrain:  epoch  8, batch     1 | loss: 3.3576627Losses:  3.0197227001190186 0.4811865985393524 0.7874742150306702
MemoryTrain:  epoch  8, batch     2 | loss: 3.0197227Losses:  2.5990025997161865 -0.0 0.9846764802932739
MemoryTrain:  epoch  8, batch     3 | loss: 2.5990026Losses:  2.6587696075439453 0.2892821431159973 0.6813032627105713
MemoryTrain:  epoch  8, batch     4 | loss: 2.6587696Losses:  2.5220401287078857 0.4153759777545929 0.708532989025116
MemoryTrain:  epoch  8, batch     5 | loss: 2.5220401Losses:  2.8860621452331543 0.49701833724975586 0.7950940728187561
MemoryTrain:  epoch  8, batch     6 | loss: 2.8860621Losses:  2.7966156005859375 -0.0 0.40757760405540466
MemoryTrain:  epoch  8, batch     7 | loss: 2.7966156Losses:  2.588554859161377 -0.0 0.9608202576637268
MemoryTrain:  epoch  9, batch     0 | loss: 2.5885549Losses:  3.6367151737213135 1.1134077310562134 0.7491968274116516
MemoryTrain:  epoch  9, batch     1 | loss: 3.6367152Losses:  2.8309340476989746 0.25971949100494385 0.9883126616477966
MemoryTrain:  epoch  9, batch     2 | loss: 2.8309340Losses:  2.8847506046295166 0.704395055770874 0.8487046360969543
MemoryTrain:  epoch  9, batch     3 | loss: 2.8847506Losses:  3.196290969848633 0.5640994310379028 0.8295841217041016
MemoryTrain:  epoch  9, batch     4 | loss: 3.1962910Losses:  2.9610276222229004 0.5121181011199951 0.8103214502334595
MemoryTrain:  epoch  9, batch     5 | loss: 2.9610276Losses:  2.3173999786376953 0.24753890931606293 0.7286673784255981
MemoryTrain:  epoch  9, batch     6 | loss: 2.3174000Losses:  2.4528250694274902 -0.0 0.4480762481689453
MemoryTrain:  epoch  9, batch     7 | loss: 2.4528251
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 51.56%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 55.00%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 61.72%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 63.89%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 66.25%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 65.91%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 70.59%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 71.53%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 72.04%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 69.38%   [EVAL] batch:   20 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 67.05%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 66.85%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 66.15%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 64.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.11%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 69.40%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.37%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 72.07%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 75.17%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 75.82%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 75.64%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 75.31%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 74.70%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 74.85%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 74.57%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 73.89%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 73.51%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 73.14%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 72.70%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 73.32%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 73.47%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 73.50%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 73.86%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 73.77%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 73.68%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 73.06%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 72.56%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 72.08%   [EVAL] batch:   60 | acc: 31.25%,  total acc: 71.41%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 71.17%   [EVAL] batch:   62 | acc: 12.50%,  total acc: 70.24%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 86.40%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 87.22%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 86.96%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 87.24%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 87.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.74%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.79%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 89.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.52%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.15%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 89.52%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 89.11%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 89.19%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 89.31%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.09%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.33%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.41%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 90.34%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 90.00%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 89.95%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 89.63%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 89.19%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 89.03%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 88.75%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 88.85%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 88.82%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 88.92%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 89.00%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 88.86%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 88.95%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 88.93%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 88.69%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 88.77%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 88.85%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 88.73%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 88.71%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 88.19%   [EVAL] batch:   63 | acc: 12.50%,  total acc: 87.01%   [EVAL] batch:   64 | acc: 18.75%,  total acc: 85.96%   [EVAL] batch:   65 | acc: 18.75%,  total acc: 84.94%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 83.96%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 82.90%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 82.43%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 82.32%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 82.48%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 82.88%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 82.85%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 82.83%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 83.06%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 83.39%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 83.52%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 83.64%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 83.08%   [EVAL] batch:   82 | acc: 43.75%,  total acc: 82.61%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 82.07%   [EVAL] batch:   84 | acc: 62.50%,  total acc: 81.84%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 81.32%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 80.75%   [EVAL] batch:   87 | acc: 31.25%,  total acc: 80.18%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 79.35%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 78.54%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 77.68%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 77.04%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 76.28%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 75.60%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 75.66%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 75.78%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 76.03%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 76.28%   [EVAL] batch:   98 | acc: 93.75%,  total acc: 76.45%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 76.50%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 76.90%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 77.12%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 77.50%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 77.65%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 77.69%   [EVAL] batch:  107 | acc: 68.75%,  total acc: 77.60%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 77.41%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 77.27%   [EVAL] batch:  110 | acc: 68.75%,  total acc: 77.20%   [EVAL] batch:  111 | acc: 75.00%,  total acc: 77.18%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 76.94%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 76.64%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 76.41%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 76.51%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 76.34%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 76.22%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 76.10%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 75.83%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 75.62%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 75.41%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 75.36%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 75.40%   [EVAL] batch:  124 | acc: 56.25%,  total acc: 75.25%   [EVAL] batch:  125 | acc: 75.00%,  total acc: 75.25%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 74.95%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 74.76%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 74.66%   [EVAL] batch:  129 | acc: 62.50%,  total acc: 74.57%   [EVAL] batch:  130 | acc: 81.25%,  total acc: 74.62%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 74.76%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 74.95%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 75.05%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 75.05%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 75.18%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 75.27%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 75.36%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 75.40%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 75.40%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 75.49%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 75.53%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 75.48%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 75.56%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 75.47%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 75.47%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 75.38%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 75.34%   [EVAL] batch:  148 | acc: 87.50%,  total acc: 75.42%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 75.42%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 74.92%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 74.51%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 74.06%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 73.58%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 73.15%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 72.76%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 72.65%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 72.55%   [EVAL] batch:  158 | acc: 62.50%,  total acc: 72.48%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 72.30%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 72.24%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 72.16%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 72.10%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 72.16%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 72.21%   [EVAL] batch:  166 | acc: 68.75%,  total acc: 72.19%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 72.28%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 72.23%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 71.95%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 71.71%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 71.44%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 71.24%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 71.01%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 70.86%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 71.19%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 71.67%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 71.82%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 71.98%   [EVAL] batch:  182 | acc: 93.75%,  total acc: 72.10%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 72.21%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 72.33%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 72.45%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 72.53%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 72.47%   [EVAL] batch:  188 | acc: 50.00%,  total acc: 72.35%   [EVAL] batch:  189 | acc: 50.00%,  total acc: 72.24%   [EVAL] batch:  190 | acc: 56.25%,  total acc: 72.15%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 72.04%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 72.05%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 72.00%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 72.05%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 72.10%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 72.11%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 72.16%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 72.14%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 72.22%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 72.29%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 72.34%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 72.35%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 72.40%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 72.35%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 72.28%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 72.18%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 72.07%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 71.85%   [EVAL] batch:  210 | acc: 68.75%,  total acc: 71.83%   [EVAL] batch:  211 | acc: 50.00%,  total acc: 71.73%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 71.65%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 72.02%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.15%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.28%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 72.37%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 72.62%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 72.87%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 73.06%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 73.02%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 72.97%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 72.93%   [EVAL] batch:  229 | acc: 81.25%,  total acc: 72.96%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 72.92%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 72.82%   [EVAL] batch:  232 | acc: 50.00%,  total acc: 72.72%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 72.68%   [EVAL] batch:  234 | acc: 50.00%,  total acc: 72.58%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 72.56%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 72.52%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.56%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 72.62%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:  240 | acc: 68.75%,  total acc: 72.72%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 72.78%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 72.81%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 72.82%   [EVAL] batch:  244 | acc: 50.00%,  total acc: 72.73%   [EVAL] batch:  245 | acc: 43.75%,  total acc: 72.61%   [EVAL] batch:  246 | acc: 25.00%,  total acc: 72.42%   [EVAL] batch:  247 | acc: 56.25%,  total acc: 72.35%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 72.16%   [EVAL] batch:  249 | acc: 50.00%,  total acc: 72.08%   
cur_acc:  ['0.9494', '0.6270', '0.7133', '0.7024']
his_acc:  ['0.9494', '0.7820', '0.7500', '0.7208']
Clustering into  24  clusters
Clusters:  [ 3  7 19 21  4  4 17  6  0 12  4  2 22 11  5 15 18  0  3  0  9  3  6  4
 11 12  3  4  2  2 23  3 20  2 13  2  1  9 10  9  7 16  1 10  6  5  0 14
  8  3]
Losses:  10.615777969360352 3.7874503135681152 0.7299081683158875
CurrentTrain: epoch  0, batch     0 | loss: 10.6157780Losses:  9.694838523864746 3.2893128395080566 0.795457124710083
CurrentTrain: epoch  0, batch     1 | loss: 9.6948385Losses:  10.530438423156738 3.7750964164733887 0.7575141191482544
CurrentTrain: epoch  0, batch     2 | loss: 10.5304384Losses:  7.628225326538086 -0.0 0.08352130651473999
CurrentTrain: epoch  0, batch     3 | loss: 7.6282253Losses:  9.750423431396484 4.242359161376953 0.6654142737388611
CurrentTrain: epoch  1, batch     0 | loss: 9.7504234Losses:  8.38956069946289 3.6955816745758057 0.729350745677948
CurrentTrain: epoch  1, batch     1 | loss: 8.3895607Losses:  10.242799758911133 3.66107177734375 0.7856985926628113
CurrentTrain: epoch  1, batch     2 | loss: 10.2427998Losses:  3.563474416732788 -0.0 0.09712054580450058
CurrentTrain: epoch  1, batch     3 | loss: 3.5634744Losses:  8.643657684326172 3.028432607650757 0.7062796950340271
CurrentTrain: epoch  2, batch     0 | loss: 8.6436577Losses:  7.815609931945801 2.8115525245666504 0.7400075197219849
CurrentTrain: epoch  2, batch     1 | loss: 7.8156099Losses:  6.998003005981445 3.1801414489746094 0.6642162203788757
CurrentTrain: epoch  2, batch     2 | loss: 6.9980030Losses:  3.880185604095459 -0.0 0.14286039769649506
CurrentTrain: epoch  2, batch     3 | loss: 3.8801856Losses:  8.112882614135742 3.886306047439575 0.6804585456848145
CurrentTrain: epoch  3, batch     0 | loss: 8.1128826Losses:  7.8025407791137695 2.8976736068725586 0.7429917454719543
CurrentTrain: epoch  3, batch     1 | loss: 7.8025408Losses:  6.979892730712891 2.198164224624634 0.8235443234443665
CurrentTrain: epoch  3, batch     2 | loss: 6.9798927Losses:  2.341479539871216 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 2.3414795Losses:  6.558151721954346 2.838348865509033 0.6950842142105103
CurrentTrain: epoch  4, batch     0 | loss: 6.5581517Losses:  6.929147720336914 3.1201400756835938 0.7787509560585022
CurrentTrain: epoch  4, batch     1 | loss: 6.9291477Losses:  8.818672180175781 3.734910726547241 0.6897612810134888
CurrentTrain: epoch  4, batch     2 | loss: 8.8186722Losses:  4.477963447570801 -0.0 0.08638271689414978
CurrentTrain: epoch  4, batch     3 | loss: 4.4779634Losses:  6.944783687591553 3.6665093898773193 0.674441933631897
CurrentTrain: epoch  5, batch     0 | loss: 6.9447837Losses:  6.540212154388428 2.5761325359344482 0.7168639898300171
CurrentTrain: epoch  5, batch     1 | loss: 6.5402122Losses:  8.584183692932129 3.9088897705078125 0.6877495050430298
CurrentTrain: epoch  5, batch     2 | loss: 8.5841837Losses:  2.0861916542053223 -0.0 0.10587777197360992
CurrentTrain: epoch  5, batch     3 | loss: 2.0861917Losses:  6.733611106872559 2.5041162967681885 0.7540130019187927
CurrentTrain: epoch  6, batch     0 | loss: 6.7336111Losses:  4.966959476470947 1.541261911392212 0.7987942099571228
CurrentTrain: epoch  6, batch     1 | loss: 4.9669595Losses:  6.519418239593506 2.7068963050842285 0.8010830283164978
CurrentTrain: epoch  6, batch     2 | loss: 6.5194182Losses:  3.5569441318511963 -0.0 0.07892543822526932
CurrentTrain: epoch  6, batch     3 | loss: 3.5569441Losses:  5.428555488586426 1.886537790298462 0.7218121886253357
CurrentTrain: epoch  7, batch     0 | loss: 5.4285555Losses:  6.924378871917725 3.4005861282348633 0.7324442267417908
CurrentTrain: epoch  7, batch     1 | loss: 6.9243789Losses:  7.468815803527832 3.602236270904541 0.7483608722686768
CurrentTrain: epoch  7, batch     2 | loss: 7.4688158Losses:  1.9279677867889404 -0.0 0.09675808995962143
CurrentTrain: epoch  7, batch     3 | loss: 1.9279678Losses:  7.130438327789307 3.3525004386901855 0.6752024292945862
CurrentTrain: epoch  8, batch     0 | loss: 7.1304383Losses:  5.200356960296631 2.013291358947754 0.7905718088150024
CurrentTrain: epoch  8, batch     1 | loss: 5.2003570Losses:  5.969248294830322 2.878990411758423 0.718190610408783
CurrentTrain: epoch  8, batch     2 | loss: 5.9692483Losses:  2.243722915649414 -0.0 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 2.2437229Losses:  5.1047539710998535 1.9301815032958984 0.7785544991493225
CurrentTrain: epoch  9, batch     0 | loss: 5.1047540Losses:  6.018070220947266 2.9631705284118652 0.7204267978668213
CurrentTrain: epoch  9, batch     1 | loss: 6.0180702Losses:  5.5468525886535645 1.9767378568649292 0.77914959192276
CurrentTrain: epoch  9, batch     2 | loss: 5.5468526Losses:  3.1182727813720703 -0.0 0.12709692120552063
CurrentTrain: epoch  9, batch     3 | loss: 3.1182728
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of Australia are the major political divisions, with New South Wales being the most populous state in the country.  
Head Entity: Australia  
Tail Entity: New South Wales  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: In some circles, the terms "artificial intelligence" and "machine learning" are said to be the same as each other, although experts argue there are key differences.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: the filming was primarily held at pollachi . it fetched pawan kalyan the filmfare award for best actor – telugu award at 59th filmfare awards south .
Head Entity: filmfare award for best actor – telugu
Tail Entity: pawan kalyan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: in the thrilling finale of the championship, the underdog team triumphed, securing the title and making history as they celebrated their victory at the national sports arena. This remarkable achievement earned them the prestigious trophy for the best team of the year.  
Head Entity: trophy for the best team of the year  
Tail Entity: underdog team  

Relation: winner  
Context: during the annual music festival, the talented singer captivated the audience with her stunning performance, ultimately leading her to win the coveted title of best performer of the night.  
Head Entity: title of best performer of the night  
Tail Entity: talented singer  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: air commodore arthur kellam tylee obe ( 24 april 1887 – 13 april 1961 ) was canadian officer who served in the royal flying corps during world war i.
Head Entity: arthur kellam tylee
Tail Entity: air commodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: general john smith was appointed as the chief of staff of the united states army in 2020, overseeing all operations and strategic planning.  
Head Entity: john smith  
Tail Entity: chief of staff  

Relation: military rank  
Context: colonel sarah jones led her battalion with distinction during the peacekeeping mission in the middle east, earning her a commendation for her leadership.  
Head Entity: sarah jones  
Tail Entity: colonel  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham, set to hit the shelves on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular online platform Wattpad has been instrumental in launching the careers of many writers, including Anna Todd, whose series "After" gained immense popularity.  
Head Entity: Anna Todd  
Tail Entity: Wattpad  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " choose you " and " homesick " were released as the album 's second and third singles , respectively , and each attained moderate chart success .
Head Entity: choose you
Tail Entity: homesick
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter delves into the backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: The opening act of the concert was electrifying, and it was followed by a stunning performance from the headliner.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: he was also associated with robert wilkinson in producing " londina illustrata " , an illustrated account of ancient buildings in london and westminster in two volumes ( 1819–25 ) .
Head Entity: robert wilkinson
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in the bustling city of San Francisco, where it has been operating since its inception in 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her tenure at the university, she conducted groundbreaking research in neuroscience, primarily based in the labs located in Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has developed several innovative applications for mobile devices.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: ancient city of Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Losses:  4.338859558105469 0.3691754937171936 0.9701004028320312
MemoryTrain:  epoch  0, batch     0 | loss: 4.3388596Losses:  4.505436897277832 0.23986178636550903 0.8441686630249023
MemoryTrain:  epoch  0, batch     1 | loss: 4.5054369Losses:  4.50630521774292 0.34753912687301636 0.8964327573776245
MemoryTrain:  epoch  0, batch     2 | loss: 4.5063052Losses:  5.397408485412598 0.24813857674598694 0.8160829544067383
MemoryTrain:  epoch  0, batch     3 | loss: 5.3974085Losses:  5.034043788909912 0.5365886688232422 0.9334887266159058
MemoryTrain:  epoch  0, batch     4 | loss: 5.0340438Losses:  4.037520885467529 0.5329265594482422 0.7690544724464417
MemoryTrain:  epoch  0, batch     5 | loss: 4.0375209Losses:  3.970829963684082 0.2621350884437561 0.909525990486145
MemoryTrain:  epoch  0, batch     6 | loss: 3.9708300Losses:  5.07051944732666 0.22958213090896606 0.9833478331565857
MemoryTrain:  epoch  0, batch     7 | loss: 5.0705194Losses:  5.283845901489258 0.7036450505256653 0.8837658762931824
MemoryTrain:  epoch  0, batch     8 | loss: 5.2838459Losses:  2.959270715713501 -0.0 0.4831309914588928
MemoryTrain:  epoch  0, batch     9 | loss: 2.9592707Losses:  5.2961249351501465 0.9834882020950317 0.8786982297897339
MemoryTrain:  epoch  1, batch     0 | loss: 5.2961249Losses:  4.938629627227783 0.4486280083656311 0.9292256236076355
MemoryTrain:  epoch  1, batch     1 | loss: 4.9386296Losses:  3.7990734577178955 -0.0 0.8114721179008484
MemoryTrain:  epoch  1, batch     2 | loss: 3.7990735Losses:  3.136808156967163 0.22659359872341156 0.801713764667511
MemoryTrain:  epoch  1, batch     3 | loss: 3.1368082Losses:  4.046498775482178 0.5147186517715454 0.7979286313056946
MemoryTrain:  epoch  1, batch     4 | loss: 4.0464988Losses:  3.695793390274048 0.4610961675643921 0.7512953281402588
MemoryTrain:  epoch  1, batch     5 | loss: 3.6957934Losses:  4.45512580871582 0.2856876850128174 0.9432144165039062
MemoryTrain:  epoch  1, batch     6 | loss: 4.4551258Losses:  3.311941623687744 -0.0 0.9579064249992371
MemoryTrain:  epoch  1, batch     7 | loss: 3.3119416Losses:  3.936058521270752 -0.0 0.9386667013168335
MemoryTrain:  epoch  1, batch     8 | loss: 3.9360585Losses:  2.361781358718872 -0.0 0.5211939811706543
MemoryTrain:  epoch  1, batch     9 | loss: 2.3617814Losses:  3.5001068115234375 0.5124207735061646 0.8570947051048279
MemoryTrain:  epoch  2, batch     0 | loss: 3.5001068Losses:  4.167806625366211 0.9416506886482239 0.7993025183677673
MemoryTrain:  epoch  2, batch     1 | loss: 4.1678066Losses:  4.474484443664551 0.3256257474422455 0.9155076146125793
MemoryTrain:  epoch  2, batch     2 | loss: 4.4744844Losses:  4.443606376647949 0.92193603515625 0.8149397969245911
MemoryTrain:  epoch  2, batch     3 | loss: 4.4436064Losses:  3.6200339794158936 0.5508908033370972 0.8989898562431335
MemoryTrain:  epoch  2, batch     4 | loss: 3.6200340Losses:  4.158304214477539 0.8028122186660767 0.7863545417785645
MemoryTrain:  epoch  2, batch     5 | loss: 4.1583042Losses:  3.6836695671081543 0.8170857429504395 0.9259581565856934
MemoryTrain:  epoch  2, batch     6 | loss: 3.6836696Losses:  3.5470426082611084 0.5325676202774048 0.9621899127960205
MemoryTrain:  epoch  2, batch     7 | loss: 3.5470426Losses:  3.795283317565918 0.2959568500518799 0.9453083276748657
MemoryTrain:  epoch  2, batch     8 | loss: 3.7952833Losses:  2.3059868812561035 -0.0 0.5338287949562073
MemoryTrain:  epoch  2, batch     9 | loss: 2.3059869Losses:  2.943148374557495 0.2670633792877197 0.8184863924980164
MemoryTrain:  epoch  3, batch     0 | loss: 2.9431484Losses:  4.094282627105713 0.9211981296539307 0.8631219267845154
MemoryTrain:  epoch  3, batch     1 | loss: 4.0942826Losses:  3.341277599334717 0.7661568522453308 0.8076462149620056
MemoryTrain:  epoch  3, batch     2 | loss: 3.3412776Losses:  2.852163791656494 -0.0 0.8676644563674927
MemoryTrain:  epoch  3, batch     3 | loss: 2.8521638Losses:  3.378527879714966 0.5460271835327148 0.8507072329521179
MemoryTrain:  epoch  3, batch     4 | loss: 3.3785279Losses:  4.804073810577393 0.9311540722846985 0.9633211493492126
MemoryTrain:  epoch  3, batch     5 | loss: 4.8040738Losses:  3.1492414474487305 1.0059449672698975 0.6676949262619019
MemoryTrain:  epoch  3, batch     6 | loss: 3.1492414Losses:  3.3776071071624756 0.5295140147209167 0.8090042471885681
MemoryTrain:  epoch  3, batch     7 | loss: 3.3776071Losses:  3.0526626110076904 -0.0 0.9276769161224365
MemoryTrain:  epoch  3, batch     8 | loss: 3.0526626Losses:  3.0215272903442383 -0.0 0.4278925061225891
MemoryTrain:  epoch  3, batch     9 | loss: 3.0215273Losses:  3.3951380252838135 0.2546028196811676 0.9354979991912842
MemoryTrain:  epoch  4, batch     0 | loss: 3.3951380Losses:  3.0215842723846436 0.2599188983440399 0.7976875305175781
MemoryTrain:  epoch  4, batch     1 | loss: 3.0215843Losses:  2.91143798828125 0.2754678726196289 0.794658362865448
MemoryTrain:  epoch  4, batch     2 | loss: 2.9114380Losses:  3.576406240463257 0.7846465110778809 0.7152736783027649
MemoryTrain:  epoch  4, batch     3 | loss: 3.5764062Losses:  3.4863011837005615 0.7794184684753418 0.9802996516227722
MemoryTrain:  epoch  4, batch     4 | loss: 3.4863012Losses:  2.8211116790771484 -0.0 0.8746211528778076
MemoryTrain:  epoch  4, batch     5 | loss: 2.8211117Losses:  2.6287269592285156 -0.0 0.9363727569580078
MemoryTrain:  epoch  4, batch     6 | loss: 2.6287270Losses:  2.4794349670410156 -0.0 0.8060956001281738
MemoryTrain:  epoch  4, batch     7 | loss: 2.4794350Losses:  2.6339988708496094 -0.0 0.9732353687286377
MemoryTrain:  epoch  4, batch     8 | loss: 2.6339989Losses:  2.9480628967285156 -0.0 0.4731353521347046
MemoryTrain:  epoch  4, batch     9 | loss: 2.9480629Losses:  2.9651522636413574 0.28553280234336853 0.837079644203186
MemoryTrain:  epoch  5, batch     0 | loss: 2.9651523Losses:  3.3040554523468018 0.4785425066947937 0.9212684631347656
MemoryTrain:  epoch  5, batch     1 | loss: 3.3040555Losses:  3.1135683059692383 0.2920072078704834 0.861452043056488
MemoryTrain:  epoch  5, batch     2 | loss: 3.1135683Losses:  2.631502628326416 0.2447347640991211 0.9658247232437134
MemoryTrain:  epoch  5, batch     3 | loss: 2.6315026Losses:  2.823233127593994 0.5183559656143188 0.853873610496521
MemoryTrain:  epoch  5, batch     4 | loss: 2.8232331Losses:  2.4294815063476562 -0.0 0.964025616645813
MemoryTrain:  epoch  5, batch     5 | loss: 2.4294815Losses:  3.7265138626098633 1.631042242050171 0.6773310899734497
MemoryTrain:  epoch  5, batch     6 | loss: 3.7265139Losses:  4.403851509094238 1.4911510944366455 0.7204102873802185
MemoryTrain:  epoch  5, batch     7 | loss: 4.4038515Losses:  3.553196430206299 0.5250707864761353 0.8837023973464966
MemoryTrain:  epoch  5, batch     8 | loss: 3.5531964Losses:  2.211326837539673 -0.0 0.5262349843978882
MemoryTrain:  epoch  5, batch     9 | loss: 2.2113268Losses:  2.4227588176727295 0.26663443446159363 0.6869415640830994
MemoryTrain:  epoch  6, batch     0 | loss: 2.4227588Losses:  2.707918643951416 0.2755458950996399 0.8028870820999146
MemoryTrain:  epoch  6, batch     1 | loss: 2.7079186Losses:  2.9091484546661377 0.26030078530311584 0.9143666625022888
MemoryTrain:  epoch  6, batch     2 | loss: 2.9091485Losses:  3.2069170475006104 0.3005180060863495 0.9371767044067383
MemoryTrain:  epoch  6, batch     3 | loss: 3.2069170Losses:  2.666005849838257 0.500333845615387 0.8493632674217224
MemoryTrain:  epoch  6, batch     4 | loss: 2.6660058Losses:  3.3987932205200195 0.8652361631393433 0.7907995581626892
MemoryTrain:  epoch  6, batch     5 | loss: 3.3987932Losses:  2.742964029312134 0.25043416023254395 0.8406617045402527
MemoryTrain:  epoch  6, batch     6 | loss: 2.7429640Losses:  2.6699883937835693 0.2957207262516022 0.8069453239440918
MemoryTrain:  epoch  6, batch     7 | loss: 2.6699884Losses:  3.413214921951294 1.1049962043762207 0.9276800155639648
MemoryTrain:  epoch  6, batch     8 | loss: 3.4132149Losses:  1.9627079963684082 -0.0 0.43412157893180847
MemoryTrain:  epoch  6, batch     9 | loss: 1.9627080Losses:  2.982640027999878 0.5693093538284302 0.7812358736991882
MemoryTrain:  epoch  7, batch     0 | loss: 2.9826400Losses:  3.1341192722320557 0.500419020652771 0.8655168414115906
MemoryTrain:  epoch  7, batch     1 | loss: 3.1341193Losses:  2.34395170211792 -0.0 0.9629935026168823
MemoryTrain:  epoch  7, batch     2 | loss: 2.3439517Losses:  3.3977110385894775 0.5760661959648132 0.801784336566925
MemoryTrain:  epoch  7, batch     3 | loss: 3.3977110Losses:  2.8526320457458496 0.5356751680374146 0.9191632270812988
MemoryTrain:  epoch  7, batch     4 | loss: 2.8526320Losses:  2.875124931335449 0.5279426574707031 0.8689610362052917
MemoryTrain:  epoch  7, batch     5 | loss: 2.8751249Losses:  2.628469944000244 0.2336711883544922 1.0108481645584106
MemoryTrain:  epoch  7, batch     6 | loss: 2.6284699Losses:  3.6769509315490723 1.1966620683670044 0.8773688077926636
MemoryTrain:  epoch  7, batch     7 | loss: 3.6769509Losses:  2.521355152130127 0.5001832246780396 0.5891456007957458
MemoryTrain:  epoch  7, batch     8 | loss: 2.5213552Losses:  1.6424598693847656 -0.0 0.32815036177635193
MemoryTrain:  epoch  7, batch     9 | loss: 1.6424599Losses:  2.8352198600769043 0.5046669840812683 0.8503570556640625
MemoryTrain:  epoch  8, batch     0 | loss: 2.8352199Losses:  2.719355821609497 0.5195477604866028 0.7782109379768372
MemoryTrain:  epoch  8, batch     1 | loss: 2.7193558Losses:  2.2551543712615967 -0.0 0.9143161177635193
MemoryTrain:  epoch  8, batch     2 | loss: 2.2551544Losses:  2.5740432739257812 0.4814397096633911 0.7238742113113403
MemoryTrain:  epoch  8, batch     3 | loss: 2.5740433Losses:  2.173346519470215 -0.0 0.7216489911079407
MemoryTrain:  epoch  8, batch     4 | loss: 2.1733465Losses:  2.909377098083496 0.4738815426826477 0.9235080480575562
MemoryTrain:  epoch  8, batch     5 | loss: 2.9093771Losses:  3.0258548259735107 0.8824455738067627 0.8642011284828186
MemoryTrain:  epoch  8, batch     6 | loss: 3.0258548Losses:  2.512077808380127 0.2685808539390564 0.9148248434066772
MemoryTrain:  epoch  8, batch     7 | loss: 2.5120778Losses:  2.5775928497314453 0.2699148952960968 0.8427881002426147
MemoryTrain:  epoch  8, batch     8 | loss: 2.5775928Losses:  2.191824197769165 -0.0 0.4122166633605957
MemoryTrain:  epoch  8, batch     9 | loss: 2.1918242Losses:  2.8698832988739014 0.5005679130554199 0.8503782153129578
MemoryTrain:  epoch  9, batch     0 | loss: 2.8698833Losses:  2.6945061683654785 0.5000010132789612 0.8233906030654907
MemoryTrain:  epoch  9, batch     1 | loss: 2.6945062Losses:  2.921138286590576 0.5383628010749817 0.9248980283737183
MemoryTrain:  epoch  9, batch     2 | loss: 2.9211383Losses:  2.7873473167419434 0.5551621913909912 0.8382213115692139
MemoryTrain:  epoch  9, batch     3 | loss: 2.7873473Losses:  2.465284824371338 -0.0 0.9938483238220215
MemoryTrain:  epoch  9, batch     4 | loss: 2.4652848Losses:  2.433229446411133 0.5038815140724182 0.6388897895812988
MemoryTrain:  epoch  9, batch     5 | loss: 2.4332294Losses:  2.3314154148101807 -0.0 0.9141284823417664
MemoryTrain:  epoch  9, batch     6 | loss: 2.3314154Losses:  2.312351703643799 0.2407515048980713 0.753513753414154
MemoryTrain:  epoch  9, batch     7 | loss: 2.3123517Losses:  3.1163315773010254 0.5347235202789307 0.9024144411087036
MemoryTrain:  epoch  9, batch     8 | loss: 3.1163316Losses:  1.759860634803772 -0.0 0.44284820556640625
MemoryTrain:  epoch  9, batch     9 | loss: 1.7598606
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 67.36%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 65.34%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 67.86%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 69.17%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 70.22%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 71.18%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 77.64%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 76.39%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 76.12%   [EVAL] batch:   28 | acc: 68.75%,  total acc: 75.86%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 75.21%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 74.61%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 74.24%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 72.43%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 71.61%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 69.97%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 68.58%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 68.59%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 69.06%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 69.05%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 69.49%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 69.77%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 69.74%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 69.31%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 69.16%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 69.02%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 68.49%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 68.11%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 68.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 69.93%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 70.49%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.54%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 72.04%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 72.41%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 72.88%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 73.77%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 74.09%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.71%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 83.93%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 81.94%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 84.87%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 84.23%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 84.24%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.64%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 84.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 85.65%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 86.21%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.10%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 87.32%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 86.96%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 87.16%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 87.17%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.81%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 87.96%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 87.95%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 87.79%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 87.64%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 87.22%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 86.96%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 86.57%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 86.20%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 86.10%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 85.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 85.10%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 85.14%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 84.72%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 84.20%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 84.04%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 83.99%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 83.84%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 84.00%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 84.17%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 84.32%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   62 | acc: 56.25%,  total acc: 83.93%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 82.71%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 81.63%   [EVAL] batch:   65 | acc: 25.00%,  total acc: 80.78%   [EVAL] batch:   66 | acc: 18.75%,  total acc: 79.85%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 78.86%   [EVAL] batch:   68 | acc: 50.00%,  total acc: 78.44%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 78.39%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 78.70%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 79.20%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 79.22%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 79.25%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 79.52%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 79.55%   [EVAL] batch:   77 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   78 | acc: 100.00%,  total acc: 80.06%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 80.23%   [EVAL] batch:   80 | acc: 93.75%,  total acc: 80.40%   [EVAL] batch:   81 | acc: 43.75%,  total acc: 79.95%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 79.44%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 78.94%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 78.90%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 78.42%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 77.87%   [EVAL] batch:   87 | acc: 25.00%,  total acc: 77.27%   [EVAL] batch:   88 | acc: 12.50%,  total acc: 76.54%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 75.90%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 75.07%   [EVAL] batch:   91 | acc: 18.75%,  total acc: 74.46%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 73.72%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 73.14%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 73.16%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 73.31%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 73.85%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 73.93%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 74.00%   [EVAL] batch:  100 | acc: 93.75%,  total acc: 74.20%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 74.33%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 74.58%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 74.82%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 75.18%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 74.82%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 74.25%   [EVAL] batch:  108 | acc: 0.00%,  total acc: 73.57%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 72.95%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 72.47%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 71.88%   [EVAL] batch:  112 | acc: 18.75%,  total acc: 71.40%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 71.22%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 71.03%   [EVAL] batch:  115 | acc: 93.75%,  total acc: 71.23%   [EVAL] batch:  116 | acc: 50.00%,  total acc: 71.05%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 70.97%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 70.85%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 70.62%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 70.35%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 70.18%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 70.12%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 70.21%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 70.00%   [EVAL] batch:  125 | acc: 37.50%,  total acc: 69.74%   [EVAL] batch:  126 | acc: 18.75%,  total acc: 69.34%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 68.99%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 68.70%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 68.51%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 68.37%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 69.07%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 69.26%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 69.48%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 69.66%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 69.69%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 69.64%   [EVAL] batch:  140 | acc: 56.25%,  total acc: 69.55%   [EVAL] batch:  141 | acc: 62.50%,  total acc: 69.50%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 69.54%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 69.57%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 69.57%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 69.65%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 69.69%   [EVAL] batch:  147 | acc: 62.50%,  total acc: 69.64%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 69.67%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 69.67%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 69.33%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 68.96%   [EVAL] batch:  152 | acc: 12.50%,  total acc: 68.59%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 68.22%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 67.90%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 67.59%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 67.48%   [EVAL] batch:  157 | acc: 43.75%,  total acc: 67.33%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 67.22%   [EVAL] batch:  159 | acc: 43.75%,  total acc: 67.07%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:  161 | acc: 56.25%,  total acc: 67.01%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 66.95%   [EVAL] batch:  163 | acc: 75.00%,  total acc: 67.00%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 67.08%   [EVAL] batch:  165 | acc: 81.25%,  total acc: 67.17%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 67.22%   [EVAL] batch:  167 | acc: 87.50%,  total acc: 67.34%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 67.31%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 67.06%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 66.89%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 66.72%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 66.58%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 66.38%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 66.29%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 66.67%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  182 | acc: 81.25%,  total acc: 67.66%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 67.94%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 68.08%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 68.15%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 68.12%   [EVAL] batch:  188 | acc: 43.75%,  total acc: 67.99%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 67.86%   [EVAL] batch:  190 | acc: 56.25%,  total acc: 67.80%   [EVAL] batch:  191 | acc: 62.50%,  total acc: 67.77%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 67.78%   [EVAL] batch:  193 | acc: 56.25%,  total acc: 67.72%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 67.76%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 67.76%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 67.70%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 67.71%   [EVAL] batch:  198 | acc: 56.25%,  total acc: 67.65%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 67.72%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 67.79%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 67.85%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 67.86%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 67.92%   [EVAL] batch:  204 | acc: 56.25%,  total acc: 67.87%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 67.81%   [EVAL] batch:  207 | acc: 0.00%,  total acc: 67.49%   [EVAL] batch:  208 | acc: 25.00%,  total acc: 67.28%   [EVAL] batch:  209 | acc: 0.00%,  total acc: 66.96%   [EVAL] batch:  210 | acc: 12.50%,  total acc: 66.71%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 66.51%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 66.46%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.62%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.20%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 67.32%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.47%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.62%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 67.85%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.14%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 68.20%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 68.23%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 68.26%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 68.29%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 68.40%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 68.43%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 68.35%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 68.24%   [EVAL] batch:  233 | acc: 50.00%,  total acc: 68.16%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 67.98%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 67.93%   [EVAL] batch:  236 | acc: 43.75%,  total acc: 67.83%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 67.86%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 67.94%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 68.13%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 68.21%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 68.26%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 68.37%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 68.42%   [EVAL] batch:  246 | acc: 75.00%,  total acc: 68.45%   [EVAL] batch:  247 | acc: 62.50%,  total acc: 68.42%   [EVAL] batch:  248 | acc: 68.75%,  total acc: 68.42%   [EVAL] batch:  249 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  250 | acc: 56.25%,  total acc: 68.48%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 68.50%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 68.53%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 68.48%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 68.48%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 68.53%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 68.46%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 68.48%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.48%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 68.49%   [EVAL] batch:  260 | acc: 43.75%,  total acc: 68.39%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 68.39%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 68.42%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 68.49%   [EVAL] batch:  264 | acc: 87.50%,  total acc: 68.56%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 68.59%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 68.63%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 68.70%   [EVAL] batch:  268 | acc: 93.75%,  total acc: 68.80%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  273 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 69.39%   [EVAL] batch:  275 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:  276 | acc: 43.75%,  total acc: 69.29%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 69.29%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 69.29%   [EVAL] batch:  279 | acc: 56.25%,  total acc: 69.24%   [EVAL] batch:  280 | acc: 68.75%,  total acc: 69.24%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 69.22%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 69.19%   [EVAL] batch:  283 | acc: 12.50%,  total acc: 68.99%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 68.90%   [EVAL] batch:  285 | acc: 12.50%,  total acc: 68.71%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 68.53%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 68.55%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 68.53%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 68.60%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 68.60%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 68.66%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 68.71%   [EVAL] batch:  293 | acc: 68.75%,  total acc: 68.71%   [EVAL] batch:  294 | acc: 50.00%,  total acc: 68.64%   [EVAL] batch:  295 | acc: 62.50%,  total acc: 68.62%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 68.60%   [EVAL] batch:  297 | acc: 43.75%,  total acc: 68.52%   [EVAL] batch:  298 | acc: 50.00%,  total acc: 68.46%   [EVAL] batch:  299 | acc: 68.75%,  total acc: 68.46%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 68.77%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 69.26%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 69.63%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 69.57%   
cur_acc:  ['0.9494', '0.6270', '0.7133', '0.7024', '0.7371']
his_acc:  ['0.9494', '0.7820', '0.7500', '0.7208', '0.6957']
Clustering into  29  clusters
Clusters:  [12  3 25  4  2  2 18  5 26  6  2  1  7 15  4 16 22 14 13 26  9 12  5  2
 15  6 12  2  1  1 27 13 19  1 17  1  8  9  0  9  3 23 10  0  5 24 14 20
 11 13  0 21 28 10  4  2  7 12  5  0]
Losses:  10.565059661865234 2.912937641143799 0.6836892366409302
CurrentTrain: epoch  0, batch     0 | loss: 10.5650597Losses:  10.319079399108887 3.270719289779663 0.7042911052703857
CurrentTrain: epoch  0, batch     1 | loss: 10.3190794Losses:  10.930384635925293 3.99884033203125 0.7496024966239929
CurrentTrain: epoch  0, batch     2 | loss: 10.9303846Losses:  7.272922515869141 -0.0 0.13359808921813965
CurrentTrain: epoch  0, batch     3 | loss: 7.2729225Losses:  12.887871742248535 4.927276134490967 0.7628697156906128
CurrentTrain: epoch  1, batch     0 | loss: 12.8878717Losses:  8.209726333618164 2.3566336631774902 0.7400708198547363
CurrentTrain: epoch  1, batch     1 | loss: 8.2097263Losses:  8.40168571472168 3.2733747959136963 0.6986762285232544
CurrentTrain: epoch  1, batch     2 | loss: 8.4016857Losses:  3.8402507305145264 -0.0 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 3.8402507Losses:  7.90150260925293 2.5833792686462402 0.7160162329673767
CurrentTrain: epoch  2, batch     0 | loss: 7.9015026Losses:  8.578523635864258 3.3396074771881104 0.6706825494766235
CurrentTrain: epoch  2, batch     1 | loss: 8.5785236Losses:  12.263466835021973 5.310520172119141 0.6855939626693726
CurrentTrain: epoch  2, batch     2 | loss: 12.2634668Losses:  3.12943434715271 -0.0 0.09281642735004425
CurrentTrain: epoch  2, batch     3 | loss: 3.1294343Losses:  7.255520820617676 2.5505690574645996 0.7079678773880005
CurrentTrain: epoch  3, batch     0 | loss: 7.2555208Losses:  7.56035041809082 2.352043628692627 0.724762499332428
CurrentTrain: epoch  3, batch     1 | loss: 7.5603504Losses:  8.478436470031738 3.038271903991699 0.6844592094421387
CurrentTrain: epoch  3, batch     2 | loss: 8.4784365Losses:  8.668367385864258 -0.0 0.12423703074455261
CurrentTrain: epoch  3, batch     3 | loss: 8.6683674Losses:  8.085956573486328 3.2024970054626465 0.7211858034133911
CurrentTrain: epoch  4, batch     0 | loss: 8.0859566Losses:  8.068442344665527 3.230910301208496 0.5815905928611755
CurrentTrain: epoch  4, batch     1 | loss: 8.0684423Losses:  8.5293550491333 3.0638813972473145 0.7485024333000183
CurrentTrain: epoch  4, batch     2 | loss: 8.5293550Losses:  3.24676775932312 -0.0 0.12923631072044373
CurrentTrain: epoch  4, batch     3 | loss: 3.2467678Losses:  7.73224401473999 2.2126615047454834 0.7302458882331848
CurrentTrain: epoch  5, batch     0 | loss: 7.7322440Losses:  8.090611457824707 3.148895740509033 0.7355901002883911
CurrentTrain: epoch  5, batch     1 | loss: 8.0906115Losses:  8.033489227294922 4.089969635009766 0.5878037214279175
CurrentTrain: epoch  5, batch     2 | loss: 8.0334892Losses:  3.1471023559570312 -0.0 0.15334275364875793
CurrentTrain: epoch  5, batch     3 | loss: 3.1471024Losses:  7.062924385070801 2.4556219577789307 0.678912878036499
CurrentTrain: epoch  6, batch     0 | loss: 7.0629244Losses:  7.121631145477295 2.575915575027466 0.7188485264778137
CurrentTrain: epoch  6, batch     1 | loss: 7.1216311Losses:  7.515463352203369 3.5815882682800293 0.570860743522644
CurrentTrain: epoch  6, batch     2 | loss: 7.5154634Losses:  5.357660293579102 -0.0 0.1265324354171753
CurrentTrain: epoch  6, batch     3 | loss: 5.3576603Losses:  6.759127616882324 2.7749898433685303 0.6754353046417236
CurrentTrain: epoch  7, batch     0 | loss: 6.7591276Losses:  7.395212173461914 3.3425369262695312 0.5576553344726562
CurrentTrain: epoch  7, batch     1 | loss: 7.3952122Losses:  8.409173011779785 3.5878491401672363 0.6858193874359131
CurrentTrain: epoch  7, batch     2 | loss: 8.4091730Losses:  4.76944637298584 -0.0 0.14615243673324585
CurrentTrain: epoch  7, batch     3 | loss: 4.7694464Losses:  7.689332485198975 3.3894262313842773 0.6709486842155457
CurrentTrain: epoch  8, batch     0 | loss: 7.6893325Losses:  5.873745918273926 1.931265115737915 0.7032053470611572
CurrentTrain: epoch  8, batch     1 | loss: 5.8737459Losses:  6.715735912322998 2.560645818710327 0.6382574439048767
CurrentTrain: epoch  8, batch     2 | loss: 6.7157359Losses:  3.753880262374878 -0.0 0.08736670762300491
CurrentTrain: epoch  8, batch     3 | loss: 3.7538803Losses:  8.086697578430176 5.4425764083862305 0.5427313446998596
CurrentTrain: epoch  9, batch     0 | loss: 8.0866976Losses:  7.212238311767578 3.0251574516296387 0.5891847014427185
CurrentTrain: epoch  9, batch     1 | loss: 7.2122383Losses:  7.918787956237793 3.493830680847168 0.5934211611747742
CurrentTrain: epoch  9, batch     2 | loss: 7.9187880Losses:  4.797253131866455 -0.0 0.14325574040412903
CurrentTrain: epoch  9, batch     3 | loss: 4.7972531
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: the " tetrabiblos " was largely responsible for laying down the basic precepts of renaissance astrology , webster ( 1979 ) p.276 .
Head Entity: tetrabiblos
Tail Entity: astrology
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: In her groundbreaking book, "The Second Sex," Simone de Beauvoir explores the role of women in society and the concept of femininity, which has influenced feminist theory significantly.  
Head Entity: The Second Sex  
Tail Entity: feminist theory  

Relation: main subject  
Context: The novel "1984" by George Orwell delves into themes of totalitarianism and surveillance, making it a critical work in discussions about privacy and government control.  
Head Entity: 1984  
Tail Entity: totalitarianism  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: group b of uefa euro 2016 contained england , russia , wales and slovakia .
Head Entity: uefa euro 2016
Tail Entity: slovakia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: the 2020 summer olympics featured countries like the united states, china, and japan competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: japan  

Relation: participating team  
Context: the fifa world cup 2018 saw teams such as france, croatia, and belgium battling for the championship title.  
Head Entity: fifa world cup 2018  
Tail Entity: croatia  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Judas Iscariot, who is depicted as part of the group gathered around Jesus.  
Head Entity: The Last Supper  
Tail Entity: Judas Iscariot  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with several components, including a powerful camera system that is part of its advanced features.  
Head Entity: Galaxy S21  
Tail Entity: camera system  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: for her performance in the musical - drama " crazy heart " ( 2009 ) , she was nominated for the academy award for best supporting actress .
Head Entity: crazy heart
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: The film "The Shape of Water" received critical acclaim and was nominated for several prestigious awards, including the Academy Award for Best Picture.  
Head Entity: The Shape of Water  
Tail Entity: Academy Award for Best Picture  

Relation: nominated for  
Context: In 2021, the popular series "The Crown" was nominated for the Primetime Emmy Award for Outstanding Drama Series, showcasing its impact on television.  
Head Entity: The Crown  
Tail Entity: Primetime Emmy Award for Outstanding Drama Series  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: on march 2015 , cube entertainment launched the multi - national girl group clc including original members seunghee , yujin , seungyeon , sorn and yeeun .
Head Entity: clc
Tail Entity: girl group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone, known for its sleek design and advanced features, was released by the tech giant Apple in 2020.  
Head Entity: iPhone 12  
Tail Entity: smartphone  

Relation: instance of  
Context: The famous painting, created by Leonardo da Vinci, is considered a masterpiece of the Renaissance period.  
Head Entity: Mona Lisa  
Tail Entity: painting  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: brand peak is a sharp snow - covered peak located east - southeast of the eternity range and northwest of mount duemler , in palmer land .
Head Entity: mount duemler
Tail Entity: eternity range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the rocky mountains extend from Canada through the United States and into Mexico, providing a stunning backdrop for outdoor enthusiasts.  
Head Entity: rocky mountains  
Tail Entity: Canada  

Relation: mountain range  
Context: the appalachian mountains are known for their rolling hills and rich biodiversity, stretching from Georgia to Maine.  
Head Entity: appalachian mountains  
Tail Entity: Maine  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: future films include " mcqueen " , ian bonhote ’s documentary about the fashion designer alexander mcqueen , brad anderson ’s thriller " beirut " , and mark pellington ’s drama " nostalgia " .
Head Entity: nostalgia
Tail Entity: mark pellington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the upcoming movie "the great adventure" is penned by renowned screenwriter jessica taylor, who has previously won multiple awards for her work in the industry.  
Head Entity: the great adventure  
Tail Entity: jessica taylor  

Relation: screenwriter  
Context: in the latest film "shadows of the past," the script was crafted by the talented screenwriter robert lang, known for his gripping storytelling and character development.  
Head Entity: shadows of the past  
Tail Entity: robert lang  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative and is primarily produced in English, appealing to a global audience.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: English  

Relation: language of work or name  
Context: The famous novel "Cien años de soledad" by Gabriel García Márquez is celebrated for its magical realism and is originally written in Spanish, captivating readers worldwide.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: udasi has been an early sect based on the teachings of sri chand ( 1494–1643 ) , the son of guru nanak , the founder and the first guru of sikhism .
Head Entity: guru nanak
Tail Entity: sikhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the baha'i faith is a religion founded by baha'u'llah in the 19th century, emphasizing the spiritual unity of all humankind.  
Head Entity: baha'u'llah  
Tail Entity: baha'i faith  

Relation: religion  
Context: the ancient greeks practiced a polytheistic religion that included a pantheon of gods and goddesses, such as zeus and athena.  
Head Entity: ancient greeks  
Tail Entity: polytheistic religion  
Losses:  4.987725257873535 -0.0 0.9659833908081055
MemoryTrain:  epoch  0, batch     0 | loss: 4.9877253Losses:  3.7724199295043945 0.6248307228088379 0.8867225646972656
MemoryTrain:  epoch  0, batch     1 | loss: 3.7724199Losses:  5.016218185424805 0.9883981943130493 0.9761993885040283
MemoryTrain:  epoch  0, batch     2 | loss: 5.0162182Losses:  4.650321006774902 0.2360660880804062 0.8608508110046387
MemoryTrain:  epoch  0, batch     3 | loss: 4.6503210Losses:  4.406682014465332 0.25950950384140015 0.9683946967124939
MemoryTrain:  epoch  0, batch     4 | loss: 4.4066820Losses:  4.572849750518799 0.7656346559524536 0.8664540648460388
MemoryTrain:  epoch  0, batch     5 | loss: 4.5728498Losses:  4.147492408752441 0.23203374445438385 0.9409677386283875
MemoryTrain:  epoch  0, batch     6 | loss: 4.1474924Losses:  4.383658409118652 0.5114378929138184 0.8729130625724792
MemoryTrain:  epoch  0, batch     7 | loss: 4.3836584Losses:  4.271455764770508 0.5714634656906128 0.87910395860672
MemoryTrain:  epoch  0, batch     8 | loss: 4.2714558Losses:  4.2651872634887695 0.5198221206665039 0.8762280941009521
MemoryTrain:  epoch  0, batch     9 | loss: 4.2651873Losses:  4.569916725158691 0.2454499900341034 1.0484442710876465
MemoryTrain:  epoch  0, batch    10 | loss: 4.5699167Losses:  3.7857556343078613 -0.0 0.20839445292949677
MemoryTrain:  epoch  0, batch    11 | loss: 3.7857556Losses:  3.700352191925049 0.7515087127685547 0.9419612884521484
MemoryTrain:  epoch  1, batch     0 | loss: 3.7003522Losses:  3.878915548324585 -0.0 0.9540244936943054
MemoryTrain:  epoch  1, batch     1 | loss: 3.8789155Losses:  3.9058990478515625 -0.0 1.0480196475982666
MemoryTrain:  epoch  1, batch     2 | loss: 3.9058990Losses:  4.2899346351623535 0.4621545076370239 0.9409955739974976
MemoryTrain:  epoch  1, batch     3 | loss: 4.2899346Losses:  3.4754600524902344 0.5313271880149841 0.8660720586776733
MemoryTrain:  epoch  1, batch     4 | loss: 3.4754601Losses:  3.8686563968658447 0.5152925252914429 0.8549392223358154
MemoryTrain:  epoch  1, batch     5 | loss: 3.8686564Losses:  3.33377742767334 -0.0 0.9825134873390198
MemoryTrain:  epoch  1, batch     6 | loss: 3.3337774Losses:  3.7072501182556152 0.2151937484741211 0.8096472024917603
MemoryTrain:  epoch  1, batch     7 | loss: 3.7072501Losses:  4.360805988311768 0.24070486426353455 0.9752071499824524
MemoryTrain:  epoch  1, batch     8 | loss: 4.3608060Losses:  3.679124116897583 -0.0 0.9667825102806091
MemoryTrain:  epoch  1, batch     9 | loss: 3.6791241Losses:  3.426119804382324 0.5011802315711975 0.8697766065597534
MemoryTrain:  epoch  1, batch    10 | loss: 3.4261198Losses:  2.198556423187256 -0.0 0.2089400291442871
MemoryTrain:  epoch  1, batch    11 | loss: 2.1985564Losses:  3.1601905822753906 0.24674555659294128 1.0104073286056519
MemoryTrain:  epoch  2, batch     0 | loss: 3.1601906Losses:  3.9816079139709473 0.48039644956588745 0.862642228603363
MemoryTrain:  epoch  2, batch     1 | loss: 3.9816079Losses:  3.7305235862731934 0.49369844794273376 0.9824265241622925
MemoryTrain:  epoch  2, batch     2 | loss: 3.7305236Losses:  3.3062477111816406 0.2757877707481384 0.8125028014183044
MemoryTrain:  epoch  2, batch     3 | loss: 3.3062477Losses:  3.1913750171661377 -0.0 0.8736985325813293
MemoryTrain:  epoch  2, batch     4 | loss: 3.1913750Losses:  3.5365796089172363 0.24977850914001465 0.9672166705131531
MemoryTrain:  epoch  2, batch     5 | loss: 3.5365796Losses:  2.7500274181365967 0.2578985095024109 0.7787876725196838
MemoryTrain:  epoch  2, batch     6 | loss: 2.7500274Losses:  3.56274151802063 0.481376051902771 0.8617932200431824
MemoryTrain:  epoch  2, batch     7 | loss: 3.5627415Losses:  3.036491632461548 0.27026882767677307 0.8305962681770325
MemoryTrain:  epoch  2, batch     8 | loss: 3.0364916Losses:  3.297330379486084 0.2716481685638428 0.974535346031189
MemoryTrain:  epoch  2, batch     9 | loss: 3.2973304Losses:  2.863377332687378 0.24597133696079254 0.857785701751709
MemoryTrain:  epoch  2, batch    10 | loss: 2.8633773Losses:  3.119791269302368 -0.0 0.31089067459106445
MemoryTrain:  epoch  2, batch    11 | loss: 3.1197913Losses:  3.037198066711426 0.2719240188598633 0.84942626953125
MemoryTrain:  epoch  3, batch     0 | loss: 3.0371981Losses:  2.963217258453369 0.27786898612976074 0.8717879056930542
MemoryTrain:  epoch  3, batch     1 | loss: 2.9632173Losses:  2.680504560470581 0.2594415545463562 0.953620195388794
MemoryTrain:  epoch  3, batch     2 | loss: 2.6805046Losses:  3.476175308227539 0.3216361105442047 0.8216623663902283
MemoryTrain:  epoch  3, batch     3 | loss: 3.4761753Losses:  3.123384714126587 0.49131935834884644 0.9272277355194092
MemoryTrain:  epoch  3, batch     4 | loss: 3.1233847Losses:  3.0439672470092773 -0.0 0.9346849322319031
MemoryTrain:  epoch  3, batch     5 | loss: 3.0439672Losses:  3.861778497695923 1.136380910873413 0.7644904255867004
MemoryTrain:  epoch  3, batch     6 | loss: 3.8617785Losses:  3.7261900901794434 0.24988260865211487 1.0664724111557007
MemoryTrain:  epoch  3, batch     7 | loss: 3.7261901Losses:  2.7344343662261963 -0.0 1.0438287258148193
MemoryTrain:  epoch  3, batch     8 | loss: 2.7344344Losses:  3.63059663772583 0.2738953232765198 0.9150985479354858
MemoryTrain:  epoch  3, batch     9 | loss: 3.6305966Losses:  2.6959269046783447 0.24554568529129028 0.7422148585319519
MemoryTrain:  epoch  3, batch    10 | loss: 2.6959269Losses:  1.6629232168197632 -0.0 0.23749248683452606
MemoryTrain:  epoch  3, batch    11 | loss: 1.6629232Losses:  3.1781601905822754 0.4949760138988495 0.847080409526825
MemoryTrain:  epoch  4, batch     0 | loss: 3.1781602Losses:  3.6890861988067627 0.5372152328491211 0.8576710820198059
MemoryTrain:  epoch  4, batch     1 | loss: 3.6890862Losses:  3.3037056922912598 0.2515234351158142 0.9807209968566895
MemoryTrain:  epoch  4, batch     2 | loss: 3.3037057Losses:  3.2515857219696045 0.5182160139083862 0.7901991009712219
MemoryTrain:  epoch  4, batch     3 | loss: 3.2515857Losses:  3.490342855453491 0.8464093804359436 0.8002859950065613
MemoryTrain:  epoch  4, batch     4 | loss: 3.4903429Losses:  3.0728015899658203 -0.0 0.8387481570243835
MemoryTrain:  epoch  4, batch     5 | loss: 3.0728016Losses:  2.774326801300049 0.2688866853713989 0.8762599229812622
MemoryTrain:  epoch  4, batch     6 | loss: 2.7743268Losses:  2.4977142810821533 -0.0 0.9662827849388123
MemoryTrain:  epoch  4, batch     7 | loss: 2.4977143Losses:  2.4343690872192383 -0.0 1.0280303955078125
MemoryTrain:  epoch  4, batch     8 | loss: 2.4343691Losses:  2.3042354583740234 -0.0 0.8577356338500977
MemoryTrain:  epoch  4, batch     9 | loss: 2.3042355Losses:  3.8337507247924805 0.9291852116584778 0.967498779296875
MemoryTrain:  epoch  4, batch    10 | loss: 3.8337507Losses:  1.6374187469482422 -0.0 0.35581785440444946
MemoryTrain:  epoch  4, batch    11 | loss: 1.6374187Losses:  2.5090975761413574 -0.0 0.8992785215377808
MemoryTrain:  epoch  5, batch     0 | loss: 2.5090976Losses:  2.7823643684387207 -0.0 1.070995807647705
MemoryTrain:  epoch  5, batch     1 | loss: 2.7823644Losses:  2.920811891555786 -0.0 0.998177707195282
MemoryTrain:  epoch  5, batch     2 | loss: 2.9208119Losses:  2.9914238452911377 0.2430924028158188 1.0362672805786133
MemoryTrain:  epoch  5, batch     3 | loss: 2.9914238Losses:  3.1966447830200195 0.2922961115837097 0.9821467995643616
MemoryTrain:  epoch  5, batch     4 | loss: 3.1966448Losses:  2.8240814208984375 -0.0 0.8625127077102661
MemoryTrain:  epoch  5, batch     5 | loss: 2.8240814Losses:  2.675736904144287 -0.0 0.9642220735549927
MemoryTrain:  epoch  5, batch     6 | loss: 2.6757369Losses:  2.6885061264038086 -0.0 1.0181280374526978
MemoryTrain:  epoch  5, batch     7 | loss: 2.6885061Losses:  2.5625834465026855 0.27963119745254517 0.9783521890640259
MemoryTrain:  epoch  5, batch     8 | loss: 2.5625834Losses:  2.214369297027588 -0.0 0.8936318159103394
MemoryTrain:  epoch  5, batch     9 | loss: 2.2143693Losses:  2.594151735305786 0.49509871006011963 0.8100787997245789
MemoryTrain:  epoch  5, batch    10 | loss: 2.5941517Losses:  1.6575759649276733 -0.0 0.34886422753334045
MemoryTrain:  epoch  5, batch    11 | loss: 1.6575760Losses:  2.3735604286193848 -0.0 1.0686084032058716
MemoryTrain:  epoch  6, batch     0 | loss: 2.3735604Losses:  2.66101336479187 -0.0 0.9710568189620972
MemoryTrain:  epoch  6, batch     1 | loss: 2.6610134Losses:  2.864006280899048 0.7654269933700562 0.8580315709114075
MemoryTrain:  epoch  6, batch     2 | loss: 2.8640063Losses:  2.550154209136963 0.25953277945518494 0.9290229082107544
MemoryTrain:  epoch  6, batch     3 | loss: 2.5501542Losses:  3.201106309890747 0.7584271430969238 0.7995874285697937
MemoryTrain:  epoch  6, batch     4 | loss: 3.2011063Losses:  2.6439027786254883 -0.0 0.9546151161193848
MemoryTrain:  epoch  6, batch     5 | loss: 2.6439028Losses:  3.812034845352173 0.6923166513442993 0.9244596362113953
MemoryTrain:  epoch  6, batch     6 | loss: 3.8120348Losses:  2.9627838134765625 0.2810644507408142 0.9143861532211304
MemoryTrain:  epoch  6, batch     7 | loss: 2.9627838Losses:  2.336019992828369 -0.0 0.8343552947044373
MemoryTrain:  epoch  6, batch     8 | loss: 2.3360200Losses:  2.9197967052459717 0.7875488996505737 0.7031124234199524
MemoryTrain:  epoch  6, batch     9 | loss: 2.9197967Losses:  2.487104892730713 0.24882560968399048 0.84088534116745
MemoryTrain:  epoch  6, batch    10 | loss: 2.4871049Losses:  1.687116026878357 -0.0 0.34991589188575745
MemoryTrain:  epoch  6, batch    11 | loss: 1.6871160Losses:  2.5882813930511475 -0.0 0.8553090691566467
MemoryTrain:  epoch  7, batch     0 | loss: 2.5882814Losses:  2.8377294540405273 0.25453007221221924 0.9116331934928894
MemoryTrain:  epoch  7, batch     1 | loss: 2.8377295Losses:  2.8666257858276367 0.2532365918159485 0.8971254229545593
MemoryTrain:  epoch  7, batch     2 | loss: 2.8666258Losses:  2.354556083679199 0.254550039768219 0.8359149694442749
MemoryTrain:  epoch  7, batch     3 | loss: 2.3545561Losses:  2.81813907623291 0.25879761576652527 0.8263663053512573
MemoryTrain:  epoch  7, batch     4 | loss: 2.8181391Losses:  2.721623420715332 -0.0 0.9103313684463501
MemoryTrain:  epoch  7, batch     5 | loss: 2.7216234Losses:  2.5974411964416504 0.25476622581481934 1.012817621231079
MemoryTrain:  epoch  7, batch     6 | loss: 2.5974412Losses:  2.96728777885437 0.2471570074558258 0.9879642128944397
MemoryTrain:  epoch  7, batch     7 | loss: 2.9672878Losses:  2.894469738006592 0.5638941526412964 0.8753924369812012
MemoryTrain:  epoch  7, batch     8 | loss: 2.8944697Losses:  2.781719207763672 0.49799156188964844 0.9057434797286987
MemoryTrain:  epoch  7, batch     9 | loss: 2.7817192Losses:  2.5872063636779785 0.24193215370178223 0.9681752920150757
MemoryTrain:  epoch  7, batch    10 | loss: 2.5872064Losses:  1.6105873584747314 -0.0 0.34695863723754883
MemoryTrain:  epoch  7, batch    11 | loss: 1.6105874Losses:  3.768955945968628 0.7414246797561646 0.9786727428436279
MemoryTrain:  epoch  8, batch     0 | loss: 3.7689559Losses:  2.8149735927581787 0.5354509949684143 0.8998705744743347
MemoryTrain:  epoch  8, batch     1 | loss: 2.8149736Losses:  2.1352450847625732 -0.0 0.8315396308898926
MemoryTrain:  epoch  8, batch     2 | loss: 2.1352451Losses:  3.0155909061431885 0.8921616673469543 0.7282462120056152
MemoryTrain:  epoch  8, batch     3 | loss: 3.0155909Losses:  2.20575213432312 -0.0 0.9577817320823669
MemoryTrain:  epoch  8, batch     4 | loss: 2.2057521Losses:  2.737591505050659 0.25637346506118774 0.8401625752449036
MemoryTrain:  epoch  8, batch     5 | loss: 2.7375915Losses:  2.841165542602539 0.7824087738990784 0.7940505743026733
MemoryTrain:  epoch  8, batch     6 | loss: 2.8411655Losses:  2.3712282180786133 -0.0 0.9015721082687378
MemoryTrain:  epoch  8, batch     7 | loss: 2.3712282Losses:  3.351408004760742 1.1200554370880127 0.9171081781387329
MemoryTrain:  epoch  8, batch     8 | loss: 3.3514080Losses:  3.1598503589630127 0.2588903307914734 0.968888521194458
MemoryTrain:  epoch  8, batch     9 | loss: 3.1598504Losses:  2.688018798828125 0.26903074979782104 1.04494309425354
MemoryTrain:  epoch  8, batch    10 | loss: 2.6880188Losses:  1.5607506036758423 0.23542006313800812 0.1140688955783844
MemoryTrain:  epoch  8, batch    11 | loss: 1.5607506Losses:  2.281482458114624 -0.0 0.7964360117912292
MemoryTrain:  epoch  9, batch     0 | loss: 2.2814825Losses:  2.2467710971832275 -0.0 0.9515706896781921
MemoryTrain:  epoch  9, batch     1 | loss: 2.2467711Losses:  2.9589738845825195 0.5200021862983704 0.8761798143386841
MemoryTrain:  epoch  9, batch     2 | loss: 2.9589739Losses:  2.879599094390869 0.7279205322265625 0.9096466898918152
MemoryTrain:  epoch  9, batch     3 | loss: 2.8795991Losses:  2.66524338722229 0.5020780563354492 0.7380284070968628
MemoryTrain:  epoch  9, batch     4 | loss: 2.6652434Losses:  2.571235179901123 -0.0 0.9804067015647888
MemoryTrain:  epoch  9, batch     5 | loss: 2.5712352Losses:  3.2426209449768066 1.0957051515579224 0.8614636659622192
MemoryTrain:  epoch  9, batch     6 | loss: 3.2426209Losses:  2.6300785541534424 0.5248655676841736 0.8001959919929504
MemoryTrain:  epoch  9, batch     7 | loss: 2.6300786Losses:  3.63907527923584 0.7520871162414551 0.8484416007995605
MemoryTrain:  epoch  9, batch     8 | loss: 3.6390753Losses:  2.4412951469421387 0.24911971390247345 0.9500342607498169
MemoryTrain:  epoch  9, batch     9 | loss: 2.4412951Losses:  3.2571496963500977 0.7604191899299622 0.9106337428092957
MemoryTrain:  epoch  9, batch    10 | loss: 3.2571497Losses:  1.4160250425338745 -0.0 0.11317364871501923
MemoryTrain:  epoch  9, batch    11 | loss: 1.4160250
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 80.77%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 6.25%,  total acc: 68.36%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 65.07%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 61.51%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 63.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 67.55%   [EVAL] batch:   26 | acc: 0.00%,  total acc: 65.05%   [EVAL] batch:   27 | acc: 0.00%,  total acc: 62.72%   [EVAL] batch:   28 | acc: 6.25%,  total acc: 60.78%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 58.96%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 57.26%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 57.62%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 58.71%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 59.93%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 60.89%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 61.63%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 63.32%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 63.46%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 64.06%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 63.87%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 63.84%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 63.95%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 64.49%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 64.86%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 65.08%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 65.43%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 65.36%   [EVAL] batch:   48 | acc: 62.50%,  total acc: 65.31%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 65.88%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 65.69%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 65.50%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 65.68%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 66.07%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 66.23%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 66.49%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 66.84%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 66.88%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 67.11%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 66.83%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 66.47%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 80.68%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 79.17%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 78.85%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 79.02%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.99%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 82.44%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 82.34%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 82.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.41%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.60%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.91%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.89%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 86.55%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 85.66%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 85.36%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 85.07%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 85.14%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 85.20%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.58%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 86.13%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 86.16%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.34%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 86.22%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 85.97%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 86.01%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 85.77%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 85.29%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 85.20%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 84.75%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 84.56%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 84.32%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 83.91%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 83.41%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 83.15%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 83.11%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 83.08%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 83.16%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 83.50%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 83.37%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 82.84%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 81.54%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 80.38%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 79.17%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 77.99%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 76.84%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 76.18%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 76.16%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 76.41%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 76.65%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 77.03%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 76.89%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 76.62%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 76.44%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 76.11%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 75.70%   [EVAL] batch:   80 | acc: 43.75%,  total acc: 75.31%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 74.62%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 74.17%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 73.59%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 73.24%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 72.67%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 72.13%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 71.45%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 70.72%   [EVAL] batch:   89 | acc: 6.25%,  total acc: 70.00%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 69.23%   [EVAL] batch:   91 | acc: 12.50%,  total acc: 68.61%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 67.94%   [EVAL] batch:   93 | acc: 18.75%,  total acc: 67.42%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 67.50%   [EVAL] batch:   95 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 68.56%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 68.69%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 69.24%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 69.83%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 70.06%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 69.92%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 69.44%   [EVAL] batch:  108 | acc: 0.00%,  total acc: 68.81%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 68.35%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 67.96%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 67.35%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 67.04%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 66.94%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 66.79%   [EVAL] batch:  115 | acc: 87.50%,  total acc: 66.97%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 66.88%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 66.84%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 66.75%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 66.56%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 66.32%   [EVAL] batch:  121 | acc: 56.25%,  total acc: 66.24%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 66.21%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 66.28%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 66.30%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 66.17%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 65.85%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 65.53%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 65.26%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 65.10%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 65.08%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 65.29%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 65.55%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 65.76%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 65.88%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 66.08%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 66.33%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 66.44%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 66.50%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 66.38%   [EVAL] batch:  140 | acc: 56.25%,  total acc: 66.31%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 66.37%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 66.39%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 66.45%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 66.47%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 66.52%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 66.41%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 66.47%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 66.53%   [EVAL] batch:  149 | acc: 62.50%,  total acc: 66.50%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 66.06%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 65.62%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 65.24%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 64.81%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 64.44%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 64.02%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 63.93%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 63.88%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 63.80%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 63.71%   [EVAL] batch:  160 | acc: 50.00%,  total acc: 63.63%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 63.54%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 63.50%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 63.49%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 63.56%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 63.59%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 63.66%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 63.65%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 63.68%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 63.46%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 63.30%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 63.15%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 63.04%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 62.86%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 62.79%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 63.00%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 63.21%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 63.82%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 64.02%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 64.18%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 64.21%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 64.33%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 64.49%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 64.62%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 64.67%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 64.66%   [EVAL] batch:  188 | acc: 50.00%,  total acc: 64.58%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 64.57%   [EVAL] batch:  190 | acc: 56.25%,  total acc: 64.53%   [EVAL] batch:  191 | acc: 56.25%,  total acc: 64.49%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 64.48%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 64.40%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 64.46%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 64.48%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 64.44%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 64.52%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 64.51%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 64.59%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 64.60%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 64.62%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 64.51%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 64.56%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 64.37%   [EVAL] batch:  207 | acc: 6.25%,  total acc: 64.09%   [EVAL] batch:  208 | acc: 6.25%,  total acc: 63.82%   [EVAL] batch:  209 | acc: 0.00%,  total acc: 63.51%   [EVAL] batch:  210 | acc: 6.25%,  total acc: 63.24%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 63.03%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 62.97%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 63.14%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.31%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 63.45%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.79%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 63.93%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 64.09%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 64.41%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 64.55%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 64.70%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.86%   [EVAL] batch:  225 | acc: 81.25%,  total acc: 64.93%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 64.98%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 65.02%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 65.07%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 65.19%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 65.23%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 65.17%   [EVAL] batch:  232 | acc: 37.50%,  total acc: 65.05%   [EVAL] batch:  233 | acc: 43.75%,  total acc: 64.96%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 64.81%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 64.75%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 64.69%   [EVAL] batch:  237 | acc: 50.00%,  total acc: 64.63%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 64.70%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.84%   [EVAL] batch:  240 | acc: 75.00%,  total acc: 64.89%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 64.98%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 65.05%   [EVAL] batch:  243 | acc: 87.50%,  total acc: 65.14%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 65.26%   [EVAL] batch:  245 | acc: 81.25%,  total acc: 65.32%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 65.31%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 65.32%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 65.39%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 65.48%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 65.46%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 65.50%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 65.56%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 65.55%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 65.61%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 65.67%   [EVAL] batch:  256 | acc: 56.25%,  total acc: 65.64%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 65.67%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 65.69%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 65.72%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 65.66%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 65.67%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 65.66%   [EVAL] batch:  263 | acc: 50.00%,  total acc: 65.60%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 65.54%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 65.46%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 65.38%   [EVAL] batch:  267 | acc: 43.75%,  total acc: 65.30%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 65.36%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 65.44%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 65.92%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 66.05%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 66.08%   [EVAL] batch:  276 | acc: 56.25%,  total acc: 66.04%   [EVAL] batch:  277 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 66.06%   [EVAL] batch:  279 | acc: 68.75%,  total acc: 66.07%   [EVAL] batch:  280 | acc: 62.50%,  total acc: 66.06%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 66.07%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 66.03%   [EVAL] batch:  283 | acc: 31.25%,  total acc: 65.91%   [EVAL] batch:  284 | acc: 50.00%,  total acc: 65.86%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 65.71%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 65.55%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 65.58%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 65.64%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 65.71%   [EVAL] batch:  290 | acc: 68.75%,  total acc: 65.72%   [EVAL] batch:  291 | acc: 93.75%,  total acc: 65.82%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 65.87%   [EVAL] batch:  293 | acc: 81.25%,  total acc: 65.92%   [EVAL] batch:  294 | acc: 56.25%,  total acc: 65.89%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 65.90%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 65.89%   [EVAL] batch:  297 | acc: 43.75%,  total acc: 65.81%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 65.78%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 65.83%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.17%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.39%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 66.61%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 66.83%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.15%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:  313 | acc: 50.00%,  total acc: 67.14%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 67.10%   [EVAL] batch:  315 | acc: 81.25%,  total acc: 67.15%   [EVAL] batch:  316 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:  317 | acc: 87.50%,  total acc: 67.26%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 67.30%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 67.40%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 67.48%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 67.55%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.63%   [EVAL] batch:  323 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  324 | acc: 93.75%,  total acc: 67.81%   [EVAL] batch:  325 | acc: 18.75%,  total acc: 67.66%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 67.51%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 67.34%   [EVAL] batch:  328 | acc: 12.50%,  total acc: 67.17%   [EVAL] batch:  329 | acc: 18.75%,  total acc: 67.03%   [EVAL] batch:  330 | acc: 25.00%,  total acc: 66.90%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 66.91%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 67.00%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 67.18%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.28%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 67.36%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 67.33%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 67.13%   [EVAL] batch:  339 | acc: 0.00%,  total acc: 66.93%   [EVAL] batch:  340 | acc: 6.25%,  total acc: 66.75%   [EVAL] batch:  341 | acc: 0.00%,  total acc: 66.56%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 66.38%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 66.28%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 66.36%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 66.44%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 66.58%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 66.65%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.75%   [EVAL] batch:  350 | acc: 75.00%,  total acc: 66.77%   [EVAL] batch:  351 | acc: 75.00%,  total acc: 66.80%   [EVAL] batch:  352 | acc: 81.25%,  total acc: 66.84%   [EVAL] batch:  353 | acc: 43.75%,  total acc: 66.77%   [EVAL] batch:  354 | acc: 87.50%,  total acc: 66.83%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 66.84%   [EVAL] batch:  356 | acc: 81.25%,  total acc: 66.88%   [EVAL] batch:  357 | acc: 81.25%,  total acc: 66.92%   [EVAL] batch:  358 | acc: 75.00%,  total acc: 66.94%   [EVAL] batch:  359 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:  360 | acc: 62.50%,  total acc: 66.95%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 66.97%   [EVAL] batch:  362 | acc: 87.50%,  total acc: 67.03%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 66.96%   [EVAL] batch:  364 | acc: 75.00%,  total acc: 66.99%   [EVAL] batch:  365 | acc: 68.75%,  total acc: 66.99%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 67.01%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 67.00%   [EVAL] batch:  368 | acc: 68.75%,  total acc: 67.01%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 67.13%   [EVAL] batch:  371 | acc: 75.00%,  total acc: 67.15%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 67.14%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 67.15%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 67.17%   
cur_acc:  ['0.9494', '0.6270', '0.7133', '0.7024', '0.7371', '0.6647']
his_acc:  ['0.9494', '0.7820', '0.7500', '0.7208', '0.6957', '0.6717']
Clustering into  34  clusters
Clusters:  [ 9  0 23 12  3  3 25  4  0 17  3  2 27 13 11 31 26  8 15  0  7  4  4  3
 13 17  9  3  2  2  5 15 24  2 33  2 21  7  1  7  5 19 20  1  4 29  8  6
 16 15  1 22 10 20 11  0 27  9 28  1 12  9  9  0 30  6 14 18 32  0]
Losses:  8.605321884155273 2.8947012424468994 0.700568675994873
CurrentTrain: epoch  0, batch     0 | loss: 8.6053219Losses:  8.990126609802246 3.214606761932373 0.6225559115409851
CurrentTrain: epoch  0, batch     1 | loss: 8.9901266Losses:  7.976156711578369 2.4878134727478027 0.6233854293823242
CurrentTrain: epoch  0, batch     2 | loss: 7.9761567Losses:  8.876699447631836 -0.0 0.2337718904018402
CurrentTrain: epoch  0, batch     3 | loss: 8.8766994Losses:  7.579838275909424 3.2947134971618652 0.6030184626579285
CurrentTrain: epoch  1, batch     0 | loss: 7.5798383Losses:  7.695678234100342 2.5876622200012207 0.6782248616218567
CurrentTrain: epoch  1, batch     1 | loss: 7.6956782Losses:  8.001347541809082 3.261202812194824 0.603345513343811
CurrentTrain: epoch  1, batch     2 | loss: 8.0013475Losses:  2.4095699787139893 -0.0 0.10403725504875183
CurrentTrain: epoch  1, batch     3 | loss: 2.4095700Losses:  6.515369892120361 2.5824952125549316 0.658534586429596
CurrentTrain: epoch  2, batch     0 | loss: 6.5153699Losses:  8.238471031188965 4.428499221801758 0.5075890421867371
CurrentTrain: epoch  2, batch     1 | loss: 8.2384710Losses:  7.501440048217773 3.334679365158081 0.5731179714202881
CurrentTrain: epoch  2, batch     2 | loss: 7.5014400Losses:  4.214701175689697 -0.0 0.10287272930145264
CurrentTrain: epoch  2, batch     3 | loss: 4.2147012Losses:  8.00571060180664 3.4013659954071045 0.6496877670288086
CurrentTrain: epoch  3, batch     0 | loss: 8.0057106Losses:  6.6493940353393555 3.1220808029174805 0.662497878074646
CurrentTrain: epoch  3, batch     1 | loss: 6.6493940Losses:  6.387026786804199 2.92324161529541 0.6362083554267883
CurrentTrain: epoch  3, batch     2 | loss: 6.3870268Losses:  2.52048921585083 -0.0 0.10798487812280655
CurrentTrain: epoch  3, batch     3 | loss: 2.5204892Losses:  5.791085243225098 2.552635908126831 0.587943971157074
CurrentTrain: epoch  4, batch     0 | loss: 5.7910852Losses:  8.249502182006836 4.3375396728515625 0.5892282724380493
CurrentTrain: epoch  4, batch     1 | loss: 8.2495022Losses:  6.739207744598389 3.6911046504974365 0.4835819900035858
CurrentTrain: epoch  4, batch     2 | loss: 6.7392077Losses:  1.8010072708129883 -0.0 0.14307521283626556
CurrentTrain: epoch  4, batch     3 | loss: 1.8010073Losses:  6.318286895751953 3.350370168685913 0.5763969421386719
CurrentTrain: epoch  5, batch     0 | loss: 6.3182869Losses:  6.1299896240234375 2.747807502746582 0.5673369765281677
CurrentTrain: epoch  5, batch     1 | loss: 6.1299896Losses:  8.45580768585205 5.3043107986450195 0.39299049973487854
CurrentTrain: epoch  5, batch     2 | loss: 8.4558077Losses:  1.9593878984451294 -0.0 0.12070189416408539
CurrentTrain: epoch  5, batch     3 | loss: 1.9593879Losses:  5.689009666442871 2.5740222930908203 0.5726553797721863
CurrentTrain: epoch  6, batch     0 | loss: 5.6890097Losses:  6.146364212036133 3.0908055305480957 0.5595029592514038
CurrentTrain: epoch  6, batch     1 | loss: 6.1463642Losses:  5.232271194458008 2.517096519470215 0.5534107685089111
CurrentTrain: epoch  6, batch     2 | loss: 5.2322712Losses:  2.6276307106018066 -0.0 0.09311401844024658
CurrentTrain: epoch  6, batch     3 | loss: 2.6276307Losses:  5.376275062561035 2.8189194202423096 0.5682048797607422
CurrentTrain: epoch  7, batch     0 | loss: 5.3762751Losses:  5.423285961151123 2.1707632541656494 0.6376002430915833
CurrentTrain: epoch  7, batch     1 | loss: 5.4232860Losses:  4.504669666290283 1.8572368621826172 0.6342348456382751
CurrentTrain: epoch  7, batch     2 | loss: 4.5046697Losses:  2.29821515083313 -0.0 0.11038649827241898
CurrentTrain: epoch  7, batch     3 | loss: 2.2982152Losses:  5.808481216430664 3.046881914138794 0.6332293748855591
CurrentTrain: epoch  8, batch     0 | loss: 5.8084812Losses:  5.236321449279785 2.2926712036132812 0.6123961210250854
CurrentTrain: epoch  8, batch     1 | loss: 5.2363214Losses:  4.969493389129639 2.5284576416015625 0.5522667169570923
CurrentTrain: epoch  8, batch     2 | loss: 4.9694934Losses:  1.8792834281921387 -0.0 0.09912687540054321
CurrentTrain: epoch  8, batch     3 | loss: 1.8792834Losses:  6.641683101654053 3.973583698272705 0.5606514811515808
CurrentTrain: epoch  9, batch     0 | loss: 6.6416831Losses:  6.203441143035889 3.5050182342529297 0.5554823279380798
CurrentTrain: epoch  9, batch     1 | loss: 6.2034411Losses:  4.207229137420654 1.7954193353652954 0.6065124273300171
CurrentTrain: epoch  9, batch     2 | loss: 4.2072291Losses:  1.7958585023880005 -0.0 0.08631885051727295
CurrentTrain: epoch  9, batch     3 | loss: 1.7958585
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: barack obama served as a member of the democratic party during his presidency from 2009 to 2017.  
Head Entity: barack obama  
Tail Entity: democratic party  

Relation: member of political party  
Context: angela merkel was a prominent member of the christian democratic union throughout her political career in germany.  
Head Entity: angela merkel  
Tail Entity: christian democratic union  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the script for " the great santini " was adapted by carlino from the 1976 novel by pat conroy , with assistance from an un - credited herman raucher .
Head Entity: the great santini
Tail Entity: pat conroy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "inception" draws heavily from the concepts presented in the 2001 novel "the dreamers" by j. k. rowling, which explores the nature of dreams and reality.  
Head Entity: inception  
Tail Entity: j. k. rowling  

Relation: after a work by  
Context: the musical "hamilton" was inspired by the biography "alexander hamilton" written by ron chernow, which details the life of the founding father.  
Head Entity: hamilton  
Tail Entity: ron chernow  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking paper on the theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after relocating its main office to new york city, the startup saw a significant increase in investment and visibility.  
Head Entity: startup  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which is classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: wjet erie , pennsylvania was his first official radio job outside of duties performed in the u.s. air force .
Head Entity: wjet
Tail Entity: erie , pennsylvania
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: wxyz television is the primary news station serving the city of springfield, known for its local coverage.  
Head Entity: wxyz  
Tail Entity: springfield  

Relation: licensed to broadcast to  
Context: kqed is a public television station that provides educational programming to the residents of san francisco.  
Head Entity: kqed  
Tail Entity: san francisco  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: hd 32518 b is an extrasolar planet which orbits the k - type giant star hd 32518 , located approximately 383 light years away in the constellation camelopardalis .
Head Entity: hd 32518
Tail Entity: camelopardalis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the star betelgeuse is located in the constellation orion and is one of the brightest stars in the night sky.  
Head Entity: betelgeuse  
Tail Entity: orion  

Relation: constellation  
Context: the andromeda galaxy is visible in the constellation andromeda, which is named after a princess in Greek mythology.  
Head Entity: andromeda galaxy  
Tail Entity: andromeda  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Losses:  3.6016221046447754 0.5059616565704346 0.8889957070350647
MemoryTrain:  epoch  0, batch     0 | loss: 3.6016221Losses:  3.965127944946289 0.7500998377799988 0.9787665605545044
MemoryTrain:  epoch  0, batch     1 | loss: 3.9651279Losses:  3.8570475578308105 -0.0 1.085879921913147
MemoryTrain:  epoch  0, batch     2 | loss: 3.8570476Losses:  4.016960620880127 0.2702832818031311 1.0102559328079224
MemoryTrain:  epoch  0, batch     3 | loss: 4.0169606Losses:  3.5736961364746094 0.2838321030139923 1.0012426376342773
MemoryTrain:  epoch  0, batch     4 | loss: 3.5736961Losses:  4.328643798828125 0.7786977291107178 0.921596884727478
MemoryTrain:  epoch  0, batch     5 | loss: 4.3286438Losses:  3.166708469390869 0.22860294580459595 0.7259519696235657
MemoryTrain:  epoch  0, batch     6 | loss: 3.1667085Losses:  3.177130699157715 0.22874636948108673 0.9698454141616821
MemoryTrain:  epoch  0, batch     7 | loss: 3.1771307Losses:  4.031342029571533 0.27282434701919556 0.9245919585227966
MemoryTrain:  epoch  0, batch     8 | loss: 4.0313420Losses:  4.478116512298584 0.27463439106941223 0.9913439154624939
MemoryTrain:  epoch  0, batch     9 | loss: 4.4781165Losses:  4.006202220916748 0.3279973864555359 0.9291335940361023
MemoryTrain:  epoch  0, batch    10 | loss: 4.0062022Losses:  3.179755926132202 0.26006993651390076 0.7987308502197266
MemoryTrain:  epoch  0, batch    11 | loss: 3.1797559Losses:  3.3338522911071777 0.23826603591442108 0.8677340745925903
MemoryTrain:  epoch  0, batch    12 | loss: 3.3338523Losses:  2.9340786933898926 -0.0 0.09800481796264648
MemoryTrain:  epoch  0, batch    13 | loss: 2.9340787Losses:  3.5387496948242188 0.4958062767982483 0.9316703081130981
MemoryTrain:  epoch  1, batch     0 | loss: 3.5387497Losses:  4.0984206199646 0.4932306110858917 0.8796059489250183
MemoryTrain:  epoch  1, batch     1 | loss: 4.0984206Losses:  3.360822916030884 -0.0 0.7992136478424072
MemoryTrain:  epoch  1, batch     2 | loss: 3.3608229Losses:  3.093198776245117 0.5144345164299011 1.074241042137146
MemoryTrain:  epoch  1, batch     3 | loss: 3.0931988Losses:  3.4563522338867188 -0.0 1.0241948366165161
MemoryTrain:  epoch  1, batch     4 | loss: 3.4563522Losses:  3.2689619064331055 -0.0 0.7865676879882812
MemoryTrain:  epoch  1, batch     5 | loss: 3.2689619Losses:  3.4037160873413086 0.7910337448120117 0.9301104545593262
MemoryTrain:  epoch  1, batch     6 | loss: 3.4037161Losses:  3.1441779136657715 -0.0 1.0133360624313354
MemoryTrain:  epoch  1, batch     7 | loss: 3.1441779Losses:  3.684440851211548 0.533206582069397 0.9241340160369873
MemoryTrain:  epoch  1, batch     8 | loss: 3.6844409Losses:  4.493133544921875 0.8807812929153442 1.0078151226043701
MemoryTrain:  epoch  1, batch     9 | loss: 4.4931335Losses:  3.303703784942627 0.5011683106422424 0.9179316759109497
MemoryTrain:  epoch  1, batch    10 | loss: 3.3037038Losses:  2.9171504974365234 0.5293850302696228 0.963727593421936
MemoryTrain:  epoch  1, batch    11 | loss: 2.9171505Losses:  2.9224765300750732 0.2504909634590149 0.8678242564201355
MemoryTrain:  epoch  1, batch    12 | loss: 2.9224765Losses:  2.1113147735595703 -0.0 0.11547566205263138
MemoryTrain:  epoch  1, batch    13 | loss: 2.1113148Losses:  2.9750800132751465 -0.0 0.9635514616966248
MemoryTrain:  epoch  2, batch     0 | loss: 2.9750800Losses:  3.312218427658081 0.5213911533355713 0.8262736201286316
MemoryTrain:  epoch  2, batch     1 | loss: 3.3122184Losses:  2.7599711418151855 0.25032639503479004 0.9511463642120361
MemoryTrain:  epoch  2, batch     2 | loss: 2.7599711Losses:  2.992166757583618 0.511902928352356 0.8730847239494324
MemoryTrain:  epoch  2, batch     3 | loss: 2.9921668Losses:  2.844265937805176 0.22797100245952606 0.9898397922515869
MemoryTrain:  epoch  2, batch     4 | loss: 2.8442659Losses:  3.7765965461730957 0.305990993976593 0.8776450157165527
MemoryTrain:  epoch  2, batch     5 | loss: 3.7765965Losses:  3.222740650177002 0.5005456805229187 0.9217734336853027
MemoryTrain:  epoch  2, batch     6 | loss: 3.2227407Losses:  3.5431416034698486 0.7926506996154785 0.848646342754364
MemoryTrain:  epoch  2, batch     7 | loss: 3.5431416Losses:  3.3004398345947266 0.7623093128204346 0.8800476789474487
MemoryTrain:  epoch  2, batch     8 | loss: 3.3004398Losses:  2.7629408836364746 -0.0 0.8543679118156433
MemoryTrain:  epoch  2, batch     9 | loss: 2.7629409Losses:  3.438737392425537 0.25469970703125 0.9759473204612732
MemoryTrain:  epoch  2, batch    10 | loss: 3.4387374Losses:  2.719482660293579 0.2703385651111603 0.954969584941864
MemoryTrain:  epoch  2, batch    11 | loss: 2.7194827Losses:  3.1666669845581055 0.2315908670425415 0.9029378890991211
MemoryTrain:  epoch  2, batch    12 | loss: 3.1666670Losses:  1.394458293914795 -0.0 0.12137056887149811
MemoryTrain:  epoch  2, batch    13 | loss: 1.3944583Losses:  3.082540512084961 0.5499441623687744 0.8855780363082886
MemoryTrain:  epoch  3, batch     0 | loss: 3.0825405Losses:  2.6000912189483643 0.2447834014892578 0.8380395770072937
MemoryTrain:  epoch  3, batch     1 | loss: 2.6000912Losses:  3.6814544200897217 0.5843472480773926 0.8724485039710999
MemoryTrain:  epoch  3, batch     2 | loss: 3.6814544Losses:  2.7355194091796875 0.24435964226722717 1.0226128101348877
MemoryTrain:  epoch  3, batch     3 | loss: 2.7355194Losses:  2.83192777633667 -0.0 1.019343376159668
MemoryTrain:  epoch  3, batch     4 | loss: 2.8319278Losses:  2.709012031555176 0.5031391382217407 0.9688766598701477
MemoryTrain:  epoch  3, batch     5 | loss: 2.7090120Losses:  3.6890149116516113 0.7634725570678711 0.9310466647148132
MemoryTrain:  epoch  3, batch     6 | loss: 3.6890149Losses:  2.7816076278686523 0.2691861093044281 0.8594533205032349
MemoryTrain:  epoch  3, batch     7 | loss: 2.7816076Losses:  2.6969223022460938 0.5121904611587524 0.8974129557609558
MemoryTrain:  epoch  3, batch     8 | loss: 2.6969223Losses:  3.2374107837677 0.5104655027389526 0.8637582659721375
MemoryTrain:  epoch  3, batch     9 | loss: 3.2374108Losses:  2.6234359741210938 0.26244837045669556 1.057540774345398
MemoryTrain:  epoch  3, batch    10 | loss: 2.6234360Losses:  2.7337496280670166 -0.0 0.9652286171913147
MemoryTrain:  epoch  3, batch    11 | loss: 2.7337496Losses:  2.9515836238861084 0.26258134841918945 0.9075290560722351
MemoryTrain:  epoch  3, batch    12 | loss: 2.9515836Losses:  1.3670289516448975 -0.0 0.10291590541601181
MemoryTrain:  epoch  3, batch    13 | loss: 1.3670290Losses:  2.6320362091064453 0.23829147219657898 0.7548587322235107
MemoryTrain:  epoch  4, batch     0 | loss: 2.6320362Losses:  3.45424747467041 0.2816106081008911 0.9773907661437988
MemoryTrain:  epoch  4, batch     1 | loss: 3.4542475Losses:  2.5669798851013184 -0.0 0.9941986799240112
MemoryTrain:  epoch  4, batch     2 | loss: 2.5669799Losses:  2.4037256240844727 -0.0 0.9876318573951721
MemoryTrain:  epoch  4, batch     3 | loss: 2.4037256Losses:  2.4902567863464355 -0.0 1.0391981601715088
MemoryTrain:  epoch  4, batch     4 | loss: 2.4902568Losses:  2.9532594680786133 0.2616739273071289 0.9570021033287048
MemoryTrain:  epoch  4, batch     5 | loss: 2.9532595Losses:  2.673815965652466 0.2419355809688568 1.0110355615615845
MemoryTrain:  epoch  4, batch     6 | loss: 2.6738160Losses:  2.6471943855285645 0.24805963039398193 0.8805167078971863
MemoryTrain:  epoch  4, batch     7 | loss: 2.6471944Losses:  2.6496024131774902 0.2668460011482239 1.034475326538086
MemoryTrain:  epoch  4, batch     8 | loss: 2.6496024Losses:  2.9141082763671875 0.24116170406341553 0.8570556640625
MemoryTrain:  epoch  4, batch     9 | loss: 2.9141083Losses:  2.8091795444488525 0.2941030263900757 0.9582436084747314
MemoryTrain:  epoch  4, batch    10 | loss: 2.8091795Losses:  2.7878174781799316 0.270332396030426 1.0045287609100342
MemoryTrain:  epoch  4, batch    11 | loss: 2.7878175Losses:  2.83571720123291 0.514367938041687 0.8624904155731201
MemoryTrain:  epoch  4, batch    12 | loss: 2.8357172Losses:  1.3610942363739014 -0.0 0.11626888811588287
MemoryTrain:  epoch  4, batch    13 | loss: 1.3610942Losses:  2.7392935752868652 0.2896656394004822 1.0912359952926636
MemoryTrain:  epoch  5, batch     0 | loss: 2.7392936Losses:  2.5738110542297363 0.2512757182121277 0.8903692960739136
MemoryTrain:  epoch  5, batch     1 | loss: 2.5738111Losses:  2.970040798187256 0.266900897026062 0.9365190267562866
MemoryTrain:  epoch  5, batch     2 | loss: 2.9700408Losses:  2.9014782905578613 0.23240381479263306 0.9147664308547974
MemoryTrain:  epoch  5, batch     3 | loss: 2.9014783Losses:  2.687246561050415 0.4845251142978668 0.8846339583396912
MemoryTrain:  epoch  5, batch     4 | loss: 2.6872466Losses:  2.359954357147217 -0.0 0.8953403234481812
MemoryTrain:  epoch  5, batch     5 | loss: 2.3599544Losses:  2.857672691345215 0.5175076723098755 0.9228852987289429
MemoryTrain:  epoch  5, batch     6 | loss: 2.8576727Losses:  2.934152841567993 0.5221874713897705 0.8959988951683044
MemoryTrain:  epoch  5, batch     7 | loss: 2.9341528Losses:  2.7178421020507812 0.49021798372268677 0.9662618041038513
MemoryTrain:  epoch  5, batch     8 | loss: 2.7178421Losses:  2.3602607250213623 -0.0 1.0166850090026855
MemoryTrain:  epoch  5, batch     9 | loss: 2.3602607Losses:  2.807559013366699 0.24597960710525513 0.9701605439186096
MemoryTrain:  epoch  5, batch    10 | loss: 2.8075590Losses:  3.059572696685791 0.5451059937477112 0.8336400985717773
MemoryTrain:  epoch  5, batch    11 | loss: 3.0595727Losses:  2.2709908485412598 -0.0 1.008954644203186
MemoryTrain:  epoch  5, batch    12 | loss: 2.2709908Losses:  2.9096007347106934 -0.0 0.09741117060184479
MemoryTrain:  epoch  5, batch    13 | loss: 2.9096007Losses:  2.7771859169006348 0.2263205349445343 1.0044646263122559
MemoryTrain:  epoch  6, batch     0 | loss: 2.7771859Losses:  2.652073383331299 0.4938203692436218 0.856196939945221
MemoryTrain:  epoch  6, batch     1 | loss: 2.6520734Losses:  2.815957546234131 0.7798851132392883 0.7961408495903015
MemoryTrain:  epoch  6, batch     2 | loss: 2.8159575Losses:  2.5193843841552734 0.22876188158988953 1.0024360418319702
MemoryTrain:  epoch  6, batch     3 | loss: 2.5193844Losses:  2.549116611480713 -0.0 0.985358715057373
MemoryTrain:  epoch  6, batch     4 | loss: 2.5491166Losses:  3.5754637718200684 0.5226397514343262 0.8723692893981934
MemoryTrain:  epoch  6, batch     5 | loss: 3.5754638Losses:  2.86997389793396 0.5249517560005188 0.9099998474121094
MemoryTrain:  epoch  6, batch     6 | loss: 2.8699739Losses:  2.3159637451171875 -0.0 0.9887675046920776
MemoryTrain:  epoch  6, batch     7 | loss: 2.3159637Losses:  3.299947500228882 0.8573769330978394 0.9191948771476746
MemoryTrain:  epoch  6, batch     8 | loss: 3.2999475Losses:  2.7768328189849854 0.2379816472530365 1.0583003759384155
MemoryTrain:  epoch  6, batch     9 | loss: 2.7768328Losses:  2.3399829864501953 -0.0 0.9735456705093384
MemoryTrain:  epoch  6, batch    10 | loss: 2.3399830Losses:  2.922816753387451 0.28256240487098694 0.9464684724807739
MemoryTrain:  epoch  6, batch    11 | loss: 2.9228168Losses:  2.4838359355926514 -0.0 0.838782012462616
MemoryTrain:  epoch  6, batch    12 | loss: 2.4838359Losses:  1.330995798110962 -0.0 0.11429546773433685
MemoryTrain:  epoch  6, batch    13 | loss: 1.3309958Losses:  2.745173692703247 -0.0 0.9718739986419678
MemoryTrain:  epoch  7, batch     0 | loss: 2.7451737Losses:  3.2241954803466797 0.5067291855812073 0.8484458923339844
MemoryTrain:  epoch  7, batch     1 | loss: 3.2241955Losses:  2.643771171569824 0.2481740117073059 0.928367018699646
MemoryTrain:  epoch  7, batch     2 | loss: 2.6437712Losses:  2.8872876167297363 0.5060759782791138 0.8538670539855957
MemoryTrain:  epoch  7, batch     3 | loss: 2.8872876Losses:  2.42233943939209 -0.0 0.9780541658401489
MemoryTrain:  epoch  7, batch     4 | loss: 2.4223394Losses:  2.870969533920288 0.2468193620443344 0.7945495247840881
MemoryTrain:  epoch  7, batch     5 | loss: 2.8709695Losses:  2.651707172393799 0.2871323823928833 1.0288139581680298
MemoryTrain:  epoch  7, batch     6 | loss: 2.6517072Losses:  3.169468879699707 0.749582827091217 0.813825249671936
MemoryTrain:  epoch  7, batch     7 | loss: 3.1694689Losses:  2.4300477504730225 0.22370977699756622 0.8930563926696777
MemoryTrain:  epoch  7, batch     8 | loss: 2.4300478Losses:  2.695571184158325 0.24590761959552765 0.9586301445960999
MemoryTrain:  epoch  7, batch     9 | loss: 2.6955712Losses:  2.412712574005127 -0.0 1.023730993270874
MemoryTrain:  epoch  7, batch    10 | loss: 2.4127126Losses:  2.476828098297119 -0.0 0.957971453666687
MemoryTrain:  epoch  7, batch    11 | loss: 2.4768281Losses:  2.5086536407470703 0.22367674112319946 0.8291321992874146
MemoryTrain:  epoch  7, batch    12 | loss: 2.5086536Losses:  1.2961878776550293 -0.0 0.10503338277339935
MemoryTrain:  epoch  7, batch    13 | loss: 1.2961879Losses:  3.583087205886841 0.9291034936904907 0.8662289977073669
MemoryTrain:  epoch  8, batch     0 | loss: 3.5830872Losses:  2.7644765377044678 0.24105119705200195 0.8092725872993469
MemoryTrain:  epoch  8, batch     1 | loss: 2.7644765Losses:  2.2404534816741943 -0.0 0.9708737730979919
MemoryTrain:  epoch  8, batch     2 | loss: 2.2404535Losses:  2.727159023284912 0.26684433221817017 1.056125283241272
MemoryTrain:  epoch  8, batch     3 | loss: 2.7271590Losses:  3.2313618659973145 0.5044429898262024 0.9052900075912476
MemoryTrain:  epoch  8, batch     4 | loss: 3.2313619Losses:  3.0803627967834473 0.5604026317596436 0.877330482006073
MemoryTrain:  epoch  8, batch     5 | loss: 3.0803628Losses:  2.3233795166015625 0.2455456256866455 0.8392512798309326
MemoryTrain:  epoch  8, batch     6 | loss: 2.3233795Losses:  3.0610530376434326 0.5338186025619507 0.9455916881561279
MemoryTrain:  epoch  8, batch     7 | loss: 3.0610530Losses:  2.636516571044922 0.5237475037574768 0.858260989189148
MemoryTrain:  epoch  8, batch     8 | loss: 2.6365166Losses:  2.5522100925445557 0.4975951313972473 0.7944417595863342
MemoryTrain:  epoch  8, batch     9 | loss: 2.5522101Losses:  2.6466798782348633 -0.0 0.922459602355957
MemoryTrain:  epoch  8, batch    10 | loss: 2.6466799Losses:  2.4362592697143555 0.2634255290031433 0.9040610194206238
MemoryTrain:  epoch  8, batch    11 | loss: 2.4362593Losses:  2.739088535308838 0.24865251779556274 1.0276697874069214
MemoryTrain:  epoch  8, batch    12 | loss: 2.7390885Losses:  1.4320379495620728 -0.0 0.13020405173301697
MemoryTrain:  epoch  8, batch    13 | loss: 1.4320379Losses:  2.677161931991577 0.5093557238578796 0.9460193514823914
MemoryTrain:  epoch  9, batch     0 | loss: 2.6771619Losses:  2.400433301925659 -0.0 0.991202175617218
MemoryTrain:  epoch  9, batch     1 | loss: 2.4004333Losses:  2.6680829524993896 0.2604463994503021 0.9616185426712036
MemoryTrain:  epoch  9, batch     2 | loss: 2.6680830Losses:  2.4142935276031494 -0.0 0.9727428555488586
MemoryTrain:  epoch  9, batch     3 | loss: 2.4142935Losses:  3.0980658531188965 0.247802734375 0.9510763883590698
MemoryTrain:  epoch  9, batch     4 | loss: 3.0980659Losses:  2.5413308143615723 0.2471718192100525 1.0336989164352417
MemoryTrain:  epoch  9, batch     5 | loss: 2.5413308Losses:  2.516258716583252 0.49117809534072876 0.8129564523696899
MemoryTrain:  epoch  9, batch     6 | loss: 2.5162587Losses:  2.641587257385254 0.26595085859298706 0.8642654418945312
MemoryTrain:  epoch  9, batch     7 | loss: 2.6415873Losses:  2.388258934020996 0.2322966307401657 0.89131760597229
MemoryTrain:  epoch  9, batch     8 | loss: 2.3882589Losses:  2.6548147201538086 -0.0 0.9641416668891907
MemoryTrain:  epoch  9, batch     9 | loss: 2.6548147Losses:  2.782634735107422 0.48576682806015015 0.8492320775985718
MemoryTrain:  epoch  9, batch    10 | loss: 2.7826347Losses:  2.498934030532837 0.2584657371044159 0.9668455719947815
MemoryTrain:  epoch  9, batch    11 | loss: 2.4989340Losses:  2.458125114440918 -0.0 1.0253113508224487
MemoryTrain:  epoch  9, batch    12 | loss: 2.4581251Losses:  1.3995181322097778 -0.0 0.12649525701999664
MemoryTrain:  epoch  9, batch    13 | loss: 1.3995181
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 91.07%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 89.20%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 88.02%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 68.75%,  total acc: 85.27%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 79.86%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 78.44%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 77.08%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 77.27%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 76.82%   [EVAL] batch:   24 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.85%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 80.47%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 80.11%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 79.41%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 79.29%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 78.82%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 78.21%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 78.69%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 79.73%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 80.67%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 80.97%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 81.39%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 81.79%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 82.55%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 82.91%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 83.09%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 83.29%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 83.37%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 83.56%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 83.52%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 83.77%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 83.94%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 84.00%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 84.53%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 84.68%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 84.13%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.37%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.39%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.63%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 77.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 77.72%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 78.39%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.03%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 81.47%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 83.52%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 82.90%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.68%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 82.77%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 82.89%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 83.99%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 84.08%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 84.30%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 84.31%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 84.10%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 83.91%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 83.59%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 83.42%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 82.88%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 82.69%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 82.67%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 82.41%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 81.93%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 81.81%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 81.80%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 81.68%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 81.67%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 81.88%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 81.97%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 81.96%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 81.45%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 80.18%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 79.04%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 77.94%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 76.77%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 75.64%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 75.26%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 75.52%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 75.86%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 75.93%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 76.00%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 75.90%   [EVAL] batch:   76 | acc: 56.25%,  total acc: 75.65%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 75.48%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 75.32%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 74.92%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 74.24%   [EVAL] batch:   82 | acc: 37.50%,  total acc: 73.80%   [EVAL] batch:   83 | acc: 37.50%,  total acc: 73.36%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 73.16%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 72.60%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 72.05%   [EVAL] batch:   87 | acc: 12.50%,  total acc: 71.38%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 70.58%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 69.79%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 69.02%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 68.27%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 67.61%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 67.02%   [EVAL] batch:   94 | acc: 75.00%,  total acc: 67.11%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 67.32%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 67.59%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 67.73%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 67.93%   [EVAL] batch:   99 | acc: 62.50%,  total acc: 67.88%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 68.44%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 68.99%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 69.23%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 69.16%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 68.63%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 68.06%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 67.56%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 67.12%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 66.52%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 66.21%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 66.01%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 65.82%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 65.89%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 65.81%   [EVAL] batch:  117 | acc: 62.50%,  total acc: 65.78%   [EVAL] batch:  118 | acc: 56.25%,  total acc: 65.70%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 65.52%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 65.34%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 65.22%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 65.19%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 65.27%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 65.30%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 65.28%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 65.06%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 64.89%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 64.78%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 64.66%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 64.74%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 64.96%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 65.39%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 65.67%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 66.08%   [EVAL] batch:  138 | acc: 62.50%,  total acc: 66.05%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 65.98%   [EVAL] batch:  140 | acc: 62.50%,  total acc: 65.96%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 66.02%   [EVAL] batch:  142 | acc: 68.75%,  total acc: 66.04%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 66.06%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 66.12%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 66.07%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 66.09%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 66.07%   [EVAL] batch:  149 | acc: 50.00%,  total acc: 65.96%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 65.52%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 65.09%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 64.71%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 64.29%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 63.91%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 63.50%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 63.46%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 63.37%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 63.29%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 63.20%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 63.24%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 63.16%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 63.19%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 63.26%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 63.33%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 63.40%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 63.39%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 63.39%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 63.20%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 63.05%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 62.90%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 62.79%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 62.61%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 62.54%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 62.96%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 63.17%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 63.78%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 63.94%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 63.97%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 64.10%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 64.26%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 64.38%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 64.47%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 64.46%   [EVAL] batch:  188 | acc: 50.00%,  total acc: 64.38%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 64.41%   [EVAL] batch:  190 | acc: 56.25%,  total acc: 64.37%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 64.29%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 64.31%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 64.30%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 64.36%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 64.35%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 64.37%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 64.43%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 64.42%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 64.50%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 64.46%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 64.48%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 64.53%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 64.55%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 64.48%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 64.50%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 64.31%   [EVAL] batch:  207 | acc: 0.00%,  total acc: 64.00%   [EVAL] batch:  208 | acc: 0.00%,  total acc: 63.70%   [EVAL] batch:  209 | acc: 0.00%,  total acc: 63.39%   [EVAL] batch:  210 | acc: 6.25%,  total acc: 63.12%   [EVAL] batch:  211 | acc: 18.75%,  total acc: 62.91%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 62.85%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 63.00%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 63.17%   [EVAL] batch:  215 | acc: 87.50%,  total acc: 63.28%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 63.45%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.62%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 63.76%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.92%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 64.08%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 64.25%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 64.41%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 64.56%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.72%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 64.77%   [EVAL] batch:  226 | acc: 75.00%,  total acc: 64.81%   [EVAL] batch:  227 | acc: 75.00%,  total acc: 64.86%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 64.90%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 65.03%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 65.07%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 65.06%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 64.97%   [EVAL] batch:  233 | acc: 56.25%,  total acc: 64.93%   [EVAL] batch:  234 | acc: 50.00%,  total acc: 64.87%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 64.86%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 64.79%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 64.81%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 64.88%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 65.03%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 65.09%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 65.19%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 65.25%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 65.32%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 65.36%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 65.40%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 65.36%   [EVAL] batch:  247 | acc: 62.50%,  total acc: 65.35%   [EVAL] batch:  248 | acc: 68.75%,  total acc: 65.36%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 65.45%   [EVAL] batch:  250 | acc: 62.50%,  total acc: 65.44%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 65.50%   [EVAL] batch:  252 | acc: 75.00%,  total acc: 65.54%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 65.55%   [EVAL] batch:  254 | acc: 87.50%,  total acc: 65.64%   [EVAL] batch:  255 | acc: 87.50%,  total acc: 65.72%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 65.71%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 65.70%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 65.71%   [EVAL] batch:  259 | acc: 62.50%,  total acc: 65.70%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 65.64%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 65.65%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 65.64%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 65.60%   [EVAL] batch:  264 | acc: 56.25%,  total acc: 65.57%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 65.51%   [EVAL] batch:  266 | acc: 50.00%,  total acc: 65.45%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 65.39%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 65.45%   [EVAL] batch:  269 | acc: 87.50%,  total acc: 65.53%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 66.01%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 66.17%   [EVAL] batch:  276 | acc: 43.75%,  total acc: 66.09%   [EVAL] batch:  277 | acc: 56.25%,  total acc: 66.05%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 66.06%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 66.00%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 65.97%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 66.00%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 66.01%   [EVAL] batch:  283 | acc: 37.50%,  total acc: 65.91%   [EVAL] batch:  284 | acc: 50.00%,  total acc: 65.86%   [EVAL] batch:  285 | acc: 25.00%,  total acc: 65.71%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 65.55%   [EVAL] batch:  287 | acc: 75.00%,  total acc: 65.58%   [EVAL] batch:  288 | acc: 56.25%,  total acc: 65.55%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 65.62%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 65.61%   [EVAL] batch:  291 | acc: 87.50%,  total acc: 65.69%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 65.72%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 65.71%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 65.72%   [EVAL] batch:  295 | acc: 68.75%,  total acc: 65.73%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 65.72%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 65.67%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 65.64%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 65.69%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 65.80%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 65.91%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 66.25%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 66.36%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 66.47%   [EVAL] batch:  307 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 67.01%   [EVAL] batch:  312 | acc: 81.25%,  total acc: 67.05%   [EVAL] batch:  313 | acc: 56.25%,  total acc: 67.02%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 66.98%   [EVAL] batch:  315 | acc: 87.50%,  total acc: 67.05%   [EVAL] batch:  316 | acc: 81.25%,  total acc: 67.09%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 67.12%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 67.16%   [EVAL] batch:  319 | acc: 87.50%,  total acc: 67.23%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 67.29%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 67.35%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 67.54%   [EVAL] batch:  325 | acc: 12.50%,  total acc: 67.37%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 67.22%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 67.05%   [EVAL] batch:  328 | acc: 25.00%,  total acc: 66.93%   [EVAL] batch:  329 | acc: 31.25%,  total acc: 66.82%   [EVAL] batch:  330 | acc: 18.75%,  total acc: 66.67%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 66.68%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 66.96%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 67.06%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 67.14%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 67.10%   [EVAL] batch:  338 | acc: 18.75%,  total acc: 66.96%   [EVAL] batch:  339 | acc: 12.50%,  total acc: 66.80%   [EVAL] batch:  340 | acc: 12.50%,  total acc: 66.64%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 66.47%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 66.29%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 66.19%   [EVAL] batch:  344 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 66.35%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 66.43%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 66.51%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.68%   [EVAL] batch:  350 | acc: 31.25%,  total acc: 66.58%   [EVAL] batch:  351 | acc: 50.00%,  total acc: 66.53%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 66.43%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 66.30%   [EVAL] batch:  354 | acc: 25.00%,  total acc: 66.18%   [EVAL] batch:  355 | acc: 43.75%,  total acc: 66.12%   [EVAL] batch:  356 | acc: 62.50%,  total acc: 66.11%   [EVAL] batch:  357 | acc: 81.25%,  total acc: 66.15%   [EVAL] batch:  358 | acc: 81.25%,  total acc: 66.19%   [EVAL] batch:  359 | acc: 81.25%,  total acc: 66.23%   [EVAL] batch:  360 | acc: 62.50%,  total acc: 66.22%   [EVAL] batch:  361 | acc: 75.00%,  total acc: 66.25%   [EVAL] batch:  362 | acc: 81.25%,  total acc: 66.29%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 66.24%   [EVAL] batch:  364 | acc: 68.75%,  total acc: 66.25%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 66.22%   [EVAL] batch:  366 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 66.25%   [EVAL] batch:  368 | acc: 68.75%,  total acc: 66.26%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 66.33%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 66.39%   [EVAL] batch:  371 | acc: 75.00%,  total acc: 66.41%   [EVAL] batch:  372 | acc: 68.75%,  total acc: 66.42%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 66.44%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 66.48%   [EVAL] batch:  375 | acc: 93.75%,  total acc: 66.56%   [EVAL] batch:  376 | acc: 93.75%,  total acc: 66.63%   [EVAL] batch:  377 | acc: 93.75%,  total acc: 66.70%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 66.75%   [EVAL] batch:  379 | acc: 87.50%,  total acc: 66.81%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 66.90%   [EVAL] batch:  381 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 67.00%   [EVAL] batch:  383 | acc: 81.25%,  total acc: 67.04%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 67.09%   [EVAL] batch:  385 | acc: 81.25%,  total acc: 67.13%   [EVAL] batch:  386 | acc: 75.00%,  total acc: 67.15%   [EVAL] batch:  387 | acc: 68.75%,  total acc: 67.16%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 67.16%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 67.15%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 67.14%   [EVAL] batch:  391 | acc: 62.50%,  total acc: 67.12%   [EVAL] batch:  392 | acc: 56.25%,  total acc: 67.10%   [EVAL] batch:  393 | acc: 68.75%,  total acc: 67.10%   [EVAL] batch:  394 | acc: 62.50%,  total acc: 67.09%   [EVAL] batch:  395 | acc: 50.00%,  total acc: 67.05%   [EVAL] batch:  396 | acc: 81.25%,  total acc: 67.08%   [EVAL] batch:  397 | acc: 68.75%,  total acc: 67.09%   [EVAL] batch:  398 | acc: 75.00%,  total acc: 67.11%   [EVAL] batch:  399 | acc: 62.50%,  total acc: 67.09%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 67.18%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 67.34%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 67.50%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 67.58%   [EVAL] batch:  407 | acc: 68.75%,  total acc: 67.59%   [EVAL] batch:  408 | acc: 56.25%,  total acc: 67.56%   [EVAL] batch:  409 | acc: 75.00%,  total acc: 67.58%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 67.56%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 67.54%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 67.55%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 67.71%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 67.79%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 68.00%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 68.23%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 68.31%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:  424 | acc: 93.75%,  total acc: 68.44%   [EVAL] batch:  425 | acc: 81.25%,  total acc: 68.47%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 68.53%   [EVAL] batch:  427 | acc: 87.50%,  total acc: 68.57%   [EVAL] batch:  428 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  429 | acc: 81.25%,  total acc: 68.66%   [EVAL] batch:  430 | acc: 87.50%,  total acc: 68.71%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 68.76%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 68.82%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 68.87%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 69.06%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 69.02%   
cur_acc:  ['0.9494', '0.6270', '0.7133', '0.7024', '0.7371', '0.6647', '0.8413']
his_acc:  ['0.9494', '0.7820', '0.7500', '0.7208', '0.6957', '0.6717', '0.6902']
Clustering into  38  clusters
Clusters:  [ 5  2 26 16  1  1 37  0  2 11  1 14 13 36  3 21 28  9  8  2 24  0  0  1
 36 11  5  1 19 19  6  8 29 14 23 19 22 24  4 17  6 35  7  4  0 20  9 10
 18  8  4 25 30  7  3  2 13  5 32  4 16  5  5  2 12 10 31 33  0  2 15 19
 24 34 27 19 17  8 11  5]
Losses:  9.309040069580078 3.4810304641723633 0.6623337268829346
CurrentTrain: epoch  0, batch     0 | loss: 9.3090401Losses:  12.275257110595703 4.006349563598633 0.6773639917373657
CurrentTrain: epoch  0, batch     1 | loss: 12.2752571Losses:  10.131753921508789 3.1736996173858643 0.7705768346786499
CurrentTrain: epoch  0, batch     2 | loss: 10.1317539Losses:  5.702431678771973 -0.0 0.11160297691822052
CurrentTrain: epoch  0, batch     3 | loss: 5.7024317Losses:  10.044591903686523 3.8893051147460938 0.6326460838317871
CurrentTrain: epoch  1, batch     0 | loss: 10.0445919Losses:  10.664835929870605 4.150250434875488 0.6861658692359924
CurrentTrain: epoch  1, batch     1 | loss: 10.6648359Losses:  7.753030300140381 2.5220139026641846 0.7378258109092712
CurrentTrain: epoch  1, batch     2 | loss: 7.7530303Losses:  2.925117015838623 -0.0 0.10303372144699097
CurrentTrain: epoch  1, batch     3 | loss: 2.9251170Losses:  7.908250331878662 3.709383249282837 0.6144920587539673
CurrentTrain: epoch  2, batch     0 | loss: 7.9082503Losses:  9.446026802062988 3.721524477005005 0.7699812650680542
CurrentTrain: epoch  2, batch     1 | loss: 9.4460268Losses:  8.90457534790039 3.487091302871704 0.736468493938446
CurrentTrain: epoch  2, batch     2 | loss: 8.9045753Losses:  4.529122829437256 -0.0 0.1041247546672821
CurrentTrain: epoch  2, batch     3 | loss: 4.5291228Losses:  9.267319679260254 4.056055068969727 0.6654156446456909
CurrentTrain: epoch  3, batch     0 | loss: 9.2673197Losses:  7.477237701416016 3.1810760498046875 0.723707377910614
CurrentTrain: epoch  3, batch     1 | loss: 7.4772377Losses:  7.375982284545898 3.2410736083984375 0.5850543975830078
CurrentTrain: epoch  3, batch     2 | loss: 7.3759823Losses:  2.942577838897705 -0.0 0.13815602660179138
CurrentTrain: epoch  3, batch     3 | loss: 2.9425778Losses:  6.4857587814331055 3.3016414642333984 0.5961018800735474
CurrentTrain: epoch  4, batch     0 | loss: 6.4857588Losses:  9.53184986114502 4.457792282104492 0.6961686611175537
CurrentTrain: epoch  4, batch     1 | loss: 9.5318499Losses:  7.1145172119140625 3.194105386734009 0.6945400834083557
CurrentTrain: epoch  4, batch     2 | loss: 7.1145172Losses:  3.6274898052215576 -0.0 0.09984276443719864
CurrentTrain: epoch  4, batch     3 | loss: 3.6274898Losses:  5.86882209777832 2.3834657669067383 0.7371904850006104
CurrentTrain: epoch  5, batch     0 | loss: 5.8688221Losses:  6.11968994140625 2.7127017974853516 0.6001749038696289
CurrentTrain: epoch  5, batch     1 | loss: 6.1196899Losses:  6.215568542480469 2.7587339878082275 0.6669213771820068
CurrentTrain: epoch  5, batch     2 | loss: 6.2155685Losses:  2.071671962738037 -0.0 0.10309523344039917
CurrentTrain: epoch  5, batch     3 | loss: 2.0716720Losses:  7.500211238861084 3.6643929481506348 0.6792663931846619
CurrentTrain: epoch  6, batch     0 | loss: 7.5002112Losses:  6.0812788009643555 2.7916274070739746 0.7246502041816711
CurrentTrain: epoch  6, batch     1 | loss: 6.0812788Losses:  5.828165531158447 3.287564277648926 0.5714486241340637
CurrentTrain: epoch  6, batch     2 | loss: 5.8281655Losses:  2.231786012649536 -0.0 0.09378038346767426
CurrentTrain: epoch  6, batch     3 | loss: 2.2317860Losses:  6.579205513000488 3.446521043777466 0.718985378742218
CurrentTrain: epoch  7, batch     0 | loss: 6.5792055Losses:  5.320239067077637 2.1699297428131104 0.7312669157981873
CurrentTrain: epoch  7, batch     1 | loss: 5.3202391Losses:  6.1029510498046875 3.400857448577881 0.5778388381004333
CurrentTrain: epoch  7, batch     2 | loss: 6.1029510Losses:  4.398807048797607 -0.0 0.10078451037406921
CurrentTrain: epoch  7, batch     3 | loss: 4.3988070Losses:  5.38306999206543 2.0338973999023438 0.7208438515663147
CurrentTrain: epoch  8, batch     0 | loss: 5.3830700Losses:  5.272085189819336 2.3468875885009766 0.6438266634941101
CurrentTrain: epoch  8, batch     1 | loss: 5.2720852Losses:  5.342658519744873 2.5954055786132812 0.6617065668106079
CurrentTrain: epoch  8, batch     2 | loss: 5.3426585Losses:  2.0815675258636475 -0.0 0.13547416031360626
CurrentTrain: epoch  8, batch     3 | loss: 2.0815675Losses:  5.936717987060547 3.0602216720581055 0.6440296173095703
CurrentTrain: epoch  9, batch     0 | loss: 5.9367180Losses:  5.174267292022705 2.608694314956665 0.6432819366455078
CurrentTrain: epoch  9, batch     1 | loss: 5.1742673Losses:  4.456911087036133 1.614145040512085 0.7037349343299866
CurrentTrain: epoch  9, batch     2 | loss: 4.4569111Losses:  2.750922441482544 -0.0 0.08099435269832611
CurrentTrain: epoch  9, batch     3 | loss: 2.7509224
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: the nikon d5500 features other improvements over its predecessor nikon d5300 .
Head Entity: nikon d5500
Tail Entity: nikon d5300
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: in the series of popular smartphone models, the iphone 12 follows the iphone 11, which was released the previous year.  
Head Entity: iphone 12  
Tail Entity: iphone 11  

Relation: follows  
Context: the latest installment in the series, the harry potter and the deathly hallows, follows the previous book, harry potter and the half-blood prince.  
Head Entity: harry potter and the deathly hallows  
Tail Entity: harry potter and the half-blood prince  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: She has been a professional player in the world of tennis, winning several championships throughout her career.  
Head Entity: She  
Tail Entity: tennis  

Relation: sport  
Context: The city is known for its vibrant soccer culture, with numerous teams competing in the national league.  
Head Entity: The city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of lucas, who had recently graduated from university.  
Head Entity: lucas  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon after it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English before being dubbed into several other languages.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was filmed in French and has since gained a global audience.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: the league champions for the fifth time in their history ( and the second season running ) were taunton town , but runners - up mangotsfield united took promotion to the southern league .
Head Entity: mangotsfield united
Tail Entity: southern league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After an impressive season, the team secured a spot in the prestigious league, where they will compete against top clubs, including their rivals, the city wanderers.  
Head Entity: city wanderers  
Tail Entity: prestigious league  

Relation: league  
Context: The young striker made headlines when he signed with the national league team, hoping to showcase his talent and help them achieve promotion.  
Head Entity: national league team  
Tail Entity: promotion  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: retrieved 29 september 2010 . the daughter of actress xenia desni , tamara desni was born in berlin .
Head Entity: tamara desni
Tail Entity: xenia desni
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: on 15th march 2015, the famous singer and actress, jennifer lopez, celebrated her birthday with her children, emme and max, who are her pride and joy.  
Head Entity: emme  
Tail Entity: jennifer lopez  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much little sophia adored her mother, elena, as they played together in the garden.  
Head Entity: sophia  
Tail Entity: elena  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: wharton tiers ( born 1953 , in philadelphia ) is an american audio engineer , record producer , drummer and percussionist .
Head Entity: wharton tiers
Tail Entity: drummer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: john smith, a talented musician from new orleans, is known for his exceptional skills as a saxophonist in various jazz bands.  
Head Entity: john smith  
Tail Entity: saxophonist  

Relation: instrument  
Context: the renowned music teacher, elena rodriguez, has dedicated her life to teaching children how to play the violin in her local community.  
Head Entity: elena rodriguez  
Tail Entity: violin
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: ploegsteert is a village in belgium located in the municipality of comines - warneton in the hainaut province and is the most westerly settlement of the walloon region .
Head Entity: hainaut
Tail Entity: belgium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the city of prague is the capital of the czech republic and is known for its beautiful architecture and rich history.  
Head Entity: prague  
Tail Entity: czech republic  

Relation: country  
Context: the great barrier reef is located off the coast of australia and is the largest coral reef system in the world.  
Head Entity: great barrier reef  
Tail Entity: australia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Losses:  4.2410712242126465 0.5380088686943054 0.9488431811332703
MemoryTrain:  epoch  0, batch     0 | loss: 4.2410712Losses:  3.445843458175659 0.26221027970314026 0.8536946177482605
MemoryTrain:  epoch  0, batch     1 | loss: 3.4458435Losses:  4.206524848937988 0.5085961818695068 0.8487370610237122
MemoryTrain:  epoch  0, batch     2 | loss: 4.2065248Losses:  3.654229164123535 -0.0 0.9852058291435242
MemoryTrain:  epoch  0, batch     3 | loss: 3.6542292Losses:  3.829521656036377 -0.0 1.0335384607315063
MemoryTrain:  epoch  0, batch     4 | loss: 3.8295217Losses:  3.9322690963745117 -0.0 1.0797699689865112
MemoryTrain:  epoch  0, batch     5 | loss: 3.9322691Losses:  3.6151063442230225 -0.0 0.9975431561470032
MemoryTrain:  epoch  0, batch     6 | loss: 3.6151063Losses:  3.7890191078186035 0.49067747592926025 0.8678755760192871
MemoryTrain:  epoch  0, batch     7 | loss: 3.7890191Losses:  4.210299015045166 0.2682167887687683 0.968357264995575
MemoryTrain:  epoch  0, batch     8 | loss: 4.2102990Losses:  3.8932032585144043 0.5837783813476562 0.8578987121582031
MemoryTrain:  epoch  0, batch     9 | loss: 3.8932033Losses:  4.090509414672852 -0.0 1.0216273069381714
MemoryTrain:  epoch  0, batch    10 | loss: 4.0905094Losses:  4.57802152633667 0.6207524538040161 1.01505708694458
MemoryTrain:  epoch  0, batch    11 | loss: 4.5780215Losses:  3.5827858448028564 0.5708438754081726 0.8084978461265564
MemoryTrain:  epoch  0, batch    12 | loss: 3.5827858Losses:  4.6748552322387695 -0.0 1.032609224319458
MemoryTrain:  epoch  0, batch    13 | loss: 4.6748552Losses:  3.5926363468170166 0.26930928230285645 1.0352439880371094
MemoryTrain:  epoch  0, batch    14 | loss: 3.5926363Losses:  3.1253085136413574 0.27612459659576416 0.9940704107284546
MemoryTrain:  epoch  1, batch     0 | loss: 3.1253085Losses:  3.701784133911133 -0.0 0.8928818106651306
MemoryTrain:  epoch  1, batch     1 | loss: 3.7017841Losses:  3.6267952919006348 0.5305182337760925 0.8989617228507996
MemoryTrain:  epoch  1, batch     2 | loss: 3.6267953Losses:  4.22490119934082 0.26579922437667847 0.9791983366012573
MemoryTrain:  epoch  1, batch     3 | loss: 4.2249012Losses:  3.022697925567627 -0.0 0.9900655150413513
MemoryTrain:  epoch  1, batch     4 | loss: 3.0226979Losses:  3.988999366760254 0.28069764375686646 0.9235698580741882
MemoryTrain:  epoch  1, batch     5 | loss: 3.9889994Losses:  3.900169610977173 0.836240828037262 0.8665034770965576
MemoryTrain:  epoch  1, batch     6 | loss: 3.9001696Losses:  3.416597366333008 -0.0 0.9426608085632324
MemoryTrain:  epoch  1, batch     7 | loss: 3.4165974Losses:  3.772719621658325 0.24819274246692657 1.0137650966644287
MemoryTrain:  epoch  1, batch     8 | loss: 3.7727196Losses:  3.6099538803100586 0.22103866934776306 0.952441930770874
MemoryTrain:  epoch  1, batch     9 | loss: 3.6099539Losses:  2.8971428871154785 -0.0 0.9355195760726929
MemoryTrain:  epoch  1, batch    10 | loss: 2.8971429Losses:  2.557065963745117 -0.0 0.880432665348053
MemoryTrain:  epoch  1, batch    11 | loss: 2.5570660Losses:  3.0844836235046387 -0.0 1.0488018989562988
MemoryTrain:  epoch  1, batch    12 | loss: 3.0844836Losses:  4.689919471740723 0.5610349178314209 0.9163188338279724
MemoryTrain:  epoch  1, batch    13 | loss: 4.6899195Losses:  4.017065048217773 0.6478824615478516 0.8406763076782227
MemoryTrain:  epoch  1, batch    14 | loss: 4.0170650Losses:  3.8058342933654785 0.48355647921562195 0.8442007899284363
MemoryTrain:  epoch  2, batch     0 | loss: 3.8058343Losses:  2.6279237270355225 0.2496081292629242 0.9186217188835144
MemoryTrain:  epoch  2, batch     1 | loss: 2.6279237Losses:  3.3822126388549805 0.5025641322135925 0.8597342371940613
MemoryTrain:  epoch  2, batch     2 | loss: 3.3822126Losses:  3.3281607627868652 0.5003089904785156 0.9214843511581421
MemoryTrain:  epoch  2, batch     3 | loss: 3.3281608Losses:  3.479191303253174 0.24815192818641663 0.79549241065979
MemoryTrain:  epoch  2, batch     4 | loss: 3.4791913Losses:  4.221773624420166 1.3759028911590576 0.7328129410743713
MemoryTrain:  epoch  2, batch     5 | loss: 4.2217736Losses:  3.0573556423187256 0.2572905123233795 1.0692003965377808
MemoryTrain:  epoch  2, batch     6 | loss: 3.0573556Losses:  2.9322900772094727 0.2567107081413269 0.9708045721054077
MemoryTrain:  epoch  2, batch     7 | loss: 2.9322901Losses:  3.4691553115844727 -0.0 0.9973751306533813
MemoryTrain:  epoch  2, batch     8 | loss: 3.4691553Losses:  3.2599549293518066 0.2514657974243164 0.9953795671463013
MemoryTrain:  epoch  2, batch     9 | loss: 3.2599549Losses:  3.218146324157715 -0.0 0.9393075704574585
MemoryTrain:  epoch  2, batch    10 | loss: 3.2181463Losses:  3.9282796382904053 1.0872927904129028 0.7722288966178894
MemoryTrain:  epoch  2, batch    11 | loss: 3.9282796Losses:  3.050755023956299 -0.0 1.0436418056488037
MemoryTrain:  epoch  2, batch    12 | loss: 3.0507550Losses:  3.1785566806793213 0.508805513381958 0.9550673961639404
MemoryTrain:  epoch  2, batch    13 | loss: 3.1785567Losses:  2.7669670581817627 0.23216275870800018 0.9045478701591492
MemoryTrain:  epoch  2, batch    14 | loss: 2.7669671Losses:  2.7990479469299316 0.2530122399330139 0.9113097190856934
MemoryTrain:  epoch  3, batch     0 | loss: 2.7990479Losses:  4.206270217895508 0.8104532957077026 0.8600820302963257
MemoryTrain:  epoch  3, batch     1 | loss: 4.2062702Losses:  3.6742327213287354 0.24838733673095703 0.9664480686187744
MemoryTrain:  epoch  3, batch     2 | loss: 3.6742327Losses:  2.9952969551086426 0.2549338936805725 0.9390648007392883
MemoryTrain:  epoch  3, batch     3 | loss: 2.9952970Losses:  3.1511662006378174 0.5206804275512695 0.9053122401237488
MemoryTrain:  epoch  3, batch     4 | loss: 3.1511662Losses:  2.7533533573150635 -0.0 0.9043068885803223
MemoryTrain:  epoch  3, batch     5 | loss: 2.7533534Losses:  3.4453377723693848 0.5315537452697754 0.9728323221206665
MemoryTrain:  epoch  3, batch     6 | loss: 3.4453378Losses:  2.4482412338256836 0.2558478116989136 0.8618298768997192
MemoryTrain:  epoch  3, batch     7 | loss: 2.4482412Losses:  3.023012638092041 0.566201388835907 0.7918052077293396
MemoryTrain:  epoch  3, batch     8 | loss: 3.0230126Losses:  3.3557379245758057 0.24733945727348328 0.9888057112693787
MemoryTrain:  epoch  3, batch     9 | loss: 3.3557379Losses:  2.3625059127807617 -0.0 0.9683629870414734
MemoryTrain:  epoch  3, batch    10 | loss: 2.3625059Losses:  3.487372636795044 -0.0 1.037627935409546
MemoryTrain:  epoch  3, batch    11 | loss: 3.4873726Losses:  2.667001962661743 -0.0 1.0791902542114258
MemoryTrain:  epoch  3, batch    12 | loss: 2.6670020Losses:  3.3850162029266357 1.1312710046768188 0.8753530383110046
MemoryTrain:  epoch  3, batch    13 | loss: 3.3850162Losses:  2.582399368286133 0.25606876611709595 0.9243227243423462
MemoryTrain:  epoch  3, batch    14 | loss: 2.5823994Losses:  3.143693685531616 0.5062873959541321 0.9698483943939209
MemoryTrain:  epoch  4, batch     0 | loss: 3.1436937Losses:  2.6398096084594727 0.24327151477336884 0.970242977142334
MemoryTrain:  epoch  4, batch     1 | loss: 2.6398096Losses:  2.48909068107605 -0.0 0.783052384853363
MemoryTrain:  epoch  4, batch     2 | loss: 2.4890907Losses:  2.8305559158325195 0.26810315251350403 0.8756041526794434
MemoryTrain:  epoch  4, batch     3 | loss: 2.8305559Losses:  2.558286666870117 -0.0 0.9159156084060669
MemoryTrain:  epoch  4, batch     4 | loss: 2.5582867Losses:  3.597761869430542 0.5832375884056091 0.8983590006828308
MemoryTrain:  epoch  4, batch     5 | loss: 3.5977619Losses:  3.3426122665405273 0.4670250117778778 1.0086373090744019
MemoryTrain:  epoch  4, batch     6 | loss: 3.3426123Losses:  2.2220709323883057 -0.0 0.9713525176048279
MemoryTrain:  epoch  4, batch     7 | loss: 2.2220709Losses:  2.4913322925567627 -0.0 1.0724073648452759
MemoryTrain:  epoch  4, batch     8 | loss: 2.4913323Losses:  2.9515366554260254 0.24584627151489258 0.8945518732070923
MemoryTrain:  epoch  4, batch     9 | loss: 2.9515367Losses:  2.780880928039551 0.5243163108825684 0.857039749622345
MemoryTrain:  epoch  4, batch    10 | loss: 2.7808809Losses:  4.286708831787109 1.1437036991119385 0.903612494468689
MemoryTrain:  epoch  4, batch    11 | loss: 4.2867088Losses:  3.301800012588501 0.5279300808906555 0.879671037197113
MemoryTrain:  epoch  4, batch    12 | loss: 3.3018000Losses:  2.7569196224212646 0.2801945209503174 1.0466187000274658
MemoryTrain:  epoch  4, batch    13 | loss: 2.7569196Losses:  2.9776668548583984 0.5372801423072815 1.046889066696167
MemoryTrain:  epoch  4, batch    14 | loss: 2.9776669Losses:  2.800515651702881 0.49479812383651733 0.9088438749313354
MemoryTrain:  epoch  5, batch     0 | loss: 2.8005157Losses:  2.174801826477051 -0.0 0.9543166756629944
MemoryTrain:  epoch  5, batch     1 | loss: 2.1748018Losses:  3.0222272872924805 0.2508257031440735 0.9480661153793335
MemoryTrain:  epoch  5, batch     2 | loss: 3.0222273Losses:  2.602344512939453 0.23439034819602966 0.9591295123100281
MemoryTrain:  epoch  5, batch     3 | loss: 2.6023445Losses:  3.187396764755249 0.2536606788635254 1.0054774284362793
MemoryTrain:  epoch  5, batch     4 | loss: 3.1873968Losses:  2.9016382694244385 0.2564297318458557 0.8428854942321777
MemoryTrain:  epoch  5, batch     5 | loss: 2.9016383Losses:  2.601080894470215 0.2360042780637741 0.8604468107223511
MemoryTrain:  epoch  5, batch     6 | loss: 2.6010809Losses:  3.0234375 0.26340043544769287 0.869577944278717
MemoryTrain:  epoch  5, batch     7 | loss: 3.0234375Losses:  2.8016746044158936 0.24569684267044067 0.9902860522270203
MemoryTrain:  epoch  5, batch     8 | loss: 2.8016746Losses:  2.490589141845703 0.2551112174987793 0.7977625131607056
MemoryTrain:  epoch  5, batch     9 | loss: 2.4905891Losses:  2.5269017219543457 -0.0 0.8374873399734497
MemoryTrain:  epoch  5, batch    10 | loss: 2.5269017Losses:  2.7663915157318115 0.5070359110832214 0.8920194506645203
MemoryTrain:  epoch  5, batch    11 | loss: 2.7663915Losses:  2.3715131282806396 -0.0 0.9419814348220825
MemoryTrain:  epoch  5, batch    12 | loss: 2.3715131Losses:  2.586477518081665 -0.0 1.0145736932754517
MemoryTrain:  epoch  5, batch    13 | loss: 2.5864775Losses:  2.4428861141204834 -0.0 0.9550750851631165
MemoryTrain:  epoch  5, batch    14 | loss: 2.4428861Losses:  2.3721537590026855 -0.0 0.9214562177658081
MemoryTrain:  epoch  6, batch     0 | loss: 2.3721538Losses:  2.677866220474243 0.2409251183271408 1.0571942329406738
MemoryTrain:  epoch  6, batch     1 | loss: 2.6778662Losses:  2.65305233001709 0.2918112277984619 0.905699610710144
MemoryTrain:  epoch  6, batch     2 | loss: 2.6530523Losses:  2.2136478424072266 -0.0 0.8677457571029663
MemoryTrain:  epoch  6, batch     3 | loss: 2.2136478Losses:  2.6365790367126465 0.24366196990013123 0.9547150135040283
MemoryTrain:  epoch  6, batch     4 | loss: 2.6365790Losses:  2.404994010925293 -0.0 1.0268973112106323
MemoryTrain:  epoch  6, batch     5 | loss: 2.4049940Losses:  2.768367290496826 0.4992258846759796 0.9082608222961426
MemoryTrain:  epoch  6, batch     6 | loss: 2.7683673Losses:  2.756248950958252 0.25661319494247437 0.9761675596237183
MemoryTrain:  epoch  6, batch     7 | loss: 2.7562490Losses:  3.006648540496826 0.2519322335720062 0.9987990856170654
MemoryTrain:  epoch  6, batch     8 | loss: 3.0066485Losses:  2.8018064498901367 -0.0 1.0090160369873047
MemoryTrain:  epoch  6, batch     9 | loss: 2.8018064Losses:  2.603506565093994 0.2680082321166992 0.9676598310470581
MemoryTrain:  epoch  6, batch    10 | loss: 2.6035066Losses:  2.4001665115356445 -0.0 0.9734488129615784
MemoryTrain:  epoch  6, batch    11 | loss: 2.4001665Losses:  2.251146078109741 -0.0 0.8686379790306091
MemoryTrain:  epoch  6, batch    12 | loss: 2.2511461Losses:  2.7329039573669434 0.23166778683662415 1.0036417245864868
MemoryTrain:  epoch  6, batch    13 | loss: 2.7329040Losses:  2.7677783966064453 0.3276703357696533 0.9048066139221191
MemoryTrain:  epoch  6, batch    14 | loss: 2.7677784Losses:  2.5332260131835938 -0.0 1.0301506519317627
MemoryTrain:  epoch  7, batch     0 | loss: 2.5332260Losses:  2.5202298164367676 0.26536333560943604 0.8966703414916992
MemoryTrain:  epoch  7, batch     1 | loss: 2.5202298Losses:  2.4147605895996094 0.24528872966766357 0.7955358624458313
MemoryTrain:  epoch  7, batch     2 | loss: 2.4147606Losses:  2.3176701068878174 -0.0 0.9669609665870667
MemoryTrain:  epoch  7, batch     3 | loss: 2.3176701Losses:  2.5403809547424316 0.2582842707633972 1.0186107158660889
MemoryTrain:  epoch  7, batch     4 | loss: 2.5403810Losses:  2.8827154636383057 0.551708996295929 0.7893059849739075
MemoryTrain:  epoch  7, batch     5 | loss: 2.8827155Losses:  2.9611477851867676 0.3146745562553406 0.8952754735946655
MemoryTrain:  epoch  7, batch     6 | loss: 2.9611478Losses:  2.426163911819458 -0.0 1.0752465724945068
MemoryTrain:  epoch  7, batch     7 | loss: 2.4261639Losses:  2.582298517227173 -0.0 0.9629533290863037
MemoryTrain:  epoch  7, batch     8 | loss: 2.5822985Losses:  2.8890507221221924 0.5605654120445251 0.8151229023933411
MemoryTrain:  epoch  7, batch     9 | loss: 2.8890507Losses:  2.440913200378418 0.2547210454940796 0.9048823118209839
MemoryTrain:  epoch  7, batch    10 | loss: 2.4409132Losses:  2.3666975498199463 0.2648668885231018 0.856935977935791
MemoryTrain:  epoch  7, batch    11 | loss: 2.3666975Losses:  2.3076725006103516 -0.0 0.9909932613372803
MemoryTrain:  epoch  7, batch    12 | loss: 2.3076725Losses:  2.626988649368286 0.2533818483352661 0.8500962257385254
MemoryTrain:  epoch  7, batch    13 | loss: 2.6269886Losses:  2.4627346992492676 0.23189690709114075 0.9777471423149109
MemoryTrain:  epoch  7, batch    14 | loss: 2.4627347Losses:  2.3288097381591797 -0.0 1.0069187879562378
MemoryTrain:  epoch  8, batch     0 | loss: 2.3288097Losses:  2.774597644805908 0.4863109886646271 0.956133246421814
MemoryTrain:  epoch  8, batch     1 | loss: 2.7745976Losses:  2.6353044509887695 0.23601529002189636 0.9780585765838623
MemoryTrain:  epoch  8, batch     2 | loss: 2.6353045Losses:  2.384068012237549 0.24276480078697205 0.8463565707206726
MemoryTrain:  epoch  8, batch     3 | loss: 2.3840680Losses:  2.55712890625 -0.0 0.9084135293960571
MemoryTrain:  epoch  8, batch     4 | loss: 2.5571289Losses:  2.225609302520752 -0.0 0.9367203712463379
MemoryTrain:  epoch  8, batch     5 | loss: 2.2256093Losses:  2.266974449157715 0.23569363355636597 0.7900863885879517
MemoryTrain:  epoch  8, batch     6 | loss: 2.2669744Losses:  2.6483914852142334 0.524486780166626 0.7784014344215393
MemoryTrain:  epoch  8, batch     7 | loss: 2.6483915Losses:  2.4710278511047363 0.23042185604572296 1.0068176984786987
MemoryTrain:  epoch  8, batch     8 | loss: 2.4710279Losses:  2.587062120437622 0.2815892994403839 1.0162066221237183
MemoryTrain:  epoch  8, batch     9 | loss: 2.5870621Losses:  2.57252836227417 0.24370621144771576 0.8621985912322998
MemoryTrain:  epoch  8, batch    10 | loss: 2.5725284Losses:  2.7205960750579834 0.500321090221405 0.9547765254974365
MemoryTrain:  epoch  8, batch    11 | loss: 2.7205961Losses:  2.6187045574188232 0.47945156693458557 0.8985282778739929
MemoryTrain:  epoch  8, batch    12 | loss: 2.6187046Losses:  2.7548372745513916 0.3219565749168396 0.9915595054626465
MemoryTrain:  epoch  8, batch    13 | loss: 2.7548373Losses:  2.7916927337646484 0.7115893363952637 0.718285083770752
MemoryTrain:  epoch  8, batch    14 | loss: 2.7916927Losses:  2.2952535152435303 -0.0 1.019014835357666
MemoryTrain:  epoch  9, batch     0 | loss: 2.2952535Losses:  2.2513957023620605 -0.0 0.97128826379776
MemoryTrain:  epoch  9, batch     1 | loss: 2.2513957Losses:  3.072883129119873 0.8299574851989746 0.9066388607025146
MemoryTrain:  epoch  9, batch     2 | loss: 3.0728831Losses:  2.625016927719116 0.4741973876953125 0.8837227821350098
MemoryTrain:  epoch  9, batch     3 | loss: 2.6250169Losses:  2.4918336868286133 0.2601073980331421 0.9401585459709167
MemoryTrain:  epoch  9, batch     4 | loss: 2.4918337Losses:  2.1736700534820557 -0.0 0.9017653465270996
MemoryTrain:  epoch  9, batch     5 | loss: 2.1736701Losses:  2.5111188888549805 0.23423969745635986 1.0059245824813843
MemoryTrain:  epoch  9, batch     6 | loss: 2.5111189Losses:  2.2935380935668945 -0.0 0.9944940805435181
MemoryTrain:  epoch  9, batch     7 | loss: 2.2935381Losses:  2.191077709197998 0.2283066213130951 0.7253913283348083
MemoryTrain:  epoch  9, batch     8 | loss: 2.1910777Losses:  2.217195749282837 -0.0 0.8366193175315857
MemoryTrain:  epoch  9, batch     9 | loss: 2.2171957Losses:  2.4592623710632324 0.24526149034500122 1.0092253684997559
MemoryTrain:  epoch  9, batch    10 | loss: 2.4592624Losses:  2.5624990463256836 0.24919898808002472 1.0104889869689941
MemoryTrain:  epoch  9, batch    11 | loss: 2.5624990Losses:  2.3301920890808105 -0.0 0.9792126417160034
MemoryTrain:  epoch  9, batch    12 | loss: 2.3301921Losses:  2.3043158054351807 -0.0 0.9707952737808228
MemoryTrain:  epoch  9, batch    13 | loss: 2.3043158Losses:  2.3261451721191406 -0.0 0.9817354083061218
MemoryTrain:  epoch  9, batch    14 | loss: 2.3261452
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 28.12%   [EVAL] batch:    2 | acc: 18.75%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 28.75%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 31.25%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 34.38%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 42.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 46.02%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 47.92%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 49.52%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 49.55%   [EVAL] batch:   14 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 50.78%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 50.00%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 52.08%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 52.96%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 55.00%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 57.14%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 58.81%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 60.60%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 62.24%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 63.25%   [EVAL] batch:   25 | acc: 81.25%,  total acc: 63.94%   [EVAL] batch:   26 | acc: 87.50%,  total acc: 64.81%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 65.95%   [EVAL] batch:   29 | acc: 81.25%,  total acc: 66.46%   [EVAL] batch:   30 | acc: 81.25%,  total acc: 66.94%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 67.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 69.67%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 70.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 71.01%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:   37 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 71.47%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 70.78%   [EVAL] batch:   40 | acc: 31.25%,  total acc: 69.82%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 69.49%   [EVAL] batch:   42 | acc: 31.25%,  total acc: 68.60%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 69.02%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 69.15%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 70.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 70.55%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 70.52%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 70.49%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 70.45%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 69.87%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 69.41%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 69.61%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 69.49%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 69.48%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 69.57%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 69.96%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 69.35%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 70.45%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 67.79%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 75.60%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 75.28%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 80.68%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 80.33%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 80.18%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.56%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 80.74%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 80.92%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 81.72%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.01%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.14%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 82.41%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 82.24%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 82.22%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 82.20%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 82.18%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 81.90%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 81.89%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 81.50%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 81.25%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 81.01%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 80.90%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 80.44%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 79.89%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 79.69%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 79.61%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 79.20%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 79.13%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 79.38%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 79.30%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 79.13%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 78.67%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 77.44%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 76.25%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 75.09%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 73.97%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 72.89%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 72.28%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 72.32%   [EVAL] batch:   70 | acc: 93.75%,  total acc: 72.62%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 73.29%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 73.40%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 73.50%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 73.30%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 73.16%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 73.02%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 72.81%   [EVAL] batch:   80 | acc: 68.75%,  total acc: 72.76%   [EVAL] batch:   81 | acc: 18.75%,  total acc: 72.10%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 71.61%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 71.13%   [EVAL] batch:   84 | acc: 56.25%,  total acc: 70.96%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 70.42%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 69.97%   [EVAL] batch:   87 | acc: 18.75%,  total acc: 69.39%   [EVAL] batch:   88 | acc: 0.00%,  total acc: 68.61%   [EVAL] batch:   89 | acc: 0.00%,  total acc: 67.85%   [EVAL] batch:   90 | acc: 0.00%,  total acc: 67.10%   [EVAL] batch:   91 | acc: 0.00%,  total acc: 66.37%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 65.73%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 65.16%   [EVAL] batch:   94 | acc: 81.25%,  total acc: 65.33%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 65.56%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 65.94%   [EVAL] batch:   98 | acc: 87.50%,  total acc: 66.16%   [EVAL] batch:   99 | acc: 56.25%,  total acc: 66.06%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  101 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 67.31%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 67.56%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 67.87%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 67.52%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 67.07%   [EVAL] batch:  108 | acc: 6.25%,  total acc: 66.51%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 66.02%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 65.65%   [EVAL] batch:  111 | acc: 0.00%,  total acc: 65.07%   [EVAL] batch:  112 | acc: 31.25%,  total acc: 64.77%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 64.64%   [EVAL] batch:  114 | acc: 43.75%,  total acc: 64.46%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 64.55%   [EVAL] batch:  116 | acc: 56.25%,  total acc: 64.48%   [EVAL] batch:  117 | acc: 68.75%,  total acc: 64.51%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 64.50%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 64.32%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 64.10%   [EVAL] batch:  121 | acc: 50.00%,  total acc: 63.99%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 63.92%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 64.01%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 64.00%   [EVAL] batch:  125 | acc: 31.25%,  total acc: 63.74%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 63.34%   [EVAL] batch:  127 | acc: 12.50%,  total acc: 62.94%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 62.65%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 62.40%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 62.21%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 62.41%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 62.69%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 62.87%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 63.01%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 63.24%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 63.50%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 63.63%   [EVAL] batch:  138 | acc: 43.75%,  total acc: 63.49%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 63.35%   [EVAL] batch:  140 | acc: 62.50%,  total acc: 63.34%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 63.42%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 63.33%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 63.37%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 63.45%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 63.57%   [EVAL] batch:  146 | acc: 68.75%,  total acc: 63.61%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 63.64%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 63.72%   [EVAL] batch:  149 | acc: 50.00%,  total acc: 63.62%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 63.20%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 62.79%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 62.42%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 62.01%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 61.69%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 61.30%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 61.15%   [EVAL] batch:  157 | acc: 43.75%,  total acc: 61.04%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 60.93%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 60.86%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 60.91%   [EVAL] batch:  161 | acc: 50.00%,  total acc: 60.84%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 60.77%   [EVAL] batch:  163 | acc: 62.50%,  total acc: 60.79%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 60.87%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 60.96%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 61.04%   [EVAL] batch:  167 | acc: 68.75%,  total acc: 61.09%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 61.13%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 60.92%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 60.78%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 60.57%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 60.48%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 60.31%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 60.25%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 60.48%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 60.70%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 60.92%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 61.14%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 61.35%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 61.57%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 61.57%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 61.51%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 61.45%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 61.55%   [EVAL] batch:  185 | acc: 62.50%,  total acc: 61.56%   [EVAL] batch:  186 | acc: 50.00%,  total acc: 61.50%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 61.47%   [EVAL] batch:  188 | acc: 50.00%,  total acc: 61.41%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 61.45%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 61.52%   [EVAL] batch:  191 | acc: 50.00%,  total acc: 61.46%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:  193 | acc: 50.00%,  total acc: 61.40%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 61.51%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 61.54%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 61.55%   [EVAL] batch:  197 | acc: 68.75%,  total acc: 61.58%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 61.62%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 61.72%   [EVAL] batch:  200 | acc: 62.50%,  total acc: 61.72%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 61.73%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 61.79%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 61.76%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 61.71%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 61.74%   [EVAL] batch:  206 | acc: 25.00%,  total acc: 61.56%   [EVAL] batch:  207 | acc: 6.25%,  total acc: 61.30%   [EVAL] batch:  208 | acc: 0.00%,  total acc: 61.00%   [EVAL] batch:  209 | acc: 0.00%,  total acc: 60.71%   [EVAL] batch:  210 | acc: 6.25%,  total acc: 60.46%   [EVAL] batch:  211 | acc: 25.00%,  total acc: 60.29%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 60.21%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 60.37%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 60.55%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 60.65%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 60.83%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 61.01%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 61.16%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 61.34%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 61.51%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 61.68%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 61.86%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 62.03%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 62.19%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 62.25%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 62.25%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 62.28%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 62.34%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 62.47%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:  231 | acc: 50.00%,  total acc: 62.45%   [EVAL] batch:  232 | acc: 18.75%,  total acc: 62.26%   [EVAL] batch:  233 | acc: 31.25%,  total acc: 62.13%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 61.97%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 61.92%   [EVAL] batch:  236 | acc: 25.00%,  total acc: 61.76%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 61.76%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 61.87%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 62.03%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 62.11%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 62.22%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 62.29%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 62.35%   [EVAL] batch:  244 | acc: 12.50%,  total acc: 62.14%   [EVAL] batch:  245 | acc: 37.50%,  total acc: 62.04%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 61.87%   [EVAL] batch:  247 | acc: 25.00%,  total acc: 61.72%   [EVAL] batch:  248 | acc: 0.00%,  total acc: 61.47%   [EVAL] batch:  249 | acc: 25.00%,  total acc: 61.32%   [EVAL] batch:  250 | acc: 43.75%,  total acc: 61.25%   [EVAL] batch:  251 | acc: 50.00%,  total acc: 61.21%   [EVAL] batch:  252 | acc: 56.25%,  total acc: 61.19%   [EVAL] batch:  253 | acc: 56.25%,  total acc: 61.17%   [EVAL] batch:  254 | acc: 68.75%,  total acc: 61.20%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 61.28%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 61.28%   [EVAL] batch:  257 | acc: 68.75%,  total acc: 61.31%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 61.34%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 61.37%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 61.33%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 61.33%   [EVAL] batch:  262 | acc: 62.50%,  total acc: 61.34%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 61.32%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 61.27%   [EVAL] batch:  265 | acc: 50.00%,  total acc: 61.23%   [EVAL] batch:  266 | acc: 43.75%,  total acc: 61.17%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 61.12%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 61.20%   [EVAL] batch:  269 | acc: 81.25%,  total acc: 61.27%   [EVAL] batch:  270 | acc: 100.00%,  total acc: 61.42%   [EVAL] batch:  271 | acc: 100.00%,  total acc: 61.56%   [EVAL] batch:  272 | acc: 100.00%,  total acc: 61.70%   [EVAL] batch:  273 | acc: 93.75%,  total acc: 61.82%   [EVAL] batch:  274 | acc: 100.00%,  total acc: 61.95%   [EVAL] batch:  275 | acc: 75.00%,  total acc: 62.00%   [EVAL] batch:  276 | acc: 37.50%,  total acc: 61.91%   [EVAL] batch:  277 | acc: 62.50%,  total acc: 61.92%   [EVAL] batch:  278 | acc: 68.75%,  total acc: 61.94%   [EVAL] batch:  279 | acc: 50.00%,  total acc: 61.90%   [EVAL] batch:  280 | acc: 56.25%,  total acc: 61.88%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 61.92%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 61.95%   [EVAL] batch:  283 | acc: 31.25%,  total acc: 61.84%   [EVAL] batch:  284 | acc: 43.75%,  total acc: 61.78%   [EVAL] batch:  285 | acc: 37.50%,  total acc: 61.69%   [EVAL] batch:  286 | acc: 18.75%,  total acc: 61.54%   [EVAL] batch:  287 | acc: 56.25%,  total acc: 61.52%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 61.53%   [EVAL] batch:  289 | acc: 87.50%,  total acc: 61.62%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 61.62%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 61.69%   [EVAL] batch:  292 | acc: 75.00%,  total acc: 61.73%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 61.73%   [EVAL] batch:  294 | acc: 68.75%,  total acc: 61.76%   [EVAL] batch:  295 | acc: 56.25%,  total acc: 61.74%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 61.74%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 61.70%   [EVAL] batch:  298 | acc: 43.75%,  total acc: 61.64%   [EVAL] batch:  299 | acc: 81.25%,  total acc: 61.71%   [EVAL] batch:  300 | acc: 100.00%,  total acc: 61.84%   [EVAL] batch:  301 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 62.09%   [EVAL] batch:  303 | acc: 100.00%,  total acc: 62.21%   [EVAL] batch:  304 | acc: 100.00%,  total acc: 62.34%   [EVAL] batch:  305 | acc: 100.00%,  total acc: 62.46%   [EVAL] batch:  306 | acc: 100.00%,  total acc: 62.58%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 62.66%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 62.74%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 62.86%   [EVAL] batch:  310 | acc: 87.50%,  total acc: 62.94%   [EVAL] batch:  311 | acc: 100.00%,  total acc: 63.06%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 63.10%   [EVAL] batch:  313 | acc: 43.75%,  total acc: 63.04%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 63.02%   [EVAL] batch:  315 | acc: 68.75%,  total acc: 63.03%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 63.03%   [EVAL] batch:  317 | acc: 75.00%,  total acc: 63.07%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 63.13%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 63.18%   [EVAL] batch:  320 | acc: 75.00%,  total acc: 63.22%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 63.30%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 63.35%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 63.43%   [EVAL] batch:  324 | acc: 62.50%,  total acc: 63.42%   [EVAL] batch:  325 | acc: 6.25%,  total acc: 63.25%   [EVAL] batch:  326 | acc: 6.25%,  total acc: 63.07%   [EVAL] batch:  327 | acc: 12.50%,  total acc: 62.92%   [EVAL] batch:  328 | acc: 18.75%,  total acc: 62.78%   [EVAL] batch:  329 | acc: 31.25%,  total acc: 62.69%   [EVAL] batch:  330 | acc: 12.50%,  total acc: 62.54%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 62.56%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 62.67%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 62.87%   [EVAL] batch:  335 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:  336 | acc: 93.75%,  total acc: 63.07%   [EVAL] batch:  337 | acc: 56.25%,  total acc: 63.05%   [EVAL] batch:  338 | acc: 25.00%,  total acc: 62.94%   [EVAL] batch:  339 | acc: 12.50%,  total acc: 62.79%   [EVAL] batch:  340 | acc: 12.50%,  total acc: 62.65%   [EVAL] batch:  341 | acc: 0.00%,  total acc: 62.46%   [EVAL] batch:  342 | acc: 12.50%,  total acc: 62.32%   [EVAL] batch:  343 | acc: 31.25%,  total acc: 62.23%   [EVAL] batch:  344 | acc: 87.50%,  total acc: 62.30%   [EVAL] batch:  345 | acc: 93.75%,  total acc: 62.39%   [EVAL] batch:  346 | acc: 93.75%,  total acc: 62.48%   [EVAL] batch:  347 | acc: 87.50%,  total acc: 62.55%   [EVAL] batch:  348 | acc: 93.75%,  total acc: 62.64%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 62.75%   [EVAL] batch:  350 | acc: 25.00%,  total acc: 62.64%   [EVAL] batch:  351 | acc: 43.75%,  total acc: 62.59%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 62.50%   [EVAL] batch:  353 | acc: 18.75%,  total acc: 62.38%   [EVAL] batch:  354 | acc: 37.50%,  total acc: 62.31%   [EVAL] batch:  355 | acc: 37.50%,  total acc: 62.24%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 62.20%   [EVAL] batch:  357 | acc: 62.50%,  total acc: 62.20%   [EVAL] batch:  358 | acc: 75.00%,  total acc: 62.24%   [EVAL] batch:  359 | acc: 68.75%,  total acc: 62.26%   [EVAL] batch:  360 | acc: 56.25%,  total acc: 62.24%   [EVAL] batch:  361 | acc: 62.50%,  total acc: 62.24%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 62.26%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 62.21%   [EVAL] batch:  364 | acc: 75.00%,  total acc: 62.24%   [EVAL] batch:  365 | acc: 75.00%,  total acc: 62.28%   [EVAL] batch:  366 | acc: 75.00%,  total acc: 62.31%   [EVAL] batch:  367 | acc: 62.50%,  total acc: 62.31%   [EVAL] batch:  368 | acc: 75.00%,  total acc: 62.35%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 62.43%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 62.55%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 62.58%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 62.62%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 62.67%   [EVAL] batch:  375 | acc: 100.00%,  total acc: 62.77%   [EVAL] batch:  376 | acc: 93.75%,  total acc: 62.85%   [EVAL] batch:  377 | acc: 93.75%,  total acc: 62.93%   [EVAL] batch:  378 | acc: 87.50%,  total acc: 62.99%   [EVAL] batch:  379 | acc: 93.75%,  total acc: 63.08%   [EVAL] batch:  380 | acc: 100.00%,  total acc: 63.17%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 63.15%   [EVAL] batch:  382 | acc: 56.25%,  total acc: 63.14%   [EVAL] batch:  383 | acc: 56.25%,  total acc: 63.12%   [EVAL] batch:  384 | acc: 68.75%,  total acc: 63.13%   [EVAL] batch:  385 | acc: 56.25%,  total acc: 63.12%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 63.11%   [EVAL] batch:  387 | acc: 68.75%,  total acc: 63.13%   [EVAL] batch:  388 | acc: 68.75%,  total acc: 63.14%   [EVAL] batch:  389 | acc: 56.25%,  total acc: 63.12%   [EVAL] batch:  390 | acc: 62.50%,  total acc: 63.12%   [EVAL] batch:  391 | acc: 62.50%,  total acc: 63.12%   [EVAL] batch:  392 | acc: 56.25%,  total acc: 63.10%   [EVAL] batch:  393 | acc: 56.25%,  total acc: 63.09%   [EVAL] batch:  394 | acc: 56.25%,  total acc: 63.07%   [EVAL] batch:  395 | acc: 50.00%,  total acc: 63.04%   [EVAL] batch:  396 | acc: 68.75%,  total acc: 63.05%   [EVAL] batch:  397 | acc: 87.50%,  total acc: 63.11%   [EVAL] batch:  398 | acc: 75.00%,  total acc: 63.14%   [EVAL] batch:  399 | acc: 62.50%,  total acc: 63.14%   [EVAL] batch:  400 | acc: 100.00%,  total acc: 63.23%   [EVAL] batch:  401 | acc: 100.00%,  total acc: 63.32%   [EVAL] batch:  402 | acc: 100.00%,  total acc: 63.42%   [EVAL] batch:  403 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:  404 | acc: 100.00%,  total acc: 63.60%   [EVAL] batch:  405 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 63.70%   [EVAL] batch:  407 | acc: 75.00%,  total acc: 63.73%   [EVAL] batch:  408 | acc: 56.25%,  total acc: 63.71%   [EVAL] batch:  409 | acc: 68.75%,  total acc: 63.72%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 63.72%   [EVAL] batch:  411 | acc: 56.25%,  total acc: 63.70%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 63.71%   [EVAL] batch:  413 | acc: 100.00%,  total acc: 63.80%   [EVAL] batch:  414 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:  415 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  416 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  417 | acc: 100.00%,  total acc: 64.14%   [EVAL] batch:  418 | acc: 93.75%,  total acc: 64.22%   [EVAL] batch:  419 | acc: 100.00%,  total acc: 64.30%   [EVAL] batch:  420 | acc: 100.00%,  total acc: 64.39%   [EVAL] batch:  421 | acc: 100.00%,  total acc: 64.47%   [EVAL] batch:  422 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  423 | acc: 100.00%,  total acc: 64.64%   [EVAL] batch:  424 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:  425 | acc: 81.25%,  total acc: 64.74%   [EVAL] batch:  426 | acc: 93.75%,  total acc: 64.81%   [EVAL] batch:  427 | acc: 93.75%,  total acc: 64.88%   [EVAL] batch:  428 | acc: 87.50%,  total acc: 64.93%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 64.96%   [EVAL] batch:  430 | acc: 87.50%,  total acc: 65.01%   [EVAL] batch:  431 | acc: 93.75%,  total acc: 65.08%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 65.14%   [EVAL] batch:  433 | acc: 87.50%,  total acc: 65.19%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 65.27%   [EVAL] batch:  435 | acc: 100.00%,  total acc: 65.35%   [EVAL] batch:  436 | acc: 93.75%,  total acc: 65.42%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 65.40%   [EVAL] batch:  438 | acc: 37.50%,  total acc: 65.33%   [EVAL] batch:  439 | acc: 25.00%,  total acc: 65.24%   [EVAL] batch:  440 | acc: 12.50%,  total acc: 65.12%   [EVAL] batch:  441 | acc: 37.50%,  total acc: 65.06%   [EVAL] batch:  442 | acc: 37.50%,  total acc: 65.00%   [EVAL] batch:  443 | acc: 37.50%,  total acc: 64.94%   [EVAL] batch:  444 | acc: 62.50%,  total acc: 64.93%   [EVAL] batch:  445 | acc: 50.00%,  total acc: 64.90%   [EVAL] batch:  446 | acc: 75.00%,  total acc: 64.92%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 64.97%   [EVAL] batch:  448 | acc: 68.75%,  total acc: 64.98%   [EVAL] batch:  449 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:  450 | acc: 62.50%,  total acc: 64.99%   [EVAL] batch:  451 | acc: 56.25%,  total acc: 64.98%   [EVAL] batch:  452 | acc: 37.50%,  total acc: 64.91%   [EVAL] batch:  453 | acc: 50.00%,  total acc: 64.88%   [EVAL] batch:  454 | acc: 68.75%,  total acc: 64.89%   [EVAL] batch:  455 | acc: 81.25%,  total acc: 64.93%   [EVAL] batch:  456 | acc: 87.50%,  total acc: 64.98%   [EVAL] batch:  457 | acc: 93.75%,  total acc: 65.04%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  459 | acc: 93.75%,  total acc: 65.18%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  462 | acc: 75.00%,  total acc: 65.35%   [EVAL] batch:  463 | acc: 81.25%,  total acc: 65.38%   [EVAL] batch:  464 | acc: 75.00%,  total acc: 65.40%   [EVAL] batch:  465 | acc: 93.75%,  total acc: 65.46%   [EVAL] batch:  466 | acc: 75.00%,  total acc: 65.48%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  468 | acc: 75.00%,  total acc: 65.58%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 65.65%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 65.72%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 65.78%   [EVAL] batch:  472 | acc: 93.75%,  total acc: 65.84%   [EVAL] batch:  473 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  475 | acc: 37.50%,  total acc: 65.93%   [EVAL] batch:  476 | acc: 56.25%,  total acc: 65.91%   [EVAL] batch:  477 | acc: 43.75%,  total acc: 65.86%   [EVAL] batch:  478 | acc: 56.25%,  total acc: 65.84%   [EVAL] batch:  479 | acc: 31.25%,  total acc: 65.77%   [EVAL] batch:  480 | acc: 56.25%,  total acc: 65.75%   [EVAL] batch:  481 | acc: 68.75%,  total acc: 65.75%   [EVAL] batch:  482 | acc: 75.00%,  total acc: 65.77%   [EVAL] batch:  483 | acc: 75.00%,  total acc: 65.79%   [EVAL] batch:  484 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:  485 | acc: 100.00%,  total acc: 65.92%   [EVAL] batch:  486 | acc: 81.25%,  total acc: 65.95%   [EVAL] batch:  487 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:  488 | acc: 68.75%,  total acc: 65.98%   [EVAL] batch:  489 | acc: 75.00%,  total acc: 65.99%   [EVAL] batch:  490 | acc: 68.75%,  total acc: 66.00%   [EVAL] batch:  491 | acc: 68.75%,  total acc: 66.01%   [EVAL] batch:  492 | acc: 56.25%,  total acc: 65.99%   [EVAL] batch:  493 | acc: 37.50%,  total acc: 65.93%   [EVAL] batch:  494 | acc: 75.00%,  total acc: 65.95%   [EVAL] batch:  495 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:  496 | acc: 50.00%,  total acc: 65.93%   [EVAL] batch:  497 | acc: 75.00%,  total acc: 65.95%   [EVAL] batch:  498 | acc: 87.50%,  total acc: 65.99%   [EVAL] batch:  499 | acc: 75.00%,  total acc: 66.01%   
cur_acc:  ['0.9494', '0.6270', '0.7133', '0.7024', '0.7371', '0.6647', '0.8413', '0.6935']
his_acc:  ['0.9494', '0.7820', '0.7500', '0.7208', '0.6957', '0.6717', '0.6902', '0.6601']
--------Round  2
seed:  300
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 3 1 5 6 0 4]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 2 0 3 3 2 0 1 1]
Losses:  19.632648468017578 5.460475921630859 1.244140386581421
CurrentTrain: epoch  0, batch     0 | loss: 19.6326485Losses:  24.01803970336914 9.95770263671875 1.186988115310669
CurrentTrain: epoch  0, batch     1 | loss: 24.0180397Losses:  18.417640686035156 4.853786468505859 1.1645574569702148
CurrentTrain: epoch  0, batch     2 | loss: 18.4176407Losses:  20.050798416137695 6.4832987785339355 1.126124620437622
CurrentTrain: epoch  0, batch     3 | loss: 20.0507984Losses:  17.823881149291992 4.466606140136719 1.091184139251709
CurrentTrain: epoch  0, batch     4 | loss: 17.8238811Losses:  16.44411849975586 3.6552488803863525 1.0392253398895264
CurrentTrain: epoch  0, batch     5 | loss: 16.4441185Losses:  16.597700119018555 4.030026435852051 0.951630711555481
CurrentTrain: epoch  0, batch     6 | loss: 16.5977001Losses:  17.29530906677246 4.6606621742248535 0.9579636454582214
CurrentTrain: epoch  0, batch     7 | loss: 17.2953091Losses:  17.50653648376465 5.424857139587402 0.7886793613433838
CurrentTrain: epoch  0, batch     8 | loss: 17.5065365Losses:  16.263959884643555 4.108605861663818 0.8768149614334106
CurrentTrain: epoch  0, batch     9 | loss: 16.2639599Losses:  18.205469131469727 5.816980838775635 0.7459172010421753
CurrentTrain: epoch  0, batch    10 | loss: 18.2054691Losses:  14.966691017150879 2.982877731323242 0.836232602596283
CurrentTrain: epoch  0, batch    11 | loss: 14.9666910Losses:  16.5125732421875 4.891071796417236 0.7223455905914307
CurrentTrain: epoch  0, batch    12 | loss: 16.5125732Losses:  17.891820907592773 6.131148815155029 0.7603774666786194
CurrentTrain: epoch  0, batch    13 | loss: 17.8918209Losses:  16.600589752197266 5.086050987243652 0.7482132315635681
CurrentTrain: epoch  0, batch    14 | loss: 16.6005898Losses:  15.051729202270508 3.7025442123413086 0.6866785287857056
CurrentTrain: epoch  0, batch    15 | loss: 15.0517292Losses:  15.666031837463379 4.432415008544922 0.6139194965362549
CurrentTrain: epoch  0, batch    16 | loss: 15.6660318Losses:  18.376972198486328 7.081350326538086 0.656909167766571
CurrentTrain: epoch  0, batch    17 | loss: 18.3769722Losses:  15.618637084960938 4.486198425292969 0.6058595180511475
CurrentTrain: epoch  0, batch    18 | loss: 15.6186371Losses:  14.553502082824707 4.180209636688232 0.5964630842208862
CurrentTrain: epoch  0, batch    19 | loss: 14.5535021Losses:  16.53040313720703 5.696717739105225 0.6012223958969116
CurrentTrain: epoch  0, batch    20 | loss: 16.5304031Losses:  17.839336395263672 7.079746723175049 0.4407247304916382
CurrentTrain: epoch  0, batch    21 | loss: 17.8393364Losses:  18.377410888671875 7.537626266479492 0.5764474272727966
CurrentTrain: epoch  0, batch    22 | loss: 18.3774109Losses:  15.38834285736084 4.734339237213135 0.5469715595245361
CurrentTrain: epoch  0, batch    23 | loss: 15.3883429Losses:  14.178436279296875 3.6926276683807373 0.5452963709831238
CurrentTrain: epoch  0, batch    24 | loss: 14.1784363Losses:  15.561887741088867 4.890714645385742 0.5382263660430908
CurrentTrain: epoch  0, batch    25 | loss: 15.5618877Losses:  14.967638969421387 4.180120468139648 0.5591630935668945
CurrentTrain: epoch  0, batch    26 | loss: 14.9676390Losses:  18.057147979736328 7.163242340087891 0.5110888481140137
CurrentTrain: epoch  0, batch    27 | loss: 18.0571480Losses:  15.357582092285156 4.868781089782715 0.508345365524292
CurrentTrain: epoch  0, batch    28 | loss: 15.3575821Losses:  18.068958282470703 7.495182037353516 0.5479660034179688
CurrentTrain: epoch  0, batch    29 | loss: 18.0689583Losses:  20.875978469848633 9.527007102966309 0.46848076581954956
CurrentTrain: epoch  0, batch    30 | loss: 20.8759785Losses:  18.7044734954834 7.904880523681641 0.45522767305374146
CurrentTrain: epoch  0, batch    31 | loss: 18.7044735Losses:  14.85659408569336 5.151578903198242 0.4829692244529724
CurrentTrain: epoch  0, batch    32 | loss: 14.8565941Losses:  14.664003372192383 4.26136589050293 0.49079620838165283
CurrentTrain: epoch  0, batch    33 | loss: 14.6640034Losses:  14.74052619934082 4.612462997436523 0.5055720806121826
CurrentTrain: epoch  0, batch    34 | loss: 14.7405262Losses:  13.33331298828125 3.5457911491394043 0.4854409694671631
CurrentTrain: epoch  0, batch    35 | loss: 13.3333130Losses:  16.443410873413086 6.229235649108887 0.5343834757804871
CurrentTrain: epoch  0, batch    36 | loss: 16.4434109Losses:  14.97994613647461 4.982637405395508 0.5255953073501587
CurrentTrain: epoch  0, batch    37 | loss: 14.9799461Losses:  14.991182327270508 4.604855537414551 0.547783374786377
CurrentTrain: epoch  0, batch    38 | loss: 14.9911823Losses:  12.653072357177734 3.153456211090088 0.5173393487930298
CurrentTrain: epoch  0, batch    39 | loss: 12.6530724Losses:  16.5814151763916 6.856009006500244 0.5000762939453125
CurrentTrain: epoch  0, batch    40 | loss: 16.5814152Losses:  15.648164749145508 6.721113204956055 0.4419676661491394
CurrentTrain: epoch  0, batch    41 | loss: 15.6481647Losses:  20.437849044799805 10.79212760925293 0.4750715494155884
CurrentTrain: epoch  0, batch    42 | loss: 20.4378490Losses:  13.996432304382324 4.551270961761475 0.4506351351737976
CurrentTrain: epoch  0, batch    43 | loss: 13.9964323Losses:  15.155301094055176 5.211118221282959 0.47980937361717224
CurrentTrain: epoch  0, batch    44 | loss: 15.1553011Losses:  14.743687629699707 5.767446994781494 0.4461846947669983
CurrentTrain: epoch  0, batch    45 | loss: 14.7436876Losses:  14.44322681427002 4.7858686447143555 0.43909114599227905
CurrentTrain: epoch  0, batch    46 | loss: 14.4432268Losses:  13.59062385559082 4.958930492401123 0.38917502760887146
CurrentTrain: epoch  0, batch    47 | loss: 13.5906239Losses:  14.485528945922852 5.564515113830566 0.4350213408470154
CurrentTrain: epoch  0, batch    48 | loss: 14.4855289Losses:  14.706446647644043 5.474755764007568 0.410156786441803
CurrentTrain: epoch  0, batch    49 | loss: 14.7064466Losses:  11.903203010559082 2.957582473754883 0.44476452469825745
CurrentTrain: epoch  0, batch    50 | loss: 11.9032030Losses:  13.106108665466309 4.297861099243164 0.38081449270248413
CurrentTrain: epoch  0, batch    51 | loss: 13.1061087Losses:  13.828853607177734 4.869645118713379 0.4179830551147461
CurrentTrain: epoch  0, batch    52 | loss: 13.8288536Losses:  16.65679168701172 7.578142166137695 0.4153859615325928
CurrentTrain: epoch  0, batch    53 | loss: 16.6567917Losses:  10.594687461853027 2.3977737426757812 0.4180322587490082
CurrentTrain: epoch  0, batch    54 | loss: 10.5946875Losses:  12.211443901062012 3.788468837738037 0.41196000576019287
CurrentTrain: epoch  0, batch    55 | loss: 12.2114439Losses:  12.102193832397461 3.9980618953704834 0.3803436756134033
CurrentTrain: epoch  0, batch    56 | loss: 12.1021938Losses:  10.532662391662598 2.8658056259155273 0.3896295428276062
CurrentTrain: epoch  0, batch    57 | loss: 10.5326624Losses:  15.164983749389648 6.422573089599609 0.39510899782180786
CurrentTrain: epoch  0, batch    58 | loss: 15.1649837Losses:  9.911476135253906 2.241612434387207 0.3901960849761963
CurrentTrain: epoch  0, batch    59 | loss: 9.9114761Losses:  13.715887069702148 5.1166229248046875 0.44658327102661133
CurrentTrain: epoch  0, batch    60 | loss: 13.7158871Losses:  14.920740127563477 5.996453285217285 0.38654762506484985
CurrentTrain: epoch  0, batch    61 | loss: 14.9207401Losses:  10.829026222229004 2.4292991161346436 0.27238041162490845
CurrentTrain: epoch  0, batch    62 | loss: 10.8290262Losses:  11.637190818786621 3.8711938858032227 0.3751433491706848
CurrentTrain: epoch  1, batch     0 | loss: 11.6371908Losses:  10.838789939880371 3.1940243244171143 0.3728657364845276
CurrentTrain: epoch  1, batch     1 | loss: 10.8387899Losses:  12.06814956665039 4.231291770935059 0.36885976791381836
CurrentTrain: epoch  1, batch     2 | loss: 12.0681496Losses:  13.038895606994629 5.358392715454102 0.35359686613082886
CurrentTrain: epoch  1, batch     3 | loss: 13.0388956Losses:  9.6470308303833 2.4290239810943604 0.3592301309108734
CurrentTrain: epoch  1, batch     4 | loss: 9.6470308Losses:  12.107621192932129 4.222522735595703 0.3676159977912903
CurrentTrain: epoch  1, batch     5 | loss: 12.1076212Losses:  11.396991729736328 3.7220873832702637 0.3690284490585327
CurrentTrain: epoch  1, batch     6 | loss: 11.3969917Losses:  10.882412910461426 3.2615742683410645 0.3496124744415283
CurrentTrain: epoch  1, batch     7 | loss: 10.8824129Losses:  11.325222969055176 3.552354335784912 0.3586476445198059
CurrentTrain: epoch  1, batch     8 | loss: 11.3252230Losses:  14.082904815673828 5.1181321144104 0.36353904008865356
CurrentTrain: epoch  1, batch     9 | loss: 14.0829048Losses:  11.861654281616211 5.139090538024902 0.23463968932628632
CurrentTrain: epoch  1, batch    10 | loss: 11.8616543Losses:  17.181827545166016 8.542322158813477 0.34738627076148987
CurrentTrain: epoch  1, batch    11 | loss: 17.1818275Losses:  12.611059188842773 4.404150009155273 0.35382193326950073
CurrentTrain: epoch  1, batch    12 | loss: 12.6110592Losses:  11.118870735168457 2.8300981521606445 0.36780762672424316
CurrentTrain: epoch  1, batch    13 | loss: 11.1188707Losses:  11.447505950927734 2.9317378997802734 0.3576923608779907
CurrentTrain: epoch  1, batch    14 | loss: 11.4475060Losses:  14.720585823059082 6.365848541259766 0.39038148522377014
CurrentTrain: epoch  1, batch    15 | loss: 14.7205858Losses:  13.200544357299805 5.253706932067871 0.35427671670913696
CurrentTrain: epoch  1, batch    16 | loss: 13.2005444Losses:  12.95179557800293 4.775987148284912 0.34209302067756653
CurrentTrain: epoch  1, batch    17 | loss: 12.9517956Losses:  9.6633939743042 1.8956263065338135 0.33869805932044983
CurrentTrain: epoch  1, batch    18 | loss: 9.6633940Losses:  11.086502075195312 3.306297779083252 0.3416210412979126
CurrentTrain: epoch  1, batch    19 | loss: 11.0865021Losses:  10.594919204711914 3.151203155517578 0.31573888659477234
CurrentTrain: epoch  1, batch    20 | loss: 10.5949192Losses:  12.095335006713867 4.354875087738037 0.334223210811615
CurrentTrain: epoch  1, batch    21 | loss: 12.0953350Losses:  14.59782886505127 7.099458694458008 0.3463800251483917
CurrentTrain: epoch  1, batch    22 | loss: 14.5978289Losses:  11.371525764465332 3.7875306606292725 0.3758588433265686
CurrentTrain: epoch  1, batch    23 | loss: 11.3715258Losses:  9.986891746520996 2.9423561096191406 0.3206670880317688
CurrentTrain: epoch  1, batch    24 | loss: 9.9868917Losses:  12.860655784606934 5.100377082824707 0.3337072432041168
CurrentTrain: epoch  1, batch    25 | loss: 12.8606558Losses:  11.307940483093262 3.670391082763672 0.3295525312423706
CurrentTrain: epoch  1, batch    26 | loss: 11.3079405Losses:  11.00058650970459 3.7076401710510254 0.3321659564971924
CurrentTrain: epoch  1, batch    27 | loss: 11.0005865Losses:  11.397079467773438 3.0931739807128906 0.3308371901512146
CurrentTrain: epoch  1, batch    28 | loss: 11.3970795Losses:  11.223562240600586 3.6523609161376953 0.31255945563316345
CurrentTrain: epoch  1, batch    29 | loss: 11.2235622Losses:  12.387292861938477 4.378534317016602 0.34007012844085693
CurrentTrain: epoch  1, batch    30 | loss: 12.3872929Losses:  14.58707046508789 7.0325188636779785 0.34011074900627136
CurrentTrain: epoch  1, batch    31 | loss: 14.5870705Losses:  11.66120719909668 4.350553035736084 0.34003105759620667
CurrentTrain: epoch  1, batch    32 | loss: 11.6612072Losses:  12.671649932861328 4.499381065368652 0.3727211356163025
CurrentTrain: epoch  1, batch    33 | loss: 12.6716499Losses:  11.95409107208252 4.663057327270508 0.3507348299026489
CurrentTrain: epoch  1, batch    34 | loss: 11.9540911Losses:  12.463735580444336 4.780832290649414 0.3482174873352051
CurrentTrain: epoch  1, batch    35 | loss: 12.4637356Losses:  13.663002967834473 5.9097490310668945 0.35938572883605957
CurrentTrain: epoch  1, batch    36 | loss: 13.6630030Losses:  9.960856437683105 3.2275376319885254 0.3279677629470825
CurrentTrain: epoch  1, batch    37 | loss: 9.9608564Losses:  12.587559700012207 5.762233734130859 0.34840908646583557
CurrentTrain: epoch  1, batch    38 | loss: 12.5875597Losses:  10.261240005493164 2.9203901290893555 0.33402347564697266
CurrentTrain: epoch  1, batch    39 | loss: 10.2612400Losses:  11.816781044006348 5.505757808685303 0.3287016749382019
CurrentTrain: epoch  1, batch    40 | loss: 11.8167810Losses:  10.748772621154785 3.452913284301758 0.32978731393814087
CurrentTrain: epoch  1, batch    41 | loss: 10.7487726Losses:  10.78127384185791 3.440971851348877 0.3415176272392273
CurrentTrain: epoch  1, batch    42 | loss: 10.7812738Losses:  12.183605194091797 5.063027381896973 0.3324214518070221
CurrentTrain: epoch  1, batch    43 | loss: 12.1836052Losses:  11.694269180297852 3.9594650268554688 0.33142969012260437
CurrentTrain: epoch  1, batch    44 | loss: 11.6942692Losses:  9.481239318847656 3.2887766361236572 0.31567519903182983
CurrentTrain: epoch  1, batch    45 | loss: 9.4812393Losses:  9.168702125549316 2.6100826263427734 0.30450713634490967
CurrentTrain: epoch  1, batch    46 | loss: 9.1687021Losses:  12.711325645446777 5.221897602081299 0.3360461890697479
CurrentTrain: epoch  1, batch    47 | loss: 12.7113256Losses:  13.671683311462402 5.963974475860596 0.2527282238006592
CurrentTrain: epoch  1, batch    48 | loss: 13.6716833Losses:  11.05892562866211 3.916001319885254 0.3095044493675232
CurrentTrain: epoch  1, batch    49 | loss: 11.0589256Losses:  14.146278381347656 5.7236714363098145 0.3168601393699646
CurrentTrain: epoch  1, batch    50 | loss: 14.1462784Losses:  11.998053550720215 5.422747611999512 0.32269132137298584
CurrentTrain: epoch  1, batch    51 | loss: 11.9980536Losses:  10.821873664855957 3.7815966606140137 0.30589982867240906
CurrentTrain: epoch  1, batch    52 | loss: 10.8218737Losses:  10.240765571594238 3.348963737487793 0.30820325016975403
CurrentTrain: epoch  1, batch    53 | loss: 10.2407656Losses:  11.018914222717285 4.499699592590332 0.32318422198295593
CurrentTrain: epoch  1, batch    54 | loss: 11.0189142Losses:  9.486886024475098 3.349233388900757 0.3011550009250641
CurrentTrain: epoch  1, batch    55 | loss: 9.4868860Losses:  9.633428573608398 3.1611385345458984 0.3203444480895996
CurrentTrain: epoch  1, batch    56 | loss: 9.6334286Losses:  8.38461971282959 2.04681134223938 0.28112471103668213
CurrentTrain: epoch  1, batch    57 | loss: 8.3846197Losses:  12.037678718566895 6.727705955505371 0.29163432121276855
CurrentTrain: epoch  1, batch    58 | loss: 12.0376787Losses:  9.110405921936035 3.018533229827881 0.3047855496406555
CurrentTrain: epoch  1, batch    59 | loss: 9.1104059Losses:  8.622103691101074 2.6161112785339355 0.30660873651504517
CurrentTrain: epoch  1, batch    60 | loss: 8.6221037Losses:  10.13969612121582 4.886807918548584 0.2932925224304199
CurrentTrain: epoch  1, batch    61 | loss: 10.1396961Losses:  7.2019805908203125 1.1925159692764282 0.28119128942489624
CurrentTrain: epoch  1, batch    62 | loss: 7.2019806Losses:  12.035873413085938 5.5961785316467285 0.31104281544685364
CurrentTrain: epoch  2, batch     0 | loss: 12.0358734Losses:  10.686984062194824 4.1678690910339355 0.3101491928100586
CurrentTrain: epoch  2, batch     1 | loss: 10.6869841Losses:  9.536107063293457 3.2046098709106445 0.29587459564208984
CurrentTrain: epoch  2, batch     2 | loss: 9.5361071Losses:  11.594955444335938 4.908347129821777 0.25754353404045105
CurrentTrain: epoch  2, batch     3 | loss: 11.5949554Losses:  9.117280960083008 3.052797317504883 0.2932945489883423
CurrentTrain: epoch  2, batch     4 | loss: 9.1172810Losses:  12.546651840209961 5.227941513061523 0.3050205707550049
CurrentTrain: epoch  2, batch     5 | loss: 12.5466518Losses:  12.277088165283203 6.568936347961426 0.29573822021484375
CurrentTrain: epoch  2, batch     6 | loss: 12.2770882Losses:  9.801384925842285 3.97761869430542 0.28961098194122314
CurrentTrain: epoch  2, batch     7 | loss: 9.8013849Losses:  8.200583457946777 2.4432756900787354 0.29681140184402466
CurrentTrain: epoch  2, batch     8 | loss: 8.2005835Losses:  8.651700019836426 2.78316593170166 0.3081032633781433
CurrentTrain: epoch  2, batch     9 | loss: 8.6517000Losses:  10.017036437988281 3.6622681617736816 0.30791059136390686
CurrentTrain: epoch  2, batch    10 | loss: 10.0170364Losses:  8.293041229248047 2.2853806018829346 0.28021740913391113
CurrentTrain: epoch  2, batch    11 | loss: 8.2930412Losses:  10.478751182556152 4.27559757232666 0.323201447725296
CurrentTrain: epoch  2, batch    12 | loss: 10.4787512Losses:  13.243438720703125 6.84324836730957 0.34062159061431885
CurrentTrain: epoch  2, batch    13 | loss: 13.2434387Losses:  9.606608390808105 3.613248825073242 0.279180645942688
CurrentTrain: epoch  2, batch    14 | loss: 9.6066084Losses:  11.555547714233398 4.6899871826171875 0.30027732253074646
CurrentTrain: epoch  2, batch    15 | loss: 11.5555477Losses:  12.047389030456543 6.448544025421143 0.17296366393566132
CurrentTrain: epoch  2, batch    16 | loss: 12.0473890Losses:  8.275507926940918 2.3105721473693848 0.30266398191452026
CurrentTrain: epoch  2, batch    17 | loss: 8.2755079Losses:  11.003830909729004 4.216041564941406 0.30414679646492004
CurrentTrain: epoch  2, batch    18 | loss: 11.0038309Losses:  11.276989936828613 4.205655097961426 0.2973898649215698
CurrentTrain: epoch  2, batch    19 | loss: 11.2769899Losses:  10.267109870910645 4.773007392883301 0.2851633131504059
CurrentTrain: epoch  2, batch    20 | loss: 10.2671099Losses:  11.910656929016113 5.881650924682617 0.3160886764526367
CurrentTrain: epoch  2, batch    21 | loss: 11.9106569Losses:  10.91316032409668 4.740301132202148 0.31043386459350586
CurrentTrain: epoch  2, batch    22 | loss: 10.9131603Losses:  11.766397476196289 6.447254180908203 0.28185170888900757
CurrentTrain: epoch  2, batch    23 | loss: 11.7663975Losses:  11.526674270629883 3.9594368934631348 0.2981949746608734
CurrentTrain: epoch  2, batch    24 | loss: 11.5266743Losses:  8.744392395019531 2.97509503364563 0.29676181077957153
CurrentTrain: epoch  2, batch    25 | loss: 8.7443924Losses:  9.05686092376709 3.6048102378845215 0.28481030464172363
CurrentTrain: epoch  2, batch    26 | loss: 9.0568609Losses:  8.740906715393066 3.027501344680786 0.2744542360305786
CurrentTrain: epoch  2, batch    27 | loss: 8.7409067Losses:  9.573891639709473 4.434088706970215 0.27232810854911804
CurrentTrain: epoch  2, batch    28 | loss: 9.5738916Losses:  10.262064933776855 4.102889060974121 0.2724984884262085
CurrentTrain: epoch  2, batch    29 | loss: 10.2620649Losses:  13.685961723327637 7.568558692932129 0.2902504801750183
CurrentTrain: epoch  2, batch    30 | loss: 13.6859617Losses:  9.450959205627441 3.4688663482666016 0.26016494631767273
CurrentTrain: epoch  2, batch    31 | loss: 9.4509592Losses:  8.338924407958984 2.4772372245788574 0.26423609256744385
CurrentTrain: epoch  2, batch    32 | loss: 8.3389244Losses:  8.913394927978516 3.2305703163146973 0.2753638029098511
CurrentTrain: epoch  2, batch    33 | loss: 8.9133949Losses:  8.198819160461426 2.6574292182922363 0.26688751578330994
CurrentTrain: epoch  2, batch    34 | loss: 8.1988192Losses:  8.701169967651367 2.681692600250244 0.28698110580444336
CurrentTrain: epoch  2, batch    35 | loss: 8.7011700Losses:  10.242889404296875 3.4370927810668945 0.290144681930542
CurrentTrain: epoch  2, batch    36 | loss: 10.2428894Losses:  8.7485990524292 3.023682117462158 0.28181368112564087
CurrentTrain: epoch  2, batch    37 | loss: 8.7485991Losses:  10.014262199401855 3.422464370727539 0.3116358518600464
CurrentTrain: epoch  2, batch    38 | loss: 10.0142622Losses:  8.343411445617676 2.4938714504241943 0.27450984716415405
CurrentTrain: epoch  2, batch    39 | loss: 8.3434114Losses:  13.64941120147705 7.294582366943359 0.18747112154960632
CurrentTrain: epoch  2, batch    40 | loss: 13.6494112Losses:  7.911477088928223 2.333625316619873 0.26280543208122253
CurrentTrain: epoch  2, batch    41 | loss: 7.9114771Losses:  7.828347206115723 2.3914618492126465 0.2610471248626709
CurrentTrain: epoch  2, batch    42 | loss: 7.8283472Losses:  11.877708435058594 5.493152618408203 0.3221953511238098
CurrentTrain: epoch  2, batch    43 | loss: 11.8777084Losses:  9.262519836425781 3.4985244274139404 0.27912676334381104
CurrentTrain: epoch  2, batch    44 | loss: 9.2625198Losses:  14.233658790588379 7.9308061599731445 0.20539727807044983
CurrentTrain: epoch  2, batch    45 | loss: 14.2336588Losses:  9.707490921020508 4.686599254608154 0.2553499639034271
CurrentTrain: epoch  2, batch    46 | loss: 9.7074909Losses:  10.029415130615234 3.4647674560546875 0.2642778754234314
CurrentTrain: epoch  2, batch    47 | loss: 10.0294151Losses:  9.903709411621094 3.150932788848877 0.30055296421051025
CurrentTrain: epoch  2, batch    48 | loss: 9.9037094Losses:  8.582645416259766 2.88653826713562 0.2819366157054901
CurrentTrain: epoch  2, batch    49 | loss: 8.5826454Losses:  9.251373291015625 3.362847328186035 0.2553502917289734
CurrentTrain: epoch  2, batch    50 | loss: 9.2513733Losses:  7.934613227844238 2.8097376823425293 0.2618536949157715
CurrentTrain: epoch  2, batch    51 | loss: 7.9346132Losses:  9.14048957824707 2.9680325984954834 0.2754005193710327
CurrentTrain: epoch  2, batch    52 | loss: 9.1404896Losses:  8.131400108337402 2.3850948810577393 0.25712499022483826
CurrentTrain: epoch  2, batch    53 | loss: 8.1314001Losses:  9.341241836547852 3.781440496444702 0.26831260323524475
CurrentTrain: epoch  2, batch    54 | loss: 9.3412418Losses:  9.149336814880371 3.5413622856140137 0.17799639701843262
CurrentTrain: epoch  2, batch    55 | loss: 9.1493368Losses:  7.458171367645264 2.1718711853027344 0.2685462236404419
CurrentTrain: epoch  2, batch    56 | loss: 7.4581714Losses:  8.630974769592285 3.0137779712677 0.2773638069629669
CurrentTrain: epoch  2, batch    57 | loss: 8.6309748Losses:  9.86232852935791 4.445444583892822 0.28010833263397217
CurrentTrain: epoch  2, batch    58 | loss: 9.8623285Losses:  7.723288536071777 2.5791544914245605 0.28060364723205566
CurrentTrain: epoch  2, batch    59 | loss: 7.7232885Losses:  9.717337608337402 3.706613302230835 0.28526318073272705
CurrentTrain: epoch  2, batch    60 | loss: 9.7173376Losses:  9.916679382324219 4.515735149383545 0.27097663283348083
CurrentTrain: epoch  2, batch    61 | loss: 9.9166794Losses:  5.687134742736816 0.27400845289230347 0.3066301941871643
CurrentTrain: epoch  2, batch    62 | loss: 5.6871347Losses:  10.33364200592041 4.257224082946777 0.26360607147216797
CurrentTrain: epoch  3, batch     0 | loss: 10.3336420Losses:  8.208757400512695 3.286996603012085 0.25029677152633667
CurrentTrain: epoch  3, batch     1 | loss: 8.2087574Losses:  7.9347243309021 2.4777865409851074 0.2602955102920532
CurrentTrain: epoch  3, batch     2 | loss: 7.9347243Losses:  10.00483512878418 4.680780410766602 0.27338582277297974
CurrentTrain: epoch  3, batch     3 | loss: 10.0048351Losses:  9.19810676574707 3.941131353378296 0.28053730726242065
CurrentTrain: epoch  3, batch     4 | loss: 9.1981068Losses:  7.3676652908325195 1.5294126272201538 0.2801794707775116
CurrentTrain: epoch  3, batch     5 | loss: 7.3676653Losses:  8.234354019165039 3.213406562805176 0.2884127199649811
CurrentTrain: epoch  3, batch     6 | loss: 8.2343540Losses:  8.135120391845703 2.8714146614074707 0.27371612191200256
CurrentTrain: epoch  3, batch     7 | loss: 8.1351204Losses:  8.283794403076172 2.7519617080688477 0.2743801474571228
CurrentTrain: epoch  3, batch     8 | loss: 8.2837944Losses:  8.607008934020996 3.211641788482666 0.27383676171302795
CurrentTrain: epoch  3, batch     9 | loss: 8.6070089Losses:  8.364049911499023 2.700188398361206 0.28074216842651367
CurrentTrain: epoch  3, batch    10 | loss: 8.3640499Losses:  7.783563137054443 2.3776164054870605 0.2684211730957031
CurrentTrain: epoch  3, batch    11 | loss: 7.7835631Losses:  7.110226631164551 2.0811355113983154 0.2412063479423523
CurrentTrain: epoch  3, batch    12 | loss: 7.1102266Losses:  9.564046859741211 4.346127033233643 0.2857416272163391
CurrentTrain: epoch  3, batch    13 | loss: 9.5640469Losses:  7.84274959564209 2.9106571674346924 0.25561222434043884
CurrentTrain: epoch  3, batch    14 | loss: 7.8427496Losses:  8.532564163208008 3.0723166465759277 0.260822057723999
CurrentTrain: epoch  3, batch    15 | loss: 8.5325642Losses:  7.27621603012085 2.147690773010254 0.2682674527168274
CurrentTrain: epoch  3, batch    16 | loss: 7.2762160Losses:  8.949989318847656 2.925074338912964 0.2808316946029663
CurrentTrain: epoch  3, batch    17 | loss: 8.9499893Losses:  8.53021240234375 3.342406749725342 0.2673817574977875
CurrentTrain: epoch  3, batch    18 | loss: 8.5302124Losses:  8.146605491638184 3.056367874145508 0.2787759304046631
CurrentTrain: epoch  3, batch    19 | loss: 8.1466055Losses:  8.088988304138184 2.6706323623657227 0.2660677433013916
CurrentTrain: epoch  3, batch    20 | loss: 8.0889883Losses:  8.645120620727539 3.7316150665283203 0.26700109243392944
CurrentTrain: epoch  3, batch    21 | loss: 8.6451206Losses:  8.180541038513184 2.4949474334716797 0.2527214288711548
CurrentTrain: epoch  3, batch    22 | loss: 8.1805410Losses:  10.488758087158203 5.639998435974121 0.28327226638793945
CurrentTrain: epoch  3, batch    23 | loss: 10.4887581Losses:  7.1018147468566895 2.1195106506347656 0.27225905656814575
CurrentTrain: epoch  3, batch    24 | loss: 7.1018147Losses:  9.452275276184082 4.844043731689453 0.2465953528881073
CurrentTrain: epoch  3, batch    25 | loss: 9.4522753Losses:  7.521523952484131 2.3162755966186523 0.24505729973316193
CurrentTrain: epoch  3, batch    26 | loss: 7.5215240Losses:  10.656064987182617 4.901493072509766 0.28311997652053833
CurrentTrain: epoch  3, batch    27 | loss: 10.6560650Losses:  7.2205705642700195 2.283224582672119 0.2548762559890747
CurrentTrain: epoch  3, batch    28 | loss: 7.2205706Losses:  10.446416854858398 5.136562824249268 0.2780759334564209
CurrentTrain: epoch  3, batch    29 | loss: 10.4464169Losses:  8.711552619934082 3.9479849338531494 0.2557228207588196
CurrentTrain: epoch  3, batch    30 | loss: 8.7115526Losses:  8.468457221984863 3.3777246475219727 0.27705198526382446
CurrentTrain: epoch  3, batch    31 | loss: 8.4684572Losses:  12.452601432800293 7.284988880157471 0.22681133449077606
CurrentTrain: epoch  3, batch    32 | loss: 12.4526014Losses:  9.781421661376953 4.817787170410156 0.2632405161857605
CurrentTrain: epoch  3, batch    33 | loss: 9.7814217Losses:  9.959502220153809 4.476282596588135 0.3007882535457611
CurrentTrain: epoch  3, batch    34 | loss: 9.9595022Losses:  7.971470355987549 2.9706361293792725 0.2796541452407837
CurrentTrain: epoch  3, batch    35 | loss: 7.9714704Losses:  8.24674129486084 3.279733657836914 0.2574698328971863
CurrentTrain: epoch  3, batch    36 | loss: 8.2467413Losses:  6.949997901916504 2.0440664291381836 0.2686641812324524
CurrentTrain: epoch  3, batch    37 | loss: 6.9499979Losses:  8.41635799407959 3.3755674362182617 0.25305822491645813
CurrentTrain: epoch  3, batch    38 | loss: 8.4163580Losses:  8.873529434204102 2.8726439476013184 0.2714197635650635
CurrentTrain: epoch  3, batch    39 | loss: 8.8735294Losses:  7.378840446472168 2.150815010070801 0.25016409158706665
CurrentTrain: epoch  3, batch    40 | loss: 7.3788404Losses:  9.89410400390625 4.130402088165283 0.2853701114654541
CurrentTrain: epoch  3, batch    41 | loss: 9.8941040Losses:  8.486434936523438 3.214803695678711 0.18528154492378235
CurrentTrain: epoch  3, batch    42 | loss: 8.4864349Losses:  9.166159629821777 3.296705484390259 0.18549031019210815
CurrentTrain: epoch  3, batch    43 | loss: 9.1661596Losses:  8.180499076843262 2.976508855819702 0.2556668221950531
CurrentTrain: epoch  3, batch    44 | loss: 8.1804991Losses:  7.765512466430664 2.5296366214752197 0.24920962750911713
CurrentTrain: epoch  3, batch    45 | loss: 7.7655125Losses:  8.843212127685547 3.7313761711120605 0.25458917021751404
CurrentTrain: epoch  3, batch    46 | loss: 8.8432121Losses:  10.165661811828613 5.368161678314209 0.267927885055542
CurrentTrain: epoch  3, batch    47 | loss: 10.1656618Losses:  8.302983283996582 3.081019878387451 0.25801903009414673
CurrentTrain: epoch  3, batch    48 | loss: 8.3029833Losses:  7.529729843139648 2.79585599899292 0.24761900305747986
CurrentTrain: epoch  3, batch    49 | loss: 7.5297298Losses:  8.160603523254395 2.868670701980591 0.25430136919021606
CurrentTrain: epoch  3, batch    50 | loss: 8.1606035Losses:  8.036587715148926 3.0820372104644775 0.26028525829315186
CurrentTrain: epoch  3, batch    51 | loss: 8.0365877Losses:  11.313447952270508 6.1492085456848145 0.26193052530288696
CurrentTrain: epoch  3, batch    52 | loss: 11.3134480Losses:  9.294017791748047 3.6931228637695312 0.26169663667678833
CurrentTrain: epoch  3, batch    53 | loss: 9.2940178Losses:  8.050666809082031 3.11529278755188 0.25061196088790894
CurrentTrain: epoch  3, batch    54 | loss: 8.0506668Losses:  10.346367835998535 5.518954277038574 0.2735435962677002
CurrentTrain: epoch  3, batch    55 | loss: 10.3463678Losses:  7.007064342498779 2.077265739440918 0.24397823214530945
CurrentTrain: epoch  3, batch    56 | loss: 7.0070643Losses:  7.3678460121154785 2.308979034423828 0.22829557955265045
CurrentTrain: epoch  3, batch    57 | loss: 7.3678460Losses:  9.148578643798828 4.215115070343018 0.2550168037414551
CurrentTrain: epoch  3, batch    58 | loss: 9.1485786Losses:  6.964719295501709 2.2454283237457275 0.24511663615703583
CurrentTrain: epoch  3, batch    59 | loss: 6.9647193Losses:  7.92073917388916 2.661562204360962 0.2518312335014343
CurrentTrain: epoch  3, batch    60 | loss: 7.9207392Losses:  11.189701080322266 6.324836730957031 0.2542526423931122
CurrentTrain: epoch  3, batch    61 | loss: 11.1897011Losses:  6.997222423553467 1.5875951051712036 0.18820993602275848
CurrentTrain: epoch  3, batch    62 | loss: 6.9972224Losses:  9.468587875366211 3.879206895828247 0.2326732873916626
CurrentTrain: epoch  4, batch     0 | loss: 9.4685879Losses:  9.046636581420898 4.488631725311279 0.1521775722503662
CurrentTrain: epoch  4, batch     1 | loss: 9.0466366Losses:  7.983479976654053 3.185375213623047 0.2536684274673462
CurrentTrain: epoch  4, batch     2 | loss: 7.9834800Losses:  7.903670310974121 3.1943392753601074 0.23379205167293549
CurrentTrain: epoch  4, batch     3 | loss: 7.9036703Losses:  6.839098930358887 2.012073516845703 0.2410033643245697
CurrentTrain: epoch  4, batch     4 | loss: 6.8390989Losses:  9.97210693359375 5.265846252441406 0.26106753945350647
CurrentTrain: epoch  4, batch     5 | loss: 9.9721069Losses:  7.311916351318359 2.5981593132019043 0.24308623373508453
CurrentTrain: epoch  4, batch     6 | loss: 7.3119164Losses:  9.075222969055176 3.8153743743896484 0.26609429717063904
CurrentTrain: epoch  4, batch     7 | loss: 9.0752230Losses:  7.1467204093933105 2.559373140335083 0.2518409490585327
CurrentTrain: epoch  4, batch     8 | loss: 7.1467204Losses:  8.088788986206055 3.1713829040527344 0.24230103194713593
CurrentTrain: epoch  4, batch     9 | loss: 8.0887890Losses:  9.014610290527344 3.62176513671875 0.255452960729599
CurrentTrain: epoch  4, batch    10 | loss: 9.0146103Losses:  6.542501926422119 1.9269397258758545 0.23411619663238525
CurrentTrain: epoch  4, batch    11 | loss: 6.5425019Losses:  9.245656967163086 4.624555587768555 0.26048219203948975
CurrentTrain: epoch  4, batch    12 | loss: 9.2456570Losses:  10.042928695678711 5.037081718444824 0.2615818977355957
CurrentTrain: epoch  4, batch    13 | loss: 10.0429287Losses:  7.498236179351807 2.7628188133239746 0.23587830364704132
CurrentTrain: epoch  4, batch    14 | loss: 7.4982362Losses:  7.878331184387207 3.2936646938323975 0.248393252491951
CurrentTrain: epoch  4, batch    15 | loss: 7.8783312Losses:  7.806653022766113 3.0534842014312744 0.22812160849571228
CurrentTrain: epoch  4, batch    16 | loss: 7.8066530Losses:  7.487649440765381 2.598322629928589 0.24964362382888794
CurrentTrain: epoch  4, batch    17 | loss: 7.4876494Losses:  8.88071060180664 3.6689469814300537 0.2337280809879303
CurrentTrain: epoch  4, batch    18 | loss: 8.8807106Losses:  6.863641262054443 2.221877098083496 0.24039079248905182
CurrentTrain: epoch  4, batch    19 | loss: 6.8636413Losses:  11.388459205627441 5.354207992553711 0.17337295413017273
CurrentTrain: epoch  4, batch    20 | loss: 11.3884592Losses:  7.499312400817871 2.856902599334717 0.24110467731952667
CurrentTrain: epoch  4, batch    21 | loss: 7.4993124Losses:  7.2441792488098145 2.312241554260254 0.2347342073917389
CurrentTrain: epoch  4, batch    22 | loss: 7.2441792Losses:  9.0218505859375 4.470522880554199 0.23713698983192444
CurrentTrain: epoch  4, batch    23 | loss: 9.0218506Losses:  7.891861915588379 2.781860589981079 0.24922561645507812
CurrentTrain: epoch  4, batch    24 | loss: 7.8918619Losses:  7.381568908691406 2.5313684940338135 0.23109059035778046
CurrentTrain: epoch  4, batch    25 | loss: 7.3815689Losses:  7.614989757537842 2.789759635925293 0.24681700766086578
CurrentTrain: epoch  4, batch    26 | loss: 7.6149898Losses:  8.559366226196289 3.447794198989868 0.2467145472764969
CurrentTrain: epoch  4, batch    27 | loss: 8.5593662Losses:  8.56104564666748 4.111353397369385 0.23735705018043518
CurrentTrain: epoch  4, batch    28 | loss: 8.5610456Losses:  8.928787231445312 3.8656177520751953 0.2534376382827759
CurrentTrain: epoch  4, batch    29 | loss: 8.9287872Losses:  9.676758766174316 4.716402053833008 0.2674783170223236
CurrentTrain: epoch  4, batch    30 | loss: 9.6767588Losses:  6.803154945373535 1.9418261051177979 0.26119884848594666
CurrentTrain: epoch  4, batch    31 | loss: 6.8031549Losses:  7.720736980438232 2.9310672283172607 0.2516236901283264
CurrentTrain: epoch  4, batch    32 | loss: 7.7207370Losses:  7.115146636962891 2.488326072692871 0.24676945805549622
CurrentTrain: epoch  4, batch    33 | loss: 7.1151466Losses:  11.468147277832031 6.901092529296875 0.25985586643218994
CurrentTrain: epoch  4, batch    34 | loss: 11.4681473Losses:  6.7934417724609375 2.253143548965454 0.24556536972522736
CurrentTrain: epoch  4, batch    35 | loss: 6.7934418Losses:  6.7332539558410645 2.1694679260253906 0.2645098865032196
CurrentTrain: epoch  4, batch    36 | loss: 6.7332540Losses:  9.250064849853516 4.640481472015381 0.25483134388923645
CurrentTrain: epoch  4, batch    37 | loss: 9.2500648Losses:  7.534286022186279 2.6737663745880127 0.23664253950119019
CurrentTrain: epoch  4, batch    38 | loss: 7.5342860Losses:  7.096128463745117 1.8061431646347046 0.2383311241865158
CurrentTrain: epoch  4, batch    39 | loss: 7.0961285Losses:  7.291902542114258 2.688316583633423 0.24298083782196045
CurrentTrain: epoch  4, batch    40 | loss: 7.2919025Losses:  8.140849113464355 3.2514452934265137 0.25861942768096924
CurrentTrain: epoch  4, batch    41 | loss: 8.1408491Losses:  8.355326652526855 3.7008583545684814 0.2525942325592041
CurrentTrain: epoch  4, batch    42 | loss: 8.3553267Losses:  8.637298583984375 4.062367916107178 0.2557294964790344
CurrentTrain: epoch  4, batch    43 | loss: 8.6372986Losses:  7.368965148925781 2.951362371444702 0.24631977081298828
CurrentTrain: epoch  4, batch    44 | loss: 7.3689651Losses:  7.808923721313477 2.697767972946167 0.2570963203907013
CurrentTrain: epoch  4, batch    45 | loss: 7.8089237Losses:  11.335488319396973 5.68808126449585 0.2585252523422241
CurrentTrain: epoch  4, batch    46 | loss: 11.3354883Losses:  7.52852201461792 2.519582748413086 0.25746119022369385
CurrentTrain: epoch  4, batch    47 | loss: 7.5285220Losses:  7.523340225219727 1.9676653146743774 0.2301669418811798
CurrentTrain: epoch  4, batch    48 | loss: 7.5233402Losses:  7.825275897979736 3.0575313568115234 0.24599570035934448
CurrentTrain: epoch  4, batch    49 | loss: 7.8252759Losses:  9.195647239685059 4.262804985046387 0.25569799542427063
CurrentTrain: epoch  4, batch    50 | loss: 9.1956472Losses:  9.406805992126465 3.675410270690918 0.26379644870758057
CurrentTrain: epoch  4, batch    51 | loss: 9.4068060Losses:  6.611652374267578 1.786219835281372 0.2207667976617813
CurrentTrain: epoch  4, batch    52 | loss: 6.6116524Losses:  7.764466285705566 3.338183879852295 0.2376941442489624
CurrentTrain: epoch  4, batch    53 | loss: 7.7644663Losses:  7.048669815063477 2.5097455978393555 0.24311959743499756
CurrentTrain: epoch  4, batch    54 | loss: 7.0486698Losses:  6.52715539932251 2.047297716140747 0.2365860939025879
CurrentTrain: epoch  4, batch    55 | loss: 6.5271554Losses:  8.20332145690918 3.2000088691711426 0.24282896518707275
CurrentTrain: epoch  4, batch    56 | loss: 8.2033215Losses:  7.607029438018799 3.1935012340545654 0.2420525848865509
CurrentTrain: epoch  4, batch    57 | loss: 7.6070294Losses:  7.658185958862305 3.000518798828125 0.23585212230682373
CurrentTrain: epoch  4, batch    58 | loss: 7.6581860Losses:  8.41305923461914 4.0513129234313965 0.15806594491004944
CurrentTrain: epoch  4, batch    59 | loss: 8.4130592Losses:  6.358689308166504 1.9396196603775024 0.23604455590248108
CurrentTrain: epoch  4, batch    60 | loss: 6.3586893Losses:  10.130241394042969 4.923273086547852 0.2592809200286865
CurrentTrain: epoch  4, batch    61 | loss: 10.1302414Losses:  7.561054229736328 1.1471420526504517 0.33148515224456787
CurrentTrain: epoch  4, batch    62 | loss: 7.5610542Losses:  8.757099151611328 4.170580863952637 0.2376696765422821
CurrentTrain: epoch  5, batch     0 | loss: 8.7570992Losses:  7.8055291175842285 2.618868827819824 0.24777822196483612
CurrentTrain: epoch  5, batch     1 | loss: 7.8055291Losses:  7.549439430236816 2.591160774230957 0.22939521074295044
CurrentTrain: epoch  5, batch     2 | loss: 7.5494394Losses:  10.617055892944336 6.1620073318481445 0.24868032336235046
CurrentTrain: epoch  5, batch     3 | loss: 10.6170559Losses:  7.6098737716674805 2.5487537384033203 0.24663609266281128
CurrentTrain: epoch  5, batch     4 | loss: 7.6098738Losses:  9.564979553222656 4.727419853210449 0.24985358119010925
CurrentTrain: epoch  5, batch     5 | loss: 9.5649796Losses:  9.190519332885742 4.489075183868408 0.25042927265167236
CurrentTrain: epoch  5, batch     6 | loss: 9.1905193Losses:  7.488056182861328 3.0200603008270264 0.254798948764801
CurrentTrain: epoch  5, batch     7 | loss: 7.4880562Losses:  8.588666915893555 4.184700012207031 0.25140583515167236
CurrentTrain: epoch  5, batch     8 | loss: 8.5886669Losses:  7.900028228759766 3.2902960777282715 0.2433502972126007
CurrentTrain: epoch  5, batch     9 | loss: 7.9000282Losses:  10.044204711914062 4.511556625366211 0.2407776117324829
CurrentTrain: epoch  5, batch    10 | loss: 10.0442047Losses:  6.425563812255859 1.8596068620681763 0.22821396589279175
CurrentTrain: epoch  5, batch    11 | loss: 6.4255638Losses:  7.435885429382324 2.2185988426208496 0.2288820743560791
CurrentTrain: epoch  5, batch    12 | loss: 7.4358854Losses:  7.879302501678467 3.382685899734497 0.2556679844856262
CurrentTrain: epoch  5, batch    13 | loss: 7.8793025Losses:  8.782336235046387 4.451901435852051 0.15683522820472717
CurrentTrain: epoch  5, batch    14 | loss: 8.7823362Losses:  7.618381023406982 3.2510993480682373 0.23139438033103943
CurrentTrain: epoch  5, batch    15 | loss: 7.6183810Losses:  7.605688571929932 3.195517063140869 0.24384158849716187
CurrentTrain: epoch  5, batch    16 | loss: 7.6056886Losses:  9.274728775024414 4.472044467926025 0.157194584608078
CurrentTrain: epoch  5, batch    17 | loss: 9.2747288Losses:  7.858609676361084 3.184530735015869 0.2329830378293991
CurrentTrain: epoch  5, batch    18 | loss: 7.8586097Losses:  6.86236572265625 2.3033933639526367 0.2546231746673584
CurrentTrain: epoch  5, batch    19 | loss: 6.8623657Losses:  6.801046371459961 2.507284641265869 0.22828620672225952
CurrentTrain: epoch  5, batch    20 | loss: 6.8010464Losses:  7.617913722991943 2.8645124435424805 0.23547551035881042
CurrentTrain: epoch  5, batch    21 | loss: 7.6179137Losses:  7.845550537109375 3.2363924980163574 0.2450263500213623
CurrentTrain: epoch  5, batch    22 | loss: 7.8455505Losses:  7.749884605407715 3.279719829559326 0.23873083293437958
CurrentTrain: epoch  5, batch    23 | loss: 7.7498846Losses:  7.84609317779541 3.2623629570007324 0.2294798195362091
CurrentTrain: epoch  5, batch    24 | loss: 7.8460932Losses:  8.125629425048828 3.26557993888855 0.2380731701850891
CurrentTrain: epoch  5, batch    25 | loss: 8.1256294Losses:  7.497983455657959 3.0515666007995605 0.23459726572036743
CurrentTrain: epoch  5, batch    26 | loss: 7.4979835Losses:  8.226990699768066 3.799379348754883 0.23205414414405823
CurrentTrain: epoch  5, batch    27 | loss: 8.2269907Losses:  7.528765678405762 3.0692148208618164 0.23981192708015442
CurrentTrain: epoch  5, batch    28 | loss: 7.5287657Losses:  7.405559539794922 3.006746292114258 0.23985357582569122
CurrentTrain: epoch  5, batch    29 | loss: 7.4055595Losses:  9.781128883361816 4.266950607299805 0.2569113075733185
CurrentTrain: epoch  5, batch    30 | loss: 9.7811289Losses:  7.3927459716796875 3.026871681213379 0.2352524846792221
CurrentTrain: epoch  5, batch    31 | loss: 7.3927460Losses:  9.013626098632812 4.698211669921875 0.23030757904052734
CurrentTrain: epoch  5, batch    32 | loss: 9.0136261Losses:  8.154699325561523 3.0236830711364746 0.2396065592765808
CurrentTrain: epoch  5, batch    33 | loss: 8.1546993Losses:  7.922605037689209 3.143460988998413 0.24977634847164154
CurrentTrain: epoch  5, batch    34 | loss: 7.9226050Losses:  11.812455177307129 7.385232448577881 0.3091861605644226
CurrentTrain: epoch  5, batch    35 | loss: 11.8124552Losses:  9.958637237548828 5.286437034606934 0.2647700905799866
CurrentTrain: epoch  5, batch    36 | loss: 9.9586372Losses:  6.970142364501953 2.538844585418701 0.2565418481826782
CurrentTrain: epoch  5, batch    37 | loss: 6.9701424Losses:  7.415842056274414 2.9808545112609863 0.2350183129310608
CurrentTrain: epoch  5, batch    38 | loss: 7.4158421Losses:  7.253471374511719 2.5914969444274902 0.24517948925495148
CurrentTrain: epoch  5, batch    39 | loss: 7.2534714Losses:  8.969683647155762 4.241837978363037 0.254628449678421
CurrentTrain: epoch  5, batch    40 | loss: 8.9696836Losses:  8.477849960327148 4.148097991943359 0.24839872121810913
CurrentTrain: epoch  5, batch    41 | loss: 8.4778500Losses:  7.483532428741455 2.988999366760254 0.23548272252082825
CurrentTrain: epoch  5, batch    42 | loss: 7.4835324Losses:  7.135570526123047 2.764453887939453 0.22924527525901794
CurrentTrain: epoch  5, batch    43 | loss: 7.1355705Losses:  7.814091205596924 3.2314300537109375 0.22940510511398315
CurrentTrain: epoch  5, batch    44 | loss: 7.8140912Losses:  7.294830322265625 2.9620707035064697 0.22737397253513336
CurrentTrain: epoch  5, batch    45 | loss: 7.2948303Losses:  6.9882893562316895 2.61728572845459 0.2264360785484314
CurrentTrain: epoch  5, batch    46 | loss: 6.9882894Losses:  7.385627746582031 3.0320749282836914 0.24728792905807495
CurrentTrain: epoch  5, batch    47 | loss: 7.3856277Losses:  6.8731369972229 2.473176956176758 0.23951968550682068
CurrentTrain: epoch  5, batch    48 | loss: 6.8731370Losses:  6.796268939971924 2.238466262817383 0.22916126251220703
CurrentTrain: epoch  5, batch    49 | loss: 6.7962689Losses:  6.529719352722168 1.921609878540039 0.21907265484333038
CurrentTrain: epoch  5, batch    50 | loss: 6.5297194Losses:  6.790359973907471 2.2639384269714355 0.2269626259803772
CurrentTrain: epoch  5, batch    51 | loss: 6.7903600Losses:  8.251529693603516 3.7067131996154785 0.26713240146636963
CurrentTrain: epoch  5, batch    52 | loss: 8.2515297Losses:  7.007503032684326 2.3843894004821777 0.2304747998714447
CurrentTrain: epoch  5, batch    53 | loss: 7.0075030Losses:  7.232641696929932 2.6964588165283203 0.2289767563343048
CurrentTrain: epoch  5, batch    54 | loss: 7.2326417Losses:  8.168754577636719 3.7984437942504883 0.16919919848442078
CurrentTrain: epoch  5, batch    55 | loss: 8.1687546Losses:  6.56887674331665 2.2138164043426514 0.23472338914871216
CurrentTrain: epoch  5, batch    56 | loss: 6.5688767Losses:  7.294931888580322 2.7522122859954834 0.1456093043088913
CurrentTrain: epoch  5, batch    57 | loss: 7.2949319Losses:  6.7430949211120605 2.313091993331909 0.2327139675617218
CurrentTrain: epoch  5, batch    58 | loss: 6.7430949Losses:  8.274784088134766 3.8574507236480713 0.25028330087661743
CurrentTrain: epoch  5, batch    59 | loss: 8.2747841Losses:  6.462747097015381 2.2281436920166016 0.23056963086128235
CurrentTrain: epoch  5, batch    60 | loss: 6.4627471Losses:  7.539377689361572 2.9910221099853516 0.23926109075546265
CurrentTrain: epoch  5, batch    61 | loss: 7.5393777Losses:  6.027279376983643 1.577906847000122 0.09784084558486938
CurrentTrain: epoch  5, batch    62 | loss: 6.0272794Losses:  7.964030742645264 3.585268020629883 0.26567402482032776
CurrentTrain: epoch  6, batch     0 | loss: 7.9640307Losses:  7.673051357269287 3.298602819442749 0.2288125604391098
CurrentTrain: epoch  6, batch     1 | loss: 7.6730514Losses:  8.561247825622559 4.100924491882324 0.23813468217849731
CurrentTrain: epoch  6, batch     2 | loss: 8.5612478Losses:  10.277628898620605 5.9211530685424805 0.23441985249519348
CurrentTrain: epoch  6, batch     3 | loss: 10.2776289Losses:  9.077494621276855 4.708159446716309 0.25255218148231506
CurrentTrain: epoch  6, batch     4 | loss: 9.0774946Losses:  6.298518657684326 1.9316270351409912 0.23471111059188843
CurrentTrain: epoch  6, batch     5 | loss: 6.2985187Losses:  6.763396739959717 2.3931422233581543 0.2348238080739975
CurrentTrain: epoch  6, batch     6 | loss: 6.7633967Losses:  7.160970211029053 2.684866428375244 0.24922242760658264
CurrentTrain: epoch  6, batch     7 | loss: 7.1609702Losses:  9.6328763961792 5.2191596031188965 0.25002938508987427
CurrentTrain: epoch  6, batch     8 | loss: 9.6328764Losses:  8.959030151367188 4.588118076324463 0.26312798261642456
CurrentTrain: epoch  6, batch     9 | loss: 8.9590302Losses:  6.610270023345947 2.267869234085083 0.22492548823356628
CurrentTrain: epoch  6, batch    10 | loss: 6.6102700Losses:  6.990853786468506 2.510528802871704 0.2368590086698532
CurrentTrain: epoch  6, batch    11 | loss: 6.9908538Losses:  9.018299102783203 4.537779808044434 0.25774824619293213
CurrentTrain: epoch  6, batch    12 | loss: 9.0182991Losses:  8.689614295959473 4.157317161560059 0.2518126666545868
CurrentTrain: epoch  6, batch    13 | loss: 8.6896143Losses:  9.24410343170166 4.845709800720215 0.25319385528564453
CurrentTrain: epoch  6, batch    14 | loss: 9.2441034Losses:  8.97182559967041 4.690829753875732 0.23721076548099518
CurrentTrain: epoch  6, batch    15 | loss: 8.9718256Losses:  6.652698516845703 2.360502004623413 0.22925394773483276
CurrentTrain: epoch  6, batch    16 | loss: 6.6526985Losses:  7.967013835906982 3.5347485542297363 0.2639363408088684
CurrentTrain: epoch  6, batch    17 | loss: 7.9670138Losses:  7.2897562980651855 2.930172920227051 0.23979000747203827
CurrentTrain: epoch  6, batch    18 | loss: 7.2897563Losses:  6.123785018920898 1.8217798471450806 0.21998989582061768
CurrentTrain: epoch  6, batch    19 | loss: 6.1237850Losses:  8.964414596557617 4.633182525634766 0.24140846729278564
CurrentTrain: epoch  6, batch    20 | loss: 8.9644146Losses:  6.372072219848633 1.9501862525939941 0.229360431432724
CurrentTrain: epoch  6, batch    21 | loss: 6.3720722Losses:  6.484663963317871 2.1186060905456543 0.23146778345108032
CurrentTrain: epoch  6, batch    22 | loss: 6.4846640Losses:  7.078278541564941 2.7101786136627197 0.239019513130188
CurrentTrain: epoch  6, batch    23 | loss: 7.0782785Losses:  7.206605911254883 2.8143882751464844 0.244283989071846
CurrentTrain: epoch  6, batch    24 | loss: 7.2066059Losses:  8.603418350219727 4.2923736572265625 0.2559646964073181
CurrentTrain: epoch  6, batch    25 | loss: 8.6034184Losses:  6.23037576675415 1.9400033950805664 0.23328928649425507
CurrentTrain: epoch  6, batch    26 | loss: 6.2303758Losses:  7.183220386505127 2.742668390274048 0.2345244288444519
CurrentTrain: epoch  6, batch    27 | loss: 7.1832204Losses:  6.6931233406066895 2.4084153175354004 0.22765257954597473
CurrentTrain: epoch  6, batch    28 | loss: 6.6931233Losses:  6.905924320220947 2.4971280097961426 0.23476094007492065
CurrentTrain: epoch  6, batch    29 | loss: 6.9059243Losses:  8.595283508300781 4.248722553253174 0.23155055940151215
CurrentTrain: epoch  6, batch    30 | loss: 8.5952835Losses:  6.260064125061035 1.8459093570709229 0.22023475170135498
CurrentTrain: epoch  6, batch    31 | loss: 6.2600641Losses:  7.56740665435791 3.1557421684265137 0.24602854251861572
CurrentTrain: epoch  6, batch    32 | loss: 7.5674067Losses:  7.543890476226807 3.211501359939575 0.2432999312877655
CurrentTrain: epoch  6, batch    33 | loss: 7.5438905Losses:  8.471792221069336 3.1174235343933105 0.22882625460624695
CurrentTrain: epoch  6, batch    34 | loss: 8.4717922Losses:  6.788705825805664 2.0826401710510254 0.22266525030136108
CurrentTrain: epoch  6, batch    35 | loss: 6.7887058Losses:  7.279448509216309 2.477111339569092 0.23493146896362305
CurrentTrain: epoch  6, batch    36 | loss: 7.2794485Losses:  7.26820182800293 2.944709300994873 0.22944365441799164
CurrentTrain: epoch  6, batch    37 | loss: 7.2682018Losses:  10.798806190490723 6.290226459503174 0.17310774326324463
CurrentTrain: epoch  6, batch    38 | loss: 10.7988062Losses:  7.545308589935303 3.128880739212036 0.23672647774219513
CurrentTrain: epoch  6, batch    39 | loss: 7.5453086Losses:  8.407636642456055 4.117571830749512 0.23500801622867584
CurrentTrain: epoch  6, batch    40 | loss: 8.4076366Losses:  7.069559097290039 2.553435802459717 0.23080500960350037
CurrentTrain: epoch  6, batch    41 | loss: 7.0695591Losses:  6.167443752288818 1.8628281354904175 0.21919631958007812
CurrentTrain: epoch  6, batch    42 | loss: 6.1674438Losses:  7.9022216796875 3.001218318939209 0.22684697806835175
CurrentTrain: epoch  6, batch    43 | loss: 7.9022217Losses:  7.453169822692871 2.8027522563934326 0.22714433073997498
CurrentTrain: epoch  6, batch    44 | loss: 7.4531698Losses:  8.034134864807129 3.185253143310547 0.24147942662239075
CurrentTrain: epoch  6, batch    45 | loss: 8.0341349Losses:  9.68892765045166 5.389429092407227 0.25406062602996826
CurrentTrain: epoch  6, batch    46 | loss: 9.6889277Losses:  7.853330135345459 3.3175225257873535 0.24658839404582977
CurrentTrain: epoch  6, batch    47 | loss: 7.8533301Losses:  7.592660903930664 2.958653450012207 0.2504284977912903
CurrentTrain: epoch  6, batch    48 | loss: 7.5926609Losses:  8.997671127319336 4.767940521240234 0.16822449862957
CurrentTrain: epoch  6, batch    49 | loss: 8.9976711Losses:  6.620594501495361 2.367309093475342 0.2302943468093872
CurrentTrain: epoch  6, batch    50 | loss: 6.6205945Losses:  7.890684604644775 3.0307464599609375 0.24060580134391785
CurrentTrain: epoch  6, batch    51 | loss: 7.8906846Losses:  7.613167762756348 3.268570899963379 0.23237141966819763
CurrentTrain: epoch  6, batch    52 | loss: 7.6131678Losses:  6.601169586181641 2.1480655670166016 0.2306138277053833
CurrentTrain: epoch  6, batch    53 | loss: 6.6011696Losses:  6.948150634765625 2.570236921310425 0.23219963908195496
CurrentTrain: epoch  6, batch    54 | loss: 6.9481506Losses:  6.424805164337158 2.138761520385742 0.22892242670059204
CurrentTrain: epoch  6, batch    55 | loss: 6.4248052Losses:  8.778000831604004 4.156311988830566 0.23433822393417358
CurrentTrain: epoch  6, batch    56 | loss: 8.7780008Losses:  6.617796421051025 2.172515392303467 0.22192007303237915
CurrentTrain: epoch  6, batch    57 | loss: 6.6177964Losses:  7.580934047698975 2.995781421661377 0.22898194193840027
CurrentTrain: epoch  6, batch    58 | loss: 7.5809340Losses:  6.624512672424316 2.361785411834717 0.23160195350646973
CurrentTrain: epoch  6, batch    59 | loss: 6.6245127Losses:  7.491875648498535 3.2419605255126953 0.23278996348381042
CurrentTrain: epoch  6, batch    60 | loss: 7.4918756Losses:  6.452447891235352 2.200151205062866 0.2283189594745636
CurrentTrain: epoch  6, batch    61 | loss: 6.4524479Losses:  5.574842929840088 1.3430371284484863 0.16661453247070312
CurrentTrain: epoch  6, batch    62 | loss: 5.5748429Losses:  7.47620964050293 3.1136207580566406 0.24014584720134735
CurrentTrain: epoch  7, batch     0 | loss: 7.4762096Losses:  7.4103217124938965 3.1401801109313965 0.23033930361270905
CurrentTrain: epoch  7, batch     1 | loss: 7.4103217Losses:  9.286422729492188 3.7958717346191406 0.22987331449985504
CurrentTrain: epoch  7, batch     2 | loss: 9.2864227Losses:  7.537001132965088 3.151998519897461 0.1434539258480072
CurrentTrain: epoch  7, batch     3 | loss: 7.5370011Losses:  7.297003269195557 2.690019369125366 0.2316240668296814
CurrentTrain: epoch  7, batch     4 | loss: 7.2970033Losses:  6.382678985595703 2.142606735229492 0.22566798329353333
CurrentTrain: epoch  7, batch     5 | loss: 6.3826790Losses:  6.827240467071533 2.5667707920074463 0.22284084558486938
CurrentTrain: epoch  7, batch     6 | loss: 6.8272405Losses:  7.268120288848877 2.9335379600524902 0.2261420041322708
CurrentTrain: epoch  7, batch     7 | loss: 7.2681203Losses:  8.058496475219727 3.7356138229370117 0.15978002548217773
CurrentTrain: epoch  7, batch     8 | loss: 8.0584965Losses:  7.783834934234619 2.9811413288116455 0.23271247744560242
CurrentTrain: epoch  7, batch     9 | loss: 7.7838349Losses:  5.90217399597168 1.621688961982727 0.21323618292808533
CurrentTrain: epoch  7, batch    10 | loss: 5.9021740Losses:  5.912545204162598 1.588708758354187 0.21617341041564941
CurrentTrain: epoch  7, batch    11 | loss: 5.9125452Losses:  7.997994422912598 3.4536330699920654 0.23431585729122162
CurrentTrain: epoch  7, batch    12 | loss: 7.9979944Losses:  7.509199142456055 3.2116124629974365 0.23737068474292755
CurrentTrain: epoch  7, batch    13 | loss: 7.5091991Losses:  9.681556701660156 4.924218654632568 0.15801827609539032
CurrentTrain: epoch  7, batch    14 | loss: 9.6815567Losses:  9.111298561096191 4.495236396789551 0.24667857587337494
CurrentTrain: epoch  7, batch    15 | loss: 9.1112986Losses:  7.862998008728027 3.3978705406188965 0.1473650336265564
CurrentTrain: epoch  7, batch    16 | loss: 7.8629980Losses:  11.765602111816406 7.258694648742676 0.2571478486061096
CurrentTrain: epoch  7, batch    17 | loss: 11.7656021Losses:  6.687020301818848 2.3373210430145264 0.23462983965873718
CurrentTrain: epoch  7, batch    18 | loss: 6.6870203Losses:  6.986286163330078 2.563969135284424 0.23663592338562012
CurrentTrain: epoch  7, batch    19 | loss: 6.9862862Losses:  6.649857044219971 2.4007253646850586 0.23326817154884338
CurrentTrain: epoch  7, batch    20 | loss: 6.6498570Losses:  8.960336685180664 4.505926132202148 0.25557661056518555
CurrentTrain: epoch  7, batch    21 | loss: 8.9603367Losses:  8.857170104980469 4.557612419128418 0.23787403106689453
CurrentTrain: epoch  7, batch    22 | loss: 8.8571701Losses:  6.932193279266357 2.7161452770233154 0.22476893663406372
CurrentTrain: epoch  7, batch    23 | loss: 6.9321933Losses:  6.406656265258789 2.1258292198181152 0.22090551257133484
CurrentTrain: epoch  7, batch    24 | loss: 6.4066563Losses:  7.7361321449279785 3.320364475250244 0.2443467676639557
CurrentTrain: epoch  7, batch    25 | loss: 7.7361321Losses:  6.669210910797119 2.387906789779663 0.22564177215099335
CurrentTrain: epoch  7, batch    26 | loss: 6.6692109Losses:  7.515836715698242 3.236943244934082 0.23691797256469727
CurrentTrain: epoch  7, batch    27 | loss: 7.5158367Losses:  6.725773334503174 2.448188543319702 0.2432495355606079
CurrentTrain: epoch  7, batch    28 | loss: 6.7257733Losses:  7.962610721588135 3.6966683864593506 0.22958630323410034
CurrentTrain: epoch  7, batch    29 | loss: 7.9626107Losses:  7.0406036376953125 2.628931999206543 0.23540520668029785
CurrentTrain: epoch  7, batch    30 | loss: 7.0406036Losses:  6.690998554229736 2.354557752609253 0.23374328017234802
CurrentTrain: epoch  7, batch    31 | loss: 6.6909986Losses:  7.200506210327148 2.9336752891540527 0.25192248821258545
CurrentTrain: epoch  7, batch    32 | loss: 7.2005062Losses:  9.412545204162598 4.9449567794799805 0.16650685667991638
CurrentTrain: epoch  7, batch    33 | loss: 9.4125452Losses:  6.790102005004883 2.3941361904144287 0.2450389266014099
CurrentTrain: epoch  7, batch    34 | loss: 6.7901020Losses:  9.082132339477539 4.738377571105957 0.25299084186553955
CurrentTrain: epoch  7, batch    35 | loss: 9.0821323Losses:  6.036154747009277 1.793026089668274 0.22028791904449463
CurrentTrain: epoch  7, batch    36 | loss: 6.0361547Losses:  6.204341411590576 1.936207890510559 0.22081990540027618
CurrentTrain: epoch  7, batch    37 | loss: 6.2043414Losses:  8.549413681030273 4.027040004730225 0.2565751075744629
CurrentTrain: epoch  7, batch    38 | loss: 8.5494137Losses:  8.93358325958252 4.549225330352783 0.24743898212909698
CurrentTrain: epoch  7, batch    39 | loss: 8.9335833Losses:  7.001746654510498 2.704115390777588 0.22202405333518982
CurrentTrain: epoch  7, batch    40 | loss: 7.0017467Losses:  8.385464668273926 4.0919270515441895 0.1426343023777008
CurrentTrain: epoch  7, batch    41 | loss: 8.3854647Losses:  7.452861309051514 3.1765918731689453 0.24134422838687897
CurrentTrain: epoch  7, batch    42 | loss: 7.4528613Losses:  11.377724647521973 7.138970375061035 0.2561856210231781
CurrentTrain: epoch  7, batch    43 | loss: 11.3777246Losses:  7.06170129776001 2.7065939903259277 0.23774823546409607
CurrentTrain: epoch  7, batch    44 | loss: 7.0617013Losses:  6.870498180389404 2.5866518020629883 0.21897916495800018
CurrentTrain: epoch  7, batch    45 | loss: 6.8704982Losses:  5.714473724365234 1.4483368396759033 0.2161426842212677
CurrentTrain: epoch  7, batch    46 | loss: 5.7144737Losses:  6.388378143310547 2.1511621475219727 0.2236005663871765
CurrentTrain: epoch  7, batch    47 | loss: 6.3883781Losses:  6.429770469665527 2.1594676971435547 0.22364354133605957
CurrentTrain: epoch  7, batch    48 | loss: 6.4297705Losses:  7.227556228637695 2.9247288703918457 0.24079161882400513
CurrentTrain: epoch  7, batch    49 | loss: 7.2275562Losses:  7.996087551116943 3.855299949645996 0.15999841690063477
CurrentTrain: epoch  7, batch    50 | loss: 7.9960876Losses:  6.124970436096191 1.9140028953552246 0.22141411900520325
CurrentTrain: epoch  7, batch    51 | loss: 6.1249704Losses:  7.4645490646362305 3.11664080619812 0.25414538383483887
CurrentTrain: epoch  7, batch    52 | loss: 7.4645491Losses:  6.457136154174805 2.191704034805298 0.2161484807729721
CurrentTrain: epoch  7, batch    53 | loss: 6.4571362Losses:  6.739365577697754 2.57328462600708 0.14471575617790222
CurrentTrain: epoch  7, batch    54 | loss: 6.7393656Losses:  7.161643028259277 2.933015823364258 0.15293288230895996
CurrentTrain: epoch  7, batch    55 | loss: 7.1616430Losses:  6.9172210693359375 2.6653900146484375 0.23781003057956696
CurrentTrain: epoch  7, batch    56 | loss: 6.9172211Losses:  11.804405212402344 7.481754302978516 0.15974698960781097
CurrentTrain: epoch  7, batch    57 | loss: 11.8044052Losses:  5.651313304901123 1.4469376802444458 0.21335262060165405
CurrentTrain: epoch  7, batch    58 | loss: 5.6513133Losses:  8.706439018249512 4.41169548034668 0.24401551485061646
CurrentTrain: epoch  7, batch    59 | loss: 8.7064390Losses:  8.798330307006836 4.276661396026611 0.24127614498138428
CurrentTrain: epoch  7, batch    60 | loss: 8.7983303Losses:  6.4994988441467285 2.21091365814209 0.2256937175989151
CurrentTrain: epoch  7, batch    61 | loss: 6.4994988Losses:  4.9006757736206055 0.5957857966423035 0.21584008634090424
CurrentTrain: epoch  7, batch    62 | loss: 4.9006758Losses:  6.6372528076171875 2.384713888168335 0.2318250685930252
CurrentTrain: epoch  8, batch     0 | loss: 6.6372528Losses:  6.816575050354004 2.6253767013549805 0.22932672500610352
CurrentTrain: epoch  8, batch     1 | loss: 6.8165751Losses:  7.317346572875977 3.0865628719329834 0.23736463487148285
CurrentTrain: epoch  8, batch     2 | loss: 7.3173466Losses:  7.411912441253662 3.12119722366333 0.23799282312393188
CurrentTrain: epoch  8, batch     3 | loss: 7.4119124Losses:  7.031886100769043 2.767660617828369 0.23547738790512085
CurrentTrain: epoch  8, batch     4 | loss: 7.0318861Losses:  6.110659122467041 1.8213655948638916 0.22605293989181519
CurrentTrain: epoch  8, batch     5 | loss: 6.1106591Losses:  7.5230841636657715 3.2923152446746826 0.23258620500564575
CurrentTrain: epoch  8, batch     6 | loss: 7.5230842Losses:  7.111313343048096 2.6916821002960205 0.22464698553085327
CurrentTrain: epoch  8, batch     7 | loss: 7.1113133Losses:  6.547472953796387 2.3358750343322754 0.22464674711227417
CurrentTrain: epoch  8, batch     8 | loss: 6.5474730Losses:  7.183956623077393 2.9735565185546875 0.229378342628479
CurrentTrain: epoch  8, batch     9 | loss: 7.1839566Losses:  7.147510528564453 2.9345195293426514 0.2225288450717926
CurrentTrain: epoch  8, batch    10 | loss: 7.1475105Losses:  6.932018280029297 2.701794385910034 0.2461176961660385
CurrentTrain: epoch  8, batch    11 | loss: 6.9320183Losses:  9.033839225769043 4.742297172546387 0.24743475019931793
CurrentTrain: epoch  8, batch    12 | loss: 9.0338392Losses:  7.8848676681518555 3.5788519382476807 0.23761631548404694
CurrentTrain: epoch  8, batch    13 | loss: 7.8848677Losses:  6.0779643058776855 1.8236570358276367 0.22063587605953217
CurrentTrain: epoch  8, batch    14 | loss: 6.0779643Losses:  6.67304801940918 2.431400775909424 0.2221357226371765
CurrentTrain: epoch  8, batch    15 | loss: 6.6730480Losses:  7.212423801422119 2.9684910774230957 0.22041872143745422
CurrentTrain: epoch  8, batch    16 | loss: 7.2124238Losses:  7.158926963806152 2.932199001312256 0.2182934433221817
CurrentTrain: epoch  8, batch    17 | loss: 7.1589270Losses:  6.586924076080322 2.341336488723755 0.22893020510673523
CurrentTrain: epoch  8, batch    18 | loss: 6.5869241Losses:  6.964798927307129 2.6952788829803467 0.21155650913715363
CurrentTrain: epoch  8, batch    19 | loss: 6.9647989Losses:  9.430414199829102 5.194052696228027 0.14390355348587036
CurrentTrain: epoch  8, batch    20 | loss: 9.4304142Losses:  7.510578155517578 3.2441697120666504 0.24036964774131775
CurrentTrain: epoch  8, batch    21 | loss: 7.5105782Losses:  7.248724460601807 3.0336616039276123 0.23014643788337708
CurrentTrain: epoch  8, batch    22 | loss: 7.2487245Losses:  8.254745483398438 2.865452527999878 0.2253561019897461
CurrentTrain: epoch  8, batch    23 | loss: 8.2547455Losses:  7.7985005378723145 2.759725570678711 0.23485799133777618
CurrentTrain: epoch  8, batch    24 | loss: 7.7985005Losses:  8.24022102355957 3.3493199348449707 0.2365078330039978
CurrentTrain: epoch  8, batch    25 | loss: 8.2402210Losses:  8.227309226989746 3.580606460571289 0.24998320639133453
CurrentTrain: epoch  8, batch    26 | loss: 8.2273092Losses:  6.546606540679932 2.1952264308929443 0.23066076636314392
CurrentTrain: epoch  8, batch    27 | loss: 6.5466065Losses:  7.401109218597412 3.119856834411621 0.2306058704853058
CurrentTrain: epoch  8, batch    28 | loss: 7.4011092Losses:  7.640895843505859 2.696045160293579 0.23921318352222443
CurrentTrain: epoch  8, batch    29 | loss: 7.6408958Losses:  8.437870025634766 4.240653038024902 0.15918320417404175
CurrentTrain: epoch  8, batch    30 | loss: 8.4378700Losses:  6.941197872161865 2.681838035583496 0.21278829872608185
CurrentTrain: epoch  8, batch    31 | loss: 6.9411979Losses:  5.782135009765625 1.229292392730713 0.20914912223815918
CurrentTrain: epoch  8, batch    32 | loss: 5.7821350Losses:  7.092466354370117 2.915034294128418 0.22113622725009918
CurrentTrain: epoch  8, batch    33 | loss: 7.0924664Losses:  5.962771892547607 1.5383570194244385 0.21294943988323212
CurrentTrain: epoch  8, batch    34 | loss: 5.9627719Losses:  6.680172920227051 2.396832227706909 0.24161624908447266
CurrentTrain: epoch  8, batch    35 | loss: 6.6801729Losses:  7.405571460723877 2.9090116024017334 0.24194112420082092
CurrentTrain: epoch  8, batch    36 | loss: 7.4055715Losses:  6.161077499389648 1.9712247848510742 0.2204911708831787
CurrentTrain: epoch  8, batch    37 | loss: 6.1610775Losses:  6.8635406494140625 2.6640734672546387 0.22494012117385864
CurrentTrain: epoch  8, batch    38 | loss: 6.8635406Losses:  6.7413835525512695 2.35722279548645 0.2219332456588745
CurrentTrain: epoch  8, batch    39 | loss: 6.7413836Losses:  7.8679680824279785 3.5029401779174805 0.150849387049675
CurrentTrain: epoch  8, batch    40 | loss: 7.8679681Losses:  6.753365993499756 2.559619426727295 0.22266033291816711
CurrentTrain: epoch  8, batch    41 | loss: 6.7533660Losses:  10.363287925720215 5.537484169006348 0.2655988335609436
CurrentTrain: epoch  8, batch    42 | loss: 10.3632879Losses:  10.653100967407227 6.423665523529053 0.2532179355621338
CurrentTrain: epoch  8, batch    43 | loss: 10.6531010Losses:  7.320300102233887 2.858302593231201 0.23072095215320587
CurrentTrain: epoch  8, batch    44 | loss: 7.3203001Losses:  6.43213415145874 2.153748035430908 0.2306174635887146
CurrentTrain: epoch  8, batch    45 | loss: 6.4321342Losses:  6.455867767333984 2.0845999717712402 0.22527630627155304
CurrentTrain: epoch  8, batch    46 | loss: 6.4558678Losses:  6.810725688934326 2.1331472396850586 0.2240215539932251
CurrentTrain: epoch  8, batch    47 | loss: 6.8107257Losses:  6.652052402496338 2.391134262084961 0.24000045657157898
CurrentTrain: epoch  8, batch    48 | loss: 6.6520524Losses:  7.6431779861450195 3.4103989601135254 0.2241569459438324
CurrentTrain: epoch  8, batch    49 | loss: 7.6431780Losses:  7.428203582763672 2.7154197692871094 0.23856648802757263
CurrentTrain: epoch  8, batch    50 | loss: 7.4282036Losses:  6.92427921295166 2.684882402420044 0.23769959807395935
CurrentTrain: epoch  8, batch    51 | loss: 6.9242792Losses:  7.161021709442139 2.6182684898376465 0.23846572637557983
CurrentTrain: epoch  8, batch    52 | loss: 7.1610217Losses:  8.061179161071777 3.8099770545959473 0.25696808099746704
CurrentTrain: epoch  8, batch    53 | loss: 8.0611792Losses:  7.345973968505859 2.958479642868042 0.22652031481266022
CurrentTrain: epoch  8, batch    54 | loss: 7.3459740Losses:  6.965875148773193 2.724412441253662 0.21622756123542786
CurrentTrain: epoch  8, batch    55 | loss: 6.9658751Losses:  10.224689483642578 5.963138103485107 0.24144935607910156
CurrentTrain: epoch  8, batch    56 | loss: 10.2246895Losses:  7.299472808837891 2.999951124191284 0.2332211136817932
CurrentTrain: epoch  8, batch    57 | loss: 7.2994728Losses:  6.155267238616943 1.8097164630889893 0.2221732884645462
CurrentTrain: epoch  8, batch    58 | loss: 6.1552672Losses:  12.670970916748047 8.443075180053711 0.24334563314914703
CurrentTrain: epoch  8, batch    59 | loss: 12.6709709Losses:  7.870550155639648 3.694340705871582 0.1514369696378708
CurrentTrain: epoch  8, batch    60 | loss: 7.8705502Losses:  9.054240226745605 4.701958179473877 0.260163277387619
CurrentTrain: epoch  8, batch    61 | loss: 9.0542402Losses:  5.9470953941345215 1.3923771381378174 0.10022421181201935
CurrentTrain: epoch  8, batch    62 | loss: 5.9470954Losses:  6.460566520690918 2.1185030937194824 0.22761473059654236
CurrentTrain: epoch  9, batch     0 | loss: 6.4605665Losses:  7.559754848480225 2.9184730052948 0.22483403980731964
CurrentTrain: epoch  9, batch     1 | loss: 7.5597548Losses:  7.209558010101318 2.9865822792053223 0.21264797449111938
CurrentTrain: epoch  9, batch     2 | loss: 7.2095580Losses:  6.679744720458984 2.3717634677886963 0.22907361388206482
CurrentTrain: epoch  9, batch     3 | loss: 6.6797447Losses:  6.529205322265625 2.3403964042663574 0.22373321652412415
CurrentTrain: epoch  9, batch     4 | loss: 6.5292053Losses:  7.4833664894104 3.1487278938293457 0.23278149962425232
CurrentTrain: epoch  9, batch     5 | loss: 7.4833665Losses:  6.650421142578125 2.361409902572632 0.13839447498321533
CurrentTrain: epoch  9, batch     6 | loss: 6.6504211Losses:  6.426924705505371 2.1878552436828613 0.217694491147995
CurrentTrain: epoch  9, batch     7 | loss: 6.4269247Losses:  7.521414279937744 3.1732969284057617 0.24664930999279022
CurrentTrain: epoch  9, batch     8 | loss: 7.5214143Losses:  9.083990097045898 4.871076583862305 0.24106596410274506
CurrentTrain: epoch  9, batch     9 | loss: 9.0839901Losses:  7.698389053344727 3.4702255725860596 0.23346710205078125
CurrentTrain: epoch  9, batch    10 | loss: 7.6983891Losses:  7.073012351989746 2.870790481567383 0.2234601080417633
CurrentTrain: epoch  9, batch    11 | loss: 7.0730124Losses:  6.375690937042236 2.1418771743774414 0.21758614480495453
CurrentTrain: epoch  9, batch    12 | loss: 6.3756909Losses:  8.393476486206055 4.1916046142578125 0.2230667620897293
CurrentTrain: epoch  9, batch    13 | loss: 8.3934765Losses:  7.562438488006592 3.1928844451904297 0.24810001254081726
CurrentTrain: epoch  9, batch    14 | loss: 7.5624385Losses:  7.212924480438232 2.9134466648101807 0.23429712653160095
CurrentTrain: epoch  9, batch    15 | loss: 7.2129245Losses:  7.3128156661987305 3.071892738342285 0.23280052840709686
CurrentTrain: epoch  9, batch    16 | loss: 7.3128157Losses:  6.376304626464844 2.0895566940307617 0.22655263543128967
CurrentTrain: epoch  9, batch    17 | loss: 6.3763046Losses:  5.938162803649902 1.5806574821472168 0.21104587614536285
CurrentTrain: epoch  9, batch    18 | loss: 5.9381628Losses:  5.390745639801025 1.203073501586914 0.20598076283931732
CurrentTrain: epoch  9, batch    19 | loss: 5.3907456Losses:  6.783166408538818 2.503788709640503 0.22479307651519775
CurrentTrain: epoch  9, batch    20 | loss: 6.7831664Losses:  6.313945293426514 2.0966691970825195 0.22278240323066711
CurrentTrain: epoch  9, batch    21 | loss: 6.3139453Losses:  12.610411643981934 8.264081954956055 0.2340589165687561
CurrentTrain: epoch  9, batch    22 | loss: 12.6104116Losses:  9.48819637298584 5.060195446014404 0.2751496434211731
CurrentTrain: epoch  9, batch    23 | loss: 9.4881964Losses:  5.924380779266357 1.564245581626892 0.21494224667549133
CurrentTrain: epoch  9, batch    24 | loss: 5.9243808Losses:  6.374198913574219 2.1258015632629395 0.23015110194683075
CurrentTrain: epoch  9, batch    25 | loss: 6.3741989Losses:  7.474822521209717 3.1148786544799805 0.23200811445713043
CurrentTrain: epoch  9, batch    26 | loss: 7.4748225Losses:  7.805849075317383 3.6359944343566895 0.1480221003293991
CurrentTrain: epoch  9, batch    27 | loss: 7.8058491Losses:  7.851974010467529 3.5300240516662598 0.24827370047569275
CurrentTrain: epoch  9, batch    28 | loss: 7.8519740Losses:  7.161591529846191 2.942059278488159 0.22192783653736115
CurrentTrain: epoch  9, batch    29 | loss: 7.1615915Losses:  6.566207408905029 2.3263301849365234 0.22963494062423706
CurrentTrain: epoch  9, batch    30 | loss: 6.5662074Losses:  7.0820841789245605 2.8720450401306152 0.15303455293178558
CurrentTrain: epoch  9, batch    31 | loss: 7.0820842Losses:  6.468585014343262 2.1814663410186768 0.23133540153503418
CurrentTrain: epoch  9, batch    32 | loss: 6.4685850Losses:  7.180719375610352 2.9455039501190186 0.2403094619512558
CurrentTrain: epoch  9, batch    33 | loss: 7.1807194Losses:  7.586494445800781 3.3076858520507812 0.2612024247646332
CurrentTrain: epoch  9, batch    34 | loss: 7.5864944Losses:  8.434880256652832 4.075103759765625 0.2474372535943985
CurrentTrain: epoch  9, batch    35 | loss: 8.4348803Losses:  7.791749000549316 3.6274900436401367 0.2358889877796173
CurrentTrain: epoch  9, batch    36 | loss: 7.7917490Losses:  5.645317077636719 1.453186273574829 0.21874970197677612
CurrentTrain: epoch  9, batch    37 | loss: 5.6453171Losses:  9.16687297821045 4.866437911987305 0.23943883180618286
CurrentTrain: epoch  9, batch    38 | loss: 9.1668730Losses:  10.330374717712402 6.0540385246276855 0.2537682056427002
CurrentTrain: epoch  9, batch    39 | loss: 10.3303747Losses:  6.9683403968811035 2.69242525100708 0.23846668004989624
CurrentTrain: epoch  9, batch    40 | loss: 6.9683404Losses:  7.935217380523682 3.6370162963867188 0.24465259909629822
CurrentTrain: epoch  9, batch    41 | loss: 7.9352174Losses:  7.999497413635254 3.6755640506744385 0.2511693239212036
CurrentTrain: epoch  9, batch    42 | loss: 7.9994974Losses:  9.076760292053223 4.8871870040893555 0.23368507623672485
CurrentTrain: epoch  9, batch    43 | loss: 9.0767603Losses:  6.7151031494140625 2.5212044715881348 0.2179146409034729
CurrentTrain: epoch  9, batch    44 | loss: 6.7151031Losses:  6.590131759643555 2.3971521854400635 0.21278175711631775
CurrentTrain: epoch  9, batch    45 | loss: 6.5901318Losses:  8.189229965209961 3.941255569458008 0.23897907137870789
CurrentTrain: epoch  9, batch    46 | loss: 8.1892300Losses:  6.435087203979492 2.177708148956299 0.22757935523986816
CurrentTrain: epoch  9, batch    47 | loss: 6.4350872Losses:  7.438570976257324 3.269681930541992 0.23867365717887878
CurrentTrain: epoch  9, batch    48 | loss: 7.4385710Losses:  7.249260902404785 2.960317611694336 0.2530219554901123
CurrentTrain: epoch  9, batch    49 | loss: 7.2492609Losses:  9.159263610839844 4.995348930358887 0.15841534733772278
CurrentTrain: epoch  9, batch    50 | loss: 9.1592636Losses:  6.713820457458496 2.49240779876709 0.22939236462116241
CurrentTrain: epoch  9, batch    51 | loss: 6.7138205Losses:  6.549692153930664 2.322345018386841 0.2362765073776245
CurrentTrain: epoch  9, batch    52 | loss: 6.5496922Losses:  5.748108386993408 1.5586433410644531 0.2088645100593567
CurrentTrain: epoch  9, batch    53 | loss: 5.7481084Losses:  7.110407829284668 2.9440107345581055 0.1482851356267929
CurrentTrain: epoch  9, batch    54 | loss: 7.1104078Losses:  11.427759170532227 6.231022834777832 0.17038075625896454
CurrentTrain: epoch  9, batch    55 | loss: 11.4277592Losses:  6.320858001708984 2.1533408164978027 0.21433240175247192
CurrentTrain: epoch  9, batch    56 | loss: 6.3208580Losses:  7.76041316986084 3.4187116622924805 0.2418421506881714
CurrentTrain: epoch  9, batch    57 | loss: 7.7604132Losses:  8.693257331848145 4.541311264038086 0.2420835793018341
CurrentTrain: epoch  9, batch    58 | loss: 8.6932573Losses:  5.843505382537842 1.556073784828186 0.20854870975017548
CurrentTrain: epoch  9, batch    59 | loss: 5.8435054Losses:  9.724458694458008 5.6035380363464355 0.21966727077960968
CurrentTrain: epoch  9, batch    60 | loss: 9.7244587Losses:  8.740890502929688 3.856534719467163 0.15751513838768005
CurrentTrain: epoch  9, batch    61 | loss: 8.7408905Losses:  5.8946533203125 1.6582181453704834 0.2013224959373474
CurrentTrain: epoch  9, batch    62 | loss: 5.8946533
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: born in copenhagen , as a young man bang became interested in surrealism as practised by giorgio de chirico , max ernst and yves tanguy .
Head Entity: yves tanguy
Tail Entity: surrealism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the artist was heavily influenced by the cubist movement, which was pioneered by pablo picasso and georges braque.  
Head Entity: pablo picasso  
Tail Entity: cubist movement  

Relation: movement  
Context: the philosopher's ideas were deeply rooted in the existentialist movement, which gained prominence in the mid-20th century through the works of thinkers like jean-paul sartre.  
Head Entity: jean-paul sartre  
Tail Entity: existentialist movement  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: martin linge was born in norddal , møre og romsdal county , norway .
Head Entity: norddal
Tail Entity: møre og romsdal
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of barcelona is situated in the province of catalonia, spain.  
Head Entity: barcelona  
Tail Entity: catalonia  

Relation: located in the administrative territorial entity  
Context: the university of toronto is located in the province of ontario, canada.  
Head Entity: toronto  
Tail Entity: ontario  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: he also went on a road trip from san antonio to oakland for the magnum project “ postcards from america ” with jim goldberg , alec soth , mikhael subotzky and susan meiselas .
Head Entity: jim goldberg
Tail Entity: magnum
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: After years of dedication and hard work, she was finally recognized as a key member of the prestigious art collective known as the "Creative Minds."  
Head Entity: she  
Tail Entity: Creative Minds  

Relation: member of  
Context: The renowned guitarist joined the band in 2015, contributing to their unique sound and becoming an integral member of the musical group "The Soundwaves."  
Head Entity: guitarist  
Tail Entity: The Soundwaves  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the belleville turnpike bridge ( also known as the rutgers street bridge ) crosses the passaic river , connecting belleville to north arlington .
Head Entity: belleville turnpike bridge
Tail Entity: passaic river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge crosses the golden gate strait, linking san francisco to marin county.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the london bridge crosses the river thames, providing a route between the city of london and southwark.  
Head Entity: london bridge  
Tail Entity: river thames  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the lake flows via the porcupine river and frederick house river to the abitibi river .
Head Entity: abitibi river
Tail Entity: frederick house river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River serves as a tributary to the Mississippi River, contributing significant water flow.  
Head Entity: Mississippi River  
Tail Entity: Missouri River  

Relation: tributary  
Context: The Rio Grande is a major river that acts as a tributary to the Gulf of Mexico, enhancing its water levels.  
Head Entity: Gulf of Mexico  
Tail Entity: Rio Grande  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: in november 2011 , coalition mp and deputy speaker peter slipper replaced labor mp harry jenkins as speaker of the house of representatives , increasing labor 's parliamentary majority from 76–74 to 77–73 .
Head Entity: harry jenkins
Tail Entity: speaker of the house of representatives
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: after serving as the mayor of new york city for three terms, michael bloomberg announced his decision to step down and was succeeded by bill de blasio in january 2014.  
Head Entity: michael bloomberg  
Tail Entity: mayor of new york city  

Relation: position held  
Context: in 2019, former president barack obama endorsed joe biden, who had previously served as vice president under him from 2009 to 2017.  
Head Entity: joe biden  
Tail Entity: vice president
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: it was developed by syn sophia , who also developed the other games in the " style savvy " series .
Head Entity: style savvy
Tail Entity: syn sophia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: the popular game was created by epic games, known for their innovative approach to game design.  
Head Entity: popular game  
Tail Entity: epic games  

Relation: developer  
Context: the groundbreaking software was launched by tech innovations inc., a leader in the tech industry.  
Head Entity: groundbreaking software  
Tail Entity: tech innovations inc.  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: third power was an american psychedelic hard rock band formed in 1969 in detroit , michigan .
Head Entity: third power
Tail Entity: detroit
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the united nations was established in 1945 in san francisco, california, to promote international cooperation.  
Head Entity: united nations  
Tail Entity: san francisco  

Relation: location of formation  
Context: the famous rock band the beatles originated in liverpool, england, in the early 1960s.  
Head Entity: the beatles  
Tail Entity: liverpool  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: during the same month , the band supported kings of leon 's tour of the united states , and coldplay 's tour of the united kingdom .
Head Entity: kings of leon
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous Italian dish, pizza, has gained popularity worldwide, but its roots can be traced back to Naples, Italy.  
Head Entity: pizza  
Tail Entity: Italy  

Relation: country of origin  
Context: The iconic brand, Rolex, is renowned for its luxury watches, which are crafted in Switzerland, known for its precision and quality in watchmaking.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 90.51%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.10%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.40%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 92.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.31%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 93.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.98%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 93.64%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.43%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.54%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 93.55%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.55%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 92.86%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 83.65%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 85.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 88.44%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 88.99%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 89.20%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 89.67%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.38%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 90.51%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.73%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.60%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.86%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 92.10%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 92.14%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 92.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 92.40%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 92.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.63%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.81%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 93.15%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 93.31%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 93.47%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 93.47%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.61%   [EVAL] batch:   46 | acc: 87.50%,  total acc: 93.48%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 93.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 93.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.87%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.98%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 93.64%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.64%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 93.43%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.54%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.54%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 93.55%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.55%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 92.86%   
cur_acc:  ['0.9286']
his_acc:  ['0.9286']
Clustering into  9  clusters
Clusters:  [0 3 1 0 6 6 1 2 8 8 3 0 2 0 5 0 7 4 0 4]
Losses:  12.959513664245605 2.780825138092041 0.7332090139389038
CurrentTrain: epoch  0, batch     0 | loss: 12.9595137Losses:  11.8346586227417 2.921628475189209 0.6925495862960815
CurrentTrain: epoch  0, batch     1 | loss: 11.8346586Losses:  11.450291633605957 2.556353807449341 0.716247022151947
CurrentTrain: epoch  0, batch     2 | loss: 11.4502916Losses:  6.118957042694092 -0.0 0.09877853095531464
CurrentTrain: epoch  0, batch     3 | loss: 6.1189570Losses:  14.091248512268066 4.407468318939209 0.6106536984443665
CurrentTrain: epoch  1, batch     0 | loss: 14.0912485Losses:  10.872631072998047 2.657170295715332 0.7024666666984558
CurrentTrain: epoch  1, batch     1 | loss: 10.8726311Losses:  10.062446594238281 3.141298770904541 0.5153234601020813
CurrentTrain: epoch  1, batch     2 | loss: 10.0624466Losses:  6.9448628425598145 -0.0 0.10521088540554047
CurrentTrain: epoch  1, batch     3 | loss: 6.9448628Losses:  11.207069396972656 3.3391027450561523 0.5644227862358093
CurrentTrain: epoch  2, batch     0 | loss: 11.2070694Losses:  9.53515625 2.80549693107605 0.6598203778266907
CurrentTrain: epoch  2, batch     1 | loss: 9.5351562Losses:  10.132466316223145 2.32942533493042 0.642548680305481
CurrentTrain: epoch  2, batch     2 | loss: 10.1324663Losses:  7.18060302734375 -0.0 0.09037720412015915
CurrentTrain: epoch  2, batch     3 | loss: 7.1806030Losses:  9.330528259277344 2.7803030014038086 0.6286145448684692
CurrentTrain: epoch  3, batch     0 | loss: 9.3305283Losses:  8.512336730957031 2.667966365814209 0.6160660982131958
CurrentTrain: epoch  3, batch     1 | loss: 8.5123367Losses:  9.873642921447754 3.446688652038574 0.4578034579753876
CurrentTrain: epoch  3, batch     2 | loss: 9.8736429Losses:  2.7160487174987793 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 2.7160487Losses:  10.081704139709473 3.4348578453063965 0.6312836408615112
CurrentTrain: epoch  4, batch     0 | loss: 10.0817041Losses:  7.860427379608154 2.9982032775878906 0.506497323513031
CurrentTrain: epoch  4, batch     1 | loss: 7.8604274Losses:  7.5840559005737305 2.121464252471924 0.5432783365249634
CurrentTrain: epoch  4, batch     2 | loss: 7.5840559Losses:  2.1386775970458984 -0.0 0.07737679034471512
CurrentTrain: epoch  4, batch     3 | loss: 2.1386776Losses:  8.987387657165527 3.4402098655700684 0.5407154560089111
CurrentTrain: epoch  5, batch     0 | loss: 8.9873877Losses:  10.743874549865723 4.350778579711914 0.5076188445091248
CurrentTrain: epoch  5, batch     1 | loss: 10.7438745Losses:  7.5232319831848145 3.2620058059692383 0.5833662748336792
CurrentTrain: epoch  5, batch     2 | loss: 7.5232320Losses:  4.501969337463379 -0.0 0.1483617126941681
CurrentTrain: epoch  5, batch     3 | loss: 4.5019693Losses:  10.790261268615723 4.8533830642700195 0.4688350260257721
CurrentTrain: epoch  6, batch     0 | loss: 10.7902613Losses:  7.047476768493652 2.5018484592437744 0.4753226339817047
CurrentTrain: epoch  6, batch     1 | loss: 7.0474768Losses:  9.112920761108398 4.035696029663086 0.5313740968704224
CurrentTrain: epoch  6, batch     2 | loss: 9.1129208Losses:  4.7300825119018555 -0.0 0.13361915946006775
CurrentTrain: epoch  6, batch     3 | loss: 4.7300825Losses:  9.61804485321045 5.388199329376221 0.4601978361606598
CurrentTrain: epoch  7, batch     0 | loss: 9.6180449Losses:  10.073294639587402 4.093123435974121 0.4814437925815582
CurrentTrain: epoch  7, batch     1 | loss: 10.0732946Losses:  8.604809761047363 3.8011107444763184 0.4543217122554779
CurrentTrain: epoch  7, batch     2 | loss: 8.6048098Losses:  5.240769386291504 -0.0 0.10846132040023804
CurrentTrain: epoch  7, batch     3 | loss: 5.2407694Losses:  8.366531372070312 2.81709361076355 0.5458841323852539
CurrentTrain: epoch  8, batch     0 | loss: 8.3665314Losses:  6.256758689880371 1.9933624267578125 0.5315160155296326
CurrentTrain: epoch  8, batch     1 | loss: 6.2567587Losses:  7.9539923667907715 3.0367989540100098 0.428911030292511
CurrentTrain: epoch  8, batch     2 | loss: 7.9539924Losses:  2.351078987121582 -0.0 0.1551838517189026
CurrentTrain: epoch  8, batch     3 | loss: 2.3510790Losses:  8.414498329162598 3.3010339736938477 0.45183125138282776
CurrentTrain: epoch  9, batch     0 | loss: 8.4144983Losses:  8.242417335510254 4.516208171844482 0.40373167395591736
CurrentTrain: epoch  9, batch     1 | loss: 8.2424173Losses:  9.281250953674316 4.083413124084473 0.5434408783912659
CurrentTrain: epoch  9, batch     2 | loss: 9.2812510Losses:  1.9825880527496338 -0.0 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 1.9825881
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Civil Code governs various aspects of civil law within the state of California, including contracts and property rights.  
Head Entity: California Civil Code  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Queen  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the concert last summer.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the next full - frame digital slrs were the canon eos-1ds of late 2002 , followed by kodak 's dcs pro 14n in 2003 .
Head Entity: dcs pro 14n
Tail Entity: kodak
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The latest smartphone model, the Galaxy S21, was released by Samsung in January 2021, showcasing advanced camera technology and performance.  
Head Entity: Galaxy S21  
Tail Entity: Samsung  

Relation: manufacturer  
Context: The iconic Model T was produced by Ford Motor Company, revolutionizing the automotive industry in the early 20th century.  
Head Entity: Model T  
Tail Entity: Ford Motor Company  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, participated to raise funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: This game runs smoothly on multiple operating systems, such as Linux, Windows 10, and the latest version of macOS.  
Head Entity: game  
Tail Entity: Windows 10  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in july 2007 , griffin lost by a technical knockout to light heavyweight champion danny green of the world boxing association at challenge stadium in perth , australia .
Head Entity: danny green
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: during the 2020 olympics, the men's 100 meters final showcased athletes from various competition classes, with usain bolt representing the sprinting class.  
Head Entity: usain bolt  
Tail Entity: sprinting  

Relation: competition class  
Context: the formula one race in monaco is known for its high-speed competition class, where lewis hamilton competed fiercely against other top drivers.  
Head Entity: lewis hamilton  
Tail Entity: formula one
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: swartz served as swedish prime minister for only seven months before being forced from power after the swedish elections of 1917 and replaced by the liberal government of nils edén .
Head Entity: swedish elections of 1917
Tail Entity: nils edén
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: in the recent municipal elections, the voters chose maria gonzalez as the new mayor of the city, marking a significant shift in local governance.  
Head Entity: municipal elections  
Tail Entity: maria gonzalez  

Relation: successful candidate  
Context: after a closely contested race, the citizens of springfield elected john doe as their new governor, succeeding the long-serving incumbent.  
Head Entity: springfield elections  
Tail Entity: john doe  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce rivalry, yet they shared a bond that only siblings could understand.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it became clear that the two sisters, Maria and Elena, had grown apart over the years, but their shared childhood memories reminded them of their strong sibling connection.  
Head Entity: Maria  
Tail Entity: Elena  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: the united states navy is known for its naval operations and maritime security.  
Head Entity: united states navy  
Tail Entity: naval operations  

Relation: military branch  
Context: general patton served in the united states army during world war ii.  
Head Entity: general patton  
Tail Entity: united states army  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
Losses:  8.024097442626953 1.220346212387085 0.6144716739654541
MemoryTrain:  epoch  0, batch     0 | loss: 8.0240974Losses:  5.831979751586914 1.843956708908081 0.551110029220581
MemoryTrain:  epoch  0, batch     1 | loss: 5.8319798Losses:  8.967528343200684 1.8346734046936035 0.5381284952163696
MemoryTrain:  epoch  0, batch     2 | loss: 8.9675283Losses:  6.599679946899414 2.0177793502807617 0.45986253023147583
MemoryTrain:  epoch  0, batch     3 | loss: 6.5996799Losses:  7.6492109298706055 1.6938133239746094 0.5610468983650208
MemoryTrain:  epoch  1, batch     0 | loss: 7.6492109Losses:  5.656561851501465 0.9177326560020447 0.5751222968101501
MemoryTrain:  epoch  1, batch     1 | loss: 5.6565619Losses:  6.281011581420898 0.6210275888442993 0.7392316460609436
MemoryTrain:  epoch  1, batch     2 | loss: 6.2810116Losses:  5.605820655822754 0.7669390439987183 0.40476343035697937
MemoryTrain:  epoch  1, batch     3 | loss: 5.6058207Losses:  5.268165111541748 0.48677492141723633 0.6957200765609741
MemoryTrain:  epoch  2, batch     0 | loss: 5.2681651Losses:  5.087131500244141 0.8426306247711182 0.76837158203125
MemoryTrain:  epoch  2, batch     1 | loss: 5.0871315Losses:  5.774990558624268 0.9070492386817932 0.5990871787071228
MemoryTrain:  epoch  2, batch     2 | loss: 5.7749906Losses:  4.894779682159424 0.4830031394958496 0.4676319360733032
MemoryTrain:  epoch  2, batch     3 | loss: 4.8947797Losses:  5.543092250823975 1.4820027351379395 0.6036189794540405
MemoryTrain:  epoch  3, batch     0 | loss: 5.5430923Losses:  4.76033353805542 1.1590168476104736 0.648196816444397
MemoryTrain:  epoch  3, batch     1 | loss: 4.7603335Losses:  4.809473037719727 0.36048686504364014 0.7830973267555237
MemoryTrain:  epoch  3, batch     2 | loss: 4.8094730Losses:  3.4946372509002686 0.21690551936626434 0.5574890971183777
MemoryTrain:  epoch  3, batch     3 | loss: 3.4946373Losses:  3.382516622543335 0.21901819109916687 0.6880362033843994
MemoryTrain:  epoch  4, batch     0 | loss: 3.3825166Losses:  5.780714511871338 1.697558879852295 0.6419816017150879
MemoryTrain:  epoch  4, batch     1 | loss: 5.7807145Losses:  3.7008872032165527 0.44808921217918396 0.6278945803642273
MemoryTrain:  epoch  4, batch     2 | loss: 3.7008872Losses:  4.193877696990967 0.8292708396911621 0.5253885388374329
MemoryTrain:  epoch  4, batch     3 | loss: 4.1938777Losses:  3.8907926082611084 0.578270435333252 0.6984739303588867
MemoryTrain:  epoch  5, batch     0 | loss: 3.8907926Losses:  4.882388591766357 1.6664924621582031 0.5748366713523865
MemoryTrain:  epoch  5, batch     1 | loss: 4.8823886Losses:  3.9380249977111816 0.8872116208076477 0.724932849407196
MemoryTrain:  epoch  5, batch     2 | loss: 3.9380250Losses:  2.594827890396118 0.45294445753097534 0.4590373635292053
MemoryTrain:  epoch  5, batch     3 | loss: 2.5948279Losses:  5.050914764404297 1.5843905210494995 0.5745552778244019
MemoryTrain:  epoch  6, batch     0 | loss: 5.0509148Losses:  4.194118499755859 1.378226637840271 0.6054260730743408
MemoryTrain:  epoch  6, batch     1 | loss: 4.1941185Losses:  4.139317989349365 1.688120722770691 0.5734118819236755
MemoryTrain:  epoch  6, batch     2 | loss: 4.1393180Losses:  2.2311389446258545 0.2324356734752655 0.4438862204551697
MemoryTrain:  epoch  6, batch     3 | loss: 2.2311389Losses:  3.5084593296051025 1.4039239883422852 0.6492540836334229
MemoryTrain:  epoch  7, batch     0 | loss: 3.5084593Losses:  3.807347297668457 1.0772151947021484 0.5862002372741699
MemoryTrain:  epoch  7, batch     1 | loss: 3.8073473Losses:  3.942985773086548 1.7233521938323975 0.5660551190376282
MemoryTrain:  epoch  7, batch     2 | loss: 3.9429858Losses:  3.9780662059783936 1.3645814657211304 0.4668004512786865
MemoryTrain:  epoch  7, batch     3 | loss: 3.9780662Losses:  2.9714670181274414 0.6799719929695129 0.6704844236373901
MemoryTrain:  epoch  8, batch     0 | loss: 2.9714670Losses:  3.5567643642425537 1.089367389678955 0.6076318621635437
MemoryTrain:  epoch  8, batch     1 | loss: 3.5567644Losses:  2.8096587657928467 0.47427964210510254 0.711414635181427
MemoryTrain:  epoch  8, batch     2 | loss: 2.8096588Losses:  2.5254244804382324 -0.0 0.6216049194335938
MemoryTrain:  epoch  8, batch     3 | loss: 2.5254245Losses:  3.1714541912078857 0.9150702357292175 0.5740692019462585
MemoryTrain:  epoch  9, batch     0 | loss: 3.1714542Losses:  3.4324827194213867 0.9209573268890381 0.6458113193511963
MemoryTrain:  epoch  9, batch     1 | loss: 3.4324827Losses:  3.1412317752838135 0.9555993676185608 0.5679336190223694
MemoryTrain:  epoch  9, batch     2 | loss: 3.1412318Losses:  2.0609257221221924 -0.0 0.7202761769294739
MemoryTrain:  epoch  9, batch     3 | loss: 2.0609257
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 59.38%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 72.16%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 75.45%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 75.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.34%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 77.21%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 78.47%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 79.28%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 80.71%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 83.26%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 83.84%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.88%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 85.16%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.61%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 86.43%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.81%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.16%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 86.68%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 86.06%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 85.31%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 84.60%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 84.23%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 84.16%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 83.81%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 83.47%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 83.02%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 82.71%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 82.68%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 82.40%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 82.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 82.23%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 82.45%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 82.55%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 82.52%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 82.73%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 82.35%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 81.68%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 80.93%   [EVAL] batch:   59 | acc: 56.25%,  total acc: 80.52%   [EVAL] batch:   60 | acc: 25.00%,  total acc: 79.61%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 79.13%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 78.17%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 86.11%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 90.28%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 90.79%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 91.19%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 91.03%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 91.15%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 91.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 91.35%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.96%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 92.03%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 92.29%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 92.54%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.99%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 93.20%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 93.21%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 93.40%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 93.58%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 93.91%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 94.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 94.21%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 94.33%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 94.18%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 94.31%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 94.02%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 93.62%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 93.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.50%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 93.51%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 93.51%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.63%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 93.41%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 93.30%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 93.31%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 93.21%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 93.33%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 93.44%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 93.45%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 93.15%   [EVAL] batch:   63 | acc: 68.75%,  total acc: 92.77%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 92.21%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 91.95%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 91.32%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 90.99%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 90.67%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 90.49%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 90.28%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 90.24%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 90.29%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 90.25%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 90.21%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:   77 | acc: 87.50%,  total acc: 90.14%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 90.11%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 90.08%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 90.20%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 90.02%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 89.98%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 90.03%   [EVAL] batch:   84 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:   85 | acc: 93.75%,  total acc: 90.04%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 90.16%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 90.20%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 90.42%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 90.38%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 90.49%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 90.59%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 90.72%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 90.82%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 91.01%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 91.10%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 90.78%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 90.50%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 90.17%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 89.84%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 89.82%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 89.56%   [EVAL] batch:  106 | acc: 68.75%,  total acc: 89.37%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 89.06%   [EVAL] batch:  108 | acc: 81.25%,  total acc: 88.99%   [EVAL] batch:  109 | acc: 68.75%,  total acc: 88.81%   [EVAL] batch:  110 | acc: 75.00%,  total acc: 88.68%   [EVAL] batch:  111 | acc: 68.75%,  total acc: 88.50%   [EVAL] batch:  112 | acc: 81.25%,  total acc: 88.44%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 88.43%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 88.53%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 88.47%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 88.46%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 88.51%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 88.45%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 87.97%   [EVAL] batch:  120 | acc: 56.25%,  total acc: 87.71%   [EVAL] batch:  121 | acc: 31.25%,  total acc: 87.24%   [EVAL] batch:  122 | acc: 50.00%,  total acc: 86.94%   [EVAL] batch:  123 | acc: 25.00%,  total acc: 86.44%   [EVAL] batch:  124 | acc: 50.00%,  total acc: 86.15%   
cur_acc:  ['0.9286', '0.7817']
his_acc:  ['0.9286', '0.8615']
Clustering into  14  clusters
Clusters:  [ 0  3 12  1  4  4  8  0  2  2  3  0 11  1  7 10  9  5 13  5  5  0  0  4
  6  2  0  4 10 10]
Losses:  11.420302391052246 3.756136894226074 0.5660158395767212
CurrentTrain: epoch  0, batch     0 | loss: 11.4203024Losses:  9.894927978515625 2.8189287185668945 0.6164716482162476
CurrentTrain: epoch  0, batch     1 | loss: 9.8949280Losses:  10.922393798828125 3.91241455078125 0.47322988510131836
CurrentTrain: epoch  0, batch     2 | loss: 10.9223938Losses:  7.643710136413574 -0.0 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 7.6437101Losses:  11.145663261413574 4.280912399291992 0.573058009147644
CurrentTrain: epoch  1, batch     0 | loss: 11.1456633Losses:  10.518831253051758 3.86629319190979 0.4544034004211426
CurrentTrain: epoch  1, batch     1 | loss: 10.5188313Losses:  9.444062232971191 3.710688591003418 0.5250083208084106
CurrentTrain: epoch  1, batch     2 | loss: 9.4440622Losses:  6.87294340133667 -0.0 0.13664768636226654
CurrentTrain: epoch  1, batch     3 | loss: 6.8729434Losses:  8.564421653747559 2.980299472808838 0.5487042665481567
CurrentTrain: epoch  2, batch     0 | loss: 8.5644217Losses:  9.237968444824219 3.3565573692321777 0.4750251770019531
CurrentTrain: epoch  2, batch     1 | loss: 9.2379684Losses:  8.71296215057373 3.2337446212768555 0.47241711616516113
CurrentTrain: epoch  2, batch     2 | loss: 8.7129622Losses:  3.415994882583618 -0.0 0.11604037135839462
CurrentTrain: epoch  2, batch     3 | loss: 3.4159949Losses:  7.817738056182861 3.034470796585083 0.48389577865600586
CurrentTrain: epoch  3, batch     0 | loss: 7.8177381Losses:  7.09390115737915 3.0459423065185547 0.4448297917842865
CurrentTrain: epoch  3, batch     1 | loss: 7.0939012Losses:  9.24516487121582 4.428811073303223 0.34993976354599
CurrentTrain: epoch  3, batch     2 | loss: 9.2451649Losses:  2.247502326965332 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 2.2475023Losses:  7.675146579742432 3.400712490081787 0.45629259943962097
CurrentTrain: epoch  4, batch     0 | loss: 7.6751466Losses:  8.704463005065918 4.770064353942871 0.5588148832321167
CurrentTrain: epoch  4, batch     1 | loss: 8.7044630Losses:  6.006911277770996 2.345864772796631 0.418381929397583
CurrentTrain: epoch  4, batch     2 | loss: 6.0069113Losses:  1.7756727933883667 -0.0 0.10532113164663315
CurrentTrain: epoch  4, batch     3 | loss: 1.7756728Losses:  5.853854179382324 2.337975025177002 0.54439377784729
CurrentTrain: epoch  5, batch     0 | loss: 5.8538542Losses:  5.911623477935791 2.1515769958496094 0.5127705335617065
CurrentTrain: epoch  5, batch     1 | loss: 5.9116235Losses:  5.717350959777832 2.644514560699463 0.42557278275489807
CurrentTrain: epoch  5, batch     2 | loss: 5.7173510Losses:  3.637070655822754 -0.0 0.09606120735406876
CurrentTrain: epoch  5, batch     3 | loss: 3.6370707Losses:  6.468640327453613 2.7078452110290527 0.5241974592208862
CurrentTrain: epoch  6, batch     0 | loss: 6.4686403Losses:  5.913691520690918 2.8900012969970703 0.4433557987213135
CurrentTrain: epoch  6, batch     1 | loss: 5.9136915Losses:  6.32851505279541 3.547222137451172 0.5018560290336609
CurrentTrain: epoch  6, batch     2 | loss: 6.3285151Losses:  4.18009090423584 -0.0 0.08658235520124435
CurrentTrain: epoch  6, batch     3 | loss: 4.1800909Losses:  7.355430603027344 4.539444923400879 0.41936570405960083
CurrentTrain: epoch  7, batch     0 | loss: 7.3554306Losses:  6.124207496643066 3.1212501525878906 0.5298681259155273
CurrentTrain: epoch  7, batch     1 | loss: 6.1242075Losses:  6.42002010345459 3.3640236854553223 0.438605397939682
CurrentTrain: epoch  7, batch     2 | loss: 6.4200201Losses:  1.9591716527938843 -0.0 0.10270508378744125
CurrentTrain: epoch  7, batch     3 | loss: 1.9591717Losses:  5.507509708404541 2.7219936847686768 0.41701456904411316
CurrentTrain: epoch  8, batch     0 | loss: 5.5075097Losses:  5.956815719604492 3.3982577323913574 0.47476768493652344
CurrentTrain: epoch  8, batch     1 | loss: 5.9568157Losses:  4.947190284729004 2.1163387298583984 0.48391032218933105
CurrentTrain: epoch  8, batch     2 | loss: 4.9471903Losses:  4.094985485076904 -0.0 0.24816659092903137
CurrentTrain: epoch  8, batch     3 | loss: 4.0949855Losses:  5.291388034820557 2.5794639587402344 0.48016783595085144
CurrentTrain: epoch  9, batch     0 | loss: 5.2913880Losses:  4.88336181640625 2.4175705909729004 0.48143690824508667
CurrentTrain: epoch  9, batch     1 | loss: 4.8833618Losses:  5.787034034729004 3.096888780593872 0.4924563765525818
CurrentTrain: epoch  9, batch     2 | loss: 5.7870340Losses:  3.827357053756714 -0.0 0.2720668613910675
CurrentTrain: epoch  9, batch     3 | loss: 3.8273571
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elena is a renowned artist who often collaborates with her husband, mark, a famous photographer known for his stunning landscapes.  
Head Entity: mark  
Tail Entity: elena  

Relation: spouse  
Context: during the award ceremony, it was revealed that the director, james, had been married to his long-time partner, lucy, for over a decade.  
Head Entity: james  
Tail Entity: lucy  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: it was first released on a single in the uk by parlophone in september 1967 , and on the group 's self - titled album " tomorrow " in february 1968 .
Head Entity: tomorrow
Tail Entity: parlophone
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The band's debut album was launched under the iconic label Atlantic Records, which has been home to many legendary artists.  
Head Entity: debut album  
Tail Entity: Atlantic Records  

Relation: record label  
Context: After signing with Universal Music, the artist released their latest single, which quickly climbed the charts.  
Head Entity: latest single  
Tail Entity: Universal Music  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: one account notes , alston and his son , peter also , practiced their counterfeiting operation , at stack island , in the lower mississippi river , about 170 miles upriver from natchez .
Head Entity: stack island
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the picturesque village of portsmouth is situated along the banks of the serene river thames, providing stunning views and a tranquil atmosphere for its residents.  
Head Entity: portsmouth  
Tail Entity: river thames  

Relation: located in or next to body of water  
Context: during our trip, we discovered that the charming town of venice is built on a series of islands surrounded by the beautiful adriatic sea, making it a unique destination for tourists.  
Head Entity: venice  
Tail Entity: adriatic sea  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, Zappos, would continue to operate independently.  
Head Entity: Zappos  
Tail Entity: Amazon  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: he challenged seven other swimmers in heat four including ukraine 's rostyslav svanidze , a top 16 finalist in atlanta four years earlier .
Head Entity: rostyslav svanidze
Tail Entity: ukraine
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: after years of living in the united states, the renowned artist decided to apply for dual citizenship, allowing him to retain his ties to his homeland, italy.  
Head Entity: the renowned artist  
Tail Entity: italy  

Relation: country of citizenship  
Context: during the international conference, the speaker proudly mentioned that she had recently obtained her citizenship in canada, where she had lived for over a decade.  
Head Entity: the speaker  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: he is also a part of the atlanta - based hip - hop / r&b;/soul musical collective the dungeon family .
Head Entity: the dungeon family
Tail Entity: hip - hop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the band is known for their unique blend of rock and electronic music, often categorized under the genre of synth-pop.  
Head Entity: the band  
Tail Entity: synth-pop  

Relation: genre  
Context: she has made significant contributions to the world of classical music, particularly in the genre of opera.  
Head Entity: she  
Tail Entity: opera  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, ultimately empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: benjamin van leer ( born 9 april 1992 in houten ) is a dutch professional footballer who currently plays as a goalkeeper for ajax in the dutch eredivisie .
Head Entity: benjamin van leer
Tail Entity: goalkeeper
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: lebron james, born on december 30, 1984, is an american professional basketball player who currently plays as a forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: serena williams, born on september 26, 1981, is an american professional tennis player who is known for her powerful serve and currently competes in singles and doubles events.  
Head Entity: serena williams  
Tail Entity: tennis player  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
Losses:  5.161467552185059 0.3042612075805664 0.8178386688232422
MemoryTrain:  epoch  0, batch     0 | loss: 5.1614676Losses:  4.373483180999756 1.126855492591858 0.8248656392097473
MemoryTrain:  epoch  0, batch     1 | loss: 4.3734832Losses:  5.681802749633789 0.8951254487037659 0.5755563974380493
MemoryTrain:  epoch  0, batch     2 | loss: 5.6818027Losses:  5.229999542236328 0.7152323722839355 0.7117164731025696
MemoryTrain:  epoch  0, batch     3 | loss: 5.2299995Losses:  5.99997091293335 0.38977059721946716 0.8315271735191345
MemoryTrain:  epoch  0, batch     4 | loss: 5.9999709Losses:  6.055142402648926 0.2205728143453598 0.5279159545898438
MemoryTrain:  epoch  0, batch     5 | loss: 6.0551424Losses:  5.5666632652282715 0.8074766397476196 0.7100806832313538
MemoryTrain:  epoch  1, batch     0 | loss: 5.5666633Losses:  6.085938930511475 1.0946825742721558 0.7461633086204529
MemoryTrain:  epoch  1, batch     1 | loss: 6.0859389Losses:  4.354001522064209 0.2243523895740509 0.748835563659668
MemoryTrain:  epoch  1, batch     2 | loss: 4.3540015Losses:  4.396173000335693 0.2338566780090332 0.8436107635498047
MemoryTrain:  epoch  1, batch     3 | loss: 4.3961730Losses:  5.617965221405029 0.8151066899299622 0.6006907224655151
MemoryTrain:  epoch  1, batch     4 | loss: 5.6179652Losses:  3.104112148284912 -0.0 0.7602643966674805
MemoryTrain:  epoch  1, batch     5 | loss: 3.1041121Losses:  4.9351348876953125 1.3002548217773438 0.5633821487426758
MemoryTrain:  epoch  2, batch     0 | loss: 4.9351349Losses:  4.769316673278809 0.7174276113510132 0.6864891648292542
MemoryTrain:  epoch  2, batch     1 | loss: 4.7693167Losses:  4.050729751586914 0.7919167280197144 0.8231755495071411
MemoryTrain:  epoch  2, batch     2 | loss: 4.0507298Losses:  4.8461527824401855 0.7693791389465332 0.6496025323867798
MemoryTrain:  epoch  2, batch     3 | loss: 4.8461528Losses:  4.212348937988281 0.7667461037635803 0.5367821455001831
MemoryTrain:  epoch  2, batch     4 | loss: 4.2123489Losses:  3.2732746601104736 0.3237800598144531 0.7376512885093689
MemoryTrain:  epoch  2, batch     5 | loss: 3.2732747Losses:  4.6007890701293945 0.984689474105835 0.7551961541175842
MemoryTrain:  epoch  3, batch     0 | loss: 4.6007891Losses:  3.853656053543091 0.5229254961013794 0.7868369221687317
MemoryTrain:  epoch  3, batch     1 | loss: 3.8536561Losses:  4.4429426193237305 1.2177302837371826 0.7455298900604248
MemoryTrain:  epoch  3, batch     2 | loss: 4.4429426Losses:  4.6525068283081055 0.2594144344329834 0.8195276260375977
MemoryTrain:  epoch  3, batch     3 | loss: 4.6525068Losses:  3.6935296058654785 0.7428072690963745 0.753209114074707
MemoryTrain:  epoch  3, batch     4 | loss: 3.6935296Losses:  3.039325714111328 -0.0 0.6306508779525757
MemoryTrain:  epoch  3, batch     5 | loss: 3.0393257Losses:  4.559779167175293 0.7215462923049927 0.6502716541290283
MemoryTrain:  epoch  4, batch     0 | loss: 4.5597792Losses:  3.948246955871582 0.8172565698623657 0.851633608341217
MemoryTrain:  epoch  4, batch     1 | loss: 3.9482470Losses:  3.6000239849090576 0.5513237714767456 0.674187421798706
MemoryTrain:  epoch  4, batch     2 | loss: 3.6000240Losses:  3.8908638954162598 0.8340232968330383 0.6747148633003235
MemoryTrain:  epoch  4, batch     3 | loss: 3.8908639Losses:  3.6800179481506348 0.6059877276420593 0.8502780199050903
MemoryTrain:  epoch  4, batch     4 | loss: 3.6800179Losses:  3.3155016899108887 0.3149805963039398 0.5806934833526611
MemoryTrain:  epoch  4, batch     5 | loss: 3.3155017Losses:  4.081486701965332 0.5363027453422546 0.6886098384857178
MemoryTrain:  epoch  5, batch     0 | loss: 4.0814867Losses:  3.30810546875 0.45106950402259827 0.8367268443107605
MemoryTrain:  epoch  5, batch     1 | loss: 3.3081055Losses:  4.082350254058838 1.0974189043045044 0.7580378651618958
MemoryTrain:  epoch  5, batch     2 | loss: 4.0823503Losses:  3.125892162322998 0.2601473331451416 0.8143162727355957
MemoryTrain:  epoch  5, batch     3 | loss: 3.1258922Losses:  3.9580917358398438 1.1313254833221436 0.7251291275024414
MemoryTrain:  epoch  5, batch     4 | loss: 3.9580917Losses:  3.2553160190582275 -0.0 0.6782469749450684
MemoryTrain:  epoch  5, batch     5 | loss: 3.2553160Losses:  3.4242568016052246 -0.0 0.7909864783287048
MemoryTrain:  epoch  6, batch     0 | loss: 3.4242568Losses:  4.285767078399658 0.8127421736717224 0.8579444289207458
MemoryTrain:  epoch  6, batch     1 | loss: 4.2857671Losses:  2.3929333686828613 0.2726074457168579 0.6942757368087769
MemoryTrain:  epoch  6, batch     2 | loss: 2.3929334Losses:  3.2081871032714844 0.2925390601158142 0.7354786992073059
MemoryTrain:  epoch  6, batch     3 | loss: 3.2081871Losses:  2.6356167793273926 0.23644332587718964 0.7898811101913452
MemoryTrain:  epoch  6, batch     4 | loss: 2.6356168Losses:  3.7040953636169434 1.2501415014266968 0.6191446185112
MemoryTrain:  epoch  6, batch     5 | loss: 3.7040954Losses:  3.5991055965423584 1.212689995765686 0.742447555065155
MemoryTrain:  epoch  7, batch     0 | loss: 3.5991056Losses:  3.14216947555542 -0.0 0.8635696172714233
MemoryTrain:  epoch  7, batch     1 | loss: 3.1421695Losses:  3.4045307636260986 0.5409893989562988 0.7267553806304932
MemoryTrain:  epoch  7, batch     2 | loss: 3.4045308Losses:  3.235332727432251 0.7287867069244385 0.6048349738121033
MemoryTrain:  epoch  7, batch     3 | loss: 3.2353327Losses:  2.9372105598449707 0.29198652505874634 0.8619885444641113
MemoryTrain:  epoch  7, batch     4 | loss: 2.9372106Losses:  2.687288999557495 0.25753575563430786 0.6110700964927673
MemoryTrain:  epoch  7, batch     5 | loss: 2.6872890Losses:  2.785578727722168 0.555525541305542 0.714461088180542
MemoryTrain:  epoch  8, batch     0 | loss: 2.7855787Losses:  2.690154552459717 -0.0 0.9177520275115967
MemoryTrain:  epoch  8, batch     1 | loss: 2.6901546Losses:  3.908205032348633 0.9845701456069946 0.5372611284255981
MemoryTrain:  epoch  8, batch     2 | loss: 3.9082050Losses:  2.973243236541748 0.4649869203567505 0.6609152555465698
MemoryTrain:  epoch  8, batch     3 | loss: 2.9732432Losses:  2.982656717300415 0.6015292406082153 0.7988107800483704
MemoryTrain:  epoch  8, batch     4 | loss: 2.9826567Losses:  2.6020548343658447 0.6026068925857544 0.6269658207893372
MemoryTrain:  epoch  8, batch     5 | loss: 2.6020548Losses:  2.8647611141204834 0.815998375415802 0.7683044075965881
MemoryTrain:  epoch  9, batch     0 | loss: 2.8647611Losses:  3.065279006958008 0.542426347732544 0.6895708441734314
MemoryTrain:  epoch  9, batch     1 | loss: 3.0652790Losses:  3.629081964492798 1.6680543422698975 0.593182384967804
MemoryTrain:  epoch  9, batch     2 | loss: 3.6290820Losses:  3.3169965744018555 0.8827201128005981 0.7258048057556152
MemoryTrain:  epoch  9, batch     3 | loss: 3.3169966Losses:  3.6003928184509277 0.53311687707901 0.804036557674408
MemoryTrain:  epoch  9, batch     4 | loss: 3.6003928Losses:  2.904761552810669 0.4000469148159027 0.5874907374382019
MemoryTrain:  epoch  9, batch     5 | loss: 2.9047616
[EVAL] batch:    0 | acc: 100.00%,  total acc: 100.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 92.86%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 88.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 86.54%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 82.92%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 82.72%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 81.88%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 81.85%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 82.39%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 82.61%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 82.55%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 83.00%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 82.69%   [EVAL] batch:   26 | acc: 68.75%,  total acc: 82.18%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 81.25%,  total acc: 81.68%   [EVAL] batch:   29 | acc: 43.75%,  total acc: 80.42%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 79.44%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 79.10%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 78.79%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 78.49%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 78.04%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 77.78%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 77.36%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 76.97%   [EVAL] batch:   38 | acc: 56.25%,  total acc: 76.44%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 76.09%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 75.91%   [EVAL] batch:   41 | acc: 56.25%,  total acc: 75.45%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 75.44%   [EVAL] batch:   43 | acc: 37.50%,  total acc: 74.57%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 73.33%   [EVAL] batch:   45 | acc: 43.75%,  total acc: 72.69%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 72.07%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 71.35%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 70.54%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 70.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 71.70%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 72.11%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 72.61%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 73.10%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 73.57%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 74.03%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 74.26%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 74.58%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 75.30%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 74.90%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 86.98%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 87.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 88.67%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 88.97%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 89.80%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 90.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 90.77%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 90.34%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 90.22%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 90.36%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 90.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 91.29%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 91.38%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 92.42%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 91.54%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 91.07%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 90.80%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 90.71%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 90.79%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.03%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 91.31%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 91.42%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 91.53%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.71%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 91.49%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.41%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.58%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 91.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.54%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.78%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.59%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.63%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 91.38%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 91.53%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 91.56%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 91.70%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 91.63%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 91.57%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 91.41%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 91.15%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 91.00%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 90.76%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 90.72%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 90.49%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 90.45%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 90.23%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 89.93%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 89.55%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 89.10%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 89.08%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 88.82%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 88.80%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 88.70%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 88.69%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 88.52%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 88.66%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 88.34%   [EVAL] batch:   82 | acc: 81.25%,  total acc: 88.25%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 88.32%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 88.24%   [EVAL] batch:   85 | acc: 93.75%,  total acc: 88.30%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 88.29%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 88.28%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 88.41%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 88.53%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 88.65%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 88.83%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 88.95%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 89.18%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 89.29%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 89.39%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 89.50%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 89.11%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 88.85%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 88.59%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 88.34%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 88.33%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 88.09%   [EVAL] batch:  106 | acc: 62.50%,  total acc: 87.85%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 87.27%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 86.98%   [EVAL] batch:  109 | acc: 31.25%,  total acc: 86.48%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 86.26%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 85.77%   [EVAL] batch:  112 | acc: 75.00%,  total acc: 85.67%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 85.69%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 85.78%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 85.79%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 85.81%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 85.77%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 85.31%   [EVAL] batch:  120 | acc: 43.75%,  total acc: 84.97%   [EVAL] batch:  121 | acc: 18.75%,  total acc: 84.43%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 84.10%   [EVAL] batch:  123 | acc: 25.00%,  total acc: 83.62%   [EVAL] batch:  124 | acc: 37.50%,  total acc: 83.25%   [EVAL] batch:  125 | acc: 100.00%,  total acc: 83.38%   [EVAL] batch:  126 | acc: 87.50%,  total acc: 83.42%   [EVAL] batch:  127 | acc: 93.75%,  total acc: 83.50%   [EVAL] batch:  128 | acc: 81.25%,  total acc: 83.48%   [EVAL] batch:  129 | acc: 87.50%,  total acc: 83.51%   [EVAL] batch:  130 | acc: 100.00%,  total acc: 83.64%   [EVAL] batch:  131 | acc: 100.00%,  total acc: 83.76%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 83.79%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 83.77%   [EVAL] batch:  134 | acc: 62.50%,  total acc: 83.61%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 83.59%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 83.62%   [EVAL] batch:  137 | acc: 75.00%,  total acc: 83.56%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 83.36%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 83.21%   [EVAL] batch:  140 | acc: 87.50%,  total acc: 83.24%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 83.19%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 83.04%   [EVAL] batch:  143 | acc: 87.50%,  total acc: 83.07%   [EVAL] batch:  144 | acc: 81.25%,  total acc: 83.06%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 83.05%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 83.12%   [EVAL] batch:  147 | acc: 87.50%,  total acc: 83.15%   [EVAL] batch:  148 | acc: 81.25%,  total acc: 83.14%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 83.21%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 83.15%   [EVAL] batch:  151 | acc: 68.75%,  total acc: 83.06%   [EVAL] batch:  152 | acc: 68.75%,  total acc: 82.97%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 82.95%   [EVAL] batch:  154 | acc: 43.75%,  total acc: 82.70%   [EVAL] batch:  155 | acc: 50.00%,  total acc: 82.49%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 82.40%   [EVAL] batch:  157 | acc: 68.75%,  total acc: 82.32%   [EVAL] batch:  158 | acc: 68.75%,  total acc: 82.23%   [EVAL] batch:  159 | acc: 62.50%,  total acc: 82.11%   [EVAL] batch:  160 | acc: 68.75%,  total acc: 82.03%   [EVAL] batch:  161 | acc: 62.50%,  total acc: 81.91%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 81.79%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 81.63%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 81.52%   [EVAL] batch:  165 | acc: 68.75%,  total acc: 81.44%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 81.29%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:  168 | acc: 37.50%,  total acc: 80.99%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 80.62%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 80.41%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 80.20%   [EVAL] batch:  172 | acc: 37.50%,  total acc: 79.95%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 79.67%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 79.46%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 79.58%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 79.70%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:  178 | acc: 93.75%,  total acc: 79.89%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 80.11%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 80.22%   [EVAL] batch:  182 | acc: 100.00%,  total acc: 80.33%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 80.37%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 80.44%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 80.54%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 80.61%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 80.45%   
cur_acc:  ['0.9286', '0.7817', '0.7490']
his_acc:  ['0.9286', '0.8615', '0.8045']
Clustering into  19  clusters
Clusters:  [ 3  1 13 18  6  6  9  2  4  4  1  3 14 10 15  7 12  0 16  0  0  2  2  6
 11  4  3  6  7  7 17  7  0  8  5  7  0  3  4  3]
Losses:  12.404516220092773 5.428022384643555 0.6362029314041138
CurrentTrain: epoch  0, batch     0 | loss: 12.4045162Losses:  11.05117416381836 4.384366989135742 0.7161648869514465
CurrentTrain: epoch  0, batch     1 | loss: 11.0511742Losses:  11.079889297485352 3.9455502033233643 0.4888487458229065
CurrentTrain: epoch  0, batch     2 | loss: 11.0798893Losses:  8.618993759155273 -0.0 0.0924372524023056
CurrentTrain: epoch  0, batch     3 | loss: 8.6189938Losses:  12.948534965515137 6.429154396057129 0.5702415704727173
CurrentTrain: epoch  1, batch     0 | loss: 12.9485350Losses:  9.685246467590332 3.364962100982666 0.6688300967216492
CurrentTrain: epoch  1, batch     1 | loss: 9.6852465Losses:  9.927599906921387 4.221844673156738 0.5972034335136414
CurrentTrain: epoch  1, batch     2 | loss: 9.9275999Losses:  4.356103897094727 -0.0 0.16455036401748657
CurrentTrain: epoch  1, batch     3 | loss: 4.3561039Losses:  8.582014083862305 2.6333818435668945 0.5902698040008545
CurrentTrain: epoch  2, batch     0 | loss: 8.5820141Losses:  7.730974197387695 2.8349246978759766 0.6577499508857727
CurrentTrain: epoch  2, batch     1 | loss: 7.7309742Losses:  9.004745483398438 4.066743850708008 0.5699784159660339
CurrentTrain: epoch  2, batch     2 | loss: 9.0047455Losses:  3.705861806869507 -0.0 0.09798357635736465
CurrentTrain: epoch  2, batch     3 | loss: 3.7058618Losses:  7.94287633895874 3.0264744758605957 0.6246946454048157
CurrentTrain: epoch  3, batch     0 | loss: 7.9428763Losses:  7.708101749420166 2.85976243019104 0.6257479786872864
CurrentTrain: epoch  3, batch     1 | loss: 7.7081017Losses:  7.529307842254639 2.5088729858398438 0.6558432579040527
CurrentTrain: epoch  3, batch     2 | loss: 7.5293078Losses:  4.821737289428711 -0.0 0.12461291998624802
CurrentTrain: epoch  3, batch     3 | loss: 4.8217373Losses:  9.014095306396484 4.120713233947754 0.637738823890686
CurrentTrain: epoch  4, batch     0 | loss: 9.0140953Losses:  7.716954231262207 3.18649959564209 0.6238280534744263
CurrentTrain: epoch  4, batch     1 | loss: 7.7169542Losses:  8.386198997497559 4.527215480804443 0.5747437477111816
CurrentTrain: epoch  4, batch     2 | loss: 8.3861990Losses:  2.3078277111053467 -0.0 0.09746091812849045
CurrentTrain: epoch  4, batch     3 | loss: 2.3078277Losses:  7.515504837036133 3.9441511631011963 0.4673187732696533
CurrentTrain: epoch  5, batch     0 | loss: 7.5155048Losses:  8.12730884552002 4.0907769203186035 0.5545946955680847
CurrentTrain: epoch  5, batch     1 | loss: 8.1273088Losses:  10.360769271850586 5.752157211303711 0.4313288629055023
CurrentTrain: epoch  5, batch     2 | loss: 10.3607693Losses:  2.6773204803466797 -0.0 0.1333668828010559
CurrentTrain: epoch  5, batch     3 | loss: 2.6773205Losses:  7.300207138061523 3.6799497604370117 0.6333467364311218
CurrentTrain: epoch  6, batch     0 | loss: 7.3002071Losses:  7.760904312133789 3.8039803504943848 0.626112163066864
CurrentTrain: epoch  6, batch     1 | loss: 7.7609043Losses:  7.17454195022583 2.9607601165771484 0.5138530731201172
CurrentTrain: epoch  6, batch     2 | loss: 7.1745420Losses:  2.6543920040130615 -0.0 0.12327750027179718
CurrentTrain: epoch  6, batch     3 | loss: 2.6543920Losses:  7.231133460998535 3.7137808799743652 0.5204600691795349
CurrentTrain: epoch  7, batch     0 | loss: 7.2311335Losses:  7.342413425445557 3.924346923828125 0.530561089515686
CurrentTrain: epoch  7, batch     1 | loss: 7.3424134Losses:  7.313580513000488 3.345870018005371 0.5931929349899292
CurrentTrain: epoch  7, batch     2 | loss: 7.3135805Losses:  1.8901017904281616 -0.0 0.13174283504486084
CurrentTrain: epoch  7, batch     3 | loss: 1.8901018Losses:  6.62148904800415 3.298506259918213 0.6126922369003296
CurrentTrain: epoch  8, batch     0 | loss: 6.6214890Losses:  6.124937057495117 2.623835563659668 0.598328709602356
CurrentTrain: epoch  8, batch     1 | loss: 6.1249371Losses:  6.183143615722656 2.6671078205108643 0.5966137647628784
CurrentTrain: epoch  8, batch     2 | loss: 6.1831436Losses:  2.030733108520508 -0.0 0.15391892194747925
CurrentTrain: epoch  8, batch     3 | loss: 2.0307331Losses:  6.5952911376953125 3.4876160621643066 0.5131409168243408
CurrentTrain: epoch  9, batch     0 | loss: 6.5952911Losses:  5.438015937805176 2.125953435897827 0.5899941921234131
CurrentTrain: epoch  9, batch     1 | loss: 5.4380159Losses:  6.082814693450928 3.0599350929260254 0.5978663563728333
CurrentTrain: epoch  9, batch     2 | loss: 6.0828147Losses:  2.0044610500335693 -0.0 0.08780339360237122
CurrentTrain: epoch  9, batch     3 | loss: 2.0044611
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: She has been a professional player in tennis since she was a teenager and has won several championships.  
Head Entity: She  
Tail Entity: tennis  

Relation: sport  
Context: The city is known for its vibrant soccer culture, with numerous teams competing in various leagues.  
Head Entity: The city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael is the father of emily, who has recently graduated from university.  
Head Entity: emily  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon when it premiered on NBC, showcasing the lives of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: that same year saran made a special appearance in a scene in the kannada film " arasu " .
Head Entity: arasu
Tail Entity: kannada
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The critically acclaimed movie "Parasite" was primarily filmed in Korean and received international recognition.  
Head Entity: Parasite  
Tail Entity: Korean  

Relation: original language of film or TV show  
Context: The animated series "Doraemon" is originally produced in Japanese and has been dubbed in several languages worldwide.  
Head Entity: Doraemon  
Tail Entity: Japanese  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: other teammates included dominique leray and élisabeth riffiod , whose son boris diaw currently plays in the nba for the spurs ( 2014 ) .
Head Entity: boris diaw
Tail Entity: élisabeth riffiod
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in a recent interview, actress jennifer garner spoke fondly of her children, including violet, who is following in her mother's footsteps in the entertainment industry.  
Head Entity: violet  
Tail Entity: jennifer garner  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much emily looked like her mother, sarah, especially when they both laughed at the same joke.  
Head Entity: emily  
Tail Entity: sarah  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he has dedicated his life to music and is known for his mastery of the cello.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: ploegsteert is a village in belgium located in the municipality of comines - warneton in the hainaut province and is the most westerly settlement of the walloon region .
Head Entity: hainaut
Tail Entity: belgium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the city of prague is the capital of the czech republic and is known for its beautiful architecture and rich history.  
Head Entity: prague  
Tail Entity: czech republic  

Relation: country  
Context: the great barrier reef is located off the coast of australia and is the largest coral reef system in the world.  
Head Entity: great barrier reef  
Tail Entity: australia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Losses:  4.5926432609558105 1.029231071472168 0.6599422097206116
MemoryTrain:  epoch  0, batch     0 | loss: 4.5926433Losses:  3.60469913482666 0.2327064573764801 0.7545188665390015
MemoryTrain:  epoch  0, batch     1 | loss: 3.6046991Losses:  3.6813154220581055 0.21947665512561798 0.7110534906387329
MemoryTrain:  epoch  0, batch     2 | loss: 3.6813154Losses:  4.984396457672119 1.0688031911849976 0.8866645097732544
MemoryTrain:  epoch  0, batch     3 | loss: 4.9843965Losses:  4.381064414978027 0.5193082094192505 0.9213261604309082
MemoryTrain:  epoch  0, batch     4 | loss: 4.3810644Losses:  4.520018577575684 0.5080887079238892 0.6449711322784424
MemoryTrain:  epoch  0, batch     5 | loss: 4.5200186Losses:  4.771456241607666 0.37792056798934937 0.7899834513664246
MemoryTrain:  epoch  0, batch     6 | loss: 4.7714562Losses:  3.6732239723205566 -0.0 0.6442684531211853
MemoryTrain:  epoch  0, batch     7 | loss: 3.6732240Losses:  3.5821359157562256 0.227597177028656 0.7769262790679932
MemoryTrain:  epoch  1, batch     0 | loss: 3.5821359Losses:  5.662600517272949 1.3514361381530762 0.8017083406448364
MemoryTrain:  epoch  1, batch     1 | loss: 5.6626005Losses:  3.283763885498047 0.23158764839172363 0.8691555857658386
MemoryTrain:  epoch  1, batch     2 | loss: 3.2837639Losses:  3.856654167175293 0.8378471732139587 0.9406113624572754
MemoryTrain:  epoch  1, batch     3 | loss: 3.8566542Losses:  5.124787330627441 1.3579468727111816 0.7447336912155151
MemoryTrain:  epoch  1, batch     4 | loss: 5.1247873Losses:  3.818887948989868 -0.0 0.7241600155830383
MemoryTrain:  epoch  1, batch     5 | loss: 3.8188879Losses:  3.700605869293213 0.5612705945968628 0.6524535417556763
MemoryTrain:  epoch  1, batch     6 | loss: 3.7006059Losses:  3.6678342819213867 0.7224493622779846 0.35697847604751587
MemoryTrain:  epoch  1, batch     7 | loss: 3.6678343Losses:  3.2973568439483643 0.31149566173553467 0.8412206768989563
MemoryTrain:  epoch  2, batch     0 | loss: 3.2973568Losses:  3.1846773624420166 0.2746093273162842 0.8232401013374329
MemoryTrain:  epoch  2, batch     1 | loss: 3.1846774Losses:  3.822308301925659 0.44753336906433105 0.7892321944236755
MemoryTrain:  epoch  2, batch     2 | loss: 3.8223083Losses:  3.3813910484313965 0.2676266133785248 0.9189626574516296
MemoryTrain:  epoch  2, batch     3 | loss: 3.3813910Losses:  3.8371164798736572 0.557589054107666 0.79825758934021
MemoryTrain:  epoch  2, batch     4 | loss: 3.8371165Losses:  3.9759488105773926 0.7638882398605347 0.7260159850120544
MemoryTrain:  epoch  2, batch     5 | loss: 3.9759488Losses:  3.185948610305786 0.467461496591568 0.7346863150596619
MemoryTrain:  epoch  2, batch     6 | loss: 3.1859486Losses:  3.2633800506591797 -0.0 0.6276248693466187
MemoryTrain:  epoch  2, batch     7 | loss: 3.2633801Losses:  3.3070504665374756 0.7919379472732544 0.8391777873039246
MemoryTrain:  epoch  3, batch     0 | loss: 3.3070505Losses:  2.752990484237671 -0.0 0.7470546364784241
MemoryTrain:  epoch  3, batch     1 | loss: 2.7529905Losses:  3.9994325637817383 1.1532429456710815 0.7885079383850098
MemoryTrain:  epoch  3, batch     2 | loss: 3.9994326Losses:  3.224720001220703 0.5573698878288269 0.5852261781692505
MemoryTrain:  epoch  3, batch     3 | loss: 3.2247200Losses:  2.644965410232544 -0.0 0.9120805263519287
MemoryTrain:  epoch  3, batch     4 | loss: 2.6449654Losses:  2.966582775115967 0.22315970063209534 0.67840576171875
MemoryTrain:  epoch  3, batch     5 | loss: 2.9665828Losses:  3.204312324523926 0.7974485754966736 0.8035371899604797
MemoryTrain:  epoch  3, batch     6 | loss: 3.2043123Losses:  2.60860013961792 -0.0 0.6229408979415894
MemoryTrain:  epoch  3, batch     7 | loss: 2.6086001Losses:  3.0648868083953857 0.5157616138458252 0.830144464969635
MemoryTrain:  epoch  4, batch     0 | loss: 3.0648868Losses:  3.0620474815368652 0.29492026567459106 0.8997880220413208
MemoryTrain:  epoch  4, batch     1 | loss: 3.0620475Losses:  2.857879638671875 0.2513136863708496 0.7537704110145569
MemoryTrain:  epoch  4, batch     2 | loss: 2.8578796Losses:  2.424481153488159 0.21547290682792664 0.7451202273368835
MemoryTrain:  epoch  4, batch     3 | loss: 2.4244812Losses:  3.435356855392456 0.2878889739513397 0.7825750708580017
MemoryTrain:  epoch  4, batch     4 | loss: 3.4353569Losses:  3.2546796798706055 0.5234829187393188 0.8085390329360962
MemoryTrain:  epoch  4, batch     5 | loss: 3.2546797Losses:  3.0667331218719482 0.7788592576980591 0.7932965159416199
MemoryTrain:  epoch  4, batch     6 | loss: 3.0667331Losses:  2.8383352756500244 0.25884705781936646 0.520700216293335
MemoryTrain:  epoch  4, batch     7 | loss: 2.8383353Losses:  2.942317247390747 0.4770864248275757 0.798388659954071
MemoryTrain:  epoch  5, batch     0 | loss: 2.9423172Losses:  2.5711312294006348 0.44145503640174866 0.7788869738578796
MemoryTrain:  epoch  5, batch     1 | loss: 2.5711312Losses:  3.7460386753082275 1.1349915266036987 0.8444378972053528
MemoryTrain:  epoch  5, batch     2 | loss: 3.7460387Losses:  2.891232490539551 0.5172405242919922 0.8166872262954712
MemoryTrain:  epoch  5, batch     3 | loss: 2.8912325Losses:  2.8775413036346436 0.24479511380195618 0.7751666903495789
MemoryTrain:  epoch  5, batch     4 | loss: 2.8775413Losses:  2.9423604011535645 0.4798243045806885 0.7648810744285583
MemoryTrain:  epoch  5, batch     5 | loss: 2.9423604Losses:  3.564497947692871 1.2179595232009888 0.7203243970870972
MemoryTrain:  epoch  5, batch     6 | loss: 3.5644979Losses:  2.242943286895752 -0.0 0.4062168300151825
MemoryTrain:  epoch  5, batch     7 | loss: 2.2429433Losses:  4.133714199066162 1.7696093320846558 0.6948824524879456
MemoryTrain:  epoch  6, batch     0 | loss: 4.1337142Losses:  2.7007524967193604 0.5012046098709106 0.8244860768318176
MemoryTrain:  epoch  6, batch     1 | loss: 2.7007525Losses:  3.0708372592926025 0.46186649799346924 0.7898025512695312
MemoryTrain:  epoch  6, batch     2 | loss: 3.0708373Losses:  2.4962735176086426 0.22942116856575012 0.8690234422683716
MemoryTrain:  epoch  6, batch     3 | loss: 2.4962735Losses:  2.9438235759735107 0.32536569237709045 0.8038780093193054
MemoryTrain:  epoch  6, batch     4 | loss: 2.9438236Losses:  2.38466739654541 0.2332228720188141 0.8581796288490295
MemoryTrain:  epoch  6, batch     5 | loss: 2.3846674Losses:  2.7351694107055664 0.5276131629943848 0.7399883270263672
MemoryTrain:  epoch  6, batch     6 | loss: 2.7351694Losses:  2.0407371520996094 -0.0 0.5946352481842041
MemoryTrain:  epoch  6, batch     7 | loss: 2.0407372Losses:  2.966261386871338 0.7700444459915161 0.7557224631309509
MemoryTrain:  epoch  7, batch     0 | loss: 2.9662614Losses:  2.751711845397949 0.4598109722137451 0.7728024125099182
MemoryTrain:  epoch  7, batch     1 | loss: 2.7517118Losses:  3.1608822345733643 0.7862774133682251 0.8327188491821289
MemoryTrain:  epoch  7, batch     2 | loss: 3.1608822Losses:  2.516104221343994 0.25629162788391113 0.6599843502044678
MemoryTrain:  epoch  7, batch     3 | loss: 2.5161042Losses:  2.479647159576416 0.27792248129844666 0.8591232299804688
MemoryTrain:  epoch  7, batch     4 | loss: 2.4796472Losses:  2.4324145317077637 0.2685697078704834 0.8148781657218933
MemoryTrain:  epoch  7, batch     5 | loss: 2.4324145Losses:  2.5556843280792236 0.23028314113616943 0.9096759557723999
MemoryTrain:  epoch  7, batch     6 | loss: 2.5556843Losses:  2.102076530456543 -0.0 0.5289766788482666
MemoryTrain:  epoch  7, batch     7 | loss: 2.1020765Losses:  2.673877239227295 0.7710877656936646 0.40820780396461487
MemoryTrain:  epoch  8, batch     0 | loss: 2.6738772Losses:  2.4729321002960205 0.20840013027191162 0.8408104777336121
MemoryTrain:  epoch  8, batch     1 | loss: 2.4729321Losses:  3.0211381912231445 0.7732011079788208 0.8586684465408325
MemoryTrain:  epoch  8, batch     2 | loss: 3.0211382Losses:  2.310112237930298 -0.0 0.9005858898162842
MemoryTrain:  epoch  8, batch     3 | loss: 2.3101122Losses:  2.3630242347717285 0.21859943866729736 0.8360404968261719
MemoryTrain:  epoch  8, batch     4 | loss: 2.3630242Losses:  2.837615489959717 0.7642258405685425 0.7197409868240356
MemoryTrain:  epoch  8, batch     5 | loss: 2.8376155Losses:  2.464258909225464 0.28128063678741455 0.786651074886322
MemoryTrain:  epoch  8, batch     6 | loss: 2.4642589Losses:  1.825294852256775 -0.0 0.4861178398132324
MemoryTrain:  epoch  8, batch     7 | loss: 1.8252949Losses:  2.445920467376709 0.2234400510787964 0.7858371138572693
MemoryTrain:  epoch  9, batch     0 | loss: 2.4459205Losses:  2.864860773086548 0.77924644947052 0.7652380466461182
MemoryTrain:  epoch  9, batch     1 | loss: 2.8648608Losses:  2.467139720916748 0.26763254404067993 0.9059111475944519
MemoryTrain:  epoch  9, batch     2 | loss: 2.4671397Losses:  2.7253258228302 0.6857750415802002 0.7573432326316833
MemoryTrain:  epoch  9, batch     3 | loss: 2.7253258Losses:  2.900749921798706 0.8156092166900635 0.6507415175437927
MemoryTrain:  epoch  9, batch     4 | loss: 2.9007499Losses:  2.647352695465088 0.48065662384033203 0.9040975570678711
MemoryTrain:  epoch  9, batch     5 | loss: 2.6473527Losses:  2.900974750518799 0.7626959085464478 0.8313168883323669
MemoryTrain:  epoch  9, batch     6 | loss: 2.9009748Losses:  1.9301899671554565 -0.0 0.5172144174575806
MemoryTrain:  epoch  9, batch     7 | loss: 1.9301900
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 56.25%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 53.12%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 52.68%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 54.69%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 58.13%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 60.80%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 61.46%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 68.75%,  total acc: 63.75%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 64.45%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 64.34%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 67.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 72.40%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 72.75%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 76.29%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.82%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.52%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 79.60%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.38%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.91%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 80.77%   [EVAL] batch:   39 | acc: 43.75%,  total acc: 79.84%   [EVAL] batch:   40 | acc: 37.50%,  total acc: 78.81%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 77.68%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 76.89%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 77.13%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 77.64%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 77.99%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 78.46%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 78.78%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 79.08%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 79.38%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 79.04%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 78.61%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 77.83%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 77.43%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 76.70%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 76.00%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 75.44%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 75.65%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 75.53%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 75.62%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 75.61%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 75.91%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 75.30%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 84.09%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 86.16%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 86.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 87.87%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 88.19%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 89.58%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 89.49%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 89.40%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 89.32%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 89.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 89.66%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 90.05%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 90.40%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 90.52%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 91.13%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 91.18%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 90.71%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 90.80%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 90.88%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 90.95%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.62%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.82%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 91.72%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 91.76%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 91.85%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 91.62%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.54%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.71%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 91.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.71%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.75%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 91.70%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.74%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 91.45%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 90.84%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 90.36%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 90.31%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 90.16%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 89.82%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 89.45%   [EVAL] batch:   64 | acc: 68.75%,  total acc: 89.13%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 88.92%   [EVAL] batch:   66 | acc: 62.50%,  total acc: 88.53%   [EVAL] batch:   67 | acc: 75.00%,  total acc: 88.33%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 88.13%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 88.04%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 87.68%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 87.24%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 86.99%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 86.82%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 86.75%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 86.60%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 86.69%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 86.46%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 86.47%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 86.41%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 86.57%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 86.36%   [EVAL] batch:   82 | acc: 87.50%,  total acc: 86.37%   [EVAL] batch:   83 | acc: 87.50%,  total acc: 86.38%   [EVAL] batch:   84 | acc: 81.25%,  total acc: 86.32%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 86.34%   [EVAL] batch:   86 | acc: 87.50%,  total acc: 86.35%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 86.43%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 86.59%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 86.74%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 86.89%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 87.03%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 87.10%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 87.24%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 87.37%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 87.63%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 87.75%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 87.88%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 87.50%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 87.25%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 87.01%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 86.78%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 86.79%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 86.56%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 86.16%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 85.59%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 85.21%   [EVAL] batch:  109 | acc: 25.00%,  total acc: 84.66%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 84.23%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 83.54%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 83.35%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 83.39%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 83.53%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 83.51%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 83.53%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 82.81%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 82.28%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 81.61%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 81.00%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 80.34%   [EVAL] batch:  124 | acc: 0.00%,  total acc: 79.70%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 79.56%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 79.23%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 78.91%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 78.54%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 78.32%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 78.01%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 78.03%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 78.10%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:  134 | acc: 62.50%,  total acc: 78.01%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 78.03%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 78.15%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 78.08%   [EVAL] batch:  138 | acc: 56.25%,  total acc: 77.92%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 77.81%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 77.84%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 77.86%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 77.71%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 77.69%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 77.67%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 77.74%   [EVAL] batch:  146 | acc: 87.50%,  total acc: 77.81%   [EVAL] batch:  147 | acc: 81.25%,  total acc: 77.83%   [EVAL] batch:  148 | acc: 87.50%,  total acc: 77.89%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 77.96%   [EVAL] batch:  150 | acc: 81.25%,  total acc: 77.98%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 77.96%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 77.98%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 78.08%   [EVAL] batch:  154 | acc: 56.25%,  total acc: 77.94%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 77.84%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 77.79%   [EVAL] batch:  157 | acc: 62.50%,  total acc: 77.69%   [EVAL] batch:  158 | acc: 56.25%,  total acc: 77.56%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 77.42%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 77.33%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 77.12%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 77.03%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 76.91%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 76.82%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 76.69%   [EVAL] batch:  166 | acc: 56.25%,  total acc: 76.57%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 76.45%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 76.33%   [EVAL] batch:  169 | acc: 18.75%,  total acc: 75.99%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 75.73%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 75.47%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 75.29%   [EVAL] batch:  173 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 74.79%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 74.93%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 75.07%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 75.35%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 75.49%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 75.76%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 75.75%   [EVAL] batch:  183 | acc: 87.50%,  total acc: 75.82%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 75.91%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 75.91%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 75.87%   [EVAL] batch:  187 | acc: 87.50%,  total acc: 75.93%   [EVAL] batch:  188 | acc: 43.75%,  total acc: 75.76%   [EVAL] batch:  189 | acc: 81.25%,  total acc: 75.79%   [EVAL] batch:  190 | acc: 56.25%,  total acc: 75.69%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 75.46%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 75.32%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 75.13%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 75.13%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 75.06%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 75.03%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 75.03%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 75.03%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 75.03%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 74.97%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 74.97%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 74.91%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 75.12%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 75.24%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 75.33%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 75.44%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 75.56%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 75.67%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 75.90%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 76.01%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 76.12%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 76.23%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 76.34%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 76.49%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 76.57%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 76.67%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 76.78%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 76.74%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 76.62%   [EVAL] batch:  227 | acc: 43.75%,  total acc: 76.48%   [EVAL] batch:  228 | acc: 50.00%,  total acc: 76.36%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 76.11%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 76.11%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 76.26%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 76.36%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 76.46%   [EVAL] batch:  235 | acc: 87.50%,  total acc: 76.51%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 76.58%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 76.60%   [EVAL] batch:  238 | acc: 50.00%,  total acc: 76.49%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 76.41%   [EVAL] batch:  240 | acc: 43.75%,  total acc: 76.27%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 76.19%   [EVAL] batch:  242 | acc: 31.25%,  total acc: 76.00%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 75.87%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 75.84%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 75.84%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 75.81%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 75.83%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 75.88%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 75.90%   
cur_acc:  ['0.9286', '0.7817', '0.7490', '0.7530']
his_acc:  ['0.9286', '0.8615', '0.8045', '0.7590']
Clustering into  24  clusters
Clusters:  [ 1  3 15  0 19 19 17  2  3  9 14  1 20  0 16  5  7  4 23  4  4 10 10 19
 13  9  1 19  5  5 18  5  4 22  8  5  4  1  9  1  0  1  1  3 12 11  6 21
  2  3]
Losses:  8.964269638061523 3.3139681816101074 0.6103378534317017
CurrentTrain: epoch  0, batch     0 | loss: 8.9642696Losses:  9.177417755126953 2.4054596424102783 0.7398959398269653
CurrentTrain: epoch  0, batch     1 | loss: 9.1774178Losses:  10.18177318572998 3.87089467048645 0.735377311706543
CurrentTrain: epoch  0, batch     2 | loss: 10.1817732Losses:  6.4851274490356445 -0.0 0.10361404716968536
CurrentTrain: epoch  0, batch     3 | loss: 6.4851274Losses:  10.2710542678833 4.295441150665283 0.6375128030776978
CurrentTrain: epoch  1, batch     0 | loss: 10.2710543Losses:  7.665051460266113 3.7188515663146973 0.5987751483917236
CurrentTrain: epoch  1, batch     1 | loss: 7.6650515Losses:  9.998588562011719 4.882399559020996 0.5267164707183838
CurrentTrain: epoch  1, batch     2 | loss: 9.9985886Losses:  4.742126941680908 -0.0 0.10027366131544113
CurrentTrain: epoch  1, batch     3 | loss: 4.7421269Losses:  6.878831386566162 2.6672728061676025 0.6693320274353027
CurrentTrain: epoch  2, batch     0 | loss: 6.8788314Losses:  7.341758728027344 3.65651798248291 0.6775285005569458
CurrentTrain: epoch  2, batch     1 | loss: 7.3417587Losses:  8.203763961791992 3.377023220062256 0.6994167566299438
CurrentTrain: epoch  2, batch     2 | loss: 8.2037640Losses:  3.4452481269836426 -0.0 0.13744184374809265
CurrentTrain: epoch  2, batch     3 | loss: 3.4452481Losses:  6.5377373695373535 3.004735231399536 0.5882631540298462
CurrentTrain: epoch  3, batch     0 | loss: 6.5377374Losses:  5.734487533569336 2.0503506660461426 0.6568233966827393
CurrentTrain: epoch  3, batch     1 | loss: 5.7344875Losses:  6.984382152557373 2.7212367057800293 0.5930554270744324
CurrentTrain: epoch  3, batch     2 | loss: 6.9843822Losses:  4.516478538513184 -0.0 0.17477566003799438
CurrentTrain: epoch  3, batch     3 | loss: 4.5164785Losses:  7.048335075378418 3.1841061115264893 0.6645086407661438
CurrentTrain: epoch  4, batch     0 | loss: 7.0483351Losses:  7.769586563110352 3.9236035346984863 0.5249369144439697
CurrentTrain: epoch  4, batch     1 | loss: 7.7695866Losses:  7.212275505065918 3.7383413314819336 0.5680448412895203
CurrentTrain: epoch  4, batch     2 | loss: 7.2122755Losses:  1.688685417175293 -0.0 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 1.6886854Losses:  5.819175720214844 2.1542582511901855 0.6608926653862
CurrentTrain: epoch  5, batch     0 | loss: 5.8191757Losses:  5.632357120513916 2.5903050899505615 0.6590075492858887
CurrentTrain: epoch  5, batch     1 | loss: 5.6323571Losses:  6.528847694396973 2.879227638244629 0.6299724578857422
CurrentTrain: epoch  5, batch     2 | loss: 6.5288477Losses:  4.356706619262695 -0.0 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 4.3567066Losses:  5.124607086181641 1.9489872455596924 0.6578978300094604
CurrentTrain: epoch  6, batch     0 | loss: 5.1246071Losses:  5.608391761779785 2.117417335510254 0.5592628121376038
CurrentTrain: epoch  6, batch     1 | loss: 5.6083918Losses:  6.654001712799072 3.385470390319824 0.6735273599624634
CurrentTrain: epoch  6, batch     2 | loss: 6.6540017Losses:  1.852159857749939 -0.0 0.12606537342071533
CurrentTrain: epoch  6, batch     3 | loss: 1.8521599Losses:  5.8173346519470215 3.1546859741210938 0.5685722231864929
CurrentTrain: epoch  7, batch     0 | loss: 5.8173347Losses:  6.4623565673828125 3.388289213180542 0.6595497131347656
CurrentTrain: epoch  7, batch     1 | loss: 6.4623566Losses:  6.731937885284424 3.6858880519866943 0.45425453782081604
CurrentTrain: epoch  7, batch     2 | loss: 6.7319379Losses:  5.614814281463623 -0.0 0.13844099640846252
CurrentTrain: epoch  7, batch     3 | loss: 5.6148143Losses:  5.373770236968994 2.4840872287750244 0.5650450587272644
CurrentTrain: epoch  8, batch     0 | loss: 5.3737702Losses:  5.391439914703369 2.131551742553711 0.6317205429077148
CurrentTrain: epoch  8, batch     1 | loss: 5.3914399Losses:  4.834821701049805 2.081963300704956 0.6377697587013245
CurrentTrain: epoch  8, batch     2 | loss: 4.8348217Losses:  2.859680652618408 -0.0 0.11329952627420425
CurrentTrain: epoch  8, batch     3 | loss: 2.8596807Losses:  4.437207221984863 1.7909595966339111 0.6306180953979492
CurrentTrain: epoch  9, batch     0 | loss: 4.4372072Losses:  5.927955627441406 2.818463087081909 0.5414621829986572
CurrentTrain: epoch  9, batch     1 | loss: 5.9279556Losses:  5.686051368713379 2.8020365238189697 0.5739664435386658
CurrentTrain: epoch  9, batch     2 | loss: 5.6860514Losses:  2.0937533378601074 -0.0 0.1153026893734932
CurrentTrain: epoch  9, batch     3 | loss: 2.0937533
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: alongside evan durbin and hugh gaitskell , he brought the thinking of john maynard keynes to the labour party , especially in relation to price determination .
Head Entity: hugh gaitskell
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: After serving as the mayor of Chicago, he became a prominent figure in the Democratic Party, advocating for social justice and economic reform.  
Head Entity: mayor of Chicago  
Tail Entity: Democratic Party  

Relation: member of political party  
Context: Throughout her career, she has been a staunch advocate for environmental policies as a member of the Green Party, pushing for sustainable practices in government.  
Head Entity: she  
Tail Entity: Green Party  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the script for " the great santini " was adapted by carlino from the 1976 novel by pat conroy , with assistance from an un - credited herman raucher .
Head Entity: the great santini
Tail Entity: pat conroy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "inception" draws heavily from the concepts presented in the 2001 novel "the dreamers" by j. k. rowling, which explores the nature of dreams and reality.  
Head Entity: inception  
Tail Entity: j. k. rowling  

Relation: after a work by  
Context: the musical "hamilton" was inspired by the biography "alexander hamilton" written by ron chernow, which details the life of the founding father.  
Head Entity: hamilton  
Tail Entity: ron chernow  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: the influential victorian art critic john ruskin expounded theories about social justice in " unto this last " ( 1860 ) .
Head Entity: john ruskin
Tail Entity: unto this last
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: the renowned physicist albert einstein published his groundbreaking paper on the theory of relativity in "on the electrodynamics of moving bodies" (1905).  
Head Entity: albert einstein  
Tail Entity: on the electrodynamics of moving bodies  

Relation: notable work  
Context: the celebrated author j.k. rowling captivated readers with her first book in the harry potter series, "harry potter and the philosopher's stone" (1997).  
Head Entity: j.k. rowling  
Tail Entity: harry potter and the philosopher's stone  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: in 1986 fram traveled to poland to play against katowice in a very even duel fram eventually lost . sparta prague came to reykjavík 1987 to play against fram reykjavik at laugardalsvöllur stadium .
Head Entity: fram reykjavik
Tail Entity: reykjavík
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: in 2001, the tech company apple inc. moved its headquarters to cupertino, california, where it has since developed numerous innovative products.  
Head Entity: apple inc.  
Tail Entity: cupertino  

Relation: headquarters location  
Context: the multinational corporation unilever has its headquarters located in london, where it oversees operations in various countries around the world.  
Head Entity: unilever  
Tail Entity: london  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: gyaclavator is an extinct genus of lace bug in the family tingidae known from a fossil found in north america .
Head Entity: tingidae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Panthera includes several species of large cats, such as lions and tigers, which are classified under the family Felidae.  
Head Entity: Felidae  
Tail Entity: family  

Relation: taxon rank  
Context: The species Homo sapiens is part of the genus Homo, which falls under the family Hominidae, known for its members' advanced cognitive abilities.  
Head Entity: Hominidae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: elspeth frances garman is professor of molecular biophysics at the university of oxford and a former president of the british crystallographic association .
Head Entity: elspeth frances garman
Tail Entity: molecular biophysics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: dr. amanda jones is a leading researcher in the field of artificial intelligence and currently works at the tech innovation lab in silicon valley.  
Head Entity: dr. amanda jones  
Tail Entity: artificial intelligence  

Relation: field of work  
Context: the world health organization has appointed dr. rajesh kumar as the chief epidemiologist, focusing on infectious diseases and public health strategies.  
Head Entity: dr. rajesh kumar  
Tail Entity: infectious diseases  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: this early involvement in radio led him to buy many small , under performing radio stations beginning with wcos in columbia , south carolina , and turn them around with new formats and improved management .
Head Entity: wcos
Tail Entity: columbia , south carolina
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: The new regulations allowed the station to expand its reach, enabling it to serve a larger audience in the surrounding areas, particularly in the vibrant community of downtown Nashville.  
Head Entity: the station  
Tail Entity: downtown Nashville  

Relation: licensed to broadcast to  
Context: After acquiring the frequency, the network announced plans to launch a new channel that would cater specifically to the diverse population of the San Francisco Bay Area.  
Head Entity: the network  
Tail Entity: San Francisco Bay Area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: tau² eridani ( τ² eridani , abbreviated tau² eri , τ² eri ) , also named angetenar , is a star in the constellation of eridanus .
Head Entity: angetenar
Tail Entity: eridanus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: betelgeuse ( α orionis ) is a red supergiant star in the constellation of orion, known for its brightness and distinctive reddish hue.  
Head Entity: betelgeuse  
Tail Entity: orion  

Relation: constellation  
Context: the star deneb is located in the constellation of cygnus and is one of the brightest stars in the northern sky.  
Head Entity: deneb  
Tail Entity: cygnus  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Losses:  3.6090078353881836 0.2573464512825012 0.8517554998397827
MemoryTrain:  epoch  0, batch     0 | loss: 3.6090078Losses:  4.542511940002441 0.7226335406303406 0.8389690518379211
MemoryTrain:  epoch  0, batch     1 | loss: 4.5425119Losses:  3.402644634246826 -0.0 0.9116374254226685
MemoryTrain:  epoch  0, batch     2 | loss: 3.4026446Losses:  3.3255228996276855 0.25478464365005493 0.8469761610031128
MemoryTrain:  epoch  0, batch     3 | loss: 3.3255229Losses:  4.292139053344727 0.23975645005702972 0.9051386713981628
MemoryTrain:  epoch  0, batch     4 | loss: 4.2921391Losses:  3.809699773788452 0.491242915391922 0.8664965629577637
MemoryTrain:  epoch  0, batch     5 | loss: 3.8096998Losses:  4.357027053833008 0.2743031978607178 0.7871267199516296
MemoryTrain:  epoch  0, batch     6 | loss: 4.3570271Losses:  3.897157669067383 0.4865856170654297 0.9114562273025513
MemoryTrain:  epoch  0, batch     7 | loss: 3.8971577Losses:  5.035006046295166 0.8922882676124573 0.9014817476272583
MemoryTrain:  epoch  0, batch     8 | loss: 5.0350060Losses:  4.785147666931152 -0.0 0.47849512100219727
MemoryTrain:  epoch  0, batch     9 | loss: 4.7851477Losses:  3.426408290863037 0.5190711617469788 0.7378371953964233
MemoryTrain:  epoch  1, batch     0 | loss: 3.4264083Losses:  2.9882071018218994 0.5000208616256714 0.7273300290107727
MemoryTrain:  epoch  1, batch     1 | loss: 2.9882071Losses:  3.6856298446655273 0.2509995698928833 0.9104679822921753
MemoryTrain:  epoch  1, batch     2 | loss: 3.6856298Losses:  3.746544122695923 0.3742945194244385 0.7018764615058899
MemoryTrain:  epoch  1, batch     3 | loss: 3.7465441Losses:  2.86273193359375 0.2720286548137665 0.9207584857940674
MemoryTrain:  epoch  1, batch     4 | loss: 2.8627319Losses:  2.907355546951294 -0.0 0.919403076171875
MemoryTrain:  epoch  1, batch     5 | loss: 2.9073555Losses:  3.723522663116455 0.279512882232666 0.8035840392112732
MemoryTrain:  epoch  1, batch     6 | loss: 3.7235227Losses:  3.4940874576568604 0.24344012141227722 0.8119638562202454
MemoryTrain:  epoch  1, batch     7 | loss: 3.4940875Losses:  4.431886672973633 0.53879714012146 0.9519033432006836
MemoryTrain:  epoch  1, batch     8 | loss: 4.4318867Losses:  3.973944664001465 -0.0 0.4995417296886444
MemoryTrain:  epoch  1, batch     9 | loss: 3.9739447Losses:  2.7573695182800293 -0.0 0.8548412322998047
MemoryTrain:  epoch  2, batch     0 | loss: 2.7573695Losses:  2.9016342163085938 -0.0 0.9284447431564331
MemoryTrain:  epoch  2, batch     1 | loss: 2.9016342Losses:  4.259149074554443 0.6396999955177307 0.8347694277763367
MemoryTrain:  epoch  2, batch     2 | loss: 4.2591491Losses:  2.896904468536377 -0.0 0.9859979152679443
MemoryTrain:  epoch  2, batch     3 | loss: 2.8969045Losses:  3.340177536010742 0.2566494345664978 0.7954296469688416
MemoryTrain:  epoch  2, batch     4 | loss: 3.3401775Losses:  3.1322240829467773 -0.0 0.9542531967163086
MemoryTrain:  epoch  2, batch     5 | loss: 3.1322241Losses:  3.5594875812530518 0.49740493297576904 0.798682451248169
MemoryTrain:  epoch  2, batch     6 | loss: 3.5594876Losses:  3.1851820945739746 0.7708542346954346 0.8470081090927124
MemoryTrain:  epoch  2, batch     7 | loss: 3.1851821Losses:  2.380786895751953 -0.0 0.8686736822128296
MemoryTrain:  epoch  2, batch     8 | loss: 2.3807869Losses:  2.4695942401885986 -0.0 0.45002123713493347
MemoryTrain:  epoch  2, batch     9 | loss: 2.4695942Losses:  3.0727310180664062 0.27484750747680664 0.9222055673599243
MemoryTrain:  epoch  3, batch     0 | loss: 3.0727310Losses:  2.629014730453491 0.2881510555744171 0.8662473559379578
MemoryTrain:  epoch  3, batch     1 | loss: 2.6290147Losses:  4.210994243621826 1.4025959968566895 0.90740966796875
MemoryTrain:  epoch  3, batch     2 | loss: 4.2109942Losses:  3.302760124206543 0.2289503514766693 0.8579518795013428
MemoryTrain:  epoch  3, batch     3 | loss: 3.3027601Losses:  2.6623096466064453 -0.0 0.9133200645446777
MemoryTrain:  epoch  3, batch     4 | loss: 2.6623096Losses:  2.542213201522827 0.2528326213359833 0.7895832061767578
MemoryTrain:  epoch  3, batch     5 | loss: 2.5422132Losses:  2.317598819732666 -0.0 0.8616212010383606
MemoryTrain:  epoch  3, batch     6 | loss: 2.3175988Losses:  2.135601758956909 -0.0 0.8018214106559753
MemoryTrain:  epoch  3, batch     7 | loss: 2.1356018Losses:  3.4839065074920654 0.5222030878067017 0.7140395045280457
MemoryTrain:  epoch  3, batch     8 | loss: 3.4839065Losses:  4.04049015045166 0.30342987179756165 0.4395471513271332
MemoryTrain:  epoch  3, batch     9 | loss: 4.0404902Losses:  3.4953575134277344 0.8530030250549316 0.8672555685043335
MemoryTrain:  epoch  4, batch     0 | loss: 3.4953575Losses:  2.7044782638549805 0.2836073040962219 0.7305787801742554
MemoryTrain:  epoch  4, batch     1 | loss: 2.7044783Losses:  2.890406608581543 0.515845000743866 0.9140077829360962
MemoryTrain:  epoch  4, batch     2 | loss: 2.8904066Losses:  2.8590924739837646 0.5054463148117065 0.7285087704658508
MemoryTrain:  epoch  4, batch     3 | loss: 2.8590925Losses:  2.857727527618408 0.46248674392700195 0.8617993593215942
MemoryTrain:  epoch  4, batch     4 | loss: 2.8577275Losses:  2.757585287094116 0.27812671661376953 0.8235461115837097
MemoryTrain:  epoch  4, batch     5 | loss: 2.7575853Losses:  3.0251171588897705 0.5295817255973816 0.7775494456291199
MemoryTrain:  epoch  4, batch     6 | loss: 3.0251172Losses:  2.4278266429901123 0.2514694929122925 0.9128033518791199
MemoryTrain:  epoch  4, batch     7 | loss: 2.4278266Losses:  2.94558048248291 0.7834179997444153 0.8239173889160156
MemoryTrain:  epoch  4, batch     8 | loss: 2.9455805Losses:  2.2553391456604004 -0.0 0.5314928293228149
MemoryTrain:  epoch  4, batch     9 | loss: 2.2553391Losses:  2.730034351348877 0.255453884601593 0.8946837186813354
MemoryTrain:  epoch  5, batch     0 | loss: 2.7300344Losses:  2.5602645874023438 -0.0 0.8506029844284058
MemoryTrain:  epoch  5, batch     1 | loss: 2.5602646Losses:  2.786911964416504 0.5373877882957458 0.8006578683853149
MemoryTrain:  epoch  5, batch     2 | loss: 2.7869120Losses:  2.146090269088745 -0.0 0.8101004958152771
MemoryTrain:  epoch  5, batch     3 | loss: 2.1460903Losses:  2.607853889465332 0.4971522092819214 0.8208217620849609
MemoryTrain:  epoch  5, batch     4 | loss: 2.6078539Losses:  2.3936893939971924 -0.0 0.8381183743476868
MemoryTrain:  epoch  5, batch     5 | loss: 2.3936894Losses:  3.0044426918029785 0.4917789101600647 0.8443238139152527
MemoryTrain:  epoch  5, batch     6 | loss: 3.0044427Losses:  2.8480048179626465 0.757752537727356 0.751373827457428
MemoryTrain:  epoch  5, batch     7 | loss: 2.8480048Losses:  2.586667060852051 0.5133742094039917 0.7828153967857361
MemoryTrain:  epoch  5, batch     8 | loss: 2.5866671Losses:  1.7415730953216553 -0.0 0.4069654643535614
MemoryTrain:  epoch  5, batch     9 | loss: 1.7415731Losses:  2.6452698707580566 0.5385465621948242 0.8014172911643982
MemoryTrain:  epoch  6, batch     0 | loss: 2.6452699Losses:  2.866359233856201 0.5221195816993713 0.9173230528831482
MemoryTrain:  epoch  6, batch     1 | loss: 2.8663592Losses:  2.565762996673584 0.25810736417770386 0.7773357629776001
MemoryTrain:  epoch  6, batch     2 | loss: 2.5657630Losses:  3.3789002895355225 1.3662450313568115 0.6559336185455322
MemoryTrain:  epoch  6, batch     3 | loss: 3.3789003Losses:  2.634507179260254 0.2515507936477661 0.8141558766365051
MemoryTrain:  epoch  6, batch     4 | loss: 2.6345072Losses:  2.2615201473236084 -0.0 0.9012052416801453
MemoryTrain:  epoch  6, batch     5 | loss: 2.2615201Losses:  2.759063720703125 0.6970648765563965 0.833633303642273
MemoryTrain:  epoch  6, batch     6 | loss: 2.7590637Losses:  2.803274393081665 0.786263108253479 0.7250908017158508
MemoryTrain:  epoch  6, batch     7 | loss: 2.8032744Losses:  2.8679440021514893 0.4924142360687256 0.8759285807609558
MemoryTrain:  epoch  6, batch     8 | loss: 2.8679440Losses:  1.7542442083358765 -0.0 0.4203472137451172
MemoryTrain:  epoch  6, batch     9 | loss: 1.7542442Losses:  2.431135654449463 0.21283334493637085 0.9009119272232056
MemoryTrain:  epoch  7, batch     0 | loss: 2.4311357Losses:  2.800427198410034 0.7361413240432739 0.7612554430961609
MemoryTrain:  epoch  7, batch     1 | loss: 2.8004272Losses:  2.5227322578430176 0.2521732449531555 0.8468360304832458
MemoryTrain:  epoch  7, batch     2 | loss: 2.5227323Losses:  2.2143027782440186 0.23927313089370728 0.7180209755897522
MemoryTrain:  epoch  7, batch     3 | loss: 2.2143028Losses:  3.2590179443359375 1.120300531387329 0.8916484117507935
MemoryTrain:  epoch  7, batch     4 | loss: 3.2590179Losses:  2.4791617393493652 0.2709708511829376 0.9043983221054077
MemoryTrain:  epoch  7, batch     5 | loss: 2.4791617Losses:  2.9426019191741943 0.5823251605033875 0.757167637348175
MemoryTrain:  epoch  7, batch     6 | loss: 2.9426019Losses:  2.2681703567504883 -0.0 1.0250263214111328
MemoryTrain:  epoch  7, batch     7 | loss: 2.2681704Losses:  2.698754072189331 0.5129858255386353 0.9115905165672302
MemoryTrain:  epoch  7, batch     8 | loss: 2.6987541Losses:  1.8912206888198853 -0.0 0.4138161838054657
MemoryTrain:  epoch  7, batch     9 | loss: 1.8912207Losses:  2.8667356967926025 0.5958301424980164 0.7792343497276306
MemoryTrain:  epoch  8, batch     0 | loss: 2.8667357Losses:  2.1514861583709717 -0.0 0.8468099236488342
MemoryTrain:  epoch  8, batch     1 | loss: 2.1514862Losses:  2.626788377761841 0.6875337362289429 0.7080381512641907
MemoryTrain:  epoch  8, batch     2 | loss: 2.6267884Losses:  2.6596693992614746 0.49194595217704773 0.91386878490448
MemoryTrain:  epoch  8, batch     3 | loss: 2.6596694Losses:  2.3242907524108887 0.24123898148536682 0.862848162651062
MemoryTrain:  epoch  8, batch     4 | loss: 2.3242908Losses:  2.289459705352783 0.24464067816734314 0.7153132557868958
MemoryTrain:  epoch  8, batch     5 | loss: 2.2894597Losses:  2.3599982261657715 -0.0 1.0206294059753418
MemoryTrain:  epoch  8, batch     6 | loss: 2.3599982Losses:  2.385791540145874 0.4840386211872101 0.6495295166969299
MemoryTrain:  epoch  8, batch     7 | loss: 2.3857915Losses:  2.3217356204986572 0.2477472871541977 0.8384064435958862
MemoryTrain:  epoch  8, batch     8 | loss: 2.3217356Losses:  2.060213565826416 0.3155122995376587 0.4858159124851227
MemoryTrain:  epoch  8, batch     9 | loss: 2.0602136Losses:  2.659233570098877 0.47215527296066284 0.8475595116615295
MemoryTrain:  epoch  9, batch     0 | loss: 2.6592336Losses:  2.2430765628814697 0.23330825567245483 0.7856147289276123
MemoryTrain:  epoch  9, batch     1 | loss: 2.2430766Losses:  2.362912654876709 0.25091004371643066 0.8605040311813354
MemoryTrain:  epoch  9, batch     2 | loss: 2.3629127Losses:  2.4415128231048584 0.47762519121170044 0.7604169249534607
MemoryTrain:  epoch  9, batch     3 | loss: 2.4415128Losses:  2.4116485118865967 0.2433120757341385 0.8957424163818359
MemoryTrain:  epoch  9, batch     4 | loss: 2.4116485Losses:  2.843092441558838 0.7290937900543213 0.84521484375
MemoryTrain:  epoch  9, batch     5 | loss: 2.8430924Losses:  2.4715380668640137 0.5007716417312622 0.6536552309989929
MemoryTrain:  epoch  9, batch     6 | loss: 2.4715381Losses:  2.3155179023742676 0.2238876223564148 0.8987843990325928
MemoryTrain:  epoch  9, batch     7 | loss: 2.3155179Losses:  3.073884963989258 0.8846539258956909 0.8492426872253418
MemoryTrain:  epoch  9, batch     8 | loss: 3.0738850Losses:  1.6436337232589722 -0.0 0.45855388045310974
MemoryTrain:  epoch  9, batch     9 | loss: 1.6436337
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 86.88%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 85.80%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 84.62%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 83.98%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 81.91%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 80.11%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 77.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.13%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 82.06%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 81.45%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 81.06%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 80.51%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 80.18%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 79.69%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 79.05%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 78.62%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 80.18%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 80.65%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 81.10%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 81.39%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 81.81%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 82.20%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 82.58%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 82.94%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 83.29%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 83.46%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 83.41%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.61%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 83.56%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 83.41%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 83.48%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 83.55%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 83.62%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 83.79%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 84.06%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 84.32%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 83.93%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 73.56%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 78.68%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.51%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.59%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.94%   [EVAL] batch:   20 | acc: 81.25%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 80.68%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 80.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.49%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 82.18%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 83.19%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.27%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.23%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 85.11%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 84.82%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 85.07%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 85.30%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 85.53%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.90%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 86.25%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.59%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 86.92%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 87.07%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 87.08%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 86.96%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 86.70%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 86.59%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.86%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 86.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 87.01%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 87.02%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 87.15%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 87.04%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 86.70%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 86.83%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 86.51%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 85.99%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 85.81%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 85.83%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 85.76%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 85.48%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 85.52%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 85.45%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 85.29%   [EVAL] batch:   65 | acc: 87.50%,  total acc: 85.32%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 85.17%   [EVAL] batch:   67 | acc: 87.50%,  total acc: 85.20%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 85.05%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 85.09%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 84.68%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 84.29%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 84.08%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 83.95%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 83.92%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 83.80%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 83.85%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 83.65%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 83.70%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 83.59%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 83.80%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 83.46%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 83.36%   [EVAL] batch:   83 | acc: 68.75%,  total acc: 83.18%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 83.01%   [EVAL] batch:   85 | acc: 87.50%,  total acc: 83.07%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 83.19%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 83.31%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 83.50%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 83.68%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 83.79%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 83.97%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 84.14%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 84.24%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 84.41%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 84.73%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 84.89%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 85.19%   [EVAL] batch:  100 | acc: 50.00%,  total acc: 84.84%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 84.62%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 84.41%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 84.13%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 84.17%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 83.96%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 83.59%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 82.93%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 82.51%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 81.88%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 81.48%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 80.80%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 80.59%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 80.65%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.82%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 80.82%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 80.88%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 80.93%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 80.93%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 80.36%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 79.86%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 79.20%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 78.61%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 77.97%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 77.40%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 77.28%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 76.97%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 76.76%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 76.41%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 76.20%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 75.86%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 75.90%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 76.08%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 76.17%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 76.16%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 76.19%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 76.37%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 76.40%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 76.21%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 76.07%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 76.11%   [EVAL] batch:  141 | acc: 81.25%,  total acc: 76.14%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 76.05%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 76.00%   [EVAL] batch:  144 | acc: 75.00%,  total acc: 75.99%   [EVAL] batch:  145 | acc: 87.50%,  total acc: 76.07%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 76.06%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 76.06%   [EVAL] batch:  148 | acc: 62.50%,  total acc: 75.96%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 75.96%   [EVAL] batch:  150 | acc: 87.50%,  total acc: 76.03%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 76.07%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 76.10%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 76.22%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 76.17%   [EVAL] batch:  155 | acc: 56.25%,  total acc: 76.04%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 76.00%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 75.87%   [EVAL] batch:  158 | acc: 43.75%,  total acc: 75.67%   [EVAL] batch:  159 | acc: 56.25%,  total acc: 75.55%   [EVAL] batch:  160 | acc: 50.00%,  total acc: 75.39%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 75.19%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 75.04%   [EVAL] batch:  163 | acc: 50.00%,  total acc: 74.89%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 74.81%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 74.70%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 74.63%   [EVAL] batch:  167 | acc: 56.25%,  total acc: 74.52%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 74.41%   [EVAL] batch:  169 | acc: 25.00%,  total acc: 74.12%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 73.87%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 73.62%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 73.45%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 73.20%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 73.04%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 73.19%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 73.34%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 73.64%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 73.78%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 73.94%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 73.80%   [EVAL] batch:  183 | acc: 81.25%,  total acc: 73.85%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 73.89%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 73.86%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 73.83%   [EVAL] batch:  187 | acc: 81.25%,  total acc: 73.87%   [EVAL] batch:  188 | acc: 50.00%,  total acc: 73.74%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 73.72%   [EVAL] batch:  190 | acc: 50.00%,  total acc: 73.59%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 73.37%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 73.28%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 73.07%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 72.96%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 72.97%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 73.04%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 73.02%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 73.03%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 73.04%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 73.02%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 72.97%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 72.95%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 72.96%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 72.88%   [EVAL] batch:  206 | acc: 100.00%,  total acc: 73.01%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 73.27%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 73.36%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 73.68%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.92%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 74.05%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.16%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.28%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.40%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.63%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 74.72%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 74.80%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 75.03%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 74.89%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 74.78%   [EVAL] batch:  228 | acc: 50.00%,  total acc: 74.67%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 74.43%   [EVAL] batch:  230 | acc: 75.00%,  total acc: 74.43%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 74.52%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 74.60%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 74.68%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:  235 | acc: 100.00%,  total acc: 74.89%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 74.92%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 74.95%   [EVAL] batch:  238 | acc: 81.25%,  total acc: 74.97%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 74.90%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 74.82%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 74.82%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 74.74%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 74.62%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 74.64%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 74.64%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 74.62%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 74.65%   [EVAL] batch:  248 | acc: 93.75%,  total acc: 74.72%   [EVAL] batch:  249 | acc: 68.75%,  total acc: 74.70%   [EVAL] batch:  250 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 74.78%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 74.83%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 74.85%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 74.93%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 75.02%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 75.02%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 75.07%   [EVAL] batch:  258 | acc: 93.75%,  total acc: 75.14%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 75.17%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 75.17%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 75.14%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 75.19%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 75.21%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 75.24%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 75.26%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 75.26%   [EVAL] batch:  267 | acc: 75.00%,  total acc: 75.26%   [EVAL] batch:  268 | acc: 62.50%,  total acc: 75.21%   [EVAL] batch:  269 | acc: 62.50%,  total acc: 75.16%   [EVAL] batch:  270 | acc: 81.25%,  total acc: 75.18%   [EVAL] batch:  271 | acc: 62.50%,  total acc: 75.14%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 75.11%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 75.11%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 74.98%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 75.07%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 75.16%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 75.34%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 75.47%   [EVAL] batch:  282 | acc: 68.75%,  total acc: 75.44%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 75.40%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 75.37%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 75.33%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 75.26%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 75.22%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 75.39%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 75.47%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 75.56%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 75.64%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 75.70%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 75.78%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 75.87%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 75.95%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 76.03%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 76.11%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 76.19%   [EVAL] batch:  300 | acc: 75.00%,  total acc: 76.18%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 76.20%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 76.26%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 76.27%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 76.27%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 76.31%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 76.34%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 76.38%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 76.44%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 76.51%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 76.59%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 76.56%   
cur_acc:  ['0.9286', '0.7817', '0.7490', '0.7530', '0.8393']
his_acc:  ['0.9286', '0.8615', '0.8045', '0.7590', '0.7656']
Clustering into  29  clusters
Clusters:  [ 5  4 17 10  0  0 19  3  4 14 25  5 18 11  8  7 20  2  1  2  2  6  6  0
 13 14  5  0  7  7 15  7  2 28 27  7  2  5 14  5 10  5  5  4 21  1  9 16
  3  4  0 11 26 13 12 23 22 24  5  4]
Losses:  12.549328804016113 4.91278076171875 0.814276397228241
CurrentTrain: epoch  0, batch     0 | loss: 12.5493288Losses:  9.45304012298584 3.3079001903533936 0.846501350402832
CurrentTrain: epoch  0, batch     1 | loss: 9.4530401Losses:  10.716702461242676 3.46921443939209 0.8074272274971008
CurrentTrain: epoch  0, batch     2 | loss: 10.7167025Losses:  6.898681163787842 -0.0 0.17162300646305084
CurrentTrain: epoch  0, batch     3 | loss: 6.8986812Losses:  10.61654281616211 4.263462066650391 0.7846755981445312
CurrentTrain: epoch  1, batch     0 | loss: 10.6165428Losses:  9.498838424682617 3.6486167907714844 0.716949462890625
CurrentTrain: epoch  1, batch     1 | loss: 9.4988384Losses:  8.741518020629883 2.983286142349243 0.8134225010871887
CurrentTrain: epoch  1, batch     2 | loss: 8.7415180Losses:  7.24022102355957 -0.0 0.12567147612571716
CurrentTrain: epoch  1, batch     3 | loss: 7.2402210Losses:  9.265915870666504 3.3086929321289062 0.8398290872573853
CurrentTrain: epoch  2, batch     0 | loss: 9.2659159Losses:  9.448385238647461 4.066272735595703 0.7412568926811218
CurrentTrain: epoch  2, batch     1 | loss: 9.4483852Losses:  10.417088508605957 4.699305534362793 0.7459120154380798
CurrentTrain: epoch  2, batch     2 | loss: 10.4170885Losses:  3.8087410926818848 -0.0 0.11762185394763947
CurrentTrain: epoch  2, batch     3 | loss: 3.8087411Losses:  8.64732551574707 2.997103214263916 0.8432121276855469
CurrentTrain: epoch  3, batch     0 | loss: 8.6473255Losses:  11.09882640838623 5.025771617889404 0.7199690937995911
CurrentTrain: epoch  3, batch     1 | loss: 11.0988264Losses:  6.532116889953613 2.4429895877838135 0.7881921529769897
CurrentTrain: epoch  3, batch     2 | loss: 6.5321169Losses:  5.133747577667236 -0.0 0.11222992837429047
CurrentTrain: epoch  3, batch     3 | loss: 5.1337476Losses:  8.612268447875977 3.2625935077667236 0.7841909527778625
CurrentTrain: epoch  4, batch     0 | loss: 8.6122684Losses:  8.640568733215332 3.5841901302337646 0.7845369577407837
CurrentTrain: epoch  4, batch     1 | loss: 8.6405687Losses:  8.124876976013184 4.000821113586426 0.6991095542907715
CurrentTrain: epoch  4, batch     2 | loss: 8.1248770Losses:  2.3842785358428955 -0.0 0.10618693381547928
CurrentTrain: epoch  4, batch     3 | loss: 2.3842785Losses:  8.098589897155762 3.2117843627929688 0.7735353112220764
CurrentTrain: epoch  5, batch     0 | loss: 8.0985899Losses:  7.906816005706787 3.998229503631592 0.695833146572113
CurrentTrain: epoch  5, batch     1 | loss: 7.9068160Losses:  7.133071422576904 2.8484201431274414 0.7704623937606812
CurrentTrain: epoch  5, batch     2 | loss: 7.1330714Losses:  5.326521396636963 -0.0 0.09408961981534958
CurrentTrain: epoch  5, batch     3 | loss: 5.3265214Losses:  6.664224147796631 2.4952385425567627 0.833394467830658
CurrentTrain: epoch  6, batch     0 | loss: 6.6642241Losses:  7.855298042297363 3.3372678756713867 0.6503679156303406
CurrentTrain: epoch  6, batch     1 | loss: 7.8552980Losses:  6.086656093597412 2.153714418411255 0.8313829302787781
CurrentTrain: epoch  6, batch     2 | loss: 6.0866561Losses:  2.067909002304077 -0.0 0.10412700474262238
CurrentTrain: epoch  6, batch     3 | loss: 2.0679090Losses:  9.547163963317871 5.7533111572265625 0.6581770181655884
CurrentTrain: epoch  7, batch     0 | loss: 9.5471640Losses:  7.043729782104492 3.493305206298828 0.6594679355621338
CurrentTrain: epoch  7, batch     1 | loss: 7.0437298Losses:  6.843689441680908 2.9525694847106934 0.7288863062858582
CurrentTrain: epoch  7, batch     2 | loss: 6.8436894Losses:  7.932509422302246 -0.0 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 7.9325094Losses:  6.825824737548828 2.8706116676330566 0.7619129419326782
CurrentTrain: epoch  8, batch     0 | loss: 6.8258247Losses:  5.658580303192139 2.206052541732788 0.7993778586387634
CurrentTrain: epoch  8, batch     1 | loss: 5.6585803Losses:  6.869935989379883 2.8480873107910156 0.6833762526512146
CurrentTrain: epoch  8, batch     2 | loss: 6.8699360Losses:  2.1023764610290527 -0.0 0.08602219820022583
CurrentTrain: epoch  8, batch     3 | loss: 2.1023765Losses:  6.270018100738525 3.6939566135406494 0.5667204856872559
CurrentTrain: epoch  9, batch     0 | loss: 6.2700181Losses:  6.716830253601074 3.483180046081543 0.733110249042511
CurrentTrain: epoch  9, batch     1 | loss: 6.7168303Losses:  9.193578720092773 4.630181312561035 0.7465861439704895
CurrentTrain: epoch  9, batch     2 | loss: 9.1935787Losses:  5.244746685028076 -0.0 0.13443168997764587
CurrentTrain: epoch  9, batch     3 | loss: 5.2447467
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found on the slopes of the majestic Andes mountains, providing a breathtaking view of the valley below.  
Head Entity: Andes mountains  
Tail Entity: ancient ruins  

Relation: located on terrain feature  
Context: The village is nestled in the heart of the Black Forest, surrounded by dense trees and picturesque landscapes.  
Head Entity: Black Forest  
Tail Entity: village  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and organizations come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy, showcasing the best of international football.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: the festival opened with " whiplash " directed by damien chazelle and closed with musical drama " rudderless " directed by william h. macy .
Head Entity: whiplash
Tail Entity: damien chazelle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the critically acclaimed series "breaking bad" was brought to life by director vince gilligan, while the spin-off "better call saul" was directed by peter gould.  
Head Entity: breaking bad  
Tail Entity: vince gilligan  

Relation: director  
Context: the animated feature "spirit: stallion of the cimarron" was directed by kelly asbury and lorna cook, showcasing stunning visuals and a heartfelt story.  
Head Entity: spirit: stallion of the cimarron  
Tail Entity: kelly asbury
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game franchise Call of Duty was acquired by Activision Blizzard, which has been a major player in the gaming industry for years.  
Head Entity: Call of Duty  
Tail Entity: Activision Blizzard  

Relation: owned by  
Context: The iconic fashion brand Gucci is a subsidiary of the French luxury group Kering, which oversees several high-end labels.  
Head Entity: Gucci  
Tail Entity: Kering  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: remains of taxa that can be confidently assigned to istiodactylidae have been found in the uk and china , in rocks dating from the early cretaceous period ( barremian to aptian stage ) .
Head Entity: barremian
Tail Entity: early cretaceous
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The Great Wall of China is a series of fortifications made of various materials, and it is part of the UNESCO World Heritage Sites.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Sites  

Relation: part of  
Context: The human heart is a vital organ that is part of the circulatory system, responsible for pumping blood throughout the body.  
Head Entity: human heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: it served as the home of k league side gyeongnam fc and national league side changwon city fc prior to the construction of the changwon football center in 2009 .
Head Entity: changwon football center
Tail Entity: gyeongnam fc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was once the residence of the famous artist, and now it serves as a gallery showcasing his works, attracting visitors from all over the world.  
Head Entity: historic building  
Tail Entity: famous artist  

Relation: occupant  
Context: After the renovation, the old factory was transformed into a vibrant co-working space, where numerous startups and freelancers now thrive.  
Head Entity: old factory  
Tail Entity: startups
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: he also commissioned renowned architect richard neutra to design the iconic desert kaufmann house ( 1946 ) in palm springs , california .
Head Entity: kaufmann house
Tail Entity: richard neutra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: the innovative design of the new city library was created by the famous architect zaha hadid, known for her futuristic structures.  
Head Entity: city library  
Tail Entity: zaha hadid  

Relation: architect  
Context: after years of planning, the historic renovation of the old courthouse was finally completed, thanks to the talented architect frank lloyd wright.  
Head Entity: old courthouse  
Tail Entity: frank lloyd wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: in 2001 brian haw set up the parliament square peace campaign outside the houses of parliament in london .
Head Entity: brian haw
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: After moving to New York City in 2010, Sarah found her dream apartment in Brooklyn.  
Head Entity: Sarah  
Tail Entity: Brooklyn  

Relation: residence  
Context: The famous author lived in a quaint cottage in the countryside for many years before relocating to the city.  
Head Entity: The famous author  
Tail Entity: the city  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle was fought near the banks of the Potomac River, a significant site in American history.  
Head Entity: historic battle  
Tail Entity: Potomac River  
Losses:  4.092754364013672 0.2717234790325165 1.049821138381958
MemoryTrain:  epoch  0, batch     0 | loss: 4.0927544Losses:  3.106311321258545 -0.0 0.9530829191207886
MemoryTrain:  epoch  0, batch     1 | loss: 3.1063113Losses:  3.400942802429199 0.2626926898956299 0.8616482615470886
MemoryTrain:  epoch  0, batch     2 | loss: 3.4009428Losses:  3.872882843017578 -0.0 0.9530493021011353
MemoryTrain:  epoch  0, batch     3 | loss: 3.8728828Losses:  4.2261176109313965 0.4373445510864258 0.904076337814331
MemoryTrain:  epoch  0, batch     4 | loss: 4.2261176Losses:  3.916553497314453 0.2625522017478943 0.7131191492080688
MemoryTrain:  epoch  0, batch     5 | loss: 3.9165535Losses:  3.396735191345215 -0.0 0.9062408804893494
MemoryTrain:  epoch  0, batch     6 | loss: 3.3967352Losses:  4.154965877532959 -0.0 0.9293691515922546
MemoryTrain:  epoch  0, batch     7 | loss: 4.1549659Losses:  4.376674175262451 -0.0 0.8627952337265015
MemoryTrain:  epoch  0, batch     8 | loss: 4.3766742Losses:  3.0578856468200684 0.5119895935058594 0.8115901947021484
MemoryTrain:  epoch  0, batch     9 | loss: 3.0578856Losses:  4.476713180541992 0.23333077132701874 0.9847915768623352
MemoryTrain:  epoch  0, batch    10 | loss: 4.4767132Losses:  2.1640186309814453 -0.0 0.3559393882751465
MemoryTrain:  epoch  0, batch    11 | loss: 2.1640186Losses:  3.6805038452148438 0.7498422861099243 0.8662904500961304
MemoryTrain:  epoch  1, batch     0 | loss: 3.6805038Losses:  3.6875996589660645 -0.0 0.9797433614730835
MemoryTrain:  epoch  1, batch     1 | loss: 3.6875997Losses:  3.390068292617798 0.5183123350143433 0.6666038036346436
MemoryTrain:  epoch  1, batch     2 | loss: 3.3900683Losses:  3.373211145401001 0.2650994062423706 0.8326126933097839
MemoryTrain:  epoch  1, batch     3 | loss: 3.3732111Losses:  3.468358039855957 -0.0 0.9107203483581543
MemoryTrain:  epoch  1, batch     4 | loss: 3.4683580Losses:  4.408008575439453 0.32663285732269287 0.9581723213195801
MemoryTrain:  epoch  1, batch     5 | loss: 4.4080086Losses:  3.370130777359009 0.5133277773857117 0.8984114527702332
MemoryTrain:  epoch  1, batch     6 | loss: 3.3701308Losses:  2.972034454345703 0.5092104077339172 0.8587360382080078
MemoryTrain:  epoch  1, batch     7 | loss: 2.9720345Losses:  5.009475231170654 1.0609028339385986 0.7894493937492371
MemoryTrain:  epoch  1, batch     8 | loss: 5.0094752Losses:  3.6212711334228516 0.25166258215904236 0.9762972593307495
MemoryTrain:  epoch  1, batch     9 | loss: 3.6212711Losses:  3.846475124359131 -0.0 0.9813257455825806
MemoryTrain:  epoch  1, batch    10 | loss: 3.8464751Losses:  2.535857915878296 -0.0 0.3569529056549072
MemoryTrain:  epoch  1, batch    11 | loss: 2.5358579Losses:  3.029228448867798 0.257615864276886 0.7707814574241638
MemoryTrain:  epoch  2, batch     0 | loss: 3.0292284Losses:  3.454005718231201 0.796802282333374 0.9255990386009216
MemoryTrain:  epoch  2, batch     1 | loss: 3.4540057Losses:  2.5402636528015137 0.25794079899787903 0.8598445057868958
MemoryTrain:  epoch  2, batch     2 | loss: 2.5402637Losses:  3.027064800262451 -0.0 1.0086826086044312
MemoryTrain:  epoch  2, batch     3 | loss: 3.0270648Losses:  3.2991092205047607 -0.0 0.9583563208580017
MemoryTrain:  epoch  2, batch     4 | loss: 3.2991092Losses:  3.402259349822998 0.48229503631591797 0.849766731262207
MemoryTrain:  epoch  2, batch     5 | loss: 3.4022593Losses:  2.56221079826355 0.4841765761375427 0.794119656085968
MemoryTrain:  epoch  2, batch     6 | loss: 2.5622108Losses:  2.8039755821228027 0.23383265733718872 0.973487138748169
MemoryTrain:  epoch  2, batch     7 | loss: 2.8039756Losses:  5.696361541748047 1.7774721384048462 0.8804681301116943
MemoryTrain:  epoch  2, batch     8 | loss: 5.6963615Losses:  2.8062896728515625 0.23049455881118774 0.9236568212509155
MemoryTrain:  epoch  2, batch     9 | loss: 2.8062897Losses:  3.089778423309326 0.3072532117366791 0.8401406407356262
MemoryTrain:  epoch  2, batch    10 | loss: 3.0897784Losses:  1.580915093421936 -0.0 0.30772900581359863
MemoryTrain:  epoch  2, batch    11 | loss: 1.5809151Losses:  2.647810220718384 0.4889135956764221 0.7947003245353699
MemoryTrain:  epoch  3, batch     0 | loss: 2.6478102Losses:  3.7209630012512207 0.5339387655258179 0.8444613814353943
MemoryTrain:  epoch  3, batch     1 | loss: 3.7209630Losses:  2.795808792114258 -0.0 0.9678463935852051
MemoryTrain:  epoch  3, batch     2 | loss: 2.7958088Losses:  3.640024423599243 0.5499125719070435 0.9212157130241394
MemoryTrain:  epoch  3, batch     3 | loss: 3.6400244Losses:  2.980006456375122 0.5262429714202881 0.7867441177368164
MemoryTrain:  epoch  3, batch     4 | loss: 2.9800065Losses:  3.1907670497894287 0.27576881647109985 0.9561805725097656
MemoryTrain:  epoch  3, batch     5 | loss: 3.1907670Losses:  2.2669036388397217 0.25328344106674194 0.7209656834602356
MemoryTrain:  epoch  3, batch     6 | loss: 2.2669036Losses:  2.9248175621032715 0.5888683795928955 0.7794517278671265
MemoryTrain:  epoch  3, batch     7 | loss: 2.9248176Losses:  3.428497314453125 0.7569820880889893 0.7114406228065491
MemoryTrain:  epoch  3, batch     8 | loss: 3.4284973Losses:  2.6930131912231445 0.47481343150138855 0.8273619413375854
MemoryTrain:  epoch  3, batch     9 | loss: 2.6930132Losses:  3.444484233856201 0.49208760261535645 0.9728451371192932
MemoryTrain:  epoch  3, batch    10 | loss: 3.4444842Losses:  1.8323259353637695 0.25333714485168457 0.22580379247665405
MemoryTrain:  epoch  3, batch    11 | loss: 1.8323259Losses:  2.501182794570923 -0.0 0.9343547224998474
MemoryTrain:  epoch  4, batch     0 | loss: 2.5011828Losses:  2.7979416847229004 0.324872225522995 0.6948138475418091
MemoryTrain:  epoch  4, batch     1 | loss: 2.7979417Losses:  2.6738839149475098 -0.0 0.9753412008285522
MemoryTrain:  epoch  4, batch     2 | loss: 2.6738839Losses:  3.4156012535095215 1.0052192211151123 0.8604648113250732
MemoryTrain:  epoch  4, batch     3 | loss: 3.4156013Losses:  3.6140310764312744 0.4534201920032501 0.9285697340965271
MemoryTrain:  epoch  4, batch     4 | loss: 3.6140311Losses:  3.2230641841888428 0.7247512340545654 0.8184053301811218
MemoryTrain:  epoch  4, batch     5 | loss: 3.2230642Losses:  3.1546475887298584 0.6092278957366943 0.7105278372764587
MemoryTrain:  epoch  4, batch     6 | loss: 3.1546476Losses:  3.094933271408081 0.7642958164215088 0.7987152934074402
MemoryTrain:  epoch  4, batch     7 | loss: 3.0949333Losses:  2.8959336280822754 0.7637644410133362 0.7273424863815308
MemoryTrain:  epoch  4, batch     8 | loss: 2.8959336Losses:  3.1382391452789307 0.6921288967132568 0.7972834706306458
MemoryTrain:  epoch  4, batch     9 | loss: 3.1382391Losses:  3.1117100715637207 0.25317174196243286 0.9678418636322021
MemoryTrain:  epoch  4, batch    10 | loss: 3.1117101Losses:  1.5747768878936768 -0.0 0.34078848361968994
MemoryTrain:  epoch  4, batch    11 | loss: 1.5747769Losses:  2.765986680984497 0.2458375096321106 0.9537803530693054
MemoryTrain:  epoch  5, batch     0 | loss: 2.7659867Losses:  2.546447992324829 0.5004517436027527 0.7847470641136169
MemoryTrain:  epoch  5, batch     1 | loss: 2.5464480Losses:  2.373504400253296 0.2479429841041565 0.8433980941772461
MemoryTrain:  epoch  5, batch     2 | loss: 2.3735044Losses:  2.673923969268799 0.48111534118652344 0.7232767343521118
MemoryTrain:  epoch  5, batch     3 | loss: 2.6739240Losses:  3.0049850940704346 0.5168794393539429 0.7809721827507019
MemoryTrain:  epoch  5, batch     4 | loss: 3.0049851Losses:  3.8767809867858887 0.6539362072944641 1.0433905124664307
MemoryTrain:  epoch  5, batch     5 | loss: 3.8767810Losses:  2.611215829849243 -0.0 0.8669650554656982
MemoryTrain:  epoch  5, batch     6 | loss: 2.6112158Losses:  4.159265518188477 1.0710787773132324 0.7542526721954346
MemoryTrain:  epoch  5, batch     7 | loss: 4.1592655Losses:  2.56654691696167 0.4943051338195801 0.7724474668502808
MemoryTrain:  epoch  5, batch     8 | loss: 2.5665469Losses:  2.9366061687469482 0.5198619961738586 0.963453471660614
MemoryTrain:  epoch  5, batch     9 | loss: 2.9366062Losses:  3.045412063598633 0.24744561314582825 0.8619629740715027
MemoryTrain:  epoch  5, batch    10 | loss: 3.0454121Losses:  1.4094802141189575 -0.0 0.23651526868343353
MemoryTrain:  epoch  5, batch    11 | loss: 1.4094802Losses:  2.352609634399414 0.24565179646015167 0.7772855758666992
MemoryTrain:  epoch  6, batch     0 | loss: 2.3526096Losses:  2.3848114013671875 0.258617639541626 0.8494881391525269
MemoryTrain:  epoch  6, batch     1 | loss: 2.3848114Losses:  2.3998589515686035 -0.0 0.8658773303031921
MemoryTrain:  epoch  6, batch     2 | loss: 2.3998590Losses:  2.1345348358154297 -0.0 0.9090937972068787
MemoryTrain:  epoch  6, batch     3 | loss: 2.1345348Losses:  2.6193442344665527 0.5332248210906982 0.8481187224388123
MemoryTrain:  epoch  6, batch     4 | loss: 2.6193442Losses:  2.8963515758514404 -0.0 1.0872668027877808
MemoryTrain:  epoch  6, batch     5 | loss: 2.8963516Losses:  3.079787492752075 0.5407552719116211 0.8443679809570312
MemoryTrain:  epoch  6, batch     6 | loss: 3.0797875Losses:  3.69210147857666 0.7643060088157654 0.8649926781654358
MemoryTrain:  epoch  6, batch     7 | loss: 3.6921015Losses:  3.259990692138672 0.724314272403717 0.8429622650146484
MemoryTrain:  epoch  6, batch     8 | loss: 3.2599907Losses:  3.777575969696045 1.1401481628417969 0.7194691896438599
MemoryTrain:  epoch  6, batch     9 | loss: 3.7775760Losses:  2.9691977500915527 0.24019020795822144 0.9425439238548279
MemoryTrain:  epoch  6, batch    10 | loss: 2.9691978Losses:  1.6270382404327393 -0.0 0.3172934353351593
MemoryTrain:  epoch  6, batch    11 | loss: 1.6270382Losses:  2.2473409175872803 -0.0 0.8414080739021301
MemoryTrain:  epoch  7, batch     0 | loss: 2.2473409Losses:  2.4566025733947754 0.4963359832763672 0.7255921363830566
MemoryTrain:  epoch  7, batch     1 | loss: 2.4566026Losses:  2.7259721755981445 0.5329126119613647 0.9519463777542114
MemoryTrain:  epoch  7, batch     2 | loss: 2.7259722Losses:  3.7052905559539795 1.025862693786621 0.8853859901428223
MemoryTrain:  epoch  7, batch     3 | loss: 3.7052906Losses:  2.7108662128448486 0.23964303731918335 0.7528752684593201
MemoryTrain:  epoch  7, batch     4 | loss: 2.7108662Losses:  2.4037435054779053 0.21971061825752258 0.7746200561523438
MemoryTrain:  epoch  7, batch     5 | loss: 2.4037435Losses:  2.8249435424804688 0.28654903173446655 0.9052774310112
MemoryTrain:  epoch  7, batch     6 | loss: 2.8249435Losses:  2.6439085006713867 0.23109248280525208 0.8556779026985168
MemoryTrain:  epoch  7, batch     7 | loss: 2.6439085Losses:  2.5274839401245117 0.5272719860076904 0.7820250391960144
MemoryTrain:  epoch  7, batch     8 | loss: 2.5274839Losses:  2.7696683406829834 0.4346941411495209 0.8988551497459412
MemoryTrain:  epoch  7, batch     9 | loss: 2.7696683Losses:  2.742276906967163 0.2517649829387665 0.7981416583061218
MemoryTrain:  epoch  7, batch    10 | loss: 2.7422769Losses:  1.7440557479858398 0.25360164046287537 0.23909887671470642
MemoryTrain:  epoch  7, batch    11 | loss: 1.7440557Losses:  2.520332098007202 0.2566739320755005 0.7761423587799072
MemoryTrain:  epoch  8, batch     0 | loss: 2.5203321Losses:  2.528355121612549 0.2509879767894745 1.0678977966308594
MemoryTrain:  epoch  8, batch     1 | loss: 2.5283551Losses:  2.9000778198242188 0.4977686405181885 0.8405661582946777
MemoryTrain:  epoch  8, batch     2 | loss: 2.9000778Losses:  2.4321484565734863 0.24942371249198914 0.9582468271255493
MemoryTrain:  epoch  8, batch     3 | loss: 2.4321485Losses:  2.864023447036743 -0.0 0.912594735622406
MemoryTrain:  epoch  8, batch     4 | loss: 2.8640234Losses:  2.395717144012451 0.24511826038360596 0.9196518659591675
MemoryTrain:  epoch  8, batch     5 | loss: 2.3957171Losses:  3.110809803009033 0.7857675552368164 0.8706088066101074
MemoryTrain:  epoch  8, batch     6 | loss: 3.1108098Losses:  2.828131675720215 0.7479802370071411 0.8529467582702637
MemoryTrain:  epoch  8, batch     7 | loss: 2.8281317Losses:  2.8716726303100586 0.5158727169036865 0.8520728349685669
MemoryTrain:  epoch  8, batch     8 | loss: 2.8716726Losses:  1.9857854843139648 -0.0 0.704960823059082
MemoryTrain:  epoch  8, batch     9 | loss: 1.9857855Losses:  2.4498612880706787 -0.0 0.8718045353889465
MemoryTrain:  epoch  8, batch    10 | loss: 2.4498613Losses:  1.709250569343567 -0.0 0.30026498436927795
MemoryTrain:  epoch  8, batch    11 | loss: 1.7092506Losses:  2.4776203632354736 0.5200608968734741 0.7722951769828796
MemoryTrain:  epoch  9, batch     0 | loss: 2.4776204Losses:  2.5823593139648438 0.2661028802394867 0.9114995002746582
MemoryTrain:  epoch  9, batch     1 | loss: 2.5823593Losses:  2.7925825119018555 0.2474319487810135 0.9007635116577148
MemoryTrain:  epoch  9, batch     2 | loss: 2.7925825Losses:  2.461958169937134 0.26258793473243713 0.8712039589881897
MemoryTrain:  epoch  9, batch     3 | loss: 2.4619582Losses:  2.6378304958343506 0.7255460023880005 0.707626461982727
MemoryTrain:  epoch  9, batch     4 | loss: 2.6378305Losses:  2.572040557861328 0.48643216490745544 0.8918150663375854
MemoryTrain:  epoch  9, batch     5 | loss: 2.5720406Losses:  2.713808059692383 0.2398761510848999 0.9056267738342285
MemoryTrain:  epoch  9, batch     6 | loss: 2.7138081Losses:  2.883378505706787 0.4918630123138428 0.7223805785179138
MemoryTrain:  epoch  9, batch     7 | loss: 2.8833785Losses:  2.273329257965088 -0.0 0.960214376449585
MemoryTrain:  epoch  9, batch     8 | loss: 2.2733293Losses:  2.491992235183716 0.21559439599514008 0.9739514589309692
MemoryTrain:  epoch  9, batch     9 | loss: 2.4919922Losses:  3.022751569747925 0.6434366106987 0.7864007353782654
MemoryTrain:  epoch  9, batch    10 | loss: 3.0227516Losses:  1.6135910749435425 -0.0 0.34066450595855713
MemoryTrain:  epoch  9, batch    11 | loss: 1.6135911
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 25.00%,  total acc: 30.00%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 30.21%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 35.71%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 42.19%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 47.92%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 51.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 55.11%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 59.62%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 61.16%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 62.92%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 67.01%   [EVAL] batch:   18 | acc: 62.50%,  total acc: 66.78%   [EVAL] batch:   19 | acc: 25.00%,  total acc: 64.69%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:   21 | acc: 12.50%,  total acc: 60.23%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 58.42%   [EVAL] batch:   23 | acc: 18.75%,  total acc: 56.77%   [EVAL] batch:   24 | acc: 31.25%,  total acc: 55.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 53.85%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 52.08%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 50.89%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 49.14%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 47.71%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 46.17%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 46.48%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 47.73%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 48.90%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 50.36%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 51.56%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 52.03%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 53.12%   [EVAL] batch:   38 | acc: 93.75%,  total acc: 54.17%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 55.31%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 56.10%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 56.85%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 57.56%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 58.52%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 59.17%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 59.51%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 59.97%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 60.29%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 60.84%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 61.12%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 60.17%   [EVAL] batch:   51 | acc: 31.25%,  total acc: 59.62%   [EVAL] batch:   52 | acc: 31.25%,  total acc: 59.08%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 58.91%   [EVAL] batch:   54 | acc: 37.50%,  total acc: 58.52%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 58.48%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 58.22%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 58.19%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 57.84%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 58.02%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 57.89%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 58.27%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 57.84%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 74.31%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 73.12%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 69.79%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 70.67%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.39%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.47%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.43%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.62%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 79.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 79.26%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 79.35%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 79.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.53%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.92%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.33%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 83.98%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 84.47%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 84.11%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 84.20%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 84.46%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.87%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.26%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.31%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 86.34%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.51%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 86.53%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 86.55%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 86.44%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 86.33%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 86.50%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 86.40%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 86.42%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.56%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 86.23%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 85.68%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 85.31%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 84.81%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 84.64%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 84.69%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 84.63%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 84.42%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 84.23%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 84.24%   [EVAL] batch:   67 | acc: 68.75%,  total acc: 84.01%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 83.88%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 83.93%   [EVAL] batch:   70 | acc: 62.50%,  total acc: 83.63%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 83.25%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 83.13%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 82.94%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 82.83%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 82.87%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 82.69%   [EVAL] batch:   78 | acc: 81.25%,  total acc: 82.67%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 82.58%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 82.79%   [EVAL] batch:   81 | acc: 56.25%,  total acc: 82.47%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 82.38%   [EVAL] batch:   83 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:   84 | acc: 68.75%,  total acc: 82.13%   [EVAL] batch:   85 | acc: 81.25%,  total acc: 82.12%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 82.33%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 82.46%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 82.65%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 82.85%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 83.15%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 83.44%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 83.62%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 83.96%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 84.12%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 84.28%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 84.44%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 84.16%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 83.95%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 83.74%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 83.47%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 83.51%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 83.31%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 82.94%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 82.35%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 81.94%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 81.36%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 80.97%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 80.36%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 80.20%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 80.26%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.43%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 80.44%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 80.56%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 80.51%   [EVAL] batch:  119 | acc: 6.25%,  total acc: 79.90%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 79.39%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 78.74%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 78.20%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 77.57%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 77.00%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 76.88%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 76.62%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 76.46%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 76.07%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 75.82%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 75.52%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 75.62%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 75.80%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 75.84%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 75.88%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 75.97%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 76.14%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 76.18%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 75.99%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 75.85%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 75.89%   [EVAL] batch:  141 | acc: 75.00%,  total acc: 75.88%   [EVAL] batch:  142 | acc: 56.25%,  total acc: 75.74%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 75.69%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 75.60%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 75.60%   [EVAL] batch:  146 | acc: 56.25%,  total acc: 75.47%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 75.46%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 75.42%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 75.38%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 75.37%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 75.37%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 75.41%   [EVAL] batch:  153 | acc: 81.25%,  total acc: 75.45%   [EVAL] batch:  154 | acc: 75.00%,  total acc: 75.44%   [EVAL] batch:  155 | acc: 62.50%,  total acc: 75.36%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 75.28%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 75.16%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 74.84%   [EVAL] batch:  160 | acc: 56.25%,  total acc: 74.73%   [EVAL] batch:  161 | acc: 43.75%,  total acc: 74.54%   [EVAL] batch:  162 | acc: 50.00%,  total acc: 74.39%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 74.28%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 74.20%   [EVAL] batch:  165 | acc: 62.50%,  total acc: 74.13%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 74.06%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 74.00%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 73.85%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 73.64%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 73.43%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 73.22%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 73.05%   [EVAL] batch:  173 | acc: 37.50%,  total acc: 72.84%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 72.68%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 72.83%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 73.29%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 73.73%   [EVAL] batch:  182 | acc: 75.00%,  total acc: 73.74%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 73.85%   [EVAL] batch:  184 | acc: 93.75%,  total acc: 73.95%   [EVAL] batch:  185 | acc: 81.25%,  total acc: 73.99%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 74.06%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 74.04%   [EVAL] batch:  188 | acc: 37.50%,  total acc: 73.84%   [EVAL] batch:  189 | acc: 50.00%,  total acc: 73.72%   [EVAL] batch:  190 | acc: 43.75%,  total acc: 73.56%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 73.31%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 73.19%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 72.94%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 72.95%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 72.83%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 72.84%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 72.90%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 72.91%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 72.90%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 72.88%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 72.86%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 72.90%   [EVAL] batch:  205 | acc: 56.25%,  total acc: 72.82%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 73.27%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 73.40%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 73.59%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 73.84%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 74.08%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 74.20%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 74.32%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 74.43%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 74.61%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 74.69%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 74.92%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 74.89%   [EVAL] batch:  226 | acc: 31.25%,  total acc: 74.70%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 74.62%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 74.48%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 74.24%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 74.30%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 74.38%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 74.47%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:  235 | acc: 81.25%,  total acc: 74.60%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 74.68%   [EVAL] batch:  237 | acc: 75.00%,  total acc: 74.68%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 74.66%   [EVAL] batch:  239 | acc: 50.00%,  total acc: 74.56%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 74.51%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 74.51%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 74.43%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 74.31%   [EVAL] batch:  244 | acc: 81.25%,  total acc: 74.34%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 74.34%   [EVAL] batch:  246 | acc: 56.25%,  total acc: 74.27%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 74.29%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 74.35%   [EVAL] batch:  249 | acc: 68.75%,  total acc: 74.33%   [EVAL] batch:  250 | acc: 87.50%,  total acc: 74.38%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 74.38%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 74.41%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 74.46%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 74.56%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 74.46%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 74.47%   [EVAL] batch:  258 | acc: 56.25%,  total acc: 74.40%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 74.28%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 74.24%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 74.22%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 74.22%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 74.20%   [EVAL] batch:  265 | acc: 56.25%,  total acc: 74.13%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 74.11%   [EVAL] batch:  267 | acc: 56.25%,  total acc: 74.04%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 74.02%   [EVAL] batch:  269 | acc: 62.50%,  total acc: 73.98%   [EVAL] batch:  270 | acc: 62.50%,  total acc: 73.94%   [EVAL] batch:  271 | acc: 62.50%,  total acc: 73.90%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 73.88%   [EVAL] batch:  273 | acc: 75.00%,  total acc: 73.88%   [EVAL] batch:  274 | acc: 43.75%,  total acc: 73.77%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 73.87%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 73.96%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 74.06%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 74.15%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 74.24%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 74.33%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 74.31%   [EVAL] batch:  282 | acc: 62.50%,  total acc: 74.27%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 74.23%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 74.21%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 74.17%   [EVAL] batch:  286 | acc: 62.50%,  total acc: 74.13%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 74.09%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 74.18%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 74.27%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 74.36%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 74.44%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 74.53%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 74.60%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 74.85%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 75.02%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 75.10%   [EVAL] batch:  300 | acc: 75.00%,  total acc: 75.10%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 75.12%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 75.19%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 75.21%   [EVAL] batch:  304 | acc: 75.00%,  total acc: 75.20%   [EVAL] batch:  305 | acc: 87.50%,  total acc: 75.25%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 75.29%   [EVAL] batch:  307 | acc: 75.00%,  total acc: 75.28%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 75.34%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 75.42%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 75.50%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 75.54%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 75.50%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 75.34%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 75.20%   [EVAL] batch:  315 | acc: 31.25%,  total acc: 75.06%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 74.92%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 74.74%   [EVAL] batch:  318 | acc: 56.25%,  total acc: 74.69%   [EVAL] batch:  319 | acc: 81.25%,  total acc: 74.71%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 74.75%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 74.79%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 74.85%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 74.86%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 74.88%   [EVAL] batch:  325 | acc: 100.00%,  total acc: 74.96%   [EVAL] batch:  326 | acc: 68.75%,  total acc: 74.94%   [EVAL] batch:  327 | acc: 100.00%,  total acc: 75.02%   [EVAL] batch:  328 | acc: 93.75%,  total acc: 75.08%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 75.08%   [EVAL] batch:  330 | acc: 93.75%,  total acc: 75.13%   [EVAL] batch:  331 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:  332 | acc: 18.75%,  total acc: 74.83%   [EVAL] batch:  333 | acc: 18.75%,  total acc: 74.66%   [EVAL] batch:  334 | acc: 12.50%,  total acc: 74.48%   [EVAL] batch:  335 | acc: 6.25%,  total acc: 74.27%   [EVAL] batch:  336 | acc: 25.00%,  total acc: 74.13%   [EVAL] batch:  337 | acc: 31.25%,  total acc: 74.00%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 73.80%   [EVAL] batch:  339 | acc: 18.75%,  total acc: 73.64%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 73.42%   [EVAL] batch:  341 | acc: 0.00%,  total acc: 73.21%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 73.01%   [EVAL] batch:  343 | acc: 12.50%,  total acc: 72.84%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 72.86%   [EVAL] batch:  345 | acc: 87.50%,  total acc: 72.90%   [EVAL] batch:  346 | acc: 100.00%,  total acc: 72.98%   [EVAL] batch:  347 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:  348 | acc: 87.50%,  total acc: 73.10%   [EVAL] batch:  349 | acc: 68.75%,  total acc: 73.09%   [EVAL] batch:  350 | acc: 100.00%,  total acc: 73.17%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 73.22%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 73.26%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 73.32%   [EVAL] batch:  354 | acc: 87.50%,  total acc: 73.36%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 73.42%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 73.48%   [EVAL] batch:  357 | acc: 81.25%,  total acc: 73.50%   [EVAL] batch:  358 | acc: 68.75%,  total acc: 73.49%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 73.52%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 73.55%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 73.58%   [EVAL] batch:  362 | acc: 37.50%,  total acc: 73.48%   [EVAL] batch:  363 | acc: 18.75%,  total acc: 73.33%   [EVAL] batch:  364 | acc: 31.25%,  total acc: 73.22%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 73.16%   [EVAL] batch:  366 | acc: 37.50%,  total acc: 73.06%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 73.00%   [EVAL] batch:  368 | acc: 43.75%,  total acc: 72.92%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 72.85%   [EVAL] batch:  370 | acc: 50.00%,  total acc: 72.79%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 72.72%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 72.69%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 72.71%   [EVAL] batch:  374 | acc: 62.50%,  total acc: 72.68%   
cur_acc:  ['0.9286', '0.7817', '0.7490', '0.7530', '0.8393', '0.5784']
his_acc:  ['0.9286', '0.8615', '0.8045', '0.7590', '0.7656', '0.7268']
Clustering into  34  clusters
Clusters:  [ 7 10 19  6  1  1 24  8 10  2 23 16 22 18 25  5 20  0  3  0  0 15 15  1
 14  2  7  1  5  5 27  5  0 31 33  5  0 16  2  7  6  7  7 10 29  3 30 32
  8 10  1 18 12 14  4  9 28 11 16 10  3 26 21 17  4  1 12  7 13  3]
Losses:  10.725720405578613 3.8921475410461426 0.579416036605835
CurrentTrain: epoch  0, batch     0 | loss: 10.7257204Losses:  10.697945594787598 3.6090750694274902 0.7239311933517456
CurrentTrain: epoch  0, batch     1 | loss: 10.6979456Losses:  10.444843292236328 4.170664310455322 0.7047550678253174
CurrentTrain: epoch  0, batch     2 | loss: 10.4448433Losses:  6.694816589355469 -0.0 0.12012463808059692
CurrentTrain: epoch  0, batch     3 | loss: 6.6948166Losses:  11.571715354919434 5.018180847167969 0.6970309019088745
CurrentTrain: epoch  1, batch     0 | loss: 11.5717154Losses:  9.10788631439209 2.730717182159424 0.7383013963699341
CurrentTrain: epoch  1, batch     1 | loss: 9.1078863Losses:  8.684263229370117 3.8966994285583496 0.5880051851272583
CurrentTrain: epoch  1, batch     2 | loss: 8.6842632Losses:  8.846317291259766 -0.0 0.1909928321838379
CurrentTrain: epoch  1, batch     3 | loss: 8.8463173Losses:  8.342620849609375 2.49174165725708 0.742303192615509
CurrentTrain: epoch  2, batch     0 | loss: 8.3426208Losses:  9.102086067199707 3.312411308288574 0.7602049112319946
CurrentTrain: epoch  2, batch     1 | loss: 9.1020861Losses:  8.163753509521484 3.1758127212524414 0.682105541229248
CurrentTrain: epoch  2, batch     2 | loss: 8.1637535Losses:  4.7689642906188965 -0.0 0.13230177760124207
CurrentTrain: epoch  2, batch     3 | loss: 4.7689643Losses:  8.51286792755127 3.331691265106201 0.6686090230941772
CurrentTrain: epoch  3, batch     0 | loss: 8.5128679Losses:  9.479912757873535 4.15866231918335 0.6533613204956055
CurrentTrain: epoch  3, batch     1 | loss: 9.4799128Losses:  8.55632209777832 3.495616912841797 0.6487621665000916
CurrentTrain: epoch  3, batch     2 | loss: 8.5563221Losses:  3.3683009147644043 -0.0 0.12442720681428909
CurrentTrain: epoch  3, batch     3 | loss: 3.3683009Losses:  10.706178665161133 5.275300979614258 0.6458425521850586
CurrentTrain: epoch  4, batch     0 | loss: 10.7061787Losses:  8.472481727600098 3.6828866004943848 0.7133980989456177
CurrentTrain: epoch  4, batch     1 | loss: 8.4724817Losses:  7.814455986022949 2.986309289932251 0.7345304489135742
CurrentTrain: epoch  4, batch     2 | loss: 7.8144560Losses:  2.104745388031006 -0.0 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 2.1047454Losses:  8.396384239196777 3.2847237586975098 0.678803563117981
CurrentTrain: epoch  5, batch     0 | loss: 8.3963842Losses:  5.716391563415527 2.2957935333251953 0.6507513523101807
CurrentTrain: epoch  5, batch     1 | loss: 5.7163916Losses:  8.727205276489258 3.559650421142578 0.6823804974555969
CurrentTrain: epoch  5, batch     2 | loss: 8.7272053Losses:  4.079527854919434 -0.0 0.1294449269771576
CurrentTrain: epoch  5, batch     3 | loss: 4.0795279Losses:  7.753399848937988 2.8441829681396484 0.6875912547111511
CurrentTrain: epoch  6, batch     0 | loss: 7.7533998Losses:  6.833152770996094 3.4753551483154297 0.5718046426773071
CurrentTrain: epoch  6, batch     1 | loss: 6.8331528Losses:  8.789809226989746 3.9434754848480225 0.6868846416473389
CurrentTrain: epoch  6, batch     2 | loss: 8.7898092Losses:  4.141529083251953 -0.0 0.11292734742164612
CurrentTrain: epoch  6, batch     3 | loss: 4.1415291Losses:  8.655701637268066 4.640848636627197 0.6060675382614136
CurrentTrain: epoch  7, batch     0 | loss: 8.6557016Losses:  7.344205856323242 3.501591444015503 0.6722232699394226
CurrentTrain: epoch  7, batch     1 | loss: 7.3442059Losses:  8.19640827178955 3.9561994075775146 0.6789883375167847
CurrentTrain: epoch  7, batch     2 | loss: 8.1964083Losses:  3.92512845993042 -0.0 0.10993638634681702
CurrentTrain: epoch  7, batch     3 | loss: 3.9251285Losses:  6.264677047729492 2.230030059814453 0.7409537434577942
CurrentTrain: epoch  8, batch     0 | loss: 6.2646770Losses:  6.9760308265686035 2.5048067569732666 0.7760516405105591
CurrentTrain: epoch  8, batch     1 | loss: 6.9760308Losses:  5.9830217361450195 2.8465023040771484 0.5674496293067932
CurrentTrain: epoch  8, batch     2 | loss: 5.9830217Losses:  4.352329254150391 -0.0 0.1598750799894333
CurrentTrain: epoch  8, batch     3 | loss: 4.3523293Losses:  6.6603240966796875 3.2642602920532227 0.6673803329467773
CurrentTrain: epoch  9, batch     0 | loss: 6.6603241Losses:  7.509233474731445 3.0504369735717773 0.6331851482391357
CurrentTrain: epoch  9, batch     1 | loss: 7.5092335Losses:  7.212613582611084 3.5422415733337402 0.585297703742981
CurrentTrain: epoch  9, batch     2 | loss: 7.2126136Losses:  3.5242745876312256 -0.0 0.1420135200023651
CurrentTrain: epoch  9, batch     3 | loss: 3.5242746
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: the " tetrabiblos " was largely responsible for laying down the basic precepts of renaissance astrology , webster ( 1979 ) p.276 .
Head Entity: tetrabiblos
Tail Entity: astrology
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: In her groundbreaking book, "The Second Sex," Simone de Beauvoir explores the role of women in society and the concept of femininity, which has influenced feminist theory significantly.  
Head Entity: The Second Sex  
Tail Entity: feminist theory  

Relation: main subject  
Context: The documentary "Planet Earth" showcases the beauty and diversity of the natural world, focusing on various ecosystems and wildlife, which has sparked interest in environmental conservation.  
Head Entity: Planet Earth  
Tail Entity: environmental conservation  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The United States women's national soccer team has consistently performed well in international competitions, winning the FIFA Women's World Cup multiple times, including in 2019 where they defeated the Netherlands 2-0.  
Head Entity: FIFA Women's World Cup  
Tail Entity: Netherlands  

Relation: participating team  
Context: In the 2021 Tokyo Olympics, the Australian basketball team showcased their skills, ultimately facing the United States in the gold medal match, where they put up a strong fight but lost 86-69.  
Head Entity: Tokyo Olympics  
Tail Entity: United States  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: gyllenhaal was nominated for an academy award for best supporting actress for her performance .
Head Entity: gyllenhaal
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: the film was nominated for several prestigious awards, including the golden globe for best drama.  
Head Entity: the film  
Tail Entity: golden globe for best drama  

Relation: nominated for  
Context: she was nominated for the emmy award for her outstanding performance in the television series.  
Head Entity: she  
Tail Entity: emmy award for her outstanding performance in the television series  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states from invasions and raids, and is considered one of the most iconic structures in the world.  
Head Entity: Great Wall of China  
Tail Entity: fortification
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: the mount kapaz or kepez ( ) is a mountain in lesser caucasus near ganja city in central azerbaijan .
Head Entity: mount kapaz
Tail Entity: lesser caucasus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada is a mountain range in the western united states, primarily located in the state of california.  
Head Entity: sierra nevada  
Tail Entity: california  

Relation: mountain range  
Context: the alps are a major mountain range in europe, stretching across eight countries including france, germany, and italy.  
Head Entity: alps  
Tail Entity: europe  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: future films include " mcqueen " , ian bonhote ’s documentary about the fashion designer alexander mcqueen , brad anderson ’s thriller " beirut " , and mark pellington ’s drama " nostalgia " .
Head Entity: nostalgia
Tail Entity: mark pellington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the upcoming movie "the great adventure" is penned by renowned screenwriter jessica taylor, who has previously won multiple awards for her work in the industry.  
Head Entity: the great adventure  
Tail Entity: jessica taylor  

Relation: screenwriter  
Context: in the latest film festival, "the last journey" received critical acclaim, with its script written by the talented screenwriter robert king, known for his unique storytelling style.  
Head Entity: the last journey  
Tail Entity: robert king  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative that draws heavily from various Asian cultures, primarily showcasing a fictional language inspired by Chinese and Inuit languages.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Chinese and Inuit languages  

Relation: language of work or name  
Context: The novel "One Hundred Years of Solitude" by Gabriel García Márquez is originally written in Spanish and has been translated into numerous languages, making it a global literary phenomenon.  
Head Entity: One Hundred Years of Solitude  
Tail Entity: Spanish
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as the seat of the archbishop of paris, representing the roman catholic faith in the heart of the city.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the world, known for his teachings on compassion and non-violence, and is the spiritual leader of tibetan buddhism.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
Losses:  3.8289637565612793 0.2731502652168274 1.0765873193740845
MemoryTrain:  epoch  0, batch     0 | loss: 3.8289638Losses:  3.1625630855560303 0.26461976766586304 1.0426063537597656
MemoryTrain:  epoch  0, batch     1 | loss: 3.1625631Losses:  3.9387998580932617 0.543447732925415 0.8976834416389465
MemoryTrain:  epoch  0, batch     2 | loss: 3.9387999Losses:  4.122615337371826 -0.0 0.8820392489433289
MemoryTrain:  epoch  0, batch     3 | loss: 4.1226153Losses:  4.525238990783691 -0.0 1.0736078023910522
MemoryTrain:  epoch  0, batch     4 | loss: 4.5252390Losses:  4.129878997802734 0.2387896329164505 0.8952980041503906
MemoryTrain:  epoch  0, batch     5 | loss: 4.1298790Losses:  3.0739023685455322 0.5128785371780396 0.8601000905036926
MemoryTrain:  epoch  0, batch     6 | loss: 3.0739024Losses:  4.542443752288818 -0.0 0.8672457933425903
MemoryTrain:  epoch  0, batch     7 | loss: 4.5424438Losses:  4.345043182373047 0.28016066551208496 0.9814057350158691
MemoryTrain:  epoch  0, batch     8 | loss: 4.3450432Losses:  4.093072891235352 -0.0 0.991523027420044
MemoryTrain:  epoch  0, batch     9 | loss: 4.0930729Losses:  5.0136003494262695 0.2535698115825653 1.061272144317627
MemoryTrain:  epoch  0, batch    10 | loss: 5.0136003Losses:  3.088491678237915 0.23786237835884094 0.8050212860107422
MemoryTrain:  epoch  0, batch    11 | loss: 3.0884917Losses:  3.9746174812316895 0.5588263869285583 0.7972343564033508
MemoryTrain:  epoch  0, batch    12 | loss: 3.9746175Losses:  1.4751609563827515 -0.0 0.12617293000221252
MemoryTrain:  epoch  0, batch    13 | loss: 1.4751610Losses:  4.111024856567383 -0.0 0.8995444178581238
MemoryTrain:  epoch  1, batch     0 | loss: 4.1110249Losses:  4.610331058502197 0.4807146489620209 0.9829823970794678
MemoryTrain:  epoch  1, batch     1 | loss: 4.6103311Losses:  3.3421666622161865 0.24624395370483398 0.8159222602844238
MemoryTrain:  epoch  1, batch     2 | loss: 3.3421667Losses:  3.7982139587402344 0.25481870770454407 0.902542233467102
MemoryTrain:  epoch  1, batch     3 | loss: 3.7982140Losses:  2.9583327770233154 -0.0 0.9691774249076843
MemoryTrain:  epoch  1, batch     4 | loss: 2.9583328Losses:  3.0689218044281006 0.24667569994926453 0.9163231253623962
MemoryTrain:  epoch  1, batch     5 | loss: 3.0689218Losses:  4.157609462738037 0.46172744035720825 0.7260052561759949
MemoryTrain:  epoch  1, batch     6 | loss: 4.1576095Losses:  2.914581775665283 0.5214711427688599 0.8503890037536621
MemoryTrain:  epoch  1, batch     7 | loss: 2.9145818Losses:  3.280367851257324 0.4984387755393982 0.9154292941093445
MemoryTrain:  epoch  1, batch     8 | loss: 3.2803679Losses:  3.056851387023926 0.2676606774330139 0.9155201315879822
MemoryTrain:  epoch  1, batch     9 | loss: 3.0568514Losses:  4.110072612762451 0.25673985481262207 0.911920964717865
MemoryTrain:  epoch  1, batch    10 | loss: 4.1100726Losses:  3.338625907897949 -0.0 1.0232634544372559
MemoryTrain:  epoch  1, batch    11 | loss: 3.3386259Losses:  4.504734516143799 0.486358106136322 0.9166024923324585
MemoryTrain:  epoch  1, batch    12 | loss: 4.5047345Losses:  3.862598419189453 -0.0 0.15004770457744598
MemoryTrain:  epoch  1, batch    13 | loss: 3.8625984Losses:  4.347639083862305 0.6694517731666565 0.9262819290161133
MemoryTrain:  epoch  2, batch     0 | loss: 4.3476391Losses:  3.362082004547119 0.23563425242900848 0.9076930284500122
MemoryTrain:  epoch  2, batch     1 | loss: 3.3620820Losses:  4.149291515350342 0.5820856690406799 0.860248863697052
MemoryTrain:  epoch  2, batch     2 | loss: 4.1492915Losses:  3.5315744876861572 0.3225146234035492 0.9704045653343201
MemoryTrain:  epoch  2, batch     3 | loss: 3.5315745Losses:  2.8751602172851562 0.2621576189994812 0.9052966833114624
MemoryTrain:  epoch  2, batch     4 | loss: 2.8751602Losses:  3.0358104705810547 0.2627250552177429 0.8550212383270264
MemoryTrain:  epoch  2, batch     5 | loss: 3.0358105Losses:  2.8483643531799316 -0.0 0.9691979885101318
MemoryTrain:  epoch  2, batch     6 | loss: 2.8483644Losses:  3.2191317081451416 0.47680869698524475 0.8460261821746826
MemoryTrain:  epoch  2, batch     7 | loss: 3.2191317Losses:  3.325240135192871 0.4932849407196045 0.7840033769607544
MemoryTrain:  epoch  2, batch     8 | loss: 3.3252401Losses:  2.6204640865325928 -0.0 0.9018940329551697
MemoryTrain:  epoch  2, batch     9 | loss: 2.6204641Losses:  2.516789674758911 -0.0 0.9612327218055725
MemoryTrain:  epoch  2, batch    10 | loss: 2.5167897Losses:  3.879796028137207 0.27474719285964966 0.9971219897270203
MemoryTrain:  epoch  2, batch    11 | loss: 3.8797960Losses:  2.9014315605163574 -0.0 0.968619167804718
MemoryTrain:  epoch  2, batch    12 | loss: 2.9014316Losses:  1.4693903923034668 -0.0 0.1760701835155487
MemoryTrain:  epoch  2, batch    13 | loss: 1.4693904Losses:  3.1475658416748047 0.23639501631259918 1.00786292552948
MemoryTrain:  epoch  3, batch     0 | loss: 3.1475658Losses:  2.896864414215088 -0.0 0.923541784286499
MemoryTrain:  epoch  3, batch     1 | loss: 2.8968644Losses:  2.4675283432006836 -0.0 0.9123291969299316
MemoryTrain:  epoch  3, batch     2 | loss: 2.4675283Losses:  2.5108115673065186 -0.0 0.9490594267845154
MemoryTrain:  epoch  3, batch     3 | loss: 2.5108116Losses:  3.7975473403930664 1.2035921812057495 0.648201584815979
MemoryTrain:  epoch  3, batch     4 | loss: 3.7975473Losses:  3.0409789085388184 0.5145242214202881 0.8444983959197998
MemoryTrain:  epoch  3, batch     5 | loss: 3.0409789Losses:  4.421302318572998 0.46839073300361633 1.0616427659988403
MemoryTrain:  epoch  3, batch     6 | loss: 4.4213023Losses:  2.7938218116760254 0.5192615985870361 0.9581385254859924
MemoryTrain:  epoch  3, batch     7 | loss: 2.7938218Losses:  2.812803030014038 0.2723867893218994 0.907272219657898
MemoryTrain:  epoch  3, batch     8 | loss: 2.8128030Losses:  3.2677009105682373 0.345792293548584 0.8349149823188782
MemoryTrain:  epoch  3, batch     9 | loss: 3.2677009Losses:  2.554130792617798 -0.0 0.9623220562934875
MemoryTrain:  epoch  3, batch    10 | loss: 2.5541308Losses:  2.666581392288208 0.48731285333633423 0.8905832171440125
MemoryTrain:  epoch  3, batch    11 | loss: 2.6665814Losses:  3.5951056480407715 0.41639813780784607 0.9740281105041504
MemoryTrain:  epoch  3, batch    12 | loss: 3.5951056Losses:  1.4523905515670776 -0.0 0.14132575690746307
MemoryTrain:  epoch  3, batch    13 | loss: 1.4523906Losses:  2.522067070007324 0.22660678625106812 0.8503087759017944
MemoryTrain:  epoch  4, batch     0 | loss: 2.5220671Losses:  2.4187793731689453 -0.0 0.8827095031738281
MemoryTrain:  epoch  4, batch     1 | loss: 2.4187794Losses:  2.830763578414917 -0.0 0.9793955087661743
MemoryTrain:  epoch  4, batch     2 | loss: 2.8307636Losses:  2.652482032775879 0.23897108435630798 0.9395613670349121
MemoryTrain:  epoch  4, batch     3 | loss: 2.6524820Losses:  3.6932873725891113 1.0596017837524414 0.8689559698104858
MemoryTrain:  epoch  4, batch     4 | loss: 3.6932874Losses:  2.53985857963562 0.2822437882423401 0.7589273452758789
MemoryTrain:  epoch  4, batch     5 | loss: 2.5398586Losses:  2.913498640060425 0.2580093443393707 0.9689680337905884
MemoryTrain:  epoch  4, batch     6 | loss: 2.9134986Losses:  3.3811330795288086 -0.0 1.02226722240448
MemoryTrain:  epoch  4, batch     7 | loss: 3.3811331Losses:  2.8989064693450928 0.2830730080604553 1.026745319366455
MemoryTrain:  epoch  4, batch     8 | loss: 2.8989065Losses:  3.0699899196624756 0.48682913184165955 0.963277280330658
MemoryTrain:  epoch  4, batch     9 | loss: 3.0699899Losses:  2.747511863708496 0.26036763191223145 0.9505122900009155
MemoryTrain:  epoch  4, batch    10 | loss: 2.7475119Losses:  2.4695637226104736 -0.0 0.8899185657501221
MemoryTrain:  epoch  4, batch    11 | loss: 2.4695637Losses:  3.3600335121154785 0.8367888927459717 0.8951688408851624
MemoryTrain:  epoch  4, batch    12 | loss: 3.3600335Losses:  1.2828246355056763 -0.0 0.11746133118867874
MemoryTrain:  epoch  4, batch    13 | loss: 1.2828246Losses:  2.1799893379211426 -0.0 0.8238735198974609
MemoryTrain:  epoch  5, batch     0 | loss: 2.1799893Losses:  3.3439104557037354 0.25860705971717834 1.0256195068359375
MemoryTrain:  epoch  5, batch     1 | loss: 3.3439105Losses:  2.7359721660614014 0.26315945386886597 0.824654757976532
MemoryTrain:  epoch  5, batch     2 | loss: 2.7359722Losses:  3.5224735736846924 0.8914629817008972 0.9810588955879211
MemoryTrain:  epoch  5, batch     3 | loss: 3.5224736Losses:  2.9127097129821777 0.2701236605644226 0.9891825914382935
MemoryTrain:  epoch  5, batch     4 | loss: 2.9127097Losses:  2.946075916290283 0.24474218487739563 0.8581547737121582
MemoryTrain:  epoch  5, batch     5 | loss: 2.9460759Losses:  3.428436279296875 0.4682912230491638 1.0058382749557495
MemoryTrain:  epoch  5, batch     6 | loss: 3.4284363Losses:  3.0398104190826416 0.49307742714881897 0.9598174691200256
MemoryTrain:  epoch  5, batch     7 | loss: 3.0398104Losses:  2.641895294189453 0.2512681186199188 1.0110528469085693
MemoryTrain:  epoch  5, batch     8 | loss: 2.6418953Losses:  2.713682174682617 0.5085386037826538 0.8535380363464355
MemoryTrain:  epoch  5, batch     9 | loss: 2.7136822Losses:  2.620577812194824 -0.0 1.0820852518081665
MemoryTrain:  epoch  5, batch    10 | loss: 2.6205778Losses:  3.1319682598114014 0.6179014444351196 0.8595187067985535
MemoryTrain:  epoch  5, batch    11 | loss: 3.1319683Losses:  2.5502114295959473 0.2931961119174957 0.6744049191474915
MemoryTrain:  epoch  5, batch    12 | loss: 2.5502114Losses:  1.5741934776306152 -0.0 0.0938572883605957
MemoryTrain:  epoch  5, batch    13 | loss: 1.5741935Losses:  3.1100006103515625 -0.0 0.9732246994972229
MemoryTrain:  epoch  6, batch     0 | loss: 3.1100006Losses:  2.701014518737793 0.49189189076423645 0.8854011297225952
MemoryTrain:  epoch  6, batch     1 | loss: 2.7010145Losses:  3.3391127586364746 0.7939390540122986 0.922872006893158
MemoryTrain:  epoch  6, batch     2 | loss: 3.3391128Losses:  2.6829113960266113 0.25134676694869995 0.9240851402282715
MemoryTrain:  epoch  6, batch     3 | loss: 2.6829114Losses:  2.7563130855560303 -0.0 1.037470817565918
MemoryTrain:  epoch  6, batch     4 | loss: 2.7563131Losses:  2.311267852783203 -0.0 1.0220698118209839
MemoryTrain:  epoch  6, batch     5 | loss: 2.3112679Losses:  2.4414515495300293 -0.0 1.0138447284698486
MemoryTrain:  epoch  6, batch     6 | loss: 2.4414515Losses:  2.519181251525879 -0.0 0.9064368009567261
MemoryTrain:  epoch  6, batch     7 | loss: 2.5191813Losses:  2.367119073867798 -0.0 0.8699104189872742
MemoryTrain:  epoch  6, batch     8 | loss: 2.3671191Losses:  2.922607660293579 0.24579676985740662 1.0831106901168823
MemoryTrain:  epoch  6, batch     9 | loss: 2.9226077Losses:  2.7568633556365967 0.26581400632858276 0.92253178358078
MemoryTrain:  epoch  6, batch    10 | loss: 2.7568634Losses:  3.1724135875701904 0.4967263340950012 0.9269129633903503
MemoryTrain:  epoch  6, batch    11 | loss: 3.1724136Losses:  4.01131534576416 1.2037768363952637 0.9126573801040649
MemoryTrain:  epoch  6, batch    12 | loss: 4.0113153Losses:  1.317416787147522 -0.0 0.1232144832611084
MemoryTrain:  epoch  6, batch    13 | loss: 1.3174168Losses:  2.3484036922454834 0.2238694429397583 0.9468384981155396
MemoryTrain:  epoch  7, batch     0 | loss: 2.3484037Losses:  2.2146692276000977 -0.0 0.9491137266159058
MemoryTrain:  epoch  7, batch     1 | loss: 2.2146692Losses:  3.0201046466827393 0.3120664060115814 0.8534622192382812
MemoryTrain:  epoch  7, batch     2 | loss: 3.0201046Losses:  2.5590295791625977 0.2615342140197754 0.905143141746521
MemoryTrain:  epoch  7, batch     3 | loss: 2.5590296Losses:  2.9963459968566895 0.4820726811885834 1.0202397108078003
MemoryTrain:  epoch  7, batch     4 | loss: 2.9963460Losses:  2.9235897064208984 0.28671756386756897 0.8516219854354858
MemoryTrain:  epoch  7, batch     5 | loss: 2.9235897Losses:  2.5624470710754395 0.25796860456466675 0.8907980918884277
MemoryTrain:  epoch  7, batch     6 | loss: 2.5624471Losses:  2.3912434577941895 -0.0 0.8627656698226929
MemoryTrain:  epoch  7, batch     7 | loss: 2.3912435Losses:  2.9219088554382324 0.36044469475746155 0.966286838054657
MemoryTrain:  epoch  7, batch     8 | loss: 2.9219089Losses:  3.222275495529175 0.6294612884521484 0.8398156762123108
MemoryTrain:  epoch  7, batch     9 | loss: 3.2222755Losses:  2.7466115951538086 0.4922536015510559 0.9940838813781738
MemoryTrain:  epoch  7, batch    10 | loss: 2.7466116Losses:  2.9067318439483643 0.24300161004066467 1.0171009302139282
MemoryTrain:  epoch  7, batch    11 | loss: 2.9067318Losses:  2.545703172683716 0.5104552507400513 0.7862445712089539
MemoryTrain:  epoch  7, batch    12 | loss: 2.5457032Losses:  1.2717094421386719 -0.0 0.13396026194095612
MemoryTrain:  epoch  7, batch    13 | loss: 1.2717094Losses:  3.1622490882873535 1.0211752653121948 0.8314766883850098
MemoryTrain:  epoch  8, batch     0 | loss: 3.1622491Losses:  2.277074098587036 -0.0 0.960647702217102
MemoryTrain:  epoch  8, batch     1 | loss: 2.2770741Losses:  2.742227792739868 -0.0 0.9709499478340149
MemoryTrain:  epoch  8, batch     2 | loss: 2.7422278Losses:  2.718951463699341 0.44992268085479736 0.9055806994438171
MemoryTrain:  epoch  8, batch     3 | loss: 2.7189515Losses:  2.510319232940674 -0.0 0.8627030849456787
MemoryTrain:  epoch  8, batch     4 | loss: 2.5103192Losses:  3.1999058723449707 0.8721810579299927 0.8026062250137329
MemoryTrain:  epoch  8, batch     5 | loss: 3.1999059Losses:  2.357855796813965 -0.0 0.848773717880249
MemoryTrain:  epoch  8, batch     6 | loss: 2.3578558Losses:  2.539548873901367 0.24308761954307556 0.9224943518638611
MemoryTrain:  epoch  8, batch     7 | loss: 2.5395489Losses:  2.591963768005371 0.4699590802192688 0.8494704961776733
MemoryTrain:  epoch  8, batch     8 | loss: 2.5919638Losses:  2.8114070892333984 0.5084294080734253 1.0253311395645142
MemoryTrain:  epoch  8, batch     9 | loss: 2.8114071Losses:  2.3820133209228516 -0.0 0.849524199962616
MemoryTrain:  epoch  8, batch    10 | loss: 2.3820133Losses:  2.2108571529388428 -0.0 0.9778891801834106
MemoryTrain:  epoch  8, batch    11 | loss: 2.2108572Losses:  2.543619155883789 0.27543801069259644 1.0121344327926636
MemoryTrain:  epoch  8, batch    12 | loss: 2.5436192Losses:  1.903151512145996 -0.0 0.14848917722702026
MemoryTrain:  epoch  8, batch    13 | loss: 1.9031515Losses:  2.475705862045288 0.24674919247627258 0.9421012997627258
MemoryTrain:  epoch  9, batch     0 | loss: 2.4757059Losses:  2.6495890617370605 -0.0 1.086862325668335
MemoryTrain:  epoch  9, batch     1 | loss: 2.6495891Losses:  2.4233169555664062 -0.0 1.1100258827209473
MemoryTrain:  epoch  9, batch     2 | loss: 2.4233170Losses:  2.2822797298431396 -0.0 0.8565542101860046
MemoryTrain:  epoch  9, batch     3 | loss: 2.2822797Losses:  2.8721227645874023 0.5347349047660828 0.835990309715271
MemoryTrain:  epoch  9, batch     4 | loss: 2.8721228Losses:  2.4261112213134766 0.2743631899356842 0.88663649559021
MemoryTrain:  epoch  9, batch     5 | loss: 2.4261112Losses:  2.4190282821655273 0.24263788759708405 0.8128539323806763
MemoryTrain:  epoch  9, batch     6 | loss: 2.4190283Losses:  2.726736068725586 0.5064404010772705 0.7860644459724426
MemoryTrain:  epoch  9, batch     7 | loss: 2.7267361Losses:  2.7263059616088867 0.5041294097900391 0.8796033263206482
MemoryTrain:  epoch  9, batch     8 | loss: 2.7263060Losses:  2.3742451667785645 -0.0 0.9860931038856506
MemoryTrain:  epoch  9, batch     9 | loss: 2.3742452Losses:  2.6121935844421387 0.4807972311973572 0.8349267244338989
MemoryTrain:  epoch  9, batch    10 | loss: 2.6121936Losses:  2.3235654830932617 -0.0 1.0629385709762573
MemoryTrain:  epoch  9, batch    11 | loss: 2.3235655Losses:  2.385225296020508 0.23503924906253815 0.9532309770584106
MemoryTrain:  epoch  9, batch    12 | loss: 2.3852253Losses:  1.2746059894561768 -0.0 -0.0
MemoryTrain:  epoch  9, batch    13 | loss: 1.2746060
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 60.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 68.75%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 72.32%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 77.78%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 75.89%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 72.50%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 68.75%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 66.91%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 37.50%,  total acc: 70.43%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 68.29%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 66.07%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 65.09%   [EVAL] batch:   29 | acc: 0.00%,  total acc: 62.92%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 60.89%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 61.13%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 61.74%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 62.86%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 63.19%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 63.85%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 63.98%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 63.94%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 63.91%   [EVAL] batch:   40 | acc: 50.00%,  total acc: 63.57%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 62.80%   [EVAL] batch:   42 | acc: 43.75%,  total acc: 62.35%   [EVAL] batch:   43 | acc: 31.25%,  total acc: 61.65%   [EVAL] batch:   44 | acc: 6.25%,  total acc: 60.42%   [EVAL] batch:   45 | acc: 18.75%,  total acc: 59.51%   [EVAL] batch:   46 | acc: 0.00%,  total acc: 58.24%   [EVAL] batch:   47 | acc: 12.50%,  total acc: 57.29%   [EVAL] batch:   48 | acc: 18.75%,  total acc: 56.51%   [EVAL] batch:   49 | acc: 6.25%,  total acc: 55.50%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 55.64%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 55.89%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 56.49%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 56.71%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 57.05%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 57.37%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 57.68%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 58.19%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 58.79%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 59.27%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 59.73%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 59.88%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 59.62%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 73.44%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 67.86%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 65.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 63.64%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 63.46%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.53%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 70.96%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 74.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 74.70%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 74.72%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 74.18%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 74.74%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.45%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.06%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 80.71%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 80.90%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 81.08%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 81.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 83.18%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 83.28%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.38%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 83.29%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 82.85%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 82.68%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 82.78%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 82.62%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 82.60%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 82.69%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 82.90%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 82.16%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 82.37%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 82.02%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 81.47%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 81.14%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 81.15%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 81.05%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 80.95%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 80.85%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 80.86%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 80.77%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 80.97%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 80.51%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 80.43%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 80.45%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 80.11%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 79.77%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 79.54%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 79.39%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 79.33%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 79.19%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 79.06%   [EVAL] batch:   77 | acc: 62.50%,  total acc: 78.85%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 78.56%   [EVAL] batch:   79 | acc: 68.75%,  total acc: 78.44%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 78.40%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 77.74%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 76.88%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 76.12%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 75.22%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 74.49%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 73.78%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 73.51%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 74.10%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 74.24%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 74.52%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 74.80%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 75.26%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 75.52%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 75.77%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 76.02%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 76.26%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 76.50%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 76.30%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 76.16%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 75.97%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 75.78%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 75.89%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 75.77%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 75.41%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 74.83%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 74.43%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 73.86%   [EVAL] batch:  110 | acc: 37.50%,  total acc: 73.54%   [EVAL] batch:  111 | acc: 6.25%,  total acc: 72.94%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 72.79%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 73.22%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 73.34%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 73.46%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 73.53%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 72.92%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 72.47%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 71.88%   [EVAL] batch:  122 | acc: 18.75%,  total acc: 71.44%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 70.87%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 70.35%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 70.29%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 70.08%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 69.87%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 69.57%   [EVAL] batch:  129 | acc: 37.50%,  total acc: 69.33%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 69.04%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 69.13%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 69.36%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 69.50%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 69.54%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 69.67%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 69.97%   [EVAL] batch:  138 | acc: 43.75%,  total acc: 69.78%   [EVAL] batch:  139 | acc: 50.00%,  total acc: 69.64%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 69.72%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 69.58%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 69.53%   [EVAL] batch:  144 | acc: 68.75%,  total acc: 69.53%   [EVAL] batch:  145 | acc: 75.00%,  total acc: 69.56%   [EVAL] batch:  146 | acc: 56.25%,  total acc: 69.47%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 69.51%   [EVAL] batch:  148 | acc: 68.75%,  total acc: 69.51%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 69.54%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 69.58%   [EVAL] batch:  151 | acc: 81.25%,  total acc: 69.65%   [EVAL] batch:  152 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 69.85%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 69.84%   [EVAL] batch:  155 | acc: 68.75%,  total acc: 69.83%   [EVAL] batch:  156 | acc: 62.50%,  total acc: 69.79%   [EVAL] batch:  157 | acc: 56.25%,  total acc: 69.70%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 69.58%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 69.45%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 69.41%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 69.21%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 69.13%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 68.98%   [EVAL] batch:  164 | acc: 62.50%,  total acc: 68.94%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 68.86%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 68.82%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 68.79%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 68.71%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 68.49%   [EVAL] batch:  170 | acc: 43.75%,  total acc: 68.35%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 68.17%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 68.03%   [EVAL] batch:  173 | acc: 37.50%,  total acc: 67.85%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 67.71%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 68.26%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 68.44%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 68.61%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  181 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 68.85%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 68.99%   [EVAL] batch:  184 | acc: 87.50%,  total acc: 69.09%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 69.12%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 69.18%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 69.18%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 68.98%   [EVAL] batch:  189 | acc: 56.25%,  total acc: 68.91%   [EVAL] batch:  190 | acc: 43.75%,  total acc: 68.78%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 68.55%   [EVAL] batch:  192 | acc: 50.00%,  total acc: 68.46%   [EVAL] batch:  193 | acc: 31.25%,  total acc: 68.27%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 68.30%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 68.21%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 68.24%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 68.34%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 68.34%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 68.38%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 68.44%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 68.47%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 68.51%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 68.48%   [EVAL] batch:  206 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.93%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 69.05%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 69.16%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 69.40%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 69.48%   [EVAL] batch:  214 | acc: 93.75%,  total acc: 69.59%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:  216 | acc: 87.50%,  total acc: 69.82%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 70.03%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 70.17%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 70.38%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 70.49%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 70.62%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 70.74%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 70.70%   [EVAL] batch:  227 | acc: 56.25%,  total acc: 70.64%   [EVAL] batch:  228 | acc: 50.00%,  total acc: 70.55%   [EVAL] batch:  229 | acc: 18.75%,  total acc: 70.33%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 70.32%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 70.52%   [EVAL] batch:  233 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  234 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:  235 | acc: 81.25%,  total acc: 70.82%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 70.89%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 70.93%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 70.89%   [EVAL] batch:  239 | acc: 75.00%,  total acc: 70.91%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 70.88%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 70.89%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 70.70%   [EVAL] batch:  244 | acc: 50.00%,  total acc: 70.61%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 70.53%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 70.45%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 70.44%   [EVAL] batch:  248 | acc: 75.00%,  total acc: 70.46%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 70.35%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 70.44%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 70.49%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 70.55%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 70.57%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 70.66%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 70.70%   [EVAL] batch:  257 | acc: 37.50%,  total acc: 70.57%   [EVAL] batch:  258 | acc: 50.00%,  total acc: 70.49%   [EVAL] batch:  259 | acc: 43.75%,  total acc: 70.38%   [EVAL] batch:  260 | acc: 25.00%,  total acc: 70.21%   [EVAL] batch:  261 | acc: 56.25%,  total acc: 70.16%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 70.15%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 70.17%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 70.17%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 70.14%   [EVAL] batch:  266 | acc: 68.75%,  total acc: 70.13%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 70.10%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 70.10%   [EVAL] batch:  269 | acc: 62.50%,  total acc: 70.07%   [EVAL] batch:  270 | acc: 68.75%,  total acc: 70.06%   [EVAL] batch:  271 | acc: 75.00%,  total acc: 70.08%   [EVAL] batch:  272 | acc: 75.00%,  total acc: 70.10%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 70.14%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 70.07%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 70.68%   [EVAL] batch:  282 | acc: 56.25%,  total acc: 70.63%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 70.60%   [EVAL] batch:  284 | acc: 62.50%,  total acc: 70.57%   [EVAL] batch:  285 | acc: 56.25%,  total acc: 70.52%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 70.47%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 70.44%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 70.54%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 70.85%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 70.95%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 71.02%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 71.12%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 71.22%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 71.32%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:  300 | acc: 81.25%,  total acc: 71.64%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 71.71%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 71.78%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 71.85%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 71.89%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 71.96%   [EVAL] batch:  306 | acc: 87.50%,  total acc: 72.01%   [EVAL] batch:  307 | acc: 68.75%,  total acc: 72.00%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 72.07%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.16%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 72.28%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 72.18%   [EVAL] batch:  313 | acc: 0.00%,  total acc: 71.95%   [EVAL] batch:  314 | acc: 18.75%,  total acc: 71.79%   [EVAL] batch:  315 | acc: 0.00%,  total acc: 71.56%   [EVAL] batch:  316 | acc: 12.50%,  total acc: 71.37%   [EVAL] batch:  317 | acc: 0.00%,  total acc: 71.15%   [EVAL] batch:  318 | acc: 31.25%,  total acc: 71.02%   [EVAL] batch:  319 | acc: 68.75%,  total acc: 71.02%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 71.07%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 71.12%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 71.19%   [EVAL] batch:  323 | acc: 81.25%,  total acc: 71.22%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 71.27%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 71.26%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 71.22%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 71.21%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 71.20%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 71.14%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 71.11%   [EVAL] batch:  331 | acc: 6.25%,  total acc: 70.91%   [EVAL] batch:  332 | acc: 18.75%,  total acc: 70.76%   [EVAL] batch:  333 | acc: 12.50%,  total acc: 70.58%   [EVAL] batch:  334 | acc: 6.25%,  total acc: 70.39%   [EVAL] batch:  335 | acc: 12.50%,  total acc: 70.22%   [EVAL] batch:  336 | acc: 25.00%,  total acc: 70.09%   [EVAL] batch:  337 | acc: 25.00%,  total acc: 69.95%   [EVAL] batch:  338 | acc: 6.25%,  total acc: 69.76%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 69.58%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 69.37%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 69.19%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 69.01%   [EVAL] batch:  343 | acc: 12.50%,  total acc: 68.84%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 68.84%   [EVAL] batch:  345 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 68.91%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.98%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 69.02%   [EVAL] batch:  349 | acc: 56.25%,  total acc: 68.98%   [EVAL] batch:  350 | acc: 100.00%,  total acc: 69.07%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 69.14%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 69.19%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 69.24%   [EVAL] batch:  354 | acc: 87.50%,  total acc: 69.30%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 69.36%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 69.43%   [EVAL] batch:  357 | acc: 81.25%,  total acc: 69.47%   [EVAL] batch:  358 | acc: 62.50%,  total acc: 69.45%   [EVAL] batch:  359 | acc: 81.25%,  total acc: 69.48%   [EVAL] batch:  360 | acc: 81.25%,  total acc: 69.51%   [EVAL] batch:  361 | acc: 87.50%,  total acc: 69.56%   [EVAL] batch:  362 | acc: 43.75%,  total acc: 69.49%   [EVAL] batch:  363 | acc: 25.00%,  total acc: 69.37%   [EVAL] batch:  364 | acc: 37.50%,  total acc: 69.28%   [EVAL] batch:  365 | acc: 56.25%,  total acc: 69.25%   [EVAL] batch:  366 | acc: 37.50%,  total acc: 69.16%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 69.11%   [EVAL] batch:  368 | acc: 50.00%,  total acc: 69.05%   [EVAL] batch:  369 | acc: 50.00%,  total acc: 69.00%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 68.92%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 68.85%   [EVAL] batch:  372 | acc: 56.25%,  total acc: 68.82%   [EVAL] batch:  373 | acc: 68.75%,  total acc: 68.82%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 68.77%   [EVAL] batch:  375 | acc: 68.75%,  total acc: 68.77%   [EVAL] batch:  376 | acc: 43.75%,  total acc: 68.70%   [EVAL] batch:  377 | acc: 68.75%,  total acc: 68.70%   [EVAL] batch:  378 | acc: 81.25%,  total acc: 68.73%   [EVAL] batch:  379 | acc: 81.25%,  total acc: 68.77%   [EVAL] batch:  380 | acc: 68.75%,  total acc: 68.77%   [EVAL] batch:  381 | acc: 93.75%,  total acc: 68.83%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 68.90%   [EVAL] batch:  383 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 69.03%   [EVAL] batch:  385 | acc: 93.75%,  total acc: 69.09%   [EVAL] batch:  386 | acc: 93.75%,  total acc: 69.15%   [EVAL] batch:  387 | acc: 68.75%,  total acc: 69.15%   [EVAL] batch:  388 | acc: 18.75%,  total acc: 69.02%   [EVAL] batch:  389 | acc: 25.00%,  total acc: 68.91%   [EVAL] batch:  390 | acc: 12.50%,  total acc: 68.77%   [EVAL] batch:  391 | acc: 37.50%,  total acc: 68.69%   [EVAL] batch:  392 | acc: 31.25%,  total acc: 68.59%   [EVAL] batch:  393 | acc: 37.50%,  total acc: 68.51%   [EVAL] batch:  394 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 68.73%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  400 | acc: 37.50%,  total acc: 68.87%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 68.73%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 68.58%   [EVAL] batch:  403 | acc: 37.50%,  total acc: 68.50%   [EVAL] batch:  404 | acc: 0.00%,  total acc: 68.33%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 68.17%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 68.17%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 68.20%   [EVAL] batch:  408 | acc: 87.50%,  total acc: 68.25%   [EVAL] batch:  409 | acc: 75.00%,  total acc: 68.26%   [EVAL] batch:  410 | acc: 75.00%,  total acc: 68.28%   [EVAL] batch:  411 | acc: 87.50%,  total acc: 68.33%   [EVAL] batch:  412 | acc: 68.75%,  total acc: 68.33%   [EVAL] batch:  413 | acc: 62.50%,  total acc: 68.31%   [EVAL] batch:  414 | acc: 62.50%,  total acc: 68.30%   [EVAL] batch:  415 | acc: 50.00%,  total acc: 68.25%   [EVAL] batch:  416 | acc: 31.25%,  total acc: 68.17%   [EVAL] batch:  417 | acc: 43.75%,  total acc: 68.11%   [EVAL] batch:  418 | acc: 31.25%,  total acc: 68.02%   [EVAL] batch:  419 | acc: 6.25%,  total acc: 67.87%   [EVAL] batch:  420 | acc: 18.75%,  total acc: 67.76%   [EVAL] batch:  421 | acc: 0.00%,  total acc: 67.59%   [EVAL] batch:  422 | acc: 12.50%,  total acc: 67.46%   [EVAL] batch:  423 | acc: 18.75%,  total acc: 67.35%   [EVAL] batch:  424 | acc: 6.25%,  total acc: 67.21%   [EVAL] batch:  425 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:  426 | acc: 68.75%,  total acc: 67.20%   [EVAL] batch:  427 | acc: 87.50%,  total acc: 67.25%   [EVAL] batch:  428 | acc: 68.75%,  total acc: 67.25%   [EVAL] batch:  429 | acc: 75.00%,  total acc: 67.27%   [EVAL] batch:  430 | acc: 75.00%,  total acc: 67.29%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 67.30%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 67.35%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 67.41%   [EVAL] batch:  434 | acc: 87.50%,  total acc: 67.46%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 67.50%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 67.51%   [EVAL] batch:  437 | acc: 43.75%,  total acc: 67.45%   
cur_acc:  ['0.9286', '0.7817', '0.7490', '0.7530', '0.8393', '0.5784', '0.5962']
his_acc:  ['0.9286', '0.8615', '0.8045', '0.7590', '0.7656', '0.7268', '0.6745']
Clustering into  39  clusters
Clusters:  [ 5  0 26 16  1  1 37  6  0 11  2  8 29 14 23 19 22 24  4 17 24  6  6  1
 36 11  5  1 19 19 31 19 24 34 27 19 17  8 11  5 16  5  5  0 35 10 32 38
 18  0  1 14 13 36  3 21 28  9  8  0  4 25 30  7  3  0 13  5 15  4  2 12
  7  4  6 20  9 10 33  8]
Losses:  9.417238235473633 3.3850491046905518 0.6980347633361816
CurrentTrain: epoch  0, batch     0 | loss: 9.4172382Losses:  8.54002571105957 3.0488739013671875 0.685127854347229
CurrentTrain: epoch  0, batch     1 | loss: 8.5400257Losses:  10.769323348999023 4.157240867614746 0.7352495789527893
CurrentTrain: epoch  0, batch     2 | loss: 10.7693233Losses:  9.346437454223633 -0.0 0.15306854248046875
CurrentTrain: epoch  0, batch     3 | loss: 9.3464375Losses:  8.988335609436035 3.0546822547912598 0.7387553453445435
CurrentTrain: epoch  1, batch     0 | loss: 8.9883356Losses:  9.570562362670898 5.203806400299072 0.6054818630218506
CurrentTrain: epoch  1, batch     1 | loss: 9.5705624Losses:  7.336338996887207 2.1953539848327637 0.7329438328742981
CurrentTrain: epoch  1, batch     2 | loss: 7.3363390Losses:  3.6144516468048096 -0.0 0.10918574035167694
CurrentTrain: epoch  1, batch     3 | loss: 3.6144516Losses:  7.996970176696777 3.4499387741088867 0.7451934218406677
CurrentTrain: epoch  2, batch     0 | loss: 7.9969702Losses:  9.75050163269043 5.329135894775391 0.7252667546272278
CurrentTrain: epoch  2, batch     1 | loss: 9.7505016Losses:  7.370628356933594 3.549232244491577 0.7049424648284912
CurrentTrain: epoch  2, batch     2 | loss: 7.3706284Losses:  6.316986560821533 -0.0 0.10176213085651398
CurrentTrain: epoch  2, batch     3 | loss: 6.3169866Losses:  7.604567527770996 3.6222121715545654 0.6647748947143555
CurrentTrain: epoch  3, batch     0 | loss: 7.6045675Losses:  8.509605407714844 3.828547954559326 0.6870012283325195
CurrentTrain: epoch  3, batch     1 | loss: 8.5096054Losses:  6.876715660095215 3.503105640411377 0.6705695390701294
CurrentTrain: epoch  3, batch     2 | loss: 6.8767157Losses:  4.627225399017334 -0.0 0.18355043232440948
CurrentTrain: epoch  3, batch     3 | loss: 4.6272254Losses:  9.295448303222656 4.476276874542236 0.734163761138916
CurrentTrain: epoch  4, batch     0 | loss: 9.2954483Losses:  7.247358322143555 3.51539945602417 0.744986355304718
CurrentTrain: epoch  4, batch     1 | loss: 7.2473583Losses:  6.228537082672119 2.6943252086639404 0.7377907633781433
CurrentTrain: epoch  4, batch     2 | loss: 6.2285371Losses:  1.9069188833236694 -0.0 0.10452573001384735
CurrentTrain: epoch  4, batch     3 | loss: 1.9069189Losses:  6.268812656402588 2.2659778594970703 0.7826896905899048
CurrentTrain: epoch  5, batch     0 | loss: 6.2688127Losses:  5.550140380859375 2.3407769203186035 0.7279080152511597
CurrentTrain: epoch  5, batch     1 | loss: 5.5501404Losses:  6.3112568855285645 2.557046413421631 0.7240846753120422
CurrentTrain: epoch  5, batch     2 | loss: 6.3112569Losses:  1.8886973857879639 -0.0 0.08324410021305084
CurrentTrain: epoch  5, batch     3 | loss: 1.8886974Losses:  7.69974422454834 3.9841818809509277 0.656848669052124
CurrentTrain: epoch  6, batch     0 | loss: 7.6997442Losses:  6.09922981262207 2.842702865600586 0.6854058504104614
CurrentTrain: epoch  6, batch     1 | loss: 6.0992298Losses:  7.716782569885254 4.643102645874023 0.6713199019432068
CurrentTrain: epoch  6, batch     2 | loss: 7.7167826Losses:  2.167240858078003 -0.0 0.11553816497325897
CurrentTrain: epoch  6, batch     3 | loss: 2.1672409Losses:  6.569757461547852 3.0442492961883545 0.6288266181945801
CurrentTrain: epoch  7, batch     0 | loss: 6.5697575Losses:  6.302416801452637 3.056807518005371 0.7311447858810425
CurrentTrain: epoch  7, batch     1 | loss: 6.3024168Losses:  6.334341526031494 3.4156064987182617 0.7159331440925598
CurrentTrain: epoch  7, batch     2 | loss: 6.3343415Losses:  1.8390247821807861 -0.0 0.10790295898914337
CurrentTrain: epoch  7, batch     3 | loss: 1.8390248Losses:  5.249486923217773 2.235302448272705 0.7216928005218506
CurrentTrain: epoch  8, batch     0 | loss: 5.2494869Losses:  4.7072248458862305 1.6094988584518433 0.7780298590660095
CurrentTrain: epoch  8, batch     1 | loss: 4.7072248Losses:  6.022610664367676 2.7205207347869873 0.6837233304977417
CurrentTrain: epoch  8, batch     2 | loss: 6.0226107Losses:  2.3550662994384766 -0.0 0.09486089646816254
CurrentTrain: epoch  8, batch     3 | loss: 2.3550663Losses:  5.436999320983887 2.5032715797424316 0.720974862575531
CurrentTrain: epoch  9, batch     0 | loss: 5.4369993Losses:  5.033767223358154 2.0117478370666504 0.7669950723648071
CurrentTrain: epoch  9, batch     1 | loss: 5.0337672Losses:  4.836746692657471 2.093010187149048 0.7128595113754272
CurrentTrain: epoch  9, batch     2 | loss: 4.8367467Losses:  3.8483481407165527 -0.0 0.10766744613647461
CurrentTrain: epoch  9, batch     3 | loss: 3.8483481
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of Australia are the major political divisions, with New South Wales being the most populous state in the country.  
Head Entity: Australia  
Tail Entity: New South Wales  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: this contrasts with the common usage of harem as an english loan - word , which implies a female - only enclave or seraglio .
Head Entity: harem
Tail Entity: seraglio
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: Some researchers argue that the term "artificial intelligence" is often used interchangeably with "machine learning," despite the latter being a subset of the former.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  

Relation: said to be the same as  
Context: The term "soda" is frequently considered synonymous with "pop" in various regions, although some people argue there are subtle differences in usage.  
Head Entity: soda  
Tail Entity: pop  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: in 2010 , ty burrell received a nomination for the television critics association award for individual achievement in comedy , winning in 2011 .
Head Entity: television critics association award for individual achievement in comedy
Tail Entity: ty burrell
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In 2022, the Boston Red Sox triumphed in the World Series, marking their fourth championship in the last two decades.  
Head Entity: World Series  
Tail Entity: Boston Red Sox  

Relation: winner  
Context: At the annual film festival, the documentary "Voices of Change" was awarded the Best Picture, highlighting its impactful storytelling and direction.  
Head Entity: Best Picture  
Tail Entity: Voices of Change  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: air commodore arthur kellam tylee obe ( 24 april 1887 – 13 april 1961 ) was canadian officer who served in the royal flying corps during world war i.
Head Entity: arthur kellam tylee
Tail Entity: air commodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: general john smith was appointed as the chief of staff of the united states army in 2020, overseeing all operations and strategic planning.  
Head Entity: john smith  
Tail Entity: general  

Relation: military rank  
Context: colonel jane doe led her battalion with distinction during the peacekeeping mission in the middle east, earning her numerous accolades.  
Head Entity: jane doe  
Tail Entity: colonel  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham, set to hit the shelves on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular science magazine, Scientific American, published an article discussing the latest advancements in renewable energy technologies.  
Head Entity: Scientific American  
Tail Entity: renewable energy technologies  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first book in the series, "The Enchanted Forest," was published in 2015, followed by "The Hidden Valley" in 2017.  
Head Entity: The Enchanted Forest  
Tail Entity: The Hidden Valley  

Relation: followed by  
Context: The opening act of the concert featured a local band, which was followed by the headlining artist who captivated the audience.  
Head Entity: local band  
Tail Entity: headlining artist  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: in 1694 , he found employment at the court chapel in weimar and was promoted to vice capellmaster ( " " ) in 1695 , succeeding august kühnel , with samuel drese as capellmaster .
Head Entity: august kühnel
Tail Entity: weimar
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: After completing his studies, John accepted a position at the tech firm in Silicon Valley, where he contributed to several innovative projects.  
Head Entity: John  
Tail Entity: Silicon Valley  

Relation: work location  
Context: The renowned artist spent several years in Paris, where he created some of his most famous works and collaborated with other influential figures in the art world.  
Head Entity: The renowned artist  
Tail Entity: Paris  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: ancient city of Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Losses:  3.9731931686401367 0.2396656572818756 0.9714716672897339
MemoryTrain:  epoch  0, batch     0 | loss: 3.9731932Losses:  3.940186023712158 0.26698336005210876 0.9690431356430054
MemoryTrain:  epoch  0, batch     1 | loss: 3.9401860Losses:  3.7193803787231445 0.7962659001350403 0.8730429410934448
MemoryTrain:  epoch  0, batch     2 | loss: 3.7193804Losses:  3.752460479736328 0.5168541073799133 0.969793438911438
MemoryTrain:  epoch  0, batch     3 | loss: 3.7524605Losses:  4.053858280181885 0.2809574007987976 0.9876216650009155
MemoryTrain:  epoch  0, batch     4 | loss: 4.0538583Losses:  4.04935359954834 0.2565739154815674 0.9774459600448608
MemoryTrain:  epoch  0, batch     5 | loss: 4.0493536Losses:  3.833991289138794 0.5236072540283203 0.9632309079170227
MemoryTrain:  epoch  0, batch     6 | loss: 3.8339913Losses:  4.49484395980835 0.36247509717941284 1.0155771970748901
MemoryTrain:  epoch  0, batch     7 | loss: 4.4948440Losses:  3.042729139328003 -0.0 0.8554198741912842
MemoryTrain:  epoch  0, batch     8 | loss: 3.0427291Losses:  3.483959197998047 -0.0 0.9040395617485046
MemoryTrain:  epoch  0, batch     9 | loss: 3.4839592Losses:  3.152505874633789 0.24829533696174622 0.9151332378387451
MemoryTrain:  epoch  0, batch    10 | loss: 3.1525059Losses:  4.333547592163086 -0.0 0.9897345900535583
MemoryTrain:  epoch  0, batch    11 | loss: 4.3335476Losses:  3.313727617263794 -0.0 1.0731956958770752
MemoryTrain:  epoch  0, batch    12 | loss: 3.3137276Losses:  3.927225351333618 -0.0 0.9342398643493652
MemoryTrain:  epoch  0, batch    13 | loss: 3.9272254Losses:  3.657804250717163 0.49738603830337524 0.9682724475860596
MemoryTrain:  epoch  0, batch    14 | loss: 3.6578043Losses:  3.5863471031188965 -0.0 0.9849315881729126
MemoryTrain:  epoch  1, batch     0 | loss: 3.5863471Losses:  2.9018192291259766 -0.0 0.9659193754196167
MemoryTrain:  epoch  1, batch     1 | loss: 2.9018192Losses:  2.618074417114258 -0.0 1.0636118650436401
MemoryTrain:  epoch  1, batch     2 | loss: 2.6180744Losses:  2.728710889816284 0.26296448707580566 0.8075458407402039
MemoryTrain:  epoch  1, batch     3 | loss: 2.7287109Losses:  3.5785293579101562 0.5140663385391235 0.9342927932739258
MemoryTrain:  epoch  1, batch     4 | loss: 3.5785294Losses:  3.198513984680176 -0.0 0.9419256448745728
MemoryTrain:  epoch  1, batch     5 | loss: 3.1985140Losses:  3.3385651111602783 0.26209545135498047 0.9640288949012756
MemoryTrain:  epoch  1, batch     6 | loss: 3.3385651Losses:  4.194159030914307 0.4997826814651489 0.8693246245384216
MemoryTrain:  epoch  1, batch     7 | loss: 4.1941590Losses:  4.118871212005615 -0.0 1.0701904296875
MemoryTrain:  epoch  1, batch     8 | loss: 4.1188712Losses:  4.079315662384033 0.24734516441822052 0.960964024066925
MemoryTrain:  epoch  1, batch     9 | loss: 4.0793157Losses:  3.0555083751678467 0.26147377490997314 0.9073483347892761
MemoryTrain:  epoch  1, batch    10 | loss: 3.0555084Losses:  3.264633893966675 0.2510457932949066 0.9591095447540283
MemoryTrain:  epoch  1, batch    11 | loss: 3.2646339Losses:  2.827134609222412 0.24105364084243774 0.902577817440033
MemoryTrain:  epoch  1, batch    12 | loss: 2.8271346Losses:  3.191896915435791 -0.0 0.8446369171142578
MemoryTrain:  epoch  1, batch    13 | loss: 3.1918969Losses:  3.019155979156494 0.5038663148880005 0.9098869562149048
MemoryTrain:  epoch  1, batch    14 | loss: 3.0191560Losses:  2.746473789215088 0.2758639454841614 1.0087217092514038
MemoryTrain:  epoch  2, batch     0 | loss: 2.7464738Losses:  2.940117835998535 0.2579858899116516 0.9235028624534607
MemoryTrain:  epoch  2, batch     1 | loss: 2.9401178Losses:  3.2450339794158936 0.7963925004005432 0.8685981631278992
MemoryTrain:  epoch  2, batch     2 | loss: 3.2450340Losses:  2.8999648094177246 0.4869946837425232 0.9772816300392151
MemoryTrain:  epoch  2, batch     3 | loss: 2.8999648Losses:  2.8705949783325195 -0.0 1.0127514600753784
MemoryTrain:  epoch  2, batch     4 | loss: 2.8705950Losses:  2.686314582824707 0.22302621603012085 0.9458935260772705
MemoryTrain:  epoch  2, batch     5 | loss: 2.6863146Losses:  4.104032039642334 0.7121658325195312 0.8555349111557007
MemoryTrain:  epoch  2, batch     6 | loss: 4.1040320Losses:  3.032761812210083 0.4562114477157593 0.949484646320343
MemoryTrain:  epoch  2, batch     7 | loss: 3.0327618Losses:  3.0094261169433594 0.2455170452594757 0.9654920101165771
MemoryTrain:  epoch  2, batch     8 | loss: 3.0094261Losses:  3.0350501537323 0.2490749955177307 0.8653342127799988
MemoryTrain:  epoch  2, batch     9 | loss: 3.0350502Losses:  3.390162467956543 0.531825065612793 0.9225172400474548
MemoryTrain:  epoch  2, batch    10 | loss: 3.3901625Losses:  3.191570281982422 -0.0 1.0323853492736816
MemoryTrain:  epoch  2, batch    11 | loss: 3.1915703Losses:  3.364090919494629 0.2382146418094635 1.0201882123947144
MemoryTrain:  epoch  2, batch    12 | loss: 3.3640909Losses:  2.5991530418395996 -0.0 0.9175543785095215
MemoryTrain:  epoch  2, batch    13 | loss: 2.5991530Losses:  4.082409381866455 0.7254385948181152 0.8534412384033203
MemoryTrain:  epoch  2, batch    14 | loss: 4.0824094Losses:  2.750558376312256 -0.0 0.9654651284217834
MemoryTrain:  epoch  3, batch     0 | loss: 2.7505584Losses:  2.8756344318389893 -0.0 0.9722141623497009
MemoryTrain:  epoch  3, batch     1 | loss: 2.8756344Losses:  3.453885555267334 0.5049940943717957 0.9623249769210815
MemoryTrain:  epoch  3, batch     2 | loss: 3.4538856Losses:  3.2570972442626953 0.506723940372467 0.9728213548660278
MemoryTrain:  epoch  3, batch     3 | loss: 3.2570972Losses:  2.1900551319122314 -0.0 0.8008427023887634
MemoryTrain:  epoch  3, batch     4 | loss: 2.1900551Losses:  3.1295340061187744 0.7523148655891418 0.8725125789642334
MemoryTrain:  epoch  3, batch     5 | loss: 3.1295340Losses:  2.7331624031066895 0.24483637511730194 0.840320885181427
MemoryTrain:  epoch  3, batch     6 | loss: 2.7331624Losses:  3.1923701763153076 0.7079946398735046 0.8685136437416077
MemoryTrain:  epoch  3, batch     7 | loss: 3.1923702Losses:  3.354707717895508 0.8100816607475281 0.9714947938919067
MemoryTrain:  epoch  3, batch     8 | loss: 3.3547077Losses:  3.1794426441192627 0.24042683839797974 0.9633564949035645
MemoryTrain:  epoch  3, batch     9 | loss: 3.1794426Losses:  2.5082266330718994 -0.0 0.8869143724441528
MemoryTrain:  epoch  3, batch    10 | loss: 2.5082266Losses:  3.1127238273620605 0.9888876080513 0.7829269766807556
MemoryTrain:  epoch  3, batch    11 | loss: 3.1127238Losses:  2.747105121612549 -0.0 1.0495928525924683
MemoryTrain:  epoch  3, batch    12 | loss: 2.7471051Losses:  2.600663900375366 0.2844102382659912 0.9565925002098083
MemoryTrain:  epoch  3, batch    13 | loss: 2.6006639Losses:  2.863569736480713 -0.0 0.8425776958465576
MemoryTrain:  epoch  3, batch    14 | loss: 2.8635697Losses:  2.3646047115325928 0.2625732421875 0.7380252480506897
MemoryTrain:  epoch  4, batch     0 | loss: 2.3646047Losses:  2.9562249183654785 0.498940110206604 0.9724227786064148
MemoryTrain:  epoch  4, batch     1 | loss: 2.9562249Losses:  3.3001978397369385 1.1538500785827637 0.786017119884491
MemoryTrain:  epoch  4, batch     2 | loss: 3.3001978Losses:  3.4854896068573 0.37216028571128845 0.9238149523735046
MemoryTrain:  epoch  4, batch     3 | loss: 3.4854896Losses:  2.7318973541259766 0.5107586979866028 0.9284101724624634
MemoryTrain:  epoch  4, batch     4 | loss: 2.7318974Losses:  2.6503872871398926 -0.0 0.9580149054527283
MemoryTrain:  epoch  4, batch     5 | loss: 2.6503873Losses:  3.212395429611206 0.273528516292572 0.9247259497642517
MemoryTrain:  epoch  4, batch     6 | loss: 3.2123954Losses:  3.9792368412017822 0.737004280090332 0.9258782863616943
MemoryTrain:  epoch  4, batch     7 | loss: 3.9792368Losses:  2.4356696605682373 0.23911353945732117 0.9300189018249512
MemoryTrain:  epoch  4, batch     8 | loss: 2.4356697Losses:  2.583164691925049 -0.0 1.071330189704895
MemoryTrain:  epoch  4, batch     9 | loss: 2.5831647Losses:  2.5109169483184814 -0.0 0.9574641585350037
MemoryTrain:  epoch  4, batch    10 | loss: 2.5109169Losses:  2.6458706855773926 0.48813965916633606 0.859429657459259
MemoryTrain:  epoch  4, batch    11 | loss: 2.6458707Losses:  2.382441997528076 -0.0 0.9983481168746948
MemoryTrain:  epoch  4, batch    12 | loss: 2.3824420Losses:  2.4661717414855957 -0.0 1.0214301347732544
MemoryTrain:  epoch  4, batch    13 | loss: 2.4661717Losses:  2.9029150009155273 0.7289496660232544 0.9031839370727539
MemoryTrain:  epoch  4, batch    14 | loss: 2.9029150Losses:  2.4291322231292725 -0.0 1.0302073955535889
MemoryTrain:  epoch  5, batch     0 | loss: 2.4291322Losses:  2.555640697479248 0.24472150206565857 1.0088763236999512
MemoryTrain:  epoch  5, batch     1 | loss: 2.5556407Losses:  2.7725563049316406 -0.0 1.08907151222229
MemoryTrain:  epoch  5, batch     2 | loss: 2.7725563Losses:  3.467575788497925 1.1725420951843262 0.83868807554245
MemoryTrain:  epoch  5, batch     3 | loss: 3.4675758Losses:  2.6483263969421387 -0.0 1.0663825273513794
MemoryTrain:  epoch  5, batch     4 | loss: 2.6483264Losses:  2.3726654052734375 -0.0 1.0205751657485962
MemoryTrain:  epoch  5, batch     5 | loss: 2.3726654Losses:  3.0560951232910156 0.5165653228759766 0.9121932983398438
MemoryTrain:  epoch  5, batch     6 | loss: 3.0560951Losses:  2.4596126079559326 0.2504027485847473 0.9117751121520996
MemoryTrain:  epoch  5, batch     7 | loss: 2.4596126Losses:  3.1597957611083984 0.34316930174827576 0.9160286784172058
MemoryTrain:  epoch  5, batch     8 | loss: 3.1597958Losses:  2.476666212081909 0.2494993358850479 0.8411855697631836
MemoryTrain:  epoch  5, batch     9 | loss: 2.4766662Losses:  3.80733585357666 0.5924029350280762 0.9486379623413086
MemoryTrain:  epoch  5, batch    10 | loss: 3.8073359Losses:  2.0320615768432617 -0.0 0.7052584886550903
MemoryTrain:  epoch  5, batch    11 | loss: 2.0320616Losses:  2.6022181510925293 -0.0 1.030685544013977
MemoryTrain:  epoch  5, batch    12 | loss: 2.6022182Losses:  2.065049648284912 -0.0 0.8372313976287842
MemoryTrain:  epoch  5, batch    13 | loss: 2.0650496Losses:  2.7572717666625977 0.5136712193489075 0.9134418368339539
MemoryTrain:  epoch  5, batch    14 | loss: 2.7572718Losses:  2.8529176712036133 0.5303449630737305 0.9611601233482361
MemoryTrain:  epoch  6, batch     0 | loss: 2.8529177Losses:  2.841597318649292 -0.0 0.967051088809967
MemoryTrain:  epoch  6, batch     1 | loss: 2.8415973Losses:  2.63114595413208 -0.0 1.0566164255142212
MemoryTrain:  epoch  6, batch     2 | loss: 2.6311460Losses:  2.64735746383667 0.24067237973213196 0.9604232907295227
MemoryTrain:  epoch  6, batch     3 | loss: 2.6473575Losses:  2.2037301063537598 -0.0 0.9627460241317749
MemoryTrain:  epoch  6, batch     4 | loss: 2.2037301Losses:  2.593831777572632 0.27768564224243164 0.9617463946342468
MemoryTrain:  epoch  6, batch     5 | loss: 2.5938318Losses:  2.4519283771514893 0.23411384224891663 0.8950690031051636
MemoryTrain:  epoch  6, batch     6 | loss: 2.4519284Losses:  2.7951788902282715 0.5170459747314453 0.9840407967567444
MemoryTrain:  epoch  6, batch     7 | loss: 2.7951789Losses:  2.8060643672943115 0.27730268239974976 0.8999466896057129
MemoryTrain:  epoch  6, batch     8 | loss: 2.8060644Losses:  2.511620044708252 0.265434592962265 0.9224297404289246
MemoryTrain:  epoch  6, batch     9 | loss: 2.5116200Losses:  2.9319701194763184 0.24852876365184784 1.0494035482406616
MemoryTrain:  epoch  6, batch    10 | loss: 2.9319701Losses:  2.491117477416992 -0.0 0.9578543305397034
MemoryTrain:  epoch  6, batch    11 | loss: 2.4911175Losses:  2.324202537536621 -0.0 0.8943257927894592
MemoryTrain:  epoch  6, batch    12 | loss: 2.3242025Losses:  2.5131189823150635 0.2549482583999634 1.002902865409851
MemoryTrain:  epoch  6, batch    13 | loss: 2.5131190Losses:  2.865548610687256 0.5527639389038086 0.9181604385375977
MemoryTrain:  epoch  6, batch    14 | loss: 2.8655486Losses:  2.167734384536743 -0.0 0.9447712898254395
MemoryTrain:  epoch  7, batch     0 | loss: 2.1677344Losses:  2.9327213764190674 0.7202675342559814 0.8958384394645691
MemoryTrain:  epoch  7, batch     1 | loss: 2.9327214Losses:  2.2667078971862793 -0.0 0.9644366502761841
MemoryTrain:  epoch  7, batch     2 | loss: 2.2667079Losses:  2.3761844635009766 -0.0 0.9590232372283936
MemoryTrain:  epoch  7, batch     3 | loss: 2.3761845Losses:  2.9278006553649902 0.23762264847755432 0.9589540362358093
MemoryTrain:  epoch  7, batch     4 | loss: 2.9278007Losses:  2.917880058288574 0.2883397340774536 0.8831758499145508
MemoryTrain:  epoch  7, batch     5 | loss: 2.9178801Losses:  2.4916176795959473 -0.0 0.9077792167663574
MemoryTrain:  epoch  7, batch     6 | loss: 2.4916177Losses:  2.904611110687256 0.5526180267333984 0.8109763264656067
MemoryTrain:  epoch  7, batch     7 | loss: 2.9046111Losses:  2.4756593704223633 0.2440337836742401 0.9673911333084106
MemoryTrain:  epoch  7, batch     8 | loss: 2.4756594Losses:  2.45936918258667 0.27071547508239746 0.8512107133865356
MemoryTrain:  epoch  7, batch     9 | loss: 2.4593692Losses:  2.29111385345459 -0.0 0.9220966100692749
MemoryTrain:  epoch  7, batch    10 | loss: 2.2911139Losses:  2.5372776985168457 0.2597391605377197 0.9743785262107849
MemoryTrain:  epoch  7, batch    11 | loss: 2.5372777Losses:  2.3191609382629395 -0.0 0.9148065447807312
MemoryTrain:  epoch  7, batch    12 | loss: 2.3191609Losses:  2.5383825302124023 0.24629314243793488 1.0145739316940308
MemoryTrain:  epoch  7, batch    13 | loss: 2.5383825Losses:  2.46416974067688 -0.0 1.0608150959014893
MemoryTrain:  epoch  7, batch    14 | loss: 2.4641697Losses:  2.360346555709839 0.21803653240203857 0.9071924686431885
MemoryTrain:  epoch  8, batch     0 | loss: 2.3603466Losses:  2.7620365619659424 0.292396605014801 0.982968807220459
MemoryTrain:  epoch  8, batch     1 | loss: 2.7620366Losses:  2.479296922683716 0.24842722713947296 1.0044456720352173
MemoryTrain:  epoch  8, batch     2 | loss: 2.4792969Losses:  2.510206460952759 0.4861809015274048 0.7822038531303406
MemoryTrain:  epoch  8, batch     3 | loss: 2.5102065Losses:  2.6761603355407715 0.4934924244880676 0.9564056992530823
MemoryTrain:  epoch  8, batch     4 | loss: 2.6761603Losses:  2.5353994369506836 -0.0 0.9073550701141357
MemoryTrain:  epoch  8, batch     5 | loss: 2.5353994Losses:  2.8094890117645264 0.2499295473098755 0.9896292686462402
MemoryTrain:  epoch  8, batch     6 | loss: 2.8094890Losses:  2.613884449005127 0.2528597414493561 0.9830260276794434
MemoryTrain:  epoch  8, batch     7 | loss: 2.6138844Losses:  2.285512685775757 -0.0 0.8946282863616943
MemoryTrain:  epoch  8, batch     8 | loss: 2.2855127Losses:  2.4615285396575928 0.22500357031822205 0.8992552757263184
MemoryTrain:  epoch  8, batch     9 | loss: 2.4615285Losses:  2.491311550140381 -0.0 0.9218456149101257
MemoryTrain:  epoch  8, batch    10 | loss: 2.4913116Losses:  2.8950490951538086 0.494922012090683 0.7798285484313965
MemoryTrain:  epoch  8, batch    11 | loss: 2.8950491Losses:  2.5375213623046875 0.26929378509521484 0.8657692670822144
MemoryTrain:  epoch  8, batch    12 | loss: 2.5375214Losses:  2.3079476356506348 0.25406304001808167 0.8256710767745972
MemoryTrain:  epoch  8, batch    13 | loss: 2.3079476Losses:  2.444528102874756 0.23963043093681335 0.8447962999343872
MemoryTrain:  epoch  8, batch    14 | loss: 2.4445281Losses:  2.4226202964782715 -0.0 1.0086208581924438
MemoryTrain:  epoch  9, batch     0 | loss: 2.4226203Losses:  2.71604585647583 0.49278897047042847 0.9830613136291504
MemoryTrain:  epoch  9, batch     1 | loss: 2.7160459Losses:  2.869654655456543 0.5082182884216309 0.9605056047439575
MemoryTrain:  epoch  9, batch     2 | loss: 2.8696547Losses:  2.243788242340088 -0.0 0.9743231534957886
MemoryTrain:  epoch  9, batch     3 | loss: 2.2437882Losses:  2.6156537532806396 -0.0 0.9648274779319763
MemoryTrain:  epoch  9, batch     4 | loss: 2.6156538Losses:  2.8956198692321777 0.5476085543632507 0.9102131128311157
MemoryTrain:  epoch  9, batch     5 | loss: 2.8956199Losses:  2.394235610961914 0.2444559633731842 0.9607540369033813
MemoryTrain:  epoch  9, batch     6 | loss: 2.3942356Losses:  2.6980748176574707 0.2811695337295532 0.9614574313163757
MemoryTrain:  epoch  9, batch     7 | loss: 2.6980748Losses:  2.53133487701416 0.26452648639678955 0.926777720451355
MemoryTrain:  epoch  9, batch     8 | loss: 2.5313349Losses:  2.3871541023254395 0.22659432888031006 0.8394159078598022
MemoryTrain:  epoch  9, batch     9 | loss: 2.3871541Losses:  2.3743200302124023 0.21999269723892212 0.8950308561325073
MemoryTrain:  epoch  9, batch    10 | loss: 2.3743200Losses:  2.7579541206359863 -0.0 1.063394546508789
MemoryTrain:  epoch  9, batch    11 | loss: 2.7579541Losses:  3.286870241165161 0.9206725358963013 0.7795665860176086
MemoryTrain:  epoch  9, batch    12 | loss: 3.2868702Losses:  2.570051670074463 0.4914858639240265 0.8542905449867249
MemoryTrain:  epoch  9, batch    13 | loss: 2.5700517Losses:  2.8012263774871826 0.25094765424728394 0.9750835299491882
MemoryTrain:  epoch  9, batch    14 | loss: 2.8012264
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 45.00%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 50.78%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 52.27%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 58.93%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 60.00%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 61.33%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 62.87%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 64.93%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 66.78%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 67.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 72.01%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:   25 | acc: 31.25%,  total acc: 72.36%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 70.60%   [EVAL] batch:   27 | acc: 56.25%,  total acc: 70.09%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 69.40%   [EVAL] batch:   29 | acc: 37.50%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 37.50%,  total acc: 67.34%   [EVAL] batch:   31 | acc: 18.75%,  total acc: 65.82%   [EVAL] batch:   32 | acc: 6.25%,  total acc: 64.02%   [EVAL] batch:   33 | acc: 0.00%,  total acc: 62.13%   [EVAL] batch:   34 | acc: 6.25%,  total acc: 60.54%   [EVAL] batch:   35 | acc: 0.00%,  total acc: 58.85%   [EVAL] batch:   36 | acc: 0.00%,  total acc: 57.26%   [EVAL] batch:   37 | acc: 50.00%,  total acc: 57.07%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 57.69%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 57.66%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 57.62%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 57.89%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 58.43%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 58.52%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 58.75%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 58.83%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 58.78%   [EVAL] batch:   47 | acc: 50.00%,  total acc: 58.59%   [EVAL] batch:   48 | acc: 43.75%,  total acc: 58.29%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 58.63%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 59.44%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 60.22%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 60.97%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 61.69%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 62.39%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 63.06%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 63.71%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 64.12%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 64.62%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 65.21%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 65.68%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 65.97%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 67.50%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 65.18%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 63.28%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 63.19%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 61.36%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 60.94%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 61.54%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 63.84%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 74.15%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 73.91%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 74.48%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 75.96%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 76.85%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.23%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 80.87%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 81.07%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 80.89%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 81.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.93%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 83.04%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 83.14%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 83.24%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 83.42%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 83.24%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 83.07%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 83.16%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 82.88%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 82.84%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 83.02%   [EVAL] batch:   53 | acc: 68.75%,  total acc: 82.75%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 82.27%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 82.37%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 82.02%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 81.47%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 81.04%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 81.04%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 80.94%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 80.75%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 80.56%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 80.57%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 80.48%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 80.40%   [EVAL] batch:   66 | acc: 68.75%,  total acc: 80.22%   [EVAL] batch:   67 | acc: 56.25%,  total acc: 79.87%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 79.71%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 79.64%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 79.31%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 78.99%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 78.68%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 78.46%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 78.42%   [EVAL] batch:   75 | acc: 62.50%,  total acc: 78.21%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 78.08%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 77.96%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 77.69%   [EVAL] batch:   79 | acc: 50.00%,  total acc: 77.34%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 77.31%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 76.68%   [EVAL] batch:   82 | acc: 6.25%,  total acc: 75.83%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 75.00%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 74.12%   [EVAL] batch:   85 | acc: 12.50%,  total acc: 73.40%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 72.77%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 72.51%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 72.75%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 73.06%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 73.15%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:   93 | acc: 87.50%,  total acc: 73.87%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 74.41%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 74.68%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 74.94%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 75.19%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 75.44%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 75.31%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 75.18%   [EVAL] batch:  102 | acc: 68.75%,  total acc: 75.12%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 75.12%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:  106 | acc: 37.50%,  total acc: 74.65%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 74.07%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 73.68%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 73.12%   [EVAL] batch:  110 | acc: 43.75%,  total acc: 72.86%   [EVAL] batch:  111 | acc: 12.50%,  total acc: 72.32%   [EVAL] batch:  112 | acc: 50.00%,  total acc: 72.12%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 72.26%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 72.58%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 72.70%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 72.83%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 72.90%   [EVAL] batch:  119 | acc: 0.00%,  total acc: 72.29%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 71.85%   [EVAL] batch:  121 | acc: 0.00%,  total acc: 71.26%   [EVAL] batch:  122 | acc: 6.25%,  total acc: 70.73%   [EVAL] batch:  123 | acc: 0.00%,  total acc: 70.16%   [EVAL] batch:  124 | acc: 6.25%,  total acc: 69.65%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 69.49%   [EVAL] batch:  126 | acc: 31.25%,  total acc: 69.19%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 68.99%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 68.65%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 68.46%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 68.23%   [EVAL] batch:  131 | acc: 81.25%,  total acc: 68.32%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 68.56%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 68.70%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 68.89%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 69.11%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 69.20%   [EVAL] batch:  138 | acc: 43.75%,  total acc: 69.02%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 68.97%   [EVAL] batch:  140 | acc: 81.25%,  total acc: 69.06%   [EVAL] batch:  141 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:  142 | acc: 50.00%,  total acc: 68.92%   [EVAL] batch:  143 | acc: 62.50%,  total acc: 68.88%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 68.79%   [EVAL] batch:  145 | acc: 81.25%,  total acc: 68.88%   [EVAL] batch:  146 | acc: 62.50%,  total acc: 68.84%   [EVAL] batch:  147 | acc: 75.00%,  total acc: 68.88%   [EVAL] batch:  148 | acc: 75.00%,  total acc: 68.92%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 68.96%   [EVAL] batch:  150 | acc: 75.00%,  total acc: 69.00%   [EVAL] batch:  151 | acc: 75.00%,  total acc: 69.04%   [EVAL] batch:  152 | acc: 75.00%,  total acc: 69.08%   [EVAL] batch:  153 | acc: 87.50%,  total acc: 69.20%   [EVAL] batch:  154 | acc: 68.75%,  total acc: 69.19%   [EVAL] batch:  155 | acc: 43.75%,  total acc: 69.03%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 68.95%   [EVAL] batch:  157 | acc: 50.00%,  total acc: 68.83%   [EVAL] batch:  158 | acc: 50.00%,  total acc: 68.71%   [EVAL] batch:  159 | acc: 50.00%,  total acc: 68.59%   [EVAL] batch:  160 | acc: 62.50%,  total acc: 68.56%   [EVAL] batch:  161 | acc: 37.50%,  total acc: 68.36%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 68.29%   [EVAL] batch:  163 | acc: 56.25%,  total acc: 68.22%   [EVAL] batch:  164 | acc: 68.75%,  total acc: 68.22%   [EVAL] batch:  165 | acc: 56.25%,  total acc: 68.15%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 68.11%   [EVAL] batch:  167 | acc: 62.50%,  total acc: 68.08%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 68.01%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 67.79%   [EVAL] batch:  170 | acc: 37.50%,  total acc: 67.62%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 67.41%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 67.27%   [EVAL] batch:  173 | acc: 31.25%,  total acc: 67.06%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 66.93%   [EVAL] batch:  175 | acc: 100.00%,  total acc: 67.12%   [EVAL] batch:  176 | acc: 100.00%,  total acc: 67.30%   [EVAL] batch:  177 | acc: 100.00%,  total acc: 67.49%   [EVAL] batch:  178 | acc: 100.00%,  total acc: 67.67%   [EVAL] batch:  179 | acc: 100.00%,  total acc: 67.85%   [EVAL] batch:  180 | acc: 100.00%,  total acc: 68.02%   [EVAL] batch:  181 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 68.07%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 68.21%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 68.28%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 68.28%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 68.32%   [EVAL] batch:  187 | acc: 75.00%,  total acc: 68.35%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 68.15%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 68.03%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 67.87%   [EVAL] batch:  191 | acc: 25.00%,  total acc: 67.64%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 67.52%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 67.27%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 67.31%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 67.28%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 67.32%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 67.39%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 67.40%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 67.47%   [EVAL] batch:  200 | acc: 75.00%,  total acc: 67.51%   [EVAL] batch:  201 | acc: 75.00%,  total acc: 67.54%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 67.58%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 67.62%   [EVAL] batch:  204 | acc: 75.00%,  total acc: 67.65%   [EVAL] batch:  205 | acc: 62.50%,  total acc: 67.63%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 67.72%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 67.88%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.03%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 68.28%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 68.43%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 68.52%   [EVAL] batch:  213 | acc: 87.50%,  total acc: 68.60%   [EVAL] batch:  214 | acc: 87.50%,  total acc: 68.69%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 69.09%   [EVAL] batch:  218 | acc: 87.50%,  total acc: 69.18%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  221 | acc: 93.75%,  total acc: 69.57%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 69.67%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 69.78%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 69.91%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 69.91%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 69.87%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 69.90%   [EVAL] batch:  229 | acc: 25.00%,  total acc: 69.70%   [EVAL] batch:  230 | acc: 81.25%,  total acc: 69.75%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 69.85%   [EVAL] batch:  232 | acc: 81.25%,  total acc: 69.90%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 69.98%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 70.08%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 70.07%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 70.12%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 70.12%   [EVAL] batch:  238 | acc: 62.50%,  total acc: 70.08%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 70.05%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 70.02%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 70.04%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 69.98%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 69.88%   [EVAL] batch:  244 | acc: 68.75%,  total acc: 69.87%   [EVAL] batch:  245 | acc: 43.75%,  total acc: 69.77%   [EVAL] batch:  246 | acc: 50.00%,  total acc: 69.69%   [EVAL] batch:  247 | acc: 68.75%,  total acc: 69.68%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 69.73%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 69.67%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 69.77%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 69.82%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 69.89%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 69.91%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 70.12%   [EVAL] batch:  256 | acc: 50.00%,  total acc: 70.04%   [EVAL] batch:  257 | acc: 31.25%,  total acc: 69.89%   [EVAL] batch:  258 | acc: 50.00%,  total acc: 69.81%   [EVAL] batch:  259 | acc: 43.75%,  total acc: 69.71%   [EVAL] batch:  260 | acc: 18.75%,  total acc: 69.52%   [EVAL] batch:  261 | acc: 50.00%,  total acc: 69.44%   [EVAL] batch:  262 | acc: 68.75%,  total acc: 69.44%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 69.46%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 69.46%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 69.43%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 69.41%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 69.38%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 69.33%   [EVAL] batch:  269 | acc: 50.00%,  total acc: 69.26%   [EVAL] batch:  270 | acc: 75.00%,  total acc: 69.28%   [EVAL] batch:  271 | acc: 68.75%,  total acc: 69.28%   [EVAL] batch:  272 | acc: 81.25%,  total acc: 69.32%   [EVAL] batch:  273 | acc: 81.25%,  total acc: 69.37%   [EVAL] batch:  274 | acc: 50.00%,  total acc: 69.30%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 69.41%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 69.95%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 69.92%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 69.94%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 69.94%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 69.93%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 69.91%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 69.86%   [EVAL] batch:  287 | acc: 62.50%,  total acc: 69.84%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 69.94%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 70.15%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 70.25%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 70.53%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 70.63%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 70.73%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 70.92%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 71.08%   [EVAL] batch:  301 | acc: 81.25%,  total acc: 71.11%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 71.20%   [EVAL] batch:  303 | acc: 87.50%,  total acc: 71.26%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 71.29%   [EVAL] batch:  305 | acc: 75.00%,  total acc: 71.30%   [EVAL] batch:  306 | acc: 81.25%,  total acc: 71.34%   [EVAL] batch:  307 | acc: 75.00%,  total acc: 71.35%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 71.42%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 71.51%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 71.63%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 71.59%   [EVAL] batch:  313 | acc: 6.25%,  total acc: 71.38%   [EVAL] batch:  314 | acc: 12.50%,  total acc: 71.19%   [EVAL] batch:  315 | acc: 6.25%,  total acc: 70.98%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 70.82%   [EVAL] batch:  317 | acc: 0.00%,  total acc: 70.60%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 70.49%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 70.51%   [EVAL] batch:  320 | acc: 93.75%,  total acc: 70.58%   [EVAL] batch:  321 | acc: 87.50%,  total acc: 70.63%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 70.68%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 70.70%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 70.73%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 70.72%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 70.66%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 70.66%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 70.65%   [EVAL] batch:  329 | acc: 50.00%,  total acc: 70.59%   [EVAL] batch:  330 | acc: 56.25%,  total acc: 70.54%   [EVAL] batch:  331 | acc: 12.50%,  total acc: 70.37%   [EVAL] batch:  332 | acc: 18.75%,  total acc: 70.21%   [EVAL] batch:  333 | acc: 18.75%,  total acc: 70.06%   [EVAL] batch:  334 | acc: 18.75%,  total acc: 69.91%   [EVAL] batch:  335 | acc: 12.50%,  total acc: 69.74%   [EVAL] batch:  336 | acc: 18.75%,  total acc: 69.58%   [EVAL] batch:  337 | acc: 25.00%,  total acc: 69.45%   [EVAL] batch:  338 | acc: 0.00%,  total acc: 69.25%   [EVAL] batch:  339 | acc: 6.25%,  total acc: 69.06%   [EVAL] batch:  340 | acc: 0.00%,  total acc: 68.86%   [EVAL] batch:  341 | acc: 6.25%,  total acc: 68.68%   [EVAL] batch:  342 | acc: 6.25%,  total acc: 68.49%   [EVAL] batch:  343 | acc: 12.50%,  total acc: 68.33%   [EVAL] batch:  344 | acc: 75.00%,  total acc: 68.35%   [EVAL] batch:  345 | acc: 81.25%,  total acc: 68.39%   [EVAL] batch:  346 | acc: 81.25%,  total acc: 68.43%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.50%   [EVAL] batch:  348 | acc: 81.25%,  total acc: 68.54%   [EVAL] batch:  349 | acc: 56.25%,  total acc: 68.50%   [EVAL] batch:  350 | acc: 100.00%,  total acc: 68.59%   [EVAL] batch:  351 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  352 | acc: 87.50%,  total acc: 68.71%   [EVAL] batch:  353 | acc: 93.75%,  total acc: 68.79%   [EVAL] batch:  354 | acc: 87.50%,  total acc: 68.84%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 68.86%   [EVAL] batch:  357 | acc: 12.50%,  total acc: 68.70%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 68.59%   [EVAL] batch:  359 | acc: 18.75%,  total acc: 68.45%   [EVAL] batch:  360 | acc: 37.50%,  total acc: 68.37%   [EVAL] batch:  361 | acc: 25.00%,  total acc: 68.25%   [EVAL] batch:  362 | acc: 18.75%,  total acc: 68.11%   [EVAL] batch:  363 | acc: 25.00%,  total acc: 67.99%   [EVAL] batch:  364 | acc: 50.00%,  total acc: 67.95%   [EVAL] batch:  365 | acc: 50.00%,  total acc: 67.90%   [EVAL] batch:  366 | acc: 37.50%,  total acc: 67.81%   [EVAL] batch:  367 | acc: 50.00%,  total acc: 67.76%   [EVAL] batch:  368 | acc: 50.00%,  total acc: 67.72%   [EVAL] batch:  369 | acc: 43.75%,  total acc: 67.65%   [EVAL] batch:  370 | acc: 37.50%,  total acc: 67.57%   [EVAL] batch:  371 | acc: 43.75%,  total acc: 67.51%   [EVAL] batch:  372 | acc: 62.50%,  total acc: 67.49%   [EVAL] batch:  373 | acc: 75.00%,  total acc: 67.51%   [EVAL] batch:  374 | acc: 50.00%,  total acc: 67.47%   [EVAL] batch:  375 | acc: 62.50%,  total acc: 67.45%   [EVAL] batch:  376 | acc: 31.25%,  total acc: 67.36%   [EVAL] batch:  377 | acc: 68.75%,  total acc: 67.36%   [EVAL] batch:  378 | acc: 81.25%,  total acc: 67.40%   [EVAL] batch:  379 | acc: 75.00%,  total acc: 67.42%   [EVAL] batch:  380 | acc: 62.50%,  total acc: 67.40%   [EVAL] batch:  381 | acc: 75.00%,  total acc: 67.42%   [EVAL] batch:  382 | acc: 43.75%,  total acc: 67.36%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 67.37%   [EVAL] batch:  384 | acc: 56.25%,  total acc: 67.34%   [EVAL] batch:  385 | acc: 62.50%,  total acc: 67.33%   [EVAL] batch:  386 | acc: 56.25%,  total acc: 67.30%   [EVAL] batch:  387 | acc: 37.50%,  total acc: 67.22%   [EVAL] batch:  388 | acc: 12.50%,  total acc: 67.08%   [EVAL] batch:  389 | acc: 31.25%,  total acc: 66.99%   [EVAL] batch:  390 | acc: 6.25%,  total acc: 66.83%   [EVAL] batch:  391 | acc: 25.00%,  total acc: 66.73%   [EVAL] batch:  392 | acc: 6.25%,  total acc: 66.57%   [EVAL] batch:  393 | acc: 31.25%,  total acc: 66.48%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 66.55%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 66.64%   [EVAL] batch:  396 | acc: 93.75%,  total acc: 66.70%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 66.79%   [EVAL] batch:  398 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 66.94%   [EVAL] batch:  400 | acc: 18.75%,  total acc: 66.82%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 66.68%   [EVAL] batch:  402 | acc: 6.25%,  total acc: 66.53%   [EVAL] batch:  403 | acc: 25.00%,  total acc: 66.43%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 66.28%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 66.12%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 66.12%   [EVAL] batch:  407 | acc: 81.25%,  total acc: 66.16%   [EVAL] batch:  408 | acc: 81.25%,  total acc: 66.20%   [EVAL] batch:  409 | acc: 68.75%,  total acc: 66.20%   [EVAL] batch:  410 | acc: 56.25%,  total acc: 66.18%   [EVAL] batch:  411 | acc: 75.00%,  total acc: 66.20%   [EVAL] batch:  412 | acc: 62.50%,  total acc: 66.19%   [EVAL] batch:  413 | acc: 62.50%,  total acc: 66.18%   [EVAL] batch:  414 | acc: 62.50%,  total acc: 66.17%   [EVAL] batch:  415 | acc: 43.75%,  total acc: 66.12%   [EVAL] batch:  416 | acc: 31.25%,  total acc: 66.04%   [EVAL] batch:  417 | acc: 43.75%,  total acc: 65.98%   [EVAL] batch:  418 | acc: 50.00%,  total acc: 65.95%   [EVAL] batch:  419 | acc: 6.25%,  total acc: 65.80%   [EVAL] batch:  420 | acc: 18.75%,  total acc: 65.69%   [EVAL] batch:  421 | acc: 6.25%,  total acc: 65.55%   [EVAL] batch:  422 | acc: 12.50%,  total acc: 65.43%   [EVAL] batch:  423 | acc: 18.75%,  total acc: 65.32%   [EVAL] batch:  424 | acc: 6.25%,  total acc: 65.18%   [EVAL] batch:  425 | acc: 50.00%,  total acc: 65.14%   [EVAL] batch:  426 | acc: 68.75%,  total acc: 65.15%   [EVAL] batch:  427 | acc: 75.00%,  total acc: 65.17%   [EVAL] batch:  428 | acc: 62.50%,  total acc: 65.17%   [EVAL] batch:  429 | acc: 62.50%,  total acc: 65.16%   [EVAL] batch:  430 | acc: 68.75%,  total acc: 65.17%   [EVAL] batch:  431 | acc: 75.00%,  total acc: 65.19%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 65.24%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 65.31%   [EVAL] batch:  434 | acc: 93.75%,  total acc: 65.37%   [EVAL] batch:  435 | acc: 87.50%,  total acc: 65.42%   [EVAL] batch:  436 | acc: 75.00%,  total acc: 65.45%   [EVAL] batch:  437 | acc: 56.25%,  total acc: 65.43%   [EVAL] batch:  438 | acc: 50.00%,  total acc: 65.39%   [EVAL] batch:  439 | acc: 56.25%,  total acc: 65.37%   [EVAL] batch:  440 | acc: 37.50%,  total acc: 65.31%   [EVAL] batch:  441 | acc: 50.00%,  total acc: 65.27%   [EVAL] batch:  442 | acc: 43.75%,  total acc: 65.22%   [EVAL] batch:  443 | acc: 50.00%,  total acc: 65.19%   [EVAL] batch:  444 | acc: 68.75%,  total acc: 65.20%   [EVAL] batch:  445 | acc: 68.75%,  total acc: 65.20%   [EVAL] batch:  446 | acc: 62.50%,  total acc: 65.20%   [EVAL] batch:  447 | acc: 62.50%,  total acc: 65.19%   [EVAL] batch:  448 | acc: 25.00%,  total acc: 65.10%   [EVAL] batch:  449 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  450 | acc: 87.50%,  total acc: 65.23%   [EVAL] batch:  451 | acc: 93.75%,  total acc: 65.29%   [EVAL] batch:  452 | acc: 75.00%,  total acc: 65.31%   [EVAL] batch:  453 | acc: 81.25%,  total acc: 65.35%   [EVAL] batch:  454 | acc: 93.75%,  total acc: 65.41%   [EVAL] batch:  455 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  456 | acc: 93.75%,  total acc: 65.55%   [EVAL] batch:  457 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 65.76%   [EVAL] batch:  460 | acc: 93.75%,  total acc: 65.82%   [EVAL] batch:  461 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  462 | acc: 62.50%,  total acc: 65.89%   [EVAL] batch:  463 | acc: 37.50%,  total acc: 65.83%   [EVAL] batch:  464 | acc: 25.00%,  total acc: 65.74%   [EVAL] batch:  465 | acc: 62.50%,  total acc: 65.73%   [EVAL] batch:  466 | acc: 37.50%,  total acc: 65.67%   [EVAL] batch:  467 | acc: 50.00%,  total acc: 65.64%   [EVAL] batch:  468 | acc: 31.25%,  total acc: 65.57%   [EVAL] batch:  469 | acc: 6.25%,  total acc: 65.44%   [EVAL] batch:  470 | acc: 0.00%,  total acc: 65.30%   [EVAL] batch:  471 | acc: 6.25%,  total acc: 65.17%   [EVAL] batch:  472 | acc: 0.00%,  total acc: 65.04%   [EVAL] batch:  473 | acc: 0.00%,  total acc: 64.90%   [EVAL] batch:  474 | acc: 6.25%,  total acc: 64.78%   [EVAL] batch:  475 | acc: 81.25%,  total acc: 64.81%   [EVAL] batch:  476 | acc: 75.00%,  total acc: 64.83%   [EVAL] batch:  477 | acc: 56.25%,  total acc: 64.81%   [EVAL] batch:  478 | acc: 62.50%,  total acc: 64.81%   [EVAL] batch:  479 | acc: 68.75%,  total acc: 64.82%   [EVAL] batch:  480 | acc: 68.75%,  total acc: 64.83%   [EVAL] batch:  481 | acc: 68.75%,  total acc: 64.83%   [EVAL] batch:  482 | acc: 68.75%,  total acc: 64.84%   [EVAL] batch:  483 | acc: 62.50%,  total acc: 64.84%   [EVAL] batch:  484 | acc: 50.00%,  total acc: 64.81%   [EVAL] batch:  485 | acc: 43.75%,  total acc: 64.76%   [EVAL] batch:  486 | acc: 56.25%,  total acc: 64.75%   [EVAL] batch:  487 | acc: 93.75%,  total acc: 64.81%   [EVAL] batch:  488 | acc: 100.00%,  total acc: 64.88%   [EVAL] batch:  489 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 65.02%   [EVAL] batch:  491 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:  492 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:  493 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  494 | acc: 100.00%,  total acc: 65.30%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 65.35%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 65.40%   [EVAL] batch:  497 | acc: 93.75%,  total acc: 65.46%   [EVAL] batch:  498 | acc: 100.00%,  total acc: 65.53%   [EVAL] batch:  499 | acc: 100.00%,  total acc: 65.60%   
cur_acc:  ['0.9286', '0.7817', '0.7490', '0.7530', '0.8393', '0.5784', '0.5962', '0.6597']
his_acc:  ['0.9286', '0.8615', '0.8045', '0.7590', '0.7656', '0.7268', '0.6745', '0.6560']
--------Round  3
seed:  400
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 0 1 2 5 3 4 6]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 2 0 3 3 2 0 1 1]
Losses:  20.088953018188477 6.057196617126465 0.9910752773284912
CurrentTrain: epoch  0, batch     0 | loss: 20.0889530Losses:  18.019695281982422 4.007472991943359 1.2255125045776367
CurrentTrain: epoch  0, batch     1 | loss: 18.0196953Losses:  20.904451370239258 7.013500690460205 1.1639232635498047
CurrentTrain: epoch  0, batch     2 | loss: 20.9044514Losses:  19.311092376708984 5.433562278747559 1.1623311042785645
CurrentTrain: epoch  0, batch     3 | loss: 19.3110924Losses:  19.289690017700195 6.1619415283203125 1.0956532955169678
CurrentTrain: epoch  0, batch     4 | loss: 19.2896900Losses:  17.99156951904297 4.760045051574707 0.9947044849395752
CurrentTrain: epoch  0, batch     5 | loss: 17.9915695Losses:  17.186790466308594 4.249220848083496 1.0319406986236572
CurrentTrain: epoch  0, batch     6 | loss: 17.1867905Losses:  17.059642791748047 3.9212117195129395 0.9816431999206543
CurrentTrain: epoch  0, batch     7 | loss: 17.0596428Losses:  19.59913444519043 7.153836727142334 0.9393409490585327
CurrentTrain: epoch  0, batch     8 | loss: 19.5991344Losses:  17.469966888427734 4.603085041046143 0.9099172353744507
CurrentTrain: epoch  0, batch     9 | loss: 17.4699669Losses:  15.465264320373535 3.0350053310394287 0.8502978086471558
CurrentTrain: epoch  0, batch    10 | loss: 15.4652643Losses:  17.671958923339844 5.462100982666016 0.8333451151847839
CurrentTrain: epoch  0, batch    11 | loss: 17.6719589Losses:  18.4838809967041 6.447478294372559 0.6820287108421326
CurrentTrain: epoch  0, batch    12 | loss: 18.4838810Losses:  18.576154708862305 6.7055511474609375 0.7345735430717468
CurrentTrain: epoch  0, batch    13 | loss: 18.5761547Losses:  17.9632568359375 6.431609153747559 0.6646742224693298
CurrentTrain: epoch  0, batch    14 | loss: 17.9632568Losses:  14.531209945678711 3.0832180976867676 0.7236542701721191
CurrentTrain: epoch  0, batch    15 | loss: 14.5312099Losses:  17.561174392700195 6.18558406829834 0.6735175251960754
CurrentTrain: epoch  0, batch    16 | loss: 17.5611744Losses:  15.012327194213867 3.7508933544158936 0.6561304330825806
CurrentTrain: epoch  0, batch    17 | loss: 15.0123272Losses:  16.546131134033203 5.481260299682617 0.5713372826576233
CurrentTrain: epoch  0, batch    18 | loss: 16.5461311Losses:  19.53382682800293 8.158452987670898 0.469205379486084
CurrentTrain: epoch  0, batch    19 | loss: 19.5338268Losses:  15.621885299682617 4.409568786621094 0.5866820812225342
CurrentTrain: epoch  0, batch    20 | loss: 15.6218853Losses:  15.802017211914062 5.045585632324219 0.5524978637695312
CurrentTrain: epoch  0, batch    21 | loss: 15.8020172Losses:  15.43027114868164 4.236808776855469 0.6222869157791138
CurrentTrain: epoch  0, batch    22 | loss: 15.4302711Losses:  15.548028945922852 4.30148983001709 0.5780743360519409
CurrentTrain: epoch  0, batch    23 | loss: 15.5480289Losses:  18.387474060058594 7.406563758850098 0.5539135336875916
CurrentTrain: epoch  0, batch    24 | loss: 18.3874741Losses:  19.377405166625977 7.821474075317383 0.5492063760757446
CurrentTrain: epoch  0, batch    25 | loss: 19.3774052Losses:  15.065605163574219 4.152237892150879 0.5411379337310791
CurrentTrain: epoch  0, batch    26 | loss: 15.0656052Losses:  17.426267623901367 6.5897111892700195 0.5431962013244629
CurrentTrain: epoch  0, batch    27 | loss: 17.4262676Losses:  15.506980895996094 4.868825912475586 0.5113062858581543
CurrentTrain: epoch  0, batch    28 | loss: 15.5069809Losses:  15.158807754516602 4.590594291687012 0.47111326456069946
CurrentTrain: epoch  0, batch    29 | loss: 15.1588078Losses:  15.297115325927734 4.664294242858887 0.552609920501709
CurrentTrain: epoch  0, batch    30 | loss: 15.2971153Losses:  15.93626880645752 5.779074668884277 0.46051275730133057
CurrentTrain: epoch  0, batch    31 | loss: 15.9362688Losses:  20.077301025390625 9.968408584594727 0.3899533450603485
CurrentTrain: epoch  0, batch    32 | loss: 20.0773010Losses:  14.078064918518066 3.410930871963501 0.598382294178009
CurrentTrain: epoch  0, batch    33 | loss: 14.0780649Losses:  15.663207054138184 5.534373760223389 0.523589551448822
CurrentTrain: epoch  0, batch    34 | loss: 15.6632071Losses:  15.031388282775879 4.455791473388672 0.5574264526367188
CurrentTrain: epoch  0, batch    35 | loss: 15.0313883Losses:  15.506487846374512 5.39622688293457 0.526802659034729
CurrentTrain: epoch  0, batch    36 | loss: 15.5064878Losses:  16.30621910095215 5.879135608673096 0.5098178386688232
CurrentTrain: epoch  0, batch    37 | loss: 16.3062191Losses:  14.897364616394043 4.436533451080322 0.5320746898651123
CurrentTrain: epoch  0, batch    38 | loss: 14.8973646Losses:  16.168657302856445 6.245415687561035 0.4542621076107025
CurrentTrain: epoch  0, batch    39 | loss: 16.1686573Losses:  13.571861267089844 4.0515031814575195 0.4851379990577698
CurrentTrain: epoch  0, batch    40 | loss: 13.5718613Losses:  14.169081687927246 5.212497234344482 0.4628608524799347
CurrentTrain: epoch  0, batch    41 | loss: 14.1690817Losses:  16.34841537475586 6.093636989593506 0.5248581171035767
CurrentTrain: epoch  0, batch    42 | loss: 16.3484154Losses:  14.459565162658691 4.8797807693481445 0.5063968896865845
CurrentTrain: epoch  0, batch    43 | loss: 14.4595652Losses:  12.466999053955078 3.5033109188079834 0.45338794589042664
CurrentTrain: epoch  0, batch    44 | loss: 12.4669991Losses:  12.08526611328125 3.092130422592163 0.45890307426452637
CurrentTrain: epoch  0, batch    45 | loss: 12.0852661Losses:  11.873032569885254 2.62127685546875 0.443976491689682
CurrentTrain: epoch  0, batch    46 | loss: 11.8730326Losses:  14.38431167602539 4.560723304748535 0.42516881227493286
CurrentTrain: epoch  0, batch    47 | loss: 14.3843117Losses:  14.99412727355957 5.308880805969238 0.46162277460098267
CurrentTrain: epoch  0, batch    48 | loss: 14.9941273Losses:  12.281017303466797 2.599839687347412 0.392680823802948
CurrentTrain: epoch  0, batch    49 | loss: 12.2810173Losses:  14.557605743408203 5.864963054656982 0.398088276386261
CurrentTrain: epoch  0, batch    50 | loss: 14.5576057Losses:  13.360971450805664 3.9117214679718018 0.40917742252349854
CurrentTrain: epoch  0, batch    51 | loss: 13.3609715Losses:  13.674001693725586 4.1076812744140625 0.40726304054260254
CurrentTrain: epoch  0, batch    52 | loss: 13.6740017Losses:  12.889704704284668 4.059966564178467 0.42626455426216125
CurrentTrain: epoch  0, batch    53 | loss: 12.8897047Losses:  12.827221870422363 3.2314765453338623 0.40332967042922974
CurrentTrain: epoch  0, batch    54 | loss: 12.8272219Losses:  13.13258171081543 4.110160827636719 0.4173399806022644
CurrentTrain: epoch  0, batch    55 | loss: 13.1325817Losses:  13.937081336975098 5.286290168762207 0.3820788860321045
CurrentTrain: epoch  0, batch    56 | loss: 13.9370813Losses:  12.643641471862793 3.914255380630493 0.4070805013179779
CurrentTrain: epoch  0, batch    57 | loss: 12.6436415Losses:  12.037700653076172 3.103424072265625 0.38442757725715637
CurrentTrain: epoch  0, batch    58 | loss: 12.0377007Losses:  12.638229370117188 4.68095588684082 0.36633506417274475
CurrentTrain: epoch  0, batch    59 | loss: 12.6382294Losses:  11.250426292419434 3.30599308013916 0.3609936535358429
CurrentTrain: epoch  0, batch    60 | loss: 11.2504263Losses:  15.03860855102539 5.906953811645508 0.3852912187576294
CurrentTrain: epoch  0, batch    61 | loss: 15.0386086Losses:  8.136446952819824 1.145721435546875 0.22863344848155975
CurrentTrain: epoch  0, batch    62 | loss: 8.1364470Losses:  13.445087432861328 5.209290981292725 0.38687658309936523
CurrentTrain: epoch  1, batch     0 | loss: 13.4450874Losses:  12.347234725952148 4.349294662475586 0.2808838486671448
CurrentTrain: epoch  1, batch     1 | loss: 12.3472347Losses:  13.62524127960205 5.219545364379883 0.3943106234073639
CurrentTrain: epoch  1, batch     2 | loss: 13.6252413Losses:  12.955912590026855 4.689009666442871 0.3832727074623108
CurrentTrain: epoch  1, batch     3 | loss: 12.9559126Losses:  16.387718200683594 8.580997467041016 0.3521502614021301
CurrentTrain: epoch  1, batch     4 | loss: 16.3877182Losses:  13.23080062866211 4.449901580810547 0.3742135167121887
CurrentTrain: epoch  1, batch     5 | loss: 13.2308006Losses:  10.764058113098145 3.2959091663360596 0.3607247769832611
CurrentTrain: epoch  1, batch     6 | loss: 10.7640581Losses:  16.67705535888672 8.510819435119629 0.3559918701648712
CurrentTrain: epoch  1, batch     7 | loss: 16.6770554Losses:  10.841501235961914 2.6978254318237305 0.372683584690094
CurrentTrain: epoch  1, batch     8 | loss: 10.8415012Losses:  10.003216743469238 2.1586251258850098 0.36943697929382324
CurrentTrain: epoch  1, batch     9 | loss: 10.0032167Losses:  11.187745094299316 3.510197639465332 0.34535515308380127
CurrentTrain: epoch  1, batch    10 | loss: 11.1877451Losses:  12.343530654907227 4.088088035583496 0.33984389901161194
CurrentTrain: epoch  1, batch    11 | loss: 12.3435307Losses:  12.947527885437012 4.695564270019531 0.34199634194374084
CurrentTrain: epoch  1, batch    12 | loss: 12.9475279Losses:  11.182823181152344 2.8705642223358154 0.34469524025917053
CurrentTrain: epoch  1, batch    13 | loss: 11.1828232Losses:  11.252909660339355 3.7182459831237793 0.3660190999507904
CurrentTrain: epoch  1, batch    14 | loss: 11.2529097Losses:  11.140397071838379 3.3300728797912598 0.35526174306869507
CurrentTrain: epoch  1, batch    15 | loss: 11.1403971Losses:  13.267684936523438 4.824897289276123 0.360767662525177
CurrentTrain: epoch  1, batch    16 | loss: 13.2676849Losses:  9.990647315979004 2.6797280311584473 0.32606640458106995
CurrentTrain: epoch  1, batch    17 | loss: 9.9906473Losses:  12.607322692871094 4.525959491729736 0.35395002365112305
CurrentTrain: epoch  1, batch    18 | loss: 12.6073227Losses:  13.682168006896973 5.759607315063477 0.3034072518348694
CurrentTrain: epoch  1, batch    19 | loss: 13.6821680Losses:  9.915448188781738 2.816518545150757 0.3610341548919678
CurrentTrain: epoch  1, batch    20 | loss: 9.9154482Losses:  10.070488929748535 2.0212173461914062 0.3466094136238098
CurrentTrain: epoch  1, batch    21 | loss: 10.0704889Losses:  10.837727546691895 2.6027309894561768 0.31990498304367065
CurrentTrain: epoch  1, batch    22 | loss: 10.8377275Losses:  13.85303020477295 4.992391586303711 0.33846575021743774
CurrentTrain: epoch  1, batch    23 | loss: 13.8530302Losses:  12.770333290100098 6.039653778076172 0.3111054003238678
CurrentTrain: epoch  1, batch    24 | loss: 12.7703333Losses:  10.022274017333984 2.7442498207092285 0.32702088356018066
CurrentTrain: epoch  1, batch    25 | loss: 10.0222740Losses:  11.798372268676758 3.739367961883545 0.3244054913520813
CurrentTrain: epoch  1, batch    26 | loss: 11.7983723Losses:  11.708026885986328 4.075231552124023 0.3301015794277191
CurrentTrain: epoch  1, batch    27 | loss: 11.7080269Losses:  11.130059242248535 3.861462116241455 0.3358853757381439
CurrentTrain: epoch  1, batch    28 | loss: 11.1300592Losses:  10.85519027709961 3.8023688793182373 0.3227829337120056
CurrentTrain: epoch  1, batch    29 | loss: 10.8551903Losses:  10.708252906799316 3.475546360015869 0.3261007070541382
CurrentTrain: epoch  1, batch    30 | loss: 10.7082529Losses:  11.260337829589844 3.2990503311157227 0.35048848390579224
CurrentTrain: epoch  1, batch    31 | loss: 11.2603378Losses:  11.696267127990723 4.402726650238037 0.34012454748153687
CurrentTrain: epoch  1, batch    32 | loss: 11.6962671Losses:  11.309667587280273 4.394265174865723 0.3682211935520172
CurrentTrain: epoch  1, batch    33 | loss: 11.3096676Losses:  10.917973518371582 4.332009315490723 0.31801319122314453
CurrentTrain: epoch  1, batch    34 | loss: 10.9179735Losses:  10.897689819335938 4.233372688293457 0.31406378746032715
CurrentTrain: epoch  1, batch    35 | loss: 10.8976898Losses:  12.299332618713379 5.113491058349609 0.2238665223121643
CurrentTrain: epoch  1, batch    36 | loss: 12.2993326Losses:  8.954310417175293 2.467985153198242 0.3177027106285095
CurrentTrain: epoch  1, batch    37 | loss: 8.9543104Losses:  9.466602325439453 2.6639580726623535 0.33353254199028015
CurrentTrain: epoch  1, batch    38 | loss: 9.4666023Losses:  12.580179214477539 4.8135480880737305 0.32129746675491333
CurrentTrain: epoch  1, batch    39 | loss: 12.5801792Losses:  12.678168296813965 5.660001754760742 0.3350217938423157
CurrentTrain: epoch  1, batch    40 | loss: 12.6781683Losses:  12.197882652282715 4.225765228271484 0.3656049370765686
CurrentTrain: epoch  1, batch    41 | loss: 12.1978827Losses:  9.040567398071289 2.1617321968078613 0.35239577293395996
CurrentTrain: epoch  1, batch    42 | loss: 9.0405674Losses:  12.885047912597656 5.262810707092285 0.36992692947387695
CurrentTrain: epoch  1, batch    43 | loss: 12.8850479Losses:  10.451033592224121 3.4906606674194336 0.33301886916160583
CurrentTrain: epoch  1, batch    44 | loss: 10.4510336Losses:  11.808534622192383 4.824647903442383 0.21820960938930511
CurrentTrain: epoch  1, batch    45 | loss: 11.8085346Losses:  11.192892074584961 3.300368547439575 0.35450565814971924
CurrentTrain: epoch  1, batch    46 | loss: 11.1928921Losses:  11.50355052947998 3.7617006301879883 0.32286128401756287
CurrentTrain: epoch  1, batch    47 | loss: 11.5035505Losses:  11.480592727661133 3.7082624435424805 0.34800615906715393
CurrentTrain: epoch  1, batch    48 | loss: 11.4805927Losses:  11.74540901184082 4.413959503173828 0.3184092342853546
CurrentTrain: epoch  1, batch    49 | loss: 11.7454090Losses:  11.21656608581543 3.624638557434082 0.33888232707977295
CurrentTrain: epoch  1, batch    50 | loss: 11.2165661Losses:  13.282941818237305 6.16923189163208 0.33558839559555054
CurrentTrain: epoch  1, batch    51 | loss: 13.2829418Losses:  13.396126747131348 5.950808525085449 0.33409708738327026
CurrentTrain: epoch  1, batch    52 | loss: 13.3961267Losses:  11.370565414428711 4.266233921051025 0.31799161434173584
CurrentTrain: epoch  1, batch    53 | loss: 11.3705654Losses:  14.347504615783691 7.599358558654785 0.3084174394607544
CurrentTrain: epoch  1, batch    54 | loss: 14.3475046Losses:  8.479776382446289 2.0438241958618164 0.30681541562080383
CurrentTrain: epoch  1, batch    55 | loss: 8.4797764Losses:  9.849695205688477 3.051813840866089 0.29828619956970215
CurrentTrain: epoch  1, batch    56 | loss: 9.8496952Losses:  10.30620002746582 3.740736246109009 0.3027503490447998
CurrentTrain: epoch  1, batch    57 | loss: 10.3062000Losses:  11.710199356079102 4.975076198577881 0.31472599506378174
CurrentTrain: epoch  1, batch    58 | loss: 11.7101994Losses:  10.08160400390625 2.944103479385376 0.311055451631546
CurrentTrain: epoch  1, batch    59 | loss: 10.0816040Losses:  9.303217887878418 3.4299488067626953 0.3212525248527527
CurrentTrain: epoch  1, batch    60 | loss: 9.3032179Losses:  12.32569694519043 5.655796051025391 0.32702726125717163
CurrentTrain: epoch  1, batch    61 | loss: 12.3256969Losses:  8.917060852050781 1.8396910429000854 0.30753058195114136
CurrentTrain: epoch  1, batch    62 | loss: 8.9170609Losses:  10.070348739624023 3.70792555809021 0.3205025792121887
CurrentTrain: epoch  2, batch     0 | loss: 10.0703487Losses:  10.799174308776855 4.494972229003906 0.30569565296173096
CurrentTrain: epoch  2, batch     1 | loss: 10.7991743Losses:  12.826623916625977 6.349762439727783 0.3040871322154999
CurrentTrain: epoch  2, batch     2 | loss: 12.8266239Losses:  8.909994125366211 2.439751148223877 0.30700477957725525
CurrentTrain: epoch  2, batch     3 | loss: 8.9099941Losses:  9.531624794006348 3.1829833984375 0.3031286597251892
CurrentTrain: epoch  2, batch     4 | loss: 9.5316248Losses:  10.529211044311523 4.570159912109375 0.29881060123443604
CurrentTrain: epoch  2, batch     5 | loss: 10.5292110Losses:  14.239933967590332 7.7668609619140625 0.28849273920059204
CurrentTrain: epoch  2, batch     6 | loss: 14.2399340Losses:  9.322105407714844 3.6191318035125732 0.29642340540885925
CurrentTrain: epoch  2, batch     7 | loss: 9.3221054Losses:  10.77026081085205 4.7438883781433105 0.24147526919841766
CurrentTrain: epoch  2, batch     8 | loss: 10.7702608Losses:  12.858603477478027 6.917497158050537 0.3056741952896118
CurrentTrain: epoch  2, batch     9 | loss: 12.8586035Losses:  12.025028228759766 6.19083833694458 0.30596742033958435
CurrentTrain: epoch  2, batch    10 | loss: 12.0250282Losses:  9.221375465393066 2.837028741836548 0.285076379776001
CurrentTrain: epoch  2, batch    11 | loss: 9.2213755Losses:  12.037846565246582 5.656082630157471 0.38294488191604614
CurrentTrain: epoch  2, batch    12 | loss: 12.0378466Losses:  9.037074089050293 2.8850574493408203 0.31074923276901245
CurrentTrain: epoch  2, batch    13 | loss: 9.0370741Losses:  7.646674156188965 1.9474767446517944 0.27874910831451416
CurrentTrain: epoch  2, batch    14 | loss: 7.6466742Losses:  9.953483581542969 3.5655593872070312 0.2999432384967804
CurrentTrain: epoch  2, batch    15 | loss: 9.9534836Losses:  9.196063995361328 3.059337854385376 0.29969650506973267
CurrentTrain: epoch  2, batch    16 | loss: 9.1960640Losses:  12.260514259338379 5.747583866119385 0.20566074550151825
CurrentTrain: epoch  2, batch    17 | loss: 12.2605143Losses:  9.677960395812988 3.269256114959717 0.296034574508667
CurrentTrain: epoch  2, batch    18 | loss: 9.6779604Losses:  10.772547721862793 4.557852745056152 0.28023627400398254
CurrentTrain: epoch  2, batch    19 | loss: 10.7725477Losses:  10.056145668029785 3.581956624984741 0.2666921019554138
CurrentTrain: epoch  2, batch    20 | loss: 10.0561457Losses:  10.660149574279785 4.426219940185547 0.3189844489097595
CurrentTrain: epoch  2, batch    21 | loss: 10.6601496Losses:  10.814508438110352 4.6618499755859375 0.3031001389026642
CurrentTrain: epoch  2, batch    22 | loss: 10.8145084Losses:  7.795966148376465 2.113083839416504 0.27552297711372375
CurrentTrain: epoch  2, batch    23 | loss: 7.7959661Losses:  12.597012519836426 6.11913537979126 0.31618431210517883
CurrentTrain: epoch  2, batch    24 | loss: 12.5970125Losses:  9.822662353515625 4.0341997146606445 0.28635087609291077
CurrentTrain: epoch  2, batch    25 | loss: 9.8226624Losses:  9.52980899810791 3.7011477947235107 0.28340208530426025
CurrentTrain: epoch  2, batch    26 | loss: 9.5298090Losses:  10.109254837036133 4.437277793884277 0.30591994524002075
CurrentTrain: epoch  2, batch    27 | loss: 10.1092548Losses:  8.310365676879883 2.296466827392578 0.2692773640155792
CurrentTrain: epoch  2, batch    28 | loss: 8.3103657Losses:  8.41101360321045 2.3844151496887207 0.2774689495563507
CurrentTrain: epoch  2, batch    29 | loss: 8.4110136Losses:  12.373519897460938 4.899993419647217 0.31130754947662354
CurrentTrain: epoch  2, batch    30 | loss: 12.3735199Losses:  8.194924354553223 2.8591833114624023 0.265643835067749
CurrentTrain: epoch  2, batch    31 | loss: 8.1949244Losses:  8.953949928283691 2.990809917449951 0.29545146226882935
CurrentTrain: epoch  2, batch    32 | loss: 8.9539499Losses:  8.101304054260254 2.7421391010284424 0.28256088495254517
CurrentTrain: epoch  2, batch    33 | loss: 8.1013041Losses:  8.68673038482666 2.435474395751953 0.2732473313808441
CurrentTrain: epoch  2, batch    34 | loss: 8.6867304Losses:  8.431466102600098 2.6388416290283203 0.258242666721344
CurrentTrain: epoch  2, batch    35 | loss: 8.4314661Losses:  8.196813583374023 2.4965715408325195 0.277360737323761
CurrentTrain: epoch  2, batch    36 | loss: 8.1968136Losses:  9.3038911819458 3.0247364044189453 0.2724565863609314
CurrentTrain: epoch  2, batch    37 | loss: 9.3038912Losses:  11.172261238098145 4.827207565307617 0.2776086628437042
CurrentTrain: epoch  2, batch    38 | loss: 11.1722612Losses:  9.201272010803223 3.2900028228759766 0.2685841917991638
CurrentTrain: epoch  2, batch    39 | loss: 9.2012720Losses:  11.020829200744629 5.235414505004883 0.26186585426330566
CurrentTrain: epoch  2, batch    40 | loss: 11.0208292Losses:  8.363235473632812 2.291398525238037 0.2596622407436371
CurrentTrain: epoch  2, batch    41 | loss: 8.3632355Losses:  10.798018455505371 3.3608145713806152 0.25927841663360596
CurrentTrain: epoch  2, batch    42 | loss: 10.7980185Losses:  11.736799240112305 6.410515308380127 0.2725442051887512
CurrentTrain: epoch  2, batch    43 | loss: 11.7367992Losses:  8.96496868133545 3.0600790977478027 0.2750077545642853
CurrentTrain: epoch  2, batch    44 | loss: 8.9649687Losses:  7.180440425872803 1.5274574756622314 0.26761019229888916
CurrentTrain: epoch  2, batch    45 | loss: 7.1804404Losses:  7.914876937866211 2.0016512870788574 0.2730586528778076
CurrentTrain: epoch  2, batch    46 | loss: 7.9148769Losses:  11.129213333129883 5.5604352951049805 0.2834228277206421
CurrentTrain: epoch  2, batch    47 | loss: 11.1292133Losses:  8.972794532775879 3.10325288772583 0.272845596075058
CurrentTrain: epoch  2, batch    48 | loss: 8.9727945Losses:  8.72166919708252 2.1661617755889893 0.2871343493461609
CurrentTrain: epoch  2, batch    49 | loss: 8.7216692Losses:  8.69874095916748 3.3306403160095215 0.2673385739326477
CurrentTrain: epoch  2, batch    50 | loss: 8.6987410Losses:  11.42729377746582 6.1146559715271 0.2879120111465454
CurrentTrain: epoch  2, batch    51 | loss: 11.4272938Losses:  7.704120635986328 2.0815494060516357 0.27390193939208984
CurrentTrain: epoch  2, batch    52 | loss: 7.7041206Losses:  9.983016967773438 4.016661167144775 0.2669902443885803
CurrentTrain: epoch  2, batch    53 | loss: 9.9830170Losses:  11.751261711120605 5.910665988922119 0.2880197763442993
CurrentTrain: epoch  2, batch    54 | loss: 11.7512617Losses:  8.566878318786621 2.726655960083008 0.2781944274902344
CurrentTrain: epoch  2, batch    55 | loss: 8.5668783Losses:  8.562605857849121 3.3342247009277344 0.26470398902893066
CurrentTrain: epoch  2, batch    56 | loss: 8.5626059Losses:  10.708623886108398 4.654409885406494 0.28485527634620667
CurrentTrain: epoch  2, batch    57 | loss: 10.7086239Losses:  11.675056457519531 5.715823173522949 0.28329232335090637
CurrentTrain: epoch  2, batch    58 | loss: 11.6750565Losses:  8.659780502319336 3.580843687057495 0.28374621272087097
CurrentTrain: epoch  2, batch    59 | loss: 8.6597805Losses:  9.849112510681152 3.9041991233825684 0.285263329744339
CurrentTrain: epoch  2, batch    60 | loss: 9.8491125Losses:  9.78620719909668 4.082602500915527 0.2763403058052063
CurrentTrain: epoch  2, batch    61 | loss: 9.7862072Losses:  6.1361870765686035 0.5345583558082581 0.29495084285736084
CurrentTrain: epoch  2, batch    62 | loss: 6.1361871Losses:  10.447394371032715 4.553536415100098 0.17475315928459167
CurrentTrain: epoch  3, batch     0 | loss: 10.4473944Losses:  7.742328643798828 2.176269054412842 0.26219314336776733
CurrentTrain: epoch  3, batch     1 | loss: 7.7423286Losses:  7.468442916870117 2.551701068878174 0.2703627645969391
CurrentTrain: epoch  3, batch     2 | loss: 7.4684429Losses:  9.105843544006348 3.478659152984619 0.2513945996761322
CurrentTrain: epoch  3, batch     3 | loss: 9.1058435Losses:  8.056992530822754 3.0629217624664307 0.2680000960826874
CurrentTrain: epoch  3, batch     4 | loss: 8.0569925Losses:  9.425837516784668 3.897899866104126 0.2975693345069885
CurrentTrain: epoch  3, batch     5 | loss: 9.4258375Losses:  9.796217918395996 4.557709217071533 0.29060935974121094
CurrentTrain: epoch  3, batch     6 | loss: 9.7962179Losses:  8.034415245056152 2.810307025909424 0.269275963306427
CurrentTrain: epoch  3, batch     7 | loss: 8.0344152Losses:  8.79343032836914 3.8389434814453125 0.28084737062454224
CurrentTrain: epoch  3, batch     8 | loss: 8.7934303Losses:  8.171198844909668 2.809857130050659 0.272477924823761
CurrentTrain: epoch  3, batch     9 | loss: 8.1711988Losses:  7.812360763549805 2.189134120941162 0.2614225149154663
CurrentTrain: epoch  3, batch    10 | loss: 7.8123608Losses:  8.288418769836426 2.863708019256592 0.17221204936504364
CurrentTrain: epoch  3, batch    11 | loss: 8.2884188Losses:  10.324040412902832 5.163236141204834 0.28295189142227173
CurrentTrain: epoch  3, batch    12 | loss: 10.3240404Losses:  7.345777988433838 2.154097080230713 0.26315706968307495
CurrentTrain: epoch  3, batch    13 | loss: 7.3457780Losses:  8.599471092224121 3.513272762298584 0.25060850381851196
CurrentTrain: epoch  3, batch    14 | loss: 8.5994711Losses:  8.06056022644043 3.3367295265197754 0.25048983097076416
CurrentTrain: epoch  3, batch    15 | loss: 8.0605602Losses:  6.82522439956665 1.769587755203247 0.24836289882659912
CurrentTrain: epoch  3, batch    16 | loss: 6.8252244Losses:  10.150833129882812 4.877572536468506 0.28236979246139526
CurrentTrain: epoch  3, batch    17 | loss: 10.1508331Losses:  8.209145545959473 3.215444326400757 0.25842007994651794
CurrentTrain: epoch  3, batch    18 | loss: 8.2091455Losses:  10.236274719238281 5.052944660186768 0.1557024121284485
CurrentTrain: epoch  3, batch    19 | loss: 10.2362747Losses:  7.936655521392822 2.841946601867676 0.2540651559829712
CurrentTrain: epoch  3, batch    20 | loss: 7.9366555Losses:  8.101028442382812 3.3243157863616943 0.2591618001461029
CurrentTrain: epoch  3, batch    21 | loss: 8.1010284Losses:  8.602180480957031 3.269458770751953 0.2508428692817688
CurrentTrain: epoch  3, batch    22 | loss: 8.6021805Losses:  7.77970027923584 3.005122184753418 0.241926908493042
CurrentTrain: epoch  3, batch    23 | loss: 7.7797003Losses:  6.7090325355529785 1.791874647140503 0.2586739659309387
CurrentTrain: epoch  3, batch    24 | loss: 6.7090325Losses:  8.952420234680176 3.0484209060668945 0.2814764678478241
CurrentTrain: epoch  3, batch    25 | loss: 8.9524202Losses:  11.103588104248047 5.50382137298584 0.30550217628479004
CurrentTrain: epoch  3, batch    26 | loss: 11.1035881Losses:  7.630812644958496 2.479954957962036 0.2520914077758789
CurrentTrain: epoch  3, batch    27 | loss: 7.6308126Losses:  9.188592910766602 4.214372634887695 0.24966052174568176
CurrentTrain: epoch  3, batch    28 | loss: 9.1885929Losses:  7.934512138366699 3.0221457481384277 0.27339136600494385
CurrentTrain: epoch  3, batch    29 | loss: 7.9345121Losses:  9.13427734375 4.199633598327637 0.21209995448589325
CurrentTrain: epoch  3, batch    30 | loss: 9.1342773Losses:  9.33008861541748 4.480262756347656 0.26384514570236206
CurrentTrain: epoch  3, batch    31 | loss: 9.3300886Losses:  9.298101425170898 3.6535463333129883 0.2619929909706116
CurrentTrain: epoch  3, batch    32 | loss: 9.2981014Losses:  8.615961074829102 3.777961254119873 0.2652340233325958
CurrentTrain: epoch  3, batch    33 | loss: 8.6159611Losses:  8.628321647644043 3.205660343170166 0.24649707973003387
CurrentTrain: epoch  3, batch    34 | loss: 8.6283216Losses:  7.723610877990723 2.870818853378296 0.25913241505622864
CurrentTrain: epoch  3, batch    35 | loss: 7.7236109Losses:  10.038674354553223 4.340271949768066 0.25887805223464966
CurrentTrain: epoch  3, batch    36 | loss: 10.0386744Losses:  6.945685863494873 2.2605173587799072 0.24108827114105225
CurrentTrain: epoch  3, batch    37 | loss: 6.9456859Losses:  8.550203323364258 3.8370370864868164 0.25123074650764465
CurrentTrain: epoch  3, batch    38 | loss: 8.5502033Losses:  9.110456466674805 3.8652899265289307 0.2705170214176178
CurrentTrain: epoch  3, batch    39 | loss: 9.1104565Losses:  10.54926586151123 5.221079349517822 0.1865304708480835
CurrentTrain: epoch  3, batch    40 | loss: 10.5492659Losses:  8.297369003295898 2.668787956237793 0.2556452751159668
CurrentTrain: epoch  3, batch    41 | loss: 8.2973690Losses:  8.493173599243164 3.097166061401367 0.24922409653663635
CurrentTrain: epoch  3, batch    42 | loss: 8.4931736Losses:  7.714560031890869 2.1709702014923096 0.2529083490371704
CurrentTrain: epoch  3, batch    43 | loss: 7.7145600Losses:  10.829339981079102 4.931009292602539 0.2598751485347748
CurrentTrain: epoch  3, batch    44 | loss: 10.8293400Losses:  11.776375770568848 5.669564247131348 0.2730559706687927
CurrentTrain: epoch  3, batch    45 | loss: 11.7763758Losses:  9.326008796691895 3.9489784240722656 0.2693088948726654
CurrentTrain: epoch  3, batch    46 | loss: 9.3260088Losses:  8.189093589782715 3.222818374633789 0.26562806963920593
CurrentTrain: epoch  3, batch    47 | loss: 8.1890936Losses:  7.792250633239746 2.9566867351531982 0.26720523834228516
CurrentTrain: epoch  3, batch    48 | loss: 7.7922506Losses:  8.337078094482422 2.9973113536834717 0.2502328157424927
CurrentTrain: epoch  3, batch    49 | loss: 8.3370781Losses:  6.980433464050293 2.2178759574890137 0.2580440044403076
CurrentTrain: epoch  3, batch    50 | loss: 6.9804335Losses:  7.712133884429932 2.6852965354919434 0.25257909297943115
CurrentTrain: epoch  3, batch    51 | loss: 7.7121339Losses:  8.273813247680664 3.3853793144226074 0.26206299662590027
CurrentTrain: epoch  3, batch    52 | loss: 8.2738132Losses:  8.00696849822998 1.7892107963562012 0.2409513294696808
CurrentTrain: epoch  3, batch    53 | loss: 8.0069685Losses:  7.6032209396362305 2.6884102821350098 0.2517277002334595
CurrentTrain: epoch  3, batch    54 | loss: 7.6032209Losses:  6.8098368644714355 2.077582597732544 0.24942940473556519
CurrentTrain: epoch  3, batch    55 | loss: 6.8098369Losses:  7.922270774841309 2.5688514709472656 0.2442474216222763
CurrentTrain: epoch  3, batch    56 | loss: 7.9222708Losses:  10.786142349243164 5.028855800628662 0.2822437286376953
CurrentTrain: epoch  3, batch    57 | loss: 10.7861423Losses:  9.66203498840332 4.673724174499512 0.2551458775997162
CurrentTrain: epoch  3, batch    58 | loss: 9.6620350Losses:  7.657591342926025 2.598881483078003 0.25469064712524414
CurrentTrain: epoch  3, batch    59 | loss: 7.6575913Losses:  9.475275039672852 4.663447380065918 0.2668936252593994
CurrentTrain: epoch  3, batch    60 | loss: 9.4752750Losses:  7.26120138168335 2.0959157943725586 0.23711413145065308
CurrentTrain: epoch  3, batch    61 | loss: 7.2612014Losses:  5.489058017730713 1.1595280170440674 0.16469430923461914
CurrentTrain: epoch  3, batch    62 | loss: 5.4890580Losses:  6.804314136505127 1.9752800464630127 0.2412029206752777
CurrentTrain: epoch  4, batch     0 | loss: 6.8043141Losses:  10.880440711975098 5.354901313781738 0.2774607539176941
CurrentTrain: epoch  4, batch     1 | loss: 10.8804407Losses:  9.13507080078125 4.636510848999023 0.2487107366323471
CurrentTrain: epoch  4, batch     2 | loss: 9.1350708Losses:  8.035506248474121 3.3257994651794434 0.24429552257061005
CurrentTrain: epoch  4, batch     3 | loss: 8.0355062Losses:  11.01919174194336 6.267854690551758 0.1759674996137619
CurrentTrain: epoch  4, batch     4 | loss: 11.0191917Losses:  7.670793533325195 2.7673795223236084 0.24531935155391693
CurrentTrain: epoch  4, batch     5 | loss: 7.6707935Losses:  6.709216117858887 2.0569686889648438 0.23610901832580566
CurrentTrain: epoch  4, batch     6 | loss: 6.7092161Losses:  8.504071235656738 3.8478753566741943 0.2528316080570221
CurrentTrain: epoch  4, batch     7 | loss: 8.5040712Losses:  7.559256076812744 2.9893784523010254 0.2560950517654419
CurrentTrain: epoch  4, batch     8 | loss: 7.5592561Losses:  8.258485794067383 3.415834903717041 0.2703852951526642
CurrentTrain: epoch  4, batch     9 | loss: 8.2584858Losses:  7.846632957458496 3.0718016624450684 0.2470400333404541
CurrentTrain: epoch  4, batch    10 | loss: 7.8466330Losses:  10.031919479370117 5.565094470977783 0.16659389436244965
CurrentTrain: epoch  4, batch    11 | loss: 10.0319195Losses:  8.781122207641602 3.6696224212646484 0.2682889997959137
CurrentTrain: epoch  4, batch    12 | loss: 8.7811222Losses:  6.844015121459961 1.8374042510986328 0.2429053783416748
CurrentTrain: epoch  4, batch    13 | loss: 6.8440151Losses:  7.288425922393799 2.2698400020599365 0.24437497556209564
CurrentTrain: epoch  4, batch    14 | loss: 7.2884259Losses:  11.185555458068848 5.950512409210205 0.16317777335643768
CurrentTrain: epoch  4, batch    15 | loss: 11.1855555Losses:  9.000686645507812 4.142387390136719 0.2580532431602478
CurrentTrain: epoch  4, batch    16 | loss: 9.0006866Losses:  9.999076843261719 5.177911758422852 0.15102419257164001
CurrentTrain: epoch  4, batch    17 | loss: 9.9990768Losses:  9.717909812927246 4.750019550323486 0.25898149609565735
CurrentTrain: epoch  4, batch    18 | loss: 9.7179098Losses:  8.063298225402832 3.29598069190979 0.2379213273525238
CurrentTrain: epoch  4, batch    19 | loss: 8.0632982Losses:  6.088671684265137 1.36615788936615 0.2222057580947876
CurrentTrain: epoch  4, batch    20 | loss: 6.0886717Losses:  7.635694980621338 2.932746410369873 0.26203930377960205
CurrentTrain: epoch  4, batch    21 | loss: 7.6356950Losses:  11.026381492614746 6.126667022705078 0.238945871591568
CurrentTrain: epoch  4, batch    22 | loss: 11.0263815Losses:  7.525239944458008 2.7763919830322266 0.23688793182373047
CurrentTrain: epoch  4, batch    23 | loss: 7.5252399Losses:  9.214200019836426 3.8924684524536133 0.26613476872444153
CurrentTrain: epoch  4, batch    24 | loss: 9.2142000Losses:  8.132155418395996 2.9564719200134277 0.24452736973762512
CurrentTrain: epoch  4, batch    25 | loss: 8.1321554Losses:  9.067105293273926 3.5901291370391846 0.26881730556488037
CurrentTrain: epoch  4, batch    26 | loss: 9.0671053Losses:  7.171231269836426 2.4478704929351807 0.23947063088417053
CurrentTrain: epoch  4, batch    27 | loss: 7.1712313Losses:  9.11874008178711 4.593814849853516 0.25448763370513916
CurrentTrain: epoch  4, batch    28 | loss: 9.1187401Losses:  7.5030107498168945 2.1224002838134766 0.22777357697486877
CurrentTrain: epoch  4, batch    29 | loss: 7.5030107Losses:  8.92833423614502 4.209188461303711 0.24621668457984924
CurrentTrain: epoch  4, batch    30 | loss: 8.9283342Losses:  7.637369632720947 2.848684072494507 0.24396005272865295
CurrentTrain: epoch  4, batch    31 | loss: 7.6373696Losses:  8.128453254699707 2.595722198486328 0.24801191687583923
CurrentTrain: epoch  4, batch    32 | loss: 8.1284533Losses:  9.90767765045166 4.1033148765563965 0.25740742683410645
CurrentTrain: epoch  4, batch    33 | loss: 9.9076777Losses:  7.551854610443115 2.3938231468200684 0.22412241995334625
CurrentTrain: epoch  4, batch    34 | loss: 7.5518546Losses:  7.534318923950195 2.661006212234497 0.25301188230514526
CurrentTrain: epoch  4, batch    35 | loss: 7.5343189Losses:  8.206741333007812 3.3109569549560547 0.23740558326244354
CurrentTrain: epoch  4, batch    36 | loss: 8.2067413Losses:  6.148663520812988 1.3840649127960205 0.2272537648677826
CurrentTrain: epoch  4, batch    37 | loss: 6.1486635Losses:  7.793222427368164 2.6718950271606445 0.2353219836950302
CurrentTrain: epoch  4, batch    38 | loss: 7.7932224Losses:  6.3693528175354 1.9281243085861206 0.2310989499092102
CurrentTrain: epoch  4, batch    39 | loss: 6.3693528Losses:  7.083012104034424 2.5782341957092285 0.24840253591537476
CurrentTrain: epoch  4, batch    40 | loss: 7.0830121Losses:  7.8858537673950195 3.2061550617218018 0.25386542081832886
CurrentTrain: epoch  4, batch    41 | loss: 7.8858538Losses:  10.321345329284668 5.679426193237305 0.24672040343284607
CurrentTrain: epoch  4, batch    42 | loss: 10.3213453Losses:  7.38265323638916 2.734163999557495 0.24340739846229553
CurrentTrain: epoch  4, batch    43 | loss: 7.3826532Losses:  8.006404876708984 3.0832293033599854 0.24872732162475586
CurrentTrain: epoch  4, batch    44 | loss: 8.0064049Losses:  6.493338584899902 1.706916093826294 0.22606578469276428
CurrentTrain: epoch  4, batch    45 | loss: 6.4933386Losses:  9.21717357635498 4.715517520904541 0.2452249526977539
CurrentTrain: epoch  4, batch    46 | loss: 9.2171736Losses:  6.395407199859619 1.8789359331130981 0.2369113266468048
CurrentTrain: epoch  4, batch    47 | loss: 6.3954072Losses:  7.596892356872559 3.226445198059082 0.24085785448551178
CurrentTrain: epoch  4, batch    48 | loss: 7.5968924Losses:  7.558171272277832 3.018386125564575 0.2423582375049591
CurrentTrain: epoch  4, batch    49 | loss: 7.5581713Losses:  9.740206718444824 4.67570161819458 0.2564384341239929
CurrentTrain: epoch  4, batch    50 | loss: 9.7402067Losses:  6.796494007110596 2.269951343536377 0.22582730650901794
CurrentTrain: epoch  4, batch    51 | loss: 6.7964940Losses:  6.979037284851074 2.491027355194092 0.249531090259552
CurrentTrain: epoch  4, batch    52 | loss: 6.9790373Losses:  8.131800651550293 3.508131504058838 0.25731411576271057
CurrentTrain: epoch  4, batch    53 | loss: 8.1318007Losses:  7.022925853729248 2.5617897510528564 0.24150025844573975
CurrentTrain: epoch  4, batch    54 | loss: 7.0229259Losses:  6.634152889251709 2.091675281524658 0.2256908416748047
CurrentTrain: epoch  4, batch    55 | loss: 6.6341529Losses:  8.219770431518555 3.8786885738372803 0.23631548881530762
CurrentTrain: epoch  4, batch    56 | loss: 8.2197704Losses:  6.885897159576416 2.2272768020629883 0.22571425139904022
CurrentTrain: epoch  4, batch    57 | loss: 6.8858972Losses:  7.308009147644043 2.5076866149902344 0.23529483377933502
CurrentTrain: epoch  4, batch    58 | loss: 7.3080091Losses:  8.578023910522461 4.165804386138916 0.24121811985969543
CurrentTrain: epoch  4, batch    59 | loss: 8.5780239Losses:  6.733330726623535 2.261777400970459 0.2340559959411621
CurrentTrain: epoch  4, batch    60 | loss: 6.7333307Losses:  9.538346290588379 5.133241176605225 0.2582026720046997
CurrentTrain: epoch  4, batch    61 | loss: 9.5383463Losses:  5.1047234535217285 0.5684260129928589 0.2731477618217468
CurrentTrain: epoch  4, batch    62 | loss: 5.1047235Losses:  6.642039775848389 2.0048210620880127 0.24186544120311737
CurrentTrain: epoch  5, batch     0 | loss: 6.6420398Losses:  7.5865092277526855 2.8420283794403076 0.2554602026939392
CurrentTrain: epoch  5, batch     1 | loss: 7.5865092Losses:  6.417571544647217 1.896005630493164 0.24147948622703552
CurrentTrain: epoch  5, batch     2 | loss: 6.4175715Losses:  5.924088001251221 1.3153390884399414 0.21948808431625366
CurrentTrain: epoch  5, batch     3 | loss: 5.9240880Losses:  7.024165153503418 2.6055283546447754 0.14667552709579468
CurrentTrain: epoch  5, batch     4 | loss: 7.0241652Losses:  7.7189483642578125 3.2463693618774414 0.23689094185829163
CurrentTrain: epoch  5, batch     5 | loss: 7.7189484Losses:  7.1185736656188965 2.6640894412994385 0.23924681544303894
CurrentTrain: epoch  5, batch     6 | loss: 7.1185737Losses:  7.268564224243164 2.8210902214050293 0.230416938662529
CurrentTrain: epoch  5, batch     7 | loss: 7.2685642Losses:  6.114648342132568 1.6837526559829712 0.2203921377658844
CurrentTrain: epoch  5, batch     8 | loss: 6.1146483Losses:  6.927172660827637 2.395385265350342 0.2478155493736267
CurrentTrain: epoch  5, batch     9 | loss: 6.9271727Losses:  6.735355854034424 2.245375633239746 0.22887955605983734
CurrentTrain: epoch  5, batch    10 | loss: 6.7353559Losses:  8.269415855407715 3.651412010192871 0.25156092643737793
CurrentTrain: epoch  5, batch    11 | loss: 8.2694159Losses:  8.007024765014648 3.5360124111175537 0.2515634596347809
CurrentTrain: epoch  5, batch    12 | loss: 8.0070248Losses:  7.410793781280518 2.3548874855041504 0.23378527164459229
CurrentTrain: epoch  5, batch    13 | loss: 7.4107938Losses:  7.55800199508667 3.1464333534240723 0.16726522147655487
CurrentTrain: epoch  5, batch    14 | loss: 7.5580020Losses:  7.072872161865234 2.727142333984375 0.23721134662628174
CurrentTrain: epoch  5, batch    15 | loss: 7.0728722Losses:  7.506626605987549 3.0888991355895996 0.23285311460494995
CurrentTrain: epoch  5, batch    16 | loss: 7.5066266Losses:  8.021635055541992 3.606748104095459 0.24203768372535706
CurrentTrain: epoch  5, batch    17 | loss: 8.0216351Losses:  8.0596923828125 3.5469918251037598 0.24563416838645935
CurrentTrain: epoch  5, batch    18 | loss: 8.0596924Losses:  7.706419467926025 3.2523720264434814 0.24195612967014313
CurrentTrain: epoch  5, batch    19 | loss: 7.7064195Losses:  9.649969100952148 5.258975982666016 0.26121509075164795
CurrentTrain: epoch  5, batch    20 | loss: 9.6499691Losses:  7.8417863845825195 3.379303455352783 0.24164336919784546
CurrentTrain: epoch  5, batch    21 | loss: 7.8417864Losses:  9.169781684875488 4.86346435546875 0.1717076599597931
CurrentTrain: epoch  5, batch    22 | loss: 9.1697817Losses:  6.864580154418945 2.3612594604492188 0.25367626547813416
CurrentTrain: epoch  5, batch    23 | loss: 6.8645802Losses:  6.247700214385986 1.7788145542144775 0.21882100403308868
CurrentTrain: epoch  5, batch    24 | loss: 6.2477002Losses:  9.424477577209473 4.66863489151001 0.2843462824821472
CurrentTrain: epoch  5, batch    25 | loss: 9.4244776Losses:  8.232905387878418 3.6864233016967773 0.24290722608566284
CurrentTrain: epoch  5, batch    26 | loss: 8.2329054Losses:  7.680319786071777 3.264099597930908 0.23656299710273743
CurrentTrain: epoch  5, batch    27 | loss: 7.6803198Losses:  7.445499897003174 2.914950370788574 0.2357979267835617
CurrentTrain: epoch  5, batch    28 | loss: 7.4454999Losses:  9.122150421142578 4.3606157302856445 0.24469207227230072
CurrentTrain: epoch  5, batch    29 | loss: 9.1221504Losses:  7.807709217071533 2.9531564712524414 0.23843127489089966
CurrentTrain: epoch  5, batch    30 | loss: 7.8077092Losses:  8.018819808959961 3.4755754470825195 0.262210488319397
CurrentTrain: epoch  5, batch    31 | loss: 8.0188198Losses:  6.861082077026367 2.4479732513427734 0.23827636241912842
CurrentTrain: epoch  5, batch    32 | loss: 6.8610821Losses:  7.1709794998168945 2.8167569637298584 0.23842407763004303
CurrentTrain: epoch  5, batch    33 | loss: 7.1709795Losses:  6.124634742736816 1.7052438259124756 0.21888446807861328
CurrentTrain: epoch  5, batch    34 | loss: 6.1246347Losses:  7.55622673034668 3.2234392166137695 0.23291289806365967
CurrentTrain: epoch  5, batch    35 | loss: 7.5562267Losses:  7.997652053833008 3.654110908508301 0.2378663271665573
CurrentTrain: epoch  5, batch    36 | loss: 7.9976521Losses:  6.7963762283325195 2.416278839111328 0.24215802550315857
CurrentTrain: epoch  5, batch    37 | loss: 6.7963762Losses:  8.705060005187988 4.239339828491211 0.24615974724292755
CurrentTrain: epoch  5, batch    38 | loss: 8.7050600Losses:  7.6290411949157715 3.1689300537109375 0.24118249118328094
CurrentTrain: epoch  5, batch    39 | loss: 7.6290412Losses:  9.497579574584961 4.9789886474609375 0.24936094880104065
CurrentTrain: epoch  5, batch    40 | loss: 9.4975796Losses:  7.424914836883545 3.032696485519409 0.24901334941387177
CurrentTrain: epoch  5, batch    41 | loss: 7.4249148Losses:  6.810804843902588 2.2629590034484863 0.2296554446220398
CurrentTrain: epoch  5, batch    42 | loss: 6.8108048Losses:  8.223564147949219 3.874041795730591 0.23817089200019836
CurrentTrain: epoch  5, batch    43 | loss: 8.2235641Losses:  6.773193836212158 2.0675997734069824 0.22269442677497864
CurrentTrain: epoch  5, batch    44 | loss: 6.7731938Losses:  8.487757682800293 3.9804916381835938 0.24850349128246307
CurrentTrain: epoch  5, batch    45 | loss: 8.4877577Losses:  6.826022148132324 2.445235013961792 0.23458102345466614
CurrentTrain: epoch  5, batch    46 | loss: 6.8260221Losses:  6.3257155418396 1.9713211059570312 0.22009439766407013
CurrentTrain: epoch  5, batch    47 | loss: 6.3257155Losses:  7.366020679473877 2.9767799377441406 0.22764751315116882
CurrentTrain: epoch  5, batch    48 | loss: 7.3660207Losses:  7.439718723297119 3.0669105052948 0.22634656727313995
CurrentTrain: epoch  5, batch    49 | loss: 7.4397187Losses:  10.59692096710205 6.1873040199279785 0.2526963949203491
CurrentTrain: epoch  5, batch    50 | loss: 10.5969210Losses:  7.084983825683594 2.763014078140259 0.24322055280208588
CurrentTrain: epoch  5, batch    51 | loss: 7.0849838Losses:  8.258389472961426 3.9062399864196777 0.23577821254730225
CurrentTrain: epoch  5, batch    52 | loss: 8.2583895Losses:  8.997143745422363 4.498545169830322 0.2442934513092041
CurrentTrain: epoch  5, batch    53 | loss: 8.9971437Losses:  6.786640167236328 2.408210277557373 0.23160791397094727
CurrentTrain: epoch  5, batch    54 | loss: 6.7866402Losses:  8.269051551818848 3.886599063873291 0.23583652079105377
CurrentTrain: epoch  5, batch    55 | loss: 8.2690516Losses:  8.377521514892578 4.0868120193481445 0.22477498650550842
CurrentTrain: epoch  5, batch    56 | loss: 8.3775215Losses:  8.63662338256836 4.309240818023682 0.23673811554908752
CurrentTrain: epoch  5, batch    57 | loss: 8.6366234Losses:  7.418288707733154 3.0284907817840576 0.25679922103881836
CurrentTrain: epoch  5, batch    58 | loss: 7.4182887Losses:  7.776388645172119 3.3619213104248047 0.2542455792427063
CurrentTrain: epoch  5, batch    59 | loss: 7.7763886Losses:  6.9503021240234375 2.6218044757843018 0.23451723158359528
CurrentTrain: epoch  5, batch    60 | loss: 6.9503021Losses:  6.732367515563965 2.456385612487793 0.23609337210655212
CurrentTrain: epoch  5, batch    61 | loss: 6.7323675Losses:  4.901444911956787 0.680508017539978 0.1576116383075714
CurrentTrain: epoch  5, batch    62 | loss: 4.9014449Losses:  6.5050368309021 2.189429998397827 0.23267148435115814
CurrentTrain: epoch  6, batch     0 | loss: 6.5050368Losses:  6.944005966186523 2.5502307415008545 0.23586469888687134
CurrentTrain: epoch  6, batch     1 | loss: 6.9440060Losses:  6.791502475738525 2.4007534980773926 0.23595425486564636
CurrentTrain: epoch  6, batch     2 | loss: 6.7915025Losses:  12.454108238220215 8.006903648376465 0.17299388349056244
CurrentTrain: epoch  6, batch     3 | loss: 12.4541082Losses:  5.794753074645996 1.4836198091506958 0.21595361828804016
CurrentTrain: epoch  6, batch     4 | loss: 5.7947531Losses:  7.358529090881348 3.0337777137756348 0.22695453464984894
CurrentTrain: epoch  6, batch     5 | loss: 7.3585291Losses:  6.438764572143555 2.0651955604553223 0.2318054437637329
CurrentTrain: epoch  6, batch     6 | loss: 6.4387646Losses:  8.104158401489258 3.7938501834869385 0.2442561239004135
CurrentTrain: epoch  6, batch     7 | loss: 8.1041584Losses:  6.6266961097717285 2.2883379459381104 0.22409483790397644
CurrentTrain: epoch  6, batch     8 | loss: 6.6266961Losses:  6.786628723144531 2.489772319793701 0.14220526814460754
CurrentTrain: epoch  6, batch     9 | loss: 6.7866287Losses:  8.740918159484863 4.370330810546875 0.24265673756599426
CurrentTrain: epoch  6, batch    10 | loss: 8.7409182Losses:  7.550987720489502 3.2068119049072266 0.24123989045619965
CurrentTrain: epoch  6, batch    11 | loss: 7.5509877Losses:  6.669772624969482 2.354523181915283 0.2302798181772232
CurrentTrain: epoch  6, batch    12 | loss: 6.6697726Losses:  8.014481544494629 3.278163433074951 0.2605929672718048
CurrentTrain: epoch  6, batch    13 | loss: 8.0144815Losses:  8.003547668457031 3.8307886123657227 0.16264724731445312
CurrentTrain: epoch  6, batch    14 | loss: 8.0035477Losses:  7.021951675415039 2.721513271331787 0.22976434230804443
CurrentTrain: epoch  6, batch    15 | loss: 7.0219517Losses:  8.502337455749512 4.182694911956787 0.24756161868572235
CurrentTrain: epoch  6, batch    16 | loss: 8.5023375Losses:  9.613954544067383 5.223399639129639 0.24041365087032318
CurrentTrain: epoch  6, batch    17 | loss: 9.6139545Losses:  5.927417278289795 1.6313031911849976 0.21766597032546997
CurrentTrain: epoch  6, batch    18 | loss: 5.9274173Losses:  6.9833903312683105 2.6170401573181152 0.23735298216342926
CurrentTrain: epoch  6, batch    19 | loss: 6.9833903Losses:  8.509150505065918 4.200385093688965 0.2385934740304947
CurrentTrain: epoch  6, batch    20 | loss: 8.5091505Losses:  7.974629878997803 3.713183879852295 0.23900996148586273
CurrentTrain: epoch  6, batch    21 | loss: 7.9746299Losses:  6.6319580078125 2.3476524353027344 0.23026683926582336
CurrentTrain: epoch  6, batch    22 | loss: 6.6319580Losses:  7.854223728179932 3.5414164066314697 0.2426932156085968
CurrentTrain: epoch  6, batch    23 | loss: 7.8542237Losses:  7.144134998321533 2.829970598220825 0.24446538090705872
CurrentTrain: epoch  6, batch    24 | loss: 7.1441350Losses:  7.183862686157227 2.948733329772949 0.21680346131324768
CurrentTrain: epoch  6, batch    25 | loss: 7.1838627Losses:  7.581691265106201 3.244263172149658 0.24083563685417175
CurrentTrain: epoch  6, batch    26 | loss: 7.5816913Losses:  7.91541051864624 3.6856679916381836 0.24981114268302917
CurrentTrain: epoch  6, batch    27 | loss: 7.9154105Losses:  8.644868850708008 4.367590427398682 0.23902711272239685
CurrentTrain: epoch  6, batch    28 | loss: 8.6448689Losses:  7.37038516998291 3.1036293506622314 0.24188879132270813
CurrentTrain: epoch  6, batch    29 | loss: 7.3703852Losses:  6.197454929351807 1.9275827407836914 0.21060127019882202
CurrentTrain: epoch  6, batch    30 | loss: 6.1974549Losses:  8.465415000915527 4.104582786560059 0.2384960949420929
CurrentTrain: epoch  6, batch    31 | loss: 8.4654150Losses:  8.35747241973877 4.045483589172363 0.23960743844509125
CurrentTrain: epoch  6, batch    32 | loss: 8.3574724Losses:  6.098737716674805 1.851374626159668 0.22400535643100739
CurrentTrain: epoch  6, batch    33 | loss: 6.0987377Losses:  5.806921482086182 1.505326271057129 0.21803000569343567
CurrentTrain: epoch  6, batch    34 | loss: 5.8069215Losses:  7.61834192276001 3.3162221908569336 0.21983136236667633
CurrentTrain: epoch  6, batch    35 | loss: 7.6183419Losses:  6.8518524169921875 2.587230682373047 0.22998705506324768
CurrentTrain: epoch  6, batch    36 | loss: 6.8518524Losses:  7.954134941101074 3.641364574432373 0.23778772354125977
CurrentTrain: epoch  6, batch    37 | loss: 7.9541349Losses:  6.738023281097412 2.546018123626709 0.2250114530324936
CurrentTrain: epoch  6, batch    38 | loss: 6.7380233Losses:  6.226423740386963 2.00688099861145 0.2223445475101471
CurrentTrain: epoch  6, batch    39 | loss: 6.2264237Losses:  8.196398735046387 3.979527473449707 0.24037353694438934
CurrentTrain: epoch  6, batch    40 | loss: 8.1963987Losses:  6.698581695556641 2.4170637130737305 0.2331029772758484
CurrentTrain: epoch  6, batch    41 | loss: 6.6985817Losses:  6.6004252433776855 2.340670108795166 0.23193463683128357
CurrentTrain: epoch  6, batch    42 | loss: 6.6004252Losses:  8.345328330993652 4.0417609214782715 0.24038448929786682
CurrentTrain: epoch  6, batch    43 | loss: 8.3453283Losses:  7.4745378494262695 3.1842947006225586 0.24246780574321747
CurrentTrain: epoch  6, batch    44 | loss: 7.4745378Losses:  8.749918937683105 4.465635776519775 0.2428685426712036
CurrentTrain: epoch  6, batch    45 | loss: 8.7499189Losses:  5.8666510581970215 1.62986159324646 0.21311146020889282
CurrentTrain: epoch  6, batch    46 | loss: 5.8666511Losses:  6.38701057434082 2.1474151611328125 0.23228785395622253
CurrentTrain: epoch  6, batch    47 | loss: 6.3870106Losses:  10.625807762145996 6.32728910446167 0.2387586534023285
CurrentTrain: epoch  6, batch    48 | loss: 10.6258078Losses:  6.674620628356934 2.462357521057129 0.15340106189250946
CurrentTrain: epoch  6, batch    49 | loss: 6.6746206Losses:  7.021177768707275 2.739741325378418 0.22552324831485748
CurrentTrain: epoch  6, batch    50 | loss: 7.0211778Losses:  7.932538986206055 3.635443687438965 0.25711095333099365
CurrentTrain: epoch  6, batch    51 | loss: 7.9325390Losses:  7.603244304656982 3.353896141052246 0.23629158735275269
CurrentTrain: epoch  6, batch    52 | loss: 7.6032443Losses:  6.242691993713379 1.952479600906372 0.22527670860290527
CurrentTrain: epoch  6, batch    53 | loss: 6.2426920Losses:  7.022620677947998 2.751394271850586 0.24370238184928894
CurrentTrain: epoch  6, batch    54 | loss: 7.0226207Losses:  5.819854736328125 1.5693738460540771 0.21164795756340027
CurrentTrain: epoch  6, batch    55 | loss: 5.8198547Losses:  7.358837127685547 3.130793809890747 0.23336413502693176
CurrentTrain: epoch  6, batch    56 | loss: 7.3588371Losses:  8.156608581542969 3.955697536468506 0.2323586493730545
CurrentTrain: epoch  6, batch    57 | loss: 8.1566086Losses:  6.466102123260498 2.1960997581481934 0.22926363348960876
CurrentTrain: epoch  6, batch    58 | loss: 6.4661021Losses:  7.520488739013672 3.2327921390533447 0.2317427396774292
CurrentTrain: epoch  6, batch    59 | loss: 7.5204887Losses:  7.955046653747559 3.7104222774505615 0.23165974020957947
CurrentTrain: epoch  6, batch    60 | loss: 7.9550467Losses:  8.288036346435547 4.066278457641602 0.2487945258617401
CurrentTrain: epoch  6, batch    61 | loss: 8.2880363Losses:  4.648781776428223 0.4771748483181 0.15791311860084534
CurrentTrain: epoch  6, batch    62 | loss: 4.6487818Losses:  6.883456707000732 2.658177614212036 0.223918616771698
CurrentTrain: epoch  7, batch     0 | loss: 6.8834567Losses:  8.697460174560547 4.460958957672119 0.24227944016456604
CurrentTrain: epoch  7, batch     1 | loss: 8.6974602Losses:  7.184324264526367 2.971163272857666 0.22238950431346893
CurrentTrain: epoch  7, batch     2 | loss: 7.1843243Losses:  5.823751926422119 1.6131162643432617 0.21077962219715118
CurrentTrain: epoch  7, batch     3 | loss: 5.8237519Losses:  7.19043493270874 2.75492525100708 0.21485552191734314
CurrentTrain: epoch  7, batch     4 | loss: 7.1904349Losses:  7.1576995849609375 2.9382786750793457 0.2234058380126953
CurrentTrain: epoch  7, batch     5 | loss: 7.1576996Losses:  6.424287796020508 2.1963744163513184 0.21060693264007568
CurrentTrain: epoch  7, batch     6 | loss: 6.4242878Losses:  7.992437839508057 3.792738199234009 0.16581223905086517
CurrentTrain: epoch  7, batch     7 | loss: 7.9924378Losses:  7.722955703735352 3.4538967609405518 0.23504149913787842
CurrentTrain: epoch  7, batch     8 | loss: 7.7229557Losses:  6.951716899871826 2.686224937438965 0.25461024045944214
CurrentTrain: epoch  7, batch     9 | loss: 6.9517169Losses:  6.756791591644287 2.5421104431152344 0.22378987073898315
CurrentTrain: epoch  7, batch    10 | loss: 6.7567916Losses:  9.063702583312988 4.871760368347168 0.24309852719306946
CurrentTrain: epoch  7, batch    11 | loss: 9.0637026Losses:  6.489439010620117 2.3022570610046387 0.2253997027873993
CurrentTrain: epoch  7, batch    12 | loss: 6.4894390Losses:  7.142999172210693 2.9001479148864746 0.22511956095695496
CurrentTrain: epoch  7, batch    13 | loss: 7.1429992Losses:  9.001704216003418 4.667324542999268 0.25387275218963623
CurrentTrain: epoch  7, batch    14 | loss: 9.0017042Losses:  10.781341552734375 6.486728668212891 0.24133527278900146
CurrentTrain: epoch  7, batch    15 | loss: 10.7813416Losses:  7.493870735168457 3.2841649055480957 0.2333945333957672
CurrentTrain: epoch  7, batch    16 | loss: 7.4938707Losses:  6.467489242553711 2.1941585540771484 0.22563153505325317
CurrentTrain: epoch  7, batch    17 | loss: 6.4674892Losses:  6.018350124359131 1.815625548362732 0.21885456144809723
CurrentTrain: epoch  7, batch    18 | loss: 6.0183501Losses:  6.5836567878723145 2.355630874633789 0.23246526718139648
CurrentTrain: epoch  7, batch    19 | loss: 6.5836568Losses:  7.0045671463012695 2.805142879486084 0.22133192420005798
CurrentTrain: epoch  7, batch    20 | loss: 7.0045671Losses:  6.474847316741943 1.88564932346344 0.22503554821014404
CurrentTrain: epoch  7, batch    21 | loss: 6.4748473Losses:  6.663773059844971 2.435279130935669 0.22383081912994385
CurrentTrain: epoch  7, batch    22 | loss: 6.6637731Losses:  9.76584243774414 5.463785171508789 0.24914196133613586
CurrentTrain: epoch  7, batch    23 | loss: 9.7658424Losses:  6.6233015060424805 2.3467416763305664 0.23098330199718475
CurrentTrain: epoch  7, batch    24 | loss: 6.6233015Losses:  7.097095012664795 2.8085577487945557 0.23460529744625092
CurrentTrain: epoch  7, batch    25 | loss: 7.0970950Losses:  11.08952522277832 6.780733108520508 0.24747154116630554
CurrentTrain: epoch  7, batch    26 | loss: 11.0895252Losses:  6.796341896057129 2.245403289794922 0.21389026939868927
CurrentTrain: epoch  7, batch    27 | loss: 6.7963419Losses:  6.9275360107421875 2.6736326217651367 0.22402408719062805
CurrentTrain: epoch  7, batch    28 | loss: 6.9275360Losses:  5.502657413482666 1.2539937496185303 0.21131183207035065
CurrentTrain: epoch  7, batch    29 | loss: 5.5026574Losses:  7.070413589477539 2.7851858139038086 0.22561557590961456
CurrentTrain: epoch  7, batch    30 | loss: 7.0704136Losses:  7.35273551940918 3.112739324569702 0.23152080178260803
CurrentTrain: epoch  7, batch    31 | loss: 7.3527355Losses:  8.0134916305542 3.773084878921509 0.1635953187942505
CurrentTrain: epoch  7, batch    32 | loss: 8.0134916Losses:  6.3232102394104 2.0607872009277344 0.2284005880355835
CurrentTrain: epoch  7, batch    33 | loss: 6.3232102Losses:  6.996668338775635 2.714116096496582 0.2221253216266632
CurrentTrain: epoch  7, batch    34 | loss: 6.9966683Losses:  10.4869384765625 6.213906288146973 0.23460979759693146
CurrentTrain: epoch  7, batch    35 | loss: 10.4869385Losses:  6.96257209777832 2.8035573959350586 0.13622525334358215
CurrentTrain: epoch  7, batch    36 | loss: 6.9625721Losses:  10.306695938110352 6.057249069213867 0.24884435534477234
CurrentTrain: epoch  7, batch    37 | loss: 10.3066959Losses:  9.337634086608887 5.14415979385376 0.2374807745218277
CurrentTrain: epoch  7, batch    38 | loss: 9.3376341Losses:  11.317773818969727 7.0851640701293945 0.24768532812595367
CurrentTrain: epoch  7, batch    39 | loss: 11.3177738Losses:  7.971498012542725 3.7254066467285156 0.22564664483070374
CurrentTrain: epoch  7, batch    40 | loss: 7.9714980Losses:  7.028123378753662 2.8080010414123535 0.22275683283805847
CurrentTrain: epoch  7, batch    41 | loss: 7.0281234Losses:  6.068206310272217 1.8421671390533447 0.21748226881027222
CurrentTrain: epoch  7, batch    42 | loss: 6.0682063Losses:  10.604652404785156 6.4582133293151855 0.15506917238235474
CurrentTrain: epoch  7, batch    43 | loss: 10.6046524Losses:  6.93259859085083 2.6978201866149902 0.2296161651611328
CurrentTrain: epoch  7, batch    44 | loss: 6.9325986Losses:  8.96697998046875 4.7221198081970215 0.23368999361991882
CurrentTrain: epoch  7, batch    45 | loss: 8.9669800Losses:  6.687793254852295 2.41848087310791 0.2151390165090561
CurrentTrain: epoch  7, batch    46 | loss: 6.6877933Losses:  5.864552021026611 1.6210166215896606 0.21365410089492798
CurrentTrain: epoch  7, batch    47 | loss: 5.8645520Losses:  6.978339672088623 2.7321524620056152 0.24022254347801208
CurrentTrain: epoch  7, batch    48 | loss: 6.9783397Losses:  8.916659355163574 4.660768508911133 0.23543843626976013
CurrentTrain: epoch  7, batch    49 | loss: 8.9166594Losses:  6.907763957977295 2.6394450664520264 0.23696833848953247
CurrentTrain: epoch  7, batch    50 | loss: 6.9077640Losses:  7.461637020111084 3.169650077819824 0.23260146379470825
CurrentTrain: epoch  7, batch    51 | loss: 7.4616370Losses:  7.331676006317139 2.4683287143707275 0.23472824692726135
CurrentTrain: epoch  7, batch    52 | loss: 7.3316760Losses:  7.874917984008789 3.7035810947418213 0.13906025886535645
CurrentTrain: epoch  7, batch    53 | loss: 7.8749180Losses:  6.069171905517578 1.791563630104065 0.22201497852802277
CurrentTrain: epoch  7, batch    54 | loss: 6.0691719Losses:  6.165799617767334 1.958137035369873 0.2103489637374878
CurrentTrain: epoch  7, batch    55 | loss: 6.1657996Losses:  6.528508186340332 2.3131179809570312 0.22899775207042694
CurrentTrain: epoch  7, batch    56 | loss: 6.5285082Losses:  7.398651599884033 3.200958013534546 0.22569085657596588
CurrentTrain: epoch  7, batch    57 | loss: 7.3986516Losses:  7.281367301940918 3.002999782562256 0.23041796684265137
CurrentTrain: epoch  7, batch    58 | loss: 7.2813673Losses:  6.059952735900879 1.8307273387908936 0.22637450695037842
CurrentTrain: epoch  7, batch    59 | loss: 6.0599527Losses:  6.59826135635376 2.375035047531128 0.23514136672019958
CurrentTrain: epoch  7, batch    60 | loss: 6.5982614Losses:  8.794824600219727 4.575737953186035 0.2377675473690033
CurrentTrain: epoch  7, batch    61 | loss: 8.7948246Losses:  9.813175201416016 5.855111122131348 0.09562128782272339
CurrentTrain: epoch  7, batch    62 | loss: 9.8131752Losses:  6.605568885803223 2.3324029445648193 0.22923265397548676
CurrentTrain: epoch  8, batch     0 | loss: 6.6055689Losses:  7.328990459442139 3.116888999938965 0.2354198396205902
CurrentTrain: epoch  8, batch     1 | loss: 7.3289905Losses:  5.785830497741699 1.5641765594482422 0.21106329560279846
CurrentTrain: epoch  8, batch     2 | loss: 5.7858305Losses:  5.995153903961182 1.6343778371810913 0.2120800018310547
CurrentTrain: epoch  8, batch     3 | loss: 5.9951539Losses:  7.693339824676514 3.4211881160736084 0.24953432381153107
CurrentTrain: epoch  8, batch     4 | loss: 7.6933398Losses:  8.437504768371582 4.165096282958984 0.23512350022792816
CurrentTrain: epoch  8, batch     5 | loss: 8.4375048Losses:  5.598187446594238 1.2867541313171387 0.20765431225299835
CurrentTrain: epoch  8, batch     6 | loss: 5.5981874Losses:  8.054134368896484 3.619755268096924 0.2379259318113327
CurrentTrain: epoch  8, batch     7 | loss: 8.0541344Losses:  5.666478633880615 1.4539852142333984 0.21639713644981384
CurrentTrain: epoch  8, batch     8 | loss: 5.6664786Losses:  7.888522148132324 3.730475902557373 0.15498653054237366
CurrentTrain: epoch  8, batch     9 | loss: 7.8885221Losses:  5.731139183044434 1.4875339269638062 0.22189509868621826
CurrentTrain: epoch  8, batch    10 | loss: 5.7311392Losses:  6.63004207611084 2.379987955093384 0.23918843269348145
CurrentTrain: epoch  8, batch    11 | loss: 6.6300421Losses:  8.598388671875 4.337040901184082 0.23360784351825714
CurrentTrain: epoch  8, batch    12 | loss: 8.5983887Losses:  6.982699871063232 2.7358615398406982 0.23006215691566467
CurrentTrain: epoch  8, batch    13 | loss: 6.9826999Losses:  9.497058868408203 5.284712791442871 0.2353188693523407
CurrentTrain: epoch  8, batch    14 | loss: 9.4970589Losses:  7.043455600738525 2.7558999061584473 0.23380276560783386
CurrentTrain: epoch  8, batch    15 | loss: 7.0434556Losses:  6.389371871948242 2.1706156730651855 0.218613862991333
CurrentTrain: epoch  8, batch    16 | loss: 6.3893719Losses:  6.002537250518799 1.8081576824188232 0.21594759821891785
CurrentTrain: epoch  8, batch    17 | loss: 6.0025373Losses:  7.9355597496032715 3.6340184211730957 0.2503148913383484
CurrentTrain: epoch  8, batch    18 | loss: 7.9355597Losses:  8.820642471313477 4.546871185302734 0.1707310676574707
CurrentTrain: epoch  8, batch    19 | loss: 8.8206425Losses:  8.360381126403809 4.068850517272949 0.24316498637199402
CurrentTrain: epoch  8, batch    20 | loss: 8.3603811Losses:  6.057102203369141 1.8238154649734497 0.22159716486930847
CurrentTrain: epoch  8, batch    21 | loss: 6.0571022Losses:  7.1433210372924805 2.9005720615386963 0.22816191613674164
CurrentTrain: epoch  8, batch    22 | loss: 7.1433210Losses:  7.717827796936035 3.4748096466064453 0.23453807830810547
CurrentTrain: epoch  8, batch    23 | loss: 7.7178278Losses:  6.7580885887146 2.537548542022705 0.2192213088274002
CurrentTrain: epoch  8, batch    24 | loss: 6.7580886Losses:  6.37190055847168 2.1336748600006104 0.2286711186170578
CurrentTrain: epoch  8, batch    25 | loss: 6.3719006Losses:  5.7681403160095215 1.561826467514038 0.20831428468227386
CurrentTrain: epoch  8, batch    26 | loss: 5.7681403Losses:  6.41586446762085 2.1902308464050293 0.23234233260154724
CurrentTrain: epoch  8, batch    27 | loss: 6.4158645Losses:  6.208187580108643 1.9862405061721802 0.22901321947574615
CurrentTrain: epoch  8, batch    28 | loss: 6.2081876Losses:  9.159226417541504 4.956699848175049 0.2510119676589966
CurrentTrain: epoch  8, batch    29 | loss: 9.1592264Losses:  6.674688816070557 2.481914520263672 0.22989457845687866
CurrentTrain: epoch  8, batch    30 | loss: 6.6746888Losses:  7.538086414337158 2.972766399383545 0.2287338227033615
CurrentTrain: epoch  8, batch    31 | loss: 7.5380864Losses:  7.799343585968018 3.584181785583496 0.23058989644050598
CurrentTrain: epoch  8, batch    32 | loss: 7.7993436Losses:  7.003104209899902 2.7556588649749756 0.23732778429985046
CurrentTrain: epoch  8, batch    33 | loss: 7.0031042Losses:  6.356148719787598 2.1690869331359863 0.22342020273208618
CurrentTrain: epoch  8, batch    34 | loss: 6.3561487Losses:  7.152763366699219 2.8963241577148438 0.2317385971546173
CurrentTrain: epoch  8, batch    35 | loss: 7.1527634Losses:  8.588356018066406 4.373718738555908 0.24134260416030884
CurrentTrain: epoch  8, batch    36 | loss: 8.5883560Losses:  7.168021202087402 2.91326904296875 0.23627112805843353
CurrentTrain: epoch  8, batch    37 | loss: 7.1680212Losses:  7.812844276428223 3.5694403648376465 0.23538385331630707
CurrentTrain: epoch  8, batch    38 | loss: 7.8128443Losses:  6.497027397155762 2.2446608543395996 0.21921128034591675
CurrentTrain: epoch  8, batch    39 | loss: 6.4970274Losses:  7.05288028717041 2.6994872093200684 0.21961086988449097
CurrentTrain: epoch  8, batch    40 | loss: 7.0528803Losses:  6.569718360900879 2.139399528503418 0.2249314934015274
CurrentTrain: epoch  8, batch    41 | loss: 6.5697184Losses:  8.698850631713867 4.480568885803223 0.1688147783279419
CurrentTrain: epoch  8, batch    42 | loss: 8.6988506Losses:  7.5398268699646 3.271178722381592 0.24303431808948517
CurrentTrain: epoch  8, batch    43 | loss: 7.5398269Losses:  7.419577121734619 3.1741466522216797 0.22602063417434692
CurrentTrain: epoch  8, batch    44 | loss: 7.4195771Losses:  8.989336967468262 4.741779327392578 0.24841175973415375
CurrentTrain: epoch  8, batch    45 | loss: 8.9893370Losses:  7.081558704376221 2.8514771461486816 0.21971459686756134
CurrentTrain: epoch  8, batch    46 | loss: 7.0815587Losses:  8.161802291870117 3.880587339401245 0.23338893055915833
CurrentTrain: epoch  8, batch    47 | loss: 8.1618023Losses:  8.908153533935547 4.583439826965332 0.2575616240501404
CurrentTrain: epoch  8, batch    48 | loss: 8.9081535Losses:  7.025645732879639 2.7630200386047363 0.23196211457252502
CurrentTrain: epoch  8, batch    49 | loss: 7.0256457Losses:  5.761621475219727 1.5816031694412231 0.20962870121002197
CurrentTrain: epoch  8, batch    50 | loss: 5.7616215Losses:  7.172140121459961 2.951709270477295 0.2204826921224594
CurrentTrain: epoch  8, batch    51 | loss: 7.1721401Losses:  6.5554046630859375 2.4127633571624756 0.1406104564666748
CurrentTrain: epoch  8, batch    52 | loss: 6.5554047Losses:  7.295464038848877 3.0770511627197266 0.22829343378543854
CurrentTrain: epoch  8, batch    53 | loss: 7.2954640Losses:  8.413718223571777 4.189447402954102 0.2320856750011444
CurrentTrain: epoch  8, batch    54 | loss: 8.4137182Losses:  10.88675594329834 6.83012580871582 0.1497640758752823
CurrentTrain: epoch  8, batch    55 | loss: 10.8867559Losses:  6.397438049316406 2.1576414108276367 0.22327226400375366
CurrentTrain: epoch  8, batch    56 | loss: 6.3974380Losses:  7.109627723693848 2.861532211303711 0.23551519215106964
CurrentTrain: epoch  8, batch    57 | loss: 7.1096277Losses:  7.77274751663208 3.529676914215088 0.23057501018047333
CurrentTrain: epoch  8, batch    58 | loss: 7.7727475Losses:  8.122846603393555 3.8881025314331055 0.2571542263031006
CurrentTrain: epoch  8, batch    59 | loss: 8.1228466Losses:  6.765373229980469 2.5497725009918213 0.23745524883270264
CurrentTrain: epoch  8, batch    60 | loss: 6.7653732Losses:  6.9722161293029785 2.7596306800842285 0.2327929139137268
CurrentTrain: epoch  8, batch    61 | loss: 6.9722161Losses:  4.669106960296631 0.48087558150291443 0.24622979760169983
CurrentTrain: epoch  8, batch    62 | loss: 4.6691070Losses:  10.017017364501953 5.838171005249023 0.15830889344215393
CurrentTrain: epoch  9, batch     0 | loss: 10.0170174Losses:  7.346595764160156 3.1656267642974854 0.23528379201889038
CurrentTrain: epoch  9, batch     1 | loss: 7.3465958Losses:  6.726783752441406 2.5066335201263428 0.2260608971118927
CurrentTrain: epoch  9, batch     2 | loss: 6.7267838Losses:  8.193022727966309 3.887028455734253 0.24089008569717407
CurrentTrain: epoch  9, batch     3 | loss: 8.1930227Losses:  7.9263739585876465 3.710512638092041 0.23733873665332794
CurrentTrain: epoch  9, batch     4 | loss: 7.9263740Losses:  5.955623149871826 1.7778571844100952 0.21804898977279663
CurrentTrain: epoch  9, batch     5 | loss: 5.9556231Losses:  6.837458610534668 2.6498725414276123 0.22278445959091187
CurrentTrain: epoch  9, batch     6 | loss: 6.8374586Losses:  7.188077926635742 2.9084115028381348 0.24348293244838715
CurrentTrain: epoch  9, batch     7 | loss: 7.1880779Losses:  7.057811737060547 2.8660926818847656 0.21891097724437714
CurrentTrain: epoch  9, batch     8 | loss: 7.0578117Losses:  7.907601833343506 3.6662113666534424 0.24943767488002777
CurrentTrain: epoch  9, batch     9 | loss: 7.9076018Losses:  7.705474376678467 3.4371275901794434 0.23359271883964539
CurrentTrain: epoch  9, batch    10 | loss: 7.7054744Losses:  6.624823093414307 2.4134631156921387 0.22349518537521362
CurrentTrain: epoch  9, batch    11 | loss: 6.6248231Losses:  7.358165740966797 3.1192893981933594 0.24744205176830292
CurrentTrain: epoch  9, batch    12 | loss: 7.3581657Losses:  6.184093475341797 1.968567132949829 0.22680774331092834
CurrentTrain: epoch  9, batch    13 | loss: 6.1840935Losses:  9.793679237365723 5.502608299255371 0.2602371573448181
CurrentTrain: epoch  9, batch    14 | loss: 9.7936792Losses:  7.402842044830322 3.1274571418762207 0.23332901298999786
CurrentTrain: epoch  9, batch    15 | loss: 7.4028420Losses:  6.030186653137207 1.7771704196929932 0.21871954202651978
CurrentTrain: epoch  9, batch    16 | loss: 6.0301867Losses:  7.825573921203613 3.603883743286133 0.23539471626281738
CurrentTrain: epoch  9, batch    17 | loss: 7.8255739Losses:  10.088802337646484 5.884826183319092 0.24436844885349274
CurrentTrain: epoch  9, batch    18 | loss: 10.0888023Losses:  6.575196266174316 2.350644588470459 0.22599603235721588
CurrentTrain: epoch  9, batch    19 | loss: 6.5751963Losses:  5.983052730560303 1.7893706560134888 0.2206384837627411
CurrentTrain: epoch  9, batch    20 | loss: 5.9830527Losses:  7.329601287841797 3.111807107925415 0.23800018429756165
CurrentTrain: epoch  9, batch    21 | loss: 7.3296013Losses:  5.9657135009765625 1.7668633460998535 0.21780943870544434
CurrentTrain: epoch  9, batch    22 | loss: 5.9657135Losses:  6.562963485717773 2.3183023929595947 0.2271701693534851
CurrentTrain: epoch  9, batch    23 | loss: 6.5629635Losses:  7.688028812408447 3.4462103843688965 0.23280423879623413
CurrentTrain: epoch  9, batch    24 | loss: 7.6880288Losses:  9.678993225097656 5.418669700622559 0.2620367407798767
CurrentTrain: epoch  9, batch    25 | loss: 9.6789932Losses:  6.285019397735596 2.080353260040283 0.2224726527929306
CurrentTrain: epoch  9, batch    26 | loss: 6.2850194Losses:  6.487376689910889 2.320465087890625 0.22582180798053741
CurrentTrain: epoch  9, batch    27 | loss: 6.4873767Losses:  9.152054786682129 4.907330513000488 0.24573203921318054
CurrentTrain: epoch  9, batch    28 | loss: 9.1520548Losses:  7.395615577697754 3.2241148948669434 0.2255685031414032
CurrentTrain: epoch  9, batch    29 | loss: 7.3956156Losses:  6.171664237976074 1.9596242904663086 0.20774464309215546
CurrentTrain: epoch  9, batch    30 | loss: 6.1716642Losses:  7.8335137367248535 3.6523733139038086 0.23306486010551453
CurrentTrain: epoch  9, batch    31 | loss: 7.8335137Losses:  9.968255996704102 5.77546501159668 0.24651557207107544
CurrentTrain: epoch  9, batch    32 | loss: 9.9682560Losses:  8.527706146240234 4.320428848266602 0.22876518964767456
CurrentTrain: epoch  9, batch    33 | loss: 8.5277061Losses:  6.934178352355957 2.714643955230713 0.23038259148597717
CurrentTrain: epoch  9, batch    34 | loss: 6.9341784Losses:  6.718055725097656 2.568950891494751 0.21922864019870758
CurrentTrain: epoch  9, batch    35 | loss: 6.7180557Losses:  6.369329929351807 2.1689188480377197 0.22312134504318237
CurrentTrain: epoch  9, batch    36 | loss: 6.3693299Losses:  5.3718366622924805 1.1889338493347168 0.20861603319644928
CurrentTrain: epoch  9, batch    37 | loss: 5.3718367Losses:  6.490527629852295 2.3211939334869385 0.21261972188949585
CurrentTrain: epoch  9, batch    38 | loss: 6.4905276Losses:  8.573718070983887 4.373222351074219 0.23594282567501068
CurrentTrain: epoch  9, batch    39 | loss: 8.5737181Losses:  8.756988525390625 4.6769843101501465 0.15763169527053833
CurrentTrain: epoch  9, batch    40 | loss: 8.7569885Losses:  7.035987377166748 2.867663860321045 0.21771439909934998
CurrentTrain: epoch  9, batch    41 | loss: 7.0359874Losses:  7.5342698097229 3.384946823120117 0.22620263695716858
CurrentTrain: epoch  9, batch    42 | loss: 7.5342698Losses:  6.583181381225586 2.3575246334075928 0.22346198558807373
CurrentTrain: epoch  9, batch    43 | loss: 6.5831814Losses:  6.517786502838135 2.2829489707946777 0.23252759873867035
CurrentTrain: epoch  9, batch    44 | loss: 6.5177865Losses:  7.126321792602539 2.8955283164978027 0.22535082697868347
CurrentTrain: epoch  9, batch    45 | loss: 7.1263218Losses:  6.337244987487793 2.1552772521972656 0.21331200003623962
CurrentTrain: epoch  9, batch    46 | loss: 6.3372450Losses:  7.205491542816162 3.098544120788574 0.14319881796836853
CurrentTrain: epoch  9, batch    47 | loss: 7.2054915Losses:  7.531766891479492 3.348292589187622 0.2312108874320984
CurrentTrain: epoch  9, batch    48 | loss: 7.5317669Losses:  7.477248668670654 3.227336883544922 0.23114734888076782
CurrentTrain: epoch  9, batch    49 | loss: 7.4772487Losses:  5.955050468444824 1.7728257179260254 0.22074861824512482
CurrentTrain: epoch  9, batch    50 | loss: 5.9550505Losses:  10.956254005432129 6.7629923820495605 0.248867005109787
CurrentTrain: epoch  9, batch    51 | loss: 10.9562540Losses:  7.531605243682861 3.202298402786255 0.23718325793743134
CurrentTrain: epoch  9, batch    52 | loss: 7.5316052Losses:  7.074962139129639 2.879789352416992 0.23769544064998627
CurrentTrain: epoch  9, batch    53 | loss: 7.0749621Losses:  8.156573295593262 3.954016923904419 0.2551068663597107
CurrentTrain: epoch  9, batch    54 | loss: 8.1565733Losses:  7.274209976196289 3.050654411315918 0.22183609008789062
CurrentTrain: epoch  9, batch    55 | loss: 7.2742100Losses:  9.115488052368164 4.882245063781738 0.24174556136131287
CurrentTrain: epoch  9, batch    56 | loss: 9.1154881Losses:  6.901992321014404 2.6987149715423584 0.2343374341726303
CurrentTrain: epoch  9, batch    57 | loss: 6.9019923Losses:  5.993423938751221 1.8053867816925049 0.21939605474472046
CurrentTrain: epoch  9, batch    58 | loss: 5.9934239Losses:  7.359041690826416 3.1752264499664307 0.2269575148820877
CurrentTrain: epoch  9, batch    59 | loss: 7.3590417Losses:  7.571120738983154 3.35979962348938 0.24332167208194733
CurrentTrain: epoch  9, batch    60 | loss: 7.5711207Losses:  6.193826675415039 1.9801795482635498 0.22687718272209167
CurrentTrain: epoch  9, batch    61 | loss: 6.1938267Losses:  4.154721260070801 -0.0 0.22728312015533447
CurrentTrain: epoch  9, batch    62 | loss: 4.1547213
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: born in copenhagen , as a young man bang became interested in surrealism as practised by giorgio de chirico , max ernst and yves tanguy .
Head Entity: yves tanguy
Tail Entity: surrealism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the artist was heavily influenced by the cubist movement, which was pioneered by pablo picasso and georges braque.  
Head Entity: pablo picasso  
Tail Entity: cubist movement  

Relation: movement  
Context: the philosopher's ideas were deeply rooted in the existentialist movement, which gained prominence in the mid-20th century through the works of thinkers like jean-paul sartre.  
Head Entity: jean-paul sartre  
Tail Entity: existentialist movement  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lyons hill is a restored village , and former parish with church , now part of the community of ardclough in north county kildare .
Head Entity: ardclough
Tail Entity: county kildare
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: The city of San Francisco is known for its iconic Golden Gate Bridge and is part of the larger San Francisco County in California.  
Head Entity: San Francisco  
Tail Entity: San Francisco County  

Relation: located in the administrative territorial entity  
Context: The village of Houghton is situated within the boundaries of the City of Cambridge, which is located in Cambridgeshire, England.  
Head Entity: Houghton  
Tail Entity: City of Cambridge  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: in 2004 , the mayor of bremen , henning scherf , repeated this act when the concert " disenchanted amazon " was presented in the bremen municipal government palace .
Head Entity: bremen
Tail Entity: henning scherf
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: In 2021, the Prime Minister of Canada, Justin Trudeau, announced new policies during a press conference held in Ottawa.  
Head Entity: Canada  
Tail Entity: Justin Trudeau  

Relation: head of government  
Context: During the annual state address, the President of France, Emmanuel Macron, outlined his vision for the country's future in Paris.  
Head Entity: France  
Tail Entity: Emmanuel Macron  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: felim egan is a member of aosdána .
Head Entity: felim egan
Tail Entity: aosdána
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: The renowned physicist Albert Einstein was a member of the Prussian Academy of Sciences.  
Head Entity: Albert Einstein  
Tail Entity: Prussian Academy of Sciences  

Relation: member of  
Context: The famous rock band The Beatles had Paul McCartney as a member.  
Head Entity: Paul McCartney  
Tail Entity: The Beatles  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County, and is a vital link for traffic crossing over the water.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a direct route for trains traveling between the UK and mainland Europe.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the municipality contains the lower reaches of the maracanã river , which is joined by the caripi river before widening into maracanã bay and emptying into the atlantic ocean beside maiandeua island .
Head Entity: maracanã river
Tail Entity: caripi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Amazon River, one of the largest rivers in the world, receives numerous tributaries, including the Madeira River, which significantly contributes to its flow.  
Head Entity: Amazon River  
Tail Entity: Madeira River  

Relation: tributary  
Context: The Mississippi River is fed by several smaller rivers, with the Missouri River being one of its major tributaries that enhances its water volume.  
Head Entity: Mississippi River  
Tail Entity: Missouri River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: his father robert docking and grandfather george docking both served as governor of kansas .
Head Entity: george docking
Tail Entity: governor of kansas
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: After a successful career in politics, Sarah Thompson was appointed as the new mayor of Springfield.  
Head Entity: Sarah Thompson  
Tail Entity: mayor of Springfield  

Relation: position held  
Context: John Smith, a renowned scientist, was the director of the National Institute of Health for over a decade.  
Head Entity: John Smith  
Tail Entity: director of the National Institute of Health  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: " jedi outcast " was developed by raven software and powered by the i d tech 3 game engine .
Head Entity: jedi outcast
Tail Entity: raven software
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: "The Witcher 3: Wild Hunt" was developed by CD Projekt Red and has received numerous awards for its storytelling and gameplay.  
Head Entity: The Witcher 3: Wild Hunt  
Tail Entity: CD Projekt Red  

Relation: developer  
Context: "Minecraft" was developed by Mojang Studios and has become one of the best-selling video games of all time.  
Head Entity: Minecraft  
Tail Entity: Mojang Studios  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: in 2008 , the montreal - based band simple plan featured koivu in its video for the song " save you . "
Head Entity: simple plan
Tail Entity: montreal
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: The famous tech company was established in a small garage in Cupertino, California, where it began its journey to revolutionize the industry.  
Head Entity: Apple Inc.  
Tail Entity: Cupertino  

Relation: location of formation  
Context: The renowned soccer club was founded in 1899 in the city of Barcelona, where it quickly gained popularity and a loyal fan base.  
Head Entity: FC Barcelona  
Tail Entity: Barcelona  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: " shining time station " won a number of awards and significantly increased the popularity of the " thomas " media franchise in the united states .
Head Entity: shining time station
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous dish "sushi" is traditionally associated with Japan and has gained immense popularity worldwide.  
Head Entity: sushi  
Tail Entity: Japan  

Relation: country of origin  
Context: The iconic brand "Guinness" is known for its rich stout beer, which originated in Ireland and is now enjoyed globally.  
Head Entity: Guinness  
Tail Entity: Ireland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.20%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.11%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.18%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 95.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.76%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.01%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 96.09%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.17%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.20%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.15%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.11%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.18%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 95.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.76%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 95.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.84%   
cur_acc:  ['0.9484']
his_acc:  ['0.9484']
Clustering into  9  clusters
Clusters:  [0 1 2 3 1 1 2 0 7 7 0 5 4 8 3 1 6 0 0 0]
Losses:  13.549482345581055 4.223880767822266 0.8742198348045349
CurrentTrain: epoch  0, batch     0 | loss: 13.5494823Losses:  14.005175590515137 3.9536895751953125 0.6667150259017944
CurrentTrain: epoch  0, batch     1 | loss: 14.0051756Losses:  13.3613920211792 4.002467632293701 0.7087451219558716
CurrentTrain: epoch  0, batch     2 | loss: 13.3613920Losses:  6.667297840118408 -0.0 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 6.6672978Losses:  11.747581481933594 3.4250192642211914 0.6743828058242798
CurrentTrain: epoch  1, batch     0 | loss: 11.7475815Losses:  12.113692283630371 3.3101401329040527 0.6518434882164001
CurrentTrain: epoch  1, batch     1 | loss: 12.1136923Losses:  12.348113059997559 3.68684720993042 0.6089929342269897
CurrentTrain: epoch  1, batch     2 | loss: 12.3481131Losses:  8.750435829162598 -0.0 0.27433907985687256
CurrentTrain: epoch  1, batch     3 | loss: 8.7504358Losses:  11.888981819152832 3.5834507942199707 0.6364063620567322
CurrentTrain: epoch  2, batch     0 | loss: 11.8889818Losses:  10.030632019042969 2.8551530838012695 0.664405345916748
CurrentTrain: epoch  2, batch     1 | loss: 10.0306320Losses:  9.9524507522583 2.6750946044921875 0.5986194610595703
CurrentTrain: epoch  2, batch     2 | loss: 9.9524508Losses:  8.90971565246582 -0.0 0.18001845479011536
CurrentTrain: epoch  2, batch     3 | loss: 8.9097157Losses:  11.577857971191406 4.077861309051514 0.6935637593269348
CurrentTrain: epoch  3, batch     0 | loss: 11.5778580Losses:  11.118800163269043 3.3079898357391357 0.6652772426605225
CurrentTrain: epoch  3, batch     1 | loss: 11.1188002Losses:  8.664277076721191 2.9597959518432617 0.6182165741920471
CurrentTrain: epoch  3, batch     2 | loss: 8.6642771Losses:  5.363212585449219 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 5.3632126Losses:  9.929500579833984 3.457162857055664 0.6017909049987793
CurrentTrain: epoch  4, batch     0 | loss: 9.9295006Losses:  13.469965934753418 6.587925910949707 0.42110323905944824
CurrentTrain: epoch  4, batch     1 | loss: 13.4699659Losses:  8.654824256896973 2.9374608993530273 0.6051901578903198
CurrentTrain: epoch  4, batch     2 | loss: 8.6548243Losses:  7.617944240570068 -0.0 0.16433565318584442
CurrentTrain: epoch  4, batch     3 | loss: 7.6179442Losses:  10.534271240234375 4.918327331542969 0.5741661190986633
CurrentTrain: epoch  5, batch     0 | loss: 10.5342712Losses:  9.756991386413574 3.578005075454712 0.6251906752586365
CurrentTrain: epoch  5, batch     1 | loss: 9.7569914Losses:  10.518770217895508 3.8338186740875244 0.6163586974143982
CurrentTrain: epoch  5, batch     2 | loss: 10.5187702Losses:  3.1086196899414062 -0.0 0.09285630285739899
CurrentTrain: epoch  5, batch     3 | loss: 3.1086197Losses:  10.853300094604492 4.60028076171875 0.5343655943870544
CurrentTrain: epoch  6, batch     0 | loss: 10.8533001Losses:  9.394341468811035 4.641083717346191 0.5332483649253845
CurrentTrain: epoch  6, batch     1 | loss: 9.3943415Losses:  10.506507873535156 4.450131416320801 0.6611424684524536
CurrentTrain: epoch  6, batch     2 | loss: 10.5065079Losses:  4.117289066314697 -0.0 0.15599052608013153
CurrentTrain: epoch  6, batch     3 | loss: 4.1172891Losses:  9.183753967285156 3.8533825874328613 0.5916942954063416
CurrentTrain: epoch  7, batch     0 | loss: 9.1837540Losses:  7.912549018859863 3.18544602394104 0.647247314453125
CurrentTrain: epoch  7, batch     1 | loss: 7.9125490Losses:  9.653298377990723 4.364155292510986 0.6352187395095825
CurrentTrain: epoch  7, batch     2 | loss: 9.6532984Losses:  8.541091918945312 -0.0 -0.0
CurrentTrain: epoch  7, batch     3 | loss: 8.5410919Losses:  7.7425384521484375 2.5105628967285156 0.548003077507019
CurrentTrain: epoch  8, batch     0 | loss: 7.7425385Losses:  8.389572143554688 3.5590226650238037 0.5675298571586609
CurrentTrain: epoch  8, batch     1 | loss: 8.3895721Losses:  9.94932746887207 5.060871124267578 0.6025201678276062
CurrentTrain: epoch  8, batch     2 | loss: 9.9493275Losses:  3.383744239807129 -0.0 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 3.3837442Losses:  7.7164812088012695 2.866880416870117 0.6575779318809509
CurrentTrain: epoch  9, batch     0 | loss: 7.7164812Losses:  8.849760055541992 3.6422479152679443 0.5549411177635193
CurrentTrain: epoch  9, batch     1 | loss: 8.8497601Losses:  7.2783942222595215 2.754699230194092 0.5408180952072144
CurrentTrain: epoch  9, batch     2 | loss: 7.2783942Losses:  5.021466255187988 -0.0 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 5.0214663
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: for more information on the grand mosque seizure itself , see " the siege of mecca " by yaroslav trofimov .
Head Entity: the siege of mecca
Tail Entity: grand mosque seizure
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: In her book "The Immortal Life of Henrietta Lacks," Rebecca Skloot explores the ethical implications of medical research through the story of Henrietta Lacks and her immortal cell line.  
Head Entity: The Immortal Life of Henrietta Lacks  
Tail Entity: Henrietta Lacks  

Relation: main subject  
Context: The documentary "13th" by Ava DuVernay examines the intersection of race, justice, and mass incarceration in the United States.  
Head Entity: 13th  
Tail Entity: mass incarceration
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: group b of uefa euro 2016 contained england , russia , wales and slovakia .
Head Entity: uefa euro 2016
Tail Entity: slovakia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: the 2020 summer olympics featured athletes from countries like usa, china, and japan.  
Head Entity: 2020 summer olympics  
Tail Entity: japan  

Relation: participating team  
Context: the fifa world cup 2018 included teams such as france, croatia, and belgium.  
Head Entity: fifa world cup 2018  
Tail Entity: croatia  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: in 1971 , chris and pat joined ratchell with bassist howard messer and former steppenwolf guitarist larry byrom .
Head Entity: ratchell
Tail Entity: larry byrom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The car is equipped with a powerful engine, which includes a turbocharger and a high-performance exhaust system.  
Head Entity: the car  
Tail Entity: a turbocharger  

Relation: has part  
Context: The human body consists of various organs, including the heart, lungs, and liver, each playing a crucial role in maintaining health.  
Head Entity: the human body  
Tail Entity: the heart  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: gyllenhaal was nominated for an academy award for best supporting actress for her performance .
Head Entity: gyllenhaal
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: the film was nominated for several prestigious awards, including the golden globe for best drama.  
Head Entity: the film  
Tail Entity: golden globe for best drama  

Relation: nominated for  
Context: she was nominated for the emmy award for her outstanding performance in the television series.  
Head Entity: she  
Tail Entity: emmy award for her outstanding performance in the television series  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: in 2007 , the county municipality created the transit authority called " skyss " that would market public transport while is would be operated by private companies based on public service obligation contracts .
Head Entity: skyss
Tail Entity: transit authority
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone known as the iPhone was first released by Apple Inc. in 2007, revolutionizing the mobile phone industry and setting new standards for design and functionality.  
Head Entity: iPhone  
Tail Entity: smartphone  

Relation: instance of  
Context: The Great Wall of China is a series of fortifications made of various materials, built to protect the Chinese states and empires from invasions and raids.  
Head Entity: Great Wall of China  
Tail Entity: fortification
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: the mount kapaz or kepez ( ) is a mountain in lesser caucasus near ganja city in central azerbaijan .
Head Entity: mount kapaz
Tail Entity: lesser caucasus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada is a mountain range in the western united states, primarily in california, known for its stunning landscapes and diverse ecosystems.  
Head Entity: sierra nevada  
Tail Entity: western united states  

Relation: mountain range  
Context: the alps are a major mountain range in europe, stretching across eight countries and famous for their breathtaking scenery and skiing resorts.  
Head Entity: alps  
Tail Entity: europe  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: future films include " mcqueen " , ian bonhote ’s documentary about the fashion designer alexander mcqueen , brad anderson ’s thriller " beirut " , and mark pellington ’s drama " nostalgia " .
Head Entity: nostalgia
Tail Entity: mark pellington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the upcoming movie "the great adventure" is penned by renowned screenwriter jessica taylor, who has previously won multiple awards for her work in the industry.  
Head Entity: the great adventure  
Tail Entity: jessica taylor  

Relation: screenwriter  
Context: in a recent interview, director samuel l. jackson revealed that the screenplay for "the last journey" was crafted by the talented writer robert king, known for his gripping narratives.  
Head Entity: the last journey  
Tail Entity: robert king  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Doraemon" has been translated into multiple languages, but the original version is in Japanese, which has captivated audiences worldwide.  
Head Entity: Doraemon  
Tail Entity: Japanese  

Relation: language of work or name  
Context: The famous novel "One Hundred Years of Solitude" was originally written in Spanish by Gabriel García Márquez, and it has since been translated into numerous languages.  
Head Entity: One Hundred Years of Solitude  
Tail Entity: Spanish  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: the aircraft was a boeing 707 - 321c that first entered service with pan american world airways ( pan am ) in 1963 .
Head Entity: boeing 707 - 321c
Tail Entity: pan american world airways
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: the new electric bus fleet is managed by the city transportation authority, which ensures efficient public transit services.  
Head Entity: electric bus fleet  
Tail Entity: city transportation authority  

Relation: operator  
Context: the state-of-the-art research facility is operated by the national science foundation, providing resources for groundbreaking studies.  
Head Entity: research facility  
Tail Entity: national science foundation  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: the most important lama of this series was the third changkya , rolpai dorje , who was preceptor to the qianlong emperor of china , and chief representative of tibetan buddhism at the qing court .
Head Entity: rolpai dorje
Tail Entity: tibetan buddhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the cathedral of notre-dame is a famous example of french gothic architecture and serves as a major center for the roman catholic faith in paris.  
Head Entity: cathedral of notre-dame  
Tail Entity: roman catholic faith  

Relation: religion  
Context: the dalai lama is a prominent figure in the gelug school of tibetan buddhism, advocating for peace and compassion worldwide.  
Head Entity: dalai lama  
Tail Entity: tibetan buddhism  
Losses:  7.310531139373779 1.7999475002288818 0.5265267491340637
MemoryTrain:  epoch  0, batch     0 | loss: 7.3105311Losses:  6.650885581970215 1.310837745666504 0.5604451894760132
MemoryTrain:  epoch  0, batch     1 | loss: 6.6508856Losses:  4.903979301452637 0.8556338548660278 0.6986879110336304
MemoryTrain:  epoch  0, batch     2 | loss: 4.9039793Losses:  4.980663299560547 0.5680818557739258 0.722152590751648
MemoryTrain:  epoch  0, batch     3 | loss: 4.9806633Losses:  7.030697822570801 1.853588581085205 0.551093339920044
MemoryTrain:  epoch  1, batch     0 | loss: 7.0306978Losses:  4.344776630401611 0.6704776287078857 0.7661101222038269
MemoryTrain:  epoch  1, batch     1 | loss: 4.3447766Losses:  5.392661094665527 1.710909128189087 0.5356026887893677
MemoryTrain:  epoch  1, batch     2 | loss: 5.3926611Losses:  6.369985580444336 0.6100785136222839 0.6065337657928467
MemoryTrain:  epoch  1, batch     3 | loss: 6.3699856Losses:  4.349275588989258 1.56514573097229 0.4975425601005554
MemoryTrain:  epoch  2, batch     0 | loss: 4.3492756Losses:  4.573463439941406 0.9214239120483398 0.6925191283226013
MemoryTrain:  epoch  2, batch     1 | loss: 4.5734634Losses:  6.378388404846191 1.3957865238189697 0.7186888456344604
MemoryTrain:  epoch  2, batch     2 | loss: 6.3783884Losses:  5.384886741638184 0.7686037421226501 0.3793427646160126
MemoryTrain:  epoch  2, batch     3 | loss: 5.3848867Losses:  4.544319152832031 1.5283076763153076 0.503645658493042
MemoryTrain:  epoch  3, batch     0 | loss: 4.5443192Losses:  5.272994041442871 1.3111944198608398 0.5208152532577515
MemoryTrain:  epoch  3, batch     1 | loss: 5.2729940Losses:  4.861650466918945 0.6895861625671387 0.6807565689086914
MemoryTrain:  epoch  3, batch     2 | loss: 4.8616505Losses:  3.2376465797424316 -0.0 0.5907830595970154
MemoryTrain:  epoch  3, batch     3 | loss: 3.2376466Losses:  3.9307000637054443 1.2194157838821411 0.6764971613883972
MemoryTrain:  epoch  4, batch     0 | loss: 3.9307001Losses:  5.152622699737549 0.47904014587402344 0.6879895329475403
MemoryTrain:  epoch  4, batch     1 | loss: 5.1526227Losses:  3.6747639179229736 1.0750536918640137 0.43205076456069946
MemoryTrain:  epoch  4, batch     2 | loss: 3.6747639Losses:  4.249979019165039 0.6249091029167175 0.6227191090583801
MemoryTrain:  epoch  4, batch     3 | loss: 4.2499790Losses:  3.858701467514038 0.20525652170181274 0.7536318898200989
MemoryTrain:  epoch  5, batch     0 | loss: 3.8587015Losses:  4.548624038696289 1.3448923826217651 0.618773877620697
MemoryTrain:  epoch  5, batch     1 | loss: 4.5486240Losses:  4.04326868057251 1.354394555091858 0.5548347234725952
MemoryTrain:  epoch  5, batch     2 | loss: 4.0432687Losses:  4.091906547546387 0.9479894638061523 0.5444611310958862
MemoryTrain:  epoch  5, batch     3 | loss: 4.0919065Losses:  4.156280994415283 1.5349457263946533 0.483006089925766
MemoryTrain:  epoch  6, batch     0 | loss: 4.1562810Losses:  4.339197158813477 1.2219871282577515 0.6403499841690063
MemoryTrain:  epoch  6, batch     1 | loss: 4.3391972Losses:  4.5012030601501465 0.8767518997192383 0.6699945330619812
MemoryTrain:  epoch  6, batch     2 | loss: 4.5012031Losses:  3.7659196853637695 0.817351222038269 0.42161089181900024
MemoryTrain:  epoch  6, batch     3 | loss: 3.7659197Losses:  3.770684242248535 1.1031160354614258 0.6429023742675781
MemoryTrain:  epoch  7, batch     0 | loss: 3.7706842Losses:  5.521681785583496 1.3388068675994873 0.6907670497894287
MemoryTrain:  epoch  7, batch     1 | loss: 5.5216818Losses:  3.772709846496582 1.2345232963562012 0.6125635504722595
MemoryTrain:  epoch  7, batch     2 | loss: 3.7727098Losses:  2.311431407928467 0.5390820503234863 0.24050414562225342
MemoryTrain:  epoch  7, batch     3 | loss: 2.3114314Losses:  3.664757490158081 0.9170017242431641 0.6309520602226257
MemoryTrain:  epoch  8, batch     0 | loss: 3.6647575Losses:  3.0037803649902344 0.963127613067627 0.5610445737838745
MemoryTrain:  epoch  8, batch     1 | loss: 3.0037804Losses:  4.392947196960449 0.8730621337890625 0.7062689065933228
MemoryTrain:  epoch  8, batch     2 | loss: 4.3929472Losses:  4.281200408935547 1.1397919654846191 0.5005462169647217
MemoryTrain:  epoch  8, batch     3 | loss: 4.2812004Losses:  2.6681532859802246 0.4905123710632324 0.5962944030761719
MemoryTrain:  epoch  9, batch     0 | loss: 2.6681533Losses:  3.4121146202087402 1.0895864963531494 0.5343913435935974
MemoryTrain:  epoch  9, batch     1 | loss: 3.4121146Losses:  4.710132598876953 1.3869649171829224 0.7030776143074036
MemoryTrain:  epoch  9, batch     2 | loss: 4.7101326Losses:  2.8086631298065186 0.31773602962493896 0.4715349078178406
MemoryTrain:  epoch  9, batch     3 | loss: 2.8086631
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 59.38%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 64.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 67.97%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 74.48%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 74.04%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 67.08%   [EVAL] batch:   15 | acc: 18.75%,  total acc: 64.06%   [EVAL] batch:   16 | acc: 43.75%,  total acc: 62.87%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 61.81%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 60.20%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 65.06%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 66.58%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 67.97%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 69.00%   [EVAL] batch:   25 | acc: 68.75%,  total acc: 68.99%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 68.06%   [EVAL] batch:   27 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:   28 | acc: 50.00%,  total acc: 66.59%   [EVAL] batch:   29 | acc: 31.25%,  total acc: 65.42%   [EVAL] batch:   30 | acc: 18.75%,  total acc: 63.91%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 64.84%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 65.34%   [EVAL] batch:   33 | acc: 81.25%,  total acc: 65.81%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 66.43%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 66.72%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 68.27%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 69.66%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 70.39%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 71.73%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 72.55%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 73.14%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 73.18%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 73.72%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 74.12%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 74.14%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 74.40%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 74.53%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 74.65%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 75.11%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 75.11%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 75.11%   [EVAL] batch:   57 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 75.11%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 74.70%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 74.11%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 84.77%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 85.66%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 87.80%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 88.07%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 88.04%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 88.28%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 88.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.70%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.51%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.32%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 91.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.15%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.53%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 92.73%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 92.90%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 93.06%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 93.21%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 93.22%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 93.23%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 93.37%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 93.38%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 93.51%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 93.63%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 93.42%   [EVAL] batch:   57 | acc: 37.50%,  total acc: 92.46%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 92.27%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 91.88%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 91.60%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 91.33%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 91.07%   [EVAL] batch:   63 | acc: 56.25%,  total acc: 90.53%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 90.00%   [EVAL] batch:   65 | acc: 81.25%,  total acc: 89.87%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 89.18%   [EVAL] batch:   67 | acc: 62.50%,  total acc: 88.79%   [EVAL] batch:   68 | acc: 81.25%,  total acc: 88.68%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 88.66%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 88.64%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 88.45%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 88.53%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 88.60%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 88.58%   [EVAL] batch:   75 | acc: 31.25%,  total acc: 87.83%   [EVAL] batch:   76 | acc: 25.00%,  total acc: 87.01%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 86.14%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 85.52%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 85.00%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 84.26%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 84.07%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 84.26%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 84.45%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 84.56%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 84.74%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 84.84%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 84.94%   [EVAL] batch:   88 | acc: 43.75%,  total acc: 84.48%   [EVAL] batch:   89 | acc: 50.00%,  total acc: 84.10%   [EVAL] batch:   90 | acc: 43.75%,  total acc: 83.65%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 83.22%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 82.53%   [EVAL] batch:   93 | acc: 56.25%,  total acc: 82.25%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 82.30%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 82.29%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 82.35%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 82.33%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 82.20%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 82.19%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 82.36%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 82.54%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 82.65%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 82.81%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 82.98%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 83.14%   [EVAL] batch:  106 | acc: 93.75%,  total acc: 83.24%   [EVAL] batch:  107 | acc: 87.50%,  total acc: 83.28%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 83.43%   [EVAL] batch:  109 | acc: 93.75%,  total acc: 83.52%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 83.50%   [EVAL] batch:  111 | acc: 93.75%,  total acc: 83.59%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 83.68%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 83.55%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 83.70%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 83.67%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 83.65%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 83.63%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 83.56%   [EVAL] batch:  119 | acc: 81.25%,  total acc: 83.54%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 83.47%   [EVAL] batch:  121 | acc: 75.00%,  total acc: 83.40%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 83.18%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 83.11%   [EVAL] batch:  124 | acc: 68.75%,  total acc: 83.00%   
cur_acc:  ['0.9484', '0.7411']
his_acc:  ['0.9484', '0.8300']
Clustering into  14  clusters
Clusters:  [ 4  1  0  2  1  1  0  6  5  5  4  3 11 12  2  1  8  7  6  4 13  3 10  9
  7  3 10  4  5  4]
Losses:  11.658466339111328 5.264950752258301 0.5551307797431946
CurrentTrain: epoch  0, batch     0 | loss: 11.6584663Losses:  12.737969398498535 5.396911144256592 0.5327483415603638
CurrentTrain: epoch  0, batch     1 | loss: 12.7379694Losses:  11.22593879699707 4.469330787658691 0.6354962587356567
CurrentTrain: epoch  0, batch     2 | loss: 11.2259388Losses:  7.709442138671875 -0.0 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 7.7094421Losses:  10.662132263183594 3.8432092666625977 0.5734182000160217
CurrentTrain: epoch  1, batch     0 | loss: 10.6621323Losses:  10.27318286895752 4.1999592781066895 0.6115860342979431
CurrentTrain: epoch  1, batch     1 | loss: 10.2731829Losses:  10.051084518432617 4.871929168701172 0.6687931418418884
CurrentTrain: epoch  1, batch     2 | loss: 10.0510845Losses:  7.663376331329346 -0.0 0.19758456945419312
CurrentTrain: epoch  1, batch     3 | loss: 7.6633763Losses:  8.606352806091309 3.1832878589630127 0.6698847413063049
CurrentTrain: epoch  2, batch     0 | loss: 8.6063528Losses:  8.399796485900879 2.910231590270996 0.7019674181938171
CurrentTrain: epoch  2, batch     1 | loss: 8.3997965Losses:  9.765876770019531 4.294004440307617 0.5983240008354187
CurrentTrain: epoch  2, batch     2 | loss: 9.7658768Losses:  4.700427055358887 -0.0 0.13821889460086823
CurrentTrain: epoch  2, batch     3 | loss: 4.7004271Losses:  7.808378219604492 2.7940354347229004 0.7168880701065063
CurrentTrain: epoch  3, batch     0 | loss: 7.8083782Losses:  7.893972396850586 2.867324113845825 0.594070553779602
CurrentTrain: epoch  3, batch     1 | loss: 7.8939724Losses:  9.369033813476562 4.867696762084961 0.47884854674339294
CurrentTrain: epoch  3, batch     2 | loss: 9.3690338Losses:  1.8893979787826538 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 1.8893980Losses:  8.617389678955078 3.6397619247436523 0.6700931191444397
CurrentTrain: epoch  4, batch     0 | loss: 8.6173897Losses:  6.937380313873291 2.561069965362549 0.7173677682876587
CurrentTrain: epoch  4, batch     1 | loss: 6.9373803Losses:  8.613301277160645 4.567996501922607 0.45762577652931213
CurrentTrain: epoch  4, batch     2 | loss: 8.6133013Losses:  3.3614847660064697 -0.0 0.10972175002098083
CurrentTrain: epoch  4, batch     3 | loss: 3.3614848Losses:  7.4048943519592285 3.1303675174713135 0.633813202381134
CurrentTrain: epoch  5, batch     0 | loss: 7.4048944Losses:  8.325538635253906 3.9681479930877686 0.540961742401123
CurrentTrain: epoch  5, batch     1 | loss: 8.3255386Losses:  8.663907051086426 4.586939811706543 0.6622159481048584
CurrentTrain: epoch  5, batch     2 | loss: 8.6639071Losses:  4.397371292114258 -0.0 0.10399927198886871
CurrentTrain: epoch  5, batch     3 | loss: 4.3973713Losses:  7.615865707397461 3.184988498687744 0.6355225443840027
CurrentTrain: epoch  6, batch     0 | loss: 7.6158657Losses:  8.505899429321289 3.6839475631713867 0.540489137172699
CurrentTrain: epoch  6, batch     1 | loss: 8.5058994Losses:  6.312249183654785 2.739434242248535 0.6186935305595398
CurrentTrain: epoch  6, batch     2 | loss: 6.3122492Losses:  3.142717123031616 -0.0 0.17082735896110535
CurrentTrain: epoch  6, batch     3 | loss: 3.1427171Losses:  6.687777519226074 2.9818406105041504 0.6689721941947937
CurrentTrain: epoch  7, batch     0 | loss: 6.6877775Losses:  7.317893981933594 2.7002487182617188 0.622138261795044
CurrentTrain: epoch  7, batch     1 | loss: 7.3178940Losses:  5.842899322509766 2.2358291149139404 0.6335389614105225
CurrentTrain: epoch  7, batch     2 | loss: 5.8428993Losses:  2.3752551078796387 -0.0 0.10468114167451859
CurrentTrain: epoch  7, batch     3 | loss: 2.3752551Losses:  6.302304267883301 2.3116049766540527 0.5497853755950928
CurrentTrain: epoch  8, batch     0 | loss: 6.3023043Losses:  5.78867244720459 2.6345157623291016 0.6245441436767578
CurrentTrain: epoch  8, batch     1 | loss: 5.7886724Losses:  7.431499481201172 4.446887016296387 0.5864317417144775
CurrentTrain: epoch  8, batch     2 | loss: 7.4314995Losses:  3.6773502826690674 -0.0 0.17449861764907837
CurrentTrain: epoch  8, batch     3 | loss: 3.6773503Losses:  5.736689567565918 2.666563034057617 0.5958947539329529
CurrentTrain: epoch  9, batch     0 | loss: 5.7366896Losses:  6.747253894805908 3.5831751823425293 0.6292170286178589
CurrentTrain: epoch  9, batch     1 | loss: 6.7472539Losses:  6.506758213043213 2.71307373046875 0.5999571084976196
CurrentTrain: epoch  9, batch     2 | loss: 6.5067582Losses:  1.8081865310668945 -0.0 0.131276935338974
CurrentTrain: epoch  9, batch     3 | loss: 1.8081865
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: peugeot took a similar step in 2010 when replacing the 407 and long - running but unpopular 607 with a single model , the 508 .
Head Entity: 508
Tail Entity: 407
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: In the 2021 season, the team introduced the new model, the X5, which follows the successful launch of the X3 in 2020.  
Head Entity: X5  
Tail Entity: X3  

Relation: follows  
Context: The latest smartphone, the Galaxy S21, follows the previous model, the Galaxy S20, which was released just a year earlier.  
Head Entity: Galaxy S21  
Tail Entity: Galaxy S20  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: dick packer was a u.s. soccer center forward who was a member of the u.s. team at the 1956 summer olympics .
Head Entity: dick packer
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams is a professional tennis player who has won numerous grand slam titles throughout her career.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james is known for his exceptional skills in basketball and has played for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of lucas, who had recently graduated from university.  
Head Entity: lucas  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: since 2009 , culshaw has starred in the bbc one comedy sketch show " the impressions show " alongside debra stephenson .
Head Entity: the impressions show
Tail Entity: bbc one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: The popular series "Breaking Bad" first aired on AMC, captivating audiences with its intense storytelling and character development.  
Head Entity: Breaking Bad  
Tail Entity: AMC  

Relation: original network  
Context: "Friends" became a cultural phenomenon when it premiered on NBC, showcasing the lives and relationships of six friends living in New York City.  
Head Entity: Friends  
Tail Entity: NBC  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English and became a global phenomenon.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2016 film "La La Land," directed by Damien Chazelle, was originally filmed in English, showcasing the vibrant culture of Los Angeles.  
Head Entity: La La Land  
Tail Entity: English  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: after years on loan with various lower division turkish teams , cangöz made his professional debut for antalyaspor in a 4 - 1 süper lig victory over gaziantepspor on 2 june 2017 .
Head Entity: antalyaspor
Tail Entity: süper lig
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: in 2020, the talented midfielder signed with the club after impressing in the youth ranks, and he quickly became a key player in the premier league for manchester city.  
Head Entity: manchester city  
Tail Entity: premier league  

Relation: league  
Context: during his time at the club, the striker helped lead the team to victory in the championship, showcasing his skills in the english football league.  
Head Entity: nottingham forest  
Tail Entity: championship  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: other teammates included dominique leray and élisabeth riffiod , whose son boris diaw currently plays in the nba for the spurs ( 2014 ) .
Head Entity: boris diaw
Tail Entity: élisabeth riffiod
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in a recent interview, actress jennifer garner spoke fondly of her children, including violet, who is the daughter of her ex-husband, ben affleck.  
Head Entity: violet  
Tail Entity: jennifer garner  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much emily cherished her time with her mother, who had always supported her dreams.  
Head Entity: emily  
Tail Entity: emily's mother  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly famous for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: ploegsteert is a village in belgium located in the municipality of comines - warneton in the hainaut province and is the most westerly settlement of the walloon region .
Head Entity: hainaut
Tail Entity: belgium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the city of prague is the capital of the czech republic and is known for its beautiful architecture and rich history.  
Head Entity: prague  
Tail Entity: czech republic  

Relation: country  
Context: the great barrier reef is located off the coast of australia and is the largest coral reef system in the world.  
Head Entity: great barrier reef  
Tail Entity: australia  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Losses:  4.574771881103516 0.2645798623561859 0.8143677115440369
MemoryTrain:  epoch  0, batch     0 | loss: 4.5747719Losses:  4.70391321182251 0.4897799789905548 0.7346227765083313
MemoryTrain:  epoch  0, batch     1 | loss: 4.7039132Losses:  4.9633283615112305 0.5648771524429321 0.758506178855896
MemoryTrain:  epoch  0, batch     2 | loss: 4.9633284Losses:  5.232000350952148 0.8886687159538269 0.7313778400421143
MemoryTrain:  epoch  0, batch     3 | loss: 5.2320004Losses:  4.917147159576416 0.485609233379364 0.7503535151481628
MemoryTrain:  epoch  0, batch     4 | loss: 4.9171472Losses:  4.720185279846191 0.39072972536087036 0.5744972825050354
MemoryTrain:  epoch  0, batch     5 | loss: 4.7201853Losses:  4.384250640869141 0.7039263248443604 0.6714382171630859
MemoryTrain:  epoch  1, batch     0 | loss: 4.3842506Losses:  3.6025218963623047 0.24035578966140747 0.8313976526260376
MemoryTrain:  epoch  1, batch     1 | loss: 3.6025219Losses:  4.746700286865234 -0.0 0.9094259142875671
MemoryTrain:  epoch  1, batch     2 | loss: 4.7467003Losses:  5.4040937423706055 1.3123748302459717 0.6783952713012695
MemoryTrain:  epoch  1, batch     3 | loss: 5.4040937Losses:  4.90932035446167 0.5468279719352722 0.766269862651825
MemoryTrain:  epoch  1, batch     4 | loss: 4.9093204Losses:  3.405088186264038 -0.0 0.6765946745872498
MemoryTrain:  epoch  1, batch     5 | loss: 3.4050882Losses:  5.318583965301514 0.7052348852157593 0.8304125070571899
MemoryTrain:  epoch  2, batch     0 | loss: 5.3185840Losses:  4.074951171875 0.7969136238098145 0.8120393753051758
MemoryTrain:  epoch  2, batch     1 | loss: 4.0749512Losses:  3.4212241172790527 0.7290502786636353 0.6554416418075562
MemoryTrain:  epoch  2, batch     2 | loss: 3.4212241Losses:  3.4629874229431152 -0.0 0.861325740814209
MemoryTrain:  epoch  2, batch     3 | loss: 3.4629874Losses:  5.222141742706299 1.1373027563095093 0.8031852841377258
MemoryTrain:  epoch  2, batch     4 | loss: 5.2221417Losses:  2.732994556427002 -0.0 0.5729134678840637
MemoryTrain:  epoch  2, batch     5 | loss: 2.7329946Losses:  3.950888156890869 0.2407885193824768 0.753501832485199
MemoryTrain:  epoch  3, batch     0 | loss: 3.9508882Losses:  4.618678092956543 0.7962437272071838 0.7590830326080322
MemoryTrain:  epoch  3, batch     1 | loss: 4.6186781Losses:  3.934781312942505 0.7335134744644165 0.7966836094856262
MemoryTrain:  epoch  3, batch     2 | loss: 3.9347813Losses:  3.002423048019409 0.5110886096954346 0.7329747676849365
MemoryTrain:  epoch  3, batch     3 | loss: 3.0024230Losses:  3.263265371322632 0.2879257798194885 0.7532461285591125
MemoryTrain:  epoch  3, batch     4 | loss: 3.2632654Losses:  3.6649415493011475 0.32184895873069763 0.5366933941841125
MemoryTrain:  epoch  3, batch     5 | loss: 3.6649415Losses:  3.8085622787475586 0.6158312559127808 0.8089157342910767
MemoryTrain:  epoch  4, batch     0 | loss: 3.8085623Losses:  3.2326455116271973 0.22037193179130554 0.7049301266670227
MemoryTrain:  epoch  4, batch     1 | loss: 3.2326455Losses:  3.303030490875244 0.2557840943336487 0.9772628545761108
MemoryTrain:  epoch  4, batch     2 | loss: 3.3030305Losses:  3.344353437423706 0.5545744299888611 0.7033503651618958
MemoryTrain:  epoch  4, batch     3 | loss: 3.3443534Losses:  4.248462677001953 1.5358154773712158 0.5616316795349121
MemoryTrain:  epoch  4, batch     4 | loss: 4.2484627Losses:  5.399295330047607 2.313758373260498 0.5506271123886108
MemoryTrain:  epoch  4, batch     5 | loss: 5.3992953Losses:  3.9821083545684814 1.08054780960083 0.7220232486724854
MemoryTrain:  epoch  5, batch     0 | loss: 3.9821084Losses:  3.6195218563079834 0.8061713576316833 0.70278000831604
MemoryTrain:  epoch  5, batch     1 | loss: 3.6195219Losses:  3.4273366928100586 0.6845368146896362 0.7946945428848267
MemoryTrain:  epoch  5, batch     2 | loss: 3.4273367Losses:  3.0756492614746094 0.5123323202133179 0.8277525305747986
MemoryTrain:  epoch  5, batch     3 | loss: 3.0756493Losses:  3.9254462718963623 0.9927823543548584 0.8691415190696716
MemoryTrain:  epoch  5, batch     4 | loss: 3.9254463Losses:  3.1169979572296143 0.4052271842956543 0.6015356183052063
MemoryTrain:  epoch  5, batch     5 | loss: 3.1169980Losses:  3.5150365829467773 1.1285362243652344 0.5651047229766846
MemoryTrain:  epoch  6, batch     0 | loss: 3.5150366Losses:  2.9576144218444824 0.47375917434692383 0.7885205149650574
MemoryTrain:  epoch  6, batch     1 | loss: 2.9576144Losses:  3.061016082763672 0.2679440975189209 0.8170726895332336
MemoryTrain:  epoch  6, batch     2 | loss: 3.0610161Losses:  3.9514710903167725 1.5290510654449463 0.6443292498588562
MemoryTrain:  epoch  6, batch     3 | loss: 3.9514711Losses:  3.120297431945801 0.28810280561447144 1.0006502866744995
MemoryTrain:  epoch  6, batch     4 | loss: 3.1202974Losses:  2.3216538429260254 -0.0 0.6804773807525635
MemoryTrain:  epoch  6, batch     5 | loss: 2.3216538Losses:  3.5607962608337402 1.612487554550171 0.6472362875938416
MemoryTrain:  epoch  7, batch     0 | loss: 3.5607963Losses:  3.6989669799804688 1.350487470626831 0.7383513450622559
MemoryTrain:  epoch  7, batch     1 | loss: 3.6989670Losses:  3.069920063018799 0.5024089217185974 0.8051527142524719
MemoryTrain:  epoch  7, batch     2 | loss: 3.0699201Losses:  2.7913570404052734 0.5471735596656799 0.8028437495231628
MemoryTrain:  epoch  7, batch     3 | loss: 2.7913570Losses:  3.1288368701934814 0.29806339740753174 0.9842000007629395
MemoryTrain:  epoch  7, batch     4 | loss: 3.1288369Losses:  2.1007771492004395 0.20871369540691376 0.6673711538314819
MemoryTrain:  epoch  7, batch     5 | loss: 2.1007771Losses:  3.390167474746704 1.3984861373901367 0.59033203125
MemoryTrain:  epoch  8, batch     0 | loss: 3.3901675Losses:  3.495278835296631 1.091376543045044 0.8060552477836609
MemoryTrain:  epoch  8, batch     1 | loss: 3.4952788Losses:  3.752707004547119 1.1768635511398315 0.680819034576416
MemoryTrain:  epoch  8, batch     2 | loss: 3.7527070Losses:  2.893681526184082 0.7717407941818237 0.7814487814903259
MemoryTrain:  epoch  8, batch     3 | loss: 2.8936815Losses:  2.9444169998168945 0.4973052740097046 0.8847106695175171
MemoryTrain:  epoch  8, batch     4 | loss: 2.9444170Losses:  2.236290216445923 0.24996362626552582 0.6493309736251831
MemoryTrain:  epoch  8, batch     5 | loss: 2.2362902Losses:  2.591174840927124 0.25098130106925964 0.7432568073272705
MemoryTrain:  epoch  9, batch     0 | loss: 2.5911748Losses:  3.169956684112549 0.5572910308837891 0.8713169097900391
MemoryTrain:  epoch  9, batch     1 | loss: 3.1699567Losses:  3.2091927528381348 1.2581334114074707 0.51221764087677
MemoryTrain:  epoch  9, batch     2 | loss: 3.2091928Losses:  2.5219480991363525 0.4541095197200775 0.7806243300437927
MemoryTrain:  epoch  9, batch     3 | loss: 2.5219481Losses:  3.6025309562683105 1.2912229299545288 0.7489705085754395
MemoryTrain:  epoch  9, batch     4 | loss: 3.6025310Losses:  1.9889236688613892 -0.0 0.5954660177230835
MemoryTrain:  epoch  9, batch     5 | loss: 1.9889237
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 58.75%   [EVAL] batch:    5 | acc: 43.75%,  total acc: 56.25%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 58.93%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 60.94%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 64.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 71.63%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 72.32%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:   15 | acc: 62.50%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 56.25%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 73.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 75.85%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 76.90%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 77.86%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 78.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 79.09%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.86%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 80.58%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 81.67%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 82.06%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 82.42%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 83.27%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.57%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 83.85%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 84.12%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.54%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 84.46%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 84.22%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 83.99%   [EVAL] batch:   41 | acc: 62.50%,  total acc: 83.48%   [EVAL] batch:   42 | acc: 50.00%,  total acc: 82.70%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 82.81%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 83.19%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 83.42%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 83.78%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 84.11%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 84.18%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 83.95%   [EVAL] batch:   51 | acc: 50.00%,  total acc: 83.29%   [EVAL] batch:   52 | acc: 37.50%,  total acc: 82.43%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 81.94%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 81.59%   [EVAL] batch:   55 | acc: 31.25%,  total acc: 80.69%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 80.37%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 80.50%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 80.08%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 80.10%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 80.02%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 80.34%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 79.76%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 81.88%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 80.11%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 80.21%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 84.19%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.72%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.53%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 86.01%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 86.08%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 86.14%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.02%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.93%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.71%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.39%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.71%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.10%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.37%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.87%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.09%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.31%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.52%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.57%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.76%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.12%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 92.15%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 92.06%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.22%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 92.25%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.28%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.33%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.48%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.39%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 92.00%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 91.38%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 91.10%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 90.94%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 90.68%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 90.22%   [EVAL] batch:   62 | acc: 68.75%,  total acc: 89.88%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 89.26%   [EVAL] batch:   64 | acc: 56.25%,  total acc: 88.75%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 88.54%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 87.78%   [EVAL] batch:   67 | acc: 50.00%,  total acc: 87.22%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 87.05%   [EVAL] batch:   69 | acc: 100.00%,  total acc: 87.23%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 87.24%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 87.15%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 87.24%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 87.33%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 87.33%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 86.68%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 85.96%   [EVAL] batch:   77 | acc: 18.75%,  total acc: 85.10%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 84.49%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 83.98%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 83.26%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 83.08%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 83.48%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 83.60%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 83.91%   [EVAL] batch:   87 | acc: 93.75%,  total acc: 84.02%   [EVAL] batch:   88 | acc: 56.25%,  total acc: 83.71%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 83.47%   [EVAL] batch:   90 | acc: 50.00%,  total acc: 83.10%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 82.74%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 82.06%   [EVAL] batch:   93 | acc: 68.75%,  total acc: 81.91%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 81.97%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 81.97%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 81.89%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 81.89%   [EVAL] batch:   98 | acc: 62.50%,  total acc: 81.69%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 81.62%   [EVAL] batch:  100 | acc: 87.50%,  total acc: 81.68%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 81.86%   [EVAL] batch:  102 | acc: 93.75%,  total acc: 81.98%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 82.09%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 82.43%   [EVAL] batch:  106 | acc: 18.75%,  total acc: 81.83%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 81.19%   [EVAL] batch:  108 | acc: 25.00%,  total acc: 80.68%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 80.06%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 79.45%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 78.96%   [EVAL] batch:  112 | acc: 43.75%,  total acc: 78.65%   [EVAL] batch:  113 | acc: 68.75%,  total acc: 78.56%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 78.75%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 78.72%   [EVAL] batch:  116 | acc: 81.25%,  total acc: 78.74%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 78.81%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 78.78%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:  120 | acc: 87.50%,  total acc: 78.98%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 79.00%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 78.86%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 78.83%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 78.80%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 78.62%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 78.40%   [EVAL] batch:  127 | acc: 68.75%,  total acc: 78.32%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 78.25%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 78.03%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 77.77%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 77.75%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 77.73%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 77.85%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 77.96%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 78.03%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 78.10%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 78.12%   [EVAL] batch:  138 | acc: 81.25%,  total acc: 78.15%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 78.17%   [EVAL] batch:  140 | acc: 62.50%,  total acc: 78.06%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 78.10%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 77.95%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 78.10%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 78.25%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 78.36%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 78.51%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 78.65%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 78.71%   [EVAL] batch:  150 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:  151 | acc: 100.00%,  total acc: 78.99%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 79.13%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 79.26%   [EVAL] batch:  154 | acc: 93.75%,  total acc: 79.35%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 79.45%   [EVAL] batch:  156 | acc: 93.75%,  total acc: 79.54%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 79.67%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 79.76%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 79.84%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 79.93%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 80.02%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 80.14%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 80.14%   [EVAL] batch:  164 | acc: 75.00%,  total acc: 80.11%   [EVAL] batch:  165 | acc: 75.00%,  total acc: 80.08%   [EVAL] batch:  166 | acc: 62.50%,  total acc: 79.98%   [EVAL] batch:  167 | acc: 50.00%,  total acc: 79.80%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 79.84%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 79.96%   [EVAL] batch:  170 | acc: 93.75%,  total acc: 80.04%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 80.16%   [EVAL] batch:  172 | acc: 100.00%,  total acc: 80.27%   [EVAL] batch:  173 | acc: 87.50%,  total acc: 80.32%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 80.39%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 80.29%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 80.12%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 79.88%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 79.75%   [EVAL] batch:  179 | acc: 62.50%,  total acc: 79.65%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 79.39%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 79.29%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 79.34%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 79.21%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 79.22%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 79.20%   [EVAL] batch:  186 | acc: 100.00%,  total acc: 79.31%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 79.12%   
cur_acc:  ['0.9484', '0.7411', '0.7976']
his_acc:  ['0.9484', '0.8300', '0.7912']
Clustering into  19  clusters
Clusters:  [ 8  4  1  2  4  4  1  6 14 14  3  0 13 16  2  4 17  9  6  3 11  0  7 12
  9  0  7  8 14  8 10  8 18  0  5  0 15  7  3  7]
Losses:  13.055992126464844 6.133893013000488 0.7679531574249268
CurrentTrain: epoch  0, batch     0 | loss: 13.0559921Losses:  10.03313159942627 4.087968826293945 0.7590166330337524
CurrentTrain: epoch  0, batch     1 | loss: 10.0331316Losses:  11.215084075927734 4.349750518798828 0.6601698994636536
CurrentTrain: epoch  0, batch     2 | loss: 11.2150841Losses:  5.765532493591309 -0.0 0.10270863771438599
CurrentTrain: epoch  0, batch     3 | loss: 5.7655325Losses:  7.71496057510376 1.9873898029327393 0.7274184226989746
CurrentTrain: epoch  1, batch     0 | loss: 7.7149606Losses:  10.040570259094238 3.7712888717651367 0.7109519839286804
CurrentTrain: epoch  1, batch     1 | loss: 10.0405703Losses:  7.845829010009766 2.525461196899414 0.6170928478240967
CurrentTrain: epoch  1, batch     2 | loss: 7.8458290Losses:  4.074975490570068 -0.0 0.22081339359283447
CurrentTrain: epoch  1, batch     3 | loss: 4.0749755Losses:  10.046650886535645 4.472198486328125 0.7496592402458191
CurrentTrain: epoch  2, batch     0 | loss: 10.0466509Losses:  8.785785675048828 3.746621608734131 0.510040283203125
CurrentTrain: epoch  2, batch     1 | loss: 8.7857857Losses:  8.333536148071289 3.3581228256225586 0.639067530632019
CurrentTrain: epoch  2, batch     2 | loss: 8.3335361Losses:  4.78902006149292 -0.0 0.11984291672706604
CurrentTrain: epoch  2, batch     3 | loss: 4.7890201Losses:  7.72105073928833 3.2915091514587402 0.682318389415741
CurrentTrain: epoch  3, batch     0 | loss: 7.7210507Losses:  9.151992797851562 3.865523338317871 0.5757352709770203
CurrentTrain: epoch  3, batch     1 | loss: 9.1519928Losses:  9.751692771911621 4.463500022888184 0.5349721908569336
CurrentTrain: epoch  3, batch     2 | loss: 9.7516928Losses:  2.4648501873016357 -0.0 0.12468322366476059
CurrentTrain: epoch  3, batch     3 | loss: 2.4648502Losses:  7.224782466888428 2.6912102699279785 0.5916010141372681
CurrentTrain: epoch  4, batch     0 | loss: 7.2247825Losses:  9.455156326293945 3.9063985347747803 0.6253652572631836
CurrentTrain: epoch  4, batch     1 | loss: 9.4551563Losses:  7.103525638580322 2.693812370300293 0.6777054667472839
CurrentTrain: epoch  4, batch     2 | loss: 7.1035256Losses:  2.7462503910064697 -0.0 0.22727349400520325
CurrentTrain: epoch  4, batch     3 | loss: 2.7462504Losses:  7.742470741271973 2.880863666534424 0.5937007069587708
CurrentTrain: epoch  5, batch     0 | loss: 7.7424707Losses:  6.586662769317627 2.8184499740600586 0.5914058685302734
CurrentTrain: epoch  5, batch     1 | loss: 6.5866628Losses:  7.8143534660339355 3.243710517883301 0.5970084071159363
CurrentTrain: epoch  5, batch     2 | loss: 7.8143535Losses:  4.19004487991333 -0.0 0.10718682408332825
CurrentTrain: epoch  5, batch     3 | loss: 4.1900449Losses:  7.3369317054748535 3.3406150341033936 0.5868587493896484
CurrentTrain: epoch  6, batch     0 | loss: 7.3369317Losses:  6.9114508628845215 2.3345541954040527 0.6631761789321899
CurrentTrain: epoch  6, batch     1 | loss: 6.9114509Losses:  6.367392063140869 2.442633628845215 0.6637355089187622
CurrentTrain: epoch  6, batch     2 | loss: 6.3673921Losses:  3.045530319213867 -0.0 0.15190088748931885
CurrentTrain: epoch  6, batch     3 | loss: 3.0455303Losses:  6.324946880340576 2.231570243835449 0.6675683259963989
CurrentTrain: epoch  7, batch     0 | loss: 6.3249469Losses:  7.090468406677246 2.8611865043640137 0.6603360176086426
CurrentTrain: epoch  7, batch     1 | loss: 7.0904684Losses:  6.265451431274414 2.6494297981262207 0.6481763124465942
CurrentTrain: epoch  7, batch     2 | loss: 6.2654514Losses:  2.24603271484375 -0.0 0.11245851218700409
CurrentTrain: epoch  7, batch     3 | loss: 2.2460327Losses:  8.004982948303223 3.9189109802246094 0.60166996717453
CurrentTrain: epoch  8, batch     0 | loss: 8.0049829Losses:  5.919022560119629 2.7196574211120605 0.6326006054878235
CurrentTrain: epoch  8, batch     1 | loss: 5.9190226Losses:  5.4397873878479 2.038053035736084 0.6062917709350586
CurrentTrain: epoch  8, batch     2 | loss: 5.4397874Losses:  1.90778648853302 -0.0 0.1672225296497345
CurrentTrain: epoch  8, batch     3 | loss: 1.9077865Losses:  7.188200950622559 3.666696071624756 0.5372067093849182
CurrentTrain: epoch  9, batch     0 | loss: 7.1882010Losses:  7.090808391571045 4.53984260559082 0.4928813576698303
CurrentTrain: epoch  9, batch     1 | loss: 7.0908084Losses:  6.593905448913574 3.129462957382202 0.5711013078689575
CurrentTrain: epoch  9, batch     2 | loss: 6.5939054Losses:  1.7518705129623413 -0.0 0.09172239899635315
CurrentTrain: epoch  9, batch     3 | loss: 1.7518705
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Queen  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the concert last summer.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: an emd gp49 is a 4-axle diesel locomotive built by general motors electro - motive division .
Head Entity: emd gp49
Tail Entity: general motors electro - motive division
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: the iphone 13 is a smartphone designed and marketed by apple inc.  
Head Entity: iphone 13  
Tail Entity: apple inc.  

Relation: manufacturer  
Context: the model s is an all-electric sedan produced by tesla, inc.  
Head Entity: model s  
Tail Entity: tesla, inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several startups showcased their innovative products, while industry leaders discussed the future of technology.  
Head Entity: tech conference  
Tail Entity: startups  

Relation: participant  
Context: The local community organized a charity run to raise funds for the homeless, with many volunteers helping to make the event a success.  
Head Entity: charity run  
Tail Entity: volunteers  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their primary operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested race, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested race  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: the deaths of his brothers wenceslaus ii ( 1487 ) , casimir ii ( 1490 ) and władysław ( 1494 ) allowed jan v to reunificated the whole duchy of zator .
Head Entity: casimir ii
Tail Entity: wenceslaus ii
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: both elizabeth and her brother, charles, were known for their contributions to the arts and culture of their time.  
Head Entity: elizabeth  
Tail Entity: charles  

Relation: sibling  
Context: during the family reunion, it was heartwarming to see how much john and his sister, sarah, resembled each other in both looks and personality.  
Head Entity: john  
Tail Entity: sarah  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: the united states navy is known for its naval operations and maritime security.  
Head Entity: united states navy  
Tail Entity: naval operations  

Relation: military branch  
Context: general patton served in the united states army during world war ii.  
Head Entity: general patton  
Tail Entity: united states army  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
Losses:  5.600916862487793 0.857141375541687 1.0202040672302246
MemoryTrain:  epoch  0, batch     0 | loss: 5.6009169Losses:  4.8029866218566895 0.26856258511543274 0.9249369502067566
MemoryTrain:  epoch  0, batch     1 | loss: 4.8029866Losses:  5.752477645874023 0.2783675193786621 1.0184385776519775
MemoryTrain:  epoch  0, batch     2 | loss: 5.7524776Losses:  6.701426029205322 1.3292391300201416 0.8545640110969543
MemoryTrain:  epoch  0, batch     3 | loss: 6.7014260Losses:  5.155658721923828 0.8273221254348755 0.8184518814086914
MemoryTrain:  epoch  0, batch     4 | loss: 5.1556587Losses:  5.7909770011901855 1.661891222000122 0.5887085199356079
MemoryTrain:  epoch  0, batch     5 | loss: 5.7909770Losses:  6.166365146636963 0.8460835218429565 0.9558886885643005
MemoryTrain:  epoch  0, batch     6 | loss: 6.1663651Losses:  3.726123332977295 -0.0 0.6945281028747559
MemoryTrain:  epoch  0, batch     7 | loss: 3.7261233Losses:  4.157068252563477 0.5384741425514221 0.8038644790649414
MemoryTrain:  epoch  1, batch     0 | loss: 4.1570683Losses:  4.176233291625977 0.2365657240152359 0.8836848735809326
MemoryTrain:  epoch  1, batch     1 | loss: 4.1762333Losses:  5.377992630004883 0.4567009210586548 0.7808175086975098
MemoryTrain:  epoch  1, batch     2 | loss: 5.3779926Losses:  4.332169055938721 0.5036308169364929 0.8502775430679321
MemoryTrain:  epoch  1, batch     3 | loss: 4.3321691Losses:  5.385530471801758 1.0904357433319092 0.806309163570404
MemoryTrain:  epoch  1, batch     4 | loss: 5.3855305Losses:  4.267708778381348 0.3007899522781372 0.9233338832855225
MemoryTrain:  epoch  1, batch     5 | loss: 4.2677088Losses:  4.587600231170654 0.795067310333252 0.8357301950454712
MemoryTrain:  epoch  1, batch     6 | loss: 4.5876002Losses:  4.026006698608398 -0.0 0.6729950308799744
MemoryTrain:  epoch  1, batch     7 | loss: 4.0260067Losses:  4.778014659881592 0.5597542524337769 0.8164005279541016
MemoryTrain:  epoch  2, batch     0 | loss: 4.7780147Losses:  3.5151283740997314 0.8151694536209106 0.6887405514717102
MemoryTrain:  epoch  2, batch     1 | loss: 3.5151284Losses:  3.7975621223449707 0.25498926639556885 0.974054753780365
MemoryTrain:  epoch  2, batch     2 | loss: 3.7975621Losses:  3.9858241081237793 0.2798452377319336 0.8480398058891296
MemoryTrain:  epoch  2, batch     3 | loss: 3.9858241Losses:  3.8139023780822754 0.31163111329078674 0.788609504699707
MemoryTrain:  epoch  2, batch     4 | loss: 3.8139024Losses:  3.4121286869049072 0.2734093964099884 0.8751127123832703
MemoryTrain:  epoch  2, batch     5 | loss: 3.4121287Losses:  4.296420574188232 0.5569417476654053 0.7924600839614868
MemoryTrain:  epoch  2, batch     6 | loss: 4.2964206Losses:  2.3560714721679688 -0.0 0.37980401515960693
MemoryTrain:  epoch  2, batch     7 | loss: 2.3560715Losses:  3.5662307739257812 0.8980045318603516 0.8445090055465698
MemoryTrain:  epoch  3, batch     0 | loss: 3.5662308Losses:  3.5646767616271973 0.25346505641937256 0.8971740007400513
MemoryTrain:  epoch  3, batch     1 | loss: 3.5646768Losses:  3.980275869369507 0.34036004543304443 0.8192031979560852
MemoryTrain:  epoch  3, batch     2 | loss: 3.9802759Losses:  3.3861804008483887 0.27037349343299866 0.9232559204101562
MemoryTrain:  epoch  3, batch     3 | loss: 3.3861804Losses:  2.8696560859680176 -0.0 0.9644428491592407
MemoryTrain:  epoch  3, batch     4 | loss: 2.8696561Losses:  4.084084510803223 0.596800684928894 0.8098365068435669
MemoryTrain:  epoch  3, batch     5 | loss: 4.0840845Losses:  2.8200666904449463 0.4831896722316742 0.8713916540145874
MemoryTrain:  epoch  3, batch     6 | loss: 2.8200667Losses:  2.5975112915039062 -0.0 0.6292736530303955
MemoryTrain:  epoch  3, batch     7 | loss: 2.5975113Losses:  3.7934281826019287 0.5681881904602051 0.6858870387077332
MemoryTrain:  epoch  4, batch     0 | loss: 3.7934282Losses:  4.080240249633789 0.6336041688919067 0.709829568862915
MemoryTrain:  epoch  4, batch     1 | loss: 4.0802402Losses:  2.924942970275879 0.23628120124340057 0.9030686616897583
MemoryTrain:  epoch  4, batch     2 | loss: 2.9249430Losses:  3.1732053756713867 0.771685779094696 0.7452908754348755
MemoryTrain:  epoch  4, batch     3 | loss: 3.1732054Losses:  2.5998928546905518 -0.0 0.9087212681770325
MemoryTrain:  epoch  4, batch     4 | loss: 2.5998929Losses:  2.8449184894561768 0.5116652250289917 0.7878864407539368
MemoryTrain:  epoch  4, batch     5 | loss: 2.8449185Losses:  3.358693838119507 0.5063691735267639 0.9229609370231628
MemoryTrain:  epoch  4, batch     6 | loss: 3.3586938Losses:  2.738852024078369 0.2681874632835388 0.5202741622924805
MemoryTrain:  epoch  4, batch     7 | loss: 2.7388520Losses:  2.778379201889038 0.27765995264053345 0.8185359835624695
MemoryTrain:  epoch  5, batch     0 | loss: 2.7783792Losses:  3.3863778114318848 0.6137973070144653 0.853914201259613
MemoryTrain:  epoch  5, batch     1 | loss: 3.3863778Losses:  3.6181414127349854 0.49650251865386963 0.7670295834541321
MemoryTrain:  epoch  5, batch     2 | loss: 3.6181414Losses:  3.8284497261047363 1.0809893608093262 0.878754734992981
MemoryTrain:  epoch  5, batch     3 | loss: 3.8284497Losses:  3.3479533195495605 0.2256619781255722 0.9334091544151306
MemoryTrain:  epoch  5, batch     4 | loss: 3.3479533Losses:  3.232424259185791 0.5259350538253784 0.8648637533187866
MemoryTrain:  epoch  5, batch     5 | loss: 3.2324243Losses:  3.1773715019226074 0.75687575340271 0.7391317486763
MemoryTrain:  epoch  5, batch     6 | loss: 3.1773715Losses:  2.7875783443450928 -0.0 0.4602949321269989
MemoryTrain:  epoch  5, batch     7 | loss: 2.7875783Losses:  2.4095003604888916 -0.0 0.7953758239746094
MemoryTrain:  epoch  6, batch     0 | loss: 2.4095004Losses:  2.878328323364258 0.35174429416656494 0.7072271704673767
MemoryTrain:  epoch  6, batch     1 | loss: 2.8783283Losses:  2.8962488174438477 -0.0 0.9029842019081116
MemoryTrain:  epoch  6, batch     2 | loss: 2.8962488Losses:  2.5810251235961914 0.4466836452484131 0.7875140309333801
MemoryTrain:  epoch  6, batch     3 | loss: 2.5810251Losses:  3.4426827430725098 0.6111823320388794 0.8719215393066406
MemoryTrain:  epoch  6, batch     4 | loss: 3.4426827Losses:  3.973728656768799 0.9918016791343689 0.8891586065292358
MemoryTrain:  epoch  6, batch     5 | loss: 3.9737287Losses:  3.1723124980926514 0.5286166667938232 0.861769437789917
MemoryTrain:  epoch  6, batch     6 | loss: 3.1723125Losses:  2.0641987323760986 -0.0 0.6857460737228394
MemoryTrain:  epoch  6, batch     7 | loss: 2.0641987Losses:  2.600471019744873 0.24442215263843536 0.8074540495872498
MemoryTrain:  epoch  7, batch     0 | loss: 2.6004710Losses:  3.349160671234131 1.1154744625091553 0.7863258719444275
MemoryTrain:  epoch  7, batch     1 | loss: 3.3491607Losses:  2.681875228881836 -0.0 0.9648216366767883
MemoryTrain:  epoch  7, batch     2 | loss: 2.6818752Losses:  2.777332305908203 0.5167672038078308 0.6198485493659973
MemoryTrain:  epoch  7, batch     3 | loss: 2.7773323Losses:  3.12121844291687 0.9661461114883423 0.7320006489753723
MemoryTrain:  epoch  7, batch     4 | loss: 3.1212184Losses:  3.6937341690063477 1.2774598598480225 0.8125364184379578
MemoryTrain:  epoch  7, batch     5 | loss: 3.6937342Losses:  3.2267935276031494 0.5128035545349121 0.963148295879364
MemoryTrain:  epoch  7, batch     6 | loss: 3.2267935Losses:  2.9104859828948975 0.33315974473953247 0.5229687690734863
MemoryTrain:  epoch  7, batch     7 | loss: 2.9104860Losses:  2.7239270210266113 0.26670077443122864 0.8457486033439636
MemoryTrain:  epoch  8, batch     0 | loss: 2.7239270Losses:  3.122424364089966 0.5424978137016296 0.8631111979484558
MemoryTrain:  epoch  8, batch     1 | loss: 3.1224244Losses:  3.3523337841033936 0.8493130207061768 0.9744728207588196
MemoryTrain:  epoch  8, batch     2 | loss: 3.3523338Losses:  2.2050747871398926 -0.0 0.7876797914505005
MemoryTrain:  epoch  8, batch     3 | loss: 2.2050748Losses:  3.0469870567321777 0.5562227964401245 0.9240260124206543
MemoryTrain:  epoch  8, batch     4 | loss: 3.0469871Losses:  2.678957939147949 0.2757814824581146 0.8725087642669678
MemoryTrain:  epoch  8, batch     5 | loss: 2.6789579Losses:  2.7828500270843506 0.5410321950912476 0.7284809947013855
MemoryTrain:  epoch  8, batch     6 | loss: 2.7828500Losses:  2.0863900184631348 -0.0 0.5977409482002258
MemoryTrain:  epoch  8, batch     7 | loss: 2.0863900Losses:  3.0153417587280273 0.7570770978927612 0.6896611452102661
MemoryTrain:  epoch  9, batch     0 | loss: 3.0153418Losses:  2.701655387878418 0.5371755361557007 0.7787647247314453
MemoryTrain:  epoch  9, batch     1 | loss: 2.7016554Losses:  2.7282142639160156 0.3064281940460205 0.8503242135047913
MemoryTrain:  epoch  9, batch     2 | loss: 2.7282143Losses:  2.3216919898986816 0.2431792914867401 0.639230489730835
MemoryTrain:  epoch  9, batch     3 | loss: 2.3216920Losses:  2.5649068355560303 0.24906578660011292 0.8593703508377075
MemoryTrain:  epoch  9, batch     4 | loss: 2.5649068Losses:  2.3856940269470215 0.24168409407138824 0.7809972763061523
MemoryTrain:  epoch  9, batch     5 | loss: 2.3856940Losses:  3.179473638534546 0.7629741430282593 0.9284045100212097
MemoryTrain:  epoch  9, batch     6 | loss: 3.1794736Losses:  2.610860824584961 -0.0 0.6770271062850952
MemoryTrain:  epoch  9, batch     7 | loss: 2.6108608
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 56.25%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 54.17%   [EVAL] batch:    3 | acc: 50.00%,  total acc: 53.12%   [EVAL] batch:    4 | acc: 31.25%,  total acc: 48.75%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 50.89%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 55.47%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 56.94%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 62.50%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 64.90%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 66.67%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 67.19%   [EVAL] batch:   16 | acc: 68.75%,  total acc: 67.28%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   19 | acc: 18.75%,  total acc: 66.25%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 66.07%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 64.49%   [EVAL] batch:   22 | acc: 43.75%,  total acc: 63.59%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 62.50%   [EVAL] batch:   24 | acc: 12.50%,  total acc: 60.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 62.02%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 63.43%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 64.73%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 65.95%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 67.08%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 72.22%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 73.36%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 73.08%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 72.66%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 72.41%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 72.62%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 72.82%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 72.73%   [EVAL] batch:   44 | acc: 50.00%,  total acc: 72.22%   [EVAL] batch:   45 | acc: 37.50%,  total acc: 71.47%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 70.48%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 70.70%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 70.41%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 70.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 70.59%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 71.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 71.46%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 71.64%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 72.05%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 71.99%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 71.71%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 71.55%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 70.87%   [EVAL] batch:   59 | acc: 43.75%,  total acc: 70.42%   [EVAL] batch:   60 | acc: 43.75%,  total acc: 69.98%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 69.35%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 68.95%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 78.47%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   10 | acc: 62.50%,  total acc: 76.70%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 77.40%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 78.57%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 79.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.86%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 81.62%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 82.29%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 83.22%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 83.63%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.52%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 82.88%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 83.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 84.82%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.13%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.09%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.52%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.93%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.32%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 87.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.18%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.49%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.78%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 89.18%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.43%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 89.39%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 89.49%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 89.44%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 89.40%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 89.10%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 88.80%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 88.90%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 89.00%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 89.09%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.18%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.27%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 89.47%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 89.43%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.51%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 89.25%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 88.04%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 87.29%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 87.19%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 86.78%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 86.09%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 85.52%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 84.96%   [EVAL] batch:   64 | acc: 37.50%,  total acc: 84.23%   [EVAL] batch:   65 | acc: 75.00%,  total acc: 84.09%   [EVAL] batch:   66 | acc: 43.75%,  total acc: 83.49%   [EVAL] batch:   67 | acc: 37.50%,  total acc: 82.81%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 82.61%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 82.77%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 82.83%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 82.73%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 82.96%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 83.19%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 83.42%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 82.81%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 82.14%   [EVAL] batch:   77 | acc: 37.50%,  total acc: 81.57%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 81.01%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 80.70%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 80.02%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 80.03%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 80.27%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 80.51%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 80.66%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 80.89%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 81.11%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 81.18%   [EVAL] batch:   88 | acc: 50.00%,  total acc: 80.83%   [EVAL] batch:   89 | acc: 62.50%,  total acc: 80.62%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 80.15%   [EVAL] batch:   91 | acc: 50.00%,  total acc: 79.82%   [EVAL] batch:   92 | acc: 25.00%,  total acc: 79.23%   [EVAL] batch:   93 | acc: 62.50%,  total acc: 79.06%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 79.14%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:   96 | acc: 87.50%,  total acc: 79.25%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 79.21%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 79.10%   [EVAL] batch:   99 | acc: 81.25%,  total acc: 79.12%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 79.33%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:  102 | acc: 87.50%,  total acc: 79.61%   [EVAL] batch:  103 | acc: 93.75%,  total acc: 79.75%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 79.94%   [EVAL] batch:  105 | acc: 100.00%,  total acc: 80.13%   [EVAL] batch:  106 | acc: 25.00%,  total acc: 79.61%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 78.99%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 78.44%   [EVAL] batch:  109 | acc: 6.25%,  total acc: 77.78%   [EVAL] batch:  110 | acc: 12.50%,  total acc: 77.20%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 76.73%   [EVAL] batch:  112 | acc: 18.75%,  total acc: 76.22%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 75.82%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 75.76%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 75.54%   [EVAL] batch:  116 | acc: 37.50%,  total acc: 75.21%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 75.00%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 75.05%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 75.21%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 75.36%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 75.41%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 75.36%   [EVAL] batch:  123 | acc: 75.00%,  total acc: 75.35%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 75.45%   [EVAL] batch:  125 | acc: 50.00%,  total acc: 75.25%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 74.71%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 74.52%   [EVAL] batch:  129 | acc: 31.25%,  total acc: 74.18%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 73.85%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 73.72%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 73.73%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 73.74%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 73.84%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 73.94%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 74.00%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 73.82%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 73.43%   [EVAL] batch:  139 | acc: 12.50%,  total acc: 72.99%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 72.52%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 72.18%   [EVAL] batch:  142 | acc: 18.75%,  total acc: 71.81%   [EVAL] batch:  143 | acc: 37.50%,  total acc: 71.57%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.96%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 72.11%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 72.30%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.48%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 72.58%   [EVAL] batch:  150 | acc: 100.00%,  total acc: 72.76%   [EVAL] batch:  151 | acc: 100.00%,  total acc: 72.94%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:  154 | acc: 100.00%,  total acc: 73.47%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 73.60%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 73.77%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 74.06%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 74.18%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 74.30%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 74.46%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 74.35%   [EVAL] batch:  163 | acc: 31.25%,  total acc: 74.09%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 73.71%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 73.42%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 73.24%   [EVAL] batch:  167 | acc: 31.25%,  total acc: 72.99%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 72.93%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 73.17%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 73.45%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 73.49%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 73.61%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 73.54%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 73.41%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 73.21%   [EVAL] batch:  178 | acc: 75.00%,  total acc: 73.22%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 73.19%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 72.96%   [EVAL] batch:  181 | acc: 62.50%,  total acc: 72.91%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 72.88%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 72.79%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 72.84%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:  186 | acc: 93.75%,  total acc: 73.03%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 72.94%   [EVAL] batch:  188 | acc: 62.50%,  total acc: 72.88%   [EVAL] batch:  189 | acc: 62.50%,  total acc: 72.83%   [EVAL] batch:  190 | acc: 50.00%,  total acc: 72.71%   [EVAL] batch:  191 | acc: 43.75%,  total acc: 72.56%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 72.38%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 72.23%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 72.34%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 72.35%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 72.30%   [EVAL] batch:  197 | acc: 87.50%,  total acc: 72.38%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 72.46%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 72.57%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 72.62%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 72.60%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 72.61%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 72.59%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 72.69%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 72.49%   [EVAL] batch:  207 | acc: 50.00%,  total acc: 72.39%   [EVAL] batch:  208 | acc: 56.25%,  total acc: 72.31%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 72.08%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 71.89%   [EVAL] batch:  211 | acc: 43.75%,  total acc: 71.76%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 71.65%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 71.79%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 71.92%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 72.05%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 72.18%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 72.31%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 72.40%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 72.53%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 72.99%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 73.11%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 73.09%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 73.05%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 73.00%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 72.95%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 73.04%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 73.00%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 72.95%   [EVAL] batch:  232 | acc: 43.75%,  total acc: 72.83%   [EVAL] batch:  233 | acc: 37.50%,  total acc: 72.68%   [EVAL] batch:  234 | acc: 50.00%,  total acc: 72.58%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 72.56%   [EVAL] batch:  236 | acc: 50.00%,  total acc: 72.47%   [EVAL] batch:  237 | acc: 81.25%,  total acc: 72.51%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 72.57%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 72.68%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 72.74%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 72.80%   [EVAL] batch:  242 | acc: 81.25%,  total acc: 72.84%   [EVAL] batch:  243 | acc: 75.00%,  total acc: 72.85%   [EVAL] batch:  244 | acc: 43.75%,  total acc: 72.73%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 72.64%   [EVAL] batch:  246 | acc: 31.25%,  total acc: 72.47%   [EVAL] batch:  247 | acc: 37.50%,  total acc: 72.33%   [EVAL] batch:  248 | acc: 43.75%,  total acc: 72.21%   [EVAL] batch:  249 | acc: 62.50%,  total acc: 72.17%   
cur_acc:  ['0.9484', '0.7411', '0.7976', '0.6895']
his_acc:  ['0.9484', '0.8300', '0.7912', '0.7218']
Clustering into  24  clusters
Clusters:  [ 1  3 20  0  8  8 17  2  3  3  4  5 23 18  0  3 22  1  2  4 15 10  6 11
 14 10  6  1  3  1 13  1 19  5 21 10 16  6  4  6  0  1  1  3 12  4  7  9
  2  3]
Losses:  10.107802391052246 4.187692165374756 0.5818160176277161
CurrentTrain: epoch  0, batch     0 | loss: 10.1078024Losses:  9.952502250671387 3.8681960105895996 0.6778479218482971
CurrentTrain: epoch  0, batch     1 | loss: 9.9525023Losses:  8.71833610534668 2.3211162090301514 0.7115310430526733
CurrentTrain: epoch  0, batch     2 | loss: 8.7183361Losses:  3.118863582611084 -0.0 0.10236251354217529
CurrentTrain: epoch  0, batch     3 | loss: 3.1188636Losses:  8.572906494140625 3.6074166297912598 0.6696215867996216
CurrentTrain: epoch  1, batch     0 | loss: 8.5729065Losses:  7.860272407531738 2.670509099960327 0.6862572431564331
CurrentTrain: epoch  1, batch     1 | loss: 7.8602724Losses:  7.938989639282227 3.2296862602233887 0.5591999292373657
CurrentTrain: epoch  1, batch     2 | loss: 7.9389896Losses:  3.7656631469726562 -0.0 0.10467921197414398
CurrentTrain: epoch  1, batch     3 | loss: 3.7656631Losses:  6.316823959350586 2.105776309967041 0.6030719876289368
CurrentTrain: epoch  2, batch     0 | loss: 6.3168240Losses:  8.914329528808594 4.4611496925354 0.6740092039108276
CurrentTrain: epoch  2, batch     1 | loss: 8.9143295Losses:  7.173150539398193 3.189136505126953 0.5953632593154907
CurrentTrain: epoch  2, batch     2 | loss: 7.1731505Losses:  2.2445321083068848 -0.0 0.11806623637676239
CurrentTrain: epoch  2, batch     3 | loss: 2.2445321Losses:  7.8475446701049805 4.3874993324279785 0.5122091770172119
CurrentTrain: epoch  3, batch     0 | loss: 7.8475447Losses:  7.035345077514648 3.353198528289795 0.5764257907867432
CurrentTrain: epoch  3, batch     1 | loss: 7.0353451Losses:  6.8719658851623535 2.9016194343566895 0.597502589225769
CurrentTrain: epoch  3, batch     2 | loss: 6.8719659Losses:  1.8681683540344238 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 1.8681684Losses:  6.178008556365967 2.4410197734832764 0.663068413734436
CurrentTrain: epoch  4, batch     0 | loss: 6.1780086Losses:  6.294332027435303 2.498384952545166 0.5841649174690247
CurrentTrain: epoch  4, batch     1 | loss: 6.2943320Losses:  6.79070520401001 3.4244768619537354 0.5774941444396973
CurrentTrain: epoch  4, batch     2 | loss: 6.7907052Losses:  2.0885562896728516 -0.0 0.1404038965702057
CurrentTrain: epoch  4, batch     3 | loss: 2.0885563Losses:  7.375114440917969 3.514190912246704 0.6614978313446045
CurrentTrain: epoch  5, batch     0 | loss: 7.3751144Losses:  6.039129257202148 2.1956591606140137 0.643215537071228
CurrentTrain: epoch  5, batch     1 | loss: 6.0391293Losses:  5.279178142547607 2.5129854679107666 0.6536220908164978
CurrentTrain: epoch  5, batch     2 | loss: 5.2791781Losses:  2.0809412002563477 -0.0 0.11457891017198563
CurrentTrain: epoch  5, batch     3 | loss: 2.0809412Losses:  6.364471435546875 3.059744119644165 0.5711718201637268
CurrentTrain: epoch  6, batch     0 | loss: 6.3644714Losses:  6.648733139038086 3.7666373252868652 0.5767906904220581
CurrentTrain: epoch  6, batch     1 | loss: 6.6487331Losses:  4.965507507324219 2.1775059700012207 0.5578665733337402
CurrentTrain: epoch  6, batch     2 | loss: 4.9655075Losses:  2.382668972015381 -0.0 0.1100202202796936
CurrentTrain: epoch  6, batch     3 | loss: 2.3826690Losses:  5.260855197906494 2.2985520362854004 0.5678753852844238
CurrentTrain: epoch  7, batch     0 | loss: 5.2608552Losses:  5.896483421325684 3.2760725021362305 0.5601169466972351
CurrentTrain: epoch  7, batch     1 | loss: 5.8964834Losses:  5.616505146026611 2.8396849632263184 0.4876236319541931
CurrentTrain: epoch  7, batch     2 | loss: 5.6165051Losses:  2.0092239379882812 -0.0 0.1365422010421753
CurrentTrain: epoch  7, batch     3 | loss: 2.0092239Losses:  5.02099609375 2.266019821166992 0.6422948241233826
CurrentTrain: epoch  8, batch     0 | loss: 5.0209961Losses:  4.486419677734375 1.8622568845748901 0.6267023086547852
CurrentTrain: epoch  8, batch     1 | loss: 4.4864197Losses:  5.331519603729248 2.610215663909912 0.5532523989677429
CurrentTrain: epoch  8, batch     2 | loss: 5.3315196Losses:  2.492974042892456 -0.0 0.18867653608322144
CurrentTrain: epoch  8, batch     3 | loss: 2.4929740Losses:  4.278773784637451 1.7892885208129883 0.6227297782897949
CurrentTrain: epoch  9, batch     0 | loss: 4.2787738Losses:  5.534409999847412 3.105795383453369 0.48821794986724854
CurrentTrain: epoch  9, batch     1 | loss: 5.5344100Losses:  5.925492286682129 3.2334275245666504 0.6391673684120178
CurrentTrain: epoch  9, batch     2 | loss: 5.9254923Losses:  1.9408643245697021 -0.0 0.1329265832901001
CurrentTrain: epoch  9, batch     3 | loss: 1.9408643
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: barack obama served as a member of the democratic party during his presidency from 2009 to 2017.  
Head Entity: barack obama  
Tail Entity: democratic party  

Relation: member of political party  
Context: angela merkel was a prominent member of the christian democratic union throughout her political career in germany.  
Head Entity: angela merkel  
Tail Entity: christian democratic union  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: the script for " the great santini " was adapted by carlino from the 1976 novel by pat conroy , with assistance from an un - credited herman raucher .
Head Entity: the great santini
Tail Entity: pat conroy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film "inception" was heavily influenced by the concepts presented in the 2001 novel "the dreamers" by j. k. rowling, which explores the nature of dreams and reality.  
Head Entity: inception  
Tail Entity: j. k. rowling  

Relation: after a work by  
Context: the musical "hamilton" draws inspiration from the biography "alexander hamilton" written by ron chernow, which details the life of the founding father.  
Head Entity: hamilton  
Tail Entity: ron chernow  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: another was " the death of nelson " by daniel maclise , a large wall painting in the royal gallery of the palace of westminster .
Head Entity: daniel maclise
Tail Entity: the death of nelson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: one of his most famous pieces is "the persistence of memory," a surreal painting that features melting clocks and explores the concept of time.  
Head Entity: salvador dalí  
Tail Entity: the persistence of memory  

Relation: notable work  
Context: her acclaimed novel "to kill a mockingbird" addresses serious issues of race and injustice in the American South through the eyes of a young girl.  
Head Entity: harper lee  
Tail Entity: to kill a mockingbird  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after years of expansion, the non-profit organization moved its headquarters to new york city to better serve its growing membership.  
Head Entity: non-profit organization  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which is classified under the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of flowering plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics and chemistry.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: wjet erie , pennsylvania was his first official radio job outside of duties performed in the u.s. air force .
Head Entity: wjet
Tail Entity: erie , pennsylvania
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: wxyz television is the primary news station serving the city of springfield, known for its local coverage.  
Head Entity: wxyz  
Tail Entity: springfield  

Relation: licensed to broadcast to  
Context: kqed is a public television station that provides educational programming to the residents of san francisco.  
Head Entity: kqed  
Tail Entity: san francisco  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 473 is a lenticular galaxy in the constellation of pisces .
Head Entity: ngc 473
Tail Entity: pisces
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the andromeda galaxy is located in the constellation of andromeda .  
Head Entity: andromeda galaxy  
Tail Entity: andromeda  

Relation: constellation  
Context: the orion nebula is situated in the constellation of orion .  
Head Entity: orion nebula  
Tail Entity: orion  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Losses:  4.981114387512207 1.0579087734222412 0.7557404041290283
MemoryTrain:  epoch  0, batch     0 | loss: 4.9811144Losses:  4.876999855041504 0.6100007891654968 0.7995583415031433
MemoryTrain:  epoch  0, batch     1 | loss: 4.8769999Losses:  3.645463228225708 0.8156921863555908 0.8068813681602478
MemoryTrain:  epoch  0, batch     2 | loss: 3.6454632Losses:  3.3483121395111084 -0.0 0.8307002186775208
MemoryTrain:  epoch  0, batch     3 | loss: 3.3483121Losses:  3.2173521518707275 -0.0 0.7561218738555908
MemoryTrain:  epoch  0, batch     4 | loss: 3.2173522Losses:  4.25706148147583 0.5680335164070129 1.0005964040756226
MemoryTrain:  epoch  0, batch     5 | loss: 4.2570615Losses:  3.986088275909424 0.253218412399292 0.9526892900466919
MemoryTrain:  epoch  0, batch     6 | loss: 3.9860883Losses:  3.9762749671936035 1.160212516784668 0.7218805551528931
MemoryTrain:  epoch  0, batch     7 | loss: 3.9762750Losses:  3.814596176147461 0.2952934205532074 0.9507001638412476
MemoryTrain:  epoch  0, batch     8 | loss: 3.8145962Losses:  3.6422200202941895 -0.0 0.4321676194667816
MemoryTrain:  epoch  0, batch     9 | loss: 3.6422200Losses:  3.7312729358673096 0.7954468727111816 0.7587311267852783
MemoryTrain:  epoch  1, batch     0 | loss: 3.7312729Losses:  3.8130922317504883 0.2710103988647461 0.7465829849243164
MemoryTrain:  epoch  1, batch     1 | loss: 3.8130922Losses:  3.7973835468292236 -0.0 0.9318744540214539
MemoryTrain:  epoch  1, batch     2 | loss: 3.7973835Losses:  3.9773261547088623 0.80465167760849 0.9176070094108582
MemoryTrain:  epoch  1, batch     3 | loss: 3.9773262Losses:  2.9488720893859863 0.5142948627471924 0.8696847558021545
MemoryTrain:  epoch  1, batch     4 | loss: 2.9488721Losses:  3.809399127960205 0.5473141074180603 0.9786883592605591
MemoryTrain:  epoch  1, batch     5 | loss: 3.8093991Losses:  3.243075132369995 0.2519266903400421 0.8142881393432617
MemoryTrain:  epoch  1, batch     6 | loss: 3.2430751Losses:  4.690145492553711 0.6773706674575806 0.8403756618499756
MemoryTrain:  epoch  1, batch     7 | loss: 4.6901455Losses:  3.1944122314453125 0.2565961182117462 1.0020556449890137
MemoryTrain:  epoch  1, batch     8 | loss: 3.1944122Losses:  2.274736166000366 -0.0 0.44664308428764343
MemoryTrain:  epoch  1, batch     9 | loss: 2.2747362Losses:  2.514082908630371 0.5002105236053467 0.680833101272583
MemoryTrain:  epoch  2, batch     0 | loss: 2.5140829Losses:  3.6109414100646973 0.21513068675994873 0.9017399549484253
MemoryTrain:  epoch  2, batch     1 | loss: 3.6109414Losses:  2.977177619934082 -0.0 0.8115262389183044
MemoryTrain:  epoch  2, batch     2 | loss: 2.9771776Losses:  3.785428762435913 1.3851261138916016 0.8760239481925964
MemoryTrain:  epoch  2, batch     3 | loss: 3.7854288Losses:  4.431561470031738 0.5413520932197571 0.8560794591903687
MemoryTrain:  epoch  2, batch     4 | loss: 4.4315615Losses:  2.3424465656280518 -0.0 0.8030800819396973
MemoryTrain:  epoch  2, batch     5 | loss: 2.3424466Losses:  3.9674668312072754 0.6521949768066406 0.7487629055976868
MemoryTrain:  epoch  2, batch     6 | loss: 3.9674668Losses:  3.76430606842041 0.7982627153396606 0.8482298851013184
MemoryTrain:  epoch  2, batch     7 | loss: 3.7643061Losses:  2.786832809448242 0.503637433052063 0.8685031533241272
MemoryTrain:  epoch  2, batch     8 | loss: 2.7868328Losses:  1.7398836612701416 -0.0 0.39349305629730225
MemoryTrain:  epoch  2, batch     9 | loss: 1.7398837Losses:  3.4799816608428955 0.7713744640350342 0.9148104190826416
MemoryTrain:  epoch  3, batch     0 | loss: 3.4799817Losses:  2.85876202583313 0.5302305221557617 0.7728204727172852
MemoryTrain:  epoch  3, batch     1 | loss: 2.8587620Losses:  2.845029592514038 0.4895346760749817 0.8441558480262756
MemoryTrain:  epoch  3, batch     2 | loss: 2.8450296Losses:  3.8825392723083496 0.49320945143699646 0.8175873160362244
MemoryTrain:  epoch  3, batch     3 | loss: 3.8825393Losses:  3.8626813888549805 1.7795535326004028 0.7328678369522095
MemoryTrain:  epoch  3, batch     4 | loss: 3.8626814Losses:  3.3812856674194336 1.024084448814392 0.7017544507980347
MemoryTrain:  epoch  3, batch     5 | loss: 3.3812857Losses:  2.9790096282958984 0.47777384519577026 0.8521522283554077
MemoryTrain:  epoch  3, batch     6 | loss: 2.9790096Losses:  2.872846841812134 0.25051069259643555 0.9641934633255005
MemoryTrain:  epoch  3, batch     7 | loss: 2.8728468Losses:  3.925145387649536 0.8323072195053101 0.6501056551933289
MemoryTrain:  epoch  3, batch     8 | loss: 3.9251454Losses:  1.89115309715271 -0.0 0.4256196618080139
MemoryTrain:  epoch  3, batch     9 | loss: 1.8911531Losses:  3.2038960456848145 -0.0 0.8001975417137146
MemoryTrain:  epoch  4, batch     0 | loss: 3.2038960Losses:  2.7733898162841797 0.23229646682739258 0.8710519671440125
MemoryTrain:  epoch  4, batch     1 | loss: 2.7733898Losses:  3.4708046913146973 1.1470285654067993 0.8589252233505249
MemoryTrain:  epoch  4, batch     2 | loss: 3.4708047Losses:  2.8010852336883545 0.20918282866477966 0.9313892722129822
MemoryTrain:  epoch  4, batch     3 | loss: 2.8010852Losses:  2.507953643798828 0.24948899447917938 1.0181398391723633
MemoryTrain:  epoch  4, batch     4 | loss: 2.5079536Losses:  2.3903117179870605 -0.0 0.8628458976745605
MemoryTrain:  epoch  4, batch     5 | loss: 2.3903117Losses:  3.048405647277832 0.5113846659660339 0.918911337852478
MemoryTrain:  epoch  4, batch     6 | loss: 3.0484056Losses:  2.7479257583618164 0.23015397787094116 0.971102774143219
MemoryTrain:  epoch  4, batch     7 | loss: 2.7479258Losses:  2.560171365737915 0.29324501752853394 0.7840933203697205
MemoryTrain:  epoch  4, batch     8 | loss: 2.5601714Losses:  1.9378118515014648 -0.0 0.4296485483646393
MemoryTrain:  epoch  4, batch     9 | loss: 1.9378119Losses:  2.581156015396118 0.27417224645614624 0.8583611845970154
MemoryTrain:  epoch  5, batch     0 | loss: 2.5811560Losses:  3.6680078506469727 1.11594820022583 0.9271939992904663
MemoryTrain:  epoch  5, batch     1 | loss: 3.6680079Losses:  2.938404083251953 -0.0 1.0725805759429932
MemoryTrain:  epoch  5, batch     2 | loss: 2.9384041Losses:  2.547065019607544 -0.0 0.9216097593307495
MemoryTrain:  epoch  5, batch     3 | loss: 2.5470650Losses:  2.793245553970337 0.21211567521095276 0.7874178290367126
MemoryTrain:  epoch  5, batch     4 | loss: 2.7932456Losses:  2.452150344848633 -0.0 0.8019930124282837
MemoryTrain:  epoch  5, batch     5 | loss: 2.4521503Losses:  2.971433639526367 0.518973708152771 0.8092765212059021
MemoryTrain:  epoch  5, batch     6 | loss: 2.9714336Losses:  2.5638415813446045 0.4741462469100952 0.820727527141571
MemoryTrain:  epoch  5, batch     7 | loss: 2.5638416Losses:  2.883451461791992 0.6950522065162659 0.8534492254257202
MemoryTrain:  epoch  5, batch     8 | loss: 2.8834515Losses:  1.6667592525482178 -0.0 0.32540440559387207
MemoryTrain:  epoch  5, batch     9 | loss: 1.6667593Losses:  2.63179874420166 0.2726479172706604 0.883661150932312
MemoryTrain:  epoch  6, batch     0 | loss: 2.6317987Losses:  2.3077151775360107 -0.0 0.9830560088157654
MemoryTrain:  epoch  6, batch     1 | loss: 2.3077152Losses:  2.7845749855041504 0.4876713156700134 0.770025372505188
MemoryTrain:  epoch  6, batch     2 | loss: 2.7845750Losses:  2.7397749423980713 0.5447155833244324 0.7878155708312988
MemoryTrain:  epoch  6, batch     3 | loss: 2.7397749Losses:  2.6505565643310547 0.25817790627479553 0.866783082485199
MemoryTrain:  epoch  6, batch     4 | loss: 2.6505566Losses:  2.808462619781494 0.27008312940597534 0.8374127745628357
MemoryTrain:  epoch  6, batch     5 | loss: 2.8084626Losses:  2.6581029891967773 0.49290284514427185 0.7849788665771484
MemoryTrain:  epoch  6, batch     6 | loss: 2.6581030Losses:  2.991270065307617 0.7765356302261353 0.8882346749305725
MemoryTrain:  epoch  6, batch     7 | loss: 2.9912701Losses:  2.6617255210876465 0.25899362564086914 0.8249201774597168
MemoryTrain:  epoch  6, batch     8 | loss: 2.6617255Losses:  2.977412223815918 -0.0 0.3301619291305542
MemoryTrain:  epoch  6, batch     9 | loss: 2.9774122Losses:  2.222832202911377 -0.0 0.839881420135498
MemoryTrain:  epoch  7, batch     0 | loss: 2.2228322Losses:  2.500336170196533 0.22160130739212036 0.777898907661438
MemoryTrain:  epoch  7, batch     1 | loss: 2.5003362Losses:  2.513676166534424 0.25819072127342224 0.8473571538925171
MemoryTrain:  epoch  7, batch     2 | loss: 2.5136762Losses:  2.963940382003784 0.5603992938995361 0.9491415023803711
MemoryTrain:  epoch  7, batch     3 | loss: 2.9639404Losses:  2.3509089946746826 0.23138175904750824 0.6995576024055481
MemoryTrain:  epoch  7, batch     4 | loss: 2.3509090Losses:  2.636343240737915 0.25423333048820496 0.883425235748291
MemoryTrain:  epoch  7, batch     5 | loss: 2.6363432Losses:  2.8566575050354004 0.5169486999511719 0.9563828706741333
MemoryTrain:  epoch  7, batch     6 | loss: 2.8566575Losses:  2.4468605518341064 -0.0 0.8449828028678894
MemoryTrain:  epoch  7, batch     7 | loss: 2.4468606Losses:  2.5307071208953857 0.2571865916252136 0.9660184383392334
MemoryTrain:  epoch  7, batch     8 | loss: 2.5307071Losses:  2.1329379081726074 0.3330305814743042 0.4857341945171356
MemoryTrain:  epoch  7, batch     9 | loss: 2.1329379Losses:  2.399306058883667 0.2446283996105194 0.7246938943862915
MemoryTrain:  epoch  8, batch     0 | loss: 2.3993061Losses:  2.4515745639801025 -0.0 1.0013501644134521
MemoryTrain:  epoch  8, batch     1 | loss: 2.4515746Losses:  2.752878427505493 0.5797642469406128 0.8856345415115356
MemoryTrain:  epoch  8, batch     2 | loss: 2.7528784Losses:  2.850595474243164 0.5252045392990112 1.0394039154052734
MemoryTrain:  epoch  8, batch     3 | loss: 2.8505955Losses:  2.5183091163635254 0.2887369990348816 0.9141048192977905
MemoryTrain:  epoch  8, batch     4 | loss: 2.5183091Losses:  2.618842124938965 0.532050371170044 0.7807722091674805
MemoryTrain:  epoch  8, batch     5 | loss: 2.6188421Losses:  2.4610395431518555 0.506719708442688 0.7104434370994568
MemoryTrain:  epoch  8, batch     6 | loss: 2.4610395Losses:  2.5569190979003906 0.2674676775932312 0.9561204314231873
MemoryTrain:  epoch  8, batch     7 | loss: 2.5569191Losses:  2.4340882301330566 0.247187539935112 0.7949262857437134
MemoryTrain:  epoch  8, batch     8 | loss: 2.4340882Losses:  1.7765603065490723 -0.0 0.4452224671840668
MemoryTrain:  epoch  8, batch     9 | loss: 1.7765603Losses:  2.355602264404297 0.24146780371665955 0.8545794486999512
MemoryTrain:  epoch  9, batch     0 | loss: 2.3556023Losses:  2.2765045166015625 -0.0 0.9057589769363403
MemoryTrain:  epoch  9, batch     1 | loss: 2.2765045Losses:  2.5674140453338623 0.26323556900024414 0.9564470648765564
MemoryTrain:  epoch  9, batch     2 | loss: 2.5674140Losses:  3.2446584701538086 1.220276117324829 0.7201374769210815
MemoryTrain:  epoch  9, batch     3 | loss: 3.2446585Losses:  2.551438331604004 0.5212693214416504 0.7423186302185059
MemoryTrain:  epoch  9, batch     4 | loss: 2.5514383Losses:  2.622638702392578 0.4578034281730652 0.9026695489883423
MemoryTrain:  epoch  9, batch     5 | loss: 2.6226387Losses:  2.771350622177124 0.518423318862915 0.9136096835136414
MemoryTrain:  epoch  9, batch     6 | loss: 2.7713506Losses:  2.3647403717041016 0.2270444631576538 0.8328525424003601
MemoryTrain:  epoch  9, batch     7 | loss: 2.3647404Losses:  2.316154956817627 0.2388210892677307 0.8573607206344604
MemoryTrain:  epoch  9, batch     8 | loss: 2.3161550Losses:  1.6989145278930664 -0.0 0.21612855792045593
MemoryTrain:  epoch  9, batch     9 | loss: 1.6989145
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 93.75%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 88.54%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 85.16%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 84.03%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 84.66%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 84.82%   [EVAL] batch:   14 | acc: 81.25%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 82.99%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 79.69%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 77.38%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 18.75%,  total acc: 73.10%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 72.14%   [EVAL] batch:   24 | acc: 37.50%,  total acc: 70.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 72.92%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 73.88%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 74.78%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 75.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 76.37%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 76.89%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 76.47%   [EVAL] batch:   34 | acc: 81.25%,  total acc: 76.61%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 76.39%   [EVAL] batch:   36 | acc: 68.75%,  total acc: 76.18%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 75.99%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 76.60%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.19%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 77.74%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 78.27%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 78.78%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.12%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 79.44%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 79.89%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 80.32%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 80.73%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.12%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 81.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 81.37%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 81.49%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 81.72%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 81.93%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 82.14%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 82.35%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 82.22%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 82.42%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 82.71%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 82.99%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 83.17%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 82.54%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 77.68%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 74.22%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 72.22%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 68.75%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.83%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 77.30%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.81%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 78.27%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 77.45%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.86%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 78.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.85%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.63%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 80.13%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.60%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.85%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.42%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.95%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 83.46%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 84.20%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.63%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 85.03%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.42%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 85.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 86.13%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.63%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 86.79%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 86.53%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 86.28%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 86.04%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 85.68%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 85.59%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 85.62%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 85.78%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 85.82%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.97%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 85.88%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 85.68%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 85.71%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 85.53%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 84.27%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 83.58%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 82.89%   [EVAL] batch:   61 | acc: 43.75%,  total acc: 82.26%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 81.55%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 80.86%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 79.81%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 79.64%   [EVAL] batch:   66 | acc: 37.50%,  total acc: 79.01%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 78.31%   [EVAL] batch:   68 | acc: 75.00%,  total acc: 78.26%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 78.48%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 78.61%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 78.65%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 79.22%   [EVAL] batch:   74 | acc: 100.00%,  total acc: 79.50%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 78.95%   [EVAL] batch:   76 | acc: 31.25%,  total acc: 78.33%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 77.72%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 77.29%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 77.03%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 76.39%   [EVAL] batch:   81 | acc: 81.25%,  total acc: 76.45%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 77.01%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 77.21%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 77.73%   [EVAL] batch:   87 | acc: 81.25%,  total acc: 77.77%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 77.18%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 76.60%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 76.03%   [EVAL] batch:   91 | acc: 56.25%,  total acc: 75.82%   [EVAL] batch:   92 | acc: 18.75%,  total acc: 75.20%   [EVAL] batch:   93 | acc: 75.00%,  total acc: 75.20%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 75.33%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 75.39%   [EVAL] batch:   96 | acc: 75.00%,  total acc: 75.39%   [EVAL] batch:   97 | acc: 75.00%,  total acc: 75.38%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 75.32%   [EVAL] batch:   99 | acc: 75.00%,  total acc: 75.31%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 75.12%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 75.06%   [EVAL] batch:  102 | acc: 37.50%,  total acc: 74.70%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 74.58%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 74.52%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 74.59%   [EVAL] batch:  106 | acc: 18.75%,  total acc: 74.07%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 73.50%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 72.99%   [EVAL] batch:  109 | acc: 12.50%,  total acc: 72.44%   [EVAL] batch:  110 | acc: 18.75%,  total acc: 71.96%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 71.54%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 71.13%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 70.78%   [EVAL] batch:  114 | acc: 75.00%,  total acc: 70.82%   [EVAL] batch:  115 | acc: 50.00%,  total acc: 70.64%   [EVAL] batch:  116 | acc: 37.50%,  total acc: 70.35%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 70.18%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 70.27%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 70.47%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 70.66%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 70.75%   [EVAL] batch:  122 | acc: 68.75%,  total acc: 70.73%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 70.82%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 70.95%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 70.59%   [EVAL] batch:  126 | acc: 25.00%,  total acc: 70.23%   [EVAL] batch:  127 | acc: 6.25%,  total acc: 69.73%   [EVAL] batch:  128 | acc: 25.00%,  total acc: 69.38%   [EVAL] batch:  129 | acc: 18.75%,  total acc: 68.99%   [EVAL] batch:  130 | acc: 12.50%,  total acc: 68.56%   [EVAL] batch:  131 | acc: 56.25%,  total acc: 68.47%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 68.61%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 68.98%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 69.07%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 69.25%   [EVAL] batch:  137 | acc: 56.25%,  total acc: 69.16%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 68.88%   [EVAL] batch:  139 | acc: 62.50%,  total acc: 68.84%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 68.44%   [EVAL] batch:  141 | acc: 37.50%,  total acc: 68.22%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 68.01%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 67.88%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 68.32%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 68.49%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 69.04%   [EVAL] batch:  150 | acc: 100.00%,  total acc: 69.25%   [EVAL] batch:  151 | acc: 100.00%,  total acc: 69.45%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 69.85%   [EVAL] batch:  154 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 70.38%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 70.72%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.86%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 71.00%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 71.14%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 71.05%   [EVAL] batch:  163 | acc: 43.75%,  total acc: 70.88%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 70.57%   [EVAL] batch:  165 | acc: 25.00%,  total acc: 70.29%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 70.13%   [EVAL] batch:  167 | acc: 25.00%,  total acc: 69.87%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 69.82%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 70.00%   [EVAL] batch:  170 | acc: 87.50%,  total acc: 70.10%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  172 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:  173 | acc: 93.75%,  total acc: 70.55%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:  175 | acc: 62.50%,  total acc: 70.60%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 70.52%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 70.37%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 70.32%   [EVAL] batch:  179 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:  180 | acc: 37.50%,  total acc: 70.13%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 70.02%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 69.98%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 69.84%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 69.86%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 69.89%   [EVAL] batch:  186 | acc: 87.50%,  total acc: 69.99%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 69.95%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 69.97%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 70.00%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 70.06%   [EVAL] batch:  191 | acc: 68.75%,  total acc: 70.05%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 70.05%   [EVAL] batch:  193 | acc: 68.75%,  total acc: 70.04%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 70.13%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 70.12%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 70.15%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 70.20%   [EVAL] batch:  198 | acc: 87.50%,  total acc: 70.29%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 70.34%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 70.43%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 70.48%   [EVAL] batch:  202 | acc: 81.25%,  total acc: 70.54%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 70.53%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 70.52%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 70.60%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 70.41%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 70.19%   [EVAL] batch:  208 | acc: 37.50%,  total acc: 70.04%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 69.82%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 69.64%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 69.49%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 69.40%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 69.54%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 69.68%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 69.82%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 69.96%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 70.10%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 70.61%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 70.87%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 71.00%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 70.99%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 70.95%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 70.92%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 70.91%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 71.01%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 70.97%   [EVAL] batch:  231 | acc: 43.75%,  total acc: 70.85%   [EVAL] batch:  232 | acc: 25.00%,  total acc: 70.65%   [EVAL] batch:  233 | acc: 25.00%,  total acc: 70.46%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 70.27%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 70.21%   [EVAL] batch:  236 | acc: 31.25%,  total acc: 70.04%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 69.98%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 70.06%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 70.18%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 70.23%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 70.30%   [EVAL] batch:  242 | acc: 93.75%,  total acc: 70.40%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 70.44%   [EVAL] batch:  244 | acc: 50.00%,  total acc: 70.36%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 70.27%   [EVAL] batch:  246 | acc: 31.25%,  total acc: 70.12%   [EVAL] batch:  247 | acc: 50.00%,  total acc: 70.04%   [EVAL] batch:  248 | acc: 43.75%,  total acc: 69.93%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 69.83%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  251 | acc: 75.00%,  total acc: 69.94%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 70.01%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 70.05%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 70.15%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 70.26%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 70.28%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 70.30%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 70.32%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 70.38%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 70.45%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 70.52%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 70.58%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 70.62%   [EVAL] batch:  264 | acc: 81.25%,  total acc: 70.66%   [EVAL] batch:  265 | acc: 81.25%,  total acc: 70.70%   [EVAL] batch:  266 | acc: 75.00%,  total acc: 70.72%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 70.71%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 70.70%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 70.56%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 70.41%   [EVAL] batch:  271 | acc: 37.50%,  total acc: 70.29%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 70.10%   [EVAL] batch:  273 | acc: 50.00%,  total acc: 70.03%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 69.91%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 70.02%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 70.13%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 70.45%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 70.55%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 70.57%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 70.65%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 70.62%   [EVAL] batch:  284 | acc: 81.25%,  total acc: 70.66%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 70.65%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 70.64%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 70.64%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 70.74%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 70.84%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 70.94%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 71.04%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 71.14%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 71.22%   [EVAL] batch:  294 | acc: 93.75%,  total acc: 71.29%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 71.39%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 71.58%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:  300 | acc: 75.00%,  total acc: 71.78%   [EVAL] batch:  301 | acc: 87.50%,  total acc: 71.83%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 71.91%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 71.98%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 72.01%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 72.08%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 72.15%   [EVAL] batch:  307 | acc: 75.00%,  total acc: 72.16%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 72.23%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 72.32%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 72.41%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 72.48%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 72.38%   
cur_acc:  ['0.9484', '0.7411', '0.7976', '0.6895', '0.8254']
his_acc:  ['0.9484', '0.8300', '0.7912', '0.7218', '0.7238']
Clustering into  29  clusters
Clusters:  [ 5  3 17  4 11 11 20  1  3 23  0 22 21 18  6 11 24  5 28  0 15  7  2 25
 26  7  2  5 23  5 16  5 14 27  8  7 19  2  0  2  4 24 24  3  9  0 13 10
  1  3  2 12 12 11  6 23  5 11  7  7]
Losses:  9.81676959991455 4.0933918952941895 0.5380560159683228
CurrentTrain: epoch  0, batch     0 | loss: 9.8167696Losses:  12.316315650939941 5.434673309326172 0.4582327902317047
CurrentTrain: epoch  0, batch     1 | loss: 12.3163157Losses:  9.991390228271484 3.6397852897644043 0.5788196325302124
CurrentTrain: epoch  0, batch     2 | loss: 9.9913902Losses:  5.414308071136475 -0.0 0.16751641035079956
CurrentTrain: epoch  0, batch     3 | loss: 5.4143081Losses:  8.443341255187988 3.11214542388916 0.517608106136322
CurrentTrain: epoch  1, batch     0 | loss: 8.4433413Losses:  9.255741119384766 3.601274251937866 0.5067528486251831
CurrentTrain: epoch  1, batch     1 | loss: 9.2557411Losses:  7.39572286605835 2.3321173191070557 0.577837347984314
CurrentTrain: epoch  1, batch     2 | loss: 7.3957229Losses:  2.5492217540740967 -0.0 0.12038736045360565
CurrentTrain: epoch  1, batch     3 | loss: 2.5492218Losses:  8.186566352844238 3.171959400177002 0.5853412747383118
CurrentTrain: epoch  2, batch     0 | loss: 8.1865664Losses:  8.548714637756348 3.9362423419952393 0.5960001349449158
CurrentTrain: epoch  2, batch     1 | loss: 8.5487146Losses:  7.28190803527832 2.8631606101989746 0.4921852946281433
CurrentTrain: epoch  2, batch     2 | loss: 7.2819080Losses:  3.6058952808380127 -0.0 0.12350905686616898
CurrentTrain: epoch  2, batch     3 | loss: 3.6058953Losses:  7.1692795753479 3.1568708419799805 0.5702496767044067
CurrentTrain: epoch  3, batch     0 | loss: 7.1692796Losses:  6.044688701629639 2.365068197250366 0.5018056631088257
CurrentTrain: epoch  3, batch     1 | loss: 6.0446887Losses:  7.557579517364502 3.4368550777435303 0.5691437125205994
CurrentTrain: epoch  3, batch     2 | loss: 7.5575795Losses:  3.4537577629089355 -0.0 0.1111675500869751
CurrentTrain: epoch  3, batch     3 | loss: 3.4537578Losses:  5.210606098175049 1.6186714172363281 0.5713527798652649
CurrentTrain: epoch  4, batch     0 | loss: 5.2106061Losses:  5.894837856292725 2.2989656925201416 0.5476365089416504
CurrentTrain: epoch  4, batch     1 | loss: 5.8948379Losses:  7.624580383300781 3.8397395610809326 0.559766948223114
CurrentTrain: epoch  4, batch     2 | loss: 7.6245804Losses:  1.9581139087677002 -0.0 0.08457855135202408
CurrentTrain: epoch  4, batch     3 | loss: 1.9581139Losses:  5.4618425369262695 2.0042495727539062 0.5739004611968994
CurrentTrain: epoch  5, batch     0 | loss: 5.4618425Losses:  5.070652008056641 2.2350564002990723 0.5441770553588867
CurrentTrain: epoch  5, batch     1 | loss: 5.0706520Losses:  7.478752136230469 3.7597639560699463 0.47178539633750916
CurrentTrain: epoch  5, batch     2 | loss: 7.4787521Losses:  2.3325092792510986 -0.0 0.11832764744758606
CurrentTrain: epoch  5, batch     3 | loss: 2.3325093Losses:  6.540085792541504 3.55049991607666 0.5629273653030396
CurrentTrain: epoch  6, batch     0 | loss: 6.5400858Losses:  5.97692346572876 2.626627206802368 0.5406435132026672
CurrentTrain: epoch  6, batch     1 | loss: 5.9769235Losses:  5.9423651695251465 2.9287214279174805 0.4688758850097656
CurrentTrain: epoch  6, batch     2 | loss: 5.9423652Losses:  2.286818504333496 -0.0 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 2.2868185Losses:  5.1597514152526855 2.3534116744995117 0.5464657545089722
CurrentTrain: epoch  7, batch     0 | loss: 5.1597514Losses:  7.033420562744141 3.63869309425354 0.5476062297821045
CurrentTrain: epoch  7, batch     1 | loss: 7.0334206Losses:  5.104487419128418 2.488110303878784 0.5300817489624023
CurrentTrain: epoch  7, batch     2 | loss: 5.1044874Losses:  1.7882981300354004 -0.0 0.10361635684967041
CurrentTrain: epoch  7, batch     3 | loss: 1.7882981Losses:  5.815339088439941 3.157172203063965 0.5465540289878845
CurrentTrain: epoch  8, batch     0 | loss: 5.8153391Losses:  4.185733318328857 1.4980871677398682 0.5272716283798218
CurrentTrain: epoch  8, batch     1 | loss: 4.1857333Losses:  6.545945644378662 2.8556950092315674 0.5382909178733826
CurrentTrain: epoch  8, batch     2 | loss: 6.5459456Losses:  1.9472076892852783 -0.0 0.0894758552312851
CurrentTrain: epoch  8, batch     3 | loss: 1.9472077Losses:  6.589179515838623 3.3618061542510986 0.4739157557487488
CurrentTrain: epoch  9, batch     0 | loss: 6.5891795Losses:  5.0260820388793945 2.401592969894409 0.527983546257019
CurrentTrain: epoch  9, batch     1 | loss: 5.0260820Losses:  5.049152851104736 2.8572304248809814 0.3680594563484192
CurrentTrain: epoch  9, batch     2 | loss: 5.0491529Losses:  3.2480053901672363 -0.0 0.12091808766126633
CurrentTrain: epoch  9, batch     3 | loss: 3.2480054
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: " clinton crazies " is a term in american politics of the 1990s and later that refers to intense criticism of united states president bill clinton and his wife hillary clinton .
Head Entity: bill clinton
Tail Entity: hillary clinton
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: "In a lavish ceremony, actor Ryan Reynolds married actress Blake Lively, solidifying their status as one of Hollywood's power couples."  
Head Entity: Ryan Reynolds  
Tail Entity: Blake Lively  

Relation: spouse  
Context: "The couple celebrated their 25th wedding anniversary, reflecting on their journey together, which began when they first met in college."  
Head Entity: John Smith  
Tail Entity: Jane Smith  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: darlene released , " darlene mccoy " , on may 8 , 2007 with label emi gospel .
Head Entity: darlene mccoy
Tail Entity: emi gospel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: the band released their latest album, "echoes of the past," under the label universal music group in 2021.  
Head Entity: the band  
Tail Entity: universal music group  

Relation: record label  
Context: after signing a new contract, the artist announced that their upcoming single would be distributed by sony music entertainment.  
Head Entity: the artist  
Tail Entity: sony music entertainment  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: " the many adventures of winnie the pooh " is a 1977 american animated buddy musical comedy film produced by walt disney productions and distributed by buena vista distribution .
Head Entity: the many adventures of winnie the pooh
Tail Entity: buena vista distribution
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: "Inception" is a 2010 science fiction film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: Inception  
Tail Entity: Warner Bros. Pictures  

Relation: distributor  
Context: "The Dark Knight" is a 2008 superhero film directed by Christopher Nolan and distributed by Warner Bros. Pictures.  
Head Entity: The Dark Knight  
Tail Entity: Warner Bros. Pictures  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: one account notes , alston and his son , peter also , practiced their counterfeiting operation , at stack island , in the lower mississippi river , about 170 miles upriver from natchez .
Head Entity: stack island
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the picturesque village of portsmouth is situated along the banks of the serene river thames, providing stunning views and a tranquil atmosphere for its residents.  
Head Entity: portsmouth  
Tail Entity: river thames  

Relation: located in or next to body of water  
Context: during the summer, families flock to the shores of lake michigan, where they enjoy picnics and water sports in the vibrant city of chicago.  
Head Entity: chicago  
Tail Entity: lake michigan  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: anders morelius from sweden placed third in the individual race .
Head Entity: anders morelius
Tail Entity: sweden
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: maria gonzalez was born in mexico but later moved to the united states.  
Head Entity: maria gonzalez  
Tail Entity: mexico  

Relation: country of citizenship  
Context: after living in canada for several years, liu wei decided to apply for citizenship.  
Head Entity: liu wei  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller that explores complex themes of dreams and reality.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: beethoven's symphonies are celebrated for their profound emotional depth and are considered masterpieces of classical music.  
Head Entity: beethoven  
Tail Entity: classical music  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: san lázaro is 660 km far from asunción and is located at the exact site of the confluence of the apa river with paraguay river , bordering brazil - chaco .
Head Entity: apa river
Tail Entity: paraguay river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river flows into the gulf of mexico, creating a rich delta ecosystem that supports diverse wildlife and plant species.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the rhine river, which runs through several countries in europe, ultimately empties into the north sea, providing a vital shipping route for international trade.  
Head Entity: rhine river  
Tail Entity: north sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan (born 2 july 1989) is an american soccer player who plays as a forward for the national team and the club san diego wave fc.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james (born 30 december 1984) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: the stade louis ii in monaco was the venue for the uefa super cup every year since 1998 .
Head Entity: 1998
Tail Entity: uefa super cup
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 NBA season was significantly impacted by the COVID-19 pandemic, leading to a unique playoff format.  
Head Entity: 2020  
Tail Entity: NBA  

Relation: sports season of league or competition  
Context: The 2019 Rugby World Cup took place in Japan, showcasing teams from around the globe.  
Head Entity: 2019  
Tail Entity: Rugby World Cup  
Losses:  4.206416130065918 -0.0 1.0170130729675293
MemoryTrain:  epoch  0, batch     0 | loss: 4.2064161Losses:  4.025429725646973 0.29015588760375977 0.8788275718688965
MemoryTrain:  epoch  0, batch     1 | loss: 4.0254297Losses:  3.0547008514404297 -0.0 0.8617928624153137
MemoryTrain:  epoch  0, batch     2 | loss: 3.0547009Losses:  2.88871431350708 -0.0 1.0091488361358643
MemoryTrain:  epoch  0, batch     3 | loss: 2.8887143Losses:  3.530632257461548 -0.0 0.8379704356193542
MemoryTrain:  epoch  0, batch     4 | loss: 3.5306323Losses:  4.481581687927246 0.9024175405502319 0.9344043731689453
MemoryTrain:  epoch  0, batch     5 | loss: 4.4815817Losses:  3.537156343460083 0.2736970782279968 0.8502416610717773
MemoryTrain:  epoch  0, batch     6 | loss: 3.5371563Losses:  3.7625863552093506 0.25294679403305054 0.9032869338989258
MemoryTrain:  epoch  0, batch     7 | loss: 3.7625864Losses:  3.298499584197998 0.2619868516921997 0.9192046523094177
MemoryTrain:  epoch  0, batch     8 | loss: 3.2984996Losses:  4.749505043029785 0.795377790927887 0.8547190427780151
MemoryTrain:  epoch  0, batch     9 | loss: 4.7495050Losses:  3.504286766052246 -0.0 0.862464427947998
MemoryTrain:  epoch  0, batch    10 | loss: 3.5042868Losses:  2.323296546936035 -0.0 0.3490574359893799
MemoryTrain:  epoch  0, batch    11 | loss: 2.3232965Losses:  3.3787007331848145 0.8329368233680725 0.6900854110717773
MemoryTrain:  epoch  1, batch     0 | loss: 3.3787007Losses:  3.6210618019104004 0.21808497607707977 0.9089988470077515
MemoryTrain:  epoch  1, batch     1 | loss: 3.6210618Losses:  3.4681568145751953 0.25431883335113525 0.8047966361045837
MemoryTrain:  epoch  1, batch     2 | loss: 3.4681568Losses:  3.3536362648010254 0.23843832314014435 0.8352952599525452
MemoryTrain:  epoch  1, batch     3 | loss: 3.3536363Losses:  3.263334035873413 0.5293341875076294 0.9302935600280762
MemoryTrain:  epoch  1, batch     4 | loss: 3.2633340Losses:  2.828521728515625 0.2244221717119217 0.9011812210083008
MemoryTrain:  epoch  1, batch     5 | loss: 2.8285217Losses:  3.2549452781677246 0.259221613407135 0.9727163910865784
MemoryTrain:  epoch  1, batch     6 | loss: 3.2549453Losses:  3.067901611328125 0.24201473593711853 0.8956949710845947
MemoryTrain:  epoch  1, batch     7 | loss: 3.0679016Losses:  3.6516666412353516 1.099212884902954 0.8148989081382751
MemoryTrain:  epoch  1, batch     8 | loss: 3.6516666Losses:  2.7968528270721436 0.30352675914764404 0.9093534350395203
MemoryTrain:  epoch  1, batch     9 | loss: 2.7968528Losses:  3.5470123291015625 0.2388397455215454 0.7808389067649841
MemoryTrain:  epoch  1, batch    10 | loss: 3.5470123Losses:  2.2500431537628174 -0.0 0.36432260274887085
MemoryTrain:  epoch  1, batch    11 | loss: 2.2500432Losses:  2.7505016326904297 0.24789416790008545 0.9286172389984131
MemoryTrain:  epoch  2, batch     0 | loss: 2.7505016Losses:  3.448237657546997 0.2585238814353943 0.788134753704071
MemoryTrain:  epoch  2, batch     1 | loss: 3.4482377Losses:  3.4649033546447754 0.788020133972168 0.8688243627548218
MemoryTrain:  epoch  2, batch     2 | loss: 3.4649034Losses:  2.5862064361572266 -0.0 1.009079933166504
MemoryTrain:  epoch  2, batch     3 | loss: 2.5862064Losses:  2.901989221572876 -0.0 0.883904218673706
MemoryTrain:  epoch  2, batch     4 | loss: 2.9019892Losses:  3.443678379058838 0.4914236068725586 0.8418234586715698
MemoryTrain:  epoch  2, batch     5 | loss: 3.4436784Losses:  2.859562873840332 -0.0 0.8529499769210815
MemoryTrain:  epoch  2, batch     6 | loss: 2.8595629Losses:  2.5959222316741943 0.4619857668876648 0.7537243366241455
MemoryTrain:  epoch  2, batch     7 | loss: 2.5959222Losses:  2.4521255493164062 0.25119996070861816 0.8422874212265015
MemoryTrain:  epoch  2, batch     8 | loss: 2.4521255Losses:  2.4472708702087402 -0.0 0.9594734311103821
MemoryTrain:  epoch  2, batch     9 | loss: 2.4472709Losses:  2.9891908168792725 0.7907556295394897 0.8673207759857178
MemoryTrain:  epoch  2, batch    10 | loss: 2.9891908Losses:  1.6271255016326904 -0.0 0.3130185902118683
MemoryTrain:  epoch  2, batch    11 | loss: 1.6271255Losses:  2.4417011737823486 0.2591758370399475 0.8575854301452637
MemoryTrain:  epoch  3, batch     0 | loss: 2.4417012Losses:  2.8318021297454834 0.2464624047279358 1.017794132232666
MemoryTrain:  epoch  3, batch     1 | loss: 2.8318021Losses:  2.7981150150299072 0.25155848264694214 0.9621743559837341
MemoryTrain:  epoch  3, batch     2 | loss: 2.7981150Losses:  2.513105869293213 -0.0 0.8744394183158875
MemoryTrain:  epoch  3, batch     3 | loss: 2.5131059Losses:  2.5811166763305664 0.2713519036769867 0.9526643753051758
MemoryTrain:  epoch  3, batch     4 | loss: 2.5811167Losses:  2.861833333969116 0.26827841997146606 0.7482168078422546
MemoryTrain:  epoch  3, batch     5 | loss: 2.8618333Losses:  3.3180534839630127 0.7169000506401062 0.7849177718162537
MemoryTrain:  epoch  3, batch     6 | loss: 3.3180535Losses:  2.9107255935668945 0.5513201355934143 0.8578881621360779
MemoryTrain:  epoch  3, batch     7 | loss: 2.9107256Losses:  3.0907812118530273 0.5310183763504028 0.7693057060241699
MemoryTrain:  epoch  3, batch     8 | loss: 3.0907812Losses:  2.8872644901275635 0.2642076015472412 0.8424100279808044
MemoryTrain:  epoch  3, batch     9 | loss: 2.8872645Losses:  2.388927459716797 -0.0 0.9786018133163452
MemoryTrain:  epoch  3, batch    10 | loss: 2.3889275Losses:  1.840937852859497 -0.0 0.3115743100643158
MemoryTrain:  epoch  3, batch    11 | loss: 1.8409379Losses:  2.659950017929077 0.27209246158599854 0.7696582078933716
MemoryTrain:  epoch  4, batch     0 | loss: 2.6599500Losses:  2.5384883880615234 -0.0 0.8843674659729004
MemoryTrain:  epoch  4, batch     1 | loss: 2.5384884Losses:  2.5806546211242676 0.2584823966026306 0.9746324419975281
MemoryTrain:  epoch  4, batch     2 | loss: 2.5806546Losses:  2.168297529220581 -0.0 0.9029245376586914
MemoryTrain:  epoch  4, batch     3 | loss: 2.1682975Losses:  3.701111316680908 1.15205717086792 0.8044886589050293
MemoryTrain:  epoch  4, batch     4 | loss: 3.7011113Losses:  2.51353120803833 0.2670806646347046 0.9677330255508423
MemoryTrain:  epoch  4, batch     5 | loss: 2.5135312Losses:  2.3115856647491455 -0.0 0.8552979230880737
MemoryTrain:  epoch  4, batch     6 | loss: 2.3115857Losses:  2.996659278869629 -0.0 1.0036731958389282
MemoryTrain:  epoch  4, batch     7 | loss: 2.9966593Losses:  2.3850512504577637 -0.0 0.9809038639068604
MemoryTrain:  epoch  4, batch     8 | loss: 2.3850513Losses:  2.565183162689209 0.2641775608062744 0.8652371168136597
MemoryTrain:  epoch  4, batch     9 | loss: 2.5651832Losses:  2.294178009033203 -0.0 0.7766930460929871
MemoryTrain:  epoch  4, batch    10 | loss: 2.2941780Losses:  1.587480068206787 -0.0 0.3131571412086487
MemoryTrain:  epoch  4, batch    11 | loss: 1.5874801Losses:  2.2232799530029297 -0.0 0.7344473600387573
MemoryTrain:  epoch  5, batch     0 | loss: 2.2232800Losses:  3.999162197113037 1.6152405738830566 0.5997442007064819
MemoryTrain:  epoch  5, batch     1 | loss: 3.9991622Losses:  2.1986217498779297 -0.0 0.9251269698143005
MemoryTrain:  epoch  5, batch     2 | loss: 2.1986217Losses:  2.6641480922698975 0.48039761185646057 0.8903669714927673
MemoryTrain:  epoch  5, batch     3 | loss: 2.6641481Losses:  2.8261260986328125 0.24746456742286682 1.055800199508667
MemoryTrain:  epoch  5, batch     4 | loss: 2.8261261Losses:  2.3862853050231934 -0.0 1.0015424489974976
MemoryTrain:  epoch  5, batch     5 | loss: 2.3862853Losses:  2.94636869430542 0.5403180718421936 0.9320255517959595
MemoryTrain:  epoch  5, batch     6 | loss: 2.9463687Losses:  3.4441232681274414 1.3784642219543457 0.7767948508262634
MemoryTrain:  epoch  5, batch     7 | loss: 3.4441233Losses:  2.7317867279052734 0.4985692799091339 0.8976945281028748
MemoryTrain:  epoch  5, batch     8 | loss: 2.7317867Losses:  3.223496198654175 1.1436022520065308 0.7827036380767822
MemoryTrain:  epoch  5, batch     9 | loss: 3.2234962Losses:  2.527150869369507 0.2620691955089569 0.9694871306419373
MemoryTrain:  epoch  5, batch    10 | loss: 2.5271509Losses:  1.510733723640442 -0.0 0.3165634870529175
MemoryTrain:  epoch  5, batch    11 | loss: 1.5107337Losses:  3.2421021461486816 1.1427240371704102 0.8627704381942749
MemoryTrain:  epoch  6, batch     0 | loss: 3.2421021Losses:  2.4509434700012207 0.2522431015968323 0.9042657613754272
MemoryTrain:  epoch  6, batch     1 | loss: 2.4509435Losses:  2.29532527923584 0.23648354411125183 0.7188471555709839
MemoryTrain:  epoch  6, batch     2 | loss: 2.2953253Losses:  2.5701279640197754 0.24043318629264832 0.9073303937911987
MemoryTrain:  epoch  6, batch     3 | loss: 2.5701280Losses:  2.280574321746826 -0.0 1.0327280759811401
MemoryTrain:  epoch  6, batch     4 | loss: 2.2805743Losses:  2.162107467651367 -0.0 0.9500555992126465
MemoryTrain:  epoch  6, batch     5 | loss: 2.1621075Losses:  2.7879815101623535 0.7348520159721375 0.764495313167572
MemoryTrain:  epoch  6, batch     6 | loss: 2.7879815Losses:  2.3871636390686035 0.21967346966266632 0.76308673620224
MemoryTrain:  epoch  6, batch     7 | loss: 2.3871636Losses:  2.591891288757324 0.5074899196624756 0.8645685911178589
MemoryTrain:  epoch  6, batch     8 | loss: 2.5918913Losses:  2.178907632827759 -0.0 0.9444637894630432
MemoryTrain:  epoch  6, batch     9 | loss: 2.1789076Losses:  2.9408974647521973 0.7466483116149902 0.8946573138237
MemoryTrain:  epoch  6, batch    10 | loss: 2.9408975Losses:  2.2324378490448 -0.0 0.3030320107936859
MemoryTrain:  epoch  6, batch    11 | loss: 2.2324378Losses:  2.476248025894165 0.23559466004371643 0.8480223417282104
MemoryTrain:  epoch  7, batch     0 | loss: 2.4762480Losses:  2.774437427520752 0.7370883226394653 0.8450706005096436
MemoryTrain:  epoch  7, batch     1 | loss: 2.7744374Losses:  2.5638108253479004 0.49533361196517944 0.7855274081230164
MemoryTrain:  epoch  7, batch     2 | loss: 2.5638108Losses:  2.765122890472412 0.5017247200012207 0.9679511785507202
MemoryTrain:  epoch  7, batch     3 | loss: 2.7651229Losses:  2.429478168487549 0.2401246279478073 0.9119330644607544
MemoryTrain:  epoch  7, batch     4 | loss: 2.4294782Losses:  2.25618314743042 -0.0 0.9098405241966248
MemoryTrain:  epoch  7, batch     5 | loss: 2.2561831Losses:  2.466219902038574 0.25026553869247437 0.9634931087493896
MemoryTrain:  epoch  7, batch     6 | loss: 2.4662199Losses:  2.7888271808624268 0.5116416811943054 0.852292537689209
MemoryTrain:  epoch  7, batch     7 | loss: 2.7888272Losses:  2.371227502822876 0.2639974355697632 0.8579041361808777
MemoryTrain:  epoch  7, batch     8 | loss: 2.3712275Losses:  2.549560785293579 0.24779373407363892 0.956328809261322
MemoryTrain:  epoch  7, batch     9 | loss: 2.5495608Losses:  2.5827956199645996 0.4907206892967224 0.8906831741333008
MemoryTrain:  epoch  7, batch    10 | loss: 2.5827956Losses:  1.5406746864318848 -0.0 0.3334818184375763
MemoryTrain:  epoch  7, batch    11 | loss: 1.5406747Losses:  2.5707459449768066 0.4907851815223694 0.8547366857528687
MemoryTrain:  epoch  8, batch     0 | loss: 2.5707459Losses:  2.2705178260803223 0.27240949869155884 0.720683217048645
MemoryTrain:  epoch  8, batch     1 | loss: 2.2705178Losses:  2.2294657230377197 0.21202898025512695 0.8172160983085632
MemoryTrain:  epoch  8, batch     2 | loss: 2.2294657Losses:  2.591827630996704 0.5124064683914185 0.8923262357711792
MemoryTrain:  epoch  8, batch     3 | loss: 2.5918276Losses:  2.6241252422332764 0.49152132868766785 0.8998395800590515
MemoryTrain:  epoch  8, batch     4 | loss: 2.6241252Losses:  2.7216203212738037 0.49173703789711 0.9439573287963867
MemoryTrain:  epoch  8, batch     5 | loss: 2.7216203Losses:  2.390019178390503 0.2489868849515915 0.9443979859352112
MemoryTrain:  epoch  8, batch     6 | loss: 2.3900192Losses:  2.931081771850586 0.7359299659729004 0.8566931486129761
MemoryTrain:  epoch  8, batch     7 | loss: 2.9310818Losses:  3.1262001991271973 1.0348600149154663 0.7748337984085083
MemoryTrain:  epoch  8, batch     8 | loss: 3.1262002Losses:  2.3310532569885254 0.2279289960861206 0.7783256769180298
MemoryTrain:  epoch  8, batch     9 | loss: 2.3310533Losses:  2.257077693939209 -0.0 0.960989773273468
MemoryTrain:  epoch  8, batch    10 | loss: 2.2570777Losses:  1.6130151748657227 -0.0 0.3103649318218231
MemoryTrain:  epoch  8, batch    11 | loss: 1.6130152Losses:  2.5703420639038086 0.24405190348625183 0.9593055248260498
MemoryTrain:  epoch  9, batch     0 | loss: 2.5703421Losses:  2.4620747566223145 0.24949973821640015 1.0110032558441162
MemoryTrain:  epoch  9, batch     1 | loss: 2.4620748Losses:  2.1399543285369873 -0.0 0.9395372271537781
MemoryTrain:  epoch  9, batch     2 | loss: 2.1399543Losses:  2.4992542266845703 0.273737370967865 0.9614075422286987
MemoryTrain:  epoch  9, batch     3 | loss: 2.4992542Losses:  2.5672097206115723 0.49403226375579834 0.8384906053543091
MemoryTrain:  epoch  9, batch     4 | loss: 2.5672097Losses:  2.2444796562194824 -0.0 1.0576884746551514
MemoryTrain:  epoch  9, batch     5 | loss: 2.2444797Losses:  2.3768575191497803 0.24018561840057373 0.9358789324760437
MemoryTrain:  epoch  9, batch     6 | loss: 2.3768575Losses:  2.71626615524292 0.7110268473625183 0.7920795679092407
MemoryTrain:  epoch  9, batch     7 | loss: 2.7162662Losses:  2.3865041732788086 0.23723497986793518 0.8850256204605103
MemoryTrain:  epoch  9, batch     8 | loss: 2.3865042Losses:  2.251284599304199 0.2480713576078415 0.8006595969200134
MemoryTrain:  epoch  9, batch     9 | loss: 2.2512846Losses:  2.5966649055480957 0.47489094734191895 0.8915223479270935
MemoryTrain:  epoch  9, batch    10 | loss: 2.5966649Losses:  1.6050949096679688 0.309556782245636 0.12444620579481125
MemoryTrain:  epoch  9, batch    11 | loss: 1.6050949
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 56.25%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 83.52%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 85.10%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 83.04%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 82.50%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 82.42%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 82.35%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 80.90%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 80.92%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 80.65%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 79.62%   [EVAL] batch:   23 | acc: 75.00%,  total acc: 79.43%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 79.50%   [EVAL] batch:   25 | acc: 75.00%,  total acc: 79.33%   [EVAL] batch:   26 | acc: 81.25%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 75.00%,  total acc: 79.24%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.74%   [EVAL] batch:   29 | acc: 56.25%,  total acc: 78.96%   [EVAL] batch:   30 | acc: 31.25%,  total acc: 77.42%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 77.15%   [EVAL] batch:   32 | acc: 62.50%,  total acc: 76.70%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 76.10%   [EVAL] batch:   34 | acc: 68.75%,  total acc: 75.89%   [EVAL] batch:   35 | acc: 75.00%,  total acc: 75.87%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 75.17%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 74.84%   [EVAL] batch:   38 | acc: 75.00%,  total acc: 74.84%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 75.16%   [EVAL] batch:   40 | acc: 81.25%,  total acc: 75.30%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 75.45%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 75.73%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 75.57%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 74.31%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 73.37%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 72.47%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 71.88%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 71.05%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 70.50%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 71.08%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 71.63%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 72.17%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 72.69%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 73.18%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 73.66%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 74.12%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 74.57%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 74.89%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 75.21%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 75.61%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 75.91%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 75.30%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 72.66%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 70.83%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 67.61%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 68.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 70.09%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 71.67%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 76.97%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 77.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 77.98%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 77.84%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 77.17%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.60%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 77.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.61%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 79.40%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 79.91%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.39%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 81.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.65%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.23%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.77%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 82.54%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 82.64%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 82.77%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 83.22%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 84.06%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 84.45%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 84.82%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 85.03%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 85.23%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 85.00%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 84.92%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 84.71%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 84.38%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 84.57%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 84.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 84.68%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 84.86%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.02%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 84.95%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 84.77%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 84.43%   [EVAL] batch:   57 | acc: 12.50%,  total acc: 83.19%   [EVAL] batch:   58 | acc: 43.75%,  total acc: 82.52%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 82.40%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 82.27%   [EVAL] batch:   61 | acc: 50.00%,  total acc: 81.75%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 81.05%   [EVAL] batch:   63 | acc: 37.50%,  total acc: 80.37%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 79.52%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 79.26%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 78.54%   [EVAL] batch:   67 | acc: 31.25%,  total acc: 77.85%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 77.72%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 77.86%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 77.82%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 77.78%   [EVAL] batch:   72 | acc: 93.75%,  total acc: 78.00%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 78.33%   [EVAL] batch:   75 | acc: 37.50%,  total acc: 77.80%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 76.95%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 76.36%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 75.87%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 75.62%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 75.00%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 75.30%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 75.60%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 75.81%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 76.09%   [EVAL] batch:   86 | acc: 100.00%,  total acc: 76.36%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 76.35%   [EVAL] batch:   88 | acc: 31.25%,  total acc: 75.84%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 75.28%   [EVAL] batch:   90 | acc: 37.50%,  total acc: 74.86%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 74.52%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 73.79%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 73.54%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 73.68%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 73.83%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 74.03%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 74.11%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 74.18%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 74.31%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 74.26%   [EVAL] batch:  101 | acc: 87.50%,  total acc: 74.39%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 74.15%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 74.04%   [EVAL] batch:  104 | acc: 75.00%,  total acc: 74.05%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 74.12%   [EVAL] batch:  106 | acc: 18.75%,  total acc: 73.60%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 73.03%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 72.53%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 72.05%   [EVAL] batch:  110 | acc: 6.25%,  total acc: 71.45%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 71.04%   [EVAL] batch:  112 | acc: 25.00%,  total acc: 70.63%   [EVAL] batch:  113 | acc: 25.00%,  total acc: 70.23%   [EVAL] batch:  114 | acc: 68.75%,  total acc: 70.22%   [EVAL] batch:  115 | acc: 43.75%,  total acc: 69.99%   [EVAL] batch:  116 | acc: 37.50%,  total acc: 69.71%   [EVAL] batch:  117 | acc: 43.75%,  total acc: 69.49%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 69.54%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 69.89%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 69.98%   [EVAL] batch:  122 | acc: 75.00%,  total acc: 70.02%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 70.11%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 70.20%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 69.84%   [EVAL] batch:  126 | acc: 12.50%,  total acc: 69.39%   [EVAL] batch:  127 | acc: 6.25%,  total acc: 68.90%   [EVAL] batch:  128 | acc: 18.75%,  total acc: 68.51%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 68.03%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 67.56%   [EVAL] batch:  131 | acc: 43.75%,  total acc: 67.38%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 67.43%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 67.58%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 67.73%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 67.88%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 68.02%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 67.98%   [EVAL] batch:  138 | acc: 25.00%,  total acc: 67.67%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 67.50%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 67.11%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 66.95%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 66.70%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 66.58%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 66.81%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 67.22%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.66%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  150 | acc: 100.00%,  total acc: 68.05%   [EVAL] batch:  151 | acc: 100.00%,  total acc: 68.26%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 68.46%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 68.67%   [EVAL] batch:  154 | acc: 100.00%,  total acc: 68.87%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 69.69%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 69.98%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 69.94%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 69.66%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 69.36%   [EVAL] batch:  165 | acc: 18.75%,  total acc: 69.05%   [EVAL] batch:  166 | acc: 25.00%,  total acc: 68.79%   [EVAL] batch:  167 | acc: 18.75%,  total acc: 68.49%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 68.42%   [EVAL] batch:  169 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 68.68%   [EVAL] batch:  171 | acc: 100.00%,  total acc: 68.86%   [EVAL] batch:  172 | acc: 81.25%,  total acc: 68.93%   [EVAL] batch:  173 | acc: 100.00%,  total acc: 69.11%   [EVAL] batch:  174 | acc: 93.75%,  total acc: 69.25%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 69.11%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 68.96%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 68.75%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 68.68%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 68.58%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 68.37%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 68.27%   [EVAL] batch:  182 | acc: 68.75%,  total acc: 68.27%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 68.14%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 68.14%   [EVAL] batch:  185 | acc: 68.75%,  total acc: 68.15%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 68.22%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 68.15%   [EVAL] batch:  188 | acc: 68.75%,  total acc: 68.15%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 68.19%   [EVAL] batch:  190 | acc: 81.25%,  total acc: 68.26%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 68.29%   [EVAL] batch:  192 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 68.30%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 68.40%   [EVAL] batch:  195 | acc: 81.25%,  total acc: 68.46%   [EVAL] batch:  196 | acc: 62.50%,  total acc: 68.43%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:  198 | acc: 81.25%,  total acc: 68.53%   [EVAL] batch:  199 | acc: 87.50%,  total acc: 68.62%   [EVAL] batch:  200 | acc: 87.50%,  total acc: 68.72%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 68.78%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 68.81%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 68.81%   [EVAL] batch:  205 | acc: 81.25%,  total acc: 68.87%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 68.69%   [EVAL] batch:  207 | acc: 31.25%,  total acc: 68.51%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 68.39%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 68.18%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 68.01%   [EVAL] batch:  211 | acc: 31.25%,  total acc: 67.84%   [EVAL] batch:  212 | acc: 50.00%,  total acc: 67.75%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.90%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 68.05%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 68.20%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 68.35%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 68.61%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 69.17%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 69.41%   [EVAL] batch:  226 | acc: 62.50%,  total acc: 69.38%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 69.35%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 69.35%   [EVAL] batch:  229 | acc: 87.50%,  total acc: 69.43%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 69.40%   [EVAL] batch:  231 | acc: 43.75%,  total acc: 69.29%   [EVAL] batch:  232 | acc: 18.75%,  total acc: 69.07%   [EVAL] batch:  233 | acc: 18.75%,  total acc: 68.86%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 68.67%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 68.62%   [EVAL] batch:  236 | acc: 18.75%,  total acc: 68.41%   [EVAL] batch:  237 | acc: 50.00%,  total acc: 68.33%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 68.41%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 68.54%   [EVAL] batch:  240 | acc: 81.25%,  total acc: 68.59%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 68.67%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 68.80%   [EVAL] batch:  244 | acc: 31.25%,  total acc: 68.65%   [EVAL] batch:  245 | acc: 37.50%,  total acc: 68.52%   [EVAL] batch:  246 | acc: 6.25%,  total acc: 68.27%   [EVAL] batch:  247 | acc: 37.50%,  total acc: 68.15%   [EVAL] batch:  248 | acc: 12.50%,  total acc: 67.92%   [EVAL] batch:  249 | acc: 25.00%,  total acc: 67.75%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 67.85%   [EVAL] batch:  251 | acc: 81.25%,  total acc: 67.91%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 67.98%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 68.04%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 68.14%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 68.26%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 68.26%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 68.29%   [EVAL] batch:  258 | acc: 68.75%,  total acc: 68.29%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 68.37%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 68.41%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 68.46%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 68.51%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 68.56%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 68.56%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 68.54%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 68.49%   [EVAL] batch:  267 | acc: 62.50%,  total acc: 68.47%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 68.47%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 68.33%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 68.20%   [EVAL] batch:  271 | acc: 37.50%,  total acc: 68.08%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 67.90%   [EVAL] batch:  273 | acc: 37.50%,  total acc: 67.79%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 67.61%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 67.85%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 67.96%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 68.08%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 68.19%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.31%   [EVAL] batch:  281 | acc: 75.00%,  total acc: 68.33%   [EVAL] batch:  282 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:  283 | acc: 62.50%,  total acc: 68.40%   [EVAL] batch:  284 | acc: 75.00%,  total acc: 68.42%   [EVAL] batch:  285 | acc: 68.75%,  total acc: 68.42%   [EVAL] batch:  286 | acc: 62.50%,  total acc: 68.40%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 68.40%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 68.51%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 68.62%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 68.84%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 68.94%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 69.03%   [EVAL] batch:  294 | acc: 93.75%,  total acc: 69.11%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 69.32%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 69.42%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:  299 | acc: 100.00%,  total acc: 69.62%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 69.68%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 69.76%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 69.84%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 69.92%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 69.96%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 70.04%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 70.11%   [EVAL] batch:  307 | acc: 87.50%,  total acc: 70.17%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 70.25%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 70.34%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 70.44%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 70.51%   [EVAL] batch:  312 | acc: 87.50%,  total acc: 70.57%   [EVAL] batch:  313 | acc: 81.25%,  total acc: 70.60%   [EVAL] batch:  314 | acc: 68.75%,  total acc: 70.60%   [EVAL] batch:  315 | acc: 68.75%,  total acc: 70.59%   [EVAL] batch:  316 | acc: 62.50%,  total acc: 70.56%   [EVAL] batch:  317 | acc: 93.75%,  total acc: 70.64%   [EVAL] batch:  318 | acc: 93.75%,  total acc: 70.71%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 70.80%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 70.89%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 70.92%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 70.98%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 71.05%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 71.10%   [EVAL] batch:  325 | acc: 81.25%,  total acc: 71.13%   [EVAL] batch:  326 | acc: 68.75%,  total acc: 71.12%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 71.13%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 71.14%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 71.16%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 71.15%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 71.16%   [EVAL] batch:  332 | acc: 68.75%,  total acc: 71.15%   [EVAL] batch:  333 | acc: 87.50%,  total acc: 71.20%   [EVAL] batch:  334 | acc: 62.50%,  total acc: 71.18%   [EVAL] batch:  335 | acc: 75.00%,  total acc: 71.19%   [EVAL] batch:  336 | acc: 75.00%,  total acc: 71.20%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 71.26%   [EVAL] batch:  338 | acc: 68.75%,  total acc: 71.26%   [EVAL] batch:  339 | acc: 75.00%,  total acc: 71.27%   [EVAL] batch:  340 | acc: 81.25%,  total acc: 71.30%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 71.31%   [EVAL] batch:  342 | acc: 43.75%,  total acc: 71.23%   [EVAL] batch:  343 | acc: 43.75%,  total acc: 71.15%   [EVAL] batch:  344 | acc: 81.25%,  total acc: 71.18%   [EVAL] batch:  345 | acc: 37.50%,  total acc: 71.08%   [EVAL] batch:  346 | acc: 68.75%,  total acc: 71.07%   [EVAL] batch:  347 | acc: 75.00%,  total acc: 71.08%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 71.10%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 71.04%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 71.01%   [EVAL] batch:  351 | acc: 87.50%,  total acc: 71.06%   [EVAL] batch:  352 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 71.13%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 71.14%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 71.21%   [EVAL] batch:  356 | acc: 37.50%,  total acc: 71.11%   [EVAL] batch:  357 | acc: 18.75%,  total acc: 70.97%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 70.86%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 70.78%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 70.67%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 70.61%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 70.59%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 70.67%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 71.07%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  370 | acc: 100.00%,  total acc: 71.23%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 71.27%   [EVAL] batch:  372 | acc: 100.00%,  total acc: 71.35%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 71.42%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 71.45%   
cur_acc:  ['0.9484', '0.7411', '0.7976', '0.6895', '0.8254', '0.7530']
his_acc:  ['0.9484', '0.8300', '0.7912', '0.7218', '0.7238', '0.7145']
Clustering into  34  clusters
Clusters:  [ 4  2 23 19  3  3 24  1  6 13  0 11 25  8 10  3  4  4 30  0 29 27 20 14
 31 27  7 28  6  4  2 28 18 32 21 27 22 20  0  7 19  4  4  6 26  9 12 33
  1  6 20  1  1  3 10 13  4  3 27 27  2  5  8  0  1 17 16  9 15 28]
Losses:  8.975067138671875 3.074205160140991 0.7440982460975647
CurrentTrain: epoch  0, batch     0 | loss: 8.9750671Losses:  9.778204917907715 3.4380311965942383 0.7683252692222595
CurrentTrain: epoch  0, batch     1 | loss: 9.7782049Losses:  9.413435935974121 2.6879472732543945 0.8070324063301086
CurrentTrain: epoch  0, batch     2 | loss: 9.4134359Losses:  4.783711910247803 -0.0 0.12475809454917908
CurrentTrain: epoch  0, batch     3 | loss: 4.7837119Losses:  8.050215721130371 2.6461915969848633 0.7299659848213196
CurrentTrain: epoch  1, batch     0 | loss: 8.0502157Losses:  8.068957328796387 2.6596333980560303 0.7660749554634094
CurrentTrain: epoch  1, batch     1 | loss: 8.0689573Losses:  9.230426788330078 3.888625383377075 0.7062053680419922
CurrentTrain: epoch  1, batch     2 | loss: 9.2304268Losses:  5.847097396850586 -0.0 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 5.8470974Losses:  11.156437873840332 5.385430335998535 0.7129382491111755
CurrentTrain: epoch  2, batch     0 | loss: 11.1564379Losses:  8.024215698242188 3.33274507522583 0.6824209690093994
CurrentTrain: epoch  2, batch     1 | loss: 8.0242157Losses:  7.063918590545654 2.758317470550537 0.745731770992279
CurrentTrain: epoch  2, batch     2 | loss: 7.0639186Losses:  4.028373718261719 -0.0 0.09736396372318268
CurrentTrain: epoch  2, batch     3 | loss: 4.0283737Losses:  6.813801288604736 2.529794454574585 0.7417309880256653
CurrentTrain: epoch  3, batch     0 | loss: 6.8138013Losses:  7.126156330108643 2.7057301998138428 0.7680955529212952
CurrentTrain: epoch  3, batch     1 | loss: 7.1261563Losses:  8.51439094543457 3.604084014892578 0.748572587966919
CurrentTrain: epoch  3, batch     2 | loss: 8.5143909Losses:  3.8119935989379883 -0.0 0.10589316487312317
CurrentTrain: epoch  3, batch     3 | loss: 3.8119936Losses:  7.412554740905762 3.308986186981201 0.6867333054542542
CurrentTrain: epoch  4, batch     0 | loss: 7.4125547Losses:  8.335620880126953 4.15997314453125 0.6737218499183655
CurrentTrain: epoch  4, batch     1 | loss: 8.3356209Losses:  7.130282402038574 3.53928279876709 0.6738827228546143
CurrentTrain: epoch  4, batch     2 | loss: 7.1302824Losses:  6.507744789123535 -0.0 0.08670882880687714
CurrentTrain: epoch  4, batch     3 | loss: 6.5077448Losses:  5.7021050453186035 2.16934871673584 0.7434566617012024
CurrentTrain: epoch  5, batch     0 | loss: 5.7021050Losses:  7.941884517669678 3.584520101547241 0.6836028099060059
CurrentTrain: epoch  5, batch     1 | loss: 7.9418845Losses:  7.868006229400635 4.1190080642700195 0.7318830490112305
CurrentTrain: epoch  5, batch     2 | loss: 7.8680062Losses:  3.470808267593384 -0.0 0.2112543135881424
CurrentTrain: epoch  5, batch     3 | loss: 3.4708083Losses:  8.713641166687012 4.6743927001953125 0.6720731258392334
CurrentTrain: epoch  6, batch     0 | loss: 8.7136412Losses:  5.634830474853516 2.5566182136535645 0.7230318188667297
CurrentTrain: epoch  6, batch     1 | loss: 5.6348305Losses:  7.695499420166016 3.8245291709899902 0.7218592762947083
CurrentTrain: epoch  6, batch     2 | loss: 7.6954994Losses:  1.759271264076233 -0.0 0.130536749958992
CurrentTrain: epoch  6, batch     3 | loss: 1.7592713Losses:  5.596404552459717 2.190587282180786 0.7182298302650452
CurrentTrain: epoch  7, batch     0 | loss: 5.5964046Losses:  7.315250873565674 3.5120973587036133 0.6476074457168579
CurrentTrain: epoch  7, batch     1 | loss: 7.3152509Losses:  5.63223123550415 2.719388961791992 0.6330709457397461
CurrentTrain: epoch  7, batch     2 | loss: 5.6322312Losses:  4.198335647583008 -0.0 0.1283164918422699
CurrentTrain: epoch  7, batch     3 | loss: 4.1983356Losses:  5.264426231384277 2.3907625675201416 0.7184981107711792
CurrentTrain: epoch  8, batch     0 | loss: 5.2644262Losses:  5.029966831207275 1.9369614124298096 0.7880997061729431
CurrentTrain: epoch  8, batch     1 | loss: 5.0299668Losses:  7.336005210876465 3.4387569427490234 0.7418924570083618
CurrentTrain: epoch  8, batch     2 | loss: 7.3360052Losses:  2.1497719287872314 -0.0 0.14602422714233398
CurrentTrain: epoch  8, batch     3 | loss: 2.1497719Losses:  5.40667200088501 2.139676332473755 0.7850307822227478
CurrentTrain: epoch  9, batch     0 | loss: 5.4066720Losses:  4.777270793914795 1.8776676654815674 0.7775761485099792
CurrentTrain: epoch  9, batch     1 | loss: 4.7772708Losses:  5.457089900970459 2.262868642807007 0.7153649926185608
CurrentTrain: epoch  9, batch     2 | loss: 5.4570899Losses:  1.802923560142517 -0.0 0.1116761714220047
CurrentTrain: epoch  9, batch     3 | loss: 1.8029236
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: hettange - grande ( , lorraine franconian : " grouss - hetténgen " ) is a commune in the moselle department in grand est in north - eastern france .
Head Entity: moselle
Tail Entity: hettange
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The state of California is divided into several counties, including Los Angeles County, which is known for its entertainment industry.  
Head Entity: California  
Tail Entity: Los Angeles County  

Relation: contains administrative territorial entity  
Context: The province of Ontario is home to many cities, with Toronto being the capital and the largest city in the province.  
Head Entity: Ontario  
Tail Entity: Toronto  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: Many historians argue that the ancient city of Byzantium is said to be the same as modern-day Istanbul, although the transition involved significant changes over time.  
Head Entity: Byzantium  
Tail Entity: Istanbul  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: he considers his father to be biggest influence on his career as his brother javier castellano recipient of four eclipse award for outstanding jockey in the row ( 2013 , 2014,2015 and 2016 ) .
Head Entity: eclipse award for outstanding jockey
Tail Entity: javier castellano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: In the thrilling finale of the championship, Sarah Thompson emerged victorious, claiming the title of best player in the tournament, while her teammate, Mark Johnson, was awarded the runner-up position.  
Head Entity: best player in the tournament  
Tail Entity: Sarah Thompson  

Relation: winner  
Context: The annual science fair concluded with Emily Chen taking home the grand prize for her innovative project on renewable energy, while her classmate, David Lee, received an honorable mention for his work on robotics.  
Head Entity: grand prize  
Tail Entity: Emily Chen  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the railroad car they were deported in was attached to the end of the last train out of drancy which also carried drancy commandant ss hauptsturmführer alois brunner and other german military personnel .
Head Entity: alois brunner
Tail Entity: hauptsturmführer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: General John Smith was awarded the title of Major General for his exceptional leadership during the conflict, which significantly contributed to the victory of his battalion.  
Head Entity: John Smith  
Tail Entity: Major General  

Relation: military rank  
Context: During the ceremony, Colonel Jane Doe was recognized for her outstanding service and was promoted to the rank of Brigadier General, a position she had long aspired to achieve.  
Head Entity: Jane Doe  
Tail Entity: Brigadier General  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham, set to hit the shelves on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular science magazine, Scientific American, published an article discussing the latest advancements in renewable energy technologies.  
Head Entity: Scientific American  
Tail Entity: renewable energy technologies  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their greatest hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: there is a memoir of his life and work by gustav queck ( jena , 1852 ) .
Head Entity: gustav queck
Tail Entity: jena
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: the headquarters of the company is situated in san francisco, where they have been operating since 2010.  
Head Entity: the company  
Tail Entity: san francisco  

Relation: work location  
Context: during her time at the university, she conducted research in various labs located in boston.  
Head Entity: she  
Tail Entity: boston  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: burt kennedy ( september 3 , 1922 – february 15 , 2001 ) was an american screenwriter and director known mainly for directing westerns .
Head Entity: burt kennedy
Tail Entity: director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: Marie Curie (November 7, 1867 – July 4, 1934) was a Polish-born physicist and chemist who conducted pioneering research on radioactivity.  
Head Entity: Marie Curie  
Tail Entity: physicist  

Relation: occupation  
Context: Albert Einstein (March 14, 1879 – April 18, 1955) was a theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics.  
Head Entity: Albert Einstein  
Tail Entity: theoretical physicist  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Losses:  4.520946502685547 0.7836324572563171 1.042677640914917
MemoryTrain:  epoch  0, batch     0 | loss: 4.5209465Losses:  3.482267141342163 0.5127025246620178 0.9305769801139832
MemoryTrain:  epoch  0, batch     1 | loss: 3.4822671Losses:  3.6113505363464355 0.25212931632995605 0.9699928164482117
MemoryTrain:  epoch  0, batch     2 | loss: 3.6113505Losses:  3.594848871231079 0.5028520226478577 0.8249590992927551
MemoryTrain:  epoch  0, batch     3 | loss: 3.5948489Losses:  3.2770633697509766 0.22769343852996826 0.8929376602172852
MemoryTrain:  epoch  0, batch     4 | loss: 3.2770634Losses:  3.758333683013916 0.5003427863121033 0.9641169309616089
MemoryTrain:  epoch  0, batch     5 | loss: 3.7583337Losses:  4.248483657836914 0.5382121801376343 0.9097601771354675
MemoryTrain:  epoch  0, batch     6 | loss: 4.2484837Losses:  3.3465018272399902 -0.0 0.7342768311500549
MemoryTrain:  epoch  0, batch     7 | loss: 3.3465018Losses:  4.301026344299316 0.8578130602836609 0.9550379514694214
MemoryTrain:  epoch  0, batch     8 | loss: 4.3010263Losses:  3.8647282123565674 -0.0 0.9750060439109802
MemoryTrain:  epoch  0, batch     9 | loss: 3.8647282Losses:  3.441821575164795 0.2366737574338913 0.8838993906974792
MemoryTrain:  epoch  0, batch    10 | loss: 3.4418216Losses:  3.644780158996582 0.2530624270439148 1.030633807182312
MemoryTrain:  epoch  0, batch    11 | loss: 3.6447802Losses:  4.066556453704834 0.31871992349624634 0.7813166975975037
MemoryTrain:  epoch  0, batch    12 | loss: 4.0665565Losses:  2.980947971343994 -0.0 0.12193728983402252
MemoryTrain:  epoch  0, batch    13 | loss: 2.9809480Losses:  3.02255916595459 0.2486455738544464 0.8666855692863464
MemoryTrain:  epoch  1, batch     0 | loss: 3.0225592Losses:  3.3881654739379883 0.24985644221305847 0.9932360053062439
MemoryTrain:  epoch  1, batch     1 | loss: 3.3881655Losses:  3.1619679927825928 0.2461206316947937 0.7905022501945496
MemoryTrain:  epoch  1, batch     2 | loss: 3.1619680Losses:  3.0321156978607178 0.26171696186065674 0.9714634418487549
MemoryTrain:  epoch  1, batch     3 | loss: 3.0321157Losses:  3.5378427505493164 0.2577309012413025 0.930381178855896
MemoryTrain:  epoch  1, batch     4 | loss: 3.5378428Losses:  2.740865707397461 -0.0 0.8544520735740662
MemoryTrain:  epoch  1, batch     5 | loss: 2.7408657Losses:  3.3141279220581055 0.2826140224933624 0.7766537070274353
MemoryTrain:  epoch  1, batch     6 | loss: 3.3141279Losses:  4.342885971069336 0.2613968849182129 1.0756843090057373
MemoryTrain:  epoch  1, batch     7 | loss: 4.3428860Losses:  3.648524045944214 0.2458164095878601 0.9998611807823181
MemoryTrain:  epoch  1, batch     8 | loss: 3.6485240Losses:  3.7403645515441895 0.2801382541656494 1.030665636062622
MemoryTrain:  epoch  1, batch     9 | loss: 3.7403646Losses:  2.6250977516174316 0.23673465847969055 0.9756596088409424
MemoryTrain:  epoch  1, batch    10 | loss: 2.6250978Losses:  3.3223531246185303 0.23862987756729126 1.0274183750152588
MemoryTrain:  epoch  1, batch    11 | loss: 3.3223531Losses:  2.995922565460205 0.24257278442382812 0.923725962638855
MemoryTrain:  epoch  1, batch    12 | loss: 2.9959226Losses:  1.4741299152374268 -0.0 0.09696930646896362
MemoryTrain:  epoch  1, batch    13 | loss: 1.4741299Losses:  2.9706945419311523 -0.0 0.7955896258354187
MemoryTrain:  epoch  2, batch     0 | loss: 2.9706945Losses:  3.517817974090576 0.7122205495834351 0.9649761915206909
MemoryTrain:  epoch  2, batch     1 | loss: 3.5178180Losses:  2.9702205657958984 -0.0 1.0235085487365723
MemoryTrain:  epoch  2, batch     2 | loss: 2.9702206Losses:  3.240506172180176 0.5601071715354919 0.7849864363670349
MemoryTrain:  epoch  2, batch     3 | loss: 3.2405062Losses:  3.022712230682373 0.25106918811798096 0.9236100912094116
MemoryTrain:  epoch  2, batch     4 | loss: 3.0227122Losses:  2.554698944091797 -0.0 0.9349318742752075
MemoryTrain:  epoch  2, batch     5 | loss: 2.5546989Losses:  2.8870978355407715 0.2287023663520813 1.016205072402954
MemoryTrain:  epoch  2, batch     6 | loss: 2.8870978Losses:  2.743852138519287 0.4948320984840393 0.8928839564323425
MemoryTrain:  epoch  2, batch     7 | loss: 2.7438521Losses:  2.840203046798706 0.48893898725509644 0.9143393039703369
MemoryTrain:  epoch  2, batch     8 | loss: 2.8402030Losses:  2.9988014698028564 0.5021124482154846 0.8604150414466858
MemoryTrain:  epoch  2, batch     9 | loss: 2.9988015Losses:  2.1193413734436035 -0.0 0.8477635383605957
MemoryTrain:  epoch  2, batch    10 | loss: 2.1193414Losses:  3.201141357421875 0.24978765845298767 0.8455438017845154
MemoryTrain:  epoch  2, batch    11 | loss: 3.2011414Losses:  2.6524720191955566 -0.0 0.9809256792068481
MemoryTrain:  epoch  2, batch    12 | loss: 2.6524720Losses:  1.4666043519973755 -0.0 0.1443038284778595
MemoryTrain:  epoch  2, batch    13 | loss: 1.4666044Losses:  2.3265867233276367 -0.0 0.9485082030296326
MemoryTrain:  epoch  3, batch     0 | loss: 2.3265867Losses:  2.6185193061828613 0.2665356397628784 1.0172985792160034
MemoryTrain:  epoch  3, batch     1 | loss: 2.6185193Losses:  2.4511172771453857 0.25151145458221436 0.9208335280418396
MemoryTrain:  epoch  3, batch     2 | loss: 2.4511173Losses:  2.6492977142333984 0.24247844517230988 0.8549827337265015
MemoryTrain:  epoch  3, batch     3 | loss: 2.6492977Losses:  2.374765396118164 -0.0 0.9733700752258301
MemoryTrain:  epoch  3, batch     4 | loss: 2.3747654Losses:  2.7847065925598145 0.259550005197525 0.7875118255615234
MemoryTrain:  epoch  3, batch     5 | loss: 2.7847066Losses:  4.765218734741211 1.4191200733184814 0.8364636898040771
MemoryTrain:  epoch  3, batch     6 | loss: 4.7652187Losses:  2.8408422470092773 0.2601373791694641 1.0005327463150024
MemoryTrain:  epoch  3, batch     7 | loss: 2.8408422Losses:  2.456637382507324 -0.0 0.8548964858055115
MemoryTrain:  epoch  3, batch     8 | loss: 2.4566374Losses:  2.760734796524048 -0.0 0.8592839241027832
MemoryTrain:  epoch  3, batch     9 | loss: 2.7607348Losses:  2.470231294631958 -0.0 0.9656816124916077
MemoryTrain:  epoch  3, batch    10 | loss: 2.4702313Losses:  2.6825499534606934 -0.0 1.005668044090271
MemoryTrain:  epoch  3, batch    11 | loss: 2.6825500Losses:  2.8476881980895996 0.4796833395957947 0.8273437023162842
MemoryTrain:  epoch  3, batch    12 | loss: 2.8476882Losses:  1.3477271795272827 -0.0 0.11397731304168701
MemoryTrain:  epoch  3, batch    13 | loss: 1.3477272Losses:  2.140932321548462 -0.0 0.8571770787239075
MemoryTrain:  epoch  4, batch     0 | loss: 2.1409323Losses:  2.8803963661193848 0.501375138759613 0.9210878610610962
MemoryTrain:  epoch  4, batch     1 | loss: 2.8803964Losses:  2.084444284439087 -0.0 0.8493776321411133
MemoryTrain:  epoch  4, batch     2 | loss: 2.0844443Losses:  3.209756374359131 0.5053315162658691 0.871086597442627
MemoryTrain:  epoch  4, batch     3 | loss: 3.2097564Losses:  2.307844638824463 -0.0 1.059133529663086
MemoryTrain:  epoch  4, batch     4 | loss: 2.3078446Losses:  2.5383641719818115 0.2538830637931824 0.9661755561828613
MemoryTrain:  epoch  4, batch     5 | loss: 2.5383642Losses:  2.7839579582214355 -0.0 1.0226943492889404
MemoryTrain:  epoch  4, batch     6 | loss: 2.7839580Losses:  3.264892101287842 1.0648200511932373 0.847028911113739
MemoryTrain:  epoch  4, batch     7 | loss: 3.2648921Losses:  2.419919490814209 0.24423860013484955 0.9187290072441101
MemoryTrain:  epoch  4, batch     8 | loss: 2.4199195Losses:  3.1047346591949463 0.24281863868236542 0.9334590435028076
MemoryTrain:  epoch  4, batch     9 | loss: 3.1047347Losses:  2.6917474269866943 0.2592030167579651 0.9184970855712891
MemoryTrain:  epoch  4, batch    10 | loss: 2.6917474Losses:  3.4201724529266357 0.25837814807891846 0.9973714351654053
MemoryTrain:  epoch  4, batch    11 | loss: 3.4201725Losses:  3.3468828201293945 0.503523051738739 0.977373480796814
MemoryTrain:  epoch  4, batch    12 | loss: 3.3468828Losses:  2.3836121559143066 -0.0 0.12896212935447693
MemoryTrain:  epoch  4, batch    13 | loss: 2.3836122Losses:  2.891871929168701 0.5342185497283936 0.9115561246871948
MemoryTrain:  epoch  5, batch     0 | loss: 2.8918719Losses:  2.8021657466888428 0.25324368476867676 1.0311496257781982
MemoryTrain:  epoch  5, batch     1 | loss: 2.8021657Losses:  2.5839667320251465 -0.0 0.9765537977218628
MemoryTrain:  epoch  5, batch     2 | loss: 2.5839667Losses:  4.15213680267334 1.4860272407531738 0.8899109959602356
MemoryTrain:  epoch  5, batch     3 | loss: 4.1521368Losses:  2.655691146850586 0.7137258052825928 0.7088533639907837
MemoryTrain:  epoch  5, batch     4 | loss: 2.6556911Losses:  2.4639105796813965 0.2385597825050354 0.86520916223526
MemoryTrain:  epoch  5, batch     5 | loss: 2.4639106Losses:  2.25045108795166 -0.0 1.0102853775024414
MemoryTrain:  epoch  5, batch     6 | loss: 2.2504511Losses:  2.825338840484619 0.2859225869178772 0.9152611494064331
MemoryTrain:  epoch  5, batch     7 | loss: 2.8253388Losses:  2.420863151550293 0.2511683404445648 0.9542698860168457
MemoryTrain:  epoch  5, batch     8 | loss: 2.4208632Losses:  2.7346041202545166 0.4668998718261719 0.8521137237548828
MemoryTrain:  epoch  5, batch     9 | loss: 2.7346041Losses:  3.1175355911254883 0.7944645881652832 0.7362347841262817
MemoryTrain:  epoch  5, batch    10 | loss: 3.1175356Losses:  2.5588221549987793 0.23218993842601776 0.9614332914352417
MemoryTrain:  epoch  5, batch    11 | loss: 2.5588222Losses:  2.321277618408203 -0.0 0.9526398181915283
MemoryTrain:  epoch  5, batch    12 | loss: 2.3212776Losses:  1.3695684671401978 -0.0 0.11324454843997955
MemoryTrain:  epoch  5, batch    13 | loss: 1.3695685Losses:  2.371847629547119 -0.0 0.9165418148040771
MemoryTrain:  epoch  6, batch     0 | loss: 2.3718476Losses:  2.60856294631958 0.5122385025024414 0.8369342088699341
MemoryTrain:  epoch  6, batch     1 | loss: 2.6085629Losses:  2.493243455886841 0.23456108570098877 0.903878927230835
MemoryTrain:  epoch  6, batch     2 | loss: 2.4932435Losses:  2.466468334197998 0.2558208703994751 0.9200069308280945
MemoryTrain:  epoch  6, batch     3 | loss: 2.4664683Losses:  3.1591665744781494 0.5837680697441101 0.939899206161499
MemoryTrain:  epoch  6, batch     4 | loss: 3.1591666Losses:  2.3161416053771973 0.23323002457618713 0.8894680738449097
MemoryTrain:  epoch  6, batch     5 | loss: 2.3161416Losses:  2.7883284091949463 0.5023934841156006 0.8730533123016357
MemoryTrain:  epoch  6, batch     6 | loss: 2.7883284Losses:  2.469860553741455 0.24994146823883057 0.8588989973068237
MemoryTrain:  epoch  6, batch     7 | loss: 2.4698606Losses:  2.457259178161621 0.25931352376937866 0.8596480488777161
MemoryTrain:  epoch  6, batch     8 | loss: 2.4572592Losses:  2.485503673553467 -0.0 1.0110390186309814
MemoryTrain:  epoch  6, batch     9 | loss: 2.4855037Losses:  2.6493401527404785 0.266681969165802 0.997299075126648
MemoryTrain:  epoch  6, batch    10 | loss: 2.6493402Losses:  2.1205010414123535 -0.0 0.9137200117111206
MemoryTrain:  epoch  6, batch    11 | loss: 2.1205010Losses:  2.7599616050720215 0.470930814743042 0.912590503692627
MemoryTrain:  epoch  6, batch    12 | loss: 2.7599616Losses:  1.3281183242797852 -0.0 0.153568297624588
MemoryTrain:  epoch  6, batch    13 | loss: 1.3281183Losses:  2.3735618591308594 0.2541935443878174 0.8608992695808411
MemoryTrain:  epoch  7, batch     0 | loss: 2.3735619Losses:  2.268886089324951 -0.0 0.9681146144866943
MemoryTrain:  epoch  7, batch     1 | loss: 2.2688861Losses:  2.2582848072052 -0.0 0.984409749507904
MemoryTrain:  epoch  7, batch     2 | loss: 2.2582848Losses:  2.4047648906707764 0.24579215049743652 0.9551777839660645
MemoryTrain:  epoch  7, batch     3 | loss: 2.4047649Losses:  2.5078983306884766 0.2462274432182312 0.8990296125411987
MemoryTrain:  epoch  7, batch     4 | loss: 2.5078983Losses:  2.5788557529449463 0.5252510905265808 0.8551997542381287
MemoryTrain:  epoch  7, batch     5 | loss: 2.5788558Losses:  2.660158395767212 0.48150187730789185 0.7146325707435608
MemoryTrain:  epoch  7, batch     6 | loss: 2.6601584Losses:  2.9563775062561035 0.5160951018333435 0.9187307357788086
MemoryTrain:  epoch  7, batch     7 | loss: 2.9563775Losses:  2.6643974781036377 0.4939674139022827 0.9665979146957397
MemoryTrain:  epoch  7, batch     8 | loss: 2.6643975Losses:  2.8033390045166016 0.555795431137085 0.7826777696609497
MemoryTrain:  epoch  7, batch     9 | loss: 2.8033390Losses:  2.4614343643188477 0.2530267536640167 0.8886248469352722
MemoryTrain:  epoch  7, batch    10 | loss: 2.4614344Losses:  2.8396854400634766 0.7475128769874573 0.8412661552429199
MemoryTrain:  epoch  7, batch    11 | loss: 2.8396854Losses:  2.3337743282318115 -0.0 1.0607613325119019
MemoryTrain:  epoch  7, batch    12 | loss: 2.3337743Losses:  1.2839497327804565 -0.0 0.09399858117103577
MemoryTrain:  epoch  7, batch    13 | loss: 1.2839497Losses:  2.8219902515411377 0.7257487773895264 0.7777402997016907
MemoryTrain:  epoch  8, batch     0 | loss: 2.8219903Losses:  2.8351552486419678 0.47060737013816833 0.9629863500595093
MemoryTrain:  epoch  8, batch     1 | loss: 2.8351552Losses:  2.406782388687134 0.4850066900253296 0.725058376789093
MemoryTrain:  epoch  8, batch     2 | loss: 2.4067824Losses:  2.7116284370422363 0.482160747051239 0.9626613855361938
MemoryTrain:  epoch  8, batch     3 | loss: 2.7116284Losses:  2.4086079597473145 0.21671025454998016 0.9298796057701111
MemoryTrain:  epoch  8, batch     4 | loss: 2.4086080Losses:  2.2769477367401123 -0.0 0.9807375073432922
MemoryTrain:  epoch  8, batch     5 | loss: 2.2769477Losses:  2.5694684982299805 0.4772334396839142 0.8949028849601746
MemoryTrain:  epoch  8, batch     6 | loss: 2.5694685Losses:  2.4180684089660645 0.25579988956451416 0.9164263606071472
MemoryTrain:  epoch  8, batch     7 | loss: 2.4180684Losses:  2.904733180999756 0.7580364346504211 0.9717525243759155
MemoryTrain:  epoch  8, batch     8 | loss: 2.9047332Losses:  2.2547199726104736 -0.0 0.8614148497581482
MemoryTrain:  epoch  8, batch     9 | loss: 2.2547200Losses:  2.6843035221099854 0.5023798942565918 0.9655195474624634
MemoryTrain:  epoch  8, batch    10 | loss: 2.6843035Losses:  2.332740306854248 -0.0 1.008984923362732
MemoryTrain:  epoch  8, batch    11 | loss: 2.3327403Losses:  2.486983299255371 0.25647056102752686 0.9803766012191772
MemoryTrain:  epoch  8, batch    12 | loss: 2.4869833Losses:  1.4564481973648071 -0.0 0.11575903743505478
MemoryTrain:  epoch  8, batch    13 | loss: 1.4564482Losses:  2.6683993339538574 0.4909515380859375 0.953512966632843
MemoryTrain:  epoch  9, batch     0 | loss: 2.6683993Losses:  2.2865679264068604 -0.0 1.0523486137390137
MemoryTrain:  epoch  9, batch     1 | loss: 2.2865679Losses:  2.312018394470215 -0.0 1.0520427227020264
MemoryTrain:  epoch  9, batch     2 | loss: 2.3120184Losses:  2.513720750808716 0.24928928911685944 0.9598246812820435
MemoryTrain:  epoch  9, batch     3 | loss: 2.5137208Losses:  2.398582696914673 0.23618295788764954 0.9709668159484863
MemoryTrain:  epoch  9, batch     4 | loss: 2.3985827Losses:  2.616173267364502 0.5128830075263977 0.8738370537757874
MemoryTrain:  epoch  9, batch     5 | loss: 2.6161733Losses:  2.3093204498291016 -0.0 1.0014865398406982
MemoryTrain:  epoch  9, batch     6 | loss: 2.3093204Losses:  2.4917140007019043 0.22886735200881958 0.9594212174415588
MemoryTrain:  epoch  9, batch     7 | loss: 2.4917140Losses:  2.9643993377685547 0.8174318075180054 0.9078220725059509
MemoryTrain:  epoch  9, batch     8 | loss: 2.9643993Losses:  2.9535670280456543 0.8692919015884399 0.898725152015686
MemoryTrain:  epoch  9, batch     9 | loss: 2.9535670Losses:  2.6589910984039307 0.5089478492736816 0.8501973748207092
MemoryTrain:  epoch  9, batch    10 | loss: 2.6589911Losses:  2.179718494415283 -0.0 0.8917373418807983
MemoryTrain:  epoch  9, batch    11 | loss: 2.1797185Losses:  2.84149169921875 0.7562894821166992 0.8122555017471313
MemoryTrain:  epoch  9, batch    12 | loss: 2.8414917Losses:  1.4098105430603027 -0.0 0.21289244294166565
MemoryTrain:  epoch  9, batch    13 | loss: 1.4098105
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 41.67%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 40.62%   [EVAL] batch:    4 | acc: 56.25%,  total acc: 43.75%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 48.21%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 55.56%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 55.68%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 56.77%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 57.69%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 57.59%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 57.92%   [EVAL] batch:   15 | acc: 56.25%,  total acc: 57.81%   [EVAL] batch:   16 | acc: 56.25%,  total acc: 57.72%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 58.68%   [EVAL] batch:   18 | acc: 93.75%,  total acc: 60.53%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 61.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 63.69%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 65.34%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 67.71%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:   25 | acc: 56.25%,  total acc: 68.51%   [EVAL] batch:   26 | acc: 43.75%,  total acc: 67.59%   [EVAL] batch:   27 | acc: 68.75%,  total acc: 67.63%   [EVAL] batch:   28 | acc: 75.00%,  total acc: 67.89%   [EVAL] batch:   29 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:   30 | acc: 62.50%,  total acc: 67.54%   [EVAL] batch:   31 | acc: 68.75%,  total acc: 67.58%   [EVAL] batch:   32 | acc: 68.75%,  total acc: 67.61%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 65.99%   [EVAL] batch:   34 | acc: 37.50%,  total acc: 65.18%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 63.89%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 62.67%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 63.16%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 63.62%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 64.22%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 64.79%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 65.48%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 65.84%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 66.19%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 66.53%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 66.85%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 67.15%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 67.32%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 67.60%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 68.63%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 69.23%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 69.81%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 70.37%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 70.91%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 71.43%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 71.93%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 72.20%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 72.67%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 73.12%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 73.46%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.51%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 80.21%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 67.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 64.77%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 63.54%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 68.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 70.31%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 71.69%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.34%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 74.69%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 75.30%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 75.57%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 75.27%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 75.52%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 75.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.31%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 77.90%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.45%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 80.47%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 81.06%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 80.71%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 81.08%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 81.74%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.21%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.66%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.93%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 83.18%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 82.99%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.24%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 83.06%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 82.88%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 82.58%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 82.16%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 82.27%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 82.12%   [EVAL] batch:   50 | acc: 56.25%,  total acc: 81.62%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 81.61%   [EVAL] batch:   52 | acc: 68.75%,  total acc: 81.37%   [EVAL] batch:   53 | acc: 37.50%,  total acc: 80.56%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 80.11%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 79.35%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 79.06%   [EVAL] batch:   57 | acc: 25.00%,  total acc: 78.12%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 77.44%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 77.50%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 77.46%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 77.22%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 76.69%   [EVAL] batch:   63 | acc: 43.75%,  total acc: 76.17%   [EVAL] batch:   64 | acc: 12.50%,  total acc: 75.19%   [EVAL] batch:   65 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:   66 | acc: 31.25%,  total acc: 74.35%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 73.90%   [EVAL] batch:   68 | acc: 62.50%,  total acc: 73.73%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 74.02%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 74.21%   [EVAL] batch:   71 | acc: 81.25%,  total acc: 74.31%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 74.66%   [EVAL] batch:   73 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 75.17%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 74.51%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 73.70%   [EVAL] batch:   77 | acc: 31.25%,  total acc: 73.16%   [EVAL] batch:   78 | acc: 31.25%,  total acc: 72.63%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 72.27%   [EVAL] batch:   80 | acc: 18.75%,  total acc: 71.60%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 71.57%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 71.91%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 72.25%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 72.50%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 72.82%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 73.06%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:   88 | acc: 25.00%,  total acc: 72.54%   [EVAL] batch:   89 | acc: 25.00%,  total acc: 72.01%   [EVAL] batch:   90 | acc: 25.00%,  total acc: 71.50%   [EVAL] batch:   91 | acc: 43.75%,  total acc: 71.20%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 70.50%   [EVAL] batch:   93 | acc: 50.00%,  total acc: 70.28%   [EVAL] batch:   94 | acc: 87.50%,  total acc: 70.46%   [EVAL] batch:   95 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:   96 | acc: 93.75%,  total acc: 70.88%   [EVAL] batch:   97 | acc: 81.25%,  total acc: 70.98%   [EVAL] batch:   98 | acc: 81.25%,  total acc: 71.09%   [EVAL] batch:   99 | acc: 87.50%,  total acc: 71.25%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 71.23%   [EVAL] batch:  101 | acc: 81.25%,  total acc: 71.32%   [EVAL] batch:  102 | acc: 50.00%,  total acc: 71.12%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 71.03%   [EVAL] batch:  104 | acc: 68.75%,  total acc: 71.01%   [EVAL] batch:  105 | acc: 81.25%,  total acc: 71.11%   [EVAL] batch:  106 | acc: 31.25%,  total acc: 70.74%   [EVAL] batch:  107 | acc: 12.50%,  total acc: 70.20%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 69.72%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 69.26%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 68.86%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 68.47%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 68.20%   [EVAL] batch:  113 | acc: 25.00%,  total acc: 67.82%   [EVAL] batch:  114 | acc: 56.25%,  total acc: 67.72%   [EVAL] batch:  115 | acc: 43.75%,  total acc: 67.51%   [EVAL] batch:  116 | acc: 37.50%,  total acc: 67.25%   [EVAL] batch:  117 | acc: 50.00%,  total acc: 67.11%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 67.12%   [EVAL] batch:  119 | acc: 93.75%,  total acc: 67.34%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 67.56%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 67.67%   [EVAL] batch:  122 | acc: 62.50%,  total acc: 67.63%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 67.74%   [EVAL] batch:  124 | acc: 81.25%,  total acc: 67.85%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 67.36%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 66.88%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 66.36%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 65.89%   [EVAL] batch:  129 | acc: 6.25%,  total acc: 65.43%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 64.98%   [EVAL] batch:  131 | acc: 43.75%,  total acc: 64.82%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 64.80%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 64.88%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 65.00%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 65.17%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 65.24%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 65.22%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 64.97%   [EVAL] batch:  139 | acc: 37.50%,  total acc: 64.78%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 64.45%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 64.30%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 64.12%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 64.02%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 64.27%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 65.18%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 65.33%   [EVAL] batch:  150 | acc: 100.00%,  total acc: 65.56%   [EVAL] batch:  151 | acc: 100.00%,  total acc: 65.79%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 66.01%   [EVAL] batch:  153 | acc: 100.00%,  total acc: 66.23%   [EVAL] batch:  154 | acc: 100.00%,  total acc: 66.45%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 66.63%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 67.05%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 67.22%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 67.38%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 67.55%   [EVAL] batch:  161 | acc: 100.00%,  total acc: 67.75%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 67.45%   [EVAL] batch:  164 | acc: 18.75%,  total acc: 67.16%   [EVAL] batch:  165 | acc: 12.50%,  total acc: 66.83%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 66.54%   [EVAL] batch:  167 | acc: 12.50%,  total acc: 66.22%   [EVAL] batch:  168 | acc: 50.00%,  total acc: 66.12%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 66.18%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 66.26%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 66.32%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 66.33%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 66.27%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 66.29%   [EVAL] batch:  175 | acc: 37.50%,  total acc: 66.12%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 66.00%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 65.80%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 65.71%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 65.66%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 65.47%   [EVAL] batch:  181 | acc: 50.00%,  total acc: 65.38%   [EVAL] batch:  182 | acc: 62.50%,  total acc: 65.37%   [EVAL] batch:  183 | acc: 43.75%,  total acc: 65.25%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 65.27%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 65.32%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 65.37%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 65.36%   [EVAL] batch:  188 | acc: 75.00%,  total acc: 65.41%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:  190 | acc: 75.00%,  total acc: 65.51%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 65.56%   [EVAL] batch:  192 | acc: 68.75%,  total acc: 65.58%   [EVAL] batch:  193 | acc: 62.50%,  total acc: 65.56%   [EVAL] batch:  194 | acc: 93.75%,  total acc: 65.71%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 65.75%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 65.70%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 65.78%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 65.91%   [EVAL] batch:  200 | acc: 93.75%,  total acc: 66.04%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:  202 | acc: 75.00%,  total acc: 66.16%   [EVAL] batch:  203 | acc: 81.25%,  total acc: 66.24%   [EVAL] batch:  204 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:  205 | acc: 93.75%,  total acc: 66.44%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 66.27%   [EVAL] batch:  207 | acc: 25.00%,  total acc: 66.08%   [EVAL] batch:  208 | acc: 43.75%,  total acc: 65.97%   [EVAL] batch:  209 | acc: 25.00%,  total acc: 65.77%   [EVAL] batch:  210 | acc: 31.25%,  total acc: 65.61%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 65.48%   [EVAL] batch:  212 | acc: 43.75%,  total acc: 65.38%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 65.54%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 65.70%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 65.83%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 66.14%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 66.57%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 66.72%   [EVAL] batch:  222 | acc: 100.00%,  total acc: 66.87%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 67.02%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 67.17%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 67.17%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 67.18%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 67.19%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 67.22%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 67.34%   [EVAL] batch:  230 | acc: 68.75%,  total acc: 67.34%   [EVAL] batch:  231 | acc: 37.50%,  total acc: 67.21%   [EVAL] batch:  232 | acc: 18.75%,  total acc: 67.01%   [EVAL] batch:  233 | acc: 25.00%,  total acc: 66.83%   [EVAL] batch:  234 | acc: 31.25%,  total acc: 66.68%   [EVAL] batch:  235 | acc: 37.50%,  total acc: 66.55%   [EVAL] batch:  236 | acc: 12.50%,  total acc: 66.32%   [EVAL] batch:  237 | acc: 50.00%,  total acc: 66.26%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 66.34%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 66.48%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 66.57%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 66.66%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 66.74%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 66.80%   [EVAL] batch:  244 | acc: 43.75%,  total acc: 66.71%   [EVAL] batch:  245 | acc: 31.25%,  total acc: 66.57%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 66.37%   [EVAL] batch:  247 | acc: 31.25%,  total acc: 66.23%   [EVAL] batch:  248 | acc: 18.75%,  total acc: 66.04%   [EVAL] batch:  249 | acc: 43.75%,  total acc: 65.95%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 66.06%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 66.15%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 66.23%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 66.29%   [EVAL] batch:  254 | acc: 87.50%,  total acc: 66.37%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 66.50%   [EVAL] batch:  256 | acc: 68.75%,  total acc: 66.51%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 66.55%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 66.58%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 66.66%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 66.71%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 66.77%   [EVAL] batch:  262 | acc: 81.25%,  total acc: 66.83%   [EVAL] batch:  263 | acc: 87.50%,  total acc: 66.90%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 66.93%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 66.96%   [EVAL] batch:  266 | acc: 62.50%,  total acc: 66.95%   [EVAL] batch:  267 | acc: 68.75%,  total acc: 66.95%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 66.96%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 66.81%   [EVAL] batch:  270 | acc: 18.75%,  total acc: 66.63%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 66.45%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 66.21%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 66.08%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 65.91%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 66.03%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 66.52%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.64%   [EVAL] batch:  281 | acc: 62.50%,  total acc: 66.62%   [EVAL] batch:  282 | acc: 81.25%,  total acc: 66.67%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 66.68%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 66.69%   [EVAL] batch:  285 | acc: 62.50%,  total acc: 66.67%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 66.64%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 66.64%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 66.76%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 66.99%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 67.10%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 67.30%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 67.42%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 67.74%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 67.85%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 67.94%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 68.00%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 68.17%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 68.26%   [EVAL] batch:  304 | acc: 81.25%,  total acc: 68.30%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 68.38%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 68.46%   [EVAL] batch:  307 | acc: 93.75%,  total acc: 68.55%   [EVAL] batch:  308 | acc: 93.75%,  total acc: 68.63%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 68.73%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 68.83%   [EVAL] batch:  311 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  312 | acc: 75.00%,  total acc: 68.93%   [EVAL] batch:  313 | acc: 75.00%,  total acc: 68.95%   [EVAL] batch:  314 | acc: 56.25%,  total acc: 68.91%   [EVAL] batch:  315 | acc: 68.75%,  total acc: 68.91%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 68.87%   [EVAL] batch:  317 | acc: 93.75%,  total acc: 68.95%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 68.99%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 69.08%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 69.18%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 69.22%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 69.27%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 69.35%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 69.40%   [EVAL] batch:  325 | acc: 75.00%,  total acc: 69.42%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 69.38%   [EVAL] batch:  327 | acc: 68.75%,  total acc: 69.38%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 69.40%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 69.41%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 69.43%   [EVAL] batch:  331 | acc: 75.00%,  total acc: 69.45%   [EVAL] batch:  332 | acc: 68.75%,  total acc: 69.44%   [EVAL] batch:  333 | acc: 81.25%,  total acc: 69.48%   [EVAL] batch:  334 | acc: 68.75%,  total acc: 69.48%   [EVAL] batch:  335 | acc: 75.00%,  total acc: 69.49%   [EVAL] batch:  336 | acc: 62.50%,  total acc: 69.47%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 69.51%   [EVAL] batch:  338 | acc: 62.50%,  total acc: 69.49%   [EVAL] batch:  339 | acc: 68.75%,  total acc: 69.49%   [EVAL] batch:  340 | acc: 68.75%,  total acc: 69.48%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 69.48%   [EVAL] batch:  342 | acc: 37.50%,  total acc: 69.39%   [EVAL] batch:  343 | acc: 50.00%,  total acc: 69.33%   [EVAL] batch:  344 | acc: 75.00%,  total acc: 69.35%   [EVAL] batch:  345 | acc: 37.50%,  total acc: 69.26%   [EVAL] batch:  346 | acc: 68.75%,  total acc: 69.25%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 69.23%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 69.25%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 69.20%   [EVAL] batch:  350 | acc: 68.75%,  total acc: 69.20%   [EVAL] batch:  351 | acc: 81.25%,  total acc: 69.23%   [EVAL] batch:  352 | acc: 93.75%,  total acc: 69.30%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 69.35%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:  355 | acc: 93.75%,  total acc: 69.45%   [EVAL] batch:  356 | acc: 37.50%,  total acc: 69.36%   [EVAL] batch:  357 | acc: 18.75%,  total acc: 69.22%   [EVAL] batch:  358 | acc: 31.25%,  total acc: 69.12%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 69.05%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 68.94%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 68.89%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 68.87%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 68.96%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 69.13%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 69.21%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 69.29%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 69.46%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 69.52%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 69.57%   [EVAL] batch:  372 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 69.77%   [EVAL] batch:  375 | acc: 31.25%,  total acc: 69.66%   [EVAL] batch:  376 | acc: 43.75%,  total acc: 69.60%   [EVAL] batch:  377 | acc: 50.00%,  total acc: 69.54%   [EVAL] batch:  378 | acc: 37.50%,  total acc: 69.46%   [EVAL] batch:  379 | acc: 56.25%,  total acc: 69.42%   [EVAL] batch:  380 | acc: 50.00%,  total acc: 69.37%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 69.37%   [EVAL] batch:  382 | acc: 87.50%,  total acc: 69.42%   [EVAL] batch:  383 | acc: 75.00%,  total acc: 69.43%   [EVAL] batch:  384 | acc: 62.50%,  total acc: 69.42%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 69.37%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 69.36%   [EVAL] batch:  387 | acc: 68.75%,  total acc: 69.36%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 69.33%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 69.31%   [EVAL] batch:  390 | acc: 56.25%,  total acc: 69.28%   [EVAL] batch:  391 | acc: 56.25%,  total acc: 69.24%   [EVAL] batch:  392 | acc: 75.00%,  total acc: 69.26%   [EVAL] batch:  393 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 69.37%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:  397 | acc: 93.75%,  total acc: 69.58%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 69.64%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 69.72%   [EVAL] batch:  400 | acc: 56.25%,  total acc: 69.69%   [EVAL] batch:  401 | acc: 43.75%,  total acc: 69.62%   [EVAL] batch:  402 | acc: 68.75%,  total acc: 69.62%   [EVAL] batch:  403 | acc: 75.00%,  total acc: 69.63%   [EVAL] batch:  404 | acc: 62.50%,  total acc: 69.61%   [EVAL] batch:  405 | acc: 62.50%,  total acc: 69.60%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 69.59%   [EVAL] batch:  407 | acc: 68.75%,  total acc: 69.59%   [EVAL] batch:  408 | acc: 12.50%,  total acc: 69.45%   [EVAL] batch:  409 | acc: 37.50%,  total acc: 69.38%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 69.25%   [EVAL] batch:  411 | acc: 18.75%,  total acc: 69.13%   [EVAL] batch:  412 | acc: 81.25%,  total acc: 69.16%   [EVAL] batch:  413 | acc: 81.25%,  total acc: 69.19%   [EVAL] batch:  414 | acc: 87.50%,  total acc: 69.23%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 69.28%   [EVAL] batch:  416 | acc: 93.75%,  total acc: 69.33%   [EVAL] batch:  417 | acc: 81.25%,  total acc: 69.36%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 69.39%   [EVAL] batch:  419 | acc: 81.25%,  total acc: 69.42%   [EVAL] batch:  420 | acc: 81.25%,  total acc: 69.45%   [EVAL] batch:  421 | acc: 81.25%,  total acc: 69.48%   [EVAL] batch:  422 | acc: 75.00%,  total acc: 69.49%   [EVAL] batch:  423 | acc: 81.25%,  total acc: 69.52%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 69.56%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 69.63%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 69.70%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 69.77%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 69.84%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 69.91%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 69.98%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 70.05%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 70.09%   [EVAL] batch:  433 | acc: 100.00%,  total acc: 70.16%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 70.23%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 70.28%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 70.35%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 70.31%   
cur_acc:  ['0.9484', '0.7411', '0.7976', '0.6895', '0.8254', '0.7530', '0.7351']
his_acc:  ['0.9484', '0.8300', '0.7912', '0.7218', '0.7238', '0.7145', '0.7031']
Clustering into  39  clusters
Clusters:  [ 5  0 26 16  1  1 37  6  0 11  4 25 29  7  3  0 13  5 32  4 31 19 24 34
 27 19 17  8 11  5  2  8 30 14 23 19 22 24  4 17 16  5  5  0 35 10 15 38
 18  0 24  6  6  1 36 11  5  1 19 19  2 12  7  4  6 20  9 10 33  8  1 14
 13 36  3 21 28  9  8  0]
Losses:  10.268172264099121 3.7628235816955566 0.8136547207832336
CurrentTrain: epoch  0, batch     0 | loss: 10.2681723Losses:  13.233382225036621 5.693405628204346 0.5993620753288269
CurrentTrain: epoch  0, batch     1 | loss: 13.2333822Losses:  11.046762466430664 3.996021270751953 0.7585196495056152
CurrentTrain: epoch  0, batch     2 | loss: 11.0467625Losses:  4.778538703918457 -0.0 0.11090660840272903
CurrentTrain: epoch  0, batch     3 | loss: 4.7785387Losses:  12.968701362609863 5.97136116027832 0.6233455538749695
CurrentTrain: epoch  1, batch     0 | loss: 12.9687014Losses:  10.12930679321289 3.993161916732788 0.6737891435623169
CurrentTrain: epoch  1, batch     1 | loss: 10.1293068Losses:  8.519414901733398 3.6073403358459473 0.7772582173347473
CurrentTrain: epoch  1, batch     2 | loss: 8.5194149Losses:  2.6711337566375732 -0.0 0.1146046370267868
CurrentTrain: epoch  1, batch     3 | loss: 2.6711338Losses:  9.06010913848877 3.7374978065490723 0.67763352394104
CurrentTrain: epoch  2, batch     0 | loss: 9.0601091Losses:  9.408422470092773 3.9750256538391113 0.8187747001647949
CurrentTrain: epoch  2, batch     1 | loss: 9.4084225Losses:  7.303246021270752 2.2521770000457764 0.8135979771614075
CurrentTrain: epoch  2, batch     2 | loss: 7.3032460Losses:  7.109125137329102 -0.0 -0.0
CurrentTrain: epoch  2, batch     3 | loss: 7.1091251Losses:  9.378837585449219 4.117466449737549 0.6982893943786621
CurrentTrain: epoch  3, batch     0 | loss: 9.3788376Losses:  8.21008014678955 3.4804129600524902 0.7369167804718018
CurrentTrain: epoch  3, batch     1 | loss: 8.2100801Losses:  7.100413799285889 2.236888885498047 0.8180872201919556
CurrentTrain: epoch  3, batch     2 | loss: 7.1004138Losses:  5.903281211853027 -0.0 0.11743265390396118
CurrentTrain: epoch  3, batch     3 | loss: 5.9032812Losses:  8.434873580932617 4.237287521362305 0.6007087230682373
CurrentTrain: epoch  4, batch     0 | loss: 8.4348736Losses:  7.88351583480835 3.3502156734466553 0.7931385636329651
CurrentTrain: epoch  4, batch     1 | loss: 7.8835158Losses:  8.57363510131836 3.426842212677002 0.7676795125007629
CurrentTrain: epoch  4, batch     2 | loss: 8.5736351Losses:  2.9823551177978516 -0.0 0.0896831601858139
CurrentTrain: epoch  4, batch     3 | loss: 2.9823551Losses:  7.361464023590088 2.979612350463867 0.7672794461250305
CurrentTrain: epoch  5, batch     0 | loss: 7.3614640Losses:  9.539261817932129 4.324202060699463 0.7522576451301575
CurrentTrain: epoch  5, batch     1 | loss: 9.5392618Losses:  6.3218994140625 3.0131993293762207 0.6919777393341064
CurrentTrain: epoch  5, batch     2 | loss: 6.3218994Losses:  3.7560460567474365 -0.0 0.10819116234779358
CurrentTrain: epoch  5, batch     3 | loss: 3.7560461Losses:  6.346026420593262 3.045518636703491 0.7067852020263672
CurrentTrain: epoch  6, batch     0 | loss: 6.3460264Losses:  7.516753673553467 3.1673121452331543 0.6655502319335938
CurrentTrain: epoch  6, batch     1 | loss: 7.5167537Losses:  7.399016857147217 3.4150521755218506 0.7022472620010376
CurrentTrain: epoch  6, batch     2 | loss: 7.3990169Losses:  5.435365676879883 -0.0 0.08997678011655807
CurrentTrain: epoch  6, batch     3 | loss: 5.4353657Losses:  5.846615314483643 1.988950490951538 0.750127375125885
CurrentTrain: epoch  7, batch     0 | loss: 5.8466153Losses:  7.904213905334473 4.472159385681152 0.7532303333282471
CurrentTrain: epoch  7, batch     1 | loss: 7.9042139Losses:  7.11234188079834 3.4516937732696533 0.6952657103538513
CurrentTrain: epoch  7, batch     2 | loss: 7.1123419Losses:  2.7776615619659424 -0.0 0.22030167281627655
CurrentTrain: epoch  7, batch     3 | loss: 2.7776616Losses:  5.741300582885742 2.511430501937866 0.6655312180519104
CurrentTrain: epoch  8, batch     0 | loss: 5.7413006Losses:  6.429266929626465 3.043199062347412 0.7955040335655212
CurrentTrain: epoch  8, batch     1 | loss: 6.4292669Losses:  6.375634670257568 3.027803421020508 0.6857202649116516
CurrentTrain: epoch  8, batch     2 | loss: 6.3756347Losses:  4.308096408843994 -0.0 0.16885259747505188
CurrentTrain: epoch  8, batch     3 | loss: 4.3080964Losses:  4.923840045928955 1.6596333980560303 0.788242757320404
CurrentTrain: epoch  9, batch     0 | loss: 4.9238400Losses:  5.638888359069824 2.36423921585083 0.7538761496543884
CurrentTrain: epoch  9, batch     1 | loss: 5.6388884Losses:  5.161835193634033 1.8777167797088623 0.8089134097099304
CurrentTrain: epoch  9, batch     2 | loss: 5.1618352Losses:  2.2712342739105225 -0.0 0.0889342725276947
CurrentTrain: epoch  9, batch     3 | loss: 2.2712343
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from all over the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, held in paris in 2015, saw numerous world leaders and activists come together to discuss global warming and its impacts.  
Head Entity: united nations climate change conference  
Tail Entity: paris agreement  

Relation: participant of  
Context: the 2022 fifa world cup in qatar featured teams from around the globe competing for the prestigious trophy, with fans cheering for their favorite nations.  
Head Entity: 2022 fifa world cup  
Tail Entity: qatar
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: the festival opened with " whiplash " directed by damien chazelle and closed with musical drama " rudderless " directed by william h. macy .
Head Entity: whiplash
Tail Entity: damien chazelle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the critically acclaimed series "breaking bad" was brought to life by director vince gilligan, while the spin-off "better call saul" was directed by peter gould.  
Head Entity: breaking bad  
Tail Entity: vince gilligan  

Relation: director  
Context: the animated feature "spider-man: into the spider-verse" was directed by bob persichetti, peter ramsey, and rodney rothman, showcasing a unique visual style.  
Head Entity: spider-man: into the spider-verse  
Tail Entity: bob persichetti  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game developer, Bungie, announced that it had become an independent studio, having previously been owned by Microsoft for several years.  
Head Entity: Bungie  
Tail Entity: Microsoft  

Relation: owned by  
Context: The luxury fashion brand Gucci, known for its high-end products, is owned by the French conglomerate Kering, which also owns several other prestigious labels.  
Head Entity: Gucci  
Tail Entity: Kering  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive the vehicle.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The leaf is an essential part of the plant, playing a vital role in photosynthesis and nutrient absorption.  
Head Entity: leaf  
Tail Entity: plant  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts various events and is home to the local art gallery.  
Head Entity: cultural center  
Tail Entity: local art gallery  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant community hub, serving as the headquarters for several non-profit organizations.  
Head Entity: community hub  
Tail Entity: non-profit organizations  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: Frank Lloyd Wright is renowned for his innovative designs, including the famous Fallingwater house, which harmonizes beautifully with its natural surroundings.  
Head Entity: Fallingwater  
Tail Entity: Frank Lloyd Wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: cyrus alexander was born in pennsylvania , and his family soon moved to illinois .
Head Entity: cyrus alexander
Tail Entity: illinois
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york for several years, emily decided to relocate to california for a fresh start.  
Head Entity: emily  
Tail Entity: california  

Relation: residence  
Context: during his childhood, michael spent most of his time in texas before moving to florida for college.  
Head Entity: michael  
Tail Entity: florida  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: the exhibition was shown for the public at kulturen i lund , regionmuseet kristianstad and falkenbergs museum .
Head Entity: kulturen
Tail Entity: lund
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The conference will be held at the Grand Hotel in Stockholm, which is known for its luxurious accommodations.  
Head Entity: Grand Hotel  
Tail Entity: Stockholm  

Relation: location  
Context: The historic battle took place near the town of Gettysburg, which is now a popular tourist destination.  
Head Entity: Gettysburg  
Tail Entity: town  
Losses:  3.5468239784240723 0.26989272236824036 0.9093529582023621
MemoryTrain:  epoch  0, batch     0 | loss: 3.5468240Losses:  5.779644966125488 0.5082795023918152 0.8810701370239258
MemoryTrain:  epoch  0, batch     1 | loss: 5.7796450Losses:  4.7444233894348145 0.2361203283071518 1.038299560546875
MemoryTrain:  epoch  0, batch     2 | loss: 4.7444234Losses:  3.4200098514556885 0.2714783251285553 1.0476231575012207
MemoryTrain:  epoch  0, batch     3 | loss: 3.4200099Losses:  3.7787024974823 -0.0 1.031681776046753
MemoryTrain:  epoch  0, batch     4 | loss: 3.7787025Losses:  3.383589744567871 0.5745065212249756 0.9649824500083923
MemoryTrain:  epoch  0, batch     5 | loss: 3.3835897Losses:  3.3744280338287354 0.720584511756897 0.7903901934623718
MemoryTrain:  epoch  0, batch     6 | loss: 3.3744280Losses:  3.7762601375579834 0.2699010372161865 0.9196548461914062
MemoryTrain:  epoch  0, batch     7 | loss: 3.7762601Losses:  4.139444828033447 -0.0 0.8599160313606262
MemoryTrain:  epoch  0, batch     8 | loss: 4.1394448Losses:  4.647093296051025 0.2525375485420227 0.953985333442688
MemoryTrain:  epoch  0, batch     9 | loss: 4.6470933Losses:  3.7427382469177246 0.24206489324569702 0.918089747428894
MemoryTrain:  epoch  0, batch    10 | loss: 3.7427382Losses:  3.457110643386841 0.2327757179737091 0.941164493560791
MemoryTrain:  epoch  0, batch    11 | loss: 3.4571106Losses:  3.6422457695007324 -0.0 0.9994915723800659
MemoryTrain:  epoch  0, batch    12 | loss: 3.6422458Losses:  3.5734708309173584 0.2643113136291504 1.0375394821166992
MemoryTrain:  epoch  0, batch    13 | loss: 3.5734708Losses:  4.371557712554932 0.7277698516845703 0.9014294743537903
MemoryTrain:  epoch  0, batch    14 | loss: 4.3715577Losses:  3.2096331119537354 0.2657942771911621 0.8778329491615295
MemoryTrain:  epoch  1, batch     0 | loss: 3.2096331Losses:  3.098386764526367 0.23625874519348145 1.0686231851577759
MemoryTrain:  epoch  1, batch     1 | loss: 3.0983868Losses:  3.0487499237060547 -0.0 1.0276211500167847
MemoryTrain:  epoch  1, batch     2 | loss: 3.0487499Losses:  3.0543172359466553 0.23222404718399048 0.898441731929779
MemoryTrain:  epoch  1, batch     3 | loss: 3.0543172Losses:  3.1082050800323486 0.48909467458724976 0.9609353542327881
MemoryTrain:  epoch  1, batch     4 | loss: 3.1082051Losses:  4.2921600341796875 0.7083297967910767 1.028688669204712
MemoryTrain:  epoch  1, batch     5 | loss: 4.2921600Losses:  3.6365809440612793 -0.0 1.0036333799362183
MemoryTrain:  epoch  1, batch     6 | loss: 3.6365809Losses:  2.988328695297241 0.5091920495033264 0.8471312522888184
MemoryTrain:  epoch  1, batch     7 | loss: 2.9883287Losses:  2.71498441696167 0.25286588072776794 0.8607555031776428
MemoryTrain:  epoch  1, batch     8 | loss: 2.7149844Losses:  4.434759616851807 0.24686264991760254 0.961912214756012
MemoryTrain:  epoch  1, batch     9 | loss: 4.4347596Losses:  4.954009056091309 0.7425215244293213 0.9478579759597778
MemoryTrain:  epoch  1, batch    10 | loss: 4.9540091Losses:  3.3898797035217285 -0.0 0.8883666396141052
MemoryTrain:  epoch  1, batch    11 | loss: 3.3898797Losses:  3.684365749359131 0.250299334526062 0.5725135803222656
MemoryTrain:  epoch  1, batch    12 | loss: 3.6843657Losses:  3.2301533222198486 0.5227462649345398 0.727782666683197
MemoryTrain:  epoch  1, batch    13 | loss: 3.2301533Losses:  4.030691623687744 -0.0 0.9243093729019165
MemoryTrain:  epoch  1, batch    14 | loss: 4.0306916Losses:  3.211087226867676 -0.0 1.0077190399169922
MemoryTrain:  epoch  2, batch     0 | loss: 3.2110872Losses:  4.8710856437683105 0.45929527282714844 0.9021309614181519
MemoryTrain:  epoch  2, batch     1 | loss: 4.8710856Losses:  3.7131147384643555 0.2535022795200348 0.855620265007019
MemoryTrain:  epoch  2, batch     2 | loss: 3.7131147Losses:  2.6246399879455566 -0.0 1.054975986480713
MemoryTrain:  epoch  2, batch     3 | loss: 2.6246400Losses:  3.1591057777404785 0.2449432611465454 1.000631332397461
MemoryTrain:  epoch  2, batch     4 | loss: 3.1591058Losses:  2.9859156608581543 0.24500998854637146 0.9109637141227722
MemoryTrain:  epoch  2, batch     5 | loss: 2.9859157Losses:  3.110954761505127 0.5052265524864197 0.9089245796203613
MemoryTrain:  epoch  2, batch     6 | loss: 3.1109548Losses:  2.9237942695617676 0.2393125593662262 1.0202172994613647
MemoryTrain:  epoch  2, batch     7 | loss: 2.9237943Losses:  2.427561044692993 -0.0 0.9081994891166687
MemoryTrain:  epoch  2, batch     8 | loss: 2.4275610Losses:  3.3906757831573486 0.24898295104503632 0.9659116864204407
MemoryTrain:  epoch  2, batch     9 | loss: 3.3906758Losses:  2.9963104724884033 -0.0 1.0375139713287354
MemoryTrain:  epoch  2, batch    10 | loss: 2.9963105Losses:  3.4048912525177 0.49422168731689453 0.9491374492645264
MemoryTrain:  epoch  2, batch    11 | loss: 3.4048913Losses:  2.8060121536254883 0.4781821668148041 0.9978727102279663
MemoryTrain:  epoch  2, batch    12 | loss: 2.8060122Losses:  3.553860902786255 -0.0 1.0058486461639404
MemoryTrain:  epoch  2, batch    13 | loss: 3.5538609Losses:  2.5664098262786865 0.5074214339256287 0.7751405835151672
MemoryTrain:  epoch  2, batch    14 | loss: 2.5664098Losses:  3.0963308811187744 0.49460434913635254 0.9198732376098633
MemoryTrain:  epoch  3, batch     0 | loss: 3.0963309Losses:  3.3978934288024902 0.7406492829322815 0.9702616930007935
MemoryTrain:  epoch  3, batch     1 | loss: 3.3978934Losses:  3.1631977558135986 0.4567433297634125 0.9715301990509033
MemoryTrain:  epoch  3, batch     2 | loss: 3.1631978Losses:  3.026841163635254 0.7402819395065308 0.8443530797958374
MemoryTrain:  epoch  3, batch     3 | loss: 3.0268412Losses:  3.6303043365478516 0.24331042170524597 0.9915632605552673
MemoryTrain:  epoch  3, batch     4 | loss: 3.6303043Losses:  2.9095168113708496 0.7188936471939087 0.9101585149765015
MemoryTrain:  epoch  3, batch     5 | loss: 2.9095168Losses:  2.540069341659546 0.25699397921562195 0.9403836131095886
MemoryTrain:  epoch  3, batch     6 | loss: 2.5400693Losses:  4.32076358795166 0.5108166933059692 0.9200214743614197
MemoryTrain:  epoch  3, batch     7 | loss: 4.3207636Losses:  2.700967788696289 -0.0 0.9661733508110046
MemoryTrain:  epoch  3, batch     8 | loss: 2.7009678Losses:  2.1190595626831055 -0.0 0.7943816184997559
MemoryTrain:  epoch  3, batch     9 | loss: 2.1190596Losses:  2.7553353309631348 -0.0 1.0602327585220337
MemoryTrain:  epoch  3, batch    10 | loss: 2.7553353Losses:  3.411778450012207 0.2465047538280487 0.9765830039978027
MemoryTrain:  epoch  3, batch    11 | loss: 3.4117785Losses:  2.552690029144287 -0.0 0.7974618077278137
MemoryTrain:  epoch  3, batch    12 | loss: 2.5526900Losses:  2.460966110229492 0.24520844221115112 0.8568817973136902
MemoryTrain:  epoch  3, batch    13 | loss: 2.4609661Losses:  3.0839805603027344 -0.0 0.9685512185096741
MemoryTrain:  epoch  3, batch    14 | loss: 3.0839806Losses:  2.568314790725708 -0.0 0.9493038654327393
MemoryTrain:  epoch  4, batch     0 | loss: 2.5683148Losses:  2.869067430496216 0.24897924065589905 0.9623945355415344
MemoryTrain:  epoch  4, batch     1 | loss: 2.8690674Losses:  2.633341073989868 0.24439793825149536 0.9773678183555603
MemoryTrain:  epoch  4, batch     2 | loss: 2.6333411Losses:  2.7214856147766113 0.3020060062408447 0.8503144979476929
MemoryTrain:  epoch  4, batch     3 | loss: 2.7214856Losses:  2.949828624725342 -0.0 1.0019549131393433
MemoryTrain:  epoch  4, batch     4 | loss: 2.9498286Losses:  2.9583792686462402 0.5107918381690979 0.7870569229125977
MemoryTrain:  epoch  4, batch     5 | loss: 2.9583793Losses:  2.479841709136963 -0.0 0.8999727964401245
MemoryTrain:  epoch  4, batch     6 | loss: 2.4798417Losses:  3.3610036373138428 0.5081082582473755 0.7821347117424011
MemoryTrain:  epoch  4, batch     7 | loss: 3.3610036Losses:  2.4527199268341064 -0.0 0.9493193030357361
MemoryTrain:  epoch  4, batch     8 | loss: 2.4527199Losses:  2.819439649581909 0.7625272274017334 0.8061787486076355
MemoryTrain:  epoch  4, batch     9 | loss: 2.8194396Losses:  3.3253204822540283 0.5308380126953125 0.9019162058830261
MemoryTrain:  epoch  4, batch    10 | loss: 3.3253205Losses:  3.470362901687622 0.6079713702201843 0.9208664894104004
MemoryTrain:  epoch  4, batch    11 | loss: 3.4703629Losses:  2.4739861488342285 -0.0 1.0215939283370972
MemoryTrain:  epoch  4, batch    12 | loss: 2.4739861Losses:  3.430055856704712 0.23500984907150269 0.9719181656837463
MemoryTrain:  epoch  4, batch    13 | loss: 3.4300559Losses:  2.8710854053497314 0.24742861092090607 0.9601163268089294
MemoryTrain:  epoch  4, batch    14 | loss: 2.8710854Losses:  2.706993579864502 0.2213144600391388 0.8890105485916138
MemoryTrain:  epoch  5, batch     0 | loss: 2.7069936Losses:  2.800271987915039 0.4815795123577118 0.8747328519821167
MemoryTrain:  epoch  5, batch     1 | loss: 2.8002720Losses:  3.0430665016174316 0.5149558186531067 0.8537474870681763
MemoryTrain:  epoch  5, batch     2 | loss: 3.0430665Losses:  2.1408634185791016 -0.0 0.9129748344421387
MemoryTrain:  epoch  5, batch     3 | loss: 2.1408634Losses:  2.8111915588378906 0.24110852181911469 0.9567223191261292
MemoryTrain:  epoch  5, batch     4 | loss: 2.8111916Losses:  3.0999057292938232 0.48804959654808044 0.9513412117958069
MemoryTrain:  epoch  5, batch     5 | loss: 3.0999057Losses:  2.939307689666748 0.24803981184959412 0.9815402626991272
MemoryTrain:  epoch  5, batch     6 | loss: 2.9393077Losses:  2.769669532775879 0.2481135129928589 0.941993772983551
MemoryTrain:  epoch  5, batch     7 | loss: 2.7696695Losses:  3.1326730251312256 0.2495376020669937 0.9729316830635071
MemoryTrain:  epoch  5, batch     8 | loss: 3.1326730Losses:  2.6016488075256348 -0.0 0.9806859493255615
MemoryTrain:  epoch  5, batch     9 | loss: 2.6016488Losses:  2.535661220550537 0.48649153113365173 0.7801026701927185
MemoryTrain:  epoch  5, batch    10 | loss: 2.5356612Losses:  2.6146016120910645 -0.0 0.9623776078224182
MemoryTrain:  epoch  5, batch    11 | loss: 2.6146016Losses:  2.9483530521392822 0.7659053802490234 0.8437519073486328
MemoryTrain:  epoch  5, batch    12 | loss: 2.9483531Losses:  2.6789445877075195 0.25401735305786133 1.0007950067520142
MemoryTrain:  epoch  5, batch    13 | loss: 2.6789446Losses:  2.723745346069336 0.25862497091293335 0.9691585302352905
MemoryTrain:  epoch  5, batch    14 | loss: 2.7237453Losses:  2.8904480934143066 0.2580541968345642 0.9146880507469177
MemoryTrain:  epoch  6, batch     0 | loss: 2.8904481Losses:  2.3605239391326904 -0.0 0.9906538128852844
MemoryTrain:  epoch  6, batch     1 | loss: 2.3605239Losses:  2.3510990142822266 -0.0 1.010475754737854
MemoryTrain:  epoch  6, batch     2 | loss: 2.3510990Losses:  2.2298336029052734 -0.0 0.8678504824638367
MemoryTrain:  epoch  6, batch     3 | loss: 2.2298336Losses:  2.336103916168213 -0.0 1.007178783416748
MemoryTrain:  epoch  6, batch     4 | loss: 2.3361039Losses:  2.5466747283935547 0.24255111813545227 1.0246140956878662
MemoryTrain:  epoch  6, batch     5 | loss: 2.5466747Losses:  3.158902168273926 0.5807285308837891 0.9581443071365356
MemoryTrain:  epoch  6, batch     6 | loss: 3.1589022Losses:  2.7870523929595947 0.50737464427948 0.968238890171051
MemoryTrain:  epoch  6, batch     7 | loss: 2.7870524Losses:  2.1608197689056396 -0.0 0.9642539620399475
MemoryTrain:  epoch  6, batch     8 | loss: 2.1608198Losses:  2.3689382076263428 0.24234215915203094 0.7862045168876648
MemoryTrain:  epoch  6, batch     9 | loss: 2.3689382Losses:  3.4688303470611572 0.48279666900634766 1.027933120727539
MemoryTrain:  epoch  6, batch    10 | loss: 3.4688303Losses:  2.4761552810668945 0.23778104782104492 0.9630557298660278
MemoryTrain:  epoch  6, batch    11 | loss: 2.4761553Losses:  2.855346202850342 0.25038135051727295 1.0086877346038818
MemoryTrain:  epoch  6, batch    12 | loss: 2.8553462Losses:  3.4694457054138184 0.3068978786468506 1.08473801612854
MemoryTrain:  epoch  6, batch    13 | loss: 3.4694457Losses:  2.4837594032287598 -0.0 1.0369338989257812
MemoryTrain:  epoch  6, batch    14 | loss: 2.4837594Losses:  2.4462995529174805 0.2391308844089508 0.9609040021896362
MemoryTrain:  epoch  7, batch     0 | loss: 2.4462996Losses:  2.4628756046295166 0.46452122926712036 0.8166193962097168
MemoryTrain:  epoch  7, batch     1 | loss: 2.4628756Losses:  2.937199115753174 0.3044193387031555 1.0293042659759521
MemoryTrain:  epoch  7, batch     2 | loss: 2.9371991Losses:  2.7658956050872803 -0.0 1.1262226104736328
MemoryTrain:  epoch  7, batch     3 | loss: 2.7658956Losses:  2.7495503425598145 0.5069848299026489 0.9583604335784912
MemoryTrain:  epoch  7, batch     4 | loss: 2.7495503Losses:  2.6689958572387695 -0.0 0.9796567559242249
MemoryTrain:  epoch  7, batch     5 | loss: 2.6689959Losses:  2.3633270263671875 -0.0 0.9555723667144775
MemoryTrain:  epoch  7, batch     6 | loss: 2.3633270Losses:  2.4758670330047607 0.2521212697029114 0.969982922077179
MemoryTrain:  epoch  7, batch     7 | loss: 2.4758670Losses:  2.384215831756592 -0.0 1.0242512226104736
MemoryTrain:  epoch  7, batch     8 | loss: 2.3842158Losses:  2.9081859588623047 0.5243340134620667 0.9660931825637817
MemoryTrain:  epoch  7, batch     9 | loss: 2.9081860Losses:  3.126370429992676 0.5162711143493652 0.9260810613632202
MemoryTrain:  epoch  7, batch    10 | loss: 3.1263704Losses:  2.9011950492858887 0.37640655040740967 0.9123473167419434
MemoryTrain:  epoch  7, batch    11 | loss: 2.9011950Losses:  2.019141912460327 -0.0 0.8458825945854187
MemoryTrain:  epoch  7, batch    12 | loss: 2.0191419Losses:  2.7104475498199463 0.4965898394584656 0.9765722155570984
MemoryTrain:  epoch  7, batch    13 | loss: 2.7104475Losses:  2.966825485229492 0.24687877297401428 0.9459446668624878
MemoryTrain:  epoch  7, batch    14 | loss: 2.9668255Losses:  2.4992995262145996 0.266899049282074 0.8949806094169617
MemoryTrain:  epoch  8, batch     0 | loss: 2.4992995Losses:  2.459052562713623 -0.0 0.9539971351623535
MemoryTrain:  epoch  8, batch     1 | loss: 2.4590526Losses:  2.893967866897583 0.504621148109436 0.9819537401199341
MemoryTrain:  epoch  8, batch     2 | loss: 2.8939679Losses:  2.5083765983581543 0.2653082013130188 0.8619999885559082
MemoryTrain:  epoch  8, batch     3 | loss: 2.5083766Losses:  2.62042498588562 -0.0 0.9631782174110413
MemoryTrain:  epoch  8, batch     4 | loss: 2.6204250Losses:  2.3773112297058105 0.23810932040214539 0.7787082195281982
MemoryTrain:  epoch  8, batch     5 | loss: 2.3773112Losses:  2.3922102451324463 0.24076317250728607 0.9183480143547058
MemoryTrain:  epoch  8, batch     6 | loss: 2.3922102Losses:  2.074453353881836 -0.0 0.8780442476272583
MemoryTrain:  epoch  8, batch     7 | loss: 2.0744534Losses:  2.6154024600982666 -0.0 1.0173957347869873
MemoryTrain:  epoch  8, batch     8 | loss: 2.6154025Losses:  3.0091006755828857 0.5464756488800049 0.914306640625
MemoryTrain:  epoch  8, batch     9 | loss: 3.0091007Losses:  2.7881526947021484 0.47257015109062195 0.9462293386459351
MemoryTrain:  epoch  8, batch    10 | loss: 2.7881527Losses:  3.3130455017089844 1.3013893365859985 0.8445858955383301
MemoryTrain:  epoch  8, batch    11 | loss: 3.3130455Losses:  2.2815499305725098 -0.0 1.024369239807129
MemoryTrain:  epoch  8, batch    12 | loss: 2.2815499Losses:  2.617077589035034 0.2372702956199646 0.9654504656791687
MemoryTrain:  epoch  8, batch    13 | loss: 2.6170776Losses:  2.47764253616333 0.25987619161605835 0.9614399671554565
MemoryTrain:  epoch  8, batch    14 | loss: 2.4776425Losses:  2.5222954750061035 -0.0 1.0178894996643066
MemoryTrain:  epoch  9, batch     0 | loss: 2.5222955Losses:  2.6604652404785156 0.22738640010356903 1.0723977088928223
MemoryTrain:  epoch  9, batch     1 | loss: 2.6604652Losses:  2.615673780441284 0.4781329333782196 0.8917187452316284
MemoryTrain:  epoch  9, batch     2 | loss: 2.6156738Losses:  2.54583477973938 0.2679074704647064 0.9351263046264648
MemoryTrain:  epoch  9, batch     3 | loss: 2.5458348Losses:  2.5426619052886963 0.4704132676124573 0.8324751853942871
MemoryTrain:  epoch  9, batch     4 | loss: 2.5426619Losses:  2.518496036529541 0.24287468194961548 0.9586784839630127
MemoryTrain:  epoch  9, batch     5 | loss: 2.5184960Losses:  2.4905850887298584 0.4833616614341736 0.728409469127655
MemoryTrain:  epoch  9, batch     6 | loss: 2.4905851Losses:  2.8942859172821045 0.25261688232421875 0.9704992771148682
MemoryTrain:  epoch  9, batch     7 | loss: 2.8942859Losses:  2.4955873489379883 0.25240856409072876 0.9531217813491821
MemoryTrain:  epoch  9, batch     8 | loss: 2.4955873Losses:  2.558530330657959 0.24869689345359802 0.9112026691436768
MemoryTrain:  epoch  9, batch     9 | loss: 2.5585303Losses:  2.2680463790893555 -0.0 0.9190859794616699
MemoryTrain:  epoch  9, batch    10 | loss: 2.2680464Losses:  2.8819618225097656 0.5436792373657227 0.9085694551467896
MemoryTrain:  epoch  9, batch    11 | loss: 2.8819618Losses:  2.3274495601654053 0.22534561157226562 0.8967449069023132
MemoryTrain:  epoch  9, batch    12 | loss: 2.3274496Losses:  2.433683395385742 0.23927189409732819 1.0140199661254883
MemoryTrain:  epoch  9, batch    13 | loss: 2.4336834Losses:  3.2057089805603027 0.5083684921264648 0.9757750034332275
MemoryTrain:  epoch  9, batch    14 | loss: 3.2057090
[EVAL] batch:    0 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    1 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    3 | acc: 18.75%,  total acc: 23.44%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 27.50%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 27.08%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 33.04%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 39.84%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 45.14%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 48.75%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 51.70%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 54.33%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 55.00%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 55.86%   [EVAL] batch:   16 | acc: 31.25%,  total acc: 54.41%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 55.56%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 55.26%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 54.69%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 54.17%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 53.12%   [EVAL] batch:   22 | acc: 50.00%,  total acc: 52.99%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 51.82%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 51.50%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 49.52%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 48.15%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 46.65%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 45.04%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 43.96%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 42.54%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 43.16%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 44.70%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 46.14%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 47.68%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 48.96%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 50.00%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 50.99%   [EVAL] batch:   38 | acc: 87.50%,  total acc: 51.92%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 53.12%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 54.12%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 54.61%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 55.67%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 56.11%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 55.28%   [EVAL] batch:   45 | acc: 0.00%,  total acc: 54.08%   [EVAL] batch:   46 | acc: 12.50%,  total acc: 53.19%   [EVAL] batch:   47 | acc: 12.50%,  total acc: 52.34%   [EVAL] batch:   48 | acc: 12.50%,  total acc: 51.53%   [EVAL] batch:   49 | acc: 12.50%,  total acc: 50.75%   [EVAL] batch:   50 | acc: 31.25%,  total acc: 50.37%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 50.48%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 50.47%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 50.58%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 50.57%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 50.67%   [EVAL] batch:   56 | acc: 43.75%,  total acc: 50.55%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 50.65%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 50.85%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 51.35%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 51.43%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 52.12%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 51.69%   
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 66.96%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 64.84%   [EVAL] batch:    8 | acc: 43.75%,  total acc: 62.50%   [EVAL] batch:    9 | acc: 37.50%,  total acc: 60.00%   [EVAL] batch:   10 | acc: 31.25%,  total acc: 57.39%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 55.21%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 56.25%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 58.48%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 60.83%   [EVAL] batch:   15 | acc: 93.75%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 64.71%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 69.06%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 69.94%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 70.45%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 71.35%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 71.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 73.61%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 74.55%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 75.22%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.04%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 76.81%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 77.54%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.22%   [EVAL] batch:   33 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 78.04%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 78.30%   [EVAL] batch:   36 | acc: 87.50%,  total acc: 78.55%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 78.95%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.49%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 80.34%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.65%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 80.67%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 80.82%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 80.56%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 80.43%   [EVAL] batch:   46 | acc: 50.00%,  total acc: 79.79%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 79.43%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:   49 | acc: 56.25%,  total acc: 79.00%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 78.92%   [EVAL] batch:   51 | acc: 81.25%,  total acc: 78.97%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 79.13%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 78.59%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 78.30%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 77.74%   [EVAL] batch:   57 | acc: 18.75%,  total acc: 76.72%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 75.95%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 75.83%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 75.92%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 75.71%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 75.20%   [EVAL] batch:   63 | acc: 50.00%,  total acc: 74.80%   [EVAL] batch:   64 | acc: 25.00%,  total acc: 74.04%   [EVAL] batch:   65 | acc: 68.75%,  total acc: 73.96%   [EVAL] batch:   66 | acc: 50.00%,  total acc: 73.60%   [EVAL] batch:   67 | acc: 43.75%,  total acc: 73.16%   [EVAL] batch:   68 | acc: 68.75%,  total acc: 73.10%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 73.04%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 73.06%   [EVAL] batch:   71 | acc: 68.75%,  total acc: 73.00%   [EVAL] batch:   72 | acc: 68.75%,  total acc: 72.95%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 72.97%   [EVAL] batch:   74 | acc: 62.50%,  total acc: 72.83%   [EVAL] batch:   75 | acc: 25.00%,  total acc: 72.20%   [EVAL] batch:   76 | acc: 12.50%,  total acc: 71.43%   [EVAL] batch:   77 | acc: 25.00%,  total acc: 70.83%   [EVAL] batch:   78 | acc: 25.00%,  total acc: 70.25%   [EVAL] batch:   79 | acc: 43.75%,  total acc: 69.92%   [EVAL] batch:   80 | acc: 25.00%,  total acc: 69.37%   [EVAL] batch:   81 | acc: 68.75%,  total acc: 69.36%   [EVAL] batch:   82 | acc: 100.00%,  total acc: 69.73%   [EVAL] batch:   83 | acc: 100.00%,  total acc: 70.09%   [EVAL] batch:   84 | acc: 93.75%,  total acc: 70.37%   [EVAL] batch:   85 | acc: 100.00%,  total acc: 70.71%   [EVAL] batch:   86 | acc: 93.75%,  total acc: 70.98%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 71.02%   [EVAL] batch:   88 | acc: 6.25%,  total acc: 70.29%   [EVAL] batch:   89 | acc: 18.75%,  total acc: 69.72%   [EVAL] batch:   90 | acc: 18.75%,  total acc: 69.16%   [EVAL] batch:   91 | acc: 37.50%,  total acc: 68.82%   [EVAL] batch:   92 | acc: 6.25%,  total acc: 68.15%   [EVAL] batch:   93 | acc: 12.50%,  total acc: 67.55%   [EVAL] batch:   94 | acc: 12.50%,  total acc: 66.97%   [EVAL] batch:   95 | acc: 12.50%,  total acc: 66.41%   [EVAL] batch:   96 | acc: 12.50%,  total acc: 65.85%   [EVAL] batch:   97 | acc: 6.25%,  total acc: 65.24%   [EVAL] batch:   98 | acc: 12.50%,  total acc: 64.71%   [EVAL] batch:   99 | acc: 12.50%,  total acc: 64.19%   [EVAL] batch:  100 | acc: 25.00%,  total acc: 63.80%   [EVAL] batch:  101 | acc: 31.25%,  total acc: 63.48%   [EVAL] batch:  102 | acc: 6.25%,  total acc: 62.92%   [EVAL] batch:  103 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:  104 | acc: 25.00%,  total acc: 62.14%   [EVAL] batch:  105 | acc: 31.25%,  total acc: 61.85%   [EVAL] batch:  106 | acc: 18.75%,  total acc: 61.45%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 61.05%   [EVAL] batch:  108 | acc: 18.75%,  total acc: 60.67%   [EVAL] batch:  109 | acc: 18.75%,  total acc: 60.28%   [EVAL] batch:  110 | acc: 25.00%,  total acc: 59.97%   [EVAL] batch:  111 | acc: 25.00%,  total acc: 59.65%   [EVAL] batch:  112 | acc: 37.50%,  total acc: 59.46%   [EVAL] batch:  113 | acc: 31.25%,  total acc: 59.21%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 59.24%   [EVAL] batch:  115 | acc: 62.50%,  total acc: 59.27%   [EVAL] batch:  116 | acc: 43.75%,  total acc: 59.13%   [EVAL] batch:  117 | acc: 43.75%,  total acc: 59.00%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 59.03%   [EVAL] batch:  119 | acc: 87.50%,  total acc: 59.27%   [EVAL] batch:  120 | acc: 93.75%,  total acc: 59.56%   [EVAL] batch:  121 | acc: 81.25%,  total acc: 59.73%   [EVAL] batch:  122 | acc: 56.25%,  total acc: 59.71%   [EVAL] batch:  123 | acc: 81.25%,  total acc: 59.88%   [EVAL] batch:  124 | acc: 75.00%,  total acc: 60.00%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 59.57%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 59.15%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 58.69%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 58.28%   [EVAL] batch:  129 | acc: 12.50%,  total acc: 57.93%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 57.54%   [EVAL] batch:  131 | acc: 31.25%,  total acc: 57.34%   [EVAL] batch:  132 | acc: 62.50%,  total acc: 57.38%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 57.51%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 57.69%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 57.90%   [EVAL] batch:  136 | acc: 68.75%,  total acc: 57.98%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 58.02%   [EVAL] batch:  138 | acc: 31.25%,  total acc: 57.82%   [EVAL] batch:  139 | acc: 43.75%,  total acc: 57.72%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 57.45%   [EVAL] batch:  141 | acc: 43.75%,  total acc: 57.35%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 57.26%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 57.20%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 57.50%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 57.79%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 58.04%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 58.32%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 58.60%   [EVAL] batch:  149 | acc: 87.50%,  total acc: 58.79%   [EVAL] batch:  150 | acc: 100.00%,  total acc: 59.06%   [EVAL] batch:  151 | acc: 100.00%,  total acc: 59.33%   [EVAL] batch:  152 | acc: 100.00%,  total acc: 59.60%   [EVAL] batch:  153 | acc: 93.75%,  total acc: 59.82%   [EVAL] batch:  154 | acc: 100.00%,  total acc: 60.08%   [EVAL] batch:  155 | acc: 93.75%,  total acc: 60.30%   [EVAL] batch:  156 | acc: 100.00%,  total acc: 60.55%   [EVAL] batch:  157 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 61.01%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 61.21%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 61.41%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 61.61%   [EVAL] batch:  162 | acc: 56.25%,  total acc: 61.58%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 61.36%   [EVAL] batch:  164 | acc: 12.50%,  total acc: 61.06%   [EVAL] batch:  165 | acc: 12.50%,  total acc: 60.77%   [EVAL] batch:  166 | acc: 18.75%,  total acc: 60.52%   [EVAL] batch:  167 | acc: 6.25%,  total acc: 60.19%   [EVAL] batch:  168 | acc: 43.75%,  total acc: 60.10%   [EVAL] batch:  169 | acc: 75.00%,  total acc: 60.18%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 60.31%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 60.39%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 60.44%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 60.42%   [EVAL] batch:  174 | acc: 68.75%,  total acc: 60.46%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 60.23%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 60.10%   [EVAL] batch:  177 | acc: 12.50%,  total acc: 59.83%   [EVAL] batch:  178 | acc: 31.25%,  total acc: 59.67%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 59.58%   [EVAL] batch:  180 | acc: 12.50%,  total acc: 59.32%   [EVAL] batch:  181 | acc: 56.25%,  total acc: 59.31%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 59.29%   [EVAL] batch:  183 | acc: 50.00%,  total acc: 59.24%   [EVAL] batch:  184 | acc: 68.75%,  total acc: 59.29%   [EVAL] batch:  185 | acc: 75.00%,  total acc: 59.38%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 59.49%   [EVAL] batch:  187 | acc: 56.25%,  total acc: 59.47%   [EVAL] batch:  188 | acc: 56.25%,  total acc: 59.46%   [EVAL] batch:  189 | acc: 75.00%,  total acc: 59.54%   [EVAL] batch:  190 | acc: 87.50%,  total acc: 59.69%   [EVAL] batch:  191 | acc: 75.00%,  total acc: 59.77%   [EVAL] batch:  192 | acc: 81.25%,  total acc: 59.88%   [EVAL] batch:  193 | acc: 75.00%,  total acc: 59.95%   [EVAL] batch:  194 | acc: 87.50%,  total acc: 60.10%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 60.14%   [EVAL] batch:  196 | acc: 56.25%,  total acc: 60.12%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 60.20%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 60.27%   [EVAL] batch:  199 | acc: 81.25%,  total acc: 60.38%   [EVAL] batch:  200 | acc: 81.25%,  total acc: 60.48%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 60.52%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 60.53%   [EVAL] batch:  203 | acc: 75.00%,  total acc: 60.60%   [EVAL] batch:  204 | acc: 68.75%,  total acc: 60.64%   [EVAL] batch:  205 | acc: 87.50%,  total acc: 60.77%   [EVAL] batch:  206 | acc: 31.25%,  total acc: 60.63%   [EVAL] batch:  207 | acc: 43.75%,  total acc: 60.55%   [EVAL] batch:  208 | acc: 50.00%,  total acc: 60.50%   [EVAL] batch:  209 | acc: 37.50%,  total acc: 60.39%   [EVAL] batch:  210 | acc: 50.00%,  total acc: 60.34%   [EVAL] batch:  211 | acc: 37.50%,  total acc: 60.23%   [EVAL] batch:  212 | acc: 56.25%,  total acc: 60.21%   [EVAL] batch:  213 | acc: 93.75%,  total acc: 60.37%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 60.55%   [EVAL] batch:  215 | acc: 93.75%,  total acc: 60.71%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 60.89%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 61.07%   [EVAL] batch:  218 | acc: 93.75%,  total acc: 61.22%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 61.39%   [EVAL] batch:  220 | acc: 93.75%,  total acc: 61.54%   [EVAL] batch:  221 | acc: 100.00%,  total acc: 61.71%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 61.86%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 62.03%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 62.19%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 62.22%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 62.25%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 62.28%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 62.34%   [EVAL] batch:  229 | acc: 93.75%,  total acc: 62.47%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 62.47%   [EVAL] batch:  231 | acc: 31.25%,  total acc: 62.34%   [EVAL] batch:  232 | acc: 12.50%,  total acc: 62.12%   [EVAL] batch:  233 | acc: 25.00%,  total acc: 61.97%   [EVAL] batch:  234 | acc: 25.00%,  total acc: 61.81%   [EVAL] batch:  235 | acc: 37.50%,  total acc: 61.71%   [EVAL] batch:  236 | acc: 6.25%,  total acc: 61.47%   [EVAL] batch:  237 | acc: 50.00%,  total acc: 61.42%   [EVAL] batch:  238 | acc: 87.50%,  total acc: 61.53%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 61.69%   [EVAL] batch:  240 | acc: 87.50%,  total acc: 61.80%   [EVAL] batch:  241 | acc: 87.50%,  total acc: 61.91%   [EVAL] batch:  242 | acc: 87.50%,  total acc: 62.01%   [EVAL] batch:  243 | acc: 81.25%,  total acc: 62.09%   [EVAL] batch:  244 | acc: 37.50%,  total acc: 61.99%   [EVAL] batch:  245 | acc: 43.75%,  total acc: 61.92%   [EVAL] batch:  246 | acc: 18.75%,  total acc: 61.74%   [EVAL] batch:  247 | acc: 37.50%,  total acc: 61.64%   [EVAL] batch:  248 | acc: 25.00%,  total acc: 61.50%   [EVAL] batch:  249 | acc: 37.50%,  total acc: 61.40%   [EVAL] batch:  250 | acc: 93.75%,  total acc: 61.53%   [EVAL] batch:  251 | acc: 87.50%,  total acc: 61.63%   [EVAL] batch:  252 | acc: 87.50%,  total acc: 61.73%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 61.81%   [EVAL] batch:  254 | acc: 93.75%,  total acc: 61.94%   [EVAL] batch:  255 | acc: 100.00%,  total acc: 62.08%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 62.14%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 62.19%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 62.23%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 62.33%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 62.40%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 62.48%   [EVAL] batch:  262 | acc: 75.00%,  total acc: 62.52%   [EVAL] batch:  263 | acc: 81.25%,  total acc: 62.59%   [EVAL] batch:  264 | acc: 68.75%,  total acc: 62.62%   [EVAL] batch:  265 | acc: 62.50%,  total acc: 62.62%   [EVAL] batch:  266 | acc: 56.25%,  total acc: 62.59%   [EVAL] batch:  267 | acc: 50.00%,  total acc: 62.55%   [EVAL] batch:  268 | acc: 56.25%,  total acc: 62.52%   [EVAL] batch:  269 | acc: 25.00%,  total acc: 62.38%   [EVAL] batch:  270 | acc: 6.25%,  total acc: 62.18%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 62.02%   [EVAL] batch:  272 | acc: 0.00%,  total acc: 61.79%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 61.63%   [EVAL] batch:  274 | acc: 18.75%,  total acc: 61.48%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 61.62%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 61.76%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 61.89%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 62.03%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 62.17%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 62.30%   [EVAL] batch:  281 | acc: 68.75%,  total acc: 62.32%   [EVAL] batch:  282 | acc: 75.00%,  total acc: 62.37%   [EVAL] batch:  283 | acc: 68.75%,  total acc: 62.39%   [EVAL] batch:  284 | acc: 68.75%,  total acc: 62.41%   [EVAL] batch:  285 | acc: 56.25%,  total acc: 62.39%   [EVAL] batch:  286 | acc: 56.25%,  total acc: 62.37%   [EVAL] batch:  287 | acc: 68.75%,  total acc: 62.39%   [EVAL] batch:  288 | acc: 100.00%,  total acc: 62.52%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 62.65%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 62.78%   [EVAL] batch:  291 | acc: 100.00%,  total acc: 62.91%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 63.03%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 63.14%   [EVAL] batch:  294 | acc: 100.00%,  total acc: 63.26%   [EVAL] batch:  295 | acc: 100.00%,  total acc: 63.39%   [EVAL] batch:  296 | acc: 100.00%,  total acc: 63.51%   [EVAL] batch:  297 | acc: 100.00%,  total acc: 63.63%   [EVAL] batch:  298 | acc: 100.00%,  total acc: 63.75%   [EVAL] batch:  299 | acc: 93.75%,  total acc: 63.85%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 63.95%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 64.05%   [EVAL] batch:  302 | acc: 100.00%,  total acc: 64.17%   [EVAL] batch:  303 | acc: 93.75%,  total acc: 64.27%   [EVAL] batch:  304 | acc: 87.50%,  total acc: 64.34%   [EVAL] batch:  305 | acc: 93.75%,  total acc: 64.44%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 64.54%   [EVAL] batch:  307 | acc: 81.25%,  total acc: 64.59%   [EVAL] batch:  308 | acc: 87.50%,  total acc: 64.66%   [EVAL] batch:  309 | acc: 100.00%,  total acc: 64.78%   [EVAL] batch:  310 | acc: 100.00%,  total acc: 64.89%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 64.96%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 64.96%   [EVAL] batch:  313 | acc: 62.50%,  total acc: 64.95%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 64.88%   [EVAL] batch:  315 | acc: 56.25%,  total acc: 64.85%   [EVAL] batch:  316 | acc: 56.25%,  total acc: 64.83%   [EVAL] batch:  317 | acc: 81.25%,  total acc: 64.88%   [EVAL] batch:  318 | acc: 81.25%,  total acc: 64.93%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 65.04%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  321 | acc: 81.25%,  total acc: 65.20%   [EVAL] batch:  322 | acc: 93.75%,  total acc: 65.29%   [EVAL] batch:  323 | acc: 93.75%,  total acc: 65.37%   [EVAL] batch:  324 | acc: 87.50%,  total acc: 65.44%   [EVAL] batch:  325 | acc: 50.00%,  total acc: 65.39%   [EVAL] batch:  326 | acc: 50.00%,  total acc: 65.35%   [EVAL] batch:  327 | acc: 62.50%,  total acc: 65.34%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 65.37%   [EVAL] batch:  329 | acc: 68.75%,  total acc: 65.38%   [EVAL] batch:  330 | acc: 68.75%,  total acc: 65.39%   [EVAL] batch:  331 | acc: 68.75%,  total acc: 65.40%   [EVAL] batch:  332 | acc: 25.00%,  total acc: 65.28%   [EVAL] batch:  333 | acc: 37.50%,  total acc: 65.19%   [EVAL] batch:  334 | acc: 43.75%,  total acc: 65.13%   [EVAL] batch:  335 | acc: 43.75%,  total acc: 65.07%   [EVAL] batch:  336 | acc: 31.25%,  total acc: 64.97%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 65.00%   [EVAL] batch:  338 | acc: 43.75%,  total acc: 64.93%   [EVAL] batch:  339 | acc: 50.00%,  total acc: 64.89%   [EVAL] batch:  340 | acc: 37.50%,  total acc: 64.81%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 64.82%   [EVAL] batch:  342 | acc: 43.75%,  total acc: 64.76%   [EVAL] batch:  343 | acc: 37.50%,  total acc: 64.68%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 64.69%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 64.63%   [EVAL] batch:  346 | acc: 68.75%,  total acc: 64.64%   [EVAL] batch:  347 | acc: 62.50%,  total acc: 64.64%   [EVAL] batch:  348 | acc: 75.00%,  total acc: 64.67%   [EVAL] batch:  349 | acc: 56.25%,  total acc: 64.64%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 64.64%   [EVAL] batch:  351 | acc: 81.25%,  total acc: 64.68%   [EVAL] batch:  352 | acc: 75.00%,  total acc: 64.71%   [EVAL] batch:  353 | acc: 87.50%,  total acc: 64.78%   [EVAL] batch:  354 | acc: 81.25%,  total acc: 64.82%   [EVAL] batch:  355 | acc: 87.50%,  total acc: 64.89%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 64.85%   [EVAL] batch:  357 | acc: 31.25%,  total acc: 64.75%   [EVAL] batch:  358 | acc: 37.50%,  total acc: 64.68%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 64.62%   [EVAL] batch:  360 | acc: 31.25%,  total acc: 64.53%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 64.49%   [EVAL] batch:  362 | acc: 62.50%,  total acc: 64.48%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 64.58%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 64.67%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 64.77%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 64.87%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 65.06%   [EVAL] batch:  369 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  370 | acc: 93.75%,  total acc: 65.23%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 65.29%   [EVAL] batch:  372 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  373 | acc: 100.00%,  total acc: 65.47%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 65.52%   [EVAL] batch:  375 | acc: 25.00%,  total acc: 65.41%   [EVAL] batch:  376 | acc: 50.00%,  total acc: 65.37%   [EVAL] batch:  377 | acc: 50.00%,  total acc: 65.33%   [EVAL] batch:  378 | acc: 43.75%,  total acc: 65.27%   [EVAL] batch:  379 | acc: 56.25%,  total acc: 65.25%   [EVAL] batch:  380 | acc: 50.00%,  total acc: 65.21%   [EVAL] batch:  381 | acc: 68.75%,  total acc: 65.22%   [EVAL] batch:  382 | acc: 81.25%,  total acc: 65.26%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 65.27%   [EVAL] batch:  384 | acc: 62.50%,  total acc: 65.26%   [EVAL] batch:  385 | acc: 50.00%,  total acc: 65.22%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 65.23%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 65.27%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 65.25%   [EVAL] batch:  389 | acc: 62.50%,  total acc: 65.24%   [EVAL] batch:  390 | acc: 50.00%,  total acc: 65.20%   [EVAL] batch:  391 | acc: 56.25%,  total acc: 65.18%   [EVAL] batch:  392 | acc: 75.00%,  total acc: 65.20%   [EVAL] batch:  393 | acc: 87.50%,  total acc: 65.26%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 65.32%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 65.65%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:  400 | acc: 50.00%,  total acc: 65.70%   [EVAL] batch:  401 | acc: 31.25%,  total acc: 65.61%   [EVAL] batch:  402 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:  403 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:  404 | acc: 37.50%,  total acc: 65.56%   [EVAL] batch:  405 | acc: 43.75%,  total acc: 65.50%   [EVAL] batch:  406 | acc: 68.75%,  total acc: 65.51%   [EVAL] batch:  407 | acc: 62.50%,  total acc: 65.50%   [EVAL] batch:  408 | acc: 18.75%,  total acc: 65.39%   [EVAL] batch:  409 | acc: 37.50%,  total acc: 65.32%   [EVAL] batch:  410 | acc: 25.00%,  total acc: 65.22%   [EVAL] batch:  411 | acc: 37.50%,  total acc: 65.15%   [EVAL] batch:  412 | acc: 75.00%,  total acc: 65.18%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 65.19%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 65.23%   [EVAL] batch:  415 | acc: 75.00%,  total acc: 65.25%   [EVAL] batch:  416 | acc: 87.50%,  total acc: 65.30%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 65.33%   [EVAL] batch:  418 | acc: 68.75%,  total acc: 65.33%   [EVAL] batch:  419 | acc: 75.00%,  total acc: 65.36%   [EVAL] batch:  420 | acc: 75.00%,  total acc: 65.38%   [EVAL] batch:  421 | acc: 75.00%,  total acc: 65.40%   [EVAL] batch:  422 | acc: 68.75%,  total acc: 65.41%   [EVAL] batch:  423 | acc: 81.25%,  total acc: 65.45%   [EVAL] batch:  424 | acc: 87.50%,  total acc: 65.50%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 65.58%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 65.90%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 65.98%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  432 | acc: 93.75%,  total acc: 66.12%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 66.19%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 66.26%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 66.33%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 66.40%   [EVAL] batch:  437 | acc: 68.75%,  total acc: 66.41%   [EVAL] batch:  438 | acc: 18.75%,  total acc: 66.30%   [EVAL] batch:  439 | acc: 25.00%,  total acc: 66.21%   [EVAL] batch:  440 | acc: 25.00%,  total acc: 66.11%   [EVAL] batch:  441 | acc: 31.25%,  total acc: 66.04%   [EVAL] batch:  442 | acc: 18.75%,  total acc: 65.93%   [EVAL] batch:  443 | acc: 56.25%,  total acc: 65.91%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 65.94%   [EVAL] batch:  445 | acc: 81.25%,  total acc: 65.98%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 66.02%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:  448 | acc: 75.00%,  total acc: 66.09%   [EVAL] batch:  449 | acc: 81.25%,  total acc: 66.12%   [EVAL] batch:  450 | acc: 43.75%,  total acc: 66.08%   [EVAL] batch:  451 | acc: 56.25%,  total acc: 66.05%   [EVAL] batch:  452 | acc: 68.75%,  total acc: 66.06%   [EVAL] batch:  453 | acc: 56.25%,  total acc: 66.04%   [EVAL] batch:  454 | acc: 56.25%,  total acc: 66.02%   [EVAL] batch:  455 | acc: 56.25%,  total acc: 66.00%   [EVAL] batch:  456 | acc: 37.50%,  total acc: 65.93%   [EVAL] batch:  457 | acc: 62.50%,  total acc: 65.93%   [EVAL] batch:  458 | acc: 37.50%,  total acc: 65.86%   [EVAL] batch:  459 | acc: 31.25%,  total acc: 65.79%   [EVAL] batch:  460 | acc: 31.25%,  total acc: 65.71%   [EVAL] batch:  461 | acc: 37.50%,  total acc: 65.65%   [EVAL] batch:  462 | acc: 25.00%,  total acc: 65.56%   [EVAL] batch:  463 | acc: 0.00%,  total acc: 65.42%   [EVAL] batch:  464 | acc: 18.75%,  total acc: 65.32%   [EVAL] batch:  465 | acc: 0.00%,  total acc: 65.18%   [EVAL] batch:  466 | acc: 6.25%,  total acc: 65.06%   [EVAL] batch:  467 | acc: 6.25%,  total acc: 64.93%   [EVAL] batch:  468 | acc: 18.75%,  total acc: 64.83%   [EVAL] batch:  469 | acc: 87.50%,  total acc: 64.88%   [EVAL] batch:  470 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 65.02%   [EVAL] batch:  472 | acc: 100.00%,  total acc: 65.09%   [EVAL] batch:  473 | acc: 87.50%,  total acc: 65.14%   [EVAL] batch:  474 | acc: 87.50%,  total acc: 65.18%   [EVAL] batch:  475 | acc: 87.50%,  total acc: 65.23%   [EVAL] batch:  476 | acc: 93.75%,  total acc: 65.29%   [EVAL] batch:  477 | acc: 93.75%,  total acc: 65.35%   [EVAL] batch:  478 | acc: 87.50%,  total acc: 65.40%   [EVAL] batch:  479 | acc: 87.50%,  total acc: 65.44%   [EVAL] batch:  480 | acc: 100.00%,  total acc: 65.51%   [EVAL] batch:  481 | acc: 31.25%,  total acc: 65.44%   [EVAL] batch:  482 | acc: 12.50%,  total acc: 65.33%   [EVAL] batch:  483 | acc: 12.50%,  total acc: 65.22%   [EVAL] batch:  484 | acc: 6.25%,  total acc: 65.10%   [EVAL] batch:  485 | acc: 12.50%,  total acc: 64.99%   [EVAL] batch:  486 | acc: 12.50%,  total acc: 64.89%   [EVAL] batch:  487 | acc: 25.00%,  total acc: 64.81%   [EVAL] batch:  488 | acc: 37.50%,  total acc: 64.75%   [EVAL] batch:  489 | acc: 50.00%,  total acc: 64.72%   [EVAL] batch:  490 | acc: 68.75%,  total acc: 64.73%   [EVAL] batch:  491 | acc: 37.50%,  total acc: 64.67%   [EVAL] batch:  492 | acc: 56.25%,  total acc: 64.66%   [EVAL] batch:  493 | acc: 50.00%,  total acc: 64.63%   [EVAL] batch:  494 | acc: 56.25%,  total acc: 64.61%   [EVAL] batch:  495 | acc: 62.50%,  total acc: 64.60%   [EVAL] batch:  496 | acc: 62.50%,  total acc: 64.60%   [EVAL] batch:  497 | acc: 68.75%,  total acc: 64.61%   [EVAL] batch:  498 | acc: 81.25%,  total acc: 64.64%   [EVAL] batch:  499 | acc: 68.75%,  total acc: 64.65%   
cur_acc:  ['0.9484', '0.7411', '0.7976', '0.6895', '0.8254', '0.7530', '0.7351', '0.5169']
his_acc:  ['0.9484', '0.8300', '0.7912', '0.7218', '0.7238', '0.7145', '0.7031', '0.6465']
--------Round  4
seed:  500
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 5 6 4 2 1 3 0]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 2 0 3 3 2 0 1 1]
Losses:  18.148780822753906 3.9411065578460693 1.2505851984024048
CurrentTrain: epoch  0, batch     0 | loss: 18.1487808Losses:  21.345245361328125 7.495120525360107 0.9757005572319031
CurrentTrain: epoch  0, batch     1 | loss: 21.3452454Losses:  21.612842559814453 8.26106071472168 1.110926866531372
CurrentTrain: epoch  0, batch     2 | loss: 21.6128426Losses:  17.260704040527344 3.9096031188964844 1.0666744709014893
CurrentTrain: epoch  0, batch     3 | loss: 17.2607040Losses:  17.773040771484375 4.727388381958008 1.0090636014938354
CurrentTrain: epoch  0, batch     4 | loss: 17.7730408Losses:  19.417882919311523 6.782268047332764 1.0073397159576416
CurrentTrain: epoch  0, batch     5 | loss: 19.4178829Losses:  16.855119705200195 4.547698497772217 0.9252574443817139
CurrentTrain: epoch  0, batch     6 | loss: 16.8551197Losses:  16.560970306396484 4.226375102996826 0.9751157760620117
CurrentTrain: epoch  0, batch     7 | loss: 16.5609703Losses:  16.123764038085938 3.7495651245117188 0.931338906288147
CurrentTrain: epoch  0, batch     8 | loss: 16.1237640Losses:  18.064491271972656 5.581602096557617 0.8452430367469788
CurrentTrain: epoch  0, batch     9 | loss: 18.0644913Losses:  17.589752197265625 5.5578413009643555 0.796843945980072
CurrentTrain: epoch  0, batch    10 | loss: 17.5897522Losses:  17.758440017700195 6.257946968078613 0.7557231187820435
CurrentTrain: epoch  0, batch    11 | loss: 17.7584400Losses:  15.721394538879395 4.310871124267578 0.752562403678894
CurrentTrain: epoch  0, batch    12 | loss: 15.7213945Losses:  15.123254776000977 3.488055944442749 0.8113051056861877
CurrentTrain: epoch  0, batch    13 | loss: 15.1232548Losses:  14.795462608337402 3.12016224861145 0.7493135929107666
CurrentTrain: epoch  0, batch    14 | loss: 14.7954626Losses:  15.090951919555664 3.944680690765381 0.7261029481887817
CurrentTrain: epoch  0, batch    15 | loss: 15.0909519Losses:  14.484220504760742 3.400512933731079 0.6513442993164062
CurrentTrain: epoch  0, batch    16 | loss: 14.4842205Losses:  16.39728546142578 5.446824073791504 0.620740532875061
CurrentTrain: epoch  0, batch    17 | loss: 16.3972855Losses:  16.006982803344727 5.259889602661133 0.633472204208374
CurrentTrain: epoch  0, batch    18 | loss: 16.0069828Losses:  14.836470603942871 3.9697012901306152 0.6611167192459106
CurrentTrain: epoch  0, batch    19 | loss: 14.8364706Losses:  15.155993461608887 4.465081214904785 0.6070939898490906
CurrentTrain: epoch  0, batch    20 | loss: 15.1559935Losses:  14.645157814025879 3.6044249534606934 0.6606646180152893
CurrentTrain: epoch  0, batch    21 | loss: 14.6451578Losses:  14.682689666748047 3.9048166275024414 0.5806326866149902
CurrentTrain: epoch  0, batch    22 | loss: 14.6826897Losses:  17.584199905395508 6.96322774887085 0.38442462682724
CurrentTrain: epoch  0, batch    23 | loss: 17.5841999Losses:  17.44930648803711 6.509112358093262 0.5312185883522034
CurrentTrain: epoch  0, batch    24 | loss: 17.4493065Losses:  15.59814453125 4.894672870635986 0.590246319770813
CurrentTrain: epoch  0, batch    25 | loss: 15.5981445Losses:  14.55588150024414 3.862781047821045 0.5287455320358276
CurrentTrain: epoch  0, batch    26 | loss: 14.5558815Losses:  16.616493225097656 6.372990131378174 0.4536641240119934
CurrentTrain: epoch  0, batch    27 | loss: 16.6164932Losses:  20.212900161743164 9.365216255187988 0.5144320726394653
CurrentTrain: epoch  0, batch    28 | loss: 20.2129002Losses:  15.71300983428955 4.958582878112793 0.5242093801498413
CurrentTrain: epoch  0, batch    29 | loss: 15.7130098Losses:  13.640995979309082 3.151463031768799 0.5374536514282227
CurrentTrain: epoch  0, batch    30 | loss: 13.6409960Losses:  12.34365463256836 2.4531397819519043 0.5208563804626465
CurrentTrain: epoch  0, batch    31 | loss: 12.3436546Losses:  15.749462127685547 5.506345748901367 0.46650320291519165
CurrentTrain: epoch  0, batch    32 | loss: 15.7494621Losses:  14.41102123260498 4.746335983276367 0.5005347728729248
CurrentTrain: epoch  0, batch    33 | loss: 14.4110212Losses:  17.075483322143555 7.8408403396606445 0.4389011263847351
CurrentTrain: epoch  0, batch    34 | loss: 17.0754833Losses:  14.637845039367676 4.806238174438477 0.457281231880188
CurrentTrain: epoch  0, batch    35 | loss: 14.6378450Losses:  13.314723014831543 3.173128128051758 0.5164209604263306
CurrentTrain: epoch  0, batch    36 | loss: 13.3147230Losses:  13.610820770263672 3.885681629180908 0.4776234030723572
CurrentTrain: epoch  0, batch    37 | loss: 13.6108208Losses:  13.721179962158203 4.577592849731445 0.4494265913963318
CurrentTrain: epoch  0, batch    38 | loss: 13.7211800Losses:  16.26926040649414 6.918022155761719 0.46684369444847107
CurrentTrain: epoch  0, batch    39 | loss: 16.2692604Losses:  16.692167282104492 7.663994789123535 0.4294542074203491
CurrentTrain: epoch  0, batch    40 | loss: 16.6921673Losses:  12.110491752624512 2.80499267578125 0.4663645625114441
CurrentTrain: epoch  0, batch    41 | loss: 12.1104918Losses:  12.764437675476074 3.8551621437072754 0.45133113861083984
CurrentTrain: epoch  0, batch    42 | loss: 12.7644377Losses:  16.13899040222168 7.068414688110352 0.4234093725681305
CurrentTrain: epoch  0, batch    43 | loss: 16.1389904Losses:  17.25408172607422 6.97286319732666 0.5092952847480774
CurrentTrain: epoch  0, batch    44 | loss: 17.2540817Losses:  15.86001968383789 5.975401401519775 0.5108796954154968
CurrentTrain: epoch  0, batch    45 | loss: 15.8600197Losses:  13.114394187927246 3.457766532897949 0.48607349395751953
CurrentTrain: epoch  0, batch    46 | loss: 13.1143942Losses:  14.033480644226074 5.026074409484863 0.4326331615447998
CurrentTrain: epoch  0, batch    47 | loss: 14.0334806Losses:  15.93609619140625 6.495295524597168 0.32635408639907837
CurrentTrain: epoch  0, batch    48 | loss: 15.9360962Losses:  11.708182334899902 2.5822038650512695 0.43996283411979675
CurrentTrain: epoch  0, batch    49 | loss: 11.7081823Losses:  13.9309720993042 5.043087005615234 0.4503815174102783
CurrentTrain: epoch  0, batch    50 | loss: 13.9309721Losses:  13.316793441772461 3.970472812652588 0.39275163412094116
CurrentTrain: epoch  0, batch    51 | loss: 13.3167934Losses:  11.832101821899414 3.1208157539367676 0.4362415671348572
CurrentTrain: epoch  0, batch    52 | loss: 11.8321018Losses:  10.863718032836914 2.5801405906677246 0.4055277109146118
CurrentTrain: epoch  0, batch    53 | loss: 10.8637180Losses:  11.060745239257812 2.7190394401550293 0.4213848114013672
CurrentTrain: epoch  0, batch    54 | loss: 11.0607452Losses:  12.872164726257324 4.370482444763184 0.39063146710395813
CurrentTrain: epoch  0, batch    55 | loss: 12.8721647Losses:  13.224691390991211 4.862313747406006 0.40425124764442444
CurrentTrain: epoch  0, batch    56 | loss: 13.2246914Losses:  12.318077087402344 3.541045665740967 0.4248366951942444
CurrentTrain: epoch  0, batch    57 | loss: 12.3180771Losses:  15.994351387023926 6.933896541595459 0.25903618335723877
CurrentTrain: epoch  0, batch    58 | loss: 15.9943514Losses:  12.218180656433105 4.242748260498047 0.3770645260810852
CurrentTrain: epoch  0, batch    59 | loss: 12.2181807Losses:  12.49508285522461 3.659783363342285 0.4155053496360779
CurrentTrain: epoch  0, batch    60 | loss: 12.4950829Losses:  13.309378623962402 5.340594291687012 0.37018388509750366
CurrentTrain: epoch  0, batch    61 | loss: 13.3093786Losses:  9.540729522705078 0.9217749834060669 0.4026361107826233
CurrentTrain: epoch  0, batch    62 | loss: 9.5407295Losses:  11.759626388549805 3.3485352993011475 0.3660222291946411
CurrentTrain: epoch  1, batch     0 | loss: 11.7596264Losses:  11.172066688537598 3.2620887756347656 0.3765352666378021
CurrentTrain: epoch  1, batch     1 | loss: 11.1720667Losses:  10.8563871383667 3.4385826587677 0.36274629831314087
CurrentTrain: epoch  1, batch     2 | loss: 10.8563871Losses:  12.028730392456055 3.904566764831543 0.38702392578125
CurrentTrain: epoch  1, batch     3 | loss: 12.0287304Losses:  11.516708374023438 3.4653124809265137 0.37144845724105835
CurrentTrain: epoch  1, batch     4 | loss: 11.5167084Losses:  11.21966552734375 3.569854259490967 0.3581107258796692
CurrentTrain: epoch  1, batch     5 | loss: 11.2196655Losses:  9.500783920288086 2.3500452041625977 0.34773629903793335
CurrentTrain: epoch  1, batch     6 | loss: 9.5007839Losses:  10.673507690429688 2.9878416061401367 0.3443519175052643
CurrentTrain: epoch  1, batch     7 | loss: 10.6735077Losses:  11.264213562011719 3.5222413539886475 0.3759758472442627
CurrentTrain: epoch  1, batch     8 | loss: 11.2642136Losses:  13.447367668151855 5.508809566497803 0.36387500166893005
CurrentTrain: epoch  1, batch     9 | loss: 13.4473677Losses:  12.289945602416992 3.3860137462615967 0.38057976961135864
CurrentTrain: epoch  1, batch    10 | loss: 12.2899456Losses:  12.865119934082031 4.256034851074219 0.3673928380012512
CurrentTrain: epoch  1, batch    11 | loss: 12.8651199Losses:  17.617294311523438 10.117445945739746 0.33592742681503296
CurrentTrain: epoch  1, batch    12 | loss: 17.6172943Losses:  10.628893852233887 3.5668370723724365 0.3539643883705139
CurrentTrain: epoch  1, batch    13 | loss: 10.6288939Losses:  13.116106986999512 5.595590591430664 0.3868163526058197
CurrentTrain: epoch  1, batch    14 | loss: 13.1161070Losses:  10.351232528686523 3.160135507583618 0.3456644117832184
CurrentTrain: epoch  1, batch    15 | loss: 10.3512325Losses:  11.52724552154541 3.8372750282287598 0.36045703291893005
CurrentTrain: epoch  1, batch    16 | loss: 11.5272455Losses:  10.378104209899902 3.5808615684509277 0.32022789120674133
CurrentTrain: epoch  1, batch    17 | loss: 10.3781042Losses:  9.73276424407959 2.6791138648986816 0.33866608142852783
CurrentTrain: epoch  1, batch    18 | loss: 9.7327642Losses:  12.087543487548828 4.040981292724609 0.35064011812210083
CurrentTrain: epoch  1, batch    19 | loss: 12.0875435Losses:  13.254958152770996 5.156103134155273 0.2876289486885071
CurrentTrain: epoch  1, batch    20 | loss: 13.2549582Losses:  12.278952598571777 4.704188823699951 0.3215671479701996
CurrentTrain: epoch  1, batch    21 | loss: 12.2789526Losses:  13.071582794189453 5.18369197845459 0.3430165648460388
CurrentTrain: epoch  1, batch    22 | loss: 13.0715828Losses:  9.826180458068848 2.932739734649658 0.3314712643623352
CurrentTrain: epoch  1, batch    23 | loss: 9.8261805Losses:  11.667925834655762 4.532514572143555 0.32562217116355896
CurrentTrain: epoch  1, batch    24 | loss: 11.6679258Losses:  11.805295944213867 4.4324798583984375 0.328075110912323
CurrentTrain: epoch  1, batch    25 | loss: 11.8052959Losses:  11.82551097869873 4.8535003662109375 0.3384523391723633
CurrentTrain: epoch  1, batch    26 | loss: 11.8255110Losses:  13.736059188842773 5.583988189697266 0.3353564143180847
CurrentTrain: epoch  1, batch    27 | loss: 13.7360592Losses:  11.46711540222168 4.749623775482178 0.235108882188797
CurrentTrain: epoch  1, batch    28 | loss: 11.4671154Losses:  12.295584678649902 5.489249229431152 0.3391605019569397
CurrentTrain: epoch  1, batch    29 | loss: 12.2955847Losses:  10.409287452697754 2.7501492500305176 0.3358278274536133
CurrentTrain: epoch  1, batch    30 | loss: 10.4092875Losses:  11.330392837524414 4.342053413391113 0.3442626893520355
CurrentTrain: epoch  1, batch    31 | loss: 11.3303928Losses:  10.240636825561523 3.1448187828063965 0.3333946168422699
CurrentTrain: epoch  1, batch    32 | loss: 10.2406368Losses:  10.148092269897461 2.693166971206665 0.33366328477859497
CurrentTrain: epoch  1, batch    33 | loss: 10.1480923Losses:  9.879291534423828 2.9296116828918457 0.3409937620162964
CurrentTrain: epoch  1, batch    34 | loss: 9.8792915Losses:  11.137605667114258 4.151293754577637 0.36305510997772217
CurrentTrain: epoch  1, batch    35 | loss: 11.1376057Losses:  15.256019592285156 7.170035362243652 0.2837408185005188
CurrentTrain: epoch  1, batch    36 | loss: 15.2560196Losses:  11.360032081604004 4.938533782958984 0.328906387090683
CurrentTrain: epoch  1, batch    37 | loss: 11.3600321Losses:  10.254197120666504 3.3219141960144043 0.33215203881263733
CurrentTrain: epoch  1, batch    38 | loss: 10.2541971Losses:  9.26501750946045 2.4698128700256348 0.3441251516342163
CurrentTrain: epoch  1, batch    39 | loss: 9.2650175Losses:  9.059046745300293 2.1624560356140137 0.3400769531726837
CurrentTrain: epoch  1, batch    40 | loss: 9.0590467Losses:  8.813396453857422 2.2755014896392822 0.334587037563324
CurrentTrain: epoch  1, batch    41 | loss: 8.8133965Losses:  14.358113288879395 6.4383015632629395 0.31764543056488037
CurrentTrain: epoch  1, batch    42 | loss: 14.3581133Losses:  9.782286643981934 3.2674570083618164 0.3508402705192566
CurrentTrain: epoch  1, batch    43 | loss: 9.7822866Losses:  11.077191352844238 3.424710750579834 0.3568832576274872
CurrentTrain: epoch  1, batch    44 | loss: 11.0771914Losses:  13.548938751220703 6.06563138961792 0.3109646141529083
CurrentTrain: epoch  1, batch    45 | loss: 13.5489388Losses:  9.582479476928711 2.5560455322265625 0.32825034856796265
CurrentTrain: epoch  1, batch    46 | loss: 9.5824795Losses:  11.027549743652344 4.1451416015625 0.31032150983810425
CurrentTrain: epoch  1, batch    47 | loss: 11.0275497Losses:  13.95682430267334 7.054939270019531 0.32499250769615173
CurrentTrain: epoch  1, batch    48 | loss: 13.9568243Losses:  9.391016960144043 2.63039493560791 0.30785059928894043
CurrentTrain: epoch  1, batch    49 | loss: 9.3910170Losses:  10.20910358428955 3.1479244232177734 0.3429100513458252
CurrentTrain: epoch  1, batch    50 | loss: 10.2091036Losses:  10.793573379516602 3.7423880100250244 0.3074533939361572
CurrentTrain: epoch  1, batch    51 | loss: 10.7935734Losses:  12.207399368286133 5.623604774475098 0.3110736012458801
CurrentTrain: epoch  1, batch    52 | loss: 12.2073994Losses:  11.41745376586914 4.414955139160156 0.3432542085647583
CurrentTrain: epoch  1, batch    53 | loss: 11.4174538Losses:  9.825719833374023 3.5625853538513184 0.3054155111312866
CurrentTrain: epoch  1, batch    54 | loss: 9.8257198Losses:  16.18927764892578 7.7728142738342285 0.30464810132980347
CurrentTrain: epoch  1, batch    55 | loss: 16.1892776Losses:  13.54134750366211 6.994678974151611 0.3212520480155945
CurrentTrain: epoch  1, batch    56 | loss: 13.5413475Losses:  9.683466911315918 3.215627431869507 0.31745314598083496
CurrentTrain: epoch  1, batch    57 | loss: 9.6834669Losses:  12.571223258972168 5.351957321166992 0.31074059009552
CurrentTrain: epoch  1, batch    58 | loss: 12.5712233Losses:  12.847851753234863 5.070199012756348 0.30903998017311096
CurrentTrain: epoch  1, batch    59 | loss: 12.8478518Losses:  14.0562744140625 7.020149230957031 0.29876142740249634
CurrentTrain: epoch  1, batch    60 | loss: 14.0562744Losses:  9.61975383758545 3.3223280906677246 0.3122395873069763
CurrentTrain: epoch  1, batch    61 | loss: 9.6197538Losses:  7.798500061035156 0.766860842704773 0.32026222348213196
CurrentTrain: epoch  1, batch    62 | loss: 7.7985001Losses:  12.40593147277832 5.313440322875977 0.31393003463745117
CurrentTrain: epoch  2, batch     0 | loss: 12.4059315Losses:  12.907835960388184 5.686553001403809 0.2560844421386719
CurrentTrain: epoch  2, batch     1 | loss: 12.9078360Losses:  10.949888229370117 5.266250133514404 0.28854215145111084
CurrentTrain: epoch  2, batch     2 | loss: 10.9498882Losses:  8.058847427368164 1.7109503746032715 0.2956404983997345
CurrentTrain: epoch  2, batch     3 | loss: 8.0588474Losses:  9.884942054748535 3.3734447956085205 0.28429272770881653
CurrentTrain: epoch  2, batch     4 | loss: 9.8849421Losses:  12.987051010131836 6.515709400177002 0.3223491907119751
CurrentTrain: epoch  2, batch     5 | loss: 12.9870510Losses:  8.8922119140625 2.775866985321045 0.28763067722320557
CurrentTrain: epoch  2, batch     6 | loss: 8.8922119Losses:  12.126664161682129 6.044681072235107 0.29436200857162476
CurrentTrain: epoch  2, batch     7 | loss: 12.1266642Losses:  9.074468612670898 3.536142349243164 0.2891314923763275
CurrentTrain: epoch  2, batch     8 | loss: 9.0744686Losses:  12.59571647644043 5.425561428070068 0.30965983867645264
CurrentTrain: epoch  2, batch     9 | loss: 12.5957165Losses:  8.39920711517334 2.0781898498535156 0.27919286489486694
CurrentTrain: epoch  2, batch    10 | loss: 8.3992071Losses:  8.368752479553223 2.190654754638672 0.2915993928909302
CurrentTrain: epoch  2, batch    11 | loss: 8.3687525Losses:  8.582261085510254 2.557462692260742 0.2938521206378937
CurrentTrain: epoch  2, batch    12 | loss: 8.5822611Losses:  10.341097831726074 3.1528406143188477 0.278124064207077
CurrentTrain: epoch  2, batch    13 | loss: 10.3410978Losses:  15.646017074584961 9.129892349243164 0.3122867941856384
CurrentTrain: epoch  2, batch    14 | loss: 15.6460171Losses:  9.693496704101562 3.56630802154541 0.30395662784576416
CurrentTrain: epoch  2, batch    15 | loss: 9.6934967Losses:  9.527962684631348 3.811941623687744 0.29986876249313354
CurrentTrain: epoch  2, batch    16 | loss: 9.5279627Losses:  8.914170265197754 2.6922595500946045 0.272921621799469
CurrentTrain: epoch  2, batch    17 | loss: 8.9141703Losses:  8.95505142211914 3.073850631713867 0.290294885635376
CurrentTrain: epoch  2, batch    18 | loss: 8.9550514Losses:  9.823902130126953 3.9946818351745605 0.28541994094848633
CurrentTrain: epoch  2, batch    19 | loss: 9.8239021Losses:  10.692599296569824 4.89552116394043 0.28193387389183044
CurrentTrain: epoch  2, batch    20 | loss: 10.6925993Losses:  7.9292378425598145 2.4906749725341797 0.2957621216773987
CurrentTrain: epoch  2, batch    21 | loss: 7.9292378Losses:  12.161270141601562 6.034970283508301 0.21982038021087646
CurrentTrain: epoch  2, batch    22 | loss: 12.1612701Losses:  10.002260208129883 3.433320999145508 0.2953410744667053
CurrentTrain: epoch  2, batch    23 | loss: 10.0022602Losses:  9.964126586914062 4.332475185394287 0.30523836612701416
CurrentTrain: epoch  2, batch    24 | loss: 9.9641266Losses:  11.372979164123535 5.728344917297363 0.2869187891483307
CurrentTrain: epoch  2, batch    25 | loss: 11.3729792Losses:  9.616086959838867 3.7320404052734375 0.29030799865722656
CurrentTrain: epoch  2, batch    26 | loss: 9.6160870Losses:  10.522137641906738 4.646875381469727 0.2811445891857147
CurrentTrain: epoch  2, batch    27 | loss: 10.5221376Losses:  11.069154739379883 5.08107852935791 0.30364298820495605
CurrentTrain: epoch  2, batch    28 | loss: 11.0691547Losses:  8.336554527282715 2.569692373275757 0.3061025142669678
CurrentTrain: epoch  2, batch    29 | loss: 8.3365545Losses:  9.365913391113281 3.5708694458007812 0.2832508087158203
CurrentTrain: epoch  2, batch    30 | loss: 9.3659134Losses:  11.015144348144531 4.4370880126953125 0.30005714297294617
CurrentTrain: epoch  2, batch    31 | loss: 11.0151443Losses:  9.771313667297363 3.0565297603607178 0.305724173784256
CurrentTrain: epoch  2, batch    32 | loss: 9.7713137Losses:  8.7903470993042 2.9873199462890625 0.29853570461273193
CurrentTrain: epoch  2, batch    33 | loss: 8.7903471Losses:  7.790544033050537 2.3587801456451416 0.28336381912231445
CurrentTrain: epoch  2, batch    34 | loss: 7.7905440Losses:  7.708243370056152 2.425337314605713 0.2832273840904236
CurrentTrain: epoch  2, batch    35 | loss: 7.7082434Losses:  10.929952621459961 4.736449241638184 0.2978106141090393
CurrentTrain: epoch  2, batch    36 | loss: 10.9299526Losses:  10.594849586486816 3.5141754150390625 0.294096440076828
CurrentTrain: epoch  2, batch    37 | loss: 10.5948496Losses:  8.682127952575684 2.9099230766296387 0.278319776058197
CurrentTrain: epoch  2, batch    38 | loss: 8.6821280Losses:  9.587409019470215 4.040182113647461 0.27904385328292847
CurrentTrain: epoch  2, batch    39 | loss: 9.5874090Losses:  10.462158203125 4.135157585144043 0.2828488051891327
CurrentTrain: epoch  2, batch    40 | loss: 10.4621582Losses:  8.783520698547363 2.724391460418701 0.26470866799354553
CurrentTrain: epoch  2, batch    41 | loss: 8.7835207Losses:  8.258177757263184 2.6730854511260986 0.27750152349472046
CurrentTrain: epoch  2, batch    42 | loss: 8.2581778Losses:  9.302404403686523 3.686476230621338 0.27386289834976196
CurrentTrain: epoch  2, batch    43 | loss: 9.3024044Losses:  10.747435569763184 4.5636186599731445 0.307115375995636
CurrentTrain: epoch  2, batch    44 | loss: 10.7474356Losses:  8.997366905212402 3.477037191390991 0.26970604062080383
CurrentTrain: epoch  2, batch    45 | loss: 8.9973669Losses:  11.420207977294922 4.528687000274658 0.2949431538581848
CurrentTrain: epoch  2, batch    46 | loss: 11.4202080Losses:  9.785699844360352 4.391855716705322 0.2752898931503296
CurrentTrain: epoch  2, batch    47 | loss: 9.7856998Losses:  8.716541290283203 3.4230942726135254 0.2662490904331207
CurrentTrain: epoch  2, batch    48 | loss: 8.7165413Losses:  7.75767183303833 2.0157554149627686 0.2825571298599243
CurrentTrain: epoch  2, batch    49 | loss: 7.7576718Losses:  9.183256149291992 3.4136948585510254 0.2777334451675415
CurrentTrain: epoch  2, batch    50 | loss: 9.1832561Losses:  9.041500091552734 3.2371907234191895 0.2757413983345032
CurrentTrain: epoch  2, batch    51 | loss: 9.0415001Losses:  8.575578689575195 2.9522223472595215 0.2702869176864624
CurrentTrain: epoch  2, batch    52 | loss: 8.5755787Losses:  10.455546379089355 4.921109199523926 0.2893226146697998
CurrentTrain: epoch  2, batch    53 | loss: 10.4555464Losses:  8.669848442077637 3.354241371154785 0.28414082527160645
CurrentTrain: epoch  2, batch    54 | loss: 8.6698484Losses:  9.491093635559082 3.64516019821167 0.29288187623023987
CurrentTrain: epoch  2, batch    55 | loss: 9.4910936Losses:  10.899513244628906 5.00875997543335 0.28288763761520386
CurrentTrain: epoch  2, batch    56 | loss: 10.8995132Losses:  10.14082145690918 4.029366493225098 0.2580810785293579
CurrentTrain: epoch  2, batch    57 | loss: 10.1408215Losses:  11.160298347473145 5.0099406242370605 0.277353972196579
CurrentTrain: epoch  2, batch    58 | loss: 11.1602983Losses:  8.126479148864746 2.1002278327941895 0.2650166451931
CurrentTrain: epoch  2, batch    59 | loss: 8.1264791Losses:  9.988265991210938 3.9078025817871094 0.27327507734298706
CurrentTrain: epoch  2, batch    60 | loss: 9.9882660Losses:  7.471096038818359 2.166079521179199 0.26442354917526245
CurrentTrain: epoch  2, batch    61 | loss: 7.4710960Losses:  5.828076362609863 0.8204935789108276 0.28501957654953003
CurrentTrain: epoch  2, batch    62 | loss: 5.8280764Losses:  10.840494155883789 5.2275166511535645 0.2842060923576355
CurrentTrain: epoch  3, batch     0 | loss: 10.8404942Losses:  8.498312950134277 3.673290729522705 0.2533814311027527
CurrentTrain: epoch  3, batch     1 | loss: 8.4983130Losses:  8.699173927307129 3.314314842224121 0.2768462598323822
CurrentTrain: epoch  3, batch     2 | loss: 8.6991739Losses:  9.12973690032959 3.191635847091675 0.27623918652534485
CurrentTrain: epoch  3, batch     3 | loss: 9.1297369Losses:  8.9276123046875 4.0656938552856445 0.2560581564903259
CurrentTrain: epoch  3, batch     4 | loss: 8.9276123Losses:  9.097813606262207 4.106208801269531 0.1703234761953354
CurrentTrain: epoch  3, batch     5 | loss: 9.0978136Losses:  7.995942115783691 2.0798494815826416 0.24847328662872314
CurrentTrain: epoch  3, batch     6 | loss: 7.9959421Losses:  8.59216594696045 3.2798991203308105 0.26711562275886536
CurrentTrain: epoch  3, batch     7 | loss: 8.5921659Losses:  8.356483459472656 3.2032551765441895 0.2688673734664917
CurrentTrain: epoch  3, batch     8 | loss: 8.3564835Losses:  11.445874214172363 5.845170497894287 0.26723429560661316
CurrentTrain: epoch  3, batch     9 | loss: 11.4458742Losses:  7.230335712432861 2.1058781147003174 0.2440774291753769
CurrentTrain: epoch  3, batch    10 | loss: 7.2303357Losses:  8.230074882507324 2.90297269821167 0.26821112632751465
CurrentTrain: epoch  3, batch    11 | loss: 8.2300749Losses:  8.166614532470703 2.663914203643799 0.2708622217178345
CurrentTrain: epoch  3, batch    12 | loss: 8.1666145Losses:  6.800573348999023 1.7642040252685547 0.2467299848794937
CurrentTrain: epoch  3, batch    13 | loss: 6.8005733Losses:  10.832747459411621 5.526810646057129 0.2670815587043762
CurrentTrain: epoch  3, batch    14 | loss: 10.8327475Losses:  9.898470878601074 4.4283127784729 0.28422296047210693
CurrentTrain: epoch  3, batch    15 | loss: 9.8984709Losses:  8.488560676574707 2.9657797813415527 0.2559354305267334
CurrentTrain: epoch  3, batch    16 | loss: 8.4885607Losses:  7.089966773986816 2.120161533355713 0.24599143862724304
CurrentTrain: epoch  3, batch    17 | loss: 7.0899668Losses:  7.631074905395508 1.9759984016418457 0.2541361451148987
CurrentTrain: epoch  3, batch    18 | loss: 7.6310749Losses:  7.388572692871094 2.321751594543457 0.26060405373573303
CurrentTrain: epoch  3, batch    19 | loss: 7.3885727Losses:  9.17198657989502 4.138607978820801 0.2711040675640106
CurrentTrain: epoch  3, batch    20 | loss: 9.1719866Losses:  9.620085716247559 4.247579574584961 0.25904038548469543
CurrentTrain: epoch  3, batch    21 | loss: 9.6200857Losses:  8.327892303466797 3.095815658569336 0.2716107666492462
CurrentTrain: epoch  3, batch    22 | loss: 8.3278923Losses:  9.867176055908203 5.112301826477051 0.2599644064903259
CurrentTrain: epoch  3, batch    23 | loss: 9.8671761Losses:  7.813220024108887 2.848404884338379 0.24362263083457947
CurrentTrain: epoch  3, batch    24 | loss: 7.8132200Losses:  7.971935749053955 2.8309712409973145 0.24983641505241394
CurrentTrain: epoch  3, batch    25 | loss: 7.9719357Losses:  7.87551736831665 3.1455984115600586 0.2529708743095398
CurrentTrain: epoch  3, batch    26 | loss: 7.8755174Losses:  8.763736724853516 3.678640365600586 0.24841949343681335
CurrentTrain: epoch  3, batch    27 | loss: 8.7637367Losses:  8.56270694732666 2.910055637359619 0.24291546642780304
CurrentTrain: epoch  3, batch    28 | loss: 8.5627069Losses:  8.418224334716797 2.7367119789123535 0.2579767107963562
CurrentTrain: epoch  3, batch    29 | loss: 8.4182243Losses:  9.344778060913086 3.686985969543457 0.263021856546402
CurrentTrain: epoch  3, batch    30 | loss: 9.3447781Losses:  8.730013847351074 2.650071144104004 0.24583113193511963
CurrentTrain: epoch  3, batch    31 | loss: 8.7300138Losses:  9.47429084777832 4.206460952758789 0.2717055678367615
CurrentTrain: epoch  3, batch    32 | loss: 9.4742908Losses:  8.836182594299316 3.65330171585083 0.27197593450546265
CurrentTrain: epoch  3, batch    33 | loss: 8.8361826Losses:  7.324455738067627 2.352851152420044 0.23937654495239258
CurrentTrain: epoch  3, batch    34 | loss: 7.3244557Losses:  8.662908554077148 2.897284984588623 0.24455887079238892
CurrentTrain: epoch  3, batch    35 | loss: 8.6629086Losses:  7.640562057495117 2.2607760429382324 0.25247693061828613
CurrentTrain: epoch  3, batch    36 | loss: 7.6405621Losses:  8.546382904052734 3.341176986694336 0.23923279345035553
CurrentTrain: epoch  3, batch    37 | loss: 8.5463829Losses:  8.60374927520752 3.405970573425293 0.2717819809913635
CurrentTrain: epoch  3, batch    38 | loss: 8.6037493Losses:  7.265655040740967 2.0193138122558594 0.25457820296287537
CurrentTrain: epoch  3, batch    39 | loss: 7.2656550Losses:  8.330280303955078 2.6688499450683594 0.25726908445358276
CurrentTrain: epoch  3, batch    40 | loss: 8.3302803Losses:  8.054758071899414 3.2125794887542725 0.27401119470596313
CurrentTrain: epoch  3, batch    41 | loss: 8.0547581Losses:  10.869380950927734 4.843508720397949 0.2785607576370239
CurrentTrain: epoch  3, batch    42 | loss: 10.8693810Losses:  8.312407493591309 3.164926528930664 0.2577053904533386
CurrentTrain: epoch  3, batch    43 | loss: 8.3124075Losses:  9.680669784545898 4.600769519805908 0.27783921360969543
CurrentTrain: epoch  3, batch    44 | loss: 9.6806698Losses:  7.844006061553955 2.70139217376709 0.2606987953186035
CurrentTrain: epoch  3, batch    45 | loss: 7.8440061Losses:  7.92657995223999 2.8045639991760254 0.25178617238998413
CurrentTrain: epoch  3, batch    46 | loss: 7.9265800Losses:  8.740445137023926 3.9602532386779785 0.24454939365386963
CurrentTrain: epoch  3, batch    47 | loss: 8.7404451Losses:  9.836084365844727 4.343967914581299 0.2575557827949524
CurrentTrain: epoch  3, batch    48 | loss: 9.8360844Losses:  7.780117988586426 2.334254026412964 0.2517908215522766
CurrentTrain: epoch  3, batch    49 | loss: 7.7801180Losses:  8.827820777893066 3.5200371742248535 0.259120374917984
CurrentTrain: epoch  3, batch    50 | loss: 8.8278208Losses:  6.361690998077393 1.7275550365447998 0.24234044551849365
CurrentTrain: epoch  3, batch    51 | loss: 6.3616910Losses:  8.204286575317383 3.6126480102539062 0.26067957282066345
CurrentTrain: epoch  3, batch    52 | loss: 8.2042866Losses:  9.834137916564941 4.592972278594971 0.28650742769241333
CurrentTrain: epoch  3, batch    53 | loss: 9.8341379Losses:  8.454100608825684 3.614387273788452 0.2600826919078827
CurrentTrain: epoch  3, batch    54 | loss: 8.4541006Losses:  12.741891860961914 6.835162162780762 0.2762436866760254
CurrentTrain: epoch  3, batch    55 | loss: 12.7418919Losses:  7.878157615661621 2.818376064300537 0.2772756814956665
CurrentTrain: epoch  3, batch    56 | loss: 7.8781576Losses:  11.082832336425781 4.773514747619629 0.273496150970459
CurrentTrain: epoch  3, batch    57 | loss: 11.0828323Losses:  9.812387466430664 4.132997035980225 0.263189435005188
CurrentTrain: epoch  3, batch    58 | loss: 9.8123875Losses:  8.785761833190918 3.9639134407043457 0.25740376114845276
CurrentTrain: epoch  3, batch    59 | loss: 8.7857618Losses:  10.716421127319336 5.571200370788574 0.28266334533691406
CurrentTrain: epoch  3, batch    60 | loss: 10.7164211Losses:  7.183588027954102 2.2942774295806885 0.2506963014602661
CurrentTrain: epoch  3, batch    61 | loss: 7.1835880Losses:  5.362714767456055 0.54435133934021 0.17553822696208954
CurrentTrain: epoch  3, batch    62 | loss: 5.3627148Losses:  10.036765098571777 4.884787559509277 0.24613192677497864
CurrentTrain: epoch  4, batch     0 | loss: 10.0367651Losses:  8.969249725341797 4.019043922424316 0.2840404510498047
CurrentTrain: epoch  4, batch     1 | loss: 8.9692497Losses:  7.31947660446167 2.089845895767212 0.2497599720954895
CurrentTrain: epoch  4, batch     2 | loss: 7.3194766Losses:  8.6520357131958 3.89391827583313 0.2745845317840576
CurrentTrain: epoch  4, batch     3 | loss: 8.6520357Losses:  12.0338716506958 7.722494125366211 0.26316702365875244
CurrentTrain: epoch  4, batch     4 | loss: 12.0338717Losses:  9.516982078552246 4.563097953796387 0.2679647207260132
CurrentTrain: epoch  4, batch     5 | loss: 9.5169821Losses:  8.76789665222168 3.6621856689453125 0.2598658800125122
CurrentTrain: epoch  4, batch     6 | loss: 8.7678967Losses:  9.352513313293457 4.595644950866699 0.27353549003601074
CurrentTrain: epoch  4, batch     7 | loss: 9.3525133Losses:  7.430637359619141 2.7302231788635254 0.2539266347885132
CurrentTrain: epoch  4, batch     8 | loss: 7.4306374Losses:  6.774425983428955 1.9620139598846436 0.24119627475738525
CurrentTrain: epoch  4, batch     9 | loss: 6.7744260Losses:  7.710460186004639 2.906158447265625 0.2397909164428711
CurrentTrain: epoch  4, batch    10 | loss: 7.7104602Losses:  8.273782730102539 3.2245795726776123 0.2548079192638397
CurrentTrain: epoch  4, batch    11 | loss: 8.2737827Losses:  7.429185390472412 2.4510552883148193 0.24682553112506866
CurrentTrain: epoch  4, batch    12 | loss: 7.4291854Losses:  7.308680057525635 2.6556787490844727 0.25192156434059143
CurrentTrain: epoch  4, batch    13 | loss: 7.3086801Losses:  8.146484375 3.449943780899048 0.24373678863048553
CurrentTrain: epoch  4, batch    14 | loss: 8.1464844Losses:  7.360698223114014 2.7919390201568604 0.24718141555786133
CurrentTrain: epoch  4, batch    15 | loss: 7.3606982Losses:  11.340493202209473 6.524364471435547 0.26491889357566833
CurrentTrain: epoch  4, batch    16 | loss: 11.3404932Losses:  9.210227012634277 3.853219985961914 0.2529144883155823
CurrentTrain: epoch  4, batch    17 | loss: 9.2102270Losses:  8.015044212341309 3.388040781021118 0.2578708529472351
CurrentTrain: epoch  4, batch    18 | loss: 8.0150442Losses:  7.846258640289307 3.1460509300231934 0.2526542544364929
CurrentTrain: epoch  4, batch    19 | loss: 7.8462586Losses:  8.073408126831055 3.436277151107788 0.24320155382156372
CurrentTrain: epoch  4, batch    20 | loss: 8.0734081Losses:  6.283498764038086 1.7401025295257568 0.23585882782936096
CurrentTrain: epoch  4, batch    21 | loss: 6.2834988Losses:  6.718840599060059 1.9392200708389282 0.2436707317829132
CurrentTrain: epoch  4, batch    22 | loss: 6.7188406Losses:  8.656152725219727 3.619424343109131 0.2496490180492401
CurrentTrain: epoch  4, batch    23 | loss: 8.6561527Losses:  8.73685073852539 4.039974212646484 0.17126694321632385
CurrentTrain: epoch  4, batch    24 | loss: 8.7368507Losses:  7.66753625869751 2.8782758712768555 0.2439909726381302
CurrentTrain: epoch  4, batch    25 | loss: 7.6675363Losses:  9.374368667602539 4.609042644500732 0.26970475912094116
CurrentTrain: epoch  4, batch    26 | loss: 9.3743687Losses:  6.190311431884766 1.6494274139404297 0.22826454043388367
CurrentTrain: epoch  4, batch    27 | loss: 6.1903114Losses:  8.323127746582031 3.2709121704101562 0.25863194465637207
CurrentTrain: epoch  4, batch    28 | loss: 8.3231277Losses:  7.84071159362793 3.225890636444092 0.2640336751937866
CurrentTrain: epoch  4, batch    29 | loss: 7.8407116Losses:  10.722332954406738 6.215692520141602 0.2740679979324341
CurrentTrain: epoch  4, batch    30 | loss: 10.7223330Losses:  7.370960712432861 2.878865957260132 0.24112290143966675
CurrentTrain: epoch  4, batch    31 | loss: 7.3709607Losses:  6.7123308181762695 2.200640916824341 0.23580093681812286
CurrentTrain: epoch  4, batch    32 | loss: 6.7123308Losses:  8.14510440826416 2.98085880279541 0.23558640480041504
CurrentTrain: epoch  4, batch    33 | loss: 8.1451044Losses:  7.488612651824951 2.9873502254486084 0.2401529848575592
CurrentTrain: epoch  4, batch    34 | loss: 7.4886127Losses:  6.950663089752197 2.089738368988037 0.24897804856300354
CurrentTrain: epoch  4, batch    35 | loss: 6.9506631Losses:  7.541189193725586 3.032956838607788 0.2676212787628174
CurrentTrain: epoch  4, batch    36 | loss: 7.5411892Losses:  9.756667137145996 5.020857810974121 0.1733618825674057
CurrentTrain: epoch  4, batch    37 | loss: 9.7566671Losses:  7.637560844421387 3.0866782665252686 0.23462308943271637
CurrentTrain: epoch  4, batch    38 | loss: 7.6375608Losses:  6.834660530090332 2.275567054748535 0.24077019095420837
CurrentTrain: epoch  4, batch    39 | loss: 6.8346605Losses:  10.760468482971191 5.128543853759766 0.2614547610282898
CurrentTrain: epoch  4, batch    40 | loss: 10.7604685Losses:  7.122084140777588 2.6799638271331787 0.23970863223075867
CurrentTrain: epoch  4, batch    41 | loss: 7.1220841Losses:  7.599674224853516 2.995588779449463 0.24600167572498322
CurrentTrain: epoch  4, batch    42 | loss: 7.5996742Losses:  9.031243324279785 4.485610485076904 0.25224390625953674
CurrentTrain: epoch  4, batch    43 | loss: 9.0312433Losses:  10.773366928100586 5.170503616333008 0.15446308255195618
CurrentTrain: epoch  4, batch    44 | loss: 10.7733669Losses:  7.774104118347168 3.2444753646850586 0.2465076595544815
CurrentTrain: epoch  4, batch    45 | loss: 7.7741041Losses:  7.446481704711914 2.6405529975891113 0.23122185468673706
CurrentTrain: epoch  4, batch    46 | loss: 7.4464817Losses:  7.664834022521973 2.5452260971069336 0.23604756593704224
CurrentTrain: epoch  4, batch    47 | loss: 7.6648340Losses:  6.673549175262451 1.6264057159423828 0.2390238642692566
CurrentTrain: epoch  4, batch    48 | loss: 6.6735492Losses:  10.076081275939941 5.134406089782715 0.24475790560245514
CurrentTrain: epoch  4, batch    49 | loss: 10.0760813Losses:  7.884662628173828 3.204706907272339 0.24255961179733276
CurrentTrain: epoch  4, batch    50 | loss: 7.8846626Losses:  9.276747703552246 4.100492477416992 0.2627021074295044
CurrentTrain: epoch  4, batch    51 | loss: 9.2767477Losses:  6.86934757232666 2.2167224884033203 0.2330002337694168
CurrentTrain: epoch  4, batch    52 | loss: 6.8693476Losses:  7.818208694458008 3.203404426574707 0.24222008883953094
CurrentTrain: epoch  4, batch    53 | loss: 7.8182087Losses:  9.221336364746094 4.204470157623291 0.22662760317325592
CurrentTrain: epoch  4, batch    54 | loss: 9.2213364Losses:  6.908377647399902 2.4422249794006348 0.24271686375141144
CurrentTrain: epoch  4, batch    55 | loss: 6.9083776Losses:  6.953287124633789 2.3933451175689697 0.2518918514251709
CurrentTrain: epoch  4, batch    56 | loss: 6.9532871Losses:  7.056926727294922 2.60650634765625 0.24473214149475098
CurrentTrain: epoch  4, batch    57 | loss: 7.0569267Losses:  7.727917194366455 2.7781050205230713 0.2363802194595337
CurrentTrain: epoch  4, batch    58 | loss: 7.7279172Losses:  8.904987335205078 3.353428840637207 0.17351286113262177
CurrentTrain: epoch  4, batch    59 | loss: 8.9049873Losses:  7.705537796020508 3.1325204372406006 0.22584998607635498
CurrentTrain: epoch  4, batch    60 | loss: 7.7055378Losses:  7.340059757232666 2.660961151123047 0.24170547723770142
CurrentTrain: epoch  4, batch    61 | loss: 7.3400598Losses:  7.876428604125977 2.1899242401123047 0.20784831047058105
CurrentTrain: epoch  4, batch    62 | loss: 7.8764286Losses:  7.9116950035095215 2.2848896980285645 0.234130859375
CurrentTrain: epoch  5, batch     0 | loss: 7.9116950Losses:  7.027137279510498 2.5427615642547607 0.2291828989982605
CurrentTrain: epoch  5, batch     1 | loss: 7.0271373Losses:  8.477460861206055 3.739569664001465 0.24267762899398804
CurrentTrain: epoch  5, batch     2 | loss: 8.4774609Losses:  7.543277740478516 2.593510866165161 0.23130816221237183
CurrentTrain: epoch  5, batch     3 | loss: 7.5432777Losses:  9.08272933959961 4.370543003082275 0.2378944307565689
CurrentTrain: epoch  5, batch     4 | loss: 9.0827293Losses:  10.739521980285645 6.292788982391357 0.23827609419822693
CurrentTrain: epoch  5, batch     5 | loss: 10.7395220Losses:  7.372824192047119 2.8154568672180176 0.24541175365447998
CurrentTrain: epoch  5, batch     6 | loss: 7.3728242Losses:  8.992110252380371 3.8506836891174316 0.25529953837394714
CurrentTrain: epoch  5, batch     7 | loss: 8.9921103Losses:  8.22030258178711 3.7903451919555664 0.2512272000312805
CurrentTrain: epoch  5, batch     8 | loss: 8.2203026Losses:  8.536580085754395 3.8800461292266846 0.25549447536468506
CurrentTrain: epoch  5, batch     9 | loss: 8.5365801Losses:  6.9164628982543945 2.2913999557495117 0.23026540875434875
CurrentTrain: epoch  5, batch    10 | loss: 6.9164629Losses:  9.58320426940918 4.949119567871094 0.25068235397338867
CurrentTrain: epoch  5, batch    11 | loss: 9.5832043Losses:  6.253419399261475 1.8767461776733398 0.2219170331954956
CurrentTrain: epoch  5, batch    12 | loss: 6.2534194Losses:  8.002656936645508 3.3970844745635986 0.2495782971382141
CurrentTrain: epoch  5, batch    13 | loss: 8.0026569Losses:  9.522250175476074 4.9261980056762695 0.16427287459373474
CurrentTrain: epoch  5, batch    14 | loss: 9.5222502Losses:  10.358232498168945 5.807552814483643 0.2551788091659546
CurrentTrain: epoch  5, batch    15 | loss: 10.3582325Losses:  6.609953880310059 2.200303554534912 0.23270012438297272
CurrentTrain: epoch  5, batch    16 | loss: 6.6099539Losses:  10.740983963012695 6.204587936401367 0.24145102500915527
CurrentTrain: epoch  5, batch    17 | loss: 10.7409840Losses:  7.362936496734619 2.7763352394104004 0.22979043424129486
CurrentTrain: epoch  5, batch    18 | loss: 7.3629365Losses:  7.884102821350098 3.240863800048828 0.24565620720386505
CurrentTrain: epoch  5, batch    19 | loss: 7.8841028Losses:  6.594401836395264 2.121387004852295 0.23308196663856506
CurrentTrain: epoch  5, batch    20 | loss: 6.5944018Losses:  9.031851768493652 4.416950225830078 0.2571902871131897
CurrentTrain: epoch  5, batch    21 | loss: 9.0318518Losses:  10.228849411010742 5.311352729797363 0.16466747224330902
CurrentTrain: epoch  5, batch    22 | loss: 10.2288494Losses:  8.02302074432373 3.491753101348877 0.25053173303604126
CurrentTrain: epoch  5, batch    23 | loss: 8.0230207Losses:  6.286011219024658 1.704982042312622 0.23311012983322144
CurrentTrain: epoch  5, batch    24 | loss: 6.2860112Losses:  6.9690260887146 2.1989290714263916 0.2340300977230072
CurrentTrain: epoch  5, batch    25 | loss: 6.9690261Losses:  7.665592193603516 3.2519516944885254 0.23795387148857117
CurrentTrain: epoch  5, batch    26 | loss: 7.6655922Losses:  10.698273658752441 6.061985015869141 0.2501879930496216
CurrentTrain: epoch  5, batch    27 | loss: 10.6982737Losses:  6.788990497589111 2.278287410736084 0.23477323353290558
CurrentTrain: epoch  5, batch    28 | loss: 6.7889905Losses:  7.367481231689453 2.8558855056762695 0.2532234191894531
CurrentTrain: epoch  5, batch    29 | loss: 7.3674812Losses:  8.217069625854492 3.6714956760406494 0.24592682719230652
CurrentTrain: epoch  5, batch    30 | loss: 8.2170696Losses:  6.748751640319824 2.246283531188965 0.2323140949010849
CurrentTrain: epoch  5, batch    31 | loss: 6.7487516Losses:  6.460108757019043 1.9721083641052246 0.22952806949615479
CurrentTrain: epoch  5, batch    32 | loss: 6.4601088Losses:  6.175515174865723 1.806520938873291 0.22195184230804443
CurrentTrain: epoch  5, batch    33 | loss: 6.1755152Losses:  7.319363117218018 2.824406147003174 0.24338199198246002
CurrentTrain: epoch  5, batch    34 | loss: 7.3193631Losses:  8.678935050964355 4.21741247177124 0.2339448630809784
CurrentTrain: epoch  5, batch    35 | loss: 8.6789351Losses:  9.266648292541504 4.7198805809021 0.2550438642501831
CurrentTrain: epoch  5, batch    36 | loss: 9.2666483Losses:  8.360157012939453 3.944866418838501 0.26016151905059814
CurrentTrain: epoch  5, batch    37 | loss: 8.3601570Losses:  6.045695781707764 1.6515034437179565 0.21542519330978394
CurrentTrain: epoch  5, batch    38 | loss: 6.0456958Losses:  6.995645999908447 2.512404441833496 0.251322478055954
CurrentTrain: epoch  5, batch    39 | loss: 6.9956460Losses:  7.957996845245361 3.5665969848632812 0.2534320056438446
CurrentTrain: epoch  5, batch    40 | loss: 7.9579968Losses:  6.960415363311768 2.6537671089172363 0.22779430449008942
CurrentTrain: epoch  5, batch    41 | loss: 6.9604154Losses:  7.590799808502197 3.1369309425354004 0.23954302072525024
CurrentTrain: epoch  5, batch    42 | loss: 7.5907998Losses:  10.529684066772461 6.075850486755371 0.24647921323776245
CurrentTrain: epoch  5, batch    43 | loss: 10.5296841Losses:  6.589829444885254 2.045041561126709 0.2306637018918991
CurrentTrain: epoch  5, batch    44 | loss: 6.5898294Losses:  7.029783725738525 2.5227246284484863 0.23989544808864594
CurrentTrain: epoch  5, batch    45 | loss: 7.0297837Losses:  7.169248580932617 2.6905250549316406 0.22885258495807648
CurrentTrain: epoch  5, batch    46 | loss: 7.1692486Losses:  8.178170204162598 3.875319004058838 0.22712162137031555
CurrentTrain: epoch  5, batch    47 | loss: 8.1781702Losses:  6.829349040985107 2.45271897315979 0.2459787279367447
CurrentTrain: epoch  5, batch    48 | loss: 6.8293490Losses:  6.587380886077881 2.209354877471924 0.23236539959907532
CurrentTrain: epoch  5, batch    49 | loss: 6.5873809Losses:  6.650778293609619 2.2550835609436035 0.23264604806900024
CurrentTrain: epoch  5, batch    50 | loss: 6.6507783Losses:  8.182319641113281 3.8091306686401367 0.1687047928571701
CurrentTrain: epoch  5, batch    51 | loss: 8.1823196Losses:  8.251959800720215 3.8141860961914062 0.2274368703365326
CurrentTrain: epoch  5, batch    52 | loss: 8.2519598Losses:  7.513600826263428 3.136481761932373 0.15136688947677612
CurrentTrain: epoch  5, batch    53 | loss: 7.5136008Losses:  6.925155162811279 2.5888805389404297 0.23879826068878174
CurrentTrain: epoch  5, batch    54 | loss: 6.9251552Losses:  7.40115213394165 3.1322600841522217 0.23527370393276215
CurrentTrain: epoch  5, batch    55 | loss: 7.4011521Losses:  7.09947395324707 2.709873676300049 0.23755860328674316
CurrentTrain: epoch  5, batch    56 | loss: 7.0994740Losses:  8.851411819458008 4.580409049987793 0.23419930040836334
CurrentTrain: epoch  5, batch    57 | loss: 8.8514118Losses:  8.064677238464355 3.7055277824401855 0.21425531804561615
CurrentTrain: epoch  5, batch    58 | loss: 8.0646772Losses:  6.762109279632568 2.395951509475708 0.23505434393882751
CurrentTrain: epoch  5, batch    59 | loss: 6.7621093Losses:  7.285247802734375 2.9294919967651367 0.24001216888427734
CurrentTrain: epoch  5, batch    60 | loss: 7.2852478Losses:  10.42239761352539 5.938290596008301 0.2482069730758667
CurrentTrain: epoch  5, batch    61 | loss: 10.4223976Losses:  4.883416175842285 0.5416547060012817 0.27946168184280396
CurrentTrain: epoch  5, batch    62 | loss: 4.8834162Losses:  6.804683685302734 2.386955738067627 0.2352348268032074
CurrentTrain: epoch  6, batch     0 | loss: 6.8046837Losses:  9.166098594665527 4.805675029754639 0.2358461320400238
CurrentTrain: epoch  6, batch     1 | loss: 9.1660986Losses:  10.040189743041992 5.629408359527588 0.2529107332229614
CurrentTrain: epoch  6, batch     2 | loss: 10.0401897Losses:  6.673613548278809 2.3580644130706787 0.2193659543991089
CurrentTrain: epoch  6, batch     3 | loss: 6.6736135Losses:  8.386148452758789 4.0114240646362305 0.26294463872909546
CurrentTrain: epoch  6, batch     4 | loss: 8.3861485Losses:  6.5851240158081055 2.2735934257507324 0.22770819067955017
CurrentTrain: epoch  6, batch     5 | loss: 6.5851240Losses:  7.271944999694824 2.9430065155029297 0.23912714421749115
CurrentTrain: epoch  6, batch     6 | loss: 7.2719450Losses:  6.735246658325195 2.423013687133789 0.23381352424621582
CurrentTrain: epoch  6, batch     7 | loss: 6.7352467Losses:  6.847393035888672 2.454812526702881 0.2343415915966034
CurrentTrain: epoch  6, batch     8 | loss: 6.8473930Losses:  8.770353317260742 4.333517074584961 0.2516944706439972
CurrentTrain: epoch  6, batch     9 | loss: 8.7703533Losses:  7.52897834777832 3.166539430618286 0.24083344638347626
CurrentTrain: epoch  6, batch    10 | loss: 7.5289783Losses:  8.363452911376953 3.7651314735412598 0.25175386667251587
CurrentTrain: epoch  6, batch    11 | loss: 8.3634529Losses:  6.1565937995910645 1.7460205554962158 0.2217440903186798
CurrentTrain: epoch  6, batch    12 | loss: 6.1565938Losses:  7.99776029586792 3.6787257194519043 0.25000983476638794
CurrentTrain: epoch  6, batch    13 | loss: 7.9977603Losses:  7.088179588317871 2.7621331214904785 0.22524023056030273
CurrentTrain: epoch  6, batch    14 | loss: 7.0881796Losses:  6.688299655914307 2.3617987632751465 0.2339688241481781
CurrentTrain: epoch  6, batch    15 | loss: 6.6882997Losses:  7.249564170837402 2.90553879737854 0.23660558462142944
CurrentTrain: epoch  6, batch    16 | loss: 7.2495642Losses:  6.817193031311035 2.5701346397399902 0.24305760860443115
CurrentTrain: epoch  6, batch    17 | loss: 6.8171930Losses:  8.278369903564453 3.9544551372528076 0.23843088746070862
CurrentTrain: epoch  6, batch    18 | loss: 8.2783699Losses:  6.6581034660339355 2.3838024139404297 0.23146218061447144
CurrentTrain: epoch  6, batch    19 | loss: 6.6581035Losses:  8.353752136230469 4.003051280975342 0.23518602550029755
CurrentTrain: epoch  6, batch    20 | loss: 8.3537521Losses:  7.083890914916992 2.7470366954803467 0.22652825713157654
CurrentTrain: epoch  6, batch    21 | loss: 7.0838909Losses:  9.554019927978516 5.315349578857422 0.23558282852172852
CurrentTrain: epoch  6, batch    22 | loss: 9.5540199Losses:  8.976183891296387 4.727931022644043 0.14608895778656006
CurrentTrain: epoch  6, batch    23 | loss: 8.9761839Losses:  7.532395362854004 3.195570468902588 0.24331170320510864
CurrentTrain: epoch  6, batch    24 | loss: 7.5323954Losses:  7.316019058227539 2.9994473457336426 0.26517200469970703
CurrentTrain: epoch  6, batch    25 | loss: 7.3160191Losses:  7.473205089569092 3.11187744140625 0.23316052556037903
CurrentTrain: epoch  6, batch    26 | loss: 7.4732051Losses:  6.979850769042969 2.665222644805908 0.2400205284357071
CurrentTrain: epoch  6, batch    27 | loss: 6.9798508Losses:  10.641993522644043 6.347009658813477 0.24146972596645355
CurrentTrain: epoch  6, batch    28 | loss: 10.6419935Losses:  8.852420806884766 4.471245765686035 0.23726877570152283
CurrentTrain: epoch  6, batch    29 | loss: 8.8524208Losses:  5.773671627044678 1.4859389066696167 0.2175372838973999
CurrentTrain: epoch  6, batch    30 | loss: 5.7736716Losses:  7.189807891845703 2.8095338344573975 0.22769926488399506
CurrentTrain: epoch  6, batch    31 | loss: 7.1898079Losses:  8.387323379516602 4.148868560791016 0.2286398559808731
CurrentTrain: epoch  6, batch    32 | loss: 8.3873234Losses:  6.317424297332764 2.0294618606567383 0.23376983404159546
CurrentTrain: epoch  6, batch    33 | loss: 6.3174243Losses:  6.814346790313721 2.5020787715911865 0.23950235545635223
CurrentTrain: epoch  6, batch    34 | loss: 6.8143468Losses:  9.374199867248535 5.099020957946777 0.19156377017498016
CurrentTrain: epoch  6, batch    35 | loss: 9.3741999Losses:  7.413632392883301 3.1949329376220703 0.2333134114742279
CurrentTrain: epoch  6, batch    36 | loss: 7.4136324Losses:  7.910968780517578 3.58396053314209 0.23674596846103668
CurrentTrain: epoch  6, batch    37 | loss: 7.9109688Losses:  7.321245193481445 2.983347177505493 0.21789073944091797
CurrentTrain: epoch  6, batch    38 | loss: 7.3212452Losses:  7.793520927429199 3.51670241355896 0.1535884439945221
CurrentTrain: epoch  6, batch    39 | loss: 7.7935209Losses:  7.830538272857666 3.547328472137451 0.23546281456947327
CurrentTrain: epoch  6, batch    40 | loss: 7.8305383Losses:  7.215040683746338 2.9431872367858887 0.23955054581165314
CurrentTrain: epoch  6, batch    41 | loss: 7.2150407Losses:  6.627108573913574 2.2779245376586914 0.21689772605895996
CurrentTrain: epoch  6, batch    42 | loss: 6.6271086Losses:  6.458728790283203 2.1781394481658936 0.23017047345638275
CurrentTrain: epoch  6, batch    43 | loss: 6.4587288Losses:  7.418028354644775 3.1620349884033203 0.23702938854694366
CurrentTrain: epoch  6, batch    44 | loss: 7.4180284Losses:  6.703671932220459 2.3828678131103516 0.23153367638587952
CurrentTrain: epoch  6, batch    45 | loss: 6.7036719Losses:  8.963173866271973 4.592563629150391 0.25009024143218994
CurrentTrain: epoch  6, batch    46 | loss: 8.9631739Losses:  7.087191104888916 2.8047666549682617 0.22759681940078735
CurrentTrain: epoch  6, batch    47 | loss: 7.0871911Losses:  6.15753698348999 1.854985237121582 0.22264930605888367
CurrentTrain: epoch  6, batch    48 | loss: 6.1575370Losses:  7.923882484436035 3.6742167472839355 0.23544248938560486
CurrentTrain: epoch  6, batch    49 | loss: 7.9238825Losses:  7.518804550170898 3.2168643474578857 0.23613065481185913
CurrentTrain: epoch  6, batch    50 | loss: 7.5188046Losses:  7.257582187652588 2.968322277069092 0.22511471807956696
CurrentTrain: epoch  6, batch    51 | loss: 7.2575822Losses:  6.461452007293701 2.205077648162842 0.2313879281282425
CurrentTrain: epoch  6, batch    52 | loss: 6.4614520Losses:  9.040558815002441 4.77406120300293 0.16316352784633636
CurrentTrain: epoch  6, batch    53 | loss: 9.0405588Losses:  8.063501358032227 3.9221973419189453 0.22978776693344116
CurrentTrain: epoch  6, batch    54 | loss: 8.0635014Losses:  10.372445106506348 6.013731002807617 0.15981581807136536
CurrentTrain: epoch  6, batch    55 | loss: 10.3724451Losses:  5.847076892852783 1.6095930337905884 0.2136358618736267
CurrentTrain: epoch  6, batch    56 | loss: 5.8470769Losses:  9.19999885559082 4.902570724487305 0.2463664710521698
CurrentTrain: epoch  6, batch    57 | loss: 9.1999989Losses:  9.432168960571289 5.137638092041016 0.2499087154865265
CurrentTrain: epoch  6, batch    58 | loss: 9.4321690Losses:  5.617595195770264 1.368844747543335 0.21099244058132172
CurrentTrain: epoch  6, batch    59 | loss: 5.6175952Losses:  6.898859977722168 2.6478238105773926 0.22592204809188843
CurrentTrain: epoch  6, batch    60 | loss: 6.8988600Losses:  6.442605018615723 2.195359230041504 0.21314740180969238
CurrentTrain: epoch  6, batch    61 | loss: 6.4426050Losses:  5.322447299957275 1.0785129070281982 0.2452179491519928
CurrentTrain: epoch  6, batch    62 | loss: 5.3224473Losses:  5.483059883117676 1.2653415203094482 0.2121870517730713
CurrentTrain: epoch  7, batch     0 | loss: 5.4830599Losses:  11.013537406921387 6.763726234436035 0.1862289011478424
CurrentTrain: epoch  7, batch     1 | loss: 11.0135374Losses:  7.155787944793701 2.92562198638916 0.23017945885658264
CurrentTrain: epoch  7, batch     2 | loss: 7.1557879Losses:  7.2510666847229 3.0003111362457275 0.24400772154331207
CurrentTrain: epoch  7, batch     3 | loss: 7.2510667Losses:  8.606882095336914 4.346122741699219 0.2576245665550232
CurrentTrain: epoch  7, batch     4 | loss: 8.6068821Losses:  10.873157501220703 6.596152305603027 0.2810779809951782
CurrentTrain: epoch  7, batch     5 | loss: 10.8731575Losses:  7.087111473083496 2.805694103240967 0.22101329267024994
CurrentTrain: epoch  7, batch     6 | loss: 7.0871115Losses:  6.078557014465332 1.8271257877349854 0.2213716208934784
CurrentTrain: epoch  7, batch     7 | loss: 6.0785570Losses:  7.89081335067749 3.566962718963623 0.2515615224838257
CurrentTrain: epoch  7, batch     8 | loss: 7.8908134Losses:  8.377311706542969 4.149408340454102 0.2451271414756775
CurrentTrain: epoch  7, batch     9 | loss: 8.3773117Losses:  8.177895545959473 3.9052438735961914 0.25383225083351135
CurrentTrain: epoch  7, batch    10 | loss: 8.1778955Losses:  6.899007320404053 2.615293025970459 0.23798570036888123
CurrentTrain: epoch  7, batch    11 | loss: 6.8990073Losses:  6.999152183532715 2.722888708114624 0.2298385202884674
CurrentTrain: epoch  7, batch    12 | loss: 6.9991522Losses:  6.0201191902160645 1.8383253812789917 0.2188343107700348
CurrentTrain: epoch  7, batch    13 | loss: 6.0201192Losses:  8.872769355773926 4.610451698303223 0.25323930382728577
CurrentTrain: epoch  7, batch    14 | loss: 8.8727694Losses:  7.4434356689453125 3.189995050430298 0.2377299964427948
CurrentTrain: epoch  7, batch    15 | loss: 7.4434357Losses:  6.584770679473877 2.4134628772735596 0.14816050231456757
CurrentTrain: epoch  7, batch    16 | loss: 6.5847707Losses:  6.818882465362549 2.5665340423583984 0.23664671182632446
CurrentTrain: epoch  7, batch    17 | loss: 6.8188825Losses:  6.9981842041015625 2.778130531311035 0.23767314851284027
CurrentTrain: epoch  7, batch    18 | loss: 6.9981842Losses:  9.771513938903809 5.584693908691406 0.24942097067832947
CurrentTrain: epoch  7, batch    19 | loss: 9.7715139Losses:  7.376515865325928 3.1232388019561768 0.23065605759620667
CurrentTrain: epoch  7, batch    20 | loss: 7.3765159Losses:  9.75628662109375 5.4784135818481445 0.2467098832130432
CurrentTrain: epoch  7, batch    21 | loss: 9.7562866Losses:  6.828400611877441 2.608076333999634 0.2190179079771042
CurrentTrain: epoch  7, batch    22 | loss: 6.8284006Losses:  7.114020347595215 2.8418068885803223 0.23737996816635132
CurrentTrain: epoch  7, batch    23 | loss: 7.1140203Losses:  7.370593547821045 3.2267203330993652 0.14766614139080048
CurrentTrain: epoch  7, batch    24 | loss: 7.3705935Losses:  7.425435543060303 3.224752426147461 0.22973394393920898
CurrentTrain: epoch  7, batch    25 | loss: 7.4254355Losses:  8.43449592590332 4.172091007232666 0.24259914457798004
CurrentTrain: epoch  7, batch    26 | loss: 8.4344959Losses:  6.8996992111206055 2.649338722229004 0.22091460227966309
CurrentTrain: epoch  7, batch    27 | loss: 6.8996992Losses:  6.635269641876221 2.4256813526153564 0.22488757967948914
CurrentTrain: epoch  7, batch    28 | loss: 6.6352696Losses:  6.1744232177734375 1.971074104309082 0.21121364831924438
CurrentTrain: epoch  7, batch    29 | loss: 6.1744232Losses:  6.537677764892578 2.30230975151062 0.23226721584796906
CurrentTrain: epoch  7, batch    30 | loss: 6.5376778Losses:  7.733395099639893 3.5637893676757812 0.13770601153373718
CurrentTrain: epoch  7, batch    31 | loss: 7.7333951Losses:  6.622534275054932 2.4001309871673584 0.22780752182006836
CurrentTrain: epoch  7, batch    32 | loss: 6.6225343Losses:  5.9667229652404785 1.7877116203308105 0.21818968653678894
CurrentTrain: epoch  7, batch    33 | loss: 5.9667230Losses:  6.510844707489014 2.2451343536376953 0.22493694722652435
CurrentTrain: epoch  7, batch    34 | loss: 6.5108447Losses:  8.636595726013184 4.4021806716918945 0.23211166262626648
CurrentTrain: epoch  7, batch    35 | loss: 8.6365957Losses:  10.01032829284668 5.766200065612793 0.25346195697784424
CurrentTrain: epoch  7, batch    36 | loss: 10.0103283Losses:  7.734809398651123 3.495032787322998 0.25689929723739624
CurrentTrain: epoch  7, batch    37 | loss: 7.7348094Losses:  7.91624641418457 3.671311140060425 0.23520168662071228
CurrentTrain: epoch  7, batch    38 | loss: 7.9162464Losses:  9.025514602661133 4.920933723449707 0.14557704329490662
CurrentTrain: epoch  7, batch    39 | loss: 9.0255146Losses:  6.744318962097168 2.498082160949707 0.22490403056144714
CurrentTrain: epoch  7, batch    40 | loss: 6.7443190Losses:  6.167862415313721 1.9589431285858154 0.21090763807296753
CurrentTrain: epoch  7, batch    41 | loss: 6.1678624Losses:  7.351273059844971 3.105282783508301 0.23750688135623932
CurrentTrain: epoch  7, batch    42 | loss: 7.3512731Losses:  6.933244228363037 2.6825499534606934 0.23144057393074036
CurrentTrain: epoch  7, batch    43 | loss: 6.9332442Losses:  8.70807933807373 4.469448089599609 0.23947222530841827
CurrentTrain: epoch  7, batch    44 | loss: 8.7080793Losses:  6.381816864013672 2.153946876525879 0.22694534063339233
CurrentTrain: epoch  7, batch    45 | loss: 6.3818169Losses:  10.773682594299316 6.615888595581055 0.18129783868789673
CurrentTrain: epoch  7, batch    46 | loss: 10.7736826Losses:  7.435710430145264 3.1854472160339355 0.2357465773820877
CurrentTrain: epoch  7, batch    47 | loss: 7.4357104Losses:  7.2804155349731445 3.035065174102783 0.22721326351165771
CurrentTrain: epoch  7, batch    48 | loss: 7.2804155Losses:  6.557587146759033 2.3707571029663086 0.2333202064037323
CurrentTrain: epoch  7, batch    49 | loss: 6.5575871Losses:  8.76559829711914 4.450075149536133 0.2391553521156311
CurrentTrain: epoch  7, batch    50 | loss: 8.7655983Losses:  5.856479644775391 1.6138654947280884 0.21622410416603088
CurrentTrain: epoch  7, batch    51 | loss: 5.8564796Losses:  7.494442939758301 3.3267881870269775 0.24558629095554352
CurrentTrain: epoch  7, batch    52 | loss: 7.4944429Losses:  6.373002052307129 2.192229747772217 0.224627286195755
CurrentTrain: epoch  7, batch    53 | loss: 6.3730021Losses:  6.0564422607421875 1.8324896097183228 0.22054246068000793
CurrentTrain: epoch  7, batch    54 | loss: 6.0564423Losses:  6.949250221252441 2.695253372192383 0.2312624454498291
CurrentTrain: epoch  7, batch    55 | loss: 6.9492502Losses:  6.975762844085693 2.81284236907959 0.22136171162128448
CurrentTrain: epoch  7, batch    56 | loss: 6.9757628Losses:  9.306585311889648 5.064239501953125 0.24125826358795166
CurrentTrain: epoch  7, batch    57 | loss: 9.3065853Losses:  8.369894981384277 4.131629943847656 0.24611936509609222
CurrentTrain: epoch  7, batch    58 | loss: 8.3698950Losses:  6.029452800750732 1.7864291667938232 0.22113169729709625
CurrentTrain: epoch  7, batch    59 | loss: 6.0294528Losses:  8.544195175170898 4.330323696136475 0.23857492208480835
CurrentTrain: epoch  7, batch    60 | loss: 8.5441952Losses:  6.997897624969482 2.7447116374969482 0.22960630059242249
CurrentTrain: epoch  7, batch    61 | loss: 6.9978976Losses:  6.998924732208252 2.8182554244995117 0.17877750098705292
CurrentTrain: epoch  7, batch    62 | loss: 6.9989247Losses:  6.319201946258545 2.1362688541412354 0.22040513157844543
CurrentTrain: epoch  8, batch     0 | loss: 6.3192019Losses:  7.107872009277344 2.884406089782715 0.23257820308208466
CurrentTrain: epoch  8, batch     1 | loss: 7.1078720Losses:  11.102205276489258 6.836605072021484 0.24464532732963562
CurrentTrain: epoch  8, batch     2 | loss: 11.1022053Losses:  6.411832332611084 2.170356273651123 0.2334933876991272
CurrentTrain: epoch  8, batch     3 | loss: 6.4118323Losses:  6.549962997436523 2.3385050296783447 0.2279851734638214
CurrentTrain: epoch  8, batch     4 | loss: 6.5499630Losses:  7.093691349029541 2.8689768314361572 0.2292216718196869
CurrentTrain: epoch  8, batch     5 | loss: 7.0936913Losses:  7.945310115814209 3.6962482929229736 0.22516487538814545
CurrentTrain: epoch  8, batch     6 | loss: 7.9453101Losses:  7.012819290161133 2.782982587814331 0.22362706065177917
CurrentTrain: epoch  8, batch     7 | loss: 7.0128193Losses:  6.968522548675537 2.732053756713867 0.23260469734668732
CurrentTrain: epoch  8, batch     8 | loss: 6.9685225Losses:  6.479613304138184 2.286776542663574 0.2180940806865692
CurrentTrain: epoch  8, batch     9 | loss: 6.4796133Losses:  6.884554862976074 2.6539835929870605 0.2276051938533783
CurrentTrain: epoch  8, batch    10 | loss: 6.8845549Losses:  7.964506149291992 3.7912561893463135 0.22300243377685547
CurrentTrain: epoch  8, batch    11 | loss: 7.9645061Losses:  6.831684589385986 2.6695728302001953 0.2329067587852478
CurrentTrain: epoch  8, batch    12 | loss: 6.8316846Losses:  6.862508773803711 2.6380958557128906 0.23004809021949768
CurrentTrain: epoch  8, batch    13 | loss: 6.8625088Losses:  7.432840824127197 3.181725025177002 0.24592715501785278
CurrentTrain: epoch  8, batch    14 | loss: 7.4328408Losses:  6.556037902832031 2.3364338874816895 0.22723156213760376
CurrentTrain: epoch  8, batch    15 | loss: 6.5560379Losses:  6.744446754455566 2.5288772583007812 0.21622440218925476
CurrentTrain: epoch  8, batch    16 | loss: 6.7444468Losses:  7.417030334472656 3.1420960426330566 0.2401711791753769
CurrentTrain: epoch  8, batch    17 | loss: 7.4170303Losses:  7.027179718017578 2.795311450958252 0.23622816801071167
CurrentTrain: epoch  8, batch    18 | loss: 7.0271797Losses:  6.037293910980225 1.8190611600875854 0.21844333410263062
CurrentTrain: epoch  8, batch    19 | loss: 6.0372939Losses:  6.230828762054443 1.9984047412872314 0.22762075066566467
CurrentTrain: epoch  8, batch    20 | loss: 6.2308288Losses:  7.385670185089111 3.16379714012146 0.23105737566947937
CurrentTrain: epoch  8, batch    21 | loss: 7.3856702Losses:  6.276679039001465 2.0253982543945312 0.2363986372947693
CurrentTrain: epoch  8, batch    22 | loss: 6.2766790Losses:  6.427063941955566 2.261395215988159 0.22087514400482178
CurrentTrain: epoch  8, batch    23 | loss: 6.4270639Losses:  8.192497253417969 3.969482898712158 0.2243398129940033
CurrentTrain: epoch  8, batch    24 | loss: 8.1924973Losses:  6.310319423675537 2.06535005569458 0.23603807389736176
CurrentTrain: epoch  8, batch    25 | loss: 6.3103194Losses:  8.18042278289795 3.956286907196045 0.2489320933818817
CurrentTrain: epoch  8, batch    26 | loss: 8.1804228Losses:  6.352519989013672 2.1392953395843506 0.22216814756393433
CurrentTrain: epoch  8, batch    27 | loss: 6.3525200Losses:  8.393710136413574 4.185215950012207 0.2412644624710083
CurrentTrain: epoch  8, batch    28 | loss: 8.3937101Losses:  8.02491283416748 3.859813928604126 0.22341257333755493
CurrentTrain: epoch  8, batch    29 | loss: 8.0249128Losses:  7.321860313415527 3.069530487060547 0.2341340035200119
CurrentTrain: epoch  8, batch    30 | loss: 7.3218603Losses:  6.751172065734863 2.5373642444610596 0.2280062437057495
CurrentTrain: epoch  8, batch    31 | loss: 6.7511721Losses:  5.780082702636719 1.614393711090088 0.20797665417194366
CurrentTrain: epoch  8, batch    32 | loss: 5.7800827Losses:  6.942579746246338 2.7008814811706543 0.23067842423915863
CurrentTrain: epoch  8, batch    33 | loss: 6.9425797Losses:  8.30737018585205 4.137729644775391 0.14494627714157104
CurrentTrain: epoch  8, batch    34 | loss: 8.3073702Losses:  10.027853965759277 5.8545074462890625 0.2370660901069641
CurrentTrain: epoch  8, batch    35 | loss: 10.0278540Losses:  7.479378700256348 3.208029270172119 0.24677926301956177
CurrentTrain: epoch  8, batch    36 | loss: 7.4793787Losses:  6.553277015686035 2.380354404449463 0.22606602311134338
CurrentTrain: epoch  8, batch    37 | loss: 6.5532770Losses:  7.24395751953125 3.033849000930786 0.22559237480163574
CurrentTrain: epoch  8, batch    38 | loss: 7.2439575Losses:  6.392148494720459 2.1857991218566895 0.2177327424287796
CurrentTrain: epoch  8, batch    39 | loss: 6.3921485Losses:  6.964073181152344 2.7075953483581543 0.2426956295967102
CurrentTrain: epoch  8, batch    40 | loss: 6.9640732Losses:  5.985227108001709 1.78273344039917 0.22009538114070892
CurrentTrain: epoch  8, batch    41 | loss: 5.9852271Losses:  6.905837535858154 2.730827808380127 0.21169811487197876
CurrentTrain: epoch  8, batch    42 | loss: 6.9058375Losses:  7.035002708435059 2.792135238647461 0.2341691255569458
CurrentTrain: epoch  8, batch    43 | loss: 7.0350027Losses:  9.444324493408203 5.179075717926025 0.2521609663963318
CurrentTrain: epoch  8, batch    44 | loss: 9.4443245Losses:  8.40881633758545 4.217996597290039 0.23570357263088226
CurrentTrain: epoch  8, batch    45 | loss: 8.4088163Losses:  5.98317813873291 1.7984941005706787 0.21467378735542297
CurrentTrain: epoch  8, batch    46 | loss: 5.9831781Losses:  5.9726948738098145 1.796218752861023 0.2221391797065735
CurrentTrain: epoch  8, batch    47 | loss: 5.9726949Losses:  6.534974575042725 2.3539481163024902 0.22874921560287476
CurrentTrain: epoch  8, batch    48 | loss: 6.5349746Losses:  6.943423271179199 2.6943583488464355 0.23712262511253357
CurrentTrain: epoch  8, batch    49 | loss: 6.9434233Losses:  6.57582950592041 2.3679068088531494 0.22577571868896484
CurrentTrain: epoch  8, batch    50 | loss: 6.5758295Losses:  6.858931541442871 2.67753267288208 0.22473004460334778
CurrentTrain: epoch  8, batch    51 | loss: 6.8589315Losses:  7.320337295532227 3.1159985065460205 0.22746634483337402
CurrentTrain: epoch  8, batch    52 | loss: 7.3203373Losses:  6.393178939819336 2.1452763080596924 0.22742632031440735
CurrentTrain: epoch  8, batch    53 | loss: 6.3931789Losses:  7.261981964111328 3.0387024879455566 0.23232689499855042
CurrentTrain: epoch  8, batch    54 | loss: 7.2619820Losses:  6.756954669952393 2.550565242767334 0.21966031193733215
CurrentTrain: epoch  8, batch    55 | loss: 6.7569547Losses:  6.33173131942749 2.150570869445801 0.22850064933300018
CurrentTrain: epoch  8, batch    56 | loss: 6.3317313Losses:  7.290568828582764 3.1012887954711914 0.22952410578727722
CurrentTrain: epoch  8, batch    57 | loss: 7.2905688Losses:  7.290104389190674 3.139279365539551 0.23094835877418518
CurrentTrain: epoch  8, batch    58 | loss: 7.2901044Losses:  5.997873306274414 1.8086847066879272 0.2201044261455536
CurrentTrain: epoch  8, batch    59 | loss: 5.9978733Losses:  8.342567443847656 4.135166168212891 0.25118589401245117
CurrentTrain: epoch  8, batch    60 | loss: 8.3425674Losses:  7.320448398590088 3.1160008907318115 0.2427988201379776
CurrentTrain: epoch  8, batch    61 | loss: 7.3204484Losses:  4.388627052307129 0.21811753511428833 0.22944042086601257
CurrentTrain: epoch  8, batch    62 | loss: 4.3886271Losses:  6.109683513641357 1.9273356199264526 0.20844589173793793
CurrentTrain: epoch  9, batch     0 | loss: 6.1096835Losses:  7.640675067901611 3.4667105674743652 0.2242128998041153
CurrentTrain: epoch  9, batch     1 | loss: 7.6406751Losses:  7.261541366577148 3.054793357849121 0.2300390899181366
CurrentTrain: epoch  9, batch     2 | loss: 7.2615414Losses:  7.348178863525391 3.155636787414551 0.23896431922912598
CurrentTrain: epoch  9, batch     3 | loss: 7.3481789Losses:  6.276549339294434 2.125539779663086 0.2205246388912201
CurrentTrain: epoch  9, batch     4 | loss: 6.2765493Losses:  5.766078472137451 1.5768349170684814 0.2088208794593811
CurrentTrain: epoch  9, batch     5 | loss: 5.7660785Losses:  6.60989236831665 2.4057278633117676 0.2256220281124115
CurrentTrain: epoch  9, batch     6 | loss: 6.6098924Losses:  7.076989650726318 2.880535125732422 0.22570262849330902
CurrentTrain: epoch  9, batch     7 | loss: 7.0769897Losses:  7.119247913360596 2.971310615539551 0.23680397868156433
CurrentTrain: epoch  9, batch     8 | loss: 7.1192479Losses:  9.491034507751465 5.215059280395508 0.24767135083675385
CurrentTrain: epoch  9, batch     9 | loss: 9.4910345Losses:  6.811039924621582 2.612877130508423 0.23275941610336304
CurrentTrain: epoch  9, batch    10 | loss: 6.8110399Losses:  7.447983264923096 3.2455334663391113 0.2254462093114853
CurrentTrain: epoch  9, batch    11 | loss: 7.4479833Losses:  6.494152069091797 2.306373119354248 0.2322748601436615
CurrentTrain: epoch  9, batch    12 | loss: 6.4941521Losses:  6.88347053527832 2.669203996658325 0.2392427921295166
CurrentTrain: epoch  9, batch    13 | loss: 6.8834705Losses:  5.935791969299316 1.7696670293807983 0.21858425438404083
CurrentTrain: epoch  9, batch    14 | loss: 5.9357920Losses:  7.351123332977295 3.129751205444336 0.23177537322044373
CurrentTrain: epoch  9, batch    15 | loss: 7.3511233Losses:  7.025757789611816 2.820892810821533 0.224287748336792
CurrentTrain: epoch  9, batch    16 | loss: 7.0257578Losses:  5.593614101409912 1.4356176853179932 0.21200883388519287
CurrentTrain: epoch  9, batch    17 | loss: 5.5936141Losses:  6.13522481918335 1.9680991172790527 0.21218882501125336
CurrentTrain: epoch  9, batch    18 | loss: 6.1352248Losses:  7.7944722175598145 3.6442196369171143 0.15596722066402435
CurrentTrain: epoch  9, batch    19 | loss: 7.7944722Losses:  7.049753665924072 2.8679885864257812 0.23429802060127258
CurrentTrain: epoch  9, batch    20 | loss: 7.0497537Losses:  8.02281379699707 3.8238327503204346 0.23252777755260468
CurrentTrain: epoch  9, batch    21 | loss: 8.0228138Losses:  6.697974681854248 2.524461269378662 0.21830356121063232
CurrentTrain: epoch  9, batch    22 | loss: 6.6979747Losses:  6.8360466957092285 2.6620559692382812 0.22065025568008423
CurrentTrain: epoch  9, batch    23 | loss: 6.8360467Losses:  6.862290382385254 2.6647706031799316 0.2274564504623413
CurrentTrain: epoch  9, batch    24 | loss: 6.8622904Losses:  7.304569721221924 3.123950242996216 0.23137155175209045
CurrentTrain: epoch  9, batch    25 | loss: 7.3045697Losses:  5.95782470703125 1.7936711311340332 0.2195194810628891
CurrentTrain: epoch  9, batch    26 | loss: 5.9578247Losses:  6.91744327545166 2.6905126571655273 0.22967886924743652
CurrentTrain: epoch  9, batch    27 | loss: 6.9174433Losses:  10.408284187316895 6.222203254699707 0.2464914172887802
CurrentTrain: epoch  9, batch    28 | loss: 10.4082842Losses:  7.482021808624268 3.3119726181030273 0.23765236139297485
CurrentTrain: epoch  9, batch    29 | loss: 7.4820218Losses:  6.730169296264648 2.5555434226989746 0.22029107809066772
CurrentTrain: epoch  9, batch    30 | loss: 6.7301693Losses:  6.270345211029053 2.1142516136169434 0.22298692166805267
CurrentTrain: epoch  9, batch    31 | loss: 6.2703452Losses:  8.092679977416992 3.898562431335449 0.2449958622455597
CurrentTrain: epoch  9, batch    32 | loss: 8.0926800Losses:  6.1710124015808105 1.9870619773864746 0.22517533600330353
CurrentTrain: epoch  9, batch    33 | loss: 6.1710124Losses:  11.135787010192871 7.020265579223633 0.2324252724647522
CurrentTrain: epoch  9, batch    34 | loss: 11.1357870Losses:  6.23905611038208 2.1115851402282715 0.2138737142086029
CurrentTrain: epoch  9, batch    35 | loss: 6.2390561Losses:  7.3097147941589355 3.1115710735321045 0.23179194331169128
CurrentTrain: epoch  9, batch    36 | loss: 7.3097148Losses:  7.256025314331055 3.129176616668701 0.15369975566864014
CurrentTrain: epoch  9, batch    37 | loss: 7.2560253Losses:  7.987851619720459 3.81141996383667 0.23091000318527222
CurrentTrain: epoch  9, batch    38 | loss: 7.9878516Losses:  6.568905353546143 2.368912696838379 0.2290571630001068
CurrentTrain: epoch  9, batch    39 | loss: 6.5689054Losses:  9.172440528869629 5.079495906829834 0.14850148558616638
CurrentTrain: epoch  9, batch    40 | loss: 9.1724405Losses:  5.9474077224731445 1.7733908891677856 0.21713882684707642
CurrentTrain: epoch  9, batch    41 | loss: 5.9474077Losses:  6.774104118347168 2.600489616394043 0.22817426919937134
CurrentTrain: epoch  9, batch    42 | loss: 6.7741041Losses:  7.907021522521973 3.726670742034912 0.24087989330291748
CurrentTrain: epoch  9, batch    43 | loss: 7.9070215Losses:  6.01246452331543 1.819326639175415 0.21324816346168518
CurrentTrain: epoch  9, batch    44 | loss: 6.0124645Losses:  6.728054046630859 2.5403671264648438 0.2254997044801712
CurrentTrain: epoch  9, batch    45 | loss: 6.7280540Losses:  8.063604354858398 3.909857988357544 0.21913990378379822
CurrentTrain: epoch  9, batch    46 | loss: 8.0636044Losses:  6.314934253692627 2.1439740657806396 0.22061587870121002
CurrentTrain: epoch  9, batch    47 | loss: 6.3149343Losses:  6.318650722503662 2.1407790184020996 0.22015956044197083
CurrentTrain: epoch  9, batch    48 | loss: 6.3186507Losses:  6.4841628074646 2.3244247436523438 0.21808482706546783
CurrentTrain: epoch  9, batch    49 | loss: 6.4841628Losses:  5.374950885772705 1.2339823246002197 0.20740455389022827
CurrentTrain: epoch  9, batch    50 | loss: 5.3749509Losses:  8.815305709838867 4.601491928100586 0.2313646376132965
CurrentTrain: epoch  9, batch    51 | loss: 8.8153057Losses:  6.974527359008789 2.7819063663482666 0.22926414012908936
CurrentTrain: epoch  9, batch    52 | loss: 6.9745274Losses:  6.793629169464111 2.6017115116119385 0.21694618463516235
CurrentTrain: epoch  9, batch    53 | loss: 6.7936292Losses:  7.823517322540283 3.6346211433410645 0.2415054738521576
CurrentTrain: epoch  9, batch    54 | loss: 7.8235173Losses:  8.958593368530273 4.749397277832031 0.22821608185768127
CurrentTrain: epoch  9, batch    55 | loss: 8.9585934Losses:  7.958813190460205 3.810607433319092 0.2352968156337738
CurrentTrain: epoch  9, batch    56 | loss: 7.9588132Losses:  6.350096702575684 2.1713361740112305 0.2191002368927002
CurrentTrain: epoch  9, batch    57 | loss: 6.3500967Losses:  7.6595940589904785 3.4613921642303467 0.23615027964115143
CurrentTrain: epoch  9, batch    58 | loss: 7.6595941Losses:  6.580877304077148 2.376950263977051 0.22519585490226746
CurrentTrain: epoch  9, batch    59 | loss: 6.5808773Losses:  8.686128616333008 4.512728691101074 0.23606157302856445
CurrentTrain: epoch  9, batch    60 | loss: 8.6861286Losses:  7.122854232788086 2.9636847972869873 0.2322995662689209
CurrentTrain: epoch  9, batch    61 | loss: 7.1228542Losses:  5.4883317947387695 1.3374075889587402 0.25872403383255005
CurrentTrain: epoch  9, batch    62 | loss: 5.4883318
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: born in copenhagen , as a young man bang became interested in surrealism as practised by giorgio de chirico , max ernst and yves tanguy .
Head Entity: yves tanguy
Tail Entity: surrealism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: during the early 20th century, the artist was heavily influenced by the cubist movement, which was pioneered by pablo picasso and georges braque.  
Head Entity: pablo picasso  
Tail Entity: cubist movement  

Relation: movement  
Context: the philosopher's ideas were central to the existentialist movement, which gained prominence in the mid-20th century through the works of thinkers like jean-paul sartre and simone de beauvoir.  
Head Entity: jean-paul sartre  
Tail Entity: existentialist movement  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: martin linge was born in norddal , møre og romsdal county , norway .
Head Entity: norddal
Tail Entity: møre og romsdal
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of barcelona is situated in the province of catalonia, spain.  
Head Entity: barcelona  
Tail Entity: catalonia  

Relation: located in the administrative territorial entity  
Context: the university of toronto is located in the province of ontario, canada.  
Head Entity: toronto  
Tail Entity: ontario  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: rage against the machine bandmates , tom morello , brad wilk , and de la rocha , as well as audioslave bandmate , chris cornell , came to support commerford .
Head Entity: chris cornell
Tail Entity: audioslave
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: The famous actor and musician, Jared Leto, is not only known for his role in "Requiem for a Dream" but also as the lead vocalist of the band Thirty Seconds to Mars.  
Head Entity: Jared Leto  
Tail Entity: Thirty Seconds to Mars  

Relation: member of  
Context: The renowned physicist, Stephen Hawking, was a member of the prestigious Royal Society, contributing significantly to the field of theoretical physics.  
Head Entity: Stephen Hawking  
Tail Entity: Royal Society  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: the belleville turnpike bridge ( also known as the rutgers street bridge ) crosses the passaic river , connecting belleville to north arlington .
Head Entity: belleville turnpike bridge
Tail Entity: passaic river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: the golden gate bridge crosses the golden gate strait, linking san francisco to marin county.  
Head Entity: golden gate bridge  
Tail Entity: golden gate strait  

Relation: crosses  
Context: the london bridge crosses the river thames, providing a route between the city of london and southwark.  
Head Entity: london bridge  
Tail Entity: river thames  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: the lake flows via the porcupine river and frederick house river to the abitibi river .
Head Entity: abitibi river
Tail Entity: frederick house river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River is fed by several smaller streams, including the Milk River and the Yellowstone River.  
Head Entity: Missouri River  
Tail Entity: Yellowstone River  

Relation: tributary  
Context: The Amazon River receives water from numerous tributaries, such as the Madeira River and the Tapajós River.  
Head Entity: Amazon River  
Tail Entity: Madeira River  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: from june 2010 to february 2012 , nyroos served as foreign policy advisor to the president of the republic tarja halonen and a member of the cabinet .
Head Entity: tarja halonen
Tail Entity: president of the republic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: in 2015, john smith was appointed as the chief executive officer of tech innovations inc., leading the company through a significant growth phase.  
Head Entity: john smith  
Tail Entity: chief executive officer  

Relation: position held  
Context: during her tenure from 2018 to 2020, dr. emily roberts served as the director of research at the national health institute, overseeing numerous groundbreaking studies.  
Head Entity: dr. emily roberts  
Tail Entity: director of research  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: it was developed by syn sophia , who also developed the other games in the " style savvy " series .
Head Entity: style savvy
Tail Entity: syn sophia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: the popular game was created by the renowned studio, pixel perfect, which is known for its innovative gameplay mechanics.  
Head Entity: pixel perfect  
Tail Entity: the popular game  

Relation: developer  
Context: the groundbreaking software was launched by tech giants, future innovations, who have been at the forefront of technology for over a decade.  
Head Entity: future innovations  
Tail Entity: the groundbreaking software  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: fuzzy duck is the self - titled album by london - based progressive rock band fuzzy duck .
Head Entity: fuzzy duck
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was founded in silicon valley by a group of innovative engineers.  
Head Entity: tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the famous painting was created by the artist in the vibrant city of paris.  
Head Entity: painting  
Tail Entity: paris  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: during the same month , the band supported kings of leon 's tour of the united states , and coldplay 's tour of the united kingdom .
Head Entity: kings of leon
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous Italian dish, pizza, has gained popularity worldwide, but its roots can be traced back to Naples, Italy.  
Head Entity: pizza  
Tail Entity: Italy  

Relation: country of origin  
Context: The iconic brand, Rolex, is renowned for its luxury watches, which are crafted in Switzerland, known for its precision and quality in watchmaking.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.76%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 88.39%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.97%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 91.48%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 92.19%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 92.41%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 92.97%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 93.38%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.40%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.05%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.03%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 94.02%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.01%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 94.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.23%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.21%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.20%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.18%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.73%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.89%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.04%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.00%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.14%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.23%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.35%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.58%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.68%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.83%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 95.92%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 95.88%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 95.96%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.05%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.12%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.08%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.03%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 95.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.06%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 95.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.76%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.72%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 95.58%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.66%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.62%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 95.70%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.67%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 94.94%   
cur_acc:  ['0.9494']
his_acc:  ['0.9494']
Clustering into  9  clusters
Clusters:  [2 0 7 1 8 8 4 3 0 0 1 2 2 0 5 2 0 6 3 0]
Losses:  10.512970924377441 2.323995590209961 0.6017912030220032
CurrentTrain: epoch  0, batch     0 | loss: 10.5129709Losses:  11.74139404296875 3.359050989151001 0.5079353451728821
CurrentTrain: epoch  0, batch     1 | loss: 11.7413940Losses:  11.942434310913086 4.410923957824707 0.6363735795021057
CurrentTrain: epoch  0, batch     2 | loss: 11.9424343Losses:  7.923927307128906 -0.0 0.10696377605199814
CurrentTrain: epoch  0, batch     3 | loss: 7.9239273Losses:  11.362671852111816 4.601746559143066 0.43271294236183167
CurrentTrain: epoch  1, batch     0 | loss: 11.3626719Losses:  9.321447372436523 2.4171526432037354 0.5775580406188965
CurrentTrain: epoch  1, batch     1 | loss: 9.3214474Losses:  8.318342208862305 2.8232150077819824 0.5214232802391052
CurrentTrain: epoch  1, batch     2 | loss: 8.3183422Losses:  9.566417694091797 -0.0 0.12858697772026062
CurrentTrain: epoch  1, batch     3 | loss: 9.5664177Losses:  8.920999526977539 2.9158358573913574 0.5593031644821167
CurrentTrain: epoch  2, batch     0 | loss: 8.9209995Losses:  8.358965873718262 2.9845590591430664 0.568862795829773
CurrentTrain: epoch  2, batch     1 | loss: 8.3589659Losses:  7.993409633636475 3.486072540283203 0.5547338724136353
CurrentTrain: epoch  2, batch     2 | loss: 7.9934096Losses:  4.171386241912842 -0.0 0.09268148988485336
CurrentTrain: epoch  2, batch     3 | loss: 4.1713862Losses:  7.763965606689453 3.7152085304260254 0.5643188953399658
CurrentTrain: epoch  3, batch     0 | loss: 7.7639656Losses:  7.723556995391846 2.820037841796875 0.5235249400138855
CurrentTrain: epoch  3, batch     1 | loss: 7.7235570Losses:  6.697722911834717 2.790437698364258 0.5229126811027527
CurrentTrain: epoch  3, batch     2 | loss: 6.6977229Losses:  5.678895950317383 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 5.6788960Losses:  9.54033088684082 5.046572685241699 0.27369076013565063
CurrentTrain: epoch  4, batch     0 | loss: 9.5403309Losses:  5.363011360168457 1.9620537757873535 0.49585986137390137
CurrentTrain: epoch  4, batch     1 | loss: 5.3630114Losses:  5.925847053527832 3.031545877456665 0.5143554210662842
CurrentTrain: epoch  4, batch     2 | loss: 5.9258471Losses:  4.819061756134033 -0.0 0.12218170613050461
CurrentTrain: epoch  4, batch     3 | loss: 4.8190618Losses:  6.076452255249023 2.7021360397338867 0.5164048671722412
CurrentTrain: epoch  5, batch     0 | loss: 6.0764523Losses:  6.970095634460449 3.1597886085510254 0.5174227356910706
CurrentTrain: epoch  5, batch     1 | loss: 6.9700956Losses:  5.675626754760742 2.231666326522827 0.42412930727005005
CurrentTrain: epoch  5, batch     2 | loss: 5.6756268Losses:  2.0655264854431152 -0.0 0.15892526507377625
CurrentTrain: epoch  5, batch     3 | loss: 2.0655265Losses:  7.992328643798828 5.229043960571289 0.42875224351882935
CurrentTrain: epoch  6, batch     0 | loss: 7.9923286Losses:  5.778225421905518 2.3939285278320312 0.39358338713645935
CurrentTrain: epoch  6, batch     1 | loss: 5.7782254Losses:  6.595373153686523 2.9034080505371094 0.5101970434188843
CurrentTrain: epoch  6, batch     2 | loss: 6.5953732Losses:  2.379781723022461 -0.0 0.13193738460540771
CurrentTrain: epoch  6, batch     3 | loss: 2.3797817Losses:  4.810490608215332 1.7181214094161987 0.5044920444488525
CurrentTrain: epoch  7, batch     0 | loss: 4.8104906Losses:  5.299370765686035 2.4856650829315186 0.38070014119148254
CurrentTrain: epoch  7, batch     1 | loss: 5.2993708Losses:  4.570404529571533 1.838494896888733 0.47926944494247437
CurrentTrain: epoch  7, batch     2 | loss: 4.5704045Losses:  2.346776008605957 -0.0 0.18275344371795654
CurrentTrain: epoch  7, batch     3 | loss: 2.3467760Losses:  4.975770950317383 2.1848909854888916 0.5117300748825073
CurrentTrain: epoch  8, batch     0 | loss: 4.9757710Losses:  5.5349440574646 2.569218873977661 0.47653335332870483
CurrentTrain: epoch  8, batch     1 | loss: 5.5349441Losses:  4.626616954803467 2.1432623863220215 0.4728066623210907
CurrentTrain: epoch  8, batch     2 | loss: 4.6266170Losses:  4.243139266967773 -0.0 0.10203257948160172
CurrentTrain: epoch  8, batch     3 | loss: 4.2431393Losses:  5.776795387268066 2.7068727016448975 0.4706494212150574
CurrentTrain: epoch  9, batch     0 | loss: 5.7767954Losses:  6.00689697265625 3.4688282012939453 0.4020155072212219
CurrentTrain: epoch  9, batch     1 | loss: 6.0068970Losses:  6.554628372192383 4.203179359436035 0.38487714529037476
CurrentTrain: epoch  9, batch     2 | loss: 6.5546284Losses:  1.8305028676986694 -0.0 0.12450283020734787
CurrentTrain: epoch  9, batch     3 | loss: 1.8305029
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: barack obama served as a member of the democratic party during his presidency from 2009 to 2017.  
Head Entity: barack obama  
Tail Entity: democratic party  

Relation: member of political party  
Context: angela merkel was a prominent member of the christian democratic union throughout her political career in germany.  
Head Entity: angela merkel  
Tail Entity: christian democratic union  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film is based on the novel "the great gatsby" by f. scott fitzgerald, capturing the essence of the roaring twenties.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: this musical is inspired by the play "hamlet" written by william shakespeare, reimagining the classic tragedy in a contemporary setting.  
Head Entity: hamlet  
Tail Entity: william shakespeare  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: several paintings are also mentioned , including rousseau 's " the dream " and van gogh 's " bedroom in arles " .
Head Entity: rousseau
Tail Entity: the dream
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: Among his many contributions to literature, one of his most acclaimed novels is "The Great Gatsby," which explores themes of wealth and social change in 1920s America.  
Head Entity: F. Scott Fitzgerald  
Tail Entity: The Great Gatsby  

Relation: notable work  
Context: The artist is best known for his groundbreaking sculpture "The Thinker," which has become a symbol of philosophy and contemplation.  
Head Entity: Auguste Rodin  
Tail Entity: The Thinker  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in software development for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after years of expansion, the non-profit organization moved its headquarters to new york city to better serve its growing membership.  
Head Entity: non-profit organization  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: since march 2006 , wkxp 's programming has been simulcasted on 97.3 wzad wurtsboro , new york .
Head Entity: wzad
Tail Entity: wurtsboro , new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: in 2010, the station wxyz received approval to broadcast its signal to the area of springfield, massachusetts.  
Head Entity: wxyz  
Tail Entity: springfield, massachusetts  

Relation: licensed to broadcast to  
Context: after acquiring the necessary permits, kqrs began broadcasting to the residents of minneapolis, minnesota in early 2015.  
Head Entity: kqrs  
Tail Entity: minneapolis, minnesota  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ζ microscopii , latinised as zeta microscopii , is a solitary , yellow - white hued star in the southern constellation of microscopium .
Head Entity: zeta microscopii
Tail Entity: microscopium
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: The star Betelgeuse is located in the constellation of Orion, which is one of the most recognizable constellations in the night sky.  
Head Entity: Betelgeuse  
Tail Entity: Orion  

Relation: constellation  
Context: The Andromeda Galaxy is part of the Andromeda constellation, which is named after a princess in Greek mythology.  
Head Entity: Andromeda Galaxy  
Tail Entity: Andromeda
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " modnation racers " for playstation 3 was announced and first shown publicly at e3 2009 .
Head Entity: modnation racers
Tail Entity: playstation 3
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was released for the Nintendo Switch and Wii U in March 2017.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Call of Duty: Warzone" is available on multiple platforms including PlayStation 4, Xbox One, and PC.  
Head Entity: Call of Duty: Warzone  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: services to the island run from san juan 's luis muñoz marín international airport , ceiba airport or isla grande airport ( 20- to 30-minute flight ) .
Head Entity: isla grande airport
Tail Entity: san juan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: The city of Barcelona is well-connected to the surrounding regions through the El Prat Airport, which offers numerous flights to various destinations.  
Head Entity: El Prat Airport  
Tail Entity: Barcelona  

Relation: place served by transport hub  
Context: The central train station in Zurich provides access to multiple Swiss cities and international destinations, making it a key transport hub for travelers.  
Head Entity: Zurich central train station  
Tail Entity: Zurich  
Losses:  6.3674445152282715 0.9590380191802979 0.5750029683113098
MemoryTrain:  epoch  0, batch     0 | loss: 6.3674445Losses:  8.933499336242676 1.6811851263046265 0.6160090565681458
MemoryTrain:  epoch  0, batch     1 | loss: 8.9334993Losses:  7.2956390380859375 1.8590887784957886 0.5562715530395508
MemoryTrain:  epoch  0, batch     2 | loss: 7.2956390Losses:  5.411586284637451 0.32389235496520996 0.423603355884552
MemoryTrain:  epoch  0, batch     3 | loss: 5.4115863Losses:  6.567586898803711 1.7018593549728394 0.4085303843021393
MemoryTrain:  epoch  1, batch     0 | loss: 6.5675869Losses:  7.644439697265625 1.4667710065841675 0.4981381893157959
MemoryTrain:  epoch  1, batch     1 | loss: 7.6444397Losses:  5.755560398101807 1.3086555004119873 0.6016535758972168
MemoryTrain:  epoch  1, batch     2 | loss: 5.7555604Losses:  4.728465557098389 0.2925558090209961 0.4525952935218811
MemoryTrain:  epoch  1, batch     3 | loss: 4.7284656Losses:  5.007309913635254 0.4989522099494934 0.6068451404571533
MemoryTrain:  epoch  2, batch     0 | loss: 5.0073099Losses:  5.505377292633057 0.8798060417175293 0.6005048751831055
MemoryTrain:  epoch  2, batch     1 | loss: 5.5053773Losses:  4.824700355529785 1.1857285499572754 0.5688068866729736
MemoryTrain:  epoch  2, batch     2 | loss: 4.8247004Losses:  5.460315227508545 0.7871479392051697 0.5303386449813843
MemoryTrain:  epoch  2, batch     3 | loss: 5.4603152Losses:  4.461604595184326 0.48950180411338806 0.5649356245994568
MemoryTrain:  epoch  3, batch     0 | loss: 4.4616046Losses:  4.214667797088623 0.680012583732605 0.5423642992973328
MemoryTrain:  epoch  3, batch     1 | loss: 4.2146678Losses:  4.476050853729248 0.5136613845825195 0.628753662109375
MemoryTrain:  epoch  3, batch     2 | loss: 4.4760509Losses:  5.295843601226807 0.8972578048706055 0.5177095532417297
MemoryTrain:  epoch  3, batch     3 | loss: 5.2958436Losses:  4.5054755210876465 1.0393956899642944 0.5374559164047241
MemoryTrain:  epoch  4, batch     0 | loss: 4.5054755Losses:  3.86210560798645 0.5349694490432739 0.46270832419395447
MemoryTrain:  epoch  4, batch     1 | loss: 3.8621056Losses:  4.3307719230651855 0.9712698459625244 0.47485634684562683
MemoryTrain:  epoch  4, batch     2 | loss: 4.3307719Losses:  3.8525257110595703 1.0338878631591797 0.5492678880691528
MemoryTrain:  epoch  4, batch     3 | loss: 3.8525257Losses:  3.654115676879883 1.1563434600830078 0.7245191335678101
MemoryTrain:  epoch  5, batch     0 | loss: 3.6541157Losses:  3.7129123210906982 0.6743983626365662 0.6423690319061279
MemoryTrain:  epoch  5, batch     1 | loss: 3.7129123Losses:  3.9927470684051514 0.9709978699684143 0.5658314228057861
MemoryTrain:  epoch  5, batch     2 | loss: 3.9927471Losses:  4.854549407958984 0.9471885561943054 0.4539811313152313
MemoryTrain:  epoch  5, batch     3 | loss: 4.8545494Losses:  3.749713897705078 0.8289881944656372 0.4754728674888611
MemoryTrain:  epoch  6, batch     0 | loss: 3.7497139Losses:  4.269960403442383 1.724034070968628 0.4520781934261322
MemoryTrain:  epoch  6, batch     1 | loss: 4.2699604Losses:  4.117951393127441 1.5944432020187378 0.6297024488449097
MemoryTrain:  epoch  6, batch     2 | loss: 4.1179514Losses:  4.918695449829102 0.6854391098022461 0.518221378326416
MemoryTrain:  epoch  6, batch     3 | loss: 4.9186954Losses:  4.0795698165893555 1.0797250270843506 0.579604983329773
MemoryTrain:  epoch  7, batch     0 | loss: 4.0795698Losses:  4.722861289978027 1.11744225025177 0.39710116386413574
MemoryTrain:  epoch  7, batch     1 | loss: 4.7228613Losses:  3.3949389457702637 0.9472416043281555 0.6473070383071899
MemoryTrain:  epoch  7, batch     2 | loss: 3.3949389Losses:  3.2659995555877686 1.030539870262146 0.5657199025154114
MemoryTrain:  epoch  7, batch     3 | loss: 3.2659996Losses:  3.181648015975952 0.5573375821113586 0.7229722142219543
MemoryTrain:  epoch  8, batch     0 | loss: 3.1816480Losses:  3.2525110244750977 0.2376648485660553 0.5316860675811768
MemoryTrain:  epoch  8, batch     1 | loss: 3.2525110Losses:  3.336026668548584 0.667802095413208 0.6352957487106323
MemoryTrain:  epoch  8, batch     2 | loss: 3.3360267Losses:  2.5476114749908447 -0.0 0.43517857789993286
MemoryTrain:  epoch  8, batch     3 | loss: 2.5476115Losses:  4.549814224243164 1.4392201900482178 0.4517029821872711
MemoryTrain:  epoch  9, batch     0 | loss: 4.5498142Losses:  2.417715072631836 0.44346991181373596 0.6317600011825562
MemoryTrain:  epoch  9, batch     1 | loss: 2.4177151Losses:  4.226279258728027 1.1253292560577393 0.4824378490447998
MemoryTrain:  epoch  9, batch     2 | loss: 4.2262793Losses:  2.60927677154541 0.4186091423034668 0.5628293752670288
MemoryTrain:  epoch  9, batch     3 | loss: 2.6092768
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 89.06%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 90.28%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 91.25%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 92.05%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 91.67%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 92.31%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 91.52%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 91.25%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 90.23%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 90.07%   [EVAL] batch:   17 | acc: 75.00%,  total acc: 89.24%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 88.82%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 86.56%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 84.82%   [EVAL] batch:   21 | acc: 37.50%,  total acc: 82.67%   [EVAL] batch:   22 | acc: 31.25%,  total acc: 80.43%   [EVAL] batch:   23 | acc: 37.50%,  total acc: 78.65%   [EVAL] batch:   24 | acc: 18.75%,  total acc: 76.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 77.16%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.01%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 78.79%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 79.53%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.21%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 80.85%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 79.36%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 78.68%   [EVAL] batch:   34 | acc: 62.50%,  total acc: 78.21%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 77.60%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 77.20%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 76.81%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 77.40%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 77.97%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 78.51%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 79.02%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 79.51%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 79.83%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 80.71%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 81.12%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 82.25%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 82.23%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 82.45%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 82.78%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 82.99%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 83.07%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 83.37%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 83.44%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 83.30%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 83.37%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 83.65%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 83.81%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 83.87%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 83.23%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 89.06%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 88.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 84.82%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 84.72%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 84.38%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 82.29%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 82.69%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.48%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 84.17%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.16%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 86.03%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 87.19%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 87.20%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 86.93%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 86.68%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 86.72%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 86.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 87.26%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.73%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 88.17%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 88.36%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.75%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 89.11%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 89.45%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.77%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 90.07%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 90.18%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 90.45%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.71%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.95%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 91.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.62%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.82%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 91.72%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 91.94%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 91.85%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 91.62%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.84%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.88%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.91%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.95%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.98%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 92.01%   [EVAL] batch:   54 | acc: 68.75%,  total acc: 91.59%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 91.52%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 91.56%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 91.59%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 91.74%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 91.88%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 92.01%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 92.16%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 92.09%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 92.02%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 92.14%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 91.88%   [EVAL] batch:   67 | acc: 81.25%,  total acc: 91.73%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 91.76%   [EVAL] batch:   69 | acc: 93.75%,  total acc: 91.79%   [EVAL] batch:   70 | acc: 100.00%,  total acc: 91.90%   [EVAL] batch:   71 | acc: 100.00%,  total acc: 92.01%   [EVAL] batch:   72 | acc: 100.00%,  total acc: 92.12%   [EVAL] batch:   73 | acc: 93.75%,  total acc: 92.15%   [EVAL] batch:   74 | acc: 93.75%,  total acc: 92.17%   [EVAL] batch:   75 | acc: 87.50%,  total acc: 92.11%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 92.05%   [EVAL] batch:   77 | acc: 93.75%,  total acc: 92.07%   [EVAL] batch:   78 | acc: 62.50%,  total acc: 91.69%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 91.64%   [EVAL] batch:   80 | acc: 75.00%,  total acc: 91.44%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 91.08%   [EVAL] batch:   82 | acc: 50.00%,  total acc: 90.59%   [EVAL] batch:   83 | acc: 43.75%,  total acc: 90.03%   [EVAL] batch:   84 | acc: 37.50%,  total acc: 89.41%   [EVAL] batch:   85 | acc: 31.25%,  total acc: 88.74%   [EVAL] batch:   86 | acc: 25.00%,  total acc: 88.00%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 87.71%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 87.85%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 87.99%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 88.25%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 88.37%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 88.43%   [EVAL] batch:   94 | acc: 37.50%,  total acc: 87.89%   [EVAL] batch:   95 | acc: 62.50%,  total acc: 87.63%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 87.37%   [EVAL] batch:   97 | acc: 50.00%,  total acc: 86.99%   [EVAL] batch:   98 | acc: 68.75%,  total acc: 86.81%   [EVAL] batch:   99 | acc: 37.50%,  total acc: 86.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 86.45%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 86.58%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 86.71%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 86.96%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 87.03%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 87.15%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 87.27%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 87.39%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 87.61%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 87.72%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 87.78%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 87.72%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 87.83%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 87.93%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 87.93%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 87.98%   [EVAL] batch:  118 | acc: 87.50%,  total acc: 87.97%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 88.07%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 87.91%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 87.96%   [EVAL] batch:  122 | acc: 93.75%,  total acc: 88.01%   [EVAL] batch:  123 | acc: 93.75%,  total acc: 88.05%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 88.05%   
cur_acc:  ['0.9494', '0.8323']
his_acc:  ['0.9494', '0.8805']
Clustering into  14  clusters
Clusters:  [10  5  9  1  6  6  8  2  0 11  1 10 10  0  7  4  0 13  2  0  5  1 10 12
  1  3  4  0 10  0]
Losses:  13.601851463317871 5.173300266265869 0.6268240213394165
CurrentTrain: epoch  0, batch     0 | loss: 13.6018515Losses:  10.238631248474121 3.3491344451904297 0.6100934147834778
CurrentTrain: epoch  0, batch     1 | loss: 10.2386312Losses:  12.027030944824219 4.336359977722168 0.7594486474990845
CurrentTrain: epoch  0, batch     2 | loss: 12.0270309Losses:  7.811557769775391 -0.0 0.12293631583452225
CurrentTrain: epoch  0, batch     3 | loss: 7.8115578Losses:  12.583285331726074 5.008079528808594 0.6068770289421082
CurrentTrain: epoch  1, batch     0 | loss: 12.5832853Losses:  10.424947738647461 4.480578422546387 0.557016909122467
CurrentTrain: epoch  1, batch     1 | loss: 10.4249477Losses:  10.834205627441406 3.7726902961730957 0.6813361644744873
CurrentTrain: epoch  1, batch     2 | loss: 10.8342056Losses:  4.2209978103637695 -0.0 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 4.2209978Losses:  11.433415412902832 5.180689334869385 0.6608455181121826
CurrentTrain: epoch  2, batch     0 | loss: 11.4334154Losses:  8.827499389648438 2.9381332397460938 0.6566987037658691
CurrentTrain: epoch  2, batch     1 | loss: 8.8274994Losses:  9.844890594482422 4.112298965454102 0.5309669971466064
CurrentTrain: epoch  2, batch     2 | loss: 9.8448906Losses:  5.193697452545166 -0.0 0.12997499108314514
CurrentTrain: epoch  2, batch     3 | loss: 5.1936975Losses:  8.993962287902832 3.778872013092041 0.5596663951873779
CurrentTrain: epoch  3, batch     0 | loss: 8.9939623Losses:  8.181962013244629 2.907104015350342 0.667483389377594
CurrentTrain: epoch  3, batch     1 | loss: 8.1819620Losses:  8.70668888092041 3.4189114570617676 0.5277640223503113
CurrentTrain: epoch  3, batch     2 | loss: 8.7066889Losses:  5.224873065948486 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 5.2248731Losses:  9.95543384552002 4.535622596740723 0.6254127025604248
CurrentTrain: epoch  4, batch     0 | loss: 9.9554338Losses:  8.336344718933105 4.1042938232421875 0.514549195766449
CurrentTrain: epoch  4, batch     1 | loss: 8.3363447Losses:  8.133056640625 3.299464464187622 0.6203112006187439
CurrentTrain: epoch  4, batch     2 | loss: 8.1330566Losses:  2.9668822288513184 -0.0 0.12751811742782593
CurrentTrain: epoch  4, batch     3 | loss: 2.9668822Losses:  7.50422477722168 3.045436382293701 0.6348389387130737
CurrentTrain: epoch  5, batch     0 | loss: 7.5042248Losses:  7.574014663696289 3.5868656635284424 0.4619155824184418
CurrentTrain: epoch  5, batch     1 | loss: 7.5740147Losses:  8.84662914276123 3.828550338745117 0.4458792805671692
CurrentTrain: epoch  5, batch     2 | loss: 8.8466291Losses:  4.748771667480469 -0.0 0.13216766715049744
CurrentTrain: epoch  5, batch     3 | loss: 4.7487717Losses:  7.1406073570251465 2.575690984725952 0.6251341104507446
CurrentTrain: epoch  6, batch     0 | loss: 7.1406074Losses:  8.114856719970703 3.778247833251953 0.6250222325325012
CurrentTrain: epoch  6, batch     1 | loss: 8.1148567Losses:  6.234878063201904 2.0693118572235107 0.6183884739875793
CurrentTrain: epoch  6, batch     2 | loss: 6.2348781Losses:  1.9965916872024536 -0.0 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 1.9965917Losses:  6.3238677978515625 2.589407444000244 0.6240872144699097
CurrentTrain: epoch  7, batch     0 | loss: 6.3238678Losses:  8.276659965515137 3.862553596496582 0.5344254970550537
CurrentTrain: epoch  7, batch     1 | loss: 8.2766600Losses:  7.719007968902588 3.94765305519104 0.5161747336387634
CurrentTrain: epoch  7, batch     2 | loss: 7.7190080Losses:  3.5243844985961914 -0.0 0.11707791686058044
CurrentTrain: epoch  7, batch     3 | loss: 3.5243845Losses:  8.705089569091797 4.041364669799805 0.49588149785995483
CurrentTrain: epoch  8, batch     0 | loss: 8.7050896Losses:  6.013206481933594 2.451082229614258 0.6062812209129333
CurrentTrain: epoch  8, batch     1 | loss: 6.0132065Losses:  4.954352378845215 1.8504239320755005 0.5936320424079895
CurrentTrain: epoch  8, batch     2 | loss: 4.9543524Losses:  3.0934786796569824 -0.0 0.09683026373386383
CurrentTrain: epoch  8, batch     3 | loss: 3.0934787Losses:  7.054133415222168 3.4591546058654785 0.5850238800048828
CurrentTrain: epoch  9, batch     0 | loss: 7.0541334Losses:  6.588095664978027 3.2722582817077637 0.5436804890632629
CurrentTrain: epoch  9, batch     1 | loss: 6.5880957Losses:  7.768581390380859 3.990206480026245 0.5369753241539001
CurrentTrain: epoch  9, batch     2 | loss: 7.7685814Losses:  2.209986686706543 -0.0 0.19262582063674927
CurrentTrain: epoch  9, batch     3 | loss: 2.2099867
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: the andesite line , a zone of intense volcanic and seismic activity , is a major regional distinction in the pacific .
Head Entity: andesite line
Tail Entity: pacific
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The Great Barrier Reef, a stunning natural wonder, stretches along the northeastern coast of Australia.  
Head Entity: Great Barrier Reef  
Tail Entity: Australia  

Relation: located on terrain feature  
Context: Mount Everest, the highest peak in the world, is part of the Himalayas and attracts climbers from around the globe.  
Head Entity: Mount Everest  
Tail Entity: Himalayas  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: tiago venâncio ( born august 19 , 1987 , setúbal ) is a freestyle swimmer from portugal , who represented his home country at the 2004 summer olympics in athens , greece .
Head Entity: tiago venâncio
Tail Entity: 2004 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: the united nations climate change conference, also known as cop26, took place in glasgow, scotland, where numerous world leaders gathered to discuss climate action.  
Head Entity: united nations climate change conference  
Tail Entity: cop26  

Relation: participant of  
Context: elon musk, the ceo of spacex, was a key participant in the 2021 met gala, showcasing his vision for sustainable energy and space exploration.  
Head Entity: elon musk  
Tail Entity: 2021 met gala  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: luna subsequently had a supporting role in the comedy film " gravy " , directed by james roday , which was released in october 2015 .
Head Entity: gravy
Tail Entity: james roday
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the acclaimed drama series "the crown" was brought to life by the visionary director peter morgan, who skillfully navigated the complexities of the royal family.  
Head Entity: the crown  
Tail Entity: peter morgan  

Relation: director  
Context: in the thrilling action movie "inception," the brilliant director christopher nolan crafted a mind-bending narrative that captivated audiences worldwide.  
Head Entity: inception  
Tail Entity: christopher nolan  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game franchise Call of Duty was acquired by Activision Blizzard, which has been a major player in the gaming industry for years.  
Head Entity: Call of Duty  
Tail Entity: Activision Blizzard  

Relation: owned by  
Context: The iconic fashion brand Gucci is a subsidiary of the French luxury group Kering, which oversees several high-end labels.  
Head Entity: Gucci  
Tail Entity: Kering  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: " put a little umph in it " is the first single from jagged edge 's studio album " baby makin ' project " .
Head Entity: put a little umph in it
Tail Entity: baby makin ' project
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial component of the car, providing the power needed for movement.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential organ in the human body, responsible for pumping blood throughout.  
Head Entity: heart  
Tail Entity: human body  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts various events and is home to the local art gallery.  
Head Entity: cultural center  
Tail Entity: local art gallery  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant community hub, serving as the headquarters for several non-profit organizations.  
Head Entity: community hub  
Tail Entity: non-profit organizations  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: Frank Lloyd Wright is renowned for his innovative designs, including the famous Fallingwater house, which seamlessly integrates with its natural surroundings.  
Head Entity: Fallingwater  
Tail Entity: Frank Lloyd Wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: fantcha was born as francelina durão almeida in mindelo on são vicente island , cape verde .
Head Entity: fantcha
Tail Entity: são vicente island
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york city for several years, john decided to move back to his hometown in los angeles.  
Head Entity: john  
Tail Entity: los angeles  

Relation: residence  
Context: the famous author spent most of her life in a quaint cottage located in the picturesque village of bath, england.  
Head Entity: the famous author  
Tail Entity: bath
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned musician was celebrated for his innovative compositions, including the famous symphony that showcased his unique style.  
Head Entity: famous symphony  
Tail Entity: renowned musician  

Relation: composer  
Context: She dedicated her life to creating beautiful melodies, and her latest album features a piece inspired by the works of a legendary composer.  
Head Entity: legendary composer  
Tail Entity: beautiful melodies  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: the exhibition was shown for the public at kulturen i lund , regionmuseet kristianstad and falkenbergs museum .
Head Entity: kulturen
Tail Entity: lund
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The conference will be held at the Grand Hotel in Stockholm, which is known for its luxurious accommodations.  
Head Entity: Grand Hotel  
Tail Entity: Stockholm  

Relation: location  
Context: The famous painting was created in the artist's studio located in the heart of Paris.  
Head Entity: artist's studio  
Tail Entity: Paris  
Losses:  5.737092018127441 1.0182502269744873 0.46540045738220215
MemoryTrain:  epoch  0, batch     0 | loss: 5.7370920Losses:  7.88705587387085 1.1736184358596802 0.6303965449333191
MemoryTrain:  epoch  0, batch     1 | loss: 7.8870559Losses:  6.049770832061768 1.5065124034881592 0.7735123634338379
MemoryTrain:  epoch  0, batch     2 | loss: 6.0497708Losses:  4.659634590148926 0.25948020815849304 0.7904173731803894
MemoryTrain:  epoch  0, batch     3 | loss: 4.6596346Losses:  7.243303298950195 1.3796918392181396 0.6379327774047852
MemoryTrain:  epoch  0, batch     4 | loss: 7.2433033Losses:  6.034419536590576 0.5332536101341248 0.5268288254737854
MemoryTrain:  epoch  0, batch     5 | loss: 6.0344195Losses:  7.327249526977539 0.8858474493026733 0.7989609241485596
MemoryTrain:  epoch  1, batch     0 | loss: 7.3272495Losses:  6.281364440917969 0.9663140773773193 0.6742209196090698
MemoryTrain:  epoch  1, batch     1 | loss: 6.2813644Losses:  5.384995460510254 0.5185052156448364 0.6760506629943848
MemoryTrain:  epoch  1, batch     2 | loss: 5.3849955Losses:  4.532169342041016 -0.0 0.871936559677124
MemoryTrain:  epoch  1, batch     3 | loss: 4.5321693Losses:  4.597358703613281 1.218717098236084 0.6705096960067749
MemoryTrain:  epoch  1, batch     4 | loss: 4.5973587Losses:  6.031244277954102 1.592023253440857 0.5817216634750366
MemoryTrain:  epoch  1, batch     5 | loss: 6.0312443Losses:  4.687699317932129 0.6262876391410828 0.6383626461029053
MemoryTrain:  epoch  2, batch     0 | loss: 4.6876993Losses:  5.046299934387207 0.26732760667800903 0.6881636381149292
MemoryTrain:  epoch  2, batch     1 | loss: 5.0462999Losses:  3.8645639419555664 0.3094215989112854 0.846368670463562
MemoryTrain:  epoch  2, batch     2 | loss: 3.8645639Losses:  3.8551836013793945 0.49631813168525696 0.8663581609725952
MemoryTrain:  epoch  2, batch     3 | loss: 3.8551836Losses:  5.534392356872559 0.8165314793586731 0.6216090321540833
MemoryTrain:  epoch  2, batch     4 | loss: 5.5343924Losses:  3.9058594703674316 -0.0 0.6649614572525024
MemoryTrain:  epoch  2, batch     5 | loss: 3.9058595Losses:  4.237239837646484 0.620158314704895 0.5847103595733643
MemoryTrain:  epoch  3, batch     0 | loss: 4.2372398Losses:  3.686309337615967 -0.0 0.8336073160171509
MemoryTrain:  epoch  3, batch     1 | loss: 3.6863093Losses:  5.156960487365723 1.135457158088684 0.6332783699035645
MemoryTrain:  epoch  3, batch     2 | loss: 5.1569605Losses:  4.7035136222839355 0.8094749450683594 0.8487054705619812
MemoryTrain:  epoch  3, batch     3 | loss: 4.7035136Losses:  3.186516046524048 -0.0 0.7956957221031189
MemoryTrain:  epoch  3, batch     4 | loss: 3.1865160Losses:  3.044955253601074 0.28039538860321045 0.5520920157432556
MemoryTrain:  epoch  3, batch     5 | loss: 3.0449553Losses:  4.218242168426514 0.5270664691925049 0.8400450944900513
MemoryTrain:  epoch  4, batch     0 | loss: 4.2182422Losses:  4.521936416625977 0.6111394166946411 0.6610915660858154
MemoryTrain:  epoch  4, batch     1 | loss: 4.5219364Losses:  3.360013961791992 0.29468870162963867 0.7548189163208008
MemoryTrain:  epoch  4, batch     2 | loss: 3.3600140Losses:  4.07170295715332 1.4038366079330444 0.5088999271392822
MemoryTrain:  epoch  4, batch     3 | loss: 4.0717030Losses:  4.017923831939697 0.5495143532752991 0.8293415307998657
MemoryTrain:  epoch  4, batch     4 | loss: 4.0179238Losses:  2.8470706939697266 0.22673754394054413 0.5934856534004211
MemoryTrain:  epoch  4, batch     5 | loss: 2.8470707Losses:  3.084385395050049 0.25068730115890503 0.721154510974884
MemoryTrain:  epoch  5, batch     0 | loss: 3.0843854Losses:  3.5678420066833496 0.2727280259132385 0.7888553738594055
MemoryTrain:  epoch  5, batch     1 | loss: 3.5678420Losses:  3.6472365856170654 0.27795034646987915 0.8347908854484558
MemoryTrain:  epoch  5, batch     2 | loss: 3.6472366Losses:  3.039724826812744 0.49604642391204834 0.6776323318481445
MemoryTrain:  epoch  5, batch     3 | loss: 3.0397248Losses:  3.9544761180877686 1.140409231185913 0.8431667685508728
MemoryTrain:  epoch  5, batch     4 | loss: 3.9544761Losses:  3.487931251525879 -0.0 0.6613683104515076
MemoryTrain:  epoch  5, batch     5 | loss: 3.4879313Losses:  3.4443187713623047 0.5586671233177185 0.8916060924530029
MemoryTrain:  epoch  6, batch     0 | loss: 3.4443188Losses:  3.4413273334503174 1.0247735977172852 0.7383406162261963
MemoryTrain:  epoch  6, batch     1 | loss: 3.4413273Losses:  3.319065809249878 0.4909825325012207 0.673563539981842
MemoryTrain:  epoch  6, batch     2 | loss: 3.3190658Losses:  3.3594326972961426 0.3311111330986023 0.8263013958930969
MemoryTrain:  epoch  6, batch     3 | loss: 3.3594327Losses:  4.669968605041504 1.7447080612182617 0.4996809959411621
MemoryTrain:  epoch  6, batch     4 | loss: 4.6699686Losses:  2.8664703369140625 -0.0 0.5194706320762634
MemoryTrain:  epoch  6, batch     5 | loss: 2.8664703Losses:  3.276540756225586 0.4813140332698822 0.7526227235794067
MemoryTrain:  epoch  7, batch     0 | loss: 3.2765408Losses:  3.1897976398468018 0.7410854697227478 0.7121763229370117
MemoryTrain:  epoch  7, batch     1 | loss: 3.1897976Losses:  3.122434139251709 0.25327596068382263 0.7827128171920776
MemoryTrain:  epoch  7, batch     2 | loss: 3.1224341Losses:  3.290792942047119 0.5431934595108032 0.8324407935142517
MemoryTrain:  epoch  7, batch     3 | loss: 3.2907929Losses:  2.9878196716308594 0.21438926458358765 0.7528108954429626
MemoryTrain:  epoch  7, batch     4 | loss: 2.9878197Losses:  3.676945209503174 1.4870105981826782 0.3150990605354309
MemoryTrain:  epoch  7, batch     5 | loss: 3.6769452Losses:  2.873173475265503 0.20530521869659424 0.8390476107597351
MemoryTrain:  epoch  8, batch     0 | loss: 2.8731735Losses:  3.2441577911376953 1.0104880332946777 0.8085559010505676
MemoryTrain:  epoch  8, batch     1 | loss: 3.2441578Losses:  4.700681686401367 1.3529212474822998 0.6839792132377625
MemoryTrain:  epoch  8, batch     2 | loss: 4.7006817Losses:  3.421653985977173 1.0434640645980835 0.7019010186195374
MemoryTrain:  epoch  8, batch     3 | loss: 3.4216540Losses:  3.118466854095459 0.8625156879425049 0.6484628915786743
MemoryTrain:  epoch  8, batch     4 | loss: 3.1184669Losses:  2.3199219703674316 0.3286730945110321 0.5465711951255798
MemoryTrain:  epoch  8, batch     5 | loss: 2.3199220Losses:  2.9280166625976562 0.7313766479492188 0.7260385155677795
MemoryTrain:  epoch  9, batch     0 | loss: 2.9280167Losses:  3.191926956176758 0.7179343700408936 0.8411941528320312
MemoryTrain:  epoch  9, batch     1 | loss: 3.1919270Losses:  2.833836555480957 0.2606455087661743 0.7727827429771423
MemoryTrain:  epoch  9, batch     2 | loss: 2.8338366Losses:  3.5170202255249023 0.679365873336792 0.7454846501350403
MemoryTrain:  epoch  9, batch     3 | loss: 3.5170202Losses:  2.9310245513916016 0.5421707630157471 0.5811015367507935
MemoryTrain:  epoch  9, batch     4 | loss: 2.9310246Losses:  2.3397510051727295 0.2503456771373749 0.6656967401504517
MemoryTrain:  epoch  9, batch     5 | loss: 2.3397510
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 32.81%   [EVAL] batch:    4 | acc: 50.00%,  total acc: 36.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 35.42%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 93.75%,  total acc: 52.78%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 60.23%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 64.42%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.52%   [EVAL] batch:   14 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 70.70%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:   17 | acc: 87.50%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 68.75%,  total acc: 72.70%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   20 | acc: 43.75%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 70.74%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 70.65%   [EVAL] batch:   23 | acc: 50.00%,  total acc: 69.79%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 69.75%   [EVAL] batch:   25 | acc: 12.50%,  total acc: 67.55%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 65.51%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 63.39%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 62.07%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 60.21%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 58.27%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 58.20%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 59.28%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 60.29%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 61.07%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 61.81%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 62.33%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 63.16%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 63.62%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 65.24%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 65.48%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 66.28%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 66.90%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 67.36%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 67.66%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 67.82%   [EVAL] batch:   47 | acc: 68.75%,  total acc: 67.84%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 68.11%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 68.50%   [EVAL] batch:   50 | acc: 18.75%,  total acc: 67.52%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 67.07%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 66.63%   [EVAL] batch:   53 | acc: 50.00%,  total acc: 66.32%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 66.14%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 66.07%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 65.57%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 65.41%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 65.25%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 65.21%   [EVAL] batch:   60 | acc: 56.25%,  total acc: 65.06%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 65.32%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 64.68%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 86.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 82.14%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 80.47%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 77.27%   [EVAL] batch:   11 | acc: 56.25%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 80.51%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.24%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 82.81%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 83.24%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 83.15%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 84.49%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 85.34%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.29%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 86.72%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.12%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 87.86%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 88.19%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 88.82%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.88%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.97%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 90.06%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 89.72%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 89.40%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 88.83%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 88.80%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 88.78%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 88.62%   [EVAL] batch:   50 | acc: 68.75%,  total acc: 88.24%   [EVAL] batch:   51 | acc: 68.75%,  total acc: 87.86%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 87.74%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 86.82%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 86.61%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 86.62%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 86.42%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 86.65%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 86.77%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 86.89%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 87.00%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 87.10%   [EVAL] batch:   63 | acc: 81.25%,  total acc: 87.01%   [EVAL] batch:   64 | acc: 93.75%,  total acc: 87.12%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 87.31%   [EVAL] batch:   66 | acc: 87.50%,  total acc: 87.31%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 87.41%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 87.32%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 87.32%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 87.41%   [EVAL] batch:   72 | acc: 81.25%,  total acc: 87.33%   [EVAL] batch:   73 | acc: 81.25%,  total acc: 87.25%   [EVAL] batch:   74 | acc: 68.75%,  total acc: 87.00%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 86.76%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 86.69%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 86.54%   [EVAL] batch:   78 | acc: 37.50%,  total acc: 85.92%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 85.78%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 85.49%   [EVAL] batch:   81 | acc: 75.00%,  total acc: 85.37%   [EVAL] batch:   82 | acc: 75.00%,  total acc: 85.24%   [EVAL] batch:   83 | acc: 56.25%,  total acc: 84.90%   [EVAL] batch:   84 | acc: 43.75%,  total acc: 84.41%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 84.30%   [EVAL] batch:   86 | acc: 62.50%,  total acc: 84.05%   [EVAL] batch:   87 | acc: 75.00%,  total acc: 83.95%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 84.13%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 84.31%   [EVAL] batch:   90 | acc: 100.00%,  total acc: 84.48%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 84.65%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 84.81%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 84.91%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 84.61%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 84.44%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 84.28%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 84.06%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 83.96%   [EVAL] batch:   99 | acc: 50.00%,  total acc: 83.62%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 83.79%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 83.95%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 84.10%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 84.25%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 84.40%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 84.43%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 84.72%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 84.86%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 85.14%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 85.27%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 85.34%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 85.31%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 85.43%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 85.56%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 85.63%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 85.70%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 85.77%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 85.89%   [EVAL] batch:  120 | acc: 75.00%,  total acc: 85.80%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 85.86%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 85.99%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 86.05%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 85.52%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 85.19%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 84.81%   [EVAL] batch:  128 | acc: 31.25%,  total acc: 84.40%   [EVAL] batch:  129 | acc: 50.00%,  total acc: 84.13%   [EVAL] batch:  130 | acc: 31.25%,  total acc: 83.73%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 83.76%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 83.74%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 83.82%   [EVAL] batch:  134 | acc: 100.00%,  total acc: 83.94%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 83.96%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 83.94%   [EVAL] batch:  137 | acc: 93.75%,  total acc: 84.01%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 84.08%   [EVAL] batch:  139 | acc: 100.00%,  total acc: 84.20%   [EVAL] batch:  140 | acc: 100.00%,  total acc: 84.31%   [EVAL] batch:  141 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:  142 | acc: 87.50%,  total acc: 84.40%   [EVAL] batch:  143 | acc: 68.75%,  total acc: 84.29%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 84.09%   [EVAL] batch:  145 | acc: 43.75%,  total acc: 83.82%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 83.76%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 83.66%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 83.43%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 83.33%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 82.86%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 82.40%   [EVAL] batch:  152 | acc: 6.25%,  total acc: 81.90%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 81.53%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 81.05%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 80.53%   [EVAL] batch:  156 | acc: 56.25%,  total acc: 80.37%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 80.46%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 80.54%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 80.59%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 80.63%   [EVAL] batch:  161 | acc: 81.25%,  total acc: 80.63%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 80.71%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 80.72%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 80.91%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 80.88%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 80.99%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 81.07%   [EVAL] batch:  169 | acc: 87.50%,  total acc: 81.10%   [EVAL] batch:  170 | acc: 81.25%,  total acc: 81.10%   [EVAL] batch:  171 | acc: 75.00%,  total acc: 81.07%   [EVAL] batch:  172 | acc: 68.75%,  total acc: 81.00%   [EVAL] batch:  173 | acc: 81.25%,  total acc: 81.00%   [EVAL] batch:  174 | acc: 87.50%,  total acc: 81.04%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 80.68%   [EVAL] batch:  176 | acc: 43.75%,  total acc: 80.47%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 80.27%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 80.10%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 79.97%   [EVAL] batch:  180 | acc: 62.50%,  total acc: 79.87%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 79.64%   [EVAL] batch:  182 | acc: 56.25%,  total acc: 79.51%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 79.38%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 79.29%   [EVAL] batch:  185 | acc: 56.25%,  total acc: 79.17%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 79.18%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 78.89%   
cur_acc:  ['0.9494', '0.8323', '0.6468']
his_acc:  ['0.9494', '0.8805', '0.7889']
Clustering into  19  clusters
Clusters:  [ 1 11 14 16  7  7 10  0  5 17 16  1  1  5  9 12 18  2  0  5  7  3  1  8
 15 13  0  5  1  5 11  4  6  3  0 15  5 12  2  1]
Losses:  10.020392417907715 3.106994152069092 0.8151865601539612
CurrentTrain: epoch  0, batch     0 | loss: 10.0203924Losses:  10.752762794494629 4.366833686828613 0.8351516723632812
CurrentTrain: epoch  0, batch     1 | loss: 10.7527628Losses:  10.12926197052002 3.138904094696045 0.7707928419113159
CurrentTrain: epoch  0, batch     2 | loss: 10.1292620Losses:  8.614378929138184 -0.0 0.14333465695381165
CurrentTrain: epoch  0, batch     3 | loss: 8.6143789Losses:  9.918243408203125 3.8187403678894043 0.7182990312576294
CurrentTrain: epoch  1, batch     0 | loss: 9.9182434Losses:  11.096152305603027 4.449997901916504 0.8464640378952026
CurrentTrain: epoch  1, batch     1 | loss: 11.0961523Losses:  9.653016090393066 4.324454307556152 0.7204185128211975
CurrentTrain: epoch  1, batch     2 | loss: 9.6530161Losses:  4.996485710144043 -0.0 0.09341482073068619
CurrentTrain: epoch  1, batch     3 | loss: 4.9964857Losses:  9.460326194763184 4.628707408905029 0.7115182876586914
CurrentTrain: epoch  2, batch     0 | loss: 9.4603262Losses:  7.695587158203125 2.2019035816192627 0.8434915542602539
CurrentTrain: epoch  2, batch     1 | loss: 7.6955872Losses:  9.704153060913086 4.080517768859863 0.672174334526062
CurrentTrain: epoch  2, batch     2 | loss: 9.7041531Losses:  5.493793487548828 -0.0 0.13313379883766174
CurrentTrain: epoch  2, batch     3 | loss: 5.4937935Losses:  7.826889514923096 3.1961584091186523 0.7711041569709778
CurrentTrain: epoch  3, batch     0 | loss: 7.8268895Losses:  10.991290092468262 4.8549981117248535 0.6990461945533752
CurrentTrain: epoch  3, batch     1 | loss: 10.9912901Losses:  6.751562118530273 3.2845253944396973 0.6853234767913818
CurrentTrain: epoch  3, batch     2 | loss: 6.7515621Losses:  2.5467164516448975 -0.0 0.12272809445858002
CurrentTrain: epoch  3, batch     3 | loss: 2.5467165Losses:  6.589026927947998 2.3412561416625977 0.7624643445014954
CurrentTrain: epoch  4, batch     0 | loss: 6.5890269Losses:  7.7604522705078125 3.5144920349121094 0.7434847354888916
CurrentTrain: epoch  4, batch     1 | loss: 7.7604523Losses:  9.150886535644531 4.274834632873535 0.7632303237915039
CurrentTrain: epoch  4, batch     2 | loss: 9.1508865Losses:  1.9001479148864746 -0.0 0.1527220755815506
CurrentTrain: epoch  4, batch     3 | loss: 1.9001479Losses:  7.059715270996094 3.4369564056396484 0.7843191623687744
CurrentTrain: epoch  5, batch     0 | loss: 7.0597153Losses:  7.442715167999268 3.1092803478240967 0.7584108710289001
CurrentTrain: epoch  5, batch     1 | loss: 7.4427152Losses:  7.545048236846924 2.799121618270874 0.7665323615074158
CurrentTrain: epoch  5, batch     2 | loss: 7.5450482Losses:  2.1635704040527344 -0.0 0.14288127422332764
CurrentTrain: epoch  5, batch     3 | loss: 2.1635704Losses:  7.614643096923828 3.4173364639282227 0.7631256580352783
CurrentTrain: epoch  6, batch     0 | loss: 7.6146431Losses:  6.875396251678467 4.007230758666992 0.4956773519515991
CurrentTrain: epoch  6, batch     1 | loss: 6.8753963Losses:  6.6701579093933105 2.2529873847961426 0.815231442451477
CurrentTrain: epoch  6, batch     2 | loss: 6.6701579Losses:  2.2038168907165527 -0.0 0.11401502788066864
CurrentTrain: epoch  6, batch     3 | loss: 2.2038169Losses:  6.663111686706543 2.9399497509002686 0.6970313787460327
CurrentTrain: epoch  7, batch     0 | loss: 6.6631117Losses:  7.057816028594971 3.4398233890533447 0.673833966255188
CurrentTrain: epoch  7, batch     1 | loss: 7.0578160Losses:  6.941012382507324 3.5393152236938477 0.6851006746292114
CurrentTrain: epoch  7, batch     2 | loss: 6.9410124Losses:  3.5070059299468994 -0.0 0.15692362189292908
CurrentTrain: epoch  7, batch     3 | loss: 3.5070059Losses:  5.8382568359375 3.3413796424865723 0.601352870464325
CurrentTrain: epoch  8, batch     0 | loss: 5.8382568Losses:  8.090453147888184 3.824687957763672 0.6742520332336426
CurrentTrain: epoch  8, batch     1 | loss: 8.0904531Losses:  7.304471969604492 3.6651041507720947 0.6917322874069214
CurrentTrain: epoch  8, batch     2 | loss: 7.3044720Losses:  1.8712588548660278 -0.0 0.1134738028049469
CurrentTrain: epoch  8, batch     3 | loss: 1.8712589Losses:  7.174041271209717 4.0657758712768555 0.6508952379226685
CurrentTrain: epoch  9, batch     0 | loss: 7.1740413Losses:  7.142606735229492 3.448587417602539 0.7302752733230591
CurrentTrain: epoch  9, batch     1 | loss: 7.1426067Losses:  5.57923698425293 2.311580181121826 0.7418707013130188
CurrentTrain: epoch  9, batch     2 | loss: 5.5792370Losses:  2.3282828330993652 -0.0 0.13152702152729034
CurrentTrain: epoch  9, batch     3 | loss: 2.3282828
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the cantons of créteil are administrative divisions of the val - de - marne department , île - de - france region , northern france .
Head Entity: île - de - france
Tail Entity: val - de - marne
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The provinces of Canada are the primary administrative divisions of the country, with Ontario being one of the most populous provinces.  
Head Entity: Canada  
Tail Entity: Ontario  

Relation: contains administrative territorial entity  
Context: The states of the United States are divided into various counties, with Los Angeles County being one of the largest in California.  
Head Entity: California  
Tail Entity: Los Angeles County  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: enzo is an italian given name derivative of heinz , a form of henry .
Head Entity: henry
Tail Entity: heinz
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are often said to be the same as each other due to their close genetic relationship.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  

Relation: said to be the same as  
Context: Many historians argue that the ancient city of Byzantium is said to be the same as modern-day Istanbul, although the transition involved significant changes over time.  
Head Entity: Byzantium  
Tail Entity: Istanbul  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: the filming was primarily held at pollachi . it fetched pawan kalyan the filmfare award for best actor – telugu award at 59th filmfare awards south .
Head Entity: filmfare award for best actor – telugu
Tail Entity: pawan kalyan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: in the thrilling finale of the championship, the underdog team triumphed, securing the title and making history as they celebrated their victory at the national sports arena. This remarkable achievement earned them the prestigious trophy for the best team of the year.  
Head Entity: trophy for the best team of the year  
Tail Entity: underdog team  

Relation: winner  
Context: during the annual music awards, the talented singer captivated the audience with her powerful performance, ultimately leading her to win the coveted title of best new artist. The event was a significant milestone in her burgeoning career.  
Head Entity: title of best new artist  
Tail Entity: talented singer  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the railroad car they were deported in was attached to the end of the last train out of drancy which also carried drancy commandant ss hauptsturmführer alois brunner and other german military personnel .
Head Entity: alois brunner
Tail Entity: hauptsturmführer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: General John Smith was awarded the title of Major General for his exceptional leadership during the conflict, which significantly contributed to the victory of his battalion.  
Head Entity: John Smith  
Tail Entity: Major General  

Relation: military rank  
Context: During the ceremony, Colonel Jane Doe was recognized for her outstanding service and was promoted to the rank of Brigadier General, a position she had long aspired to achieve.  
Head Entity: Jane Doe  
Tail Entity: Brigadier General  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham, set to hit the shelves on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular science magazine, Scientific American, published an article discussing the latest advancements in renewable energy technologies.  
Head Entity: Scientific American  
Tail Entity: renewable energy technologies  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " choose you " and " homesick " were released as the album 's second and third singles , respectively , and each attained moderate chart success .
Head Entity: choose you
Tail Entity: homesick
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter delves into the backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: The opening act of the concert was a local band, followed by a well-known pop artist who energized the crowd.  
Head Entity: local band  
Tail Entity: well-known pop artist  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: he was also associated with robert wilkinson in producing " londina illustrata " , an illustrated account of ancient buildings in london and westminster in two volumes ( 1819–25 ) .
Head Entity: robert wilkinson
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in the bustling city of San Francisco, where it has been operating since its inception in 2010.  
Head Entity: the company  
Tail Entity: San Francisco  

Relation: work location  
Context: During her time at the university, she conducted groundbreaking research in the field of neuroscience, primarily at the campus located in Cambridge.  
Head Entity: she  
Tail Entity: Cambridge  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in cancer treatment.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working at tech innovations for over five years.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: ada adini ( 1855 – february 1924 ) was an american operatic soprano who had an active international career from 1876 up into the first decade of the 20th century .
Head Entity: ada adini
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12, 1935 – september 6, 2007 ) was an italian operatic tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27, 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Losses:  5.0763773918151855 0.5868259072303772 0.9103164076805115
MemoryTrain:  epoch  0, batch     0 | loss: 5.0763774Losses:  5.11002779006958 0.5897116661071777 0.7233462333679199
MemoryTrain:  epoch  0, batch     1 | loss: 5.1100278Losses:  5.322507381439209 0.5670899152755737 0.8125381469726562
MemoryTrain:  epoch  0, batch     2 | loss: 5.3225074Losses:  5.311610221862793 -0.0 0.7679852247238159
MemoryTrain:  epoch  0, batch     3 | loss: 5.3116102Losses:  3.5561444759368896 0.2527926564216614 0.649078369140625
MemoryTrain:  epoch  0, batch     4 | loss: 3.5561445Losses:  5.159358978271484 0.8814668655395508 0.8945662975311279
MemoryTrain:  epoch  0, batch     5 | loss: 5.1593590Losses:  5.132169723510742 0.4760624170303345 0.7731193900108337
MemoryTrain:  epoch  0, batch     6 | loss: 5.1321697Losses:  3.044513702392578 -0.0 0.5968431234359741
MemoryTrain:  epoch  0, batch     7 | loss: 3.0445137Losses:  3.6984658241271973 -0.0 0.9321703910827637
MemoryTrain:  epoch  1, batch     0 | loss: 3.6984658Losses:  5.339609146118164 0.5568261742591858 0.7553942203521729
MemoryTrain:  epoch  1, batch     1 | loss: 5.3396091Losses:  4.435558795928955 0.8389981985092163 0.8228862881660461
MemoryTrain:  epoch  1, batch     2 | loss: 4.4355588Losses:  4.122031211853027 0.47531288862228394 0.7343372106552124
MemoryTrain:  epoch  1, batch     3 | loss: 4.1220312Losses:  4.07519006729126 0.8135603666305542 0.8062090277671814
MemoryTrain:  epoch  1, batch     4 | loss: 4.0751901Losses:  5.632604122161865 0.6111844778060913 0.9499722719192505
MemoryTrain:  epoch  1, batch     5 | loss: 5.6326041Losses:  4.305102348327637 0.2265566885471344 0.8031740188598633
MemoryTrain:  epoch  1, batch     6 | loss: 4.3051023Losses:  3.557791233062744 -0.0 0.6427333354949951
MemoryTrain:  epoch  1, batch     7 | loss: 3.5577912Losses:  4.363217830657959 0.7770858407020569 0.8317632675170898
MemoryTrain:  epoch  2, batch     0 | loss: 4.3632178Losses:  3.8945772647857666 0.939944326877594 0.7336810231208801
MemoryTrain:  epoch  2, batch     1 | loss: 3.8945773Losses:  3.477869987487793 0.27585697174072266 0.8609799742698669
MemoryTrain:  epoch  2, batch     2 | loss: 3.4778700Losses:  4.56803035736084 0.48055142164230347 0.8075556755065918
MemoryTrain:  epoch  2, batch     3 | loss: 4.5680304Losses:  3.7969396114349365 0.25018075108528137 0.8649452328681946
MemoryTrain:  epoch  2, batch     4 | loss: 3.7969396Losses:  3.906381845474243 0.5931020975112915 0.6943843364715576
MemoryTrain:  epoch  2, batch     5 | loss: 3.9063818Losses:  3.954692840576172 0.7109736204147339 0.7484226226806641
MemoryTrain:  epoch  2, batch     6 | loss: 3.9546928Losses:  2.603896141052246 -0.0 0.588737964630127
MemoryTrain:  epoch  2, batch     7 | loss: 2.6038961Losses:  3.4574666023254395 0.4827433228492737 0.8777832984924316
MemoryTrain:  epoch  3, batch     0 | loss: 3.4574666Losses:  3.291520595550537 0.2974194288253784 0.881718635559082
MemoryTrain:  epoch  3, batch     1 | loss: 3.2915206Losses:  3.076361894607544 -0.0 0.9299229979515076
MemoryTrain:  epoch  3, batch     2 | loss: 3.0763619Losses:  3.8256449699401855 0.4351491928100586 0.7745257616043091
MemoryTrain:  epoch  3, batch     3 | loss: 3.8256450Losses:  3.8463313579559326 0.5481860041618347 0.7763770222663879
MemoryTrain:  epoch  3, batch     4 | loss: 3.8463314Losses:  3.250035047531128 0.7424482107162476 0.7412522435188293
MemoryTrain:  epoch  3, batch     5 | loss: 3.2500350Losses:  3.8747477531433105 0.3140922486782074 0.644027829170227
MemoryTrain:  epoch  3, batch     6 | loss: 3.8747478Losses:  2.897937536239624 -0.0 0.6458198428153992
MemoryTrain:  epoch  3, batch     7 | loss: 2.8979375Losses:  3.1889760494232178 0.24280023574829102 0.8281828761100769
MemoryTrain:  epoch  4, batch     0 | loss: 3.1889760Losses:  3.3812451362609863 0.5323587656021118 0.7493105530738831
MemoryTrain:  epoch  4, batch     1 | loss: 3.3812451Losses:  3.3927063941955566 0.234478160738945 0.8867921829223633
MemoryTrain:  epoch  4, batch     2 | loss: 3.3927064Losses:  3.3898203372955322 0.5563002824783325 0.8628365397453308
MemoryTrain:  epoch  4, batch     3 | loss: 3.3898203Losses:  3.7993645668029785 0.5135705471038818 0.6679531335830688
MemoryTrain:  epoch  4, batch     4 | loss: 3.7993646Losses:  3.6396255493164062 0.5059605836868286 0.932064414024353
MemoryTrain:  epoch  4, batch     5 | loss: 3.6396255Losses:  3.7344658374786377 0.6812352538108826 0.7635710835456848
MemoryTrain:  epoch  4, batch     6 | loss: 3.7344658Losses:  2.9442298412323 0.3378531336784363 0.48828908801078796
MemoryTrain:  epoch  4, batch     7 | loss: 2.9442298Losses:  3.345546245574951 0.8282641172409058 0.7093162536621094
MemoryTrain:  epoch  5, batch     0 | loss: 3.3455462Losses:  3.5423827171325684 0.2606329321861267 0.8667740821838379
MemoryTrain:  epoch  5, batch     1 | loss: 3.5423827Losses:  3.39475679397583 0.6247296333312988 0.79961758852005
MemoryTrain:  epoch  5, batch     2 | loss: 3.3947568Losses:  3.724271535873413 1.0348591804504395 0.7873031497001648
MemoryTrain:  epoch  5, batch     3 | loss: 3.7242715Losses:  3.0198233127593994 0.7271101474761963 0.8645791411399841
MemoryTrain:  epoch  5, batch     4 | loss: 3.0198233Losses:  3.4982824325561523 0.2670223116874695 0.7281148433685303
MemoryTrain:  epoch  5, batch     5 | loss: 3.4982824Losses:  2.8477087020874023 0.5064328908920288 0.7691582441329956
MemoryTrain:  epoch  5, batch     6 | loss: 2.8477087Losses:  3.2272286415100098 0.6725307703018188 0.519279956817627
MemoryTrain:  epoch  5, batch     7 | loss: 3.2272286Losses:  2.559215784072876 0.22021803259849548 0.7936405539512634
MemoryTrain:  epoch  6, batch     0 | loss: 2.5592158Losses:  3.2038519382476807 0.967850923538208 0.5633348822593689
MemoryTrain:  epoch  6, batch     1 | loss: 3.2038519Losses:  2.8715991973876953 -0.0 0.934859037399292
MemoryTrain:  epoch  6, batch     2 | loss: 2.8715992Losses:  2.632111072540283 0.24684403836727142 0.8634883165359497
MemoryTrain:  epoch  6, batch     3 | loss: 2.6321111Losses:  2.724433422088623 -0.0 0.7957431674003601
MemoryTrain:  epoch  6, batch     4 | loss: 2.7244334Losses:  2.6692328453063965 0.263663113117218 0.9376580119132996
MemoryTrain:  epoch  6, batch     5 | loss: 2.6692328Losses:  3.5048346519470215 0.27311351895332336 0.7936179041862488
MemoryTrain:  epoch  6, batch     6 | loss: 3.5048347Losses:  2.4456748962402344 -0.0 0.6119416952133179
MemoryTrain:  epoch  6, batch     7 | loss: 2.4456749Losses:  3.695749044418335 0.8563735485076904 0.7598270773887634
MemoryTrain:  epoch  7, batch     0 | loss: 3.6957490Losses:  3.426694631576538 0.6515357494354248 0.7953745722770691
MemoryTrain:  epoch  7, batch     1 | loss: 3.4266946Losses:  2.4773716926574707 0.2677386403083801 0.7008391618728638
MemoryTrain:  epoch  7, batch     2 | loss: 2.4773717Losses:  3.0043249130249023 0.7289429903030396 0.8430699706077576
MemoryTrain:  epoch  7, batch     3 | loss: 3.0043249Losses:  3.3538105487823486 0.7433222532272339 0.9671867489814758
MemoryTrain:  epoch  7, batch     4 | loss: 3.3538105Losses:  3.143920660018921 0.5417977571487427 0.7586161494255066
MemoryTrain:  epoch  7, batch     5 | loss: 3.1439207Losses:  3.379956007003784 0.9817578196525574 0.716454267501831
MemoryTrain:  epoch  7, batch     6 | loss: 3.3799560Losses:  1.839944839477539 -0.0 0.516082763671875
MemoryTrain:  epoch  7, batch     7 | loss: 1.8399448Losses:  2.903695583343506 0.47638335824012756 0.7870279550552368
MemoryTrain:  epoch  8, batch     0 | loss: 2.9036956Losses:  3.5341956615448 0.8198302984237671 0.9373065829277039
MemoryTrain:  epoch  8, batch     1 | loss: 3.5341957Losses:  3.247849702835083 0.5302019119262695 0.7558086514472961
MemoryTrain:  epoch  8, batch     2 | loss: 3.2478497Losses:  2.6364080905914307 0.7102063894271851 0.6457386612892151
MemoryTrain:  epoch  8, batch     3 | loss: 2.6364081Losses:  2.945009708404541 0.7307187914848328 0.7919658422470093
MemoryTrain:  epoch  8, batch     4 | loss: 2.9450097Losses:  3.1588385105133057 0.5483468770980835 0.9245688915252686
MemoryTrain:  epoch  8, batch     5 | loss: 3.1588385Losses:  2.187734603881836 -0.0 0.8044582605361938
MemoryTrain:  epoch  8, batch     6 | loss: 2.1877346Losses:  2.7751307487487793 0.2835896611213684 0.6020774245262146
MemoryTrain:  epoch  8, batch     7 | loss: 2.7751307Losses:  2.5598559379577637 0.25398916006088257 0.7816392183303833
MemoryTrain:  epoch  9, batch     0 | loss: 2.5598559Losses:  2.7529795169830322 0.49084052443504333 0.798291027545929
MemoryTrain:  epoch  9, batch     1 | loss: 2.7529795Losses:  2.444697141647339 0.26102414727211 0.7975001931190491
MemoryTrain:  epoch  9, batch     2 | loss: 2.4446971Losses:  2.9901249408721924 0.4755663275718689 0.9699406027793884
MemoryTrain:  epoch  9, batch     3 | loss: 2.9901249Losses:  3.4371371269226074 0.7114236354827881 0.7653679251670837
MemoryTrain:  epoch  9, batch     4 | loss: 3.4371371Losses:  2.4780020713806152 0.2416967749595642 0.8085604906082153
MemoryTrain:  epoch  9, batch     5 | loss: 2.4780021Losses:  2.670891523361206 0.28102681040763855 0.835951030254364
MemoryTrain:  epoch  9, batch     6 | loss: 2.6708915Losses:  1.935088872909546 -0.0 0.6660448908805847
MemoryTrain:  epoch  9, batch     7 | loss: 1.9350889
[EVAL] batch:    0 | acc: 18.75%,  total acc: 18.75%   [EVAL] batch:    1 | acc: 43.75%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 32.50%   [EVAL] batch:    5 | acc: 37.50%,  total acc: 33.33%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 36.61%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 42.19%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 46.53%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 49.38%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 52.08%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 55.29%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 58.04%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 60.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 64.34%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 68.09%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 69.38%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 70.54%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 72.83%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 73.70%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 74.75%   [EVAL] batch:   25 | acc: 62.50%,  total acc: 74.28%   [EVAL] batch:   26 | acc: 56.25%,  total acc: 73.61%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 74.11%   [EVAL] batch:   28 | acc: 56.25%,  total acc: 73.49%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 73.33%   [EVAL] batch:   30 | acc: 68.75%,  total acc: 73.19%   [EVAL] batch:   31 | acc: 56.25%,  total acc: 72.66%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 72.16%   [EVAL] batch:   33 | acc: 12.50%,  total acc: 70.40%   [EVAL] batch:   34 | acc: 25.00%,  total acc: 69.11%   [EVAL] batch:   35 | acc: 12.50%,  total acc: 67.53%   [EVAL] batch:   36 | acc: 6.25%,  total acc: 65.88%   [EVAL] batch:   37 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 65.71%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 65.78%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 65.70%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 65.92%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 65.84%   [EVAL] batch:   43 | acc: 62.50%,  total acc: 65.77%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 66.17%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 66.36%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 66.54%   [EVAL] batch:   48 | acc: 75.00%,  total acc: 66.71%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 67.12%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 67.77%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 68.99%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 70.11%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 70.65%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 71.16%   [EVAL] batch:   57 | acc: 100.00%,  total acc: 71.66%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 72.14%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 72.60%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 73.05%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 73.49%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.12%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 84.38%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 83.75%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 85.42%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 78.91%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 76.88%   [EVAL] batch:   10 | acc: 50.00%,  total acc: 74.43%   [EVAL] batch:   11 | acc: 50.00%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 81.55%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 82.03%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 82.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.93%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.56%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 84.15%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.48%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 85.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.94%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 86.36%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.76%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.96%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 87.33%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.67%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 87.99%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 88.30%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 88.44%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 88.72%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 88.84%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 88.81%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 88.78%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 88.47%   [EVAL] batch:   45 | acc: 50.00%,  total acc: 87.64%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 87.10%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 86.85%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 86.10%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 85.62%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 84.19%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 82.69%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 81.25%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 80.09%   [EVAL] batch:   54 | acc: 6.25%,  total acc: 78.75%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 77.68%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 77.52%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 77.69%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 78.07%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 78.59%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 78.83%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 79.07%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 79.20%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 79.33%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 79.64%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 79.66%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 79.87%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 80.07%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   70 | acc: 87.50%,  total acc: 80.11%   [EVAL] batch:   71 | acc: 93.75%,  total acc: 80.30%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 80.39%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 80.32%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 80.25%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 80.18%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 80.11%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 80.13%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 79.67%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 79.48%   [EVAL] batch:   81 | acc: 62.50%,  total acc: 79.27%   [EVAL] batch:   82 | acc: 31.25%,  total acc: 78.69%   [EVAL] batch:   83 | acc: 25.00%,  total acc: 78.05%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 77.35%   [EVAL] batch:   85 | acc: 50.00%,  total acc: 77.03%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 76.36%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 76.21%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 76.47%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.74%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 76.92%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 77.17%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 77.42%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 77.59%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 77.37%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 77.28%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 77.06%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 76.91%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 76.89%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 76.56%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 76.79%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 77.02%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 77.25%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 77.46%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 77.68%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 77.77%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 77.98%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 78.18%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 78.58%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 78.77%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 78.96%   [EVAL] batch:  112 | acc: 93.75%,  total acc: 79.09%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 79.11%   [EVAL] batch:  114 | acc: 93.75%,  total acc: 79.24%   [EVAL] batch:  115 | acc: 100.00%,  total acc: 79.42%   [EVAL] batch:  116 | acc: 93.75%,  total acc: 79.54%   [EVAL] batch:  117 | acc: 100.00%,  total acc: 79.71%   [EVAL] batch:  118 | acc: 93.75%,  total acc: 79.83%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 80.00%   [EVAL] batch:  120 | acc: 81.25%,  total acc: 80.01%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 80.12%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 80.28%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 80.34%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 80.45%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 79.96%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 79.68%   [EVAL] batch:  127 | acc: 43.75%,  total acc: 79.39%   [EVAL] batch:  128 | acc: 43.75%,  total acc: 79.12%   [EVAL] batch:  129 | acc: 56.25%,  total acc: 78.94%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 78.63%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 78.60%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 78.62%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 78.73%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 78.84%   [EVAL] batch:  135 | acc: 81.25%,  total acc: 78.86%   [EVAL] batch:  136 | acc: 62.50%,  total acc: 78.74%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 78.80%   [EVAL] batch:  138 | acc: 93.75%,  total acc: 78.91%   [EVAL] batch:  139 | acc: 100.00%,  total acc: 79.06%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:  141 | acc: 93.75%,  total acc: 79.27%   [EVAL] batch:  142 | acc: 93.75%,  total acc: 79.37%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 79.34%   [EVAL] batch:  144 | acc: 62.50%,  total acc: 79.22%   [EVAL] batch:  145 | acc: 50.00%,  total acc: 79.02%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 79.04%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 78.97%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 78.82%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 78.75%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 78.23%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 77.80%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 77.29%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 76.79%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 76.33%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 75.84%   [EVAL] batch:  156 | acc: 50.00%,  total acc: 75.68%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 75.79%   [EVAL] batch:  158 | acc: 100.00%,  total acc: 75.94%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 76.05%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 76.16%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 76.16%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 76.27%   [EVAL] batch:  163 | acc: 81.25%,  total acc: 76.30%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 76.44%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 76.54%   [EVAL] batch:  166 | acc: 75.00%,  total acc: 76.53%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 76.64%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 76.70%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 76.54%   [EVAL] batch:  170 | acc: 25.00%,  total acc: 76.24%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 76.05%   [EVAL] batch:  172 | acc: 56.25%,  total acc: 75.94%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 75.90%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 75.75%   [EVAL] batch:  175 | acc: 18.75%,  total acc: 75.43%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 75.28%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 75.07%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 74.93%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 74.83%   [EVAL] batch:  180 | acc: 50.00%,  total acc: 74.69%   [EVAL] batch:  181 | acc: 37.50%,  total acc: 74.48%   [EVAL] batch:  182 | acc: 50.00%,  total acc: 74.35%   [EVAL] batch:  183 | acc: 56.25%,  total acc: 74.25%   [EVAL] batch:  184 | acc: 62.50%,  total acc: 74.19%   [EVAL] batch:  185 | acc: 50.00%,  total acc: 74.06%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 74.03%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 73.77%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 73.54%   [EVAL] batch:  189 | acc: 43.75%,  total acc: 73.39%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 73.20%   [EVAL] batch:  191 | acc: 31.25%,  total acc: 72.98%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 72.80%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 72.62%   [EVAL] batch:  194 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:  195 | acc: 75.00%,  total acc: 72.67%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 72.72%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 72.76%   [EVAL] batch:  198 | acc: 37.50%,  total acc: 72.58%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 72.72%   [EVAL] batch:  200 | acc: 93.75%,  total acc: 72.82%   [EVAL] batch:  201 | acc: 87.50%,  total acc: 72.90%   [EVAL] batch:  202 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  203 | acc: 93.75%,  total acc: 73.13%   [EVAL] batch:  204 | acc: 93.75%,  total acc: 73.23%   [EVAL] batch:  205 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 73.46%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 73.56%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 73.68%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 73.81%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 73.87%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 74.00%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 74.03%   [EVAL] batch:  213 | acc: 56.25%,  total acc: 73.95%   [EVAL] batch:  214 | acc: 75.00%,  total acc: 73.95%   [EVAL] batch:  215 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:  216 | acc: 56.25%,  total acc: 73.88%   [EVAL] batch:  217 | acc: 75.00%,  total acc: 73.88%   [EVAL] batch:  218 | acc: 62.50%,  total acc: 73.83%   [EVAL] batch:  219 | acc: 56.25%,  total acc: 73.75%   [EVAL] batch:  220 | acc: 25.00%,  total acc: 73.53%   [EVAL] batch:  221 | acc: 18.75%,  total acc: 73.28%   [EVAL] batch:  222 | acc: 25.00%,  total acc: 73.07%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 72.77%   [EVAL] batch:  224 | acc: 31.25%,  total acc: 72.58%   [EVAL] batch:  225 | acc: 56.25%,  total acc: 72.51%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 72.49%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 72.48%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 72.52%   [EVAL] batch:  229 | acc: 62.50%,  total acc: 72.47%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 72.40%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 72.41%   [EVAL] batch:  232 | acc: 68.75%,  total acc: 72.40%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 72.44%   [EVAL] batch:  234 | acc: 75.00%,  total acc: 72.45%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 72.43%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 72.49%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 72.58%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 72.70%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 72.81%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 72.93%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 73.04%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 73.15%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 73.37%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 73.58%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 73.90%   
cur_acc:  ['0.9494', '0.8323', '0.6468', '0.7312']
his_acc:  ['0.9494', '0.8805', '0.7889', '0.7390']
Clustering into  24  clusters
Clusters:  [ 0  1 18  4  2  2 15 14  5 22  4  0  0  5 13 17 20  3 14  5  2  6  0 23
 11 16 21  5  0  5  1 19  8  7 14 11  5 17  3  0  1  0  9  6 10  6  8 12
  7 12]
Losses:  10.986462593078613 3.893843173980713 0.7728698253631592
CurrentTrain: epoch  0, batch     0 | loss: 10.9864626Losses:  12.690693855285645 5.198781967163086 0.6487127542495728
CurrentTrain: epoch  0, batch     1 | loss: 12.6906939Losses:  8.369146347045898 2.541461706161499 0.7423912286758423
CurrentTrain: epoch  0, batch     2 | loss: 8.3691463Losses:  5.472524166107178 -0.0 0.1372511237859726
CurrentTrain: epoch  0, batch     3 | loss: 5.4725242Losses:  8.648295402526855 3.699660539627075 0.6479323506355286
CurrentTrain: epoch  1, batch     0 | loss: 8.6482954Losses:  9.616242408752441 4.29157829284668 0.6262063980102539
CurrentTrain: epoch  1, batch     1 | loss: 9.6162424Losses:  10.235050201416016 3.61049747467041 0.6321673393249512
CurrentTrain: epoch  1, batch     2 | loss: 10.2350502Losses:  8.067057609558105 -0.0 0.0963563621044159
CurrentTrain: epoch  1, batch     3 | loss: 8.0670576Losses:  7.91677188873291 2.3519816398620605 0.6866719722747803
CurrentTrain: epoch  2, batch     0 | loss: 7.9167719Losses:  7.122325897216797 2.757812976837158 0.6904418468475342
CurrentTrain: epoch  2, batch     1 | loss: 7.1223259Losses:  9.42379093170166 3.9953081607818604 0.6488463878631592
CurrentTrain: epoch  2, batch     2 | loss: 9.4237909Losses:  5.391777515411377 -0.0 0.11051127314567566
CurrentTrain: epoch  2, batch     3 | loss: 5.3917775Losses:  6.758002758026123 2.126945734024048 0.670524001121521
CurrentTrain: epoch  3, batch     0 | loss: 6.7580028Losses:  7.942781925201416 3.198072910308838 0.5941556096076965
CurrentTrain: epoch  3, batch     1 | loss: 7.9427819Losses:  8.434242248535156 3.0743322372436523 0.599554717540741
CurrentTrain: epoch  3, batch     2 | loss: 8.4342422Losses:  2.0534675121307373 -0.0 0.12498488277196884
CurrentTrain: epoch  3, batch     3 | loss: 2.0534675Losses:  8.2964448928833 3.5875165462493896 0.6091018915176392
CurrentTrain: epoch  4, batch     0 | loss: 8.2964449Losses:  6.642548084259033 2.9024202823638916 0.6665455102920532
CurrentTrain: epoch  4, batch     1 | loss: 6.6425481Losses:  10.065838813781738 4.8254804611206055 0.5612562298774719
CurrentTrain: epoch  4, batch     2 | loss: 10.0658388Losses:  3.496612310409546 -0.0 0.17068319022655487
CurrentTrain: epoch  4, batch     3 | loss: 3.4966123Losses:  8.236156463623047 3.5268163681030273 0.6015728712081909
CurrentTrain: epoch  5, batch     0 | loss: 8.2361565Losses:  7.811317443847656 3.3662185668945312 0.6629951596260071
CurrentTrain: epoch  5, batch     1 | loss: 7.8113174Losses:  6.899177551269531 3.672553539276123 0.5771999359130859
CurrentTrain: epoch  5, batch     2 | loss: 6.8991776Losses:  2.334005355834961 -0.0 0.10498218983411789
CurrentTrain: epoch  5, batch     3 | loss: 2.3340054Losses:  7.614781379699707 3.8131182193756104 0.5079156160354614
CurrentTrain: epoch  6, batch     0 | loss: 7.6147814Losses:  7.41283655166626 3.6758604049682617 0.5366872549057007
CurrentTrain: epoch  6, batch     1 | loss: 7.4128366Losses:  7.239983558654785 3.731611728668213 0.5491940975189209
CurrentTrain: epoch  6, batch     2 | loss: 7.2399836Losses:  2.826941967010498 -0.0 0.09748167544603348
CurrentTrain: epoch  6, batch     3 | loss: 2.8269420Losses:  6.12591028213501 2.6512856483459473 0.5705248117446899
CurrentTrain: epoch  7, batch     0 | loss: 6.1259103Losses:  6.360468864440918 2.8268637657165527 0.5673836469650269
CurrentTrain: epoch  7, batch     1 | loss: 6.3604689Losses:  5.608325958251953 2.2588016986846924 0.6422623991966248
CurrentTrain: epoch  7, batch     2 | loss: 5.6083260Losses:  2.4715769290924072 -0.0 0.16502076387405396
CurrentTrain: epoch  7, batch     3 | loss: 2.4715769Losses:  5.91957950592041 2.6715493202209473 0.5725400447845459
CurrentTrain: epoch  8, batch     0 | loss: 5.9195795Losses:  6.177863121032715 2.546257495880127 0.6235637664794922
CurrentTrain: epoch  8, batch     1 | loss: 6.1778631Losses:  5.334959983825684 2.545032024383545 0.5503699779510498
CurrentTrain: epoch  8, batch     2 | loss: 5.3349600Losses:  1.8127615451812744 -0.0 0.14878273010253906
CurrentTrain: epoch  8, batch     3 | loss: 1.8127615Losses:  6.1297993659973145 3.0953919887542725 0.6147300601005554
CurrentTrain: epoch  9, batch     0 | loss: 6.1297994Losses:  5.2059736251831055 2.4327235221862793 0.630376935005188
CurrentTrain: epoch  9, batch     1 | loss: 5.2059736Losses:  4.897611141204834 2.150594711303711 0.5727303624153137
CurrentTrain: epoch  9, batch     2 | loss: 4.8976111Losses:  3.4001004695892334 -0.0 0.1015564501285553
CurrentTrain: epoch  9, batch     3 | loss: 3.4001005
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: george lawson sheldon ( may 31 , 1870 – april 4 , 1960 ) was 14th governor of nebraska with service from 1907 until 1909 .
Head Entity: governor
Tail Entity: nebraska
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The Environmental Protection Agency (EPA) is responsible for enforcing regulations related to environmental protection in the United States.  
Head Entity: Environmental Protection Agency  
Tail Entity: United States  

Relation: applies to jurisdiction  
Context: The California Consumer Privacy Act (CCPA) provides residents of California with specific rights regarding their personal information.  
Head Entity: California Consumer Privacy Act  
Tail Entity: California  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: " tenth avenue freeze - out " is the second song on bruce springsteen 's breakthrough album " born to run " , released in 1975 .
Head Entity: tenth avenue freeze - out
Tail Entity: bruce springsteen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: "Shape of You" is a popular song by Ed Sheeran that topped charts worldwide after its release in 2017.  
Head Entity: Shape of You  
Tail Entity: Ed Sheeran  

Relation: performer  
Context: The Beatles are widely regarded as one of the most influential bands in the history of music, with hits like "Hey Jude" and "Let It Be."  
Head Entity: The Beatles  
Tail Entity: Hey Jude  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the next full - frame digital slrs were the canon eos-1ds of late 2002 , followed by kodak 's dcs pro 14n in 2003 .
Head Entity: dcs pro 14n
Tail Entity: kodak
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The latest smartphone model, the Galaxy S21, was developed by Samsung and released in early 2021.  
Head Entity: Galaxy S21  
Tail Entity: Samsung  

Relation: manufacturer  
Context: The iconic Model T was produced by Ford Motor Company and revolutionized the automotive industry in the early 20th century.  
Head Entity: Model T  
Tail Entity: Ford Motor Company  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, were active participants in raising funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: " robowar " is a similar game that was released later on the macintosh , and is now available for both mac os and windows .
Head Entity: robowar
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: "The latest version of Adobe Photoshop is compatible with both Windows 10 and macOS Monterey."  
Head Entity: Adobe Photoshop  
Tail Entity: Windows 10  

Relation: operating system  
Context: "Minecraft runs smoothly on various platforms, including Linux and the latest version of Windows."  
Head Entity: Minecraft  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in july 2007 , griffin lost by a technical knockout to light heavyweight champion danny green of the world boxing association at challenge stadium in perth , australia .
Head Entity: danny green
Tail Entity: light heavyweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: during the 2020 olympics, the men's 100 meters final showcased athletes from various competition classes, with usain bolt representing the sprinting class.  
Head Entity: usain bolt  
Tail Entity: sprinting  

Relation: competition class  
Context: the formula one race in monaco is known for its high-speed competition class, where lewis hamilton competed fiercely against other top drivers.  
Head Entity: lewis hamilton  
Tail Entity: formula one
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: the qup gained 3 seats from its previous election result , and won a 10 % swing to it in primary votes , but failed to defeat the incumbent labor government led by ted theodore .
Head Entity: previous election
Tail Entity: ted theodore
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: After a closely contested race, the voters chose Sarah Johnson as their representative, marking a significant shift in the political landscape of the district.  
Head Entity: closely contested race  
Tail Entity: Sarah Johnson  

Relation: successful candidate  
Context: In the recent mayoral election, the community rallied behind Mark Thompson, who promised to bring new initiatives to the city.  
Head Entity: recent mayoral election  
Tail Entity: Mark Thompson  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family, both Aeliana and her brother Cedric were known for their adventurous spirits, often embarking on quests together, much to the delight of their parents.  
Head Entity: Aeliana  
Tail Entity: Cedric  

Relation: sibling  
Context: During the family reunion, it was heartwarming to see how much Emma and her sister Lily had grown, sharing stories of their childhood and the mischief they caused together under their parents' watchful eyes.  
Head Entity: Emma  
Tail Entity: Lily  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: dennis chalker is a retired navy seal , inventor and author who has written six books about the united states navy seals .
Head Entity: dennis chalker
Tail Entity: united states navy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: the general served in the air force for over twenty years before retiring and taking on a civilian role in defense consulting.  
Head Entity: the general  
Tail Entity: air force  

Relation: military branch  
Context: during the ceremony, the admiral was recognized for his service in the coast guard, where he led several important missions.  
Head Entity: the admiral  
Tail Entity: coast guard  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated film, the character of simba is the son of mufasa, the wise and noble king of the pride lands.  
Head Entity: mufasa  
Tail Entity: simba  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born before he married mileva maric.  
Head Entity: albert einstein  
Tail Entity: lieserl einstein  
Losses:  4.781035900115967 0.272905170917511 1.0245624780654907
MemoryTrain:  epoch  0, batch     0 | loss: 4.7810359Losses:  6.156280994415283 1.3655636310577393 0.7697007060050964
MemoryTrain:  epoch  0, batch     1 | loss: 6.1562810Losses:  4.590856552124023 0.28326350450515747 0.8836446404457092
MemoryTrain:  epoch  0, batch     2 | loss: 4.5908566Losses:  3.7253921031951904 0.5453693866729736 0.8241724371910095
MemoryTrain:  epoch  0, batch     3 | loss: 3.7253921Losses:  4.440868854522705 0.4672700762748718 0.9573325514793396
MemoryTrain:  epoch  0, batch     4 | loss: 4.4408689Losses:  6.437582015991211 0.958225429058075 0.9369659423828125
MemoryTrain:  epoch  0, batch     5 | loss: 6.4375820Losses:  4.717918872833252 0.6292612552642822 0.8260529637336731
MemoryTrain:  epoch  0, batch     6 | loss: 4.7179189Losses:  4.692804336547852 0.7862392067909241 0.8710290789604187
MemoryTrain:  epoch  0, batch     7 | loss: 4.6928043Losses:  3.6583523750305176 0.2689312696456909 0.7829780578613281
MemoryTrain:  epoch  0, batch     8 | loss: 3.6583524Losses:  3.507061719894409 -0.0 0.5184952616691589
MemoryTrain:  epoch  0, batch     9 | loss: 3.5070617Losses:  3.8099026679992676 0.262207567691803 0.8467082977294922
MemoryTrain:  epoch  1, batch     0 | loss: 3.8099027Losses:  5.769685745239258 1.4156323671340942 0.7932572364807129
MemoryTrain:  epoch  1, batch     1 | loss: 5.7696857Losses:  4.118459701538086 0.4396952986717224 0.8341042399406433
MemoryTrain:  epoch  1, batch     2 | loss: 4.1184597Losses:  3.1132659912109375 0.26494520902633667 0.7209086418151855
MemoryTrain:  epoch  1, batch     3 | loss: 3.1132660Losses:  4.47763729095459 0.8931167125701904 1.0576075315475464
MemoryTrain:  epoch  1, batch     4 | loss: 4.4776373Losses:  4.256649494171143 0.7826496362686157 0.9316765666007996
MemoryTrain:  epoch  1, batch     5 | loss: 4.2566495Losses:  4.237960338592529 0.5764910578727722 0.8672580718994141
MemoryTrain:  epoch  1, batch     6 | loss: 4.2379603Losses:  3.724634885787964 0.2864985764026642 0.9861233234405518
MemoryTrain:  epoch  1, batch     7 | loss: 3.7246349Losses:  3.6257030963897705 0.5663751363754272 0.8462626934051514
MemoryTrain:  epoch  1, batch     8 | loss: 3.6257031Losses:  2.9756529331207275 -0.0 0.5096260905265808
MemoryTrain:  epoch  1, batch     9 | loss: 2.9756529Losses:  3.0823440551757812 0.2332444041967392 0.8781735301017761
MemoryTrain:  epoch  2, batch     0 | loss: 3.0823441Losses:  3.9267423152923584 0.5053220987319946 0.8355405926704407
MemoryTrain:  epoch  2, batch     1 | loss: 3.9267423Losses:  3.143740653991699 0.2526635527610779 0.9145659804344177
MemoryTrain:  epoch  2, batch     2 | loss: 3.1437407Losses:  3.058617115020752 0.2521909475326538 1.0140970945358276
MemoryTrain:  epoch  2, batch     3 | loss: 3.0586171Losses:  3.6567938327789307 0.8575130701065063 0.7804977893829346
MemoryTrain:  epoch  2, batch     4 | loss: 3.6567938Losses:  3.322579860687256 0.32930877804756165 0.9136770963668823
MemoryTrain:  epoch  2, batch     5 | loss: 3.3225799Losses:  3.2429089546203613 0.4562249779701233 0.8165668249130249
MemoryTrain:  epoch  2, batch     6 | loss: 3.2429090Losses:  3.7114500999450684 0.237870991230011 0.8319355249404907
MemoryTrain:  epoch  2, batch     7 | loss: 3.7114501Losses:  3.2256412506103516 0.24956487119197845 0.7623685002326965
MemoryTrain:  epoch  2, batch     8 | loss: 3.2256413Losses:  1.7734200954437256 -0.0 0.40244314074516296
MemoryTrain:  epoch  2, batch     9 | loss: 1.7734201Losses:  3.4080018997192383 0.5447466373443604 0.8630620837211609
MemoryTrain:  epoch  3, batch     0 | loss: 3.4080019Losses:  3.638545513153076 0.5162116289138794 0.9561609029769897
MemoryTrain:  epoch  3, batch     1 | loss: 3.6385455Losses:  3.23968768119812 0.7916203737258911 0.7724549770355225
MemoryTrain:  epoch  3, batch     2 | loss: 3.2396877Losses:  2.9570674896240234 -0.0 0.8651117086410522
MemoryTrain:  epoch  3, batch     3 | loss: 2.9570675Losses:  3.730170249938965 0.6470823287963867 0.9631772041320801
MemoryTrain:  epoch  3, batch     4 | loss: 3.7301702Losses:  3.3247008323669434 0.4905465841293335 0.8241338729858398
MemoryTrain:  epoch  3, batch     5 | loss: 3.3247008Losses:  4.109417915344238 1.089725375175476 0.8087441325187683
MemoryTrain:  epoch  3, batch     6 | loss: 4.1094179Losses:  2.9597179889678955 0.31025993824005127 0.9076134562492371
MemoryTrain:  epoch  3, batch     7 | loss: 2.9597180Losses:  2.6446685791015625 0.2740568220615387 0.9805624485015869
MemoryTrain:  epoch  3, batch     8 | loss: 2.6446686Losses:  2.5230722427368164 -0.0 0.5178953409194946
MemoryTrain:  epoch  3, batch     9 | loss: 2.5230722Losses:  3.1177444458007812 0.5472220778465271 0.8266558051109314
MemoryTrain:  epoch  4, batch     0 | loss: 3.1177444Losses:  2.645083427429199 0.2436663806438446 0.9451897144317627
MemoryTrain:  epoch  4, batch     1 | loss: 2.6450834Losses:  3.274221181869507 0.791641116142273 0.8136586546897888
MemoryTrain:  epoch  4, batch     2 | loss: 3.2742212Losses:  2.8946664333343506 0.25503548979759216 0.9753900766372681
MemoryTrain:  epoch  4, batch     3 | loss: 2.8946664Losses:  2.7821903228759766 0.28494036197662354 1.0023905038833618
MemoryTrain:  epoch  4, batch     4 | loss: 2.7821903Losses:  2.93501353263855 0.49070537090301514 0.8825814127922058
MemoryTrain:  epoch  4, batch     5 | loss: 2.9350135Losses:  3.7007663249969482 0.8637053966522217 0.8821597099304199
MemoryTrain:  epoch  4, batch     6 | loss: 3.7007663Losses:  2.643522262573242 -0.0 0.9199957251548767
MemoryTrain:  epoch  4, batch     7 | loss: 2.6435223Losses:  3.552645683288574 0.6030494570732117 0.9690830707550049
MemoryTrain:  epoch  4, batch     8 | loss: 3.5526457Losses:  2.619158983230591 -0.0 0.5145115852355957
MemoryTrain:  epoch  4, batch     9 | loss: 2.6191590Losses:  2.990093469619751 0.5311862230300903 0.7436873316764832
MemoryTrain:  epoch  5, batch     0 | loss: 2.9900935Losses:  2.730534791946411 0.2503049075603485 0.8070876002311707
MemoryTrain:  epoch  5, batch     1 | loss: 2.7305348Losses:  3.235379457473755 0.5295087099075317 0.9195563197135925
MemoryTrain:  epoch  5, batch     2 | loss: 3.2353795Losses:  3.3469138145446777 0.26845017075538635 0.9636582732200623
MemoryTrain:  epoch  5, batch     3 | loss: 3.3469138Losses:  3.036135196685791 0.7349356412887573 0.9202111959457397
MemoryTrain:  epoch  5, batch     4 | loss: 3.0361352Losses:  2.8457822799682617 0.45520728826522827 0.9703603982925415
MemoryTrain:  epoch  5, batch     5 | loss: 2.8457823Losses:  3.688234806060791 1.2730536460876465 0.786492645740509
MemoryTrain:  epoch  5, batch     6 | loss: 3.6882348Losses:  2.6462666988372803 0.261890709400177 0.8764882683753967
MemoryTrain:  epoch  5, batch     7 | loss: 2.6462667Losses:  3.0733203887939453 0.5508986115455627 0.8162856101989746
MemoryTrain:  epoch  5, batch     8 | loss: 3.0733204Losses:  2.1520986557006836 -0.0 0.5036951899528503
MemoryTrain:  epoch  5, batch     9 | loss: 2.1520987Losses:  2.7767701148986816 0.3370979130268097 0.8947628736495972
MemoryTrain:  epoch  6, batch     0 | loss: 2.7767701Losses:  2.958528518676758 0.5306615233421326 0.8070670962333679
MemoryTrain:  epoch  6, batch     1 | loss: 2.9585285Losses:  2.7035093307495117 0.2529633045196533 0.9931886196136475
MemoryTrain:  epoch  6, batch     2 | loss: 2.7035093Losses:  2.8121538162231445 0.2794860005378723 0.9830284714698792
MemoryTrain:  epoch  6, batch     3 | loss: 2.8121538Losses:  2.9906320571899414 0.5913025736808777 0.8621581792831421
MemoryTrain:  epoch  6, batch     4 | loss: 2.9906321Losses:  2.757295608520508 0.22891920804977417 0.9847778081893921
MemoryTrain:  epoch  6, batch     5 | loss: 2.7572956Losses:  2.96964168548584 0.5189893841743469 0.9100607633590698
MemoryTrain:  epoch  6, batch     6 | loss: 2.9696417Losses:  2.5053601264953613 0.22581106424331665 0.8566588163375854
MemoryTrain:  epoch  6, batch     7 | loss: 2.5053601Losses:  4.124636650085449 1.7280193567276 0.7575365900993347
MemoryTrain:  epoch  6, batch     8 | loss: 4.1246367Losses:  1.7053645849227905 -0.0 0.4028911292552948
MemoryTrain:  epoch  6, batch     9 | loss: 1.7053646Losses:  2.444483757019043 0.2281647026538849 0.7225674390792847
MemoryTrain:  epoch  7, batch     0 | loss: 2.4444838Losses:  2.7517623901367188 0.5103192925453186 0.799807071685791
MemoryTrain:  epoch  7, batch     1 | loss: 2.7517624Losses:  2.747279405593872 0.6907379627227783 0.8310763835906982
MemoryTrain:  epoch  7, batch     2 | loss: 2.7472794Losses:  2.753070116043091 0.2808375954627991 0.9154052734375
MemoryTrain:  epoch  7, batch     3 | loss: 2.7530701Losses:  3.3324127197265625 1.1236658096313477 0.8894095420837402
MemoryTrain:  epoch  7, batch     4 | loss: 3.3324127Losses:  2.897670269012451 0.47435811161994934 0.9323269724845886
MemoryTrain:  epoch  7, batch     5 | loss: 2.8976703Losses:  2.7818613052368164 0.3040992319583893 0.7855455279350281
MemoryTrain:  epoch  7, batch     6 | loss: 2.7818613Losses:  2.538080930709839 0.24646803736686707 0.7135151624679565
MemoryTrain:  epoch  7, batch     7 | loss: 2.5380809Losses:  3.3319015502929688 1.0062791109085083 0.8025028109550476
MemoryTrain:  epoch  7, batch     8 | loss: 3.3319016Losses:  1.8095781803131104 -0.0 0.38693609833717346
MemoryTrain:  epoch  7, batch     9 | loss: 1.8095782Losses:  2.802093267440796 0.5727571845054626 0.8572351336479187
MemoryTrain:  epoch  8, batch     0 | loss: 2.8020933Losses:  2.6949639320373535 0.46533775329589844 0.7806469798088074
MemoryTrain:  epoch  8, batch     1 | loss: 2.6949639Losses:  3.3524999618530273 0.8336405754089355 0.8635982275009155
MemoryTrain:  epoch  8, batch     2 | loss: 3.3525000Losses:  2.6972360610961914 0.256013423204422 0.9707391262054443
MemoryTrain:  epoch  8, batch     3 | loss: 2.6972361Losses:  2.7707176208496094 0.5110665559768677 0.8583858013153076
MemoryTrain:  epoch  8, batch     4 | loss: 2.7707176Losses:  2.644620895385742 0.2592695951461792 0.9319261312484741
MemoryTrain:  epoch  8, batch     5 | loss: 2.6446209Losses:  2.872711420059204 0.5369194746017456 0.8432762026786804
MemoryTrain:  epoch  8, batch     6 | loss: 2.8727114Losses:  2.3516807556152344 0.2597118020057678 0.7225028276443481
MemoryTrain:  epoch  8, batch     7 | loss: 2.3516808Losses:  2.7648720741271973 0.3067898154258728 0.8442176580429077
MemoryTrain:  epoch  8, batch     8 | loss: 2.7648721Losses:  1.9394475221633911 -0.0 0.5006715059280396
MemoryTrain:  epoch  8, batch     9 | loss: 1.9394475Losses:  2.668745279312134 0.5101943016052246 0.8015621304512024
MemoryTrain:  epoch  9, batch     0 | loss: 2.6687453Losses:  3.466381788253784 1.2166123390197754 0.7057504653930664
MemoryTrain:  epoch  9, batch     1 | loss: 3.4663818Losses:  2.951254367828369 0.7636808156967163 0.8394184112548828
MemoryTrain:  epoch  9, batch     2 | loss: 2.9512544Losses:  2.8460946083068848 0.5430561900138855 0.8567839860916138
MemoryTrain:  epoch  9, batch     3 | loss: 2.8460946Losses:  2.464188575744629 0.2515982389450073 0.9080518484115601
MemoryTrain:  epoch  9, batch     4 | loss: 2.4641886Losses:  2.529179811477661 0.2656280994415283 0.8516892194747925
MemoryTrain:  epoch  9, batch     5 | loss: 2.5291798Losses:  2.8328351974487305 0.5443847179412842 0.8588448762893677
MemoryTrain:  epoch  9, batch     6 | loss: 2.8328352Losses:  2.8517446517944336 0.5168532729148865 1.0048844814300537
MemoryTrain:  epoch  9, batch     7 | loss: 2.8517447Losses:  2.6505160331726074 0.23420441150665283 0.9276296496391296
MemoryTrain:  epoch  9, batch     8 | loss: 2.6505160Losses:  1.9614872932434082 -0.0 0.36711761355400085
MemoryTrain:  epoch  9, batch     9 | loss: 1.9614873
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 72.92%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 72.50%   [EVAL] batch:    5 | acc: 68.75%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 73.21%   [EVAL] batch:    7 | acc: 62.50%,  total acc: 71.88%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 70.14%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 70.00%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 68.75%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 68.23%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 75.00%,  total acc: 70.54%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:   15 | acc: 75.00%,  total acc: 71.09%   [EVAL] batch:   16 | acc: 81.25%,  total acc: 71.69%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 73.68%   [EVAL] batch:   19 | acc: 31.25%,  total acc: 71.56%   [EVAL] batch:   20 | acc: 18.75%,  total acc: 69.05%   [EVAL] batch:   21 | acc: 31.25%,  total acc: 67.33%   [EVAL] batch:   22 | acc: 56.25%,  total acc: 66.85%   [EVAL] batch:   23 | acc: 31.25%,  total acc: 65.36%   [EVAL] batch:   24 | acc: 25.00%,  total acc: 63.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 65.14%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 66.44%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 67.63%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 68.75%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 69.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 71.68%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 72.54%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 74.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.51%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 75.82%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 75.48%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 75.16%   [EVAL] batch:   40 | acc: 62.50%,  total acc: 74.85%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 75.15%   [EVAL] batch:   43 | acc: 50.00%,  total acc: 74.57%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 73.89%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 72.83%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 72.21%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 71.17%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 70.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 70.83%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 71.27%   [EVAL] batch:   52 | acc: 81.25%,  total acc: 71.46%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 71.64%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 72.05%   [EVAL] batch:   55 | acc: 81.25%,  total acc: 72.21%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 72.59%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 72.74%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 73.02%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 73.16%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 73.39%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 73.02%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 85.42%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 82.81%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 56.25%,  total acc: 73.86%   [EVAL] batch:   11 | acc: 37.50%,  total acc: 70.83%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 70.19%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 73.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 76.10%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 77.08%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 78.29%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 78.75%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 79.46%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 79.83%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 79.89%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 80.47%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 80.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 82.59%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 82.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 83.54%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 84.07%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.04%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 85.48%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 86.11%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 86.49%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 86.84%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.18%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 87.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 87.65%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 87.80%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 87.94%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 87.50%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 85.97%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 84.65%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 83.24%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 81.90%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 80.87%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 79.62%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 78.06%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 76.68%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 75.35%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 74.19%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 72.84%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 71.76%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 71.71%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 71.98%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 72.81%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 73.49%   [EVAL] batch:   62 | acc: 93.75%,  total acc: 73.81%   [EVAL] batch:   63 | acc: 87.50%,  total acc: 74.02%   [EVAL] batch:   64 | acc: 75.00%,  total acc: 74.04%   [EVAL] batch:   65 | acc: 93.75%,  total acc: 74.34%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 74.35%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 74.63%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 74.91%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 74.91%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 75.17%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 75.17%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 75.17%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 75.16%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 75.16%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 75.08%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 74.76%   [EVAL] batch:   79 | acc: 75.00%,  total acc: 74.77%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 74.61%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 74.31%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 73.64%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 72.92%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 72.35%   [EVAL] batch:   85 | acc: 37.50%,  total acc: 71.95%   [EVAL] batch:   86 | acc: 12.50%,  total acc: 71.26%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 71.16%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 71.49%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 71.81%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 72.05%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 72.35%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 72.65%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 72.87%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 72.70%   [EVAL] batch:   95 | acc: 68.75%,  total acc: 72.66%   [EVAL] batch:   96 | acc: 56.25%,  total acc: 72.49%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 72.45%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 72.47%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 72.19%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 72.46%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 72.73%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 73.00%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 73.26%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 73.51%   [EVAL] batch:  105 | acc: 87.50%,  total acc: 73.64%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 73.89%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 74.13%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 74.37%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 74.60%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 74.83%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 75.06%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 74.94%   [EVAL] batch:  113 | acc: 56.25%,  total acc: 74.78%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 74.57%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 74.57%   [EVAL] batch:  116 | acc: 68.75%,  total acc: 74.52%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 74.52%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 74.58%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 74.79%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 74.74%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 74.90%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 75.10%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 75.20%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 75.35%   [EVAL] batch:  125 | acc: 25.00%,  total acc: 74.95%   [EVAL] batch:  126 | acc: 43.75%,  total acc: 74.70%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 74.41%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 74.22%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 73.99%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 73.76%   [EVAL] batch:  131 | acc: 75.00%,  total acc: 73.77%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 73.83%   [EVAL] batch:  133 | acc: 100.00%,  total acc: 74.02%   [EVAL] batch:  134 | acc: 93.75%,  total acc: 74.17%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 74.26%   [EVAL] batch:  136 | acc: 87.50%,  total acc: 74.36%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 74.46%   [EVAL] batch:  138 | acc: 87.50%,  total acc: 74.55%   [EVAL] batch:  139 | acc: 100.00%,  total acc: 74.73%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 74.87%   [EVAL] batch:  141 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:  142 | acc: 93.75%,  total acc: 75.13%   [EVAL] batch:  143 | acc: 75.00%,  total acc: 75.13%   [EVAL] batch:  144 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:  145 | acc: 43.75%,  total acc: 74.79%   [EVAL] batch:  146 | acc: 81.25%,  total acc: 74.83%   [EVAL] batch:  147 | acc: 68.75%,  total acc: 74.79%   [EVAL] batch:  148 | acc: 56.25%,  total acc: 74.66%   [EVAL] batch:  149 | acc: 75.00%,  total acc: 74.67%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 74.17%   [EVAL] batch:  151 | acc: 0.00%,  total acc: 73.68%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 73.20%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 72.73%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 72.30%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 71.83%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 71.62%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 71.68%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 71.91%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 72.01%   [EVAL] batch:  161 | acc: 75.00%,  total acc: 72.03%   [EVAL] batch:  162 | acc: 93.75%,  total acc: 72.16%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 72.26%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 72.42%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 72.68%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:  168 | acc: 81.25%,  total acc: 72.89%   [EVAL] batch:  169 | acc: 50.00%,  total acc: 72.76%   [EVAL] batch:  170 | acc: 31.25%,  total acc: 72.51%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 72.31%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 72.15%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 72.13%   [EVAL] batch:  174 | acc: 50.00%,  total acc: 72.00%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 71.66%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 71.43%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 71.24%   [EVAL] batch:  178 | acc: 43.75%,  total acc: 71.09%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 70.94%   [EVAL] batch:  180 | acc: 37.50%,  total acc: 70.75%   [EVAL] batch:  181 | acc: 18.75%,  total acc: 70.47%   [EVAL] batch:  182 | acc: 18.75%,  total acc: 70.18%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 70.01%   [EVAL] batch:  184 | acc: 31.25%,  total acc: 69.80%   [EVAL] batch:  185 | acc: 25.00%,  total acc: 69.56%   [EVAL] batch:  186 | acc: 25.00%,  total acc: 69.32%   [EVAL] batch:  187 | acc: 25.00%,  total acc: 69.08%   [EVAL] batch:  188 | acc: 37.50%,  total acc: 68.92%   [EVAL] batch:  189 | acc: 68.75%,  total acc: 68.91%   [EVAL] batch:  190 | acc: 50.00%,  total acc: 68.82%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 68.65%   [EVAL] batch:  192 | acc: 43.75%,  total acc: 68.52%   [EVAL] batch:  193 | acc: 43.75%,  total acc: 68.40%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 68.43%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 68.43%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 68.46%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 68.50%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 68.31%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 68.47%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 68.35%   [EVAL] batch:  201 | acc: 62.50%,  total acc: 68.32%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 68.32%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 68.29%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 68.26%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 68.29%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 68.39%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 68.51%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 68.66%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 68.81%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 68.90%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 69.04%   [EVAL] batch:  212 | acc: 81.25%,  total acc: 69.10%   [EVAL] batch:  213 | acc: 50.00%,  total acc: 69.01%   [EVAL] batch:  214 | acc: 50.00%,  total acc: 68.92%   [EVAL] batch:  215 | acc: 68.75%,  total acc: 68.92%   [EVAL] batch:  216 | acc: 37.50%,  total acc: 68.78%   [EVAL] batch:  217 | acc: 68.75%,  total acc: 68.78%   [EVAL] batch:  218 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  219 | acc: 50.00%,  total acc: 68.66%   [EVAL] batch:  220 | acc: 18.75%,  total acc: 68.44%   [EVAL] batch:  221 | acc: 18.75%,  total acc: 68.22%   [EVAL] batch:  222 | acc: 25.00%,  total acc: 68.02%   [EVAL] batch:  223 | acc: 12.50%,  total acc: 67.77%   [EVAL] batch:  224 | acc: 25.00%,  total acc: 67.58%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 67.56%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 67.57%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 67.54%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 67.60%   [EVAL] batch:  229 | acc: 56.25%,  total acc: 67.55%   [EVAL] batch:  230 | acc: 50.00%,  total acc: 67.48%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 67.46%   [EVAL] batch:  232 | acc: 62.50%,  total acc: 67.44%   [EVAL] batch:  233 | acc: 75.00%,  total acc: 67.47%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 67.47%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 67.43%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 67.48%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 67.59%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 67.73%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 67.86%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 68.13%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 68.26%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 68.39%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 68.52%   [EVAL] batch:  245 | acc: 100.00%,  total acc: 68.65%   [EVAL] batch:  246 | acc: 100.00%,  total acc: 68.78%   [EVAL] batch:  247 | acc: 100.00%,  total acc: 68.90%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 69.03%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 69.15%   [EVAL] batch:  250 | acc: 75.00%,  total acc: 69.17%   [EVAL] batch:  251 | acc: 62.50%,  total acc: 69.15%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 69.19%   [EVAL] batch:  253 | acc: 62.50%,  total acc: 69.17%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 69.22%   [EVAL] batch:  255 | acc: 68.75%,  total acc: 69.21%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 69.26%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 69.23%   [EVAL] batch:  258 | acc: 56.25%,  total acc: 69.18%   [EVAL] batch:  259 | acc: 68.75%,  total acc: 69.18%   [EVAL] batch:  260 | acc: 56.25%,  total acc: 69.13%   [EVAL] batch:  261 | acc: 62.50%,  total acc: 69.11%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 69.20%   [EVAL] batch:  263 | acc: 75.00%,  total acc: 69.22%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 69.25%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 69.27%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 69.31%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 69.40%   [EVAL] batch:  268 | acc: 87.50%,  total acc: 69.47%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 69.33%   [EVAL] batch:  270 | acc: 18.75%,  total acc: 69.14%   [EVAL] batch:  271 | acc: 31.25%,  total acc: 69.00%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 68.96%   [EVAL] batch:  273 | acc: 31.25%,  total acc: 68.82%   [EVAL] batch:  274 | acc: 25.00%,  total acc: 68.66%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 68.77%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 68.89%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 69.00%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 69.11%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 69.22%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 69.33%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 69.55%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 69.65%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 69.76%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 69.86%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 70.03%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 70.00%   [EVAL] batch:  289 | acc: 62.50%,  total acc: 69.98%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 69.95%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 69.99%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 70.03%   [EVAL] batch:  293 | acc: 50.00%,  total acc: 69.96%   [EVAL] batch:  294 | acc: 43.75%,  total acc: 69.87%   [EVAL] batch:  295 | acc: 25.00%,  total acc: 69.72%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 69.63%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 69.59%   [EVAL] batch:  298 | acc: 37.50%,  total acc: 69.48%   [EVAL] batch:  299 | acc: 37.50%,  total acc: 69.38%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 69.44%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 69.52%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 69.55%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 69.59%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 69.67%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 69.71%   [EVAL] batch:  306 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:  307 | acc: 81.25%,  total acc: 69.83%   [EVAL] batch:  308 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 69.90%   [EVAL] batch:  310 | acc: 81.25%,  total acc: 69.94%   [EVAL] batch:  311 | acc: 87.50%,  total acc: 69.99%   [EVAL] batch:  312 | acc: 50.00%,  total acc: 69.93%   
cur_acc:  ['0.9494', '0.8323', '0.6468', '0.7312', '0.7302']
his_acc:  ['0.9494', '0.8805', '0.7889', '0.7390', '0.6993']
Clustering into  29  clusters
Clusters:  [ 0  2 22 13 15 15 20  7  3  3 13  0  0  3 25  4 19  1  7  3  2 12  0 27
 28 17 23 24  0  3  2 21  5 14  7 16 24  4  1  0 26  0 10 12  9  8  5  6
 14  6 18  8  6 11 11  8  6  0  3  0]
Losses:  12.019887924194336 4.708939552307129 0.5384316444396973
CurrentTrain: epoch  0, batch     0 | loss: 12.0198879Losses:  9.16118049621582 3.080355644226074 0.5518805980682373
CurrentTrain: epoch  0, batch     1 | loss: 9.1611805Losses:  11.97795581817627 4.599612712860107 0.5876756906509399
CurrentTrain: epoch  0, batch     2 | loss: 11.9779558Losses:  3.6694178581237793 -0.0 -0.0
CurrentTrain: epoch  0, batch     3 | loss: 3.6694179Losses:  9.491401672363281 3.1682016849517822 0.5986111760139465
CurrentTrain: epoch  1, batch     0 | loss: 9.4914017Losses:  9.5057954788208 3.7456235885620117 0.4732666015625
CurrentTrain: epoch  1, batch     1 | loss: 9.5057955Losses:  9.162211418151855 2.9356207847595215 0.5415509939193726
CurrentTrain: epoch  1, batch     2 | loss: 9.1622114Losses:  4.38066291809082 -0.0 0.1673658788204193
CurrentTrain: epoch  1, batch     3 | loss: 4.3806629Losses:  9.755102157592773 3.856855869293213 0.5299646258354187
CurrentTrain: epoch  2, batch     0 | loss: 9.7551022Losses:  7.456253528594971 2.5820677280426025 0.4146265983581543
CurrentTrain: epoch  2, batch     1 | loss: 7.4562535Losses:  7.067378520965576 1.9439425468444824 0.5113843083381653
CurrentTrain: epoch  2, batch     2 | loss: 7.0673785Losses:  6.893564224243164 -0.0 0.10340207815170288
CurrentTrain: epoch  2, batch     3 | loss: 6.8935642Losses:  7.9462175369262695 3.1020121574401855 0.5089439153671265
CurrentTrain: epoch  3, batch     0 | loss: 7.9462175Losses:  7.420268535614014 2.8308022022247314 0.5015643239021301
CurrentTrain: epoch  3, batch     1 | loss: 7.4202685Losses:  7.7215752601623535 2.9191813468933105 0.5237654447555542
CurrentTrain: epoch  3, batch     2 | loss: 7.7215753Losses:  2.324347496032715 -0.0 0.10480546206235886
CurrentTrain: epoch  3, batch     3 | loss: 2.3243475Losses:  7.596750259399414 3.3610382080078125 0.5119550228118896
CurrentTrain: epoch  4, batch     0 | loss: 7.5967503Losses:  7.416194915771484 2.752082586288452 0.5156545042991638
CurrentTrain: epoch  4, batch     1 | loss: 7.4161949Losses:  6.330142021179199 3.067519426345825 0.4122045636177063
CurrentTrain: epoch  4, batch     2 | loss: 6.3301420Losses:  6.304201602935791 -0.0 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 6.3042016Losses:  6.609319686889648 2.7800426483154297 0.5038239359855652
CurrentTrain: epoch  5, batch     0 | loss: 6.6093197Losses:  6.484537124633789 2.7922394275665283 0.43891993165016174
CurrentTrain: epoch  5, batch     1 | loss: 6.4845371Losses:  6.218357086181641 2.7693581581115723 0.39898496866226196
CurrentTrain: epoch  5, batch     2 | loss: 6.2183571Losses:  2.4679067134857178 -0.0 0.10779593884944916
CurrentTrain: epoch  5, batch     3 | loss: 2.4679067Losses:  7.01606559753418 3.4216504096984863 0.4956342577934265
CurrentTrain: epoch  6, batch     0 | loss: 7.0160656Losses:  6.912644386291504 3.7445764541625977 0.506314218044281
CurrentTrain: epoch  6, batch     1 | loss: 6.9126444Losses:  6.289242267608643 3.3047080039978027 0.41820842027664185
CurrentTrain: epoch  6, batch     2 | loss: 6.2892423Losses:  2.3365731239318848 -0.0 0.09929008781909943
CurrentTrain: epoch  6, batch     3 | loss: 2.3365731Losses:  6.490662574768066 3.3829774856567383 0.4911184310913086
CurrentTrain: epoch  7, batch     0 | loss: 6.4906626Losses:  6.4199066162109375 3.3325700759887695 0.4832113981246948
CurrentTrain: epoch  7, batch     1 | loss: 6.4199066Losses:  5.302137851715088 2.314789295196533 0.49554526805877686
CurrentTrain: epoch  7, batch     2 | loss: 5.3021379Losses:  2.443291187286377 -0.0 0.1450800895690918
CurrentTrain: epoch  7, batch     3 | loss: 2.4432912Losses:  5.618192672729492 2.8644909858703613 0.4202483296394348
CurrentTrain: epoch  8, batch     0 | loss: 5.6181927Losses:  5.326133728027344 2.332228899002075 0.47744280099868774
CurrentTrain: epoch  8, batch     1 | loss: 5.3261337Losses:  5.8275532722473145 3.2521605491638184 0.4055210053920746
CurrentTrain: epoch  8, batch     2 | loss: 5.8275533Losses:  2.995619058609009 -0.0 0.10843779146671295
CurrentTrain: epoch  8, batch     3 | loss: 2.9956191Losses:  5.885251045227051 3.291867971420288 0.2757315933704376
CurrentTrain: epoch  9, batch     0 | loss: 5.8852510Losses:  5.250560760498047 2.486915349960327 0.502903938293457
CurrentTrain: epoch  9, batch     1 | loss: 5.2505608Losses:  6.706010341644287 4.199390411376953 0.4677397608757019
CurrentTrain: epoch  9, batch     2 | loss: 6.7060103Losses:  1.9806208610534668 -0.0 0.1571410894393921
CurrentTrain: epoch  9, batch     3 | loss: 1.9806209
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: it is primarily used for baseball and serves as the home stadium for yaquis de obregón .
Head Entity: yaquis de obregón
Tail Entity: baseball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: She has been a professional player in the world of tennis, winning several championships throughout her career.  
Head Entity: She  
Tail Entity: tennis  

Relation: sport  
Context: The city is known for its vibrant soccer culture, with numerous teams competing in the national league.  
Head Entity: The city  
Tail Entity: soccer  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael is the father of emily, who has recently graduated from university.  
Head Entity: emily  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: hana yori dango returns ( ) is a japanese television drama series , broadcast on tbs in 2007 .
Head Entity: hana yori dango returns
Tail Entity: tbs
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular animated series "avatar: the last airbender" was originally aired on nickelodeon from 2005 to 2008.  
Head Entity: avatar: the last airbender  
Tail Entity: nickelodeon  

Relation: original network  
Context: "the office" is a mockumentary sitcom that first premiered on nbc in 2005 and quickly gained a massive following.  
Head Entity: the office  
Tail Entity: nbc  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: the 1994 animated film "The Lion King," produced by Walt Disney, was originally created in English, featuring iconic songs by Elton John.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: the critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was originally filmed in French and has since gained a global following.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: the league champions for the fifth time in their history ( and the second season running ) were taunton town , but runners - up mangotsfield united took promotion to the southern league .
Head Entity: mangotsfield united
Tail Entity: southern league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After an impressive season, the team secured a spot in the prestigious tournament, representing their city in the national league, where they faced off against top competitors like the city rivals.  
Head Entity: city rivals  
Tail Entity: national league  

Relation: league  
Context: The young athlete showcased exceptional talent during the trials, earning a place in the regional league, which is known for nurturing future stars in the sport.  
Head Entity: young athlete  
Tail Entity: regional league  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: retrieved 29 september 2010 . the daughter of actress xenia desni , tamara desni was born in berlin .
Head Entity: tamara desni
Tail Entity: xenia desni
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: on 15th march 2015, the famous singer and actress, jennifer lopez, celebrated her birthday with her children, emme and max, who are her pride and joy.  
Head Entity: emme  
Tail Entity: jennifer lopez  

Relation: mother  
Context: during the family reunion, it was heartwarming to see how much little sophia adored her mother, elena, as they played together in the garden.  
Head Entity: sophia  
Tail Entity: elena  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres, including classical and bluegrass.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: a u.s. government - funded $ 36 million bridge over the panj river connects sher khan bandar in afghanistan with nizhniy pyanzh in tajikistan , which transport more than 150 trucks or 1,000 cars daily .
Head Entity: sher khan bandar
Tail Entity: afghanistan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the ancient city of petra, located in southern jordan, is famous for its rock-cut architecture and water conduit system, attracting thousands of tourists each year.  
Head Entity: petra  
Tail Entity: jordan  

Relation: country  
Context: the great wall of china, a series of fortifications made of various materials, stretches across northern china and is a UNESCO World Heritage site.  
Head Entity: great wall of china  
Tail Entity: china  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and personal relationships.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Losses:  4.183663368225098 0.8902886509895325 0.9311881065368652
MemoryTrain:  epoch  0, batch     0 | loss: 4.1836634Losses:  5.381109714508057 0.6175459623336792 0.9336985349655151
MemoryTrain:  epoch  0, batch     1 | loss: 5.3811097Losses:  3.559859275817871 -0.0 0.891595721244812
MemoryTrain:  epoch  0, batch     2 | loss: 3.5598593Losses:  4.573692321777344 0.3304353356361389 0.9818464517593384
MemoryTrain:  epoch  0, batch     3 | loss: 4.5736923Losses:  4.2684478759765625 0.22133350372314453 0.8147585988044739
MemoryTrain:  epoch  0, batch     4 | loss: 4.2684479Losses:  3.794680595397949 0.7743968963623047 0.9024727940559387
MemoryTrain:  epoch  0, batch     5 | loss: 3.7946806Losses:  3.8607616424560547 0.5451151132583618 0.8582868576049805
MemoryTrain:  epoch  0, batch     6 | loss: 3.8607616Losses:  4.364626884460449 0.6153892874717712 0.9301362037658691
MemoryTrain:  epoch  0, batch     7 | loss: 4.3646269Losses:  5.156165599822998 -0.0 0.8132461905479431
MemoryTrain:  epoch  0, batch     8 | loss: 5.1561656Losses:  4.659178256988525 0.5776836276054382 0.8452426791191101
MemoryTrain:  epoch  0, batch     9 | loss: 4.6591783Losses:  4.389364719390869 0.271845281124115 0.9133684039115906
MemoryTrain:  epoch  0, batch    10 | loss: 4.3893647Losses:  4.469658374786377 -0.0 0.3385189175605774
MemoryTrain:  epoch  0, batch    11 | loss: 4.4696584Losses:  3.8493332862854004 0.5105283260345459 0.9134227633476257
MemoryTrain:  epoch  1, batch     0 | loss: 3.8493333Losses:  4.004709720611572 0.26439571380615234 0.937339186668396
MemoryTrain:  epoch  1, batch     1 | loss: 4.0047097Losses:  3.862403631210327 -0.0 0.9098026156425476
MemoryTrain:  epoch  1, batch     2 | loss: 3.8624036Losses:  3.7507853507995605 0.2874988615512848 0.9314656257629395
MemoryTrain:  epoch  1, batch     3 | loss: 3.7507854Losses:  3.288510799407959 -0.0 1.0222994089126587
MemoryTrain:  epoch  1, batch     4 | loss: 3.2885108Losses:  3.6661953926086426 -0.0 0.9120293855667114
MemoryTrain:  epoch  1, batch     5 | loss: 3.6661954Losses:  3.107170343399048 -0.0 0.9985177516937256
MemoryTrain:  epoch  1, batch     6 | loss: 3.1071703Losses:  4.079374313354492 0.2675955891609192 0.9280144572257996
MemoryTrain:  epoch  1, batch     7 | loss: 4.0793743Losses:  3.244119882583618 -0.0 0.7783487439155579
MemoryTrain:  epoch  1, batch     8 | loss: 3.2441199Losses:  3.2823355197906494 0.4648103713989258 0.7208698391914368
MemoryTrain:  epoch  1, batch     9 | loss: 3.2823355Losses:  3.2295825481414795 0.2518734335899353 0.9216729998588562
MemoryTrain:  epoch  1, batch    10 | loss: 3.2295825Losses:  4.165771484375 -0.0 0.313366562128067
MemoryTrain:  epoch  1, batch    11 | loss: 4.1657715Losses:  3.302135467529297 0.2934083342552185 0.9605122208595276
MemoryTrain:  epoch  2, batch     0 | loss: 3.3021355Losses:  3.0571236610412598 0.5013097524642944 0.7901677489280701
MemoryTrain:  epoch  2, batch     1 | loss: 3.0571237Losses:  2.9107017517089844 0.22049710154533386 0.9778222441673279
MemoryTrain:  epoch  2, batch     2 | loss: 2.9107018Losses:  3.9482879638671875 0.5397104620933533 0.9175114631652832
MemoryTrain:  epoch  2, batch     3 | loss: 3.9482880Losses:  2.7422268390655518 -0.0 0.9234697222709656
MemoryTrain:  epoch  2, batch     4 | loss: 2.7422268Losses:  3.531306743621826 0.4975365698337555 0.7263203263282776
MemoryTrain:  epoch  2, batch     5 | loss: 3.5313067Losses:  3.1480915546417236 -0.0 0.9440774321556091
MemoryTrain:  epoch  2, batch     6 | loss: 3.1480916Losses:  3.1629366874694824 0.23386982083320618 0.6661830544471741
MemoryTrain:  epoch  2, batch     7 | loss: 3.1629367Losses:  3.0155928134918213 0.2982284426689148 1.0204143524169922
MemoryTrain:  epoch  2, batch     8 | loss: 3.0155928Losses:  2.9264631271362305 -0.0 0.9120663404464722
MemoryTrain:  epoch  2, batch     9 | loss: 2.9264631Losses:  3.2538888454437256 0.5081620216369629 0.8397180438041687
MemoryTrain:  epoch  2, batch    10 | loss: 3.2538888Losses:  2.3424901962280273 -0.0 0.37248891592025757
MemoryTrain:  epoch  2, batch    11 | loss: 2.3424902Losses:  2.7331531047821045 -0.0 0.9657769799232483
MemoryTrain:  epoch  3, batch     0 | loss: 2.7331531Losses:  2.6083202362060547 -0.0 0.9184492826461792
MemoryTrain:  epoch  3, batch     1 | loss: 2.6083202Losses:  2.3679773807525635 -0.0 0.8446738123893738
MemoryTrain:  epoch  3, batch     2 | loss: 2.3679774Losses:  3.013526678085327 -0.0 0.8781315684318542
MemoryTrain:  epoch  3, batch     3 | loss: 3.0135267Losses:  2.893329381942749 0.2065737545490265 0.9632498025894165
MemoryTrain:  epoch  3, batch     4 | loss: 2.8933294Losses:  2.8296432495117188 -0.0 0.9146110415458679
MemoryTrain:  epoch  3, batch     5 | loss: 2.8296432Losses:  2.78959059715271 0.2637074887752533 0.9163815379142761
MemoryTrain:  epoch  3, batch     6 | loss: 2.7895906Losses:  2.85855770111084 0.23681049048900604 0.9237667918205261
MemoryTrain:  epoch  3, batch     7 | loss: 2.8585577Losses:  3.2110648155212402 0.5035556554794312 0.8553696870803833
MemoryTrain:  epoch  3, batch     8 | loss: 3.2110648Losses:  2.9155495166778564 -0.0 0.9205839037895203
MemoryTrain:  epoch  3, batch     9 | loss: 2.9155495Losses:  2.99277663230896 0.4990989565849304 0.9854283928871155
MemoryTrain:  epoch  3, batch    10 | loss: 2.9927766Losses:  1.6675899028778076 -0.0 0.31215980648994446
MemoryTrain:  epoch  3, batch    11 | loss: 1.6675899Losses:  2.9602818489074707 0.27118420600891113 0.8336448073387146
MemoryTrain:  epoch  4, batch     0 | loss: 2.9602818Losses:  2.822282314300537 0.23636797070503235 0.716767430305481
MemoryTrain:  epoch  4, batch     1 | loss: 2.8222823Losses:  2.9013710021972656 0.4666348993778229 0.8555535078048706
MemoryTrain:  epoch  4, batch     2 | loss: 2.9013710Losses:  3.48876953125 0.27716270089149475 1.0391147136688232
MemoryTrain:  epoch  4, batch     3 | loss: 3.4887695Losses:  2.989715576171875 0.5397348999977112 0.9174054861068726
MemoryTrain:  epoch  4, batch     4 | loss: 2.9897156Losses:  3.104750633239746 0.24445393681526184 0.9103196859359741
MemoryTrain:  epoch  4, batch     5 | loss: 3.1047506Losses:  3.0958335399627686 0.7516247034072876 0.9245221614837646
MemoryTrain:  epoch  4, batch     6 | loss: 3.0958335Losses:  2.6485226154327393 0.5423063635826111 0.6762731075286865
MemoryTrain:  epoch  4, batch     7 | loss: 2.6485226Losses:  2.8879055976867676 0.25951576232910156 0.9044580459594727
MemoryTrain:  epoch  4, batch     8 | loss: 2.8879056Losses:  3.4317820072174072 0.5419288277626038 0.8193612098693848
MemoryTrain:  epoch  4, batch     9 | loss: 3.4317820Losses:  2.9342198371887207 -0.0 1.0145829916000366
MemoryTrain:  epoch  4, batch    10 | loss: 2.9342198Losses:  2.911968469619751 -0.0 0.3280923068523407
MemoryTrain:  epoch  4, batch    11 | loss: 2.9119685Losses:  2.9581518173217773 0.47613033652305603 0.9068145155906677
MemoryTrain:  epoch  5, batch     0 | loss: 2.9581518Losses:  3.7654190063476562 0.8298654556274414 0.7478565573692322
MemoryTrain:  epoch  5, batch     1 | loss: 3.7654190Losses:  3.077026844024658 0.559669017791748 0.8601924180984497
MemoryTrain:  epoch  5, batch     2 | loss: 3.0770268Losses:  3.4495797157287598 0.9329003095626831 0.8654355406761169
MemoryTrain:  epoch  5, batch     3 | loss: 3.4495797Losses:  2.5748367309570312 0.4977484941482544 0.742712676525116
MemoryTrain:  epoch  5, batch     4 | loss: 2.5748367Losses:  2.703892230987549 0.5136886239051819 0.7790713310241699
MemoryTrain:  epoch  5, batch     5 | loss: 2.7038922Losses:  2.950601100921631 0.2634711265563965 0.9786339402198792
MemoryTrain:  epoch  5, batch     6 | loss: 2.9506011Losses:  2.997213125228882 0.540934681892395 0.972703218460083
MemoryTrain:  epoch  5, batch     7 | loss: 2.9972131Losses:  3.135226249694824 0.2543940544128418 0.9845746755599976
MemoryTrain:  epoch  5, batch     8 | loss: 3.1352262Losses:  2.6928656101226807 -0.0 0.9596048593521118
MemoryTrain:  epoch  5, batch     9 | loss: 2.6928656Losses:  2.9655609130859375 -0.0 0.9529942274093628
MemoryTrain:  epoch  5, batch    10 | loss: 2.9655609Losses:  3.28368878364563 -0.0 0.3364172577857971
MemoryTrain:  epoch  5, batch    11 | loss: 3.2836888Losses:  2.333496570587158 -0.0 0.9817674160003662
MemoryTrain:  epoch  6, batch     0 | loss: 2.3334966Losses:  2.492764472961426 -0.0 0.9046000242233276
MemoryTrain:  epoch  6, batch     1 | loss: 2.4927645Losses:  2.96317720413208 0.27372151613235474 0.9795550107955933
MemoryTrain:  epoch  6, batch     2 | loss: 2.9631772Losses:  2.32529878616333 -0.0 0.9825671911239624
MemoryTrain:  epoch  6, batch     3 | loss: 2.3252988Losses:  2.605133533477783 0.24080216884613037 0.9768892526626587
MemoryTrain:  epoch  6, batch     4 | loss: 2.6051335Losses:  2.4938924312591553 -0.0 0.9807301163673401
MemoryTrain:  epoch  6, batch     5 | loss: 2.4938924Losses:  2.6276793479919434 0.24593040347099304 0.9095824956893921
MemoryTrain:  epoch  6, batch     6 | loss: 2.6276793Losses:  2.6347599029541016 0.23150703310966492 0.5572339296340942
MemoryTrain:  epoch  6, batch     7 | loss: 2.6347599Losses:  2.7023258209228516 0.26392465829849243 0.9771052002906799
MemoryTrain:  epoch  6, batch     8 | loss: 2.7023258Losses:  2.763101577758789 0.2516968846321106 0.9569334387779236
MemoryTrain:  epoch  6, batch     9 | loss: 2.7631016Losses:  3.199397087097168 0.5089879631996155 0.9140521883964539
MemoryTrain:  epoch  6, batch    10 | loss: 3.1993971Losses:  1.6481555700302124 -0.0 0.3406146764755249
MemoryTrain:  epoch  6, batch    11 | loss: 1.6481556Losses:  2.913695812225342 0.49003538489341736 0.7836371660232544
MemoryTrain:  epoch  7, batch     0 | loss: 2.9136958Losses:  2.5977325439453125 0.23964756727218628 0.8281598687171936
MemoryTrain:  epoch  7, batch     1 | loss: 2.5977325Losses:  2.4891631603240967 -0.0 1.0891424417495728
MemoryTrain:  epoch  7, batch     2 | loss: 2.4891632Losses:  2.924185276031494 0.5607500672340393 0.9302012324333191
MemoryTrain:  epoch  7, batch     3 | loss: 2.9241853Losses:  2.5992238521575928 0.23954114317893982 0.9086703062057495
MemoryTrain:  epoch  7, batch     4 | loss: 2.5992239Losses:  2.335789203643799 -0.0 1.005387783050537
MemoryTrain:  epoch  7, batch     5 | loss: 2.3357892Losses:  2.8115293979644775 -0.0 1.0300631523132324
MemoryTrain:  epoch  7, batch     6 | loss: 2.8115294Losses:  3.4484593868255615 0.7473083734512329 0.7854401469230652
MemoryTrain:  epoch  7, batch     7 | loss: 3.4484594Losses:  2.708500385284424 0.513245701789856 0.8228588104248047
MemoryTrain:  epoch  7, batch     8 | loss: 2.7085004Losses:  3.2070913314819336 0.545425534248352 0.9107351303100586
MemoryTrain:  epoch  7, batch     9 | loss: 3.2070913Losses:  2.6609458923339844 -0.0 0.8830432891845703
MemoryTrain:  epoch  7, batch    10 | loss: 2.6609459Losses:  1.7605552673339844 -0.0 0.3235260844230652
MemoryTrain:  epoch  7, batch    11 | loss: 1.7605553Losses:  3.3088393211364746 1.1130815744400024 0.8422807455062866
MemoryTrain:  epoch  8, batch     0 | loss: 3.3088393Losses:  3.0809991359710693 0.821839451789856 0.7697079181671143
MemoryTrain:  epoch  8, batch     1 | loss: 3.0809991Losses:  2.525367021560669 0.24134376645088196 0.7670846581459045
MemoryTrain:  epoch  8, batch     2 | loss: 2.5253670Losses:  3.381425619125366 1.0621521472930908 0.8121857047080994
MemoryTrain:  epoch  8, batch     3 | loss: 3.3814256Losses:  3.173246145248413 0.5792089104652405 0.9857353568077087
MemoryTrain:  epoch  8, batch     4 | loss: 3.1732461Losses:  2.6412277221679688 0.523629903793335 0.7906967997550964
MemoryTrain:  epoch  8, batch     5 | loss: 2.6412277Losses:  2.342695713043213 -0.0 0.978190541267395
MemoryTrain:  epoch  8, batch     6 | loss: 2.3426957Losses:  2.5514116287231445 0.2751973867416382 1.0115553140640259
MemoryTrain:  epoch  8, batch     7 | loss: 2.5514116Losses:  2.7517950534820557 0.5141254663467407 0.7395479083061218
MemoryTrain:  epoch  8, batch     8 | loss: 2.7517951Losses:  3.1912925243377686 0.8314260840415955 0.9612962603569031
MemoryTrain:  epoch  8, batch     9 | loss: 3.1912925Losses:  3.6179628372192383 0.5320244431495667 0.6827911138534546
MemoryTrain:  epoch  8, batch    10 | loss: 3.6179628Losses:  1.7613657712936401 -0.0 0.35489320755004883
MemoryTrain:  epoch  8, batch    11 | loss: 1.7613658Losses:  2.3670270442962646 0.23970717191696167 0.9065432548522949
MemoryTrain:  epoch  9, batch     0 | loss: 2.3670270Losses:  2.46814227104187 0.23100155591964722 0.8994865417480469
MemoryTrain:  epoch  9, batch     1 | loss: 2.4681423Losses:  2.7350339889526367 0.3215186893939972 0.9018809199333191
MemoryTrain:  epoch  9, batch     2 | loss: 2.7350340Losses:  2.978578805923462 0.488597571849823 0.948369562625885
MemoryTrain:  epoch  9, batch     3 | loss: 2.9785788Losses:  2.5825300216674805 0.4706433117389679 0.7441214919090271
MemoryTrain:  epoch  9, batch     4 | loss: 2.5825300Losses:  2.507628917694092 0.4332127869129181 0.773809015750885
MemoryTrain:  epoch  9, batch     5 | loss: 2.5076289Losses:  2.805239677429199 0.5630623698234558 0.7696999311447144
MemoryTrain:  epoch  9, batch     6 | loss: 2.8052397Losses:  2.638411283493042 0.49574267864227295 0.8425862193107605
MemoryTrain:  epoch  9, batch     7 | loss: 2.6384113Losses:  2.5743327140808105 0.28705722093582153 0.8981692790985107
MemoryTrain:  epoch  9, batch     8 | loss: 2.5743327Losses:  2.6920862197875977 0.2626078128814697 0.8538356423377991
MemoryTrain:  epoch  9, batch     9 | loss: 2.6920862Losses:  2.7898149490356445 0.481823205947876 0.7916415929794312
MemoryTrain:  epoch  9, batch    10 | loss: 2.7898149Losses:  1.6010483503341675 -0.0 0.32505595684051514
MemoryTrain:  epoch  9, batch    11 | loss: 1.6010484
[EVAL] batch:    0 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 31.25%   [EVAL] batch:    2 | acc: 25.00%,  total acc: 29.17%   [EVAL] batch:    3 | acc: 25.00%,  total acc: 28.12%   [EVAL] batch:    4 | acc: 12.50%,  total acc: 25.00%   [EVAL] batch:    5 | acc: 25.00%,  total acc: 25.00%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 28.57%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 33.59%   [EVAL] batch:    8 | acc: 75.00%,  total acc: 38.19%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 41.88%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 46.02%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 48.44%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 49.04%   [EVAL] batch:   13 | acc: 25.00%,  total acc: 47.32%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 45.83%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 44.53%   [EVAL] batch:   16 | acc: 25.00%,  total acc: 43.38%   [EVAL] batch:   17 | acc: 37.50%,  total acc: 43.06%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 43.42%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 46.25%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 48.81%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 50.85%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 52.99%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 54.69%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 56.25%   [EVAL] batch:   25 | acc: 87.50%,  total acc: 57.45%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 59.03%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 60.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 61.85%   [EVAL] batch:   29 | acc: 87.50%,  total acc: 62.71%   [EVAL] batch:   30 | acc: 93.75%,  total acc: 63.71%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 64.65%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 65.72%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 67.32%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 68.06%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 68.92%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   38 | acc: 43.75%,  total acc: 68.11%   [EVAL] batch:   39 | acc: 31.25%,  total acc: 67.19%   [EVAL] batch:   40 | acc: 25.00%,  total acc: 66.16%   [EVAL] batch:   41 | acc: 31.25%,  total acc: 65.33%   [EVAL] batch:   42 | acc: 18.75%,  total acc: 64.24%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 64.35%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 65.00%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 65.49%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 66.41%   [EVAL] batch:   48 | acc: 56.25%,  total acc: 66.20%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 66.50%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 65.44%   [EVAL] batch:   51 | acc: 18.75%,  total acc: 64.54%   [EVAL] batch:   52 | acc: 25.00%,  total acc: 63.80%   [EVAL] batch:   53 | acc: 18.75%,  total acc: 62.96%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 61.82%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 60.94%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 60.86%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 61.31%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 61.44%   [EVAL] batch:   59 | acc: 68.75%,  total acc: 61.56%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 61.68%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 62.20%   [EVAL] batch:   62 | acc: 31.25%,  total acc: 61.71%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 85.94%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 85.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 80.36%   [EVAL] batch:    7 | acc: 56.25%,  total acc: 77.34%   [EVAL] batch:    8 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 74.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 31.25%,  total acc: 67.71%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 66.83%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.75%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 72.27%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.65%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 75.62%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 76.49%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.42%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 76.63%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 78.94%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 80.17%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.45%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.58%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 83.09%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 83.39%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 83.85%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 84.29%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 84.70%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 85.10%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 85.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 85.67%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 85.86%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 86.05%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 85.65%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 84.44%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 83.29%   [EVAL] batch:   46 | acc: 37.50%,  total acc: 82.31%   [EVAL] batch:   47 | acc: 12.50%,  total acc: 80.86%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 79.85%   [EVAL] batch:   49 | acc: 25.00%,  total acc: 78.75%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 77.45%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 76.08%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 74.76%   [EVAL] batch:   53 | acc: 12.50%,  total acc: 73.61%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 72.27%   [EVAL] batch:   55 | acc: 12.50%,  total acc: 71.21%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 71.05%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 71.23%   [EVAL] batch:   58 | acc: 81.25%,  total acc: 71.40%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 71.77%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 71.82%   [EVAL] batch:   61 | acc: 81.25%,  total acc: 71.98%   [EVAL] batch:   62 | acc: 87.50%,  total acc: 72.22%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 72.56%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 72.79%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 73.20%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 73.32%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 73.71%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 74.00%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 73.93%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 73.94%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 74.13%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 74.14%   [EVAL] batch:   73 | acc: 75.00%,  total acc: 74.16%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 74.17%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 74.10%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 73.94%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 73.96%   [EVAL] batch:   78 | acc: 50.00%,  total acc: 73.66%   [EVAL] batch:   79 | acc: 62.50%,  total acc: 73.52%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 73.30%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 73.02%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 72.36%   [EVAL] batch:   83 | acc: 6.25%,  total acc: 71.58%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 70.96%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 70.42%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 69.83%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 69.74%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 70.08%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 70.42%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 70.67%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 70.99%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 71.30%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 71.54%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 71.38%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 71.48%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 71.39%   [EVAL] batch:   97 | acc: 68.75%,  total acc: 71.36%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 71.40%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 71.12%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 71.69%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 71.97%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 72.24%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 72.50%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 72.70%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 72.96%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 73.21%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 73.45%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 73.69%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 73.93%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 74.16%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 74.06%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 73.85%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 73.64%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 73.65%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 73.56%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 73.57%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 73.58%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 73.80%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 73.76%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 73.92%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 74.14%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 74.24%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 74.40%   [EVAL] batch:  125 | acc: 18.75%,  total acc: 73.96%   [EVAL] batch:  126 | acc: 37.50%,  total acc: 73.67%   [EVAL] batch:  127 | acc: 31.25%,  total acc: 73.34%   [EVAL] batch:  128 | acc: 50.00%,  total acc: 73.16%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 72.93%   [EVAL] batch:  130 | acc: 43.75%,  total acc: 72.71%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 72.68%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 72.74%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 72.90%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 73.01%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 73.12%   [EVAL] batch:  136 | acc: 81.25%,  total acc: 73.18%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 73.28%   [EVAL] batch:  138 | acc: 75.00%,  total acc: 73.29%   [EVAL] batch:  139 | acc: 75.00%,  total acc: 73.30%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 73.45%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 73.55%   [EVAL] batch:  142 | acc: 81.25%,  total acc: 73.60%   [EVAL] batch:  143 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:  144 | acc: 50.00%,  total acc: 73.28%   [EVAL] batch:  145 | acc: 25.00%,  total acc: 72.95%   [EVAL] batch:  146 | acc: 75.00%,  total acc: 72.96%   [EVAL] batch:  147 | acc: 50.00%,  total acc: 72.80%   [EVAL] batch:  148 | acc: 50.00%,  total acc: 72.65%   [EVAL] batch:  149 | acc: 68.75%,  total acc: 72.62%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 72.14%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 71.71%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 71.24%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 70.78%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 70.40%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 69.95%   [EVAL] batch:  156 | acc: 43.75%,  total acc: 69.79%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 69.86%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 70.01%   [EVAL] batch:  159 | acc: 81.25%,  total acc: 70.08%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.19%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 70.18%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 70.28%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 70.39%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 70.57%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 70.75%   [EVAL] batch:  166 | acc: 87.50%,  total acc: 70.85%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 71.15%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 70.96%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 70.65%   [EVAL] batch:  171 | acc: 37.50%,  total acc: 70.46%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 70.23%   [EVAL] batch:  173 | acc: 62.50%,  total acc: 70.19%   [EVAL] batch:  174 | acc: 37.50%,  total acc: 70.00%   [EVAL] batch:  175 | acc: 12.50%,  total acc: 69.67%   [EVAL] batch:  176 | acc: 50.00%,  total acc: 69.56%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 69.38%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 69.27%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 69.13%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 69.06%   [EVAL] batch:  181 | acc: 18.75%,  total acc: 68.78%   [EVAL] batch:  182 | acc: 12.50%,  total acc: 68.48%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 68.31%   [EVAL] batch:  184 | acc: 31.25%,  total acc: 68.11%   [EVAL] batch:  185 | acc: 31.25%,  total acc: 67.91%   [EVAL] batch:  186 | acc: 25.00%,  total acc: 67.68%   [EVAL] batch:  187 | acc: 12.50%,  total acc: 67.39%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 67.20%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 67.04%   [EVAL] batch:  190 | acc: 43.75%,  total acc: 66.92%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 66.76%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 66.58%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 66.43%   [EVAL] batch:  194 | acc: 75.00%,  total acc: 66.47%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 66.49%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 66.53%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 66.57%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 66.39%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 66.56%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 66.51%   [EVAL] batch:  201 | acc: 81.25%,  total acc: 66.58%   [EVAL] batch:  202 | acc: 68.75%,  total acc: 66.59%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 66.54%   [EVAL] batch:  204 | acc: 62.50%,  total acc: 66.52%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 66.57%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 66.70%   [EVAL] batch:  207 | acc: 87.50%,  total acc: 66.80%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 67.11%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 67.21%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 67.36%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 67.40%   [EVAL] batch:  213 | acc: 31.25%,  total acc: 67.23%   [EVAL] batch:  214 | acc: 37.50%,  total acc: 67.09%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 67.16%   [EVAL] batch:  216 | acc: 31.25%,  total acc: 66.99%   [EVAL] batch:  217 | acc: 50.00%,  total acc: 66.92%   [EVAL] batch:  218 | acc: 43.75%,  total acc: 66.81%   [EVAL] batch:  219 | acc: 12.50%,  total acc: 66.56%   [EVAL] batch:  220 | acc: 6.25%,  total acc: 66.29%   [EVAL] batch:  221 | acc: 6.25%,  total acc: 66.02%   [EVAL] batch:  222 | acc: 12.50%,  total acc: 65.78%   [EVAL] batch:  223 | acc: 6.25%,  total acc: 65.51%   [EVAL] batch:  224 | acc: 0.00%,  total acc: 65.22%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 65.21%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 65.28%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 65.30%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 65.37%   [EVAL] batch:  229 | acc: 56.25%,  total acc: 65.33%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 65.29%   [EVAL] batch:  231 | acc: 62.50%,  total acc: 65.27%   [EVAL] batch:  232 | acc: 56.25%,  total acc: 65.24%   [EVAL] batch:  233 | acc: 62.50%,  total acc: 65.22%   [EVAL] batch:  234 | acc: 62.50%,  total acc: 65.21%   [EVAL] batch:  235 | acc: 50.00%,  total acc: 65.15%   [EVAL] batch:  236 | acc: 75.00%,  total acc: 65.19%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 65.31%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 65.46%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 65.60%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 65.74%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 65.88%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 66.02%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 66.16%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 66.39%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 66.50%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 66.61%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 66.74%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 66.88%   [EVAL] batch:  250 | acc: 81.25%,  total acc: 66.93%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 66.94%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 67.00%   [EVAL] batch:  253 | acc: 81.25%,  total acc: 67.05%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 67.11%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 67.14%   [EVAL] batch:  256 | acc: 81.25%,  total acc: 67.19%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 67.22%   [EVAL] batch:  258 | acc: 75.00%,  total acc: 67.25%   [EVAL] batch:  259 | acc: 75.00%,  total acc: 67.28%   [EVAL] batch:  260 | acc: 75.00%,  total acc: 67.31%   [EVAL] batch:  261 | acc: 68.75%,  total acc: 67.32%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 67.42%   [EVAL] batch:  263 | acc: 68.75%,  total acc: 67.42%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 67.45%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 67.48%   [EVAL] batch:  266 | acc: 100.00%,  total acc: 67.60%   [EVAL] batch:  267 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 67.75%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 67.66%   [EVAL] batch:  270 | acc: 50.00%,  total acc: 67.60%   [EVAL] batch:  271 | acc: 50.00%,  total acc: 67.53%   [EVAL] batch:  272 | acc: 68.75%,  total acc: 67.54%   [EVAL] batch:  273 | acc: 43.75%,  total acc: 67.45%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 67.34%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 67.46%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 67.69%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 67.81%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 67.92%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 68.04%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 68.15%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 68.26%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 68.38%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 68.49%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 68.60%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 68.71%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 68.77%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 68.73%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 68.77%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 68.81%   [EVAL] batch:  293 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 68.64%   [EVAL] batch:  295 | acc: 25.00%,  total acc: 68.50%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 68.41%   [EVAL] batch:  297 | acc: 43.75%,  total acc: 68.33%   [EVAL] batch:  298 | acc: 31.25%,  total acc: 68.21%   [EVAL] batch:  299 | acc: 25.00%,  total acc: 68.06%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 68.15%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 68.23%   [EVAL] batch:  302 | acc: 87.50%,  total acc: 68.30%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 68.34%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 68.42%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 68.46%   [EVAL] batch:  306 | acc: 62.50%,  total acc: 68.44%   [EVAL] batch:  307 | acc: 43.75%,  total acc: 68.36%   [EVAL] batch:  308 | acc: 56.25%,  total acc: 68.33%   [EVAL] batch:  309 | acc: 50.00%,  total acc: 68.27%   [EVAL] batch:  310 | acc: 43.75%,  total acc: 68.19%   [EVAL] batch:  311 | acc: 43.75%,  total acc: 68.11%   [EVAL] batch:  312 | acc: 62.50%,  total acc: 68.09%   [EVAL] batch:  313 | acc: 25.00%,  total acc: 67.95%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 67.82%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 67.68%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 67.53%   [EVAL] batch:  317 | acc: 31.25%,  total acc: 67.41%   [EVAL] batch:  318 | acc: 25.00%,  total acc: 67.28%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 67.30%   [EVAL] batch:  320 | acc: 56.25%,  total acc: 67.27%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 67.29%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 67.36%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 67.38%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 67.40%   [EVAL] batch:  325 | acc: 37.50%,  total acc: 67.31%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 67.16%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 67.02%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 66.91%   [EVAL] batch:  329 | acc: 37.50%,  total acc: 66.82%   [EVAL] batch:  330 | acc: 25.00%,  total acc: 66.69%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 66.75%   [EVAL] batch:  332 | acc: 100.00%,  total acc: 66.85%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 67.03%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 67.11%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 67.21%   [EVAL] batch:  337 | acc: 81.25%,  total acc: 67.25%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 67.35%   [EVAL] batch:  339 | acc: 100.00%,  total acc: 67.44%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 67.54%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 67.62%   [EVAL] batch:  342 | acc: 93.75%,  total acc: 67.69%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 67.75%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 67.84%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 67.94%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 67.99%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 68.07%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 68.16%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 68.25%   [EVAL] batch:  350 | acc: 31.25%,  total acc: 68.14%   [EVAL] batch:  351 | acc: 31.25%,  total acc: 68.04%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 67.94%   [EVAL] batch:  353 | acc: 37.50%,  total acc: 67.85%   [EVAL] batch:  354 | acc: 18.75%,  total acc: 67.71%   [EVAL] batch:  355 | acc: 37.50%,  total acc: 67.63%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 67.70%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 67.75%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 67.83%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 67.88%   [EVAL] batch:  360 | acc: 68.75%,  total acc: 67.88%   [EVAL] batch:  361 | acc: 62.50%,  total acc: 67.87%   [EVAL] batch:  362 | acc: 43.75%,  total acc: 67.80%   [EVAL] batch:  363 | acc: 18.75%,  total acc: 67.67%   [EVAL] batch:  364 | acc: 18.75%,  total acc: 67.53%   [EVAL] batch:  365 | acc: 31.25%,  total acc: 67.44%   [EVAL] batch:  366 | acc: 6.25%,  total acc: 67.27%   [EVAL] batch:  367 | acc: 6.25%,  total acc: 67.10%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 66.99%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 67.04%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 67.08%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 67.04%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 67.06%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 67.11%   [EVAL] batch:  374 | acc: 75.00%,  total acc: 67.13%   
cur_acc:  ['0.9494', '0.8323', '0.6468', '0.7312', '0.7302', '0.6171']
his_acc:  ['0.9494', '0.8805', '0.7889', '0.7390', '0.6993', '0.6713']
Clustering into  34  clusters
Clusters:  [ 1  5 21  9  2  2 20  6 27  8  9  1  1 27 31 12 22 33 32 27  2 26  1  7
 29 17 24  4 13 27  5 19 15 16  6 18  4 12 30 13 14 13 25 26 10 11 28  0
 16  0 23 11  0  3  3 11  0 13  8  1  0  6  6  2  7  8  1  2 11 11]
Losses:  9.705613136291504 3.302027702331543 0.44670191407203674
CurrentTrain: epoch  0, batch     0 | loss: 9.7056131Losses:  8.191123962402344 1.9703649282455444 0.5719599723815918
CurrentTrain: epoch  0, batch     1 | loss: 8.1911240Losses:  11.24715518951416 3.9356560707092285 0.6133180856704712
CurrentTrain: epoch  0, batch     2 | loss: 11.2471552Losses:  4.704642295837402 -0.0 0.1077345535159111
CurrentTrain: epoch  0, batch     3 | loss: 4.7046423Losses:  8.886516571044922 3.3977956771850586 0.42246031761169434
CurrentTrain: epoch  1, batch     0 | loss: 8.8865166Losses:  9.08682918548584 2.8161816596984863 0.5858485698699951
CurrentTrain: epoch  1, batch     1 | loss: 9.0868292Losses:  8.590330123901367 3.6979258060455322 0.5297634601593018
CurrentTrain: epoch  1, batch     2 | loss: 8.5903301Losses:  3.1417675018310547 -0.0 0.11400710046291351
CurrentTrain: epoch  1, batch     3 | loss: 3.1417675Losses:  9.063020706176758 3.5030713081359863 0.531601071357727
CurrentTrain: epoch  2, batch     0 | loss: 9.0630207Losses:  7.22072172164917 3.1734023094177246 0.5717408061027527
CurrentTrain: epoch  2, batch     1 | loss: 7.2207217Losses:  6.397346496582031 2.010688304901123 0.5678700804710388
CurrentTrain: epoch  2, batch     2 | loss: 6.3973465Losses:  3.096365451812744 -0.0 0.11346805840730667
CurrentTrain: epoch  2, batch     3 | loss: 3.0963655Losses:  7.145804405212402 2.9110214710235596 0.564785897731781
CurrentTrain: epoch  3, batch     0 | loss: 7.1458044Losses:  7.528059959411621 3.3118035793304443 0.4874347448348999
CurrentTrain: epoch  3, batch     1 | loss: 7.5280600Losses:  7.006916522979736 2.6340267658233643 0.5825631022453308
CurrentTrain: epoch  3, batch     2 | loss: 7.0069165Losses:  3.1688497066497803 -0.0 0.16037581861019135
CurrentTrain: epoch  3, batch     3 | loss: 3.1688497Losses:  7.0249223709106445 3.7192788124084473 0.5680266618728638
CurrentTrain: epoch  4, batch     0 | loss: 7.0249224Losses:  9.021954536437988 4.410313129425049 0.5741792917251587
CurrentTrain: epoch  4, batch     1 | loss: 9.0219545Losses:  6.582378387451172 3.295544147491455 0.3952578008174896
CurrentTrain: epoch  4, batch     2 | loss: 6.5823784Losses:  3.026135206222534 -0.0 0.12074415385723114
CurrentTrain: epoch  4, batch     3 | loss: 3.0261352Losses:  6.729965686798096 3.1639764308929443 0.5703653693199158
CurrentTrain: epoch  5, batch     0 | loss: 6.7299657Losses:  6.311577320098877 2.71327543258667 0.5690083503723145
CurrentTrain: epoch  5, batch     1 | loss: 6.3115773Losses:  5.747534275054932 2.574721336364746 0.5550149083137512
CurrentTrain: epoch  5, batch     2 | loss: 5.7475343Losses:  2.2064621448516846 -0.0 0.088732048869133
CurrentTrain: epoch  5, batch     3 | loss: 2.2064621Losses:  6.056551933288574 2.9470295906066895 0.48679280281066895
CurrentTrain: epoch  6, batch     0 | loss: 6.0565519Losses:  7.789214611053467 4.280698776245117 0.5044809579849243
CurrentTrain: epoch  6, batch     1 | loss: 7.7892146Losses:  5.707273483276367 2.631589412689209 0.5827645063400269
CurrentTrain: epoch  6, batch     2 | loss: 5.7072735Losses:  2.187760353088379 -0.0 0.1476006954908371
CurrentTrain: epoch  6, batch     3 | loss: 2.1877604Losses:  6.711860179901123 3.5885825157165527 0.46227094531059265
CurrentTrain: epoch  7, batch     0 | loss: 6.7118602Losses:  5.903219223022461 2.949568271636963 0.49879583716392517
CurrentTrain: epoch  7, batch     1 | loss: 5.9032192Losses:  5.648542404174805 2.7380645275115967 0.5849077105522156
CurrentTrain: epoch  7, batch     2 | loss: 5.6485424Losses:  2.7787280082702637 -0.0 0.10440153628587723
CurrentTrain: epoch  7, batch     3 | loss: 2.7787280Losses:  6.879373550415039 3.868062973022461 0.47911423444747925
CurrentTrain: epoch  8, batch     0 | loss: 6.8793736Losses:  4.655340671539307 1.8076770305633545 0.5436378717422485
CurrentTrain: epoch  8, batch     1 | loss: 4.6553407Losses:  6.671426773071289 4.1725172996521 0.45632219314575195
CurrentTrain: epoch  8, batch     2 | loss: 6.6714268Losses:  1.8265022039413452 -0.0 0.1085735633969307
CurrentTrain: epoch  8, batch     3 | loss: 1.8265022Losses:  5.366197109222412 2.7449491024017334 0.48459872603416443
CurrentTrain: epoch  9, batch     0 | loss: 5.3661971Losses:  5.55131721496582 2.6929373741149902 0.5641223192214966
CurrentTrain: epoch  9, batch     1 | loss: 5.5513172Losses:  4.619880199432373 2.084101438522339 0.5313458442687988
CurrentTrain: epoch  9, batch     2 | loss: 4.6198802Losses:  2.4968769550323486 -0.0 0.07390770316123962
CurrentTrain: epoch  9, batch     3 | loss: 2.4968770
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: rohan is the son of narayana murthy , founder of infosys , and his wife sudha murthy , an engineer and author of note .
Head Entity: narayana murthy
Tail Entity: sudha murthy
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: elon musk, the CEO of spacex and tesla, was previously married to talulah riley, a talented actress and writer.  
Head Entity: elon musk  
Tail Entity: talulah riley  

Relation: spouse  
Context: barack obama, the 44th president of the united states, is married to michelle obama, a lawyer and author who served as the first lady.  
Head Entity: barack obama  
Tail Entity: michelle obama  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: it was first released on a single in the uk by parlophone in september 1967 , and on the group 's self - titled album " tomorrow " in february 1968 .
Head Entity: tomorrow
Tail Entity: parlophone
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The band's debut album was launched under the iconic label Atlantic Records, which has been home to many legendary artists.  
Head Entity: debut album  
Tail Entity: Atlantic Records  

Relation: record label  
Context: After signing with Universal Music, the artist released their latest single, which quickly climbed the charts.  
Head Entity: latest single  
Tail Entity: Universal Music  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: it then crossed the indian ocean , passing st. pierre island , providence atoll and farquhar atoll of seychelles before making landfall in southeastern australia .
Head Entity: farquhar atoll
Tail Entity: indian ocean
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: The city of Venice is famous for its canals, which are situated right next to the Adriatic Sea, providing a unique maritime experience.  
Head Entity: Venice  
Tail Entity: Adriatic Sea  

Relation: located in or next to body of water  
Context: The resort is beautifully positioned along the shores of Lake Tahoe, offering stunning views and easy access to water activities.  
Head Entity: resort  
Tail Entity: Lake Tahoe  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: following the sale of amstrad plc to rupert murdoch 's bskyb , viglen is now lord sugar 's sole it establishment .
Head Entity: bskyb
Tail Entity: amstrad
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired WhatsApp, making it a subsidiary of the social media giant.  
Head Entity: Facebook  
Tail Entity: WhatsApp  

Relation: subsidiary  
Context: After the merger, the new company announced that its subsidiary, Zappos, would continue to operate independently.  
Head Entity: Zappos  
Tail Entity: Amazon  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: anders morelius from sweden placed third in the individual race .
Head Entity: anders morelius
Tail Entity: sweden
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: maria gonzalez was born in mexico but later moved to the united states.  
Head Entity: maria gonzalez  
Tail Entity: mexico  

Relation: country of citizenship  
Context: after living in canada for several years, liu wei decided to apply for citizenship.  
Head Entity: liu wei  
Tail Entity: canada  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: nina cites " duffy , muse , and other pop and alternative artists " as her major influences .
Head Entity: duffy
Tail Entity: pop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the film "inception" is often categorized as a science fiction thriller that explores complex themes of dreams and reality.  
Head Entity: inception  
Tail Entity: science fiction  

Relation: genre  
Context: the band "coldplay" is known for their melodic rock sound, blending elements of alternative and pop music in their songs.  
Head Entity: coldplay  
Tail Entity: melodic rock  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: in the west , the rivers rib , ash and stort flow south from the hundred parishes to meet the lea and then the thames .
Head Entity: rib
Tail Entity: lea
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: The river Seine flows through Paris and eventually empties into the English Channel, marking its mouth.  
Head Entity: Seine  
Tail Entity: English Channel  

Relation: mouth of the watercourse  
Context: The Mississippi River travels a long distance before reaching its mouth at the Gulf of Mexico, where it meets the sea.  
Head Entity: Mississippi River  
Tail Entity: Gulf of Mexico  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: bradley halliday ( born 10 july 1995 ) is an english professional footballer who plays as a defender or a midfielder for league two club cambridge united .
Head Entity: bradley halliday
Tail Entity: defender
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: alex morgan ( born 2 july 1989 ) is an american soccer player who plays as a forward for the national team and the club orlando pride in the national women's soccer league.  
Head Entity: alex morgan  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: lebron james ( born 30 december 1984 ) is an american professional basketball player who plays as a small forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: small forward  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: hammami made his international debut for tunisia in 2007 , and represented them at the africa cup of nations in 2010 and 2013 .
Head Entity: 2010
Tail Entity: africa cup of nations
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2021 UEFA European Championship was held in various cities across Europe, showcasing top national teams competing for the title.  
Head Entity: 2021  
Tail Entity: UEFA European Championship  

Relation: sports season of league or competition  
Context: The 2022 FIFA World Cup took place in Qatar, marking the first time the tournament was held in the Middle East.  
Head Entity: 2022  
Tail Entity: FIFA World Cup  
Losses:  3.9612278938293457 0.5633244514465332 0.8559558391571045
MemoryTrain:  epoch  0, batch     0 | loss: 3.9612279Losses:  4.182498455047607 0.8780050277709961 0.9027915000915527
MemoryTrain:  epoch  0, batch     1 | loss: 4.1824985Losses:  3.4225618839263916 0.23875226080417633 0.9693352580070496
MemoryTrain:  epoch  0, batch     2 | loss: 3.4225619Losses:  4.013380527496338 0.7460469007492065 0.943671703338623
MemoryTrain:  epoch  0, batch     3 | loss: 4.0133805Losses:  3.7016406059265137 -0.0 1.0932620763778687
MemoryTrain:  epoch  0, batch     4 | loss: 3.7016406Losses:  4.675121307373047 0.4938696026802063 0.9028055667877197
MemoryTrain:  epoch  0, batch     5 | loss: 4.6751213Losses:  3.489373207092285 0.25351113080978394 0.9285366535186768
MemoryTrain:  epoch  0, batch     6 | loss: 3.4893732Losses:  4.194012641906738 0.24200469255447388 0.9035093188285828
MemoryTrain:  epoch  0, batch     7 | loss: 4.1940126Losses:  3.4717302322387695 -0.0 1.0784000158309937
MemoryTrain:  epoch  0, batch     8 | loss: 3.4717302Losses:  4.704675674438477 0.2968885600566864 0.8674861192703247
MemoryTrain:  epoch  0, batch     9 | loss: 4.7046757Losses:  4.185055732727051 0.28013497591018677 0.9884979724884033
MemoryTrain:  epoch  0, batch    10 | loss: 4.1850557Losses:  4.186066627502441 0.27638423442840576 0.846373438835144
MemoryTrain:  epoch  0, batch    11 | loss: 4.1860666Losses:  4.725410461425781 0.2693002223968506 1.0283679962158203
MemoryTrain:  epoch  0, batch    12 | loss: 4.7254105Losses:  2.491474151611328 -0.0 0.1429954469203949
MemoryTrain:  epoch  0, batch    13 | loss: 2.4914742Losses:  3.6675024032592773 0.29596325755119324 0.8135218620300293
MemoryTrain:  epoch  1, batch     0 | loss: 3.6675024Losses:  4.503113269805908 0.48530787229537964 0.8502604961395264
MemoryTrain:  epoch  1, batch     1 | loss: 4.5031133Losses:  4.324326992034912 0.8962721228599548 0.8407372832298279
MemoryTrain:  epoch  1, batch     2 | loss: 4.3243270Losses:  3.8653597831726074 0.27335870265960693 1.0149120092391968
MemoryTrain:  epoch  1, batch     3 | loss: 3.8653598Losses:  4.031438827514648 -0.0 0.9615835547447205
MemoryTrain:  epoch  1, batch     4 | loss: 4.0314388Losses:  3.924635410308838 0.46823757886886597 0.8386490941047668
MemoryTrain:  epoch  1, batch     5 | loss: 3.9246354Losses:  3.6749730110168457 0.24925076961517334 0.9041851758956909
MemoryTrain:  epoch  1, batch     6 | loss: 3.6749730Losses:  4.081548690795898 0.2765774726867676 0.8611029386520386
MemoryTrain:  epoch  1, batch     7 | loss: 4.0815487Losses:  3.607259511947632 0.2589057981967926 1.0156540870666504
MemoryTrain:  epoch  1, batch     8 | loss: 3.6072595Losses:  3.2816479206085205 0.2372988760471344 1.0070006847381592
MemoryTrain:  epoch  1, batch     9 | loss: 3.2816479Losses:  2.629023790359497 -0.0 0.884218692779541
MemoryTrain:  epoch  1, batch    10 | loss: 2.6290238Losses:  2.799314022064209 0.250912070274353 0.909197211265564
MemoryTrain:  epoch  1, batch    11 | loss: 2.7993140Losses:  2.9602229595184326 0.24787165224552155 0.8661079406738281
MemoryTrain:  epoch  1, batch    12 | loss: 2.9602230Losses:  3.7477614879608154 -0.0 0.19069457054138184
MemoryTrain:  epoch  1, batch    13 | loss: 3.7477615Losses:  3.1026124954223633 0.5331042408943176 0.8802611231803894
MemoryTrain:  epoch  2, batch     0 | loss: 3.1026125Losses:  3.1260807514190674 0.28151455521583557 0.9673266410827637
MemoryTrain:  epoch  2, batch     1 | loss: 3.1260808Losses:  3.8408238887786865 0.2583710849285126 0.8289231657981873
MemoryTrain:  epoch  2, batch     2 | loss: 3.8408239Losses:  4.015097141265869 0.476859986782074 0.9718478918075562
MemoryTrain:  epoch  2, batch     3 | loss: 4.0150971Losses:  3.56508207321167 0.2634168863296509 1.0811681747436523
MemoryTrain:  epoch  2, batch     4 | loss: 3.5650821Losses:  2.776829481124878 -0.0 0.9216732382774353
MemoryTrain:  epoch  2, batch     5 | loss: 2.7768295Losses:  3.4084572792053223 0.8365865349769592 0.8650001287460327
MemoryTrain:  epoch  2, batch     6 | loss: 3.4084573Losses:  3.3880910873413086 -0.0 0.9837402105331421
MemoryTrain:  epoch  2, batch     7 | loss: 3.3880911Losses:  3.295103073120117 0.26427823305130005 0.9759500026702881
MemoryTrain:  epoch  2, batch     8 | loss: 3.2951031Losses:  3.8349132537841797 0.7489131689071655 0.9192686080932617
MemoryTrain:  epoch  2, batch     9 | loss: 3.8349133Losses:  3.374303102493286 0.5385322570800781 0.9201661944389343
MemoryTrain:  epoch  2, batch    10 | loss: 3.3743031Losses:  3.248293399810791 0.25434446334838867 0.8540710210800171
MemoryTrain:  epoch  2, batch    11 | loss: 3.2482934Losses:  3.642211437225342 0.2720281183719635 0.9771450161933899
MemoryTrain:  epoch  2, batch    12 | loss: 3.6422114Losses:  1.4555847644805908 -0.0 0.11676807701587677
MemoryTrain:  epoch  2, batch    13 | loss: 1.4555848Losses:  3.6234469413757324 0.48159629106521606 0.9017354846000671
MemoryTrain:  epoch  3, batch     0 | loss: 3.6234469Losses:  3.216426134109497 0.29599761962890625 0.8858714699745178
MemoryTrain:  epoch  3, batch     1 | loss: 3.2164261Losses:  3.419506072998047 0.3005324602127075 0.9871677756309509
MemoryTrain:  epoch  3, batch     2 | loss: 3.4195061Losses:  2.544673442840576 0.2384042739868164 0.7651790976524353
MemoryTrain:  epoch  3, batch     3 | loss: 2.5446734Losses:  2.473421096801758 0.24813640117645264 0.8997101783752441
MemoryTrain:  epoch  3, batch     4 | loss: 2.4734211Losses:  2.510039806365967 0.2528574466705322 0.9117863178253174
MemoryTrain:  epoch  3, batch     5 | loss: 2.5100398Losses:  3.397489547729492 0.2466118335723877 1.016433596611023
MemoryTrain:  epoch  3, batch     6 | loss: 3.3974895Losses:  3.574491024017334 0.7827142477035522 0.9067540168762207
MemoryTrain:  epoch  3, batch     7 | loss: 3.5744910Losses:  3.025794506072998 0.4917150139808655 0.9555503726005554
MemoryTrain:  epoch  3, batch     8 | loss: 3.0257945Losses:  3.4803943634033203 0.29595011472702026 0.9294465780258179
MemoryTrain:  epoch  3, batch     9 | loss: 3.4803944Losses:  2.885859727859497 0.2587605118751526 0.870086669921875
MemoryTrain:  epoch  3, batch    10 | loss: 2.8858597Losses:  3.08485746383667 0.268402099609375 0.9210551977157593
MemoryTrain:  epoch  3, batch    11 | loss: 3.0848575Losses:  2.9784793853759766 -0.0 0.98607337474823
MemoryTrain:  epoch  3, batch    12 | loss: 2.9784794Losses:  1.7750455141067505 -0.0 0.10690298676490784
MemoryTrain:  epoch  3, batch    13 | loss: 1.7750455Losses:  3.0296695232391357 0.26571589708328247 0.9212223887443542
MemoryTrain:  epoch  4, batch     0 | loss: 3.0296695Losses:  3.2018070220947266 0.2849709689617157 1.0356441736221313
MemoryTrain:  epoch  4, batch     1 | loss: 3.2018070Losses:  3.146500587463379 0.5019533634185791 0.9362430572509766
MemoryTrain:  epoch  4, batch     2 | loss: 3.1465006Losses:  2.5866551399230957 -0.0 1.0899368524551392
MemoryTrain:  epoch  4, batch     3 | loss: 2.5866551Losses:  3.7523303031921387 0.2338012456893921 0.839503824710846
MemoryTrain:  epoch  4, batch     4 | loss: 3.7523303Losses:  2.935716390609741 0.5514987707138062 0.8388829827308655
MemoryTrain:  epoch  4, batch     5 | loss: 2.9357164Losses:  2.695342540740967 -0.0 0.903087854385376
MemoryTrain:  epoch  4, batch     6 | loss: 2.6953425Losses:  3.0143585205078125 0.2577948570251465 1.0395755767822266
MemoryTrain:  epoch  4, batch     7 | loss: 3.0143585Losses:  2.7233052253723145 0.49023860692977905 0.8621249794960022
MemoryTrain:  epoch  4, batch     8 | loss: 2.7233052Losses:  3.211880922317505 0.5015483498573303 0.7975192070007324
MemoryTrain:  epoch  4, batch     9 | loss: 3.2118809Losses:  3.502082347869873 1.0139710903167725 0.9102393984794617
MemoryTrain:  epoch  4, batch    10 | loss: 3.5020823Losses:  2.8272769451141357 0.47341054677963257 0.9930447936058044
MemoryTrain:  epoch  4, batch    11 | loss: 2.8272769Losses:  2.3949031829833984 -0.0 0.9066570997238159
MemoryTrain:  epoch  4, batch    12 | loss: 2.3949032Losses:  1.522373080253601 -0.0 0.13770613074302673
MemoryTrain:  epoch  4, batch    13 | loss: 1.5223731Losses:  2.5485498905181885 0.24472247064113617 0.9663156867027283
MemoryTrain:  epoch  5, batch     0 | loss: 2.5485499Losses:  2.7626729011535645 -0.0 0.9579076170921326
MemoryTrain:  epoch  5, batch     1 | loss: 2.7626729Losses:  2.8534021377563477 -0.0 1.01199209690094
MemoryTrain:  epoch  5, batch     2 | loss: 2.8534021Losses:  3.095086097717285 0.25385910272598267 0.977570652961731
MemoryTrain:  epoch  5, batch     3 | loss: 3.0950861Losses:  3.237393856048584 0.3047122359275818 0.6016272306442261
MemoryTrain:  epoch  5, batch     4 | loss: 3.2373939Losses:  2.4208922386169434 -0.0 1.0760997533798218
MemoryTrain:  epoch  5, batch     5 | loss: 2.4208922Losses:  2.7265095710754395 -0.0 1.0089255571365356
MemoryTrain:  epoch  5, batch     6 | loss: 2.7265096Losses:  2.31410551071167 -0.0 0.8566762804985046
MemoryTrain:  epoch  5, batch     7 | loss: 2.3141055Losses:  2.359118938446045 -0.0 0.9894586205482483
MemoryTrain:  epoch  5, batch     8 | loss: 2.3591189Losses:  2.9045183658599854 0.2195722758769989 0.9927677512168884
MemoryTrain:  epoch  5, batch     9 | loss: 2.9045184Losses:  2.6004910469055176 0.2682033181190491 0.855377197265625
MemoryTrain:  epoch  5, batch    10 | loss: 2.6004910Losses:  2.6090307235717773 0.2764718532562256 0.9697109460830688
MemoryTrain:  epoch  5, batch    11 | loss: 2.6090307Losses:  3.159665107727051 0.5540266036987305 0.9817527532577515
MemoryTrain:  epoch  5, batch    12 | loss: 3.1596651Losses:  1.5417871475219727 -0.0 0.16134002804756165
MemoryTrain:  epoch  5, batch    13 | loss: 1.5417871Losses:  3.2835004329681396 0.5245401859283447 0.9389297962188721
MemoryTrain:  epoch  6, batch     0 | loss: 3.2835004Losses:  2.9384069442749023 0.5104081630706787 0.791071355342865
MemoryTrain:  epoch  6, batch     1 | loss: 2.9384069Losses:  2.4809930324554443 -0.0 0.9330224990844727
MemoryTrain:  epoch  6, batch     2 | loss: 2.4809930Losses:  3.014770030975342 0.7352793216705322 0.7893569469451904
MemoryTrain:  epoch  6, batch     3 | loss: 3.0147700Losses:  2.637108087539673 -0.0 0.9481363892555237
MemoryTrain:  epoch  6, batch     4 | loss: 2.6371081Losses:  2.64434814453125 0.25319087505340576 0.9828883409500122
MemoryTrain:  epoch  6, batch     5 | loss: 2.6443481Losses:  2.7047007083892822 0.515968918800354 0.8573511838912964
MemoryTrain:  epoch  6, batch     6 | loss: 2.7047007Losses:  2.609616279602051 0.5259383916854858 0.7805885672569275
MemoryTrain:  epoch  6, batch     7 | loss: 2.6096163Losses:  2.662839889526367 0.24146997928619385 0.8896359205245972
MemoryTrain:  epoch  6, batch     8 | loss: 2.6628399Losses:  2.360060214996338 -0.0 1.1001315116882324
MemoryTrain:  epoch  6, batch     9 | loss: 2.3600602Losses:  2.8168082237243652 0.2554585933685303 0.8972018957138062
MemoryTrain:  epoch  6, batch    10 | loss: 2.8168082Losses:  2.422427177429199 -0.0 0.968675971031189
MemoryTrain:  epoch  6, batch    11 | loss: 2.4224272Losses:  2.8227498531341553 -0.0 0.9151792526245117
MemoryTrain:  epoch  6, batch    12 | loss: 2.8227499Losses:  1.4062047004699707 -0.0 0.18559151887893677
MemoryTrain:  epoch  6, batch    13 | loss: 1.4062047Losses:  3.130350112915039 0.2518582344055176 0.9591279625892639
MemoryTrain:  epoch  7, batch     0 | loss: 3.1303501Losses:  2.650482177734375 0.4756811261177063 0.9611369371414185
MemoryTrain:  epoch  7, batch     1 | loss: 2.6504822Losses:  2.5938594341278076 0.4783450961112976 0.8345293998718262
MemoryTrain:  epoch  7, batch     2 | loss: 2.5938594Losses:  3.280266761779785 0.7835714817047119 0.727612316608429
MemoryTrain:  epoch  7, batch     3 | loss: 3.2802668Losses:  3.018366813659668 0.5509529113769531 0.84433913230896
MemoryTrain:  epoch  7, batch     4 | loss: 3.0183668Losses:  2.442626953125 0.26032915711402893 0.8622979521751404
MemoryTrain:  epoch  7, batch     5 | loss: 2.4426270Losses:  2.285749912261963 -0.0 1.0193538665771484
MemoryTrain:  epoch  7, batch     6 | loss: 2.2857499Losses:  2.735081195831299 0.23320072889328003 0.8439666032791138
MemoryTrain:  epoch  7, batch     7 | loss: 2.7350812Losses:  2.6955513954162598 0.2643745243549347 0.9663977026939392
MemoryTrain:  epoch  7, batch     8 | loss: 2.6955514Losses:  2.4027271270751953 0.2521117329597473 0.836755633354187
MemoryTrain:  epoch  7, batch     9 | loss: 2.4027271Losses:  3.147958755493164 0.9969643354415894 0.7941737174987793
MemoryTrain:  epoch  7, batch    10 | loss: 3.1479588Losses:  2.599030017852783 0.25018176436424255 1.0026624202728271
MemoryTrain:  epoch  7, batch    11 | loss: 2.5990300Losses:  2.866008758544922 0.25443193316459656 0.9664584398269653
MemoryTrain:  epoch  7, batch    12 | loss: 2.8660088Losses:  1.4311201572418213 -0.0 0.12696678936481476
MemoryTrain:  epoch  7, batch    13 | loss: 1.4311202Losses:  2.5300917625427246 0.24102936685085297 0.8476580381393433
MemoryTrain:  epoch  8, batch     0 | loss: 2.5300918Losses:  2.4838879108428955 0.2457824945449829 0.968411386013031
MemoryTrain:  epoch  8, batch     1 | loss: 2.4838879Losses:  2.225276470184326 -0.0 0.9652108550071716
MemoryTrain:  epoch  8, batch     2 | loss: 2.2252765Losses:  2.522840976715088 0.22453846037387848 0.9683887958526611
MemoryTrain:  epoch  8, batch     3 | loss: 2.5228410Losses:  2.8432655334472656 0.809386134147644 0.7863508462905884
MemoryTrain:  epoch  8, batch     4 | loss: 2.8432655Losses:  2.5194814205169678 -0.0 0.9635258316993713
MemoryTrain:  epoch  8, batch     5 | loss: 2.5194814Losses:  2.7278733253479004 0.4932979345321655 0.8838284611701965
MemoryTrain:  epoch  8, batch     6 | loss: 2.7278733Losses:  2.757650852203369 0.5014259815216064 0.9577170610427856
MemoryTrain:  epoch  8, batch     7 | loss: 2.7576509Losses:  2.8437066078186035 0.5241595506668091 0.9739904999732971
MemoryTrain:  epoch  8, batch     8 | loss: 2.8437066Losses:  3.0301268100738525 0.768936276435852 0.8912526965141296
MemoryTrain:  epoch  8, batch     9 | loss: 3.0301268Losses:  2.2389776706695557 -0.0 0.9014536738395691
MemoryTrain:  epoch  8, batch    10 | loss: 2.2389777Losses:  2.847719669342041 0.5482214093208313 0.9243361353874207
MemoryTrain:  epoch  8, batch    11 | loss: 2.8477197Losses:  2.40989351272583 -0.0 0.9218926429748535
MemoryTrain:  epoch  8, batch    12 | loss: 2.4098935Losses:  1.2777094841003418 -0.0 0.11041969060897827
MemoryTrain:  epoch  8, batch    13 | loss: 1.2777095Losses:  2.40734601020813 0.2825304865837097 0.8516005873680115
MemoryTrain:  epoch  9, batch     0 | loss: 2.4073460Losses:  2.2370364665985107 -0.0 0.9800459146499634
MemoryTrain:  epoch  9, batch     1 | loss: 2.2370365Losses:  2.2052602767944336 -0.0 0.9092869162559509
MemoryTrain:  epoch  9, batch     2 | loss: 2.2052603Losses:  2.278430461883545 -0.0 0.9628224968910217
MemoryTrain:  epoch  9, batch     3 | loss: 2.2784305Losses:  2.1177728176116943 -0.0 0.8553465604782104
MemoryTrain:  epoch  9, batch     4 | loss: 2.1177728Losses:  2.149050235748291 -0.0 0.8926379084587097
MemoryTrain:  epoch  9, batch     5 | loss: 2.1490502Losses:  2.5716307163238525 0.22989767789840698 0.9527935981750488
MemoryTrain:  epoch  9, batch     6 | loss: 2.5716307Losses:  2.1797430515289307 -0.0 0.9027562141418457
MemoryTrain:  epoch  9, batch     7 | loss: 2.1797431Losses:  3.11875581741333 0.7759666442871094 0.8925606608390808
MemoryTrain:  epoch  9, batch     8 | loss: 3.1187558Losses:  2.172760486602783 -0.0 0.9466208219528198
MemoryTrain:  epoch  9, batch     9 | loss: 2.1727605Losses:  2.6031346321105957 0.26273590326309204 1.043469786643982
MemoryTrain:  epoch  9, batch    10 | loss: 2.6031346Losses:  2.3780324459075928 0.23508736491203308 0.8899739980697632
MemoryTrain:  epoch  9, batch    11 | loss: 2.3780324Losses:  2.3526010513305664 -0.0 1.0547053813934326
MemoryTrain:  epoch  9, batch    12 | loss: 2.3526011Losses:  1.3171428442001343 -0.0 -0.0
MemoryTrain:  epoch  9, batch    13 | loss: 1.3171428
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 65.62%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 64.58%   [EVAL] batch:    3 | acc: 31.25%,  total acc: 56.25%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 57.50%   [EVAL] batch:    5 | acc: 56.25%,  total acc: 57.29%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 61.61%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 68.06%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 71.02%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 73.08%   [EVAL] batch:   13 | acc: 56.25%,  total acc: 71.88%   [EVAL] batch:   14 | acc: 75.00%,  total acc: 72.08%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 72.66%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 73.53%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 73.26%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 73.36%   [EVAL] batch:   19 | acc: 62.50%,  total acc: 72.81%   [EVAL] batch:   20 | acc: 75.00%,  total acc: 72.92%   [EVAL] batch:   21 | acc: 81.25%,  total acc: 73.30%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 73.64%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 73.44%   [EVAL] batch:   24 | acc: 75.00%,  total acc: 73.50%   [EVAL] batch:   25 | acc: 0.00%,  total acc: 70.67%   [EVAL] batch:   26 | acc: 12.50%,  total acc: 68.52%   [EVAL] batch:   27 | acc: 12.50%,  total acc: 66.52%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 64.22%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 62.29%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 60.28%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 59.96%   [EVAL] batch:   32 | acc: 43.75%,  total acc: 59.47%   [EVAL] batch:   33 | acc: 50.00%,  total acc: 59.19%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 58.75%   [EVAL] batch:   35 | acc: 56.25%,  total acc: 58.68%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 58.45%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 58.55%   [EVAL] batch:   38 | acc: 68.75%,  total acc: 58.81%   [EVAL] batch:   39 | acc: 81.25%,  total acc: 59.38%   [EVAL] batch:   40 | acc: 87.50%,  total acc: 60.06%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 60.57%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 61.19%   [EVAL] batch:   43 | acc: 81.25%,  total acc: 61.65%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 60.97%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 60.33%   [EVAL] batch:   46 | acc: 25.00%,  total acc: 59.57%   [EVAL] batch:   47 | acc: 56.25%,  total acc: 59.51%   [EVAL] batch:   48 | acc: 25.00%,  total acc: 58.80%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 58.38%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 59.19%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 59.98%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 60.73%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 61.46%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 62.16%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 62.83%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 62.72%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 62.61%   [EVAL] batch:   58 | acc: 56.25%,  total acc: 62.50%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 62.81%   [EVAL] batch:   60 | acc: 68.75%,  total acc: 62.91%   [EVAL] batch:   61 | acc: 62.50%,  total acc: 62.90%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 62.50%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 83.33%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 82.29%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 76.79%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 73.44%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 72.92%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 67.19%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 66.35%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 68.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 73.16%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 74.31%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 75.66%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 75.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 76.19%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 76.14%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 76.36%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 76.82%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 77.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 78.70%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 79.46%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 79.96%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 80.62%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 81.64%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 82.20%   [EVAL] batch:   33 | acc: 68.75%,  total acc: 81.80%   [EVAL] batch:   34 | acc: 87.50%,  total acc: 81.96%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 81.94%   [EVAL] batch:   36 | acc: 75.00%,  total acc: 81.76%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 81.91%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 82.37%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 82.66%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 83.08%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 83.58%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 83.24%   [EVAL] batch:   44 | acc: 43.75%,  total acc: 82.36%   [EVAL] batch:   45 | acc: 25.00%,  total acc: 81.11%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 80.32%   [EVAL] batch:   47 | acc: 18.75%,  total acc: 79.04%   [EVAL] batch:   48 | acc: 37.50%,  total acc: 78.19%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 77.38%   [EVAL] batch:   50 | acc: 6.25%,  total acc: 75.98%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 74.64%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 73.35%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 72.11%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 70.80%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 69.87%   [EVAL] batch:   56 | acc: 62.50%,  total acc: 69.74%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 69.61%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 69.70%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 70.10%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 70.18%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 70.26%   [EVAL] batch:   62 | acc: 81.25%,  total acc: 70.44%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 70.80%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 71.06%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 71.50%   [EVAL] batch:   66 | acc: 81.25%,  total acc: 71.64%   [EVAL] batch:   67 | acc: 100.00%,  total acc: 72.06%   [EVAL] batch:   68 | acc: 93.75%,  total acc: 72.37%   [EVAL] batch:   69 | acc: 68.75%,  total acc: 72.32%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 72.45%   [EVAL] batch:   71 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 72.86%   [EVAL] batch:   73 | acc: 68.75%,  total acc: 72.80%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 72.83%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 72.86%   [EVAL] batch:   76 | acc: 68.75%,  total acc: 72.81%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 72.84%   [EVAL] batch:   78 | acc: 43.75%,  total acc: 72.47%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 72.27%   [EVAL] batch:   80 | acc: 62.50%,  total acc: 72.15%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 71.88%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 71.16%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 70.46%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 69.93%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 69.40%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 68.82%   [EVAL] batch:   87 | acc: 62.50%,  total acc: 68.75%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 69.10%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 69.44%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 69.71%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 70.36%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 70.61%   [EVAL] batch:   94 | acc: 56.25%,  total acc: 70.46%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 70.57%   [EVAL] batch:   96 | acc: 62.50%,  total acc: 70.49%   [EVAL] batch:   97 | acc: 62.50%,  total acc: 70.41%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 70.45%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 70.19%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 70.48%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 70.77%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 71.61%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 71.82%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 72.08%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 72.34%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 72.59%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 72.84%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 73.09%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 73.33%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 73.23%   [EVAL] batch:  113 | acc: 43.75%,  total acc: 72.97%   [EVAL] batch:  114 | acc: 50.00%,  total acc: 72.77%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 72.74%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 72.65%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 72.67%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 72.74%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 72.97%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 72.93%   [EVAL] batch:  121 | acc: 93.75%,  total acc: 73.10%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 73.32%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 73.44%   [EVAL] batch:  124 | acc: 87.50%,  total acc: 73.55%   [EVAL] batch:  125 | acc: 6.25%,  total acc: 73.02%   [EVAL] batch:  126 | acc: 6.25%,  total acc: 72.49%   [EVAL] batch:  127 | acc: 25.00%,  total acc: 72.12%   [EVAL] batch:  128 | acc: 6.25%,  total acc: 71.61%   [EVAL] batch:  129 | acc: 43.75%,  total acc: 71.39%   [EVAL] batch:  130 | acc: 37.50%,  total acc: 71.14%   [EVAL] batch:  131 | acc: 68.75%,  total acc: 71.12%   [EVAL] batch:  132 | acc: 75.00%,  total acc: 71.15%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 71.27%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 71.30%   [EVAL] batch:  135 | acc: 75.00%,  total acc: 71.32%   [EVAL] batch:  136 | acc: 68.75%,  total acc: 71.30%   [EVAL] batch:  137 | acc: 87.50%,  total acc: 71.42%   [EVAL] batch:  138 | acc: 68.75%,  total acc: 71.40%   [EVAL] batch:  139 | acc: 81.25%,  total acc: 71.47%   [EVAL] batch:  140 | acc: 93.75%,  total acc: 71.63%   [EVAL] batch:  141 | acc: 87.50%,  total acc: 71.74%   [EVAL] batch:  142 | acc: 75.00%,  total acc: 71.77%   [EVAL] batch:  143 | acc: 56.25%,  total acc: 71.66%   [EVAL] batch:  144 | acc: 31.25%,  total acc: 71.38%   [EVAL] batch:  145 | acc: 12.50%,  total acc: 70.98%   [EVAL] batch:  146 | acc: 50.00%,  total acc: 70.83%   [EVAL] batch:  147 | acc: 31.25%,  total acc: 70.57%   [EVAL] batch:  148 | acc: 25.00%,  total acc: 70.26%   [EVAL] batch:  149 | acc: 31.25%,  total acc: 70.00%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 69.54%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 69.12%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 68.67%   [EVAL] batch:  153 | acc: 0.00%,  total acc: 68.22%   [EVAL] batch:  154 | acc: 6.25%,  total acc: 67.82%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 67.39%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 67.20%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 67.29%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 67.41%   [EVAL] batch:  159 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 67.62%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 67.63%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 67.75%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 67.87%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 68.07%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 68.26%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 68.41%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 68.56%   [EVAL] batch:  168 | acc: 93.75%,  total acc: 68.71%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 68.53%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 68.24%   [EVAL] batch:  171 | acc: 31.25%,  total acc: 68.02%   [EVAL] batch:  172 | acc: 31.25%,  total acc: 67.81%   [EVAL] batch:  173 | acc: 68.75%,  total acc: 67.82%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 67.68%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 67.44%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 67.41%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 67.24%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 67.14%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 67.01%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 66.89%   [EVAL] batch:  181 | acc: 18.75%,  total acc: 66.62%   [EVAL] batch:  182 | acc: 31.25%,  total acc: 66.43%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 66.27%   [EVAL] batch:  184 | acc: 25.00%,  total acc: 66.05%   [EVAL] batch:  185 | acc: 31.25%,  total acc: 65.86%   [EVAL] batch:  186 | acc: 31.25%,  total acc: 65.68%   [EVAL] batch:  187 | acc: 18.75%,  total acc: 65.43%   [EVAL] batch:  188 | acc: 37.50%,  total acc: 65.28%   [EVAL] batch:  189 | acc: 56.25%,  total acc: 65.23%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 65.09%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 64.94%   [EVAL] batch:  192 | acc: 31.25%,  total acc: 64.77%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 64.63%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 64.62%   [EVAL] batch:  195 | acc: 68.75%,  total acc: 64.64%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 64.66%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 64.71%   [EVAL] batch:  198 | acc: 25.00%,  total acc: 64.51%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 64.69%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:  201 | acc: 68.75%,  total acc: 64.60%   [EVAL] batch:  202 | acc: 62.50%,  total acc: 64.59%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 64.55%   [EVAL] batch:  204 | acc: 50.00%,  total acc: 64.48%   [EVAL] batch:  205 | acc: 75.00%,  total acc: 64.53%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 64.67%   [EVAL] batch:  207 | acc: 93.75%,  total acc: 64.81%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 64.98%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 65.15%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 65.25%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 65.42%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 65.46%   [EVAL] batch:  213 | acc: 37.50%,  total acc: 65.33%   [EVAL] batch:  214 | acc: 37.50%,  total acc: 65.20%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 65.28%   [EVAL] batch:  216 | acc: 31.25%,  total acc: 65.12%   [EVAL] batch:  217 | acc: 50.00%,  total acc: 65.05%   [EVAL] batch:  218 | acc: 43.75%,  total acc: 64.95%   [EVAL] batch:  219 | acc: 12.50%,  total acc: 64.72%   [EVAL] batch:  220 | acc: 6.25%,  total acc: 64.45%   [EVAL] batch:  221 | acc: 0.00%,  total acc: 64.16%   [EVAL] batch:  222 | acc: 6.25%,  total acc: 63.90%   [EVAL] batch:  223 | acc: 0.00%,  total acc: 63.62%   [EVAL] batch:  224 | acc: 0.00%,  total acc: 63.33%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 63.33%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 63.41%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 63.43%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 63.51%   [EVAL] batch:  229 | acc: 56.25%,  total acc: 63.48%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 63.45%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 63.50%   [EVAL] batch:  232 | acc: 62.50%,  total acc: 63.49%   [EVAL] batch:  233 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 63.56%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 63.56%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 63.63%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 63.76%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 63.91%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 64.21%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 64.36%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 64.51%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 64.65%   [EVAL] batch:  244 | acc: 100.00%,  total acc: 64.80%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 64.89%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 65.01%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 65.12%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 65.26%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 65.40%   [EVAL] batch:  250 | acc: 81.25%,  total acc: 65.46%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 65.48%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 65.54%   [EVAL] batch:  253 | acc: 75.00%,  total acc: 65.58%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 65.64%   [EVAL] batch:  255 | acc: 75.00%,  total acc: 65.67%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 65.71%   [EVAL] batch:  257 | acc: 62.50%,  total acc: 65.70%   [EVAL] batch:  258 | acc: 56.25%,  total acc: 65.66%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 65.62%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 65.57%   [EVAL] batch:  261 | acc: 43.75%,  total acc: 65.48%   [EVAL] batch:  262 | acc: 87.50%,  total acc: 65.57%   [EVAL] batch:  263 | acc: 62.50%,  total acc: 65.55%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 65.59%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 65.62%   [EVAL] batch:  266 | acc: 87.50%,  total acc: 65.71%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:  268 | acc: 81.25%,  total acc: 65.85%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 65.72%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 65.59%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 65.51%   [EVAL] batch:  272 | acc: 56.25%,  total acc: 65.48%   [EVAL] batch:  273 | acc: 50.00%,  total acc: 65.42%   [EVAL] batch:  274 | acc: 37.50%,  total acc: 65.32%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 65.44%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 65.57%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 65.69%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 65.94%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 66.06%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 66.18%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 66.30%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 66.42%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 66.54%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 66.65%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 66.77%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 66.84%   [EVAL] batch:  288 | acc: 62.50%,  total acc: 66.83%   [EVAL] batch:  289 | acc: 56.25%,  total acc: 66.79%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 66.77%   [EVAL] batch:  291 | acc: 75.00%,  total acc: 66.80%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 66.85%   [EVAL] batch:  293 | acc: 50.00%,  total acc: 66.79%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 66.69%   [EVAL] batch:  295 | acc: 18.75%,  total acc: 66.53%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 66.46%   [EVAL] batch:  297 | acc: 50.00%,  total acc: 66.40%   [EVAL] batch:  298 | acc: 31.25%,  total acc: 66.28%   [EVAL] batch:  299 | acc: 31.25%,  total acc: 66.17%   [EVAL] batch:  300 | acc: 93.75%,  total acc: 66.26%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 66.35%   [EVAL] batch:  302 | acc: 93.75%,  total acc: 66.44%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 66.49%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 66.58%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 66.63%   [EVAL] batch:  306 | acc: 62.50%,  total acc: 66.61%   [EVAL] batch:  307 | acc: 43.75%,  total acc: 66.54%   [EVAL] batch:  308 | acc: 43.75%,  total acc: 66.46%   [EVAL] batch:  309 | acc: 31.25%,  total acc: 66.35%   [EVAL] batch:  310 | acc: 25.00%,  total acc: 66.22%   [EVAL] batch:  311 | acc: 43.75%,  total acc: 66.15%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 66.07%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 65.90%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 65.77%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 65.61%   [EVAL] batch:  316 | acc: 18.75%,  total acc: 65.46%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 65.31%   [EVAL] batch:  318 | acc: 18.75%,  total acc: 65.16%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 65.20%   [EVAL] batch:  320 | acc: 56.25%,  total acc: 65.17%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 65.20%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 65.27%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 65.30%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 65.33%   [EVAL] batch:  325 | acc: 31.25%,  total acc: 65.22%   [EVAL] batch:  326 | acc: 25.00%,  total acc: 65.10%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 64.96%   [EVAL] batch:  328 | acc: 37.50%,  total acc: 64.87%   [EVAL] batch:  329 | acc: 31.25%,  total acc: 64.77%   [EVAL] batch:  330 | acc: 31.25%,  total acc: 64.67%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 64.76%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 64.85%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 64.95%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 65.04%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 65.12%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  337 | acc: 75.00%,  total acc: 65.26%   [EVAL] batch:  338 | acc: 100.00%,  total acc: 65.36%   [EVAL] batch:  339 | acc: 93.75%,  total acc: 65.44%   [EVAL] batch:  340 | acc: 100.00%,  total acc: 65.54%   [EVAL] batch:  341 | acc: 93.75%,  total acc: 65.62%   [EVAL] batch:  342 | acc: 100.00%,  total acc: 65.73%   [EVAL] batch:  343 | acc: 87.50%,  total acc: 65.79%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 66.05%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 66.13%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 66.22%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  350 | acc: 37.50%,  total acc: 66.24%   [EVAL] batch:  351 | acc: 43.75%,  total acc: 66.18%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 66.08%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 65.96%   [EVAL] batch:  354 | acc: 25.00%,  total acc: 65.85%   [EVAL] batch:  355 | acc: 50.00%,  total acc: 65.80%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 65.88%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 65.94%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 66.08%   [EVAL] batch:  360 | acc: 62.50%,  total acc: 66.07%   [EVAL] batch:  361 | acc: 56.25%,  total acc: 66.04%   [EVAL] batch:  362 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:  363 | acc: 43.75%,  total acc: 65.99%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 65.92%   [EVAL] batch:  365 | acc: 43.75%,  total acc: 65.86%   [EVAL] batch:  366 | acc: 50.00%,  total acc: 65.82%   [EVAL] batch:  367 | acc: 43.75%,  total acc: 65.76%   [EVAL] batch:  368 | acc: 31.25%,  total acc: 65.67%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 65.79%   [EVAL] batch:  371 | acc: 56.25%,  total acc: 65.76%   [EVAL] batch:  372 | acc: 75.00%,  total acc: 65.78%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 65.84%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 65.88%   [EVAL] batch:  375 | acc: 68.75%,  total acc: 65.89%   [EVAL] batch:  376 | acc: 62.50%,  total acc: 65.88%   [EVAL] batch:  377 | acc: 62.50%,  total acc: 65.87%   [EVAL] batch:  378 | acc: 31.25%,  total acc: 65.78%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 65.77%   [EVAL] batch:  380 | acc: 56.25%,  total acc: 65.75%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 65.80%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 65.88%   [EVAL] batch:  383 | acc: 87.50%,  total acc: 65.93%   [EVAL] batch:  384 | acc: 81.25%,  total acc: 65.97%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 66.03%   [EVAL] batch:  386 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:  387 | acc: 75.00%,  total acc: 66.12%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 66.10%   [EVAL] batch:  389 | acc: 75.00%,  total acc: 66.12%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 66.16%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 66.21%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 66.22%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 66.24%   [EVAL] batch:  394 | acc: 62.50%,  total acc: 66.23%   [EVAL] batch:  395 | acc: 75.00%,  total acc: 66.26%   [EVAL] batch:  396 | acc: 81.25%,  total acc: 66.29%   [EVAL] batch:  397 | acc: 81.25%,  total acc: 66.33%   [EVAL] batch:  398 | acc: 68.75%,  total acc: 66.34%   [EVAL] batch:  399 | acc: 75.00%,  total acc: 66.36%   [EVAL] batch:  400 | acc: 0.00%,  total acc: 66.19%   [EVAL] batch:  401 | acc: 12.50%,  total acc: 66.06%   [EVAL] batch:  402 | acc: 12.50%,  total acc: 65.93%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 65.76%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 65.62%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 65.46%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 65.42%   [EVAL] batch:  407 | acc: 43.75%,  total acc: 65.36%   [EVAL] batch:  408 | acc: 50.00%,  total acc: 65.33%   [EVAL] batch:  409 | acc: 43.75%,  total acc: 65.27%   [EVAL] batch:  410 | acc: 56.25%,  total acc: 65.25%   [EVAL] batch:  411 | acc: 50.00%,  total acc: 65.22%   [EVAL] batch:  412 | acc: 62.50%,  total acc: 65.21%   [EVAL] batch:  413 | acc: 68.75%,  total acc: 65.22%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 65.26%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 65.31%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 65.35%   [EVAL] batch:  417 | acc: 87.50%,  total acc: 65.40%   [EVAL] batch:  418 | acc: 81.25%,  total acc: 65.44%   [EVAL] batch:  419 | acc: 31.25%,  total acc: 65.36%   [EVAL] batch:  420 | acc: 31.25%,  total acc: 65.28%   [EVAL] batch:  421 | acc: 25.00%,  total acc: 65.18%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 65.16%   [EVAL] batch:  423 | acc: 25.00%,  total acc: 65.06%   [EVAL] batch:  424 | acc: 37.50%,  total acc: 65.00%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 65.08%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 65.16%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 65.33%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 65.41%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 65.49%   [EVAL] batch:  431 | acc: 56.25%,  total acc: 65.47%   [EVAL] batch:  432 | acc: 56.25%,  total acc: 65.44%   [EVAL] batch:  433 | acc: 56.25%,  total acc: 65.42%   [EVAL] batch:  434 | acc: 81.25%,  total acc: 65.46%   [EVAL] batch:  435 | acc: 68.75%,  total acc: 65.47%   [EVAL] batch:  436 | acc: 62.50%,  total acc: 65.46%   [EVAL] batch:  437 | acc: 37.50%,  total acc: 65.40%   
cur_acc:  ['0.9494', '0.8323', '0.6468', '0.7312', '0.7302', '0.6171', '0.6250']
his_acc:  ['0.9494', '0.8805', '0.7889', '0.7390', '0.6993', '0.6713', '0.6540']
Clustering into  39  clusters
Clusters:  [ 5  0 26 16  1  1 37  6  0 11 16  5  5  0 35 10 31 38 18  0  1 14 13 36
  3 21 28  9  8  0  2 25  7  4  6 20  9 10 33  8  2  8 29 14 23 19 22 24
  4 17 32 19 24 34 27 19 17  8 11  5 24  6  6  1 36 11  5  1 19 19  4 12
 30  7  3  0 13  5 15  4]
Losses:  9.696904182434082 2.8137354850769043 0.651185154914856
CurrentTrain: epoch  0, batch     0 | loss: 9.6969042Losses:  11.752001762390137 4.028021335601807 0.6855493783950806
CurrentTrain: epoch  0, batch     1 | loss: 11.7520018Losses:  9.969636917114258 3.3001651763916016 0.6711559295654297
CurrentTrain: epoch  0, batch     2 | loss: 9.9696369Losses:  5.655322551727295 -0.0 0.11064288020133972
CurrentTrain: epoch  0, batch     3 | loss: 5.6553226Losses:  10.583773612976074 4.72898006439209 0.6541434526443481
CurrentTrain: epoch  1, batch     0 | loss: 10.5837736Losses:  8.774291038513184 3.1044068336486816 0.5865098834037781
CurrentTrain: epoch  1, batch     1 | loss: 8.7742910Losses:  10.852813720703125 4.277992248535156 0.712245762348175
CurrentTrain: epoch  1, batch     2 | loss: 10.8528137Losses:  5.936385154724121 -0.0 0.1003773882985115
CurrentTrain: epoch  1, batch     3 | loss: 5.9363852Losses:  7.157559394836426 3.0583932399749756 0.5667307376861572
CurrentTrain: epoch  2, batch     0 | loss: 7.1575594Losses:  9.367032051086426 3.487682819366455 0.6455910801887512
CurrentTrain: epoch  2, batch     1 | loss: 9.3670321Losses:  11.155131340026855 4.758418083190918 0.6543735265731812
CurrentTrain: epoch  2, batch     2 | loss: 11.1551313Losses:  2.364987850189209 -0.0 0.1280917525291443
CurrentTrain: epoch  2, batch     3 | loss: 2.3649879Losses:  8.790319442749023 3.654062271118164 0.6432633996009827
CurrentTrain: epoch  3, batch     0 | loss: 8.7903194Losses:  8.362136840820312 3.6117513179779053 0.59090256690979
CurrentTrain: epoch  3, batch     1 | loss: 8.3621368Losses:  11.786574363708496 6.294032573699951 0.5724151134490967
CurrentTrain: epoch  3, batch     2 | loss: 11.7865744Losses:  2.6124753952026367 -0.0 0.1163882166147232
CurrentTrain: epoch  3, batch     3 | loss: 2.6124754Losses:  7.902565002441406 2.7944865226745605 0.6641905903816223
CurrentTrain: epoch  4, batch     0 | loss: 7.9025650Losses:  6.849331378936768 2.0323781967163086 0.7171902060508728
CurrentTrain: epoch  4, batch     1 | loss: 6.8493314Losses:  6.819424152374268 2.6332573890686035 0.6446410417556763
CurrentTrain: epoch  4, batch     2 | loss: 6.8194242Losses:  4.812457084655762 -0.0 0.16033300757408142
CurrentTrain: epoch  4, batch     3 | loss: 4.8124571Losses:  7.530299663543701 2.530724048614502 0.7306020855903625
CurrentTrain: epoch  5, batch     0 | loss: 7.5302997Losses:  7.2435197830200195 3.4977917671203613 0.635101854801178
CurrentTrain: epoch  5, batch     1 | loss: 7.2435198Losses:  7.610743045806885 2.6885223388671875 0.6396389007568359
CurrentTrain: epoch  5, batch     2 | loss: 7.6107430Losses:  4.976612091064453 -0.0 0.11337860673666
CurrentTrain: epoch  5, batch     3 | loss: 4.9766121Losses:  6.7698516845703125 3.3368029594421387 0.5696346759796143
CurrentTrain: epoch  6, batch     0 | loss: 6.7698517Losses:  6.845215797424316 2.4290504455566406 0.7168487906455994
CurrentTrain: epoch  6, batch     1 | loss: 6.8452158Losses:  11.214007377624512 6.519869804382324 0.6420132517814636
CurrentTrain: epoch  6, batch     2 | loss: 11.2140074Losses:  7.8744401931762695 -0.0 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 7.8744402Losses:  6.635234832763672 2.847633123397827 0.6498709917068481
CurrentTrain: epoch  7, batch     0 | loss: 6.6352348Losses:  6.882976531982422 2.8586249351501465 0.5810248851776123
CurrentTrain: epoch  7, batch     1 | loss: 6.8829765Losses:  7.892168045043945 3.4108853340148926 0.7163697481155396
CurrentTrain: epoch  7, batch     2 | loss: 7.8921680Losses:  4.642069339752197 -0.0 0.1082763746380806
CurrentTrain: epoch  7, batch     3 | loss: 4.6420693Losses:  8.333431243896484 3.8443503379821777 0.5839013457298279
CurrentTrain: epoch  8, batch     0 | loss: 8.3334312Losses:  7.256871223449707 3.4185194969177246 0.6521071791648865
CurrentTrain: epoch  8, batch     1 | loss: 7.2568712Losses:  6.4340105056762695 3.1909871101379395 0.5625531673431396
CurrentTrain: epoch  8, batch     2 | loss: 6.4340105Losses:  6.789966583251953 -0.0 0.2603655755519867
CurrentTrain: epoch  8, batch     3 | loss: 6.7899666Losses:  8.24008846282959 4.393283843994141 0.5400275588035583
CurrentTrain: epoch  9, batch     0 | loss: 8.2400885Losses:  7.728781700134277 3.8891873359680176 0.6385945081710815
CurrentTrain: epoch  9, batch     1 | loss: 7.7287817Losses:  7.841522693634033 4.263486862182617 0.5473610758781433
CurrentTrain: epoch  9, batch     2 | loss: 7.8415227Losses:  3.1565961837768555 -0.0 0.15955930948257446
CurrentTrain: epoch  9, batch     3 | loss: 3.1565962
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the book "sapiens: a brief history of humankind" explores the evolution of human societies and cultures.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: human societies  

Relation: main subject  
Context: the documentary "our planet" showcases the beauty of nature and the impact of climate change on wildlife.  
Head Entity: our planet  
Tail Entity: climate change  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: he represented romania at euro 2000 , his first major tournament where they beat england 3–2 , and drew 1–1 against germany .
Head Entity: euro 2000
Tail Entity: germany
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: The Brazilian national team showcased their skills at the 2014 FIFA World Cup, where they faced off against Germany in the semi-finals.  
Head Entity: 2014 FIFA World Cup  
Tail Entity: Germany  

Relation: participating team  
Context: During the 2021 Tokyo Olympics, the United States women's soccer team competed fiercely, ultimately playing against Canada in the semi-finals.  
Head Entity: 2021 Tokyo Olympics  
Tail Entity: Canada  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a high-resolution camera that includes a wide-angle lens, allowing users to capture stunning landscapes.  
Head Entity: Galaxy S21  
Tail Entity: wide-angle lens  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: gyllenhaal was nominated for an academy award for best supporting actress for her performance .
Head Entity: gyllenhaal
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: the film was nominated for several prestigious awards, including the golden globe for best drama.  
Head Entity: the film  
Tail Entity: golden globe for best drama  

Relation: nominated for  
Context: she was nominated for the emmy award for her outstanding performance in the television series.  
Head Entity: she  
Tail Entity: emmy award for her outstanding performance in the television series  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: bell gothic was replaced by at&t ; with matthew carter 's typeface bell centennial in 1978 , the one hundredth anniversary of at&t ; 's founding .
Head Entity: bell centennial
Tail Entity: typeface
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: the smartphone is a type of mobile phone that combines cellular and mobile computing functions into one device.  
Head Entity: smartphone  
Tail Entity: mobile phone  

Relation: instance of  
Context: the golden retriever is a popular breed known for its friendly and tolerant attitude.  
Head Entity: golden retriever  
Tail Entity: breed  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: brand peak is a sharp snow - covered peak located east - southeast of the eternity range and northwest of mount duemler , in palmer land .
Head Entity: mount duemler
Tail Entity: eternity range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the rocky mountains extend from Canada through the United States and into Mexico, forming a major mountain range in North America.  
Head Entity: rocky mountains  
Tail Entity: North America  

Relation: mountain range  
Context: the appalachian mountains are one of the oldest mountain ranges in the world, stretching from Canada down to Alabama.  
Head Entity: appalachian mountains  
Tail Entity: Alabama  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: future films include " mcqueen " , ian bonhote ’s documentary about the fashion designer alexander mcqueen , brad anderson ’s thriller " beirut " , and mark pellington ’s drama " nostalgia " .
Head Entity: nostalgia
Tail Entity: mark pellington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the upcoming movie "the great adventure" is penned by renowned screenwriter jessica taylor, who has previously won multiple awards for her work in the industry.  
Head Entity: the great adventure  
Tail Entity: jessica taylor  

Relation: screenwriter  
Context: in the latest film "shadows of the past," the script was crafted by the talented screenwriter robert lang, known for his gripping storytelling and character development.  
Head Entity: shadows of the past  
Tail Entity: robert lang  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: " aamulehti " was founded in 1881 to “ improve the position of the finnish people and the finnish language ” during russia ’s rule over finland .
Head Entity: aamulehti
Tail Entity: finnish language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The novel "Cien años de soledad" is written in Spanish and has become a classic of Latin American literature.  
Head Entity: Cien años de soledad  
Tail Entity: Spanish  

Relation: language of work or name  
Context: The popular animated series "Naruto" is primarily produced in Japanese, reflecting the culture and language of its origin.  
Head Entity: Naruto  
Tail Entity: Japanese  
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: it hosts both freight traffic and metra 's union pacific / northwest line commuter rail service .
Head Entity: union pacific / northwest line
Tail Entity: metra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: The city council has awarded the contract to GreenTech Solutions, which will manage the waste disposal services for the next five years.  
Head Entity: GreenTech Solutions  
Tail Entity: city council  

Relation: operator  
Context: The new software platform is being developed by Tech Innovations Inc., which specializes in providing cloud-based solutions for businesses.  
Head Entity: Tech Innovations Inc.  
Tail Entity: software platform  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: udasi has been an early sect based on the teachings of sri chand ( 1494–1643 ) , the son of guru nanak , the founder and the first guru of sikhism .
Head Entity: guru nanak
Tail Entity: sikhism
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the baha'i faith is a religion founded by baha'u'llah in the 19th century, emphasizing the spiritual unity of all humankind.  
Head Entity: baha'u'llah  
Tail Entity: baha'i faith  

Relation: religion  
Context: the ancient greeks practiced a polytheistic religion that included a pantheon of gods and goddesses, such as zeus and athena.  
Head Entity: ancient greeks  
Tail Entity: polytheistic religion  
Losses:  3.360900402069092 0.24860692024230957 1.0306638479232788
MemoryTrain:  epoch  0, batch     0 | loss: 3.3609004Losses:  3.841012954711914 0.5363838076591492 0.8341852426528931
MemoryTrain:  epoch  0, batch     1 | loss: 3.8410130Losses:  2.8683054447174072 0.2439669966697693 1.055159091949463
MemoryTrain:  epoch  0, batch     2 | loss: 2.8683054Losses:  3.2612013816833496 0.5134918689727783 0.9470872282981873
MemoryTrain:  epoch  0, batch     3 | loss: 3.2612014Losses:  3.285634756088257 0.3165263831615448 0.9419751167297363
MemoryTrain:  epoch  0, batch     4 | loss: 3.2856348Losses:  4.002462863922119 -0.0 0.9278138875961304
MemoryTrain:  epoch  0, batch     5 | loss: 4.0024629Losses:  3.8388137817382812 0.25896012783050537 1.0478949546813965
MemoryTrain:  epoch  0, batch     6 | loss: 3.8388138Losses:  2.914586305618286 0.24842342734336853 0.9261325597763062
MemoryTrain:  epoch  0, batch     7 | loss: 2.9145863Losses:  4.557304382324219 0.24888920783996582 1.0827009677886963
MemoryTrain:  epoch  0, batch     8 | loss: 4.5573044Losses:  3.4296858310699463 0.5259412527084351 0.9284517168998718
MemoryTrain:  epoch  0, batch     9 | loss: 3.4296858Losses:  4.211514949798584 0.35571998357772827 0.9710835218429565
MemoryTrain:  epoch  0, batch    10 | loss: 4.2115149Losses:  3.987513303756714 0.5728545784950256 1.012223720550537
MemoryTrain:  epoch  0, batch    11 | loss: 3.9875133Losses:  4.468046188354492 0.4989052414894104 0.8326594233512878
MemoryTrain:  epoch  0, batch    12 | loss: 4.4680462Losses:  4.554627418518066 0.26825687289237976 0.9836658239364624
MemoryTrain:  epoch  0, batch    13 | loss: 4.5546274Losses:  2.9360945224761963 -0.0 0.9637541174888611
MemoryTrain:  epoch  0, batch    14 | loss: 2.9360945Losses:  2.7607343196868896 -0.0 0.966788113117218
MemoryTrain:  epoch  1, batch     0 | loss: 2.7607343Losses:  3.16191029548645 0.49477851390838623 0.8635842204093933
MemoryTrain:  epoch  1, batch     1 | loss: 3.1619103Losses:  3.0282959938049316 0.262077271938324 0.9187259674072266
MemoryTrain:  epoch  1, batch     2 | loss: 3.0282960Losses:  3.1857409477233887 0.4882752001285553 0.9675900936126709
MemoryTrain:  epoch  1, batch     3 | loss: 3.1857409Losses:  2.5255608558654785 0.25272324681282043 0.9793612957000732
MemoryTrain:  epoch  1, batch     4 | loss: 2.5255609Losses:  3.229526996612549 0.5282314419746399 0.9110811948776245
MemoryTrain:  epoch  1, batch     5 | loss: 3.2295270Losses:  3.8148961067199707 0.4905865788459778 0.8024749159812927
MemoryTrain:  epoch  1, batch     6 | loss: 3.8148961Losses:  3.5249183177948 0.25540590286254883 0.9618335366249084
MemoryTrain:  epoch  1, batch     7 | loss: 3.5249183Losses:  3.8227956295013428 0.2626532316207886 0.9351050853729248
MemoryTrain:  epoch  1, batch     8 | loss: 3.8227956Losses:  3.41414213180542 0.2556503415107727 1.0103040933609009
MemoryTrain:  epoch  1, batch     9 | loss: 3.4141421Losses:  3.5898942947387695 0.8351117372512817 1.007812738418579
MemoryTrain:  epoch  1, batch    10 | loss: 3.5898943Losses:  3.0054047107696533 0.7558088302612305 0.8796374797821045
MemoryTrain:  epoch  1, batch    11 | loss: 3.0054047Losses:  3.9492132663726807 -0.0 1.0377867221832275
MemoryTrain:  epoch  1, batch    12 | loss: 3.9492133Losses:  3.7578577995300293 0.31668686866760254 0.858307957649231
MemoryTrain:  epoch  1, batch    13 | loss: 3.7578578Losses:  3.4789817333221436 0.27606430649757385 0.9008263945579529
MemoryTrain:  epoch  1, batch    14 | loss: 3.4789817Losses:  2.9718635082244873 0.49628710746765137 0.8577263951301575
MemoryTrain:  epoch  2, batch     0 | loss: 2.9718635Losses:  3.515389919281006 0.27019286155700684 1.0156277418136597
MemoryTrain:  epoch  2, batch     1 | loss: 3.5153899Losses:  2.2633843421936035 -0.0 0.9629843235015869
MemoryTrain:  epoch  2, batch     2 | loss: 2.2633843Losses:  3.1983087062835693 -0.0 0.9476053714752197
MemoryTrain:  epoch  2, batch     3 | loss: 3.1983087Losses:  2.2439422607421875 -0.0 0.9033011198043823
MemoryTrain:  epoch  2, batch     4 | loss: 2.2439423Losses:  4.175241470336914 0.4618177115917206 0.8929290175437927
MemoryTrain:  epoch  2, batch     5 | loss: 4.1752415Losses:  2.7697229385375977 -0.0 1.0981730222702026
MemoryTrain:  epoch  2, batch     6 | loss: 2.7697229Losses:  3.0252814292907715 -0.0 0.9669795632362366
MemoryTrain:  epoch  2, batch     7 | loss: 3.0252814Losses:  3.436476230621338 0.2523081302642822 1.0864275693893433
MemoryTrain:  epoch  2, batch     8 | loss: 3.4364762Losses:  2.6902215480804443 -0.0 0.9767594933509827
MemoryTrain:  epoch  2, batch     9 | loss: 2.6902215Losses:  2.771533250808716 0.2523002624511719 0.9432340860366821
MemoryTrain:  epoch  2, batch    10 | loss: 2.7715333Losses:  3.251649856567383 -0.0 0.9547523260116577
MemoryTrain:  epoch  2, batch    11 | loss: 3.2516499Losses:  3.850876808166504 0.2993549704551697 0.8519067168235779
MemoryTrain:  epoch  2, batch    12 | loss: 3.8508768Losses:  3.384225845336914 0.5108035802841187 1.0265260934829712
MemoryTrain:  epoch  2, batch    13 | loss: 3.3842258Losses:  2.76043963432312 0.5203523635864258 0.9365279674530029
MemoryTrain:  epoch  2, batch    14 | loss: 2.7604396Losses:  3.182544231414795 -0.0 0.8460665941238403
MemoryTrain:  epoch  3, batch     0 | loss: 3.1825442Losses:  2.536264419555664 0.263999879360199 0.8526998162269592
MemoryTrain:  epoch  3, batch     1 | loss: 2.5362644Losses:  2.752235174179077 0.24910183250904083 0.9236633777618408
MemoryTrain:  epoch  3, batch     2 | loss: 2.7522352Losses:  2.930067539215088 -0.0 1.0149897336959839
MemoryTrain:  epoch  3, batch     3 | loss: 2.9300675Losses:  3.1179394721984863 0.5140460133552551 0.9638828039169312
MemoryTrain:  epoch  3, batch     4 | loss: 3.1179395Losses:  2.982128620147705 0.23907530307769775 1.0590206384658813
MemoryTrain:  epoch  3, batch     5 | loss: 2.9821286Losses:  2.293020725250244 -0.0 1.0201002359390259
MemoryTrain:  epoch  3, batch     6 | loss: 2.2930207Losses:  2.685472249984741 -0.0 0.9129467010498047
MemoryTrain:  epoch  3, batch     7 | loss: 2.6854722Losses:  2.883364200592041 -0.0 0.9609209299087524
MemoryTrain:  epoch  3, batch     8 | loss: 2.8833642Losses:  3.1156697273254395 0.2635982632637024 0.8990687131881714
MemoryTrain:  epoch  3, batch     9 | loss: 3.1156697Losses:  2.643695831298828 -0.0 0.9668880701065063
MemoryTrain:  epoch  3, batch    10 | loss: 2.6436958Losses:  2.573333263397217 0.2628980875015259 0.9444946050643921
MemoryTrain:  epoch  3, batch    11 | loss: 2.5733333Losses:  3.1657862663269043 -0.0 0.9469472169876099
MemoryTrain:  epoch  3, batch    12 | loss: 3.1657863Losses:  3.099588394165039 -0.0 1.046761393547058
MemoryTrain:  epoch  3, batch    13 | loss: 3.0995884Losses:  2.361421823501587 -0.0 0.8396545052528381
MemoryTrain:  epoch  3, batch    14 | loss: 2.3614218Losses:  2.8612914085388184 -0.0 1.0148152112960815
MemoryTrain:  epoch  4, batch     0 | loss: 2.8612914Losses:  4.426908016204834 1.1263058185577393 0.8515750169754028
MemoryTrain:  epoch  4, batch     1 | loss: 4.4269080Losses:  2.7711081504821777 -0.0 1.0166454315185547
MemoryTrain:  epoch  4, batch     2 | loss: 2.7711082Losses:  2.776719093322754 0.24562981724739075 0.9672048091888428
MemoryTrain:  epoch  4, batch     3 | loss: 2.7767191Losses:  2.143244504928589 -0.0 0.8491979241371155
MemoryTrain:  epoch  4, batch     4 | loss: 2.1432445Losses:  3.1713593006134033 0.24389135837554932 0.9648770689964294
MemoryTrain:  epoch  4, batch     5 | loss: 3.1713593Losses:  2.4302968978881836 -0.0 1.020276427268982
MemoryTrain:  epoch  4, batch     6 | loss: 2.4302969Losses:  2.2768754959106445 -0.0 0.9156898260116577
MemoryTrain:  epoch  4, batch     7 | loss: 2.2768755Losses:  2.4811952114105225 0.27007874846458435 0.9441961646080017
MemoryTrain:  epoch  4, batch     8 | loss: 2.4811952Losses:  2.481243848800659 -0.0 0.9215965270996094
MemoryTrain:  epoch  4, batch     9 | loss: 2.4812438Losses:  2.896630048751831 0.24935045838356018 1.0496400594711304
MemoryTrain:  epoch  4, batch    10 | loss: 2.8966300Losses:  2.89638090133667 0.5211328268051147 0.8793689012527466
MemoryTrain:  epoch  4, batch    11 | loss: 2.8963809Losses:  2.678654193878174 -0.0 1.0085901021957397
MemoryTrain:  epoch  4, batch    12 | loss: 2.6786542Losses:  2.5454916954040527 -0.0 0.9827829599380493
MemoryTrain:  epoch  4, batch    13 | loss: 2.5454917Losses:  2.9651548862457275 0.2730969786643982 0.9131028652191162
MemoryTrain:  epoch  4, batch    14 | loss: 2.9651549Losses:  2.4172215461730957 -0.0 1.0345795154571533
MemoryTrain:  epoch  5, batch     0 | loss: 2.4172215Losses:  2.7189478874206543 0.5308959484100342 0.8456388115882874
MemoryTrain:  epoch  5, batch     1 | loss: 2.7189479Losses:  2.412182331085205 -0.0 0.9751209020614624
MemoryTrain:  epoch  5, batch     2 | loss: 2.4121823Losses:  3.215224027633667 -0.0 1.0868303775787354
MemoryTrain:  epoch  5, batch     3 | loss: 3.2152240Losses:  2.13069224357605 -0.0 0.8493734002113342
MemoryTrain:  epoch  5, batch     4 | loss: 2.1306922Losses:  2.348538398742676 -0.0 0.8395461440086365
MemoryTrain:  epoch  5, batch     5 | loss: 2.3485384Losses:  2.4559733867645264 -0.0 1.0125489234924316
MemoryTrain:  epoch  5, batch     6 | loss: 2.4559734Losses:  3.131443977355957 0.4801403880119324 0.8412343263626099
MemoryTrain:  epoch  5, batch     7 | loss: 3.1314440Losses:  2.700561046600342 -0.0 1.0092036724090576
MemoryTrain:  epoch  5, batch     8 | loss: 2.7005610Losses:  3.0257673263549805 -0.0 0.9660782814025879
MemoryTrain:  epoch  5, batch     9 | loss: 3.0257673Losses:  3.1658411026000977 0.5711143612861633 0.8439792394638062
MemoryTrain:  epoch  5, batch    10 | loss: 3.1658411Losses:  3.158191204071045 0.5638414025306702 0.8719355463981628
MemoryTrain:  epoch  5, batch    11 | loss: 3.1581912Losses:  3.1265740394592285 0.2432558536529541 1.0638093948364258
MemoryTrain:  epoch  5, batch    12 | loss: 3.1265740Losses:  2.850454330444336 0.7845569849014282 0.8057609796524048
MemoryTrain:  epoch  5, batch    13 | loss: 2.8504543Losses:  2.1576390266418457 -0.0 0.9186099767684937
MemoryTrain:  epoch  5, batch    14 | loss: 2.1576390Losses:  2.803408622741699 0.5310794115066528 0.9677538275718689
MemoryTrain:  epoch  6, batch     0 | loss: 2.8034086Losses:  2.500138759613037 0.28107911348342896 0.8562373518943787
MemoryTrain:  epoch  6, batch     1 | loss: 2.5001388Losses:  2.4533567428588867 -0.0 1.0345096588134766
MemoryTrain:  epoch  6, batch     2 | loss: 2.4533567Losses:  2.537781000137329 0.23324942588806152 1.0731916427612305
MemoryTrain:  epoch  6, batch     3 | loss: 2.5377810Losses:  2.9424643516540527 0.7876901626586914 0.7944604158401489
MemoryTrain:  epoch  6, batch     4 | loss: 2.9424644Losses:  2.6604866981506348 -0.0 0.951632022857666
MemoryTrain:  epoch  6, batch     5 | loss: 2.6604867Losses:  2.5387120246887207 -0.0 0.9657270312309265
MemoryTrain:  epoch  6, batch     6 | loss: 2.5387120Losses:  2.986605644226074 0.27015990018844604 1.0386557579040527
MemoryTrain:  epoch  6, batch     7 | loss: 2.9866056Losses:  2.333498001098633 -0.0 0.9007261991500854
MemoryTrain:  epoch  6, batch     8 | loss: 2.3334980Losses:  2.772507667541504 -0.0 1.012690544128418
MemoryTrain:  epoch  6, batch     9 | loss: 2.7725077Losses:  2.870753049850464 0.25589919090270996 0.8623499870300293
MemoryTrain:  epoch  6, batch    10 | loss: 2.8707530Losses:  2.2611517906188965 -0.0 1.0098328590393066
MemoryTrain:  epoch  6, batch    11 | loss: 2.2611518Losses:  2.68346905708313 0.4877779483795166 0.8437413573265076
MemoryTrain:  epoch  6, batch    12 | loss: 2.6834691Losses:  2.7110490798950195 0.26813021302223206 0.868279218673706
MemoryTrain:  epoch  6, batch    13 | loss: 2.7110491Losses:  2.7802772521972656 0.516464352607727 0.8532706499099731
MemoryTrain:  epoch  6, batch    14 | loss: 2.7802773Losses:  2.5070555210113525 0.2561807632446289 0.964174211025238
MemoryTrain:  epoch  7, batch     0 | loss: 2.5070555Losses:  2.7477974891662598 -0.0 1.071186900138855
MemoryTrain:  epoch  7, batch     1 | loss: 2.7477975Losses:  2.5266008377075195 0.24047306180000305 1.0163122415542603
MemoryTrain:  epoch  7, batch     2 | loss: 2.5266008Losses:  2.490100860595703 0.23591560125350952 0.7746739387512207
MemoryTrain:  epoch  7, batch     3 | loss: 2.4901009Losses:  2.724514961242676 0.22556829452514648 0.9070656299591064
MemoryTrain:  epoch  7, batch     4 | loss: 2.7245150Losses:  2.44962215423584 0.22766663134098053 0.8501365184783936
MemoryTrain:  epoch  7, batch     5 | loss: 2.4496222Losses:  3.332392930984497 0.7352595329284668 0.9608156085014343
MemoryTrain:  epoch  7, batch     6 | loss: 3.3323929Losses:  2.4031424522399902 0.2584354877471924 0.9162809252738953
MemoryTrain:  epoch  7, batch     7 | loss: 2.4031425Losses:  2.7544684410095215 0.24877482652664185 1.0302785634994507
MemoryTrain:  epoch  7, batch     8 | loss: 2.7544684Losses:  2.8791322708129883 0.24462518095970154 1.0054781436920166
MemoryTrain:  epoch  7, batch     9 | loss: 2.8791323Losses:  2.350275993347168 0.2191280722618103 0.777409017086029
MemoryTrain:  epoch  7, batch    10 | loss: 2.3502760Losses:  2.6261730194091797 0.24778211116790771 0.9238119125366211
MemoryTrain:  epoch  7, batch    11 | loss: 2.6261730Losses:  2.512315273284912 -0.0 1.0055615901947021
MemoryTrain:  epoch  7, batch    12 | loss: 2.5123153Losses:  2.525102138519287 -0.0 1.0033479928970337
MemoryTrain:  epoch  7, batch    13 | loss: 2.5251021Losses:  2.474475622177124 0.2566350996494293 0.9660874009132385
MemoryTrain:  epoch  7, batch    14 | loss: 2.4744756Losses:  2.942737102508545 0.23654301464557648 0.9670789241790771
MemoryTrain:  epoch  8, batch     0 | loss: 2.9427371Losses:  3.021381378173828 0.5014511346817017 0.9142823219299316
MemoryTrain:  epoch  8, batch     1 | loss: 3.0213814Losses:  2.977419376373291 0.7557715773582458 0.8997899293899536
MemoryTrain:  epoch  8, batch     2 | loss: 2.9774194Losses:  2.099339485168457 -0.0 0.788450300693512
MemoryTrain:  epoch  8, batch     3 | loss: 2.0993395Losses:  2.7254467010498047 0.23281121253967285 0.9552549719810486
MemoryTrain:  epoch  8, batch     4 | loss: 2.7254467Losses:  2.520848274230957 0.2553107738494873 1.0139565467834473
MemoryTrain:  epoch  8, batch     5 | loss: 2.5208483Losses:  2.638791561126709 0.49694347381591797 0.8469923734664917
MemoryTrain:  epoch  8, batch     6 | loss: 2.6387916Losses:  2.3952295780181885 0.25363075733184814 0.912855327129364
MemoryTrain:  epoch  8, batch     7 | loss: 2.3952296Losses:  2.6547605991363525 0.4800718128681183 0.9668968319892883
MemoryTrain:  epoch  8, batch     8 | loss: 2.6547606Losses:  2.7589094638824463 -0.0 1.0577994585037231
MemoryTrain:  epoch  8, batch     9 | loss: 2.7589095Losses:  2.6667633056640625 0.5137743949890137 0.9110136032104492
MemoryTrain:  epoch  8, batch    10 | loss: 2.6667633Losses:  2.6532552242279053 0.49087920784950256 0.8692900538444519
MemoryTrain:  epoch  8, batch    11 | loss: 2.6532552Losses:  2.62337064743042 -0.0 0.9622865915298462
MemoryTrain:  epoch  8, batch    12 | loss: 2.6233706Losses:  2.727360963821411 0.2759079933166504 0.9183441996574402
MemoryTrain:  epoch  8, batch    13 | loss: 2.7273610Losses:  2.821791172027588 0.7482587099075317 0.8651204705238342
MemoryTrain:  epoch  8, batch    14 | loss: 2.8217912Losses:  2.368680953979492 0.2326326072216034 0.8559057116508484
MemoryTrain:  epoch  9, batch     0 | loss: 2.3686810Losses:  2.108362913131714 -0.0 0.8909820318222046
MemoryTrain:  epoch  9, batch     1 | loss: 2.1083629Losses:  2.697126626968384 0.23254424333572388 0.9702188372612
MemoryTrain:  epoch  9, batch     2 | loss: 2.6971266Losses:  2.6253786087036133 0.4951082468032837 0.79344642162323
MemoryTrain:  epoch  9, batch     3 | loss: 2.6253786Losses:  3.0079245567321777 0.7646137475967407 0.9139641523361206
MemoryTrain:  epoch  9, batch     4 | loss: 3.0079246Losses:  2.6654961109161377 0.475921630859375 0.891692578792572
MemoryTrain:  epoch  9, batch     5 | loss: 2.6654961Losses:  2.8201255798339844 0.5181386470794678 0.9886840581893921
MemoryTrain:  epoch  9, batch     6 | loss: 2.8201256Losses:  2.377647638320923 0.23991644382476807 0.900482177734375
MemoryTrain:  epoch  9, batch     7 | loss: 2.3776476Losses:  2.509744644165039 0.22438403964042664 0.9532517790794373
MemoryTrain:  epoch  9, batch     8 | loss: 2.5097446Losses:  2.3326239585876465 -0.0 1.0589321851730347
MemoryTrain:  epoch  9, batch     9 | loss: 2.3326240Losses:  2.351915121078491 -0.0 0.9511993527412415
MemoryTrain:  epoch  9, batch    10 | loss: 2.3519151Losses:  2.163545846939087 -0.0 0.9238535761833191
MemoryTrain:  epoch  9, batch    11 | loss: 2.1635458Losses:  2.816603183746338 0.2656906247138977 0.9620264768600464
MemoryTrain:  epoch  9, batch    12 | loss: 2.8166032Losses:  2.9890403747558594 0.25245192646980286 1.0229276418685913
MemoryTrain:  epoch  9, batch    13 | loss: 2.9890404Losses:  2.345278024673462 -0.0 0.9683361649513245
MemoryTrain:  epoch  9, batch    14 | loss: 2.3452780
[EVAL] batch:    0 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    1 | acc: 62.50%,  total acc: 62.50%   [EVAL] batch:    2 | acc: 50.00%,  total acc: 58.33%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 60.94%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 63.75%   [EVAL] batch:    5 | acc: 62.50%,  total acc: 63.54%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 63.39%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 65.62%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 66.88%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 69.32%   [EVAL] batch:   11 | acc: 93.75%,  total acc: 71.35%   [EVAL] batch:   12 | acc: 43.75%,  total acc: 69.23%   [EVAL] batch:   13 | acc: 12.50%,  total acc: 65.18%   [EVAL] batch:   14 | acc: 25.00%,  total acc: 62.50%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 59.38%   [EVAL] batch:   16 | acc: 37.50%,  total acc: 58.09%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 57.29%   [EVAL] batch:   18 | acc: 37.50%,  total acc: 56.25%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 58.13%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 60.12%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 61.93%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 63.59%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 65.10%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 66.25%   [EVAL] batch:   25 | acc: 12.50%,  total acc: 64.18%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 62.50%   [EVAL] batch:   27 | acc: 6.25%,  total acc: 60.49%   [EVAL] batch:   28 | acc: 37.50%,  total acc: 59.70%   [EVAL] batch:   29 | acc: 6.25%,  total acc: 57.92%   [EVAL] batch:   30 | acc: 6.25%,  total acc: 56.25%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 56.84%   [EVAL] batch:   32 | acc: 93.75%,  total acc: 57.95%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 59.19%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 60.18%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 61.82%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 62.50%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 62.98%   [EVAL] batch:   39 | acc: 62.50%,  total acc: 62.97%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 63.26%   [EVAL] batch:   41 | acc: 75.00%,  total acc: 63.54%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 63.81%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 64.06%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 64.17%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 64.67%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 64.89%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 64.84%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 65.18%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 65.38%   [EVAL] batch:   50 | acc: 62.50%,  total acc: 65.32%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 64.90%   [EVAL] batch:   52 | acc: 50.00%,  total acc: 64.62%   [EVAL] batch:   53 | acc: 43.75%,  total acc: 64.24%   [EVAL] batch:   54 | acc: 62.50%,  total acc: 64.20%   [EVAL] batch:   55 | acc: 56.25%,  total acc: 64.06%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 64.14%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 64.33%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 64.83%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 65.10%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 65.27%   [EVAL] batch:   61 | acc: 56.25%,  total acc: 65.12%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 64.68%   
[EVAL] batch:    0 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:    5 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 43.75%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 50.00%,  total acc: 71.09%   [EVAL] batch:    8 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 69.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 66.48%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 64.58%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 63.94%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 66.07%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 67.92%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 71.32%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 74.01%   [EVAL] batch:   19 | acc: 68.75%,  total acc: 73.75%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 74.40%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 74.43%   [EVAL] batch:   22 | acc: 75.00%,  total acc: 74.46%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 75.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 76.20%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 77.08%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 77.90%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 78.45%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 79.84%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 80.27%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 80.49%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 79.78%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 79.64%   [EVAL] batch:   35 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:   36 | acc: 56.25%,  total acc: 79.05%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 79.28%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 79.81%   [EVAL] batch:   39 | acc: 93.75%,  total acc: 80.16%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 80.49%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 80.80%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 80.96%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 80.68%   [EVAL] batch:   44 | acc: 18.75%,  total acc: 79.31%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 78.26%   [EVAL] batch:   46 | acc: 31.25%,  total acc: 77.26%   [EVAL] batch:   47 | acc: 6.25%,  total acc: 75.78%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 74.87%   [EVAL] batch:   49 | acc: 18.75%,  total acc: 73.75%   [EVAL] batch:   50 | acc: 0.00%,  total acc: 72.30%   [EVAL] batch:   51 | acc: 6.25%,  total acc: 71.03%   [EVAL] batch:   52 | acc: 6.25%,  total acc: 69.81%   [EVAL] batch:   53 | acc: 6.25%,  total acc: 68.63%   [EVAL] batch:   54 | acc: 0.00%,  total acc: 67.39%   [EVAL] batch:   55 | acc: 18.75%,  total acc: 66.52%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 66.01%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 65.73%   [EVAL] batch:   58 | acc: 62.50%,  total acc: 65.68%   [EVAL] batch:   59 | acc: 75.00%,  total acc: 65.83%   [EVAL] batch:   60 | acc: 62.50%,  total acc: 65.78%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 65.83%   [EVAL] batch:   62 | acc: 75.00%,  total acc: 65.97%   [EVAL] batch:   63 | acc: 93.75%,  total acc: 66.41%   [EVAL] batch:   64 | acc: 87.50%,  total acc: 66.73%   [EVAL] batch:   65 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:   66 | acc: 75.00%,  total acc: 67.35%   [EVAL] batch:   67 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:   68 | acc: 87.50%,  total acc: 68.03%   [EVAL] batch:   69 | acc: 56.25%,  total acc: 67.86%   [EVAL] batch:   70 | acc: 56.25%,  total acc: 67.69%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 67.62%   [EVAL] batch:   72 | acc: 37.50%,  total acc: 67.21%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 67.15%   [EVAL] batch:   74 | acc: 50.00%,  total acc: 66.92%   [EVAL] batch:   75 | acc: 68.75%,  total acc: 66.94%   [EVAL] batch:   76 | acc: 62.50%,  total acc: 66.88%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 66.91%   [EVAL] batch:   78 | acc: 56.25%,  total acc: 66.77%   [EVAL] batch:   79 | acc: 56.25%,  total acc: 66.64%   [EVAL] batch:   80 | acc: 56.25%,  total acc: 66.51%   [EVAL] batch:   81 | acc: 37.50%,  total acc: 66.16%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 65.51%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 64.88%   [EVAL] batch:   84 | acc: 25.00%,  total acc: 64.41%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 63.95%   [EVAL] batch:   86 | acc: 18.75%,  total acc: 63.43%   [EVAL] batch:   87 | acc: 56.25%,  total acc: 63.35%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 63.76%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 64.17%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 64.42%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 64.81%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 65.19%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 65.49%   [EVAL] batch:   94 | acc: 50.00%,  total acc: 65.33%   [EVAL] batch:   95 | acc: 81.25%,  total acc: 65.49%   [EVAL] batch:   96 | acc: 68.75%,  total acc: 65.53%   [EVAL] batch:   97 | acc: 56.25%,  total acc: 65.43%   [EVAL] batch:   98 | acc: 75.00%,  total acc: 65.53%   [EVAL] batch:   99 | acc: 43.75%,  total acc: 65.31%   [EVAL] batch:  100 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  101 | acc: 100.00%,  total acc: 65.99%   [EVAL] batch:  102 | acc: 100.00%,  total acc: 66.32%   [EVAL] batch:  103 | acc: 100.00%,  total acc: 66.65%   [EVAL] batch:  104 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:  105 | acc: 93.75%,  total acc: 67.22%   [EVAL] batch:  106 | acc: 100.00%,  total acc: 67.52%   [EVAL] batch:  107 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:  108 | acc: 100.00%,  total acc: 68.12%   [EVAL] batch:  109 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:  110 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  111 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 68.92%   [EVAL] batch:  113 | acc: 50.00%,  total acc: 68.75%   [EVAL] batch:  114 | acc: 62.50%,  total acc: 68.70%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 68.70%   [EVAL] batch:  116 | acc: 62.50%,  total acc: 68.64%   [EVAL] batch:  117 | acc: 75.00%,  total acc: 68.70%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 68.80%   [EVAL] batch:  119 | acc: 100.00%,  total acc: 69.06%   [EVAL] batch:  120 | acc: 68.75%,  total acc: 69.06%   [EVAL] batch:  121 | acc: 100.00%,  total acc: 69.31%   [EVAL] batch:  122 | acc: 100.00%,  total acc: 69.56%   [EVAL] batch:  123 | acc: 87.50%,  total acc: 69.71%   [EVAL] batch:  124 | acc: 93.75%,  total acc: 69.90%   [EVAL] batch:  125 | acc: 0.00%,  total acc: 69.35%   [EVAL] batch:  126 | acc: 0.00%,  total acc: 68.80%   [EVAL] batch:  127 | acc: 0.00%,  total acc: 68.26%   [EVAL] batch:  128 | acc: 0.00%,  total acc: 67.73%   [EVAL] batch:  129 | acc: 0.00%,  total acc: 67.21%   [EVAL] batch:  130 | acc: 6.25%,  total acc: 66.75%   [EVAL] batch:  131 | acc: 62.50%,  total acc: 66.71%   [EVAL] batch:  132 | acc: 81.25%,  total acc: 66.82%   [EVAL] batch:  133 | acc: 93.75%,  total acc: 67.02%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 67.13%   [EVAL] batch:  135 | acc: 87.50%,  total acc: 67.28%   [EVAL] batch:  136 | acc: 75.00%,  total acc: 67.34%   [EVAL] batch:  137 | acc: 81.25%,  total acc: 67.44%   [EVAL] batch:  138 | acc: 50.00%,  total acc: 67.31%   [EVAL] batch:  139 | acc: 56.25%,  total acc: 67.23%   [EVAL] batch:  140 | acc: 75.00%,  total acc: 67.29%   [EVAL] batch:  141 | acc: 50.00%,  total acc: 67.17%   [EVAL] batch:  142 | acc: 62.50%,  total acc: 67.13%   [EVAL] batch:  143 | acc: 25.00%,  total acc: 66.84%   [EVAL] batch:  144 | acc: 12.50%,  total acc: 66.47%   [EVAL] batch:  145 | acc: 6.25%,  total acc: 66.05%   [EVAL] batch:  146 | acc: 31.25%,  total acc: 65.82%   [EVAL] batch:  147 | acc: 31.25%,  total acc: 65.58%   [EVAL] batch:  148 | acc: 6.25%,  total acc: 65.18%   [EVAL] batch:  149 | acc: 6.25%,  total acc: 64.79%   [EVAL] batch:  150 | acc: 0.00%,  total acc: 64.36%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 63.98%   [EVAL] batch:  152 | acc: 0.00%,  total acc: 63.56%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 63.19%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 62.86%   [EVAL] batch:  155 | acc: 0.00%,  total acc: 62.46%   [EVAL] batch:  156 | acc: 37.50%,  total acc: 62.30%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 62.42%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 62.58%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 62.73%   [EVAL] batch:  160 | acc: 81.25%,  total acc: 62.85%   [EVAL] batch:  161 | acc: 68.75%,  total acc: 62.89%   [EVAL] batch:  162 | acc: 87.50%,  total acc: 63.04%   [EVAL] batch:  163 | acc: 87.50%,  total acc: 63.19%   [EVAL] batch:  164 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  165 | acc: 100.00%,  total acc: 63.63%   [EVAL] batch:  166 | acc: 81.25%,  total acc: 63.74%   [EVAL] batch:  167 | acc: 93.75%,  total acc: 63.91%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 64.05%   [EVAL] batch:  169 | acc: 31.25%,  total acc: 63.86%   [EVAL] batch:  170 | acc: 18.75%,  total acc: 63.60%   [EVAL] batch:  171 | acc: 25.00%,  total acc: 63.37%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 63.11%   [EVAL] batch:  173 | acc: 56.25%,  total acc: 63.07%   [EVAL] batch:  174 | acc: 31.25%,  total acc: 62.89%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 62.68%   [EVAL] batch:  176 | acc: 62.50%,  total acc: 62.68%   [EVAL] batch:  177 | acc: 37.50%,  total acc: 62.54%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 62.47%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 62.36%   [EVAL] batch:  180 | acc: 56.25%,  total acc: 62.33%   [EVAL] batch:  181 | acc: 18.75%,  total acc: 62.09%   [EVAL] batch:  182 | acc: 31.25%,  total acc: 61.92%   [EVAL] batch:  183 | acc: 37.50%,  total acc: 61.79%   [EVAL] batch:  184 | acc: 25.00%,  total acc: 61.59%   [EVAL] batch:  185 | acc: 31.25%,  total acc: 61.42%   [EVAL] batch:  186 | acc: 25.00%,  total acc: 61.23%   [EVAL] batch:  187 | acc: 18.75%,  total acc: 61.00%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 60.85%   [EVAL] batch:  189 | acc: 37.50%,  total acc: 60.72%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 60.60%   [EVAL] batch:  191 | acc: 37.50%,  total acc: 60.48%   [EVAL] batch:  192 | acc: 37.50%,  total acc: 60.36%   [EVAL] batch:  193 | acc: 37.50%,  total acc: 60.24%   [EVAL] batch:  194 | acc: 62.50%,  total acc: 60.26%   [EVAL] batch:  195 | acc: 62.50%,  total acc: 60.27%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 60.31%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 60.42%   [EVAL] batch:  198 | acc: 31.25%,  total acc: 60.27%   [EVAL] batch:  199 | acc: 100.00%,  total acc: 60.47%   [EVAL] batch:  200 | acc: 50.00%,  total acc: 60.42%   [EVAL] batch:  201 | acc: 50.00%,  total acc: 60.37%   [EVAL] batch:  202 | acc: 56.25%,  total acc: 60.34%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 60.29%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 60.21%   [EVAL] batch:  205 | acc: 68.75%,  total acc: 60.25%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 60.39%   [EVAL] batch:  207 | acc: 87.50%,  total acc: 60.52%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:  209 | acc: 100.00%,  total acc: 60.89%   [EVAL] batch:  210 | acc: 87.50%,  total acc: 61.02%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 61.20%   [EVAL] batch:  212 | acc: 75.00%,  total acc: 61.27%   [EVAL] batch:  213 | acc: 43.75%,  total acc: 61.19%   [EVAL] batch:  214 | acc: 37.50%,  total acc: 61.08%   [EVAL] batch:  215 | acc: 81.25%,  total acc: 61.17%   [EVAL] batch:  216 | acc: 37.50%,  total acc: 61.06%   [EVAL] batch:  217 | acc: 50.00%,  total acc: 61.01%   [EVAL] batch:  218 | acc: 43.75%,  total acc: 60.93%   [EVAL] batch:  219 | acc: 18.75%,  total acc: 60.74%   [EVAL] batch:  220 | acc: 6.25%,  total acc: 60.49%   [EVAL] batch:  221 | acc: 0.00%,  total acc: 60.22%   [EVAL] batch:  222 | acc: 6.25%,  total acc: 59.98%   [EVAL] batch:  223 | acc: 0.00%,  total acc: 59.71%   [EVAL] batch:  224 | acc: 18.75%,  total acc: 59.53%   [EVAL] batch:  225 | acc: 68.75%,  total acc: 59.57%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 59.66%   [EVAL] batch:  227 | acc: 68.75%,  total acc: 59.70%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 59.80%   [EVAL] batch:  229 | acc: 62.50%,  total acc: 59.81%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 59.79%   [EVAL] batch:  231 | acc: 75.00%,  total acc: 59.86%   [EVAL] batch:  232 | acc: 56.25%,  total acc: 59.84%   [EVAL] batch:  233 | acc: 81.25%,  total acc: 59.94%   [EVAL] batch:  234 | acc: 68.75%,  total acc: 59.97%   [EVAL] batch:  235 | acc: 62.50%,  total acc: 59.98%   [EVAL] batch:  236 | acc: 81.25%,  total acc: 60.07%   [EVAL] batch:  237 | acc: 93.75%,  total acc: 60.22%   [EVAL] batch:  238 | acc: 100.00%,  total acc: 60.38%   [EVAL] batch:  239 | acc: 100.00%,  total acc: 60.55%   [EVAL] batch:  240 | acc: 100.00%,  total acc: 60.71%   [EVAL] batch:  241 | acc: 100.00%,  total acc: 60.87%   [EVAL] batch:  242 | acc: 100.00%,  total acc: 61.03%   [EVAL] batch:  243 | acc: 100.00%,  total acc: 61.19%   [EVAL] batch:  244 | acc: 93.75%,  total acc: 61.33%   [EVAL] batch:  245 | acc: 87.50%,  total acc: 61.43%   [EVAL] batch:  246 | acc: 93.75%,  total acc: 61.56%   [EVAL] batch:  247 | acc: 93.75%,  total acc: 61.69%   [EVAL] batch:  248 | acc: 100.00%,  total acc: 61.85%   [EVAL] batch:  249 | acc: 100.00%,  total acc: 62.00%   [EVAL] batch:  250 | acc: 81.25%,  total acc: 62.08%   [EVAL] batch:  251 | acc: 68.75%,  total acc: 62.10%   [EVAL] batch:  252 | acc: 81.25%,  total acc: 62.18%   [EVAL] batch:  253 | acc: 68.75%,  total acc: 62.20%   [EVAL] batch:  254 | acc: 81.25%,  total acc: 62.28%   [EVAL] batch:  255 | acc: 81.25%,  total acc: 62.35%   [EVAL] batch:  256 | acc: 75.00%,  total acc: 62.40%   [EVAL] batch:  257 | acc: 75.00%,  total acc: 62.45%   [EVAL] batch:  258 | acc: 56.25%,  total acc: 62.43%   [EVAL] batch:  259 | acc: 56.25%,  total acc: 62.40%   [EVAL] batch:  260 | acc: 50.00%,  total acc: 62.36%   [EVAL] batch:  261 | acc: 50.00%,  total acc: 62.31%   [EVAL] batch:  262 | acc: 93.75%,  total acc: 62.43%   [EVAL] batch:  263 | acc: 56.25%,  total acc: 62.41%   [EVAL] batch:  264 | acc: 75.00%,  total acc: 62.45%   [EVAL] batch:  265 | acc: 75.00%,  total acc: 62.50%   [EVAL] batch:  266 | acc: 81.25%,  total acc: 62.57%   [EVAL] batch:  267 | acc: 87.50%,  total acc: 62.66%   [EVAL] batch:  268 | acc: 68.75%,  total acc: 62.69%   [EVAL] batch:  269 | acc: 0.00%,  total acc: 62.45%   [EVAL] batch:  270 | acc: 0.00%,  total acc: 62.22%   [EVAL] batch:  271 | acc: 0.00%,  total acc: 61.99%   [EVAL] batch:  272 | acc: 6.25%,  total acc: 61.79%   [EVAL] batch:  273 | acc: 12.50%,  total acc: 61.61%   [EVAL] batch:  274 | acc: 6.25%,  total acc: 61.41%   [EVAL] batch:  275 | acc: 100.00%,  total acc: 61.55%   [EVAL] batch:  276 | acc: 100.00%,  total acc: 61.69%   [EVAL] batch:  277 | acc: 100.00%,  total acc: 61.83%   [EVAL] batch:  278 | acc: 100.00%,  total acc: 61.96%   [EVAL] batch:  279 | acc: 100.00%,  total acc: 62.10%   [EVAL] batch:  280 | acc: 100.00%,  total acc: 62.23%   [EVAL] batch:  281 | acc: 100.00%,  total acc: 62.37%   [EVAL] batch:  282 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:  283 | acc: 100.00%,  total acc: 62.63%   [EVAL] batch:  284 | acc: 100.00%,  total acc: 62.76%   [EVAL] batch:  285 | acc: 100.00%,  total acc: 62.89%   [EVAL] batch:  286 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 63.11%   [EVAL] batch:  288 | acc: 68.75%,  total acc: 63.13%   [EVAL] batch:  289 | acc: 68.75%,  total acc: 63.15%   [EVAL] batch:  290 | acc: 62.50%,  total acc: 63.14%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 63.21%   [EVAL] batch:  292 | acc: 81.25%,  total acc: 63.27%   [EVAL] batch:  293 | acc: 62.50%,  total acc: 63.27%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 63.18%   [EVAL] batch:  295 | acc: 25.00%,  total acc: 63.05%   [EVAL] batch:  296 | acc: 43.75%,  total acc: 62.98%   [EVAL] batch:  297 | acc: 56.25%,  total acc: 62.96%   [EVAL] batch:  298 | acc: 31.25%,  total acc: 62.86%   [EVAL] batch:  299 | acc: 37.50%,  total acc: 62.77%   [EVAL] batch:  300 | acc: 87.50%,  total acc: 62.85%   [EVAL] batch:  301 | acc: 93.75%,  total acc: 62.96%   [EVAL] batch:  302 | acc: 81.25%,  total acc: 63.02%   [EVAL] batch:  303 | acc: 81.25%,  total acc: 63.08%   [EVAL] batch:  304 | acc: 93.75%,  total acc: 63.18%   [EVAL] batch:  305 | acc: 81.25%,  total acc: 63.24%   [EVAL] batch:  306 | acc: 56.25%,  total acc: 63.21%   [EVAL] batch:  307 | acc: 37.50%,  total acc: 63.13%   [EVAL] batch:  308 | acc: 37.50%,  total acc: 63.05%   [EVAL] batch:  309 | acc: 25.00%,  total acc: 62.92%   [EVAL] batch:  310 | acc: 25.00%,  total acc: 62.80%   [EVAL] batch:  311 | acc: 43.75%,  total acc: 62.74%   [EVAL] batch:  312 | acc: 37.50%,  total acc: 62.66%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 62.50%   [EVAL] batch:  314 | acc: 25.00%,  total acc: 62.38%   [EVAL] batch:  315 | acc: 12.50%,  total acc: 62.22%   [EVAL] batch:  316 | acc: 6.25%,  total acc: 62.05%   [EVAL] batch:  317 | acc: 18.75%,  total acc: 61.91%   [EVAL] batch:  318 | acc: 18.75%,  total acc: 61.78%   [EVAL] batch:  319 | acc: 75.00%,  total acc: 61.82%   [EVAL] batch:  320 | acc: 50.00%,  total acc: 61.78%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 61.82%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 61.90%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 61.94%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 61.98%   [EVAL] batch:  325 | acc: 43.75%,  total acc: 61.92%   [EVAL] batch:  326 | acc: 18.75%,  total acc: 61.79%   [EVAL] batch:  327 | acc: 18.75%,  total acc: 61.66%   [EVAL] batch:  328 | acc: 31.25%,  total acc: 61.57%   [EVAL] batch:  329 | acc: 37.50%,  total acc: 61.50%   [EVAL] batch:  330 | acc: 31.25%,  total acc: 61.40%   [EVAL] batch:  331 | acc: 87.50%,  total acc: 61.48%   [EVAL] batch:  332 | acc: 93.75%,  total acc: 61.58%   [EVAL] batch:  333 | acc: 100.00%,  total acc: 61.70%   [EVAL] batch:  334 | acc: 93.75%,  total acc: 61.79%   [EVAL] batch:  335 | acc: 93.75%,  total acc: 61.89%   [EVAL] batch:  336 | acc: 100.00%,  total acc: 62.00%   [EVAL] batch:  337 | acc: 50.00%,  total acc: 61.96%   [EVAL] batch:  338 | acc: 18.75%,  total acc: 61.84%   [EVAL] batch:  339 | acc: 43.75%,  total acc: 61.78%   [EVAL] batch:  340 | acc: 25.00%,  total acc: 61.68%   [EVAL] batch:  341 | acc: 12.50%,  total acc: 61.53%   [EVAL] batch:  342 | acc: 25.00%,  total acc: 61.42%   [EVAL] batch:  343 | acc: 62.50%,  total acc: 61.43%   [EVAL] batch:  344 | acc: 100.00%,  total acc: 61.54%   [EVAL] batch:  345 | acc: 100.00%,  total acc: 61.65%   [EVAL] batch:  346 | acc: 87.50%,  total acc: 61.73%   [EVAL] batch:  347 | acc: 93.75%,  total acc: 61.82%   [EVAL] batch:  348 | acc: 100.00%,  total acc: 61.93%   [EVAL] batch:  349 | acc: 100.00%,  total acc: 62.04%   [EVAL] batch:  350 | acc: 50.00%,  total acc: 62.00%   [EVAL] batch:  351 | acc: 43.75%,  total acc: 61.95%   [EVAL] batch:  352 | acc: 31.25%,  total acc: 61.86%   [EVAL] batch:  353 | acc: 25.00%,  total acc: 61.76%   [EVAL] batch:  354 | acc: 25.00%,  total acc: 61.65%   [EVAL] batch:  355 | acc: 56.25%,  total acc: 61.64%   [EVAL] batch:  356 | acc: 93.75%,  total acc: 61.73%   [EVAL] batch:  357 | acc: 87.50%,  total acc: 61.80%   [EVAL] batch:  358 | acc: 93.75%,  total acc: 61.89%   [EVAL] batch:  359 | acc: 87.50%,  total acc: 61.96%   [EVAL] batch:  360 | acc: 62.50%,  total acc: 61.96%   [EVAL] batch:  361 | acc: 62.50%,  total acc: 61.96%   [EVAL] batch:  362 | acc: 50.00%,  total acc: 61.93%   [EVAL] batch:  363 | acc: 50.00%,  total acc: 61.90%   [EVAL] batch:  364 | acc: 43.75%,  total acc: 61.85%   [EVAL] batch:  365 | acc: 37.50%,  total acc: 61.78%   [EVAL] batch:  366 | acc: 43.75%,  total acc: 61.73%   [EVAL] batch:  367 | acc: 37.50%,  total acc: 61.67%   [EVAL] batch:  368 | acc: 25.00%,  total acc: 61.57%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 61.64%   [EVAL] batch:  370 | acc: 56.25%,  total acc: 61.62%   [EVAL] batch:  371 | acc: 50.00%,  total acc: 61.59%   [EVAL] batch:  372 | acc: 81.25%,  total acc: 61.65%   [EVAL] batch:  373 | acc: 87.50%,  total acc: 61.71%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 61.78%   [EVAL] batch:  375 | acc: 62.50%,  total acc: 61.79%   [EVAL] batch:  376 | acc: 62.50%,  total acc: 61.79%   [EVAL] batch:  377 | acc: 68.75%,  total acc: 61.81%   [EVAL] batch:  378 | acc: 25.00%,  total acc: 61.71%   [EVAL] batch:  379 | acc: 62.50%,  total acc: 61.71%   [EVAL] batch:  380 | acc: 50.00%,  total acc: 61.68%   [EVAL] batch:  381 | acc: 87.50%,  total acc: 61.75%   [EVAL] batch:  382 | acc: 100.00%,  total acc: 61.85%   [EVAL] batch:  383 | acc: 93.75%,  total acc: 61.93%   [EVAL] batch:  384 | acc: 87.50%,  total acc: 62.00%   [EVAL] batch:  385 | acc: 87.50%,  total acc: 62.06%   [EVAL] batch:  386 | acc: 100.00%,  total acc: 62.16%   [EVAL] batch:  387 | acc: 81.25%,  total acc: 62.21%   [EVAL] batch:  388 | acc: 56.25%,  total acc: 62.19%   [EVAL] batch:  389 | acc: 68.75%,  total acc: 62.21%   [EVAL] batch:  390 | acc: 81.25%,  total acc: 62.26%   [EVAL] batch:  391 | acc: 87.50%,  total acc: 62.32%   [EVAL] batch:  392 | acc: 68.75%,  total acc: 62.34%   [EVAL] batch:  393 | acc: 75.00%,  total acc: 62.37%   [EVAL] batch:  394 | acc: 68.75%,  total acc: 62.39%   [EVAL] batch:  395 | acc: 68.75%,  total acc: 62.41%   [EVAL] batch:  396 | acc: 75.00%,  total acc: 62.44%   [EVAL] batch:  397 | acc: 81.25%,  total acc: 62.48%   [EVAL] batch:  398 | acc: 62.50%,  total acc: 62.48%   [EVAL] batch:  399 | acc: 62.50%,  total acc: 62.48%   [EVAL] batch:  400 | acc: 0.00%,  total acc: 62.33%   [EVAL] batch:  401 | acc: 6.25%,  total acc: 62.19%   [EVAL] batch:  402 | acc: 12.50%,  total acc: 62.07%   [EVAL] batch:  403 | acc: 0.00%,  total acc: 61.91%   [EVAL] batch:  404 | acc: 6.25%,  total acc: 61.77%   [EVAL] batch:  405 | acc: 0.00%,  total acc: 61.62%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 61.59%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 61.57%   [EVAL] batch:  408 | acc: 50.00%,  total acc: 61.54%   [EVAL] batch:  409 | acc: 37.50%,  total acc: 61.48%   [EVAL] batch:  410 | acc: 62.50%,  total acc: 61.48%   [EVAL] batch:  411 | acc: 50.00%,  total acc: 61.45%   [EVAL] batch:  412 | acc: 56.25%,  total acc: 61.44%   [EVAL] batch:  413 | acc: 75.00%,  total acc: 61.47%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 61.52%   [EVAL] batch:  415 | acc: 87.50%,  total acc: 61.58%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 61.63%   [EVAL] batch:  417 | acc: 87.50%,  total acc: 61.69%   [EVAL] batch:  418 | acc: 87.50%,  total acc: 61.75%   [EVAL] batch:  419 | acc: 37.50%,  total acc: 61.70%   [EVAL] batch:  420 | acc: 43.75%,  total acc: 61.65%   [EVAL] batch:  421 | acc: 25.00%,  total acc: 61.57%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 61.55%   [EVAL] batch:  423 | acc: 31.25%,  total acc: 61.48%   [EVAL] batch:  424 | acc: 50.00%,  total acc: 61.46%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 61.55%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 61.64%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 61.73%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 61.82%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 61.90%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 61.99%   [EVAL] batch:  431 | acc: 68.75%,  total acc: 62.01%   [EVAL] batch:  432 | acc: 56.25%,  total acc: 61.99%   [EVAL] batch:  433 | acc: 56.25%,  total acc: 61.98%   [EVAL] batch:  434 | acc: 87.50%,  total acc: 62.04%   [EVAL] batch:  435 | acc: 75.00%,  total acc: 62.07%   [EVAL] batch:  436 | acc: 68.75%,  total acc: 62.09%   [EVAL] batch:  437 | acc: 75.00%,  total acc: 62.11%   [EVAL] batch:  438 | acc: 56.25%,  total acc: 62.10%   [EVAL] batch:  439 | acc: 56.25%,  total acc: 62.09%   [EVAL] batch:  440 | acc: 68.75%,  total acc: 62.10%   [EVAL] batch:  441 | acc: 62.50%,  total acc: 62.10%   [EVAL] batch:  442 | acc: 75.00%,  total acc: 62.13%   [EVAL] batch:  443 | acc: 56.25%,  total acc: 62.12%   [EVAL] batch:  444 | acc: 81.25%,  total acc: 62.16%   [EVAL] batch:  445 | acc: 68.75%,  total acc: 62.18%   [EVAL] batch:  446 | acc: 68.75%,  total acc: 62.19%   [EVAL] batch:  447 | acc: 87.50%,  total acc: 62.25%   [EVAL] batch:  448 | acc: 93.75%,  total acc: 62.32%   [EVAL] batch:  449 | acc: 68.75%,  total acc: 62.33%   [EVAL] batch:  450 | acc: 25.00%,  total acc: 62.25%   [EVAL] batch:  451 | acc: 18.75%,  total acc: 62.15%   [EVAL] batch:  452 | acc: 12.50%,  total acc: 62.04%   [EVAL] batch:  453 | acc: 25.00%,  total acc: 61.96%   [EVAL] batch:  454 | acc: 50.00%,  total acc: 61.94%   [EVAL] batch:  455 | acc: 25.00%,  total acc: 61.86%   [EVAL] batch:  456 | acc: 75.00%,  total acc: 61.88%   [EVAL] batch:  457 | acc: 100.00%,  total acc: 61.97%   [EVAL] batch:  458 | acc: 100.00%,  total acc: 62.05%   [EVAL] batch:  459 | acc: 100.00%,  total acc: 62.13%   [EVAL] batch:  460 | acc: 100.00%,  total acc: 62.22%   [EVAL] batch:  461 | acc: 93.75%,  total acc: 62.28%   [EVAL] batch:  462 | acc: 56.25%,  total acc: 62.27%   [EVAL] batch:  463 | acc: 12.50%,  total acc: 62.16%   [EVAL] batch:  464 | acc: 12.50%,  total acc: 62.06%   [EVAL] batch:  465 | acc: 25.00%,  total acc: 61.98%   [EVAL] batch:  466 | acc: 18.75%,  total acc: 61.88%   [EVAL] batch:  467 | acc: 6.25%,  total acc: 61.77%   [EVAL] batch:  468 | acc: 31.25%,  total acc: 61.70%   [EVAL] batch:  469 | acc: 100.00%,  total acc: 61.78%   [EVAL] batch:  470 | acc: 93.75%,  total acc: 61.85%   [EVAL] batch:  471 | acc: 93.75%,  total acc: 61.92%   [EVAL] batch:  472 | acc: 87.50%,  total acc: 61.97%   [EVAL] batch:  473 | acc: 93.75%,  total acc: 62.04%   [EVAL] batch:  474 | acc: 100.00%,  total acc: 62.12%   [EVAL] batch:  475 | acc: 75.00%,  total acc: 62.15%   [EVAL] batch:  476 | acc: 87.50%,  total acc: 62.20%   [EVAL] batch:  477 | acc: 62.50%,  total acc: 62.20%   [EVAL] batch:  478 | acc: 68.75%,  total acc: 62.21%   [EVAL] batch:  479 | acc: 75.00%,  total acc: 62.24%   [EVAL] batch:  480 | acc: 75.00%,  total acc: 62.27%   [EVAL] batch:  481 | acc: 68.75%,  total acc: 62.28%   [EVAL] batch:  482 | acc: 81.25%,  total acc: 62.32%   [EVAL] batch:  483 | acc: 75.00%,  total acc: 62.35%   [EVAL] batch:  484 | acc: 81.25%,  total acc: 62.38%   [EVAL] batch:  485 | acc: 68.75%,  total acc: 62.40%   [EVAL] batch:  486 | acc: 81.25%,  total acc: 62.44%   [EVAL] batch:  487 | acc: 62.50%,  total acc: 62.44%   [EVAL] batch:  488 | acc: 56.25%,  total acc: 62.42%   [EVAL] batch:  489 | acc: 56.25%,  total acc: 62.41%   [EVAL] batch:  490 | acc: 43.75%,  total acc: 62.37%   [EVAL] batch:  491 | acc: 50.00%,  total acc: 62.35%   [EVAL] batch:  492 | acc: 50.00%,  total acc: 62.32%   [EVAL] batch:  493 | acc: 56.25%,  total acc: 62.31%   [EVAL] batch:  494 | acc: 87.50%,  total acc: 62.36%   [EVAL] batch:  495 | acc: 81.25%,  total acc: 62.40%   [EVAL] batch:  496 | acc: 81.25%,  total acc: 62.44%   [EVAL] batch:  497 | acc: 68.75%,  total acc: 62.45%   [EVAL] batch:  498 | acc: 75.00%,  total acc: 62.47%   [EVAL] batch:  499 | acc: 68.75%,  total acc: 62.49%   
cur_acc:  ['0.9494', '0.8323', '0.6468', '0.7312', '0.7302', '0.6171', '0.6250', '0.6468']
his_acc:  ['0.9494', '0.8805', '0.7889', '0.7390', '0.6993', '0.6713', '0.6540', '0.6249']
--------Round  5
seed:  600
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/train.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/valid.pkl
data/CFRLFewRel/CFRLdata_10_100_10_5/_process_BERT_hybridprompt_12token/test.pkl
Task_order: [7 2 0 1 6 3 4 5]
prepared data!
Clustering into  4  clusters
Clusters:  [0 1 2 0 3 3 2 0 1 1]
Losses:  18.191686630249023 3.9093708992004395 1.2311686277389526
CurrentTrain: epoch  0, batch     0 | loss: 18.1916866Losses:  19.133758544921875 5.368856430053711 0.9722613096237183
CurrentTrain: epoch  0, batch     1 | loss: 19.1337585Losses:  19.64826202392578 5.558585166931152 1.2100872993469238
CurrentTrain: epoch  0, batch     2 | loss: 19.6482620Losses:  18.444961547851562 4.485507011413574 1.1457191705703735
CurrentTrain: epoch  0, batch     3 | loss: 18.4449615Losses:  19.935779571533203 6.393089294433594 1.0827069282531738
CurrentTrain: epoch  0, batch     4 | loss: 19.9357796Losses:  17.991743087768555 4.79251766204834 1.0189945697784424
CurrentTrain: epoch  0, batch     5 | loss: 17.9917431Losses:  19.051607131958008 6.178459167480469 0.9725342988967896
CurrentTrain: epoch  0, batch     6 | loss: 19.0516071Losses:  19.785696029663086 7.0547003746032715 0.9279911518096924
CurrentTrain: epoch  0, batch     7 | loss: 19.7856960Losses:  17.889930725097656 5.251070022583008 0.8897184133529663
CurrentTrain: epoch  0, batch     8 | loss: 17.8899307Losses:  18.827877044677734 6.366794586181641 0.8713865876197815
CurrentTrain: epoch  0, batch     9 | loss: 18.8278770Losses:  17.866527557373047 5.942898273468018 0.8259394764900208
CurrentTrain: epoch  0, batch    10 | loss: 17.8665276Losses:  17.932723999023438 5.954112529754639 0.8722162246704102
CurrentTrain: epoch  0, batch    11 | loss: 17.9327240Losses:  15.725098609924316 3.693716526031494 0.8012130260467529
CurrentTrain: epoch  0, batch    12 | loss: 15.7250986Losses:  16.347326278686523 4.9637651443481445 0.7517184019088745
CurrentTrain: epoch  0, batch    13 | loss: 16.3473263Losses:  15.938163757324219 4.060240268707275 0.77529376745224
CurrentTrain: epoch  0, batch    14 | loss: 15.9381638Losses:  16.704483032226562 5.219937801361084 0.6964958310127258
CurrentTrain: epoch  0, batch    15 | loss: 16.7044830Losses:  16.922895431518555 5.602421760559082 0.6491862535476685
CurrentTrain: epoch  0, batch    16 | loss: 16.9228954Losses:  15.635274887084961 4.533978462219238 0.6698832511901855
CurrentTrain: epoch  0, batch    17 | loss: 15.6352749Losses:  16.814043045043945 5.968944072723389 0.6519508957862854
CurrentTrain: epoch  0, batch    18 | loss: 16.8140430Losses:  16.310495376586914 5.88295841217041 0.5510684251785278
CurrentTrain: epoch  0, batch    19 | loss: 16.3104954Losses:  16.437517166137695 5.1253156661987305 0.6354577541351318
CurrentTrain: epoch  0, batch    20 | loss: 16.4375172Losses:  17.246028900146484 5.894294738769531 0.6167349815368652
CurrentTrain: epoch  0, batch    21 | loss: 17.2460289Losses:  14.457148551940918 3.2899210453033447 0.5729116201400757
CurrentTrain: epoch  0, batch    22 | loss: 14.4571486Losses:  16.542377471923828 5.199751377105713 0.5623180866241455
CurrentTrain: epoch  0, batch    23 | loss: 16.5423775Losses:  18.74199104309082 7.375507831573486 0.5093607902526855
CurrentTrain: epoch  0, batch    24 | loss: 18.7419910Losses:  14.579160690307617 3.591676950454712 0.5178924798965454
CurrentTrain: epoch  0, batch    25 | loss: 14.5791607Losses:  15.441750526428223 4.636494159698486 0.565537691116333
CurrentTrain: epoch  0, batch    26 | loss: 15.4417505Losses:  16.431344985961914 5.984414100646973 0.513936460018158
CurrentTrain: epoch  0, batch    27 | loss: 16.4313450Losses:  15.084352493286133 4.357143878936768 0.5607967972755432
CurrentTrain: epoch  0, batch    28 | loss: 15.0843525Losses:  14.162199974060059 3.650250196456909 0.5459483861923218
CurrentTrain: epoch  0, batch    29 | loss: 14.1622000Losses:  14.829737663269043 4.056807518005371 0.5330312252044678
CurrentTrain: epoch  0, batch    30 | loss: 14.8297377Losses:  14.72270393371582 4.054058074951172 0.5395733118057251
CurrentTrain: epoch  0, batch    31 | loss: 14.7227039Losses:  15.965777397155762 6.170612335205078 0.5151090621948242
CurrentTrain: epoch  0, batch    32 | loss: 15.9657774Losses:  15.373046875 5.402972221374512 0.5016923546791077
CurrentTrain: epoch  0, batch    33 | loss: 15.3730469Losses:  13.454233169555664 3.5306029319763184 0.5088123679161072
CurrentTrain: epoch  0, batch    34 | loss: 13.4542332Losses:  14.801017761230469 4.637718677520752 0.492801308631897
CurrentTrain: epoch  0, batch    35 | loss: 14.8010178Losses:  15.760828018188477 5.854743003845215 0.4842240810394287
CurrentTrain: epoch  0, batch    36 | loss: 15.7608280Losses:  13.563345909118652 3.7800722122192383 0.5036238431930542
CurrentTrain: epoch  0, batch    37 | loss: 13.5633459Losses:  15.572216033935547 5.920535087585449 0.46181976795196533
CurrentTrain: epoch  0, batch    38 | loss: 15.5722160Losses:  14.395218849182129 5.2181901931762695 0.4916042983531952
CurrentTrain: epoch  0, batch    39 | loss: 14.3952188Losses:  15.975564002990723 6.597788333892822 0.46786585450172424
CurrentTrain: epoch  0, batch    40 | loss: 15.9755640Losses:  12.375877380371094 2.967522144317627 0.4757100045681
CurrentTrain: epoch  0, batch    41 | loss: 12.3758774Losses:  13.640275955200195 4.29763650894165 0.4755667448043823
CurrentTrain: epoch  0, batch    42 | loss: 13.6402760Losses:  14.228034019470215 4.3342437744140625 0.4761350154876709
CurrentTrain: epoch  0, batch    43 | loss: 14.2280340Losses:  15.286616325378418 5.519040107727051 0.460560142993927
CurrentTrain: epoch  0, batch    44 | loss: 15.2866163Losses:  13.192618370056152 3.859576940536499 0.4628227949142456
CurrentTrain: epoch  0, batch    45 | loss: 13.1926184Losses:  14.961859703063965 4.967767715454102 0.4601592421531677
CurrentTrain: epoch  0, batch    46 | loss: 14.9618597Losses:  15.40279769897461 5.9753265380859375 0.47472715377807617
CurrentTrain: epoch  0, batch    47 | loss: 15.4027977Losses:  12.203917503356934 3.6057016849517822 0.42175614833831787
CurrentTrain: epoch  0, batch    48 | loss: 12.2039175Losses:  13.797760009765625 4.201122283935547 0.45708101987838745
CurrentTrain: epoch  0, batch    49 | loss: 13.7977600Losses:  13.557016372680664 4.861051082611084 0.4024145007133484
CurrentTrain: epoch  0, batch    50 | loss: 13.5570164Losses:  14.245240211486816 4.916429042816162 0.35361167788505554
CurrentTrain: epoch  0, batch    51 | loss: 14.2452402Losses:  13.115610122680664 4.429728984832764 0.4326133131980896
CurrentTrain: epoch  0, batch    52 | loss: 13.1156101Losses:  12.736363410949707 3.789841890335083 0.4008689224720001
CurrentTrain: epoch  0, batch    53 | loss: 12.7363634Losses:  16.081829071044922 6.535491466522217 0.4282785654067993
CurrentTrain: epoch  0, batch    54 | loss: 16.0818291Losses:  11.212126731872559 2.8684513568878174 0.4222937226295471
CurrentTrain: epoch  0, batch    55 | loss: 11.2121267Losses:  14.246317863464355 5.303250312805176 0.2956487536430359
CurrentTrain: epoch  0, batch    56 | loss: 14.2463179Losses:  13.878866195678711 5.421564102172852 0.2875671088695526
CurrentTrain: epoch  0, batch    57 | loss: 13.8788662Losses:  11.457212448120117 3.7463135719299316 0.3845714330673218
CurrentTrain: epoch  0, batch    58 | loss: 11.4572124Losses:  12.42898941040039 4.305041313171387 0.38672298192977905
CurrentTrain: epoch  0, batch    59 | loss: 12.4289894Losses:  12.024133682250977 3.608323335647583 0.39770424365997314
CurrentTrain: epoch  0, batch    60 | loss: 12.0241337Losses:  13.380847930908203 5.721939563751221 0.3766366243362427
CurrentTrain: epoch  0, batch    61 | loss: 13.3808479Losses:  8.889734268188477 0.994418740272522 0.38150492310523987
CurrentTrain: epoch  0, batch    62 | loss: 8.8897343Losses:  14.249900817871094 6.337785720825195 0.34425193071365356
CurrentTrain: epoch  1, batch     0 | loss: 14.2499008Losses:  10.749585151672363 2.6121606826782227 0.3968046307563782
CurrentTrain: epoch  1, batch     1 | loss: 10.7495852Losses:  11.22136116027832 3.6163251399993896 0.35134419798851013
CurrentTrain: epoch  1, batch     2 | loss: 11.2213612Losses:  11.297872543334961 3.6790926456451416 0.364481657743454
CurrentTrain: epoch  1, batch     3 | loss: 11.2978725Losses:  11.101161003112793 3.190101385116577 0.359914094209671
CurrentTrain: epoch  1, batch     4 | loss: 11.1011610Losses:  13.887375831604004 5.253264427185059 0.39082127809524536
CurrentTrain: epoch  1, batch     5 | loss: 13.8873758Losses:  11.890551567077637 3.2819266319274902 0.41437241435050964
CurrentTrain: epoch  1, batch     6 | loss: 11.8905516Losses:  14.576347351074219 7.307720184326172 0.37712204456329346
CurrentTrain: epoch  1, batch     7 | loss: 14.5763474Losses:  14.04749870300293 5.395566940307617 0.4054201543331146
CurrentTrain: epoch  1, batch     8 | loss: 14.0474987Losses:  11.666162490844727 4.083691120147705 0.34186556935310364
CurrentTrain: epoch  1, batch     9 | loss: 11.6661625Losses:  12.111720085144043 3.7916343212127686 0.39095181226730347
CurrentTrain: epoch  1, batch    10 | loss: 12.1117201Losses:  11.3248872756958 3.4739551544189453 0.36510559916496277
CurrentTrain: epoch  1, batch    11 | loss: 11.3248873Losses:  11.493902206420898 3.3187484741210938 0.3954123854637146
CurrentTrain: epoch  1, batch    12 | loss: 11.4939022Losses:  10.938605308532715 2.8246471881866455 0.3893006443977356
CurrentTrain: epoch  1, batch    13 | loss: 10.9386053Losses:  11.69996452331543 3.4245917797088623 0.36368393898010254
CurrentTrain: epoch  1, batch    14 | loss: 11.6999645Losses:  10.217975616455078 2.5648770332336426 0.33961915969848633
CurrentTrain: epoch  1, batch    15 | loss: 10.2179756Losses:  9.93105411529541 2.2908639907836914 0.3662470281124115
CurrentTrain: epoch  1, batch    16 | loss: 9.9310541Losses:  10.099869728088379 2.1623787879943848 0.36683180928230286
CurrentTrain: epoch  1, batch    17 | loss: 10.0998697Losses:  12.161933898925781 4.677145481109619 0.3475880026817322
CurrentTrain: epoch  1, batch    18 | loss: 12.1619339Losses:  13.520383834838867 4.333106994628906 0.36506399512290955
CurrentTrain: epoch  1, batch    19 | loss: 13.5203838Losses:  11.285989761352539 3.2247791290283203 0.3637875020503998
CurrentTrain: epoch  1, batch    20 | loss: 11.2859898Losses:  12.378366470336914 5.216975212097168 0.3392142951488495
CurrentTrain: epoch  1, batch    21 | loss: 12.3783665Losses:  10.155688285827637 2.6287732124328613 0.3444734513759613
CurrentTrain: epoch  1, batch    22 | loss: 10.1556883Losses:  9.94192123413086 2.708615779876709 0.324905663728714
CurrentTrain: epoch  1, batch    23 | loss: 9.9419212Losses:  13.862100601196289 5.983994483947754 0.34211266040802
CurrentTrain: epoch  1, batch    24 | loss: 13.8621006Losses:  11.82490348815918 4.164188861846924 0.2712612748146057
CurrentTrain: epoch  1, batch    25 | loss: 11.8249035Losses:  11.64645004272461 3.3552772998809814 0.345659077167511
CurrentTrain: epoch  1, batch    26 | loss: 11.6464500Losses:  10.834031105041504 3.8368663787841797 0.3461484909057617
CurrentTrain: epoch  1, batch    27 | loss: 10.8340311Losses:  10.84561824798584 3.2643256187438965 0.3546034097671509
CurrentTrain: epoch  1, batch    28 | loss: 10.8456182Losses:  9.4119291305542 2.098094940185547 0.32919302582740784
CurrentTrain: epoch  1, batch    29 | loss: 9.4119291Losses:  10.801013946533203 3.4472222328186035 0.34663572907447815
CurrentTrain: epoch  1, batch    30 | loss: 10.8010139Losses:  13.251537322998047 5.463253498077393 0.3440154194831848
CurrentTrain: epoch  1, batch    31 | loss: 13.2515373Losses:  14.207174301147461 5.8015570640563965 0.3487420082092285
CurrentTrain: epoch  1, batch    32 | loss: 14.2071743Losses:  11.616969108581543 4.103771209716797 0.34985801577568054
CurrentTrain: epoch  1, batch    33 | loss: 11.6169691Losses:  11.507729530334473 3.5264976024627686 0.3631044626235962
CurrentTrain: epoch  1, batch    34 | loss: 11.5077295Losses:  9.907825469970703 2.984229564666748 0.32759732007980347
CurrentTrain: epoch  1, batch    35 | loss: 9.9078255Losses:  9.33674430847168 2.2054290771484375 0.3323667049407959
CurrentTrain: epoch  1, batch    36 | loss: 9.3367443Losses:  11.420454025268555 4.657068252563477 0.3439479470252991
CurrentTrain: epoch  1, batch    37 | loss: 11.4204540Losses:  16.217641830444336 8.863560676574707 0.35102200508117676
CurrentTrain: epoch  1, batch    38 | loss: 16.2176418Losses:  11.918177604675293 4.396416664123535 0.3443787693977356
CurrentTrain: epoch  1, batch    39 | loss: 11.9181776Losses:  10.422357559204102 3.394012212753296 0.35483816266059875
CurrentTrain: epoch  1, batch    40 | loss: 10.4223576Losses:  9.112815856933594 1.9852359294891357 0.330797016620636
CurrentTrain: epoch  1, batch    41 | loss: 9.1128159Losses:  13.74995231628418 5.647453784942627 0.2658999562263489
CurrentTrain: epoch  1, batch    42 | loss: 13.7499523Losses:  12.425477981567383 4.831301689147949 0.3646910786628723
CurrentTrain: epoch  1, batch    43 | loss: 12.4254780Losses:  9.82443618774414 2.5201797485351562 0.32754361629486084
CurrentTrain: epoch  1, batch    44 | loss: 9.8244362Losses:  12.646910667419434 5.440221786499023 0.3624069094657898
CurrentTrain: epoch  1, batch    45 | loss: 12.6469107Losses:  8.514137268066406 2.229860305786133 0.3220614194869995
CurrentTrain: epoch  1, batch    46 | loss: 8.5141373Losses:  9.779984474182129 3.191819190979004 0.35345038771629333
CurrentTrain: epoch  1, batch    47 | loss: 9.7799845Losses:  10.662698745727539 3.5091428756713867 0.3667929172515869
CurrentTrain: epoch  1, batch    48 | loss: 10.6626987Losses:  14.667394638061523 7.407319068908691 0.323307067155838
CurrentTrain: epoch  1, batch    49 | loss: 14.6673946Losses:  8.898026466369629 2.6165246963500977 0.31504693627357483
CurrentTrain: epoch  1, batch    50 | loss: 8.8980265Losses:  12.397359848022461 6.023298740386963 0.3209911286830902
CurrentTrain: epoch  1, batch    51 | loss: 12.3973598Losses:  9.780550956726074 3.4098281860351562 0.3336430788040161
CurrentTrain: epoch  1, batch    52 | loss: 9.7805510Losses:  10.847921371459961 3.5742688179016113 0.3281577527523041
CurrentTrain: epoch  1, batch    53 | loss: 10.8479214Losses:  9.648092269897461 3.117938995361328 0.3101619482040405
CurrentTrain: epoch  1, batch    54 | loss: 9.6480923Losses:  9.381808280944824 3.3919663429260254 0.30341631174087524
CurrentTrain: epoch  1, batch    55 | loss: 9.3818083Losses:  10.355451583862305 3.659062385559082 0.34308725595474243
CurrentTrain: epoch  1, batch    56 | loss: 10.3554516Losses:  9.07933235168457 2.4224696159362793 0.32802796363830566
CurrentTrain: epoch  1, batch    57 | loss: 9.0793324Losses:  9.797659873962402 3.347550868988037 0.33023715019226074
CurrentTrain: epoch  1, batch    58 | loss: 9.7976599Losses:  9.506564140319824 3.508146047592163 0.2993074655532837
CurrentTrain: epoch  1, batch    59 | loss: 9.5065641Losses:  10.394508361816406 3.767751455307007 0.3293169140815735
CurrentTrain: epoch  1, batch    60 | loss: 10.3945084Losses:  10.53927230834961 3.741283416748047 0.32013845443725586
CurrentTrain: epoch  1, batch    61 | loss: 10.5392723Losses:  6.687053203582764 0.47136715054512024 0.2992897629737854
CurrentTrain: epoch  1, batch    62 | loss: 6.6870532Losses:  9.633528709411621 3.4482221603393555 0.30133193731307983
CurrentTrain: epoch  2, batch     0 | loss: 9.6335287Losses:  12.079839706420898 4.332568168640137 0.3452390432357788
CurrentTrain: epoch  2, batch     1 | loss: 12.0798397Losses:  11.810247421264648 5.050928115844727 0.3048323690891266
CurrentTrain: epoch  2, batch     2 | loss: 11.8102474Losses:  9.082262992858887 2.915487289428711 0.29113197326660156
CurrentTrain: epoch  2, batch     3 | loss: 9.0822630Losses:  8.976055145263672 3.182555675506592 0.2915041148662567
CurrentTrain: epoch  2, batch     4 | loss: 8.9760551Losses:  10.19261360168457 3.148259162902832 0.2984340786933899
CurrentTrain: epoch  2, batch     5 | loss: 10.1926136Losses:  8.336813926696777 2.481625556945801 0.30323660373687744
CurrentTrain: epoch  2, batch     6 | loss: 8.3368139Losses:  8.498841285705566 2.1953611373901367 0.29711058735847473
CurrentTrain: epoch  2, batch     7 | loss: 8.4988413Losses:  9.331302642822266 3.200005054473877 0.2974015474319458
CurrentTrain: epoch  2, batch     8 | loss: 9.3313026Losses:  10.189962387084961 4.694034576416016 0.29869550466537476
CurrentTrain: epoch  2, batch     9 | loss: 10.1899624Losses:  9.91221809387207 3.5557150840759277 0.2805424630641937
CurrentTrain: epoch  2, batch    10 | loss: 9.9122181Losses:  8.788573265075684 2.7403066158294678 0.2836804687976837
CurrentTrain: epoch  2, batch    11 | loss: 8.7885733Losses:  9.76694393157959 3.8391969203948975 0.32762640714645386
CurrentTrain: epoch  2, batch    12 | loss: 9.7669439Losses:  11.85575008392334 3.898303985595703 0.30263063311576843
CurrentTrain: epoch  2, batch    13 | loss: 11.8557501Losses:  8.08317756652832 2.019954204559326 0.2956851124763489
CurrentTrain: epoch  2, batch    14 | loss: 8.0831776Losses:  10.27631950378418 4.116177558898926 0.2945409417152405
CurrentTrain: epoch  2, batch    15 | loss: 10.2763195Losses:  10.855672836303711 4.23166561126709 0.31183701753616333
CurrentTrain: epoch  2, batch    16 | loss: 10.8556728Losses:  9.198060989379883 2.9502360820770264 0.2849031686782837
CurrentTrain: epoch  2, batch    17 | loss: 9.1980610Losses:  8.708145141601562 2.231971502304077 0.28412872552871704
CurrentTrain: epoch  2, batch    18 | loss: 8.7081451Losses:  14.846541404724121 8.032722473144531 0.31687015295028687
CurrentTrain: epoch  2, batch    19 | loss: 14.8465414Losses:  9.998641967773438 3.6743109226226807 0.2790835201740265
CurrentTrain: epoch  2, batch    20 | loss: 9.9986420Losses:  10.719078063964844 4.284140110015869 0.2764008045196533
CurrentTrain: epoch  2, batch    21 | loss: 10.7190781Losses:  8.327486038208008 2.123253583908081 0.2765679359436035
CurrentTrain: epoch  2, batch    22 | loss: 8.3274860Losses:  8.093894004821777 2.433255672454834 0.26446646451950073
CurrentTrain: epoch  2, batch    23 | loss: 8.0938940Losses:  13.251260757446289 7.15875768661499 0.28373482823371887
CurrentTrain: epoch  2, batch    24 | loss: 13.2512608Losses:  9.77556037902832 3.333407402038574 0.29420995712280273
CurrentTrain: epoch  2, batch    25 | loss: 9.7755604Losses:  9.972088813781738 4.570273399353027 0.29637178778648376
CurrentTrain: epoch  2, batch    26 | loss: 9.9720888Losses:  10.778965950012207 4.936896324157715 0.29828354716300964
CurrentTrain: epoch  2, batch    27 | loss: 10.7789660Losses:  9.322675704956055 2.1095316410064697 0.2741848826408386
CurrentTrain: epoch  2, batch    28 | loss: 9.3226757Losses:  13.827425956726074 8.739099502563477 0.2772282660007477
CurrentTrain: epoch  2, batch    29 | loss: 13.8274260Losses:  10.083608627319336 4.419867992401123 0.2788558304309845
CurrentTrain: epoch  2, batch    30 | loss: 10.0836086Losses:  10.9561767578125 5.030774116516113 0.3004738688468933
CurrentTrain: epoch  2, batch    31 | loss: 10.9561768Losses:  10.042866706848145 4.388901710510254 0.2866191565990448
CurrentTrain: epoch  2, batch    32 | loss: 10.0428667Losses:  11.000584602355957 5.569736480712891 0.2944628596305847
CurrentTrain: epoch  2, batch    33 | loss: 11.0005846Losses:  11.998478889465332 5.830532550811768 0.2889814078807831
CurrentTrain: epoch  2, batch    34 | loss: 11.9984789Losses:  7.388376712799072 1.649390459060669 0.26595818996429443
CurrentTrain: epoch  2, batch    35 | loss: 7.3883767Losses:  10.697141647338867 4.858353137969971 0.28520190715789795
CurrentTrain: epoch  2, batch    36 | loss: 10.6971416Losses:  12.347926139831543 5.706859588623047 0.2710243761539459
CurrentTrain: epoch  2, batch    37 | loss: 12.3479261Losses:  10.217705726623535 4.254088401794434 0.29992610216140747
CurrentTrain: epoch  2, batch    38 | loss: 10.2177057Losses:  12.144657135009766 6.02849006652832 0.2819288372993469
CurrentTrain: epoch  2, batch    39 | loss: 12.1446571Losses:  8.292952537536621 2.536807060241699 0.27163398265838623
CurrentTrain: epoch  2, batch    40 | loss: 8.2929525Losses:  8.9649019241333 3.5251643657684326 0.29271167516708374
CurrentTrain: epoch  2, batch    41 | loss: 8.9649019Losses:  9.228453636169434 3.28671932220459 0.30626845359802246
CurrentTrain: epoch  2, batch    42 | loss: 9.2284536Losses:  10.331267356872559 4.906433582305908 0.2834348976612091
CurrentTrain: epoch  2, batch    43 | loss: 10.3312674Losses:  7.405350685119629 2.1801934242248535 0.2676827311515808
CurrentTrain: epoch  2, batch    44 | loss: 7.4053507Losses:  9.922075271606445 3.5292181968688965 0.27727484703063965
CurrentTrain: epoch  2, batch    45 | loss: 9.9220753Losses:  8.099420547485352 2.154989242553711 0.2596794366836548
CurrentTrain: epoch  2, batch    46 | loss: 8.0994205Losses:  9.507473945617676 3.443584442138672 0.28945109248161316
CurrentTrain: epoch  2, batch    47 | loss: 9.5074739Losses:  9.525806427001953 3.420523166656494 0.26548898220062256
CurrentTrain: epoch  2, batch    48 | loss: 9.5258064Losses:  8.520792007446289 2.8063228130340576 0.2742895483970642
CurrentTrain: epoch  2, batch    49 | loss: 8.5207920Losses:  10.299910545349121 4.232050895690918 0.29222527146339417
CurrentTrain: epoch  2, batch    50 | loss: 10.2999105Losses:  10.488743782043457 5.002350807189941 0.28155332803726196
CurrentTrain: epoch  2, batch    51 | loss: 10.4887438Losses:  10.398055076599121 4.827244758605957 0.27224746346473694
CurrentTrain: epoch  2, batch    52 | loss: 10.3980551Losses:  11.880338668823242 7.105377197265625 0.2698710560798645
CurrentTrain: epoch  2, batch    53 | loss: 11.8803387Losses:  8.183111190795898 2.5753707885742188 0.2956559956073761
CurrentTrain: epoch  2, batch    54 | loss: 8.1831112Losses:  9.613977432250977 3.5613973140716553 0.2729277014732361
CurrentTrain: epoch  2, batch    55 | loss: 9.6139774Losses:  9.548759460449219 3.7715976238250732 0.28047266602516174
CurrentTrain: epoch  2, batch    56 | loss: 9.5487595Losses:  7.691082954406738 2.5030651092529297 0.25729358196258545
CurrentTrain: epoch  2, batch    57 | loss: 7.6910830Losses:  9.028497695922852 3.2429606914520264 0.2738291025161743
CurrentTrain: epoch  2, batch    58 | loss: 9.0284977Losses:  8.197423934936523 2.7290163040161133 0.27345186471939087
CurrentTrain: epoch  2, batch    59 | loss: 8.1974239Losses:  7.564176559448242 2.590296745300293 0.26092463731765747
CurrentTrain: epoch  2, batch    60 | loss: 7.5641766Losses:  9.693541526794434 3.963057041168213 0.30857568979263306
CurrentTrain: epoch  2, batch    61 | loss: 9.6935415Losses:  7.545809745788574 0.3082183003425598 0.29399171471595764
CurrentTrain: epoch  2, batch    62 | loss: 7.5458097Losses:  10.216663360595703 5.432820796966553 0.19505023956298828
CurrentTrain: epoch  3, batch     0 | loss: 10.2166634Losses:  10.387228965759277 4.998818397521973 0.28423428535461426
CurrentTrain: epoch  3, batch     1 | loss: 10.3872290Losses:  7.454371929168701 2.032350540161133 0.2574332356452942
CurrentTrain: epoch  3, batch     2 | loss: 7.4543719Losses:  8.455750465393066 3.3860740661621094 0.27963337302207947
CurrentTrain: epoch  3, batch     3 | loss: 8.4557505Losses:  7.982424736022949 2.5044872760772705 0.2468670904636383
CurrentTrain: epoch  3, batch     4 | loss: 7.9824247Losses:  7.898897171020508 2.9635958671569824 0.25223928689956665
CurrentTrain: epoch  3, batch     5 | loss: 7.8988972Losses:  7.671576499938965 2.540571689605713 0.26417720317840576
CurrentTrain: epoch  3, batch     6 | loss: 7.6715765Losses:  7.9905104637146 2.958238363265991 0.2549206018447876
CurrentTrain: epoch  3, batch     7 | loss: 7.9905105Losses:  11.088988304138184 4.012667179107666 0.255083829164505
CurrentTrain: epoch  3, batch     8 | loss: 11.0889883Losses:  7.474331855773926 2.605041742324829 0.2634563148021698
CurrentTrain: epoch  3, batch     9 | loss: 7.4743319Losses:  8.692404747009277 2.8268420696258545 0.2679072320461273
CurrentTrain: epoch  3, batch    10 | loss: 8.6924047Losses:  10.20617389678955 4.903939247131348 0.2635311782360077
CurrentTrain: epoch  3, batch    11 | loss: 10.2061739Losses:  7.605480194091797 1.8689491748809814 0.2574654519557953
CurrentTrain: epoch  3, batch    12 | loss: 7.6054802Losses:  9.629902839660645 4.283867359161377 0.2751529812812805
CurrentTrain: epoch  3, batch    13 | loss: 9.6299028Losses:  7.984886169433594 2.807554244995117 0.26655566692352295
CurrentTrain: epoch  3, batch    14 | loss: 7.9848862Losses:  10.463903427124023 5.219470977783203 0.2736685872077942
CurrentTrain: epoch  3, batch    15 | loss: 10.4639034Losses:  7.561053276062012 1.8788976669311523 0.25319093465805054
CurrentTrain: epoch  3, batch    16 | loss: 7.5610533Losses:  7.179571628570557 2.042088270187378 0.24843916296958923
CurrentTrain: epoch  3, batch    17 | loss: 7.1795716Losses:  9.917596817016602 4.015129566192627 0.2684251368045807
CurrentTrain: epoch  3, batch    18 | loss: 9.9175968Losses:  7.585602760314941 2.682915449142456 0.2624233365058899
CurrentTrain: epoch  3, batch    19 | loss: 7.5856028Losses:  9.075652122497559 2.9928739070892334 0.276183545589447
CurrentTrain: epoch  3, batch    20 | loss: 9.0756521Losses:  9.08111572265625 4.193719863891602 0.17262610793113708
CurrentTrain: epoch  3, batch    21 | loss: 9.0811157Losses:  13.670980453491211 8.601335525512695 0.26712167263031006
CurrentTrain: epoch  3, batch    22 | loss: 13.6709805Losses:  8.0912504196167 2.863252639770508 0.29970014095306396
CurrentTrain: epoch  3, batch    23 | loss: 8.0912504Losses:  7.386446475982666 2.116818428039551 0.2544110417366028
CurrentTrain: epoch  3, batch    24 | loss: 7.3864465Losses:  7.909580707550049 2.7267355918884277 0.26725223660469055
CurrentTrain: epoch  3, batch    25 | loss: 7.9095807Losses:  7.4774861335754395 2.0243117809295654 0.2551018297672272
CurrentTrain: epoch  3, batch    26 | loss: 7.4774861Losses:  9.868785858154297 4.245362281799316 0.3141908645629883
CurrentTrain: epoch  3, batch    27 | loss: 9.8687859Losses:  9.994709014892578 4.752215385437012 0.29769352078437805
CurrentTrain: epoch  3, batch    28 | loss: 9.9947090Losses:  7.917335510253906 2.7884271144866943 0.27279675006866455
CurrentTrain: epoch  3, batch    29 | loss: 7.9173355Losses:  11.026957511901855 5.875000953674316 0.2835931181907654
CurrentTrain: epoch  3, batch    30 | loss: 11.0269575Losses:  7.974392414093018 2.919612169265747 0.24196398258209229
CurrentTrain: epoch  3, batch    31 | loss: 7.9743924Losses:  10.861932754516602 5.246724605560303 0.26986587047576904
CurrentTrain: epoch  3, batch    32 | loss: 10.8619328Losses:  9.579547882080078 4.116723537445068 0.29192307591438293
CurrentTrain: epoch  3, batch    33 | loss: 9.5795479Losses:  6.639550685882568 1.8082114458084106 0.2462727129459381
CurrentTrain: epoch  3, batch    34 | loss: 6.6395507Losses:  8.06709098815918 2.5603175163269043 0.2725422978401184
CurrentTrain: epoch  3, batch    35 | loss: 8.0670910Losses:  7.390264511108398 2.6427268981933594 0.24761295318603516
CurrentTrain: epoch  3, batch    36 | loss: 7.3902645Losses:  11.62037467956543 6.388885021209717 0.2867891490459442
CurrentTrain: epoch  3, batch    37 | loss: 11.6203747Losses:  9.006178855895996 3.7267234325408936 0.27665579319000244
CurrentTrain: epoch  3, batch    38 | loss: 9.0061789Losses:  10.094531059265137 5.032900810241699 0.26888197660446167
CurrentTrain: epoch  3, batch    39 | loss: 10.0945311Losses:  8.882275581359863 4.131255149841309 0.2519388794898987
CurrentTrain: epoch  3, batch    40 | loss: 8.8822756Losses:  7.553613185882568 2.3981759548187256 0.24561163783073425
CurrentTrain: epoch  3, batch    41 | loss: 7.5536132Losses:  7.788111209869385 2.6422760486602783 0.27298396825790405
CurrentTrain: epoch  3, batch    42 | loss: 7.7881112Losses:  9.252398490905762 4.234742164611816 0.27203232049942017
CurrentTrain: epoch  3, batch    43 | loss: 9.2523985Losses:  7.664976119995117 2.265523672103882 0.25704967975616455
CurrentTrain: epoch  3, batch    44 | loss: 7.6649761Losses:  6.9951605796813965 2.037370204925537 0.24591857194900513
CurrentTrain: epoch  3, batch    45 | loss: 6.9951606Losses:  8.526737213134766 3.384469509124756 0.2797892987728119
CurrentTrain: epoch  3, batch    46 | loss: 8.5267372Losses:  8.065448760986328 3.3073177337646484 0.2524108588695526
CurrentTrain: epoch  3, batch    47 | loss: 8.0654488Losses:  9.008491516113281 4.185235500335693 0.26111263036727905
CurrentTrain: epoch  3, batch    48 | loss: 9.0084915Losses:  6.995919227600098 2.1426455974578857 0.2513189911842346
CurrentTrain: epoch  3, batch    49 | loss: 6.9959192Losses:  7.004258155822754 2.207784414291382 0.2538090944290161
CurrentTrain: epoch  3, batch    50 | loss: 7.0042582Losses:  9.354517936706543 4.107975006103516 0.25091227889060974
CurrentTrain: epoch  3, batch    51 | loss: 9.3545179Losses:  8.20506763458252 2.3774826526641846 0.2480844259262085
CurrentTrain: epoch  3, batch    52 | loss: 8.2050676Losses:  11.329142570495605 5.970147609710693 0.28076452016830444
CurrentTrain: epoch  3, batch    53 | loss: 11.3291426Losses:  7.276435852050781 1.640197992324829 0.2414177805185318
CurrentTrain: epoch  3, batch    54 | loss: 7.2764359Losses:  10.300503730773926 4.850322246551514 0.27084654569625854
CurrentTrain: epoch  3, batch    55 | loss: 10.3005037Losses:  7.712272644042969 2.7166733741760254 0.23377281427383423
CurrentTrain: epoch  3, batch    56 | loss: 7.7122726Losses:  8.47524642944336 3.5861387252807617 0.2682204246520996
CurrentTrain: epoch  3, batch    57 | loss: 8.4752464Losses:  8.960041046142578 3.066922664642334 0.26184362173080444
CurrentTrain: epoch  3, batch    58 | loss: 8.9600410Losses:  10.805421829223633 4.660119533538818 0.24750861525535583
CurrentTrain: epoch  3, batch    59 | loss: 10.8054218Losses:  10.842784881591797 5.484772682189941 0.27935993671417236
CurrentTrain: epoch  3, batch    60 | loss: 10.8427849Losses:  7.178277015686035 2.0361759662628174 0.23646730184555054
CurrentTrain: epoch  3, batch    61 | loss: 7.1782770Losses:  5.313479900360107 0.8201413750648499 0.17340755462646484
CurrentTrain: epoch  3, batch    62 | loss: 5.3134799Losses:  9.925074577331543 4.785472869873047 0.1491069346666336
CurrentTrain: epoch  4, batch     0 | loss: 9.9250746Losses:  7.187894821166992 2.2178955078125 0.239793598651886
CurrentTrain: epoch  4, batch     1 | loss: 7.1878948Losses:  9.108444213867188 4.261659145355225 0.24876432120800018
CurrentTrain: epoch  4, batch     2 | loss: 9.1084442Losses:  7.234492301940918 1.9881988763809204 0.24258898198604584
CurrentTrain: epoch  4, batch     3 | loss: 7.2344923Losses:  10.738801002502441 4.505277156829834 0.24642130732536316
CurrentTrain: epoch  4, batch     4 | loss: 10.7388010Losses:  7.463890075683594 2.4147298336029053 0.24383170902729034
CurrentTrain: epoch  4, batch     5 | loss: 7.4638901Losses:  10.444852828979492 5.001331329345703 0.3266884684562683
CurrentTrain: epoch  4, batch     6 | loss: 10.4448528Losses:  7.918972492218018 3.1610708236694336 0.23943674564361572
CurrentTrain: epoch  4, batch     7 | loss: 7.9189725Losses:  7.031816482543945 2.18998384475708 0.24210408329963684
CurrentTrain: epoch  4, batch     8 | loss: 7.0318165Losses:  8.952009201049805 3.6014564037323 0.17575329542160034
CurrentTrain: epoch  4, batch     9 | loss: 8.9520092Losses:  11.276697158813477 6.29104471206665 0.253772497177124
CurrentTrain: epoch  4, batch    10 | loss: 11.2766972Losses:  9.725687980651855 4.8515472412109375 0.2544734477996826
CurrentTrain: epoch  4, batch    11 | loss: 9.7256880Losses:  8.805757522583008 3.492875576019287 0.24535733461380005
CurrentTrain: epoch  4, batch    12 | loss: 8.8057575Losses:  8.645598411560059 3.4941601753234863 0.2570871114730835
CurrentTrain: epoch  4, batch    13 | loss: 8.6455984Losses:  7.1475396156311035 2.250420570373535 0.15575727820396423
CurrentTrain: epoch  4, batch    14 | loss: 7.1475396Losses:  11.034886360168457 5.053711891174316 0.26059257984161377
CurrentTrain: epoch  4, batch    15 | loss: 11.0348864Losses:  8.077287673950195 2.869666337966919 0.2524101734161377
CurrentTrain: epoch  4, batch    16 | loss: 8.0772877Losses:  7.129438400268555 2.4160494804382324 0.27168601751327515
CurrentTrain: epoch  4, batch    17 | loss: 7.1294384Losses:  7.694037914276123 2.679443597793579 0.2479119598865509
CurrentTrain: epoch  4, batch    18 | loss: 7.6940379Losses:  7.086033821105957 2.0401813983917236 0.23244482278823853
CurrentTrain: epoch  4, batch    19 | loss: 7.0860338Losses:  7.29838228225708 2.418306589126587 0.24165482819080353
CurrentTrain: epoch  4, batch    20 | loss: 7.2983823Losses:  7.756137371063232 3.0242199897766113 0.25663456320762634
CurrentTrain: epoch  4, batch    21 | loss: 7.7561374Losses:  10.534415245056152 5.6562910079956055 0.28153514862060547
CurrentTrain: epoch  4, batch    22 | loss: 10.5344152Losses:  8.615066528320312 2.7440943717956543 0.2672847509384155
CurrentTrain: epoch  4, batch    23 | loss: 8.6150665Losses:  8.095867156982422 2.8540024757385254 0.2613639533519745
CurrentTrain: epoch  4, batch    24 | loss: 8.0958672Losses:  8.570568084716797 3.9891772270202637 0.25767087936401367
CurrentTrain: epoch  4, batch    25 | loss: 8.5705681Losses:  9.185160636901855 4.531605243682861 0.25348708033561707
CurrentTrain: epoch  4, batch    26 | loss: 9.1851606Losses:  7.665746688842773 2.8927242755889893 0.24348565936088562
CurrentTrain: epoch  4, batch    27 | loss: 7.6657467Losses:  8.093387603759766 2.7179017066955566 0.24891245365142822
CurrentTrain: epoch  4, batch    28 | loss: 8.0933876Losses:  6.992257595062256 1.9225447177886963 0.24241022765636444
CurrentTrain: epoch  4, batch    29 | loss: 6.9922576Losses:  7.328005790710449 2.6141085624694824 0.23319858312606812
CurrentTrain: epoch  4, batch    30 | loss: 7.3280058Losses:  7.39890193939209 2.3229618072509766 0.25049853324890137
CurrentTrain: epoch  4, batch    31 | loss: 7.3989019Losses:  7.3447980880737305 2.378457546234131 0.25022703409194946
CurrentTrain: epoch  4, batch    32 | loss: 7.3447981Losses:  9.441056251525879 3.8711047172546387 0.25181296467781067
CurrentTrain: epoch  4, batch    33 | loss: 9.4410563Losses:  7.442870140075684 2.4018630981445312 0.2491714060306549
CurrentTrain: epoch  4, batch    34 | loss: 7.4428701Losses:  9.801576614379883 4.48137092590332 0.22371047735214233
CurrentTrain: epoch  4, batch    35 | loss: 9.8015766Losses:  7.073550701141357 2.2945377826690674 0.2449105679988861
CurrentTrain: epoch  4, batch    36 | loss: 7.0735507Losses:  7.307727813720703 2.2686514854431152 0.23858368396759033
CurrentTrain: epoch  4, batch    37 | loss: 7.3077278Losses:  9.638459205627441 5.065721035003662 0.26173847913742065
CurrentTrain: epoch  4, batch    38 | loss: 9.6384592Losses:  8.845291137695312 3.255039930343628 0.2320556491613388
CurrentTrain: epoch  4, batch    39 | loss: 8.8452911Losses:  7.168646812438965 2.4833359718322754 0.2255469560623169
CurrentTrain: epoch  4, batch    40 | loss: 7.1686468Losses:  10.029461860656738 5.375972747802734 0.2527264356613159
CurrentTrain: epoch  4, batch    41 | loss: 10.0294619Losses:  7.559809684753418 2.5285468101501465 0.2413991093635559
CurrentTrain: epoch  4, batch    42 | loss: 7.5598097Losses:  6.498623847961426 2.0026021003723145 0.22669997811317444
CurrentTrain: epoch  4, batch    43 | loss: 6.4986238Losses:  8.024999618530273 3.118709087371826 0.2533445358276367
CurrentTrain: epoch  4, batch    44 | loss: 8.0249996Losses:  10.011034965515137 5.163911819458008 0.2814561724662781
CurrentTrain: epoch  4, batch    45 | loss: 10.0110350Losses:  8.200020790100098 3.538330554962158 0.2555962800979614
CurrentTrain: epoch  4, batch    46 | loss: 8.2000208Losses:  9.044644355773926 4.532693862915039 0.2500322461128235
CurrentTrain: epoch  4, batch    47 | loss: 9.0446444Losses:  8.485495567321777 3.862008571624756 0.23169346153736115
CurrentTrain: epoch  4, batch    48 | loss: 8.4854956Losses:  7.489683628082275 2.587092399597168 0.15061521530151367
CurrentTrain: epoch  4, batch    49 | loss: 7.4896836Losses:  7.449028015136719 2.873995780944824 0.247248575091362
CurrentTrain: epoch  4, batch    50 | loss: 7.4490280Losses:  8.509807586669922 3.690478801727295 0.22657239437103271
CurrentTrain: epoch  4, batch    51 | loss: 8.5098076Losses:  7.669246196746826 2.5239338874816895 0.23196467757225037
CurrentTrain: epoch  4, batch    52 | loss: 7.6692462Losses:  6.927036762237549 2.3161661624908447 0.2289835810661316
CurrentTrain: epoch  4, batch    53 | loss: 6.9270368Losses:  7.827017784118652 3.060030460357666 0.23070663213729858
CurrentTrain: epoch  4, batch    54 | loss: 7.8270178Losses:  9.164946556091309 4.572776794433594 0.24325767159461975
CurrentTrain: epoch  4, batch    55 | loss: 9.1649466Losses:  7.308931827545166 2.68064546585083 0.24104031920433044
CurrentTrain: epoch  4, batch    56 | loss: 7.3089318Losses:  8.61676025390625 4.0850019454956055 0.24293872714042664
CurrentTrain: epoch  4, batch    57 | loss: 8.6167603Losses:  7.321905136108398 2.6829476356506348 0.23963677883148193
CurrentTrain: epoch  4, batch    58 | loss: 7.3219051Losses:  9.614322662353516 5.019454002380371 0.24895146489143372
CurrentTrain: epoch  4, batch    59 | loss: 9.6143227Losses:  10.114078521728516 4.583207607269287 0.24776247143745422
CurrentTrain: epoch  4, batch    60 | loss: 10.1140785Losses:  7.691384315490723 3.208993911743164 0.24731478095054626
CurrentTrain: epoch  4, batch    61 | loss: 7.6913843Losses:  5.11129093170166 0.5128830671310425 0.28350529074668884
CurrentTrain: epoch  4, batch    62 | loss: 5.1112909Losses:  9.301918029785156 4.580975532531738 0.2587321400642395
CurrentTrain: epoch  5, batch     0 | loss: 9.3019180Losses:  10.194002151489258 5.638158798217773 0.2603906989097595
CurrentTrain: epoch  5, batch     1 | loss: 10.1940022Losses:  6.787399768829346 2.3266043663024902 0.24440428614616394
CurrentTrain: epoch  5, batch     2 | loss: 6.7873998Losses:  8.195229530334473 3.2887587547302246 0.24059699475765228
CurrentTrain: epoch  5, batch     3 | loss: 8.1952295Losses:  8.018922805786133 3.6561942100524902 0.25373560190200806
CurrentTrain: epoch  5, batch     4 | loss: 8.0189228Losses:  7.612296104431152 3.1175591945648193 0.23837369680404663
CurrentTrain: epoch  5, batch     5 | loss: 7.6122961Losses:  8.24750804901123 2.622732162475586 0.23013578355312347
CurrentTrain: epoch  5, batch     6 | loss: 8.2475080Losses:  8.733467102050781 3.062466859817505 0.23806138336658478
CurrentTrain: epoch  5, batch     7 | loss: 8.7334671Losses:  9.745397567749023 5.382359504699707 0.17733804881572723
CurrentTrain: epoch  5, batch     8 | loss: 9.7453976Losses:  6.700433731079102 2.2104780673980713 0.23341858386993408
CurrentTrain: epoch  5, batch     9 | loss: 6.7004337Losses:  9.224876403808594 4.535147666931152 0.16143572330474854
CurrentTrain: epoch  5, batch    10 | loss: 9.2248764Losses:  6.295307636260986 1.57387113571167 0.23005063831806183
CurrentTrain: epoch  5, batch    11 | loss: 6.2953076Losses:  5.9853515625 1.5809167623519897 0.22349673509597778
CurrentTrain: epoch  5, batch    12 | loss: 5.9853516Losses:  9.824281692504883 4.905957221984863 0.25731199979782104
CurrentTrain: epoch  5, batch    13 | loss: 9.8242817Losses:  7.383909225463867 2.95563006401062 0.23128312826156616
CurrentTrain: epoch  5, batch    14 | loss: 7.3839092Losses:  7.894630432128906 3.123640537261963 0.2539762556552887
CurrentTrain: epoch  5, batch    15 | loss: 7.8946304Losses:  7.010725975036621 2.596111536026001 0.24066339433193207
CurrentTrain: epoch  5, batch    16 | loss: 7.0107260Losses:  10.675445556640625 5.088150978088379 0.2522956132888794
CurrentTrain: epoch  5, batch    17 | loss: 10.6754456Losses:  8.461891174316406 3.85703182220459 0.24430084228515625
CurrentTrain: epoch  5, batch    18 | loss: 8.4618912Losses:  9.5495023727417 5.100454330444336 0.23852290213108063
CurrentTrain: epoch  5, batch    19 | loss: 9.5495024Losses:  8.925771713256836 3.995415210723877 0.2619004249572754
CurrentTrain: epoch  5, batch    20 | loss: 8.9257717Losses:  11.552392959594727 7.144326686859131 0.24487850069999695
CurrentTrain: epoch  5, batch    21 | loss: 11.5523930Losses:  8.376054763793945 4.077177047729492 0.16975390911102295
CurrentTrain: epoch  5, batch    22 | loss: 8.3760548Losses:  6.944052696228027 2.5257387161254883 0.24052676558494568
CurrentTrain: epoch  5, batch    23 | loss: 6.9440527Losses:  14.730896949768066 9.854154586791992 0.18177065253257751
CurrentTrain: epoch  5, batch    24 | loss: 14.7308969Losses:  8.914417266845703 3.8714406490325928 0.25990408658981323
CurrentTrain: epoch  5, batch    25 | loss: 8.9144173Losses:  8.474818229675293 3.9935784339904785 0.24675709009170532
CurrentTrain: epoch  5, batch    26 | loss: 8.4748182Losses:  6.565845966339111 2.1175127029418945 0.23938554525375366
CurrentTrain: epoch  5, batch    27 | loss: 6.5658460Losses:  6.840615272521973 2.358640432357788 0.2320602536201477
CurrentTrain: epoch  5, batch    28 | loss: 6.8406153Losses:  6.934050559997559 1.9759204387664795 0.22117340564727783
CurrentTrain: epoch  5, batch    29 | loss: 6.9340506Losses:  7.833707809448242 3.382249593734741 0.246354341506958
CurrentTrain: epoch  5, batch    30 | loss: 7.8337078Losses:  8.487160682678223 4.071014881134033 0.24746763706207275
CurrentTrain: epoch  5, batch    31 | loss: 8.4871607Losses:  11.140495300292969 6.697957515716553 0.26698657870292664
CurrentTrain: epoch  5, batch    32 | loss: 11.1404953Losses:  8.402251243591309 3.5458602905273438 0.25216901302337646
CurrentTrain: epoch  5, batch    33 | loss: 8.4022512Losses:  10.303400039672852 5.784635543823242 0.1579992175102234
CurrentTrain: epoch  5, batch    34 | loss: 10.3034000Losses:  8.196269989013672 3.3447320461273193 0.22947418689727783
CurrentTrain: epoch  5, batch    35 | loss: 8.1962700Losses:  11.03404426574707 6.523962020874023 0.23740923404693604
CurrentTrain: epoch  5, batch    36 | loss: 11.0340443Losses:  8.57691764831543 4.024596691131592 0.25461405515670776
CurrentTrain: epoch  5, batch    37 | loss: 8.5769176Losses:  7.160829067230225 2.7339224815368652 0.25770294666290283
CurrentTrain: epoch  5, batch    38 | loss: 7.1608291Losses:  8.770925521850586 3.5226645469665527 0.2383728325366974
CurrentTrain: epoch  5, batch    39 | loss: 8.7709255Losses:  6.295741081237793 1.879288673400879 0.2266782522201538
CurrentTrain: epoch  5, batch    40 | loss: 6.2957411Losses:  6.634191036224365 2.226348876953125 0.2285524308681488
CurrentTrain: epoch  5, batch    41 | loss: 6.6341910Losses:  9.29177474975586 4.194311141967773 0.2537054419517517
CurrentTrain: epoch  5, batch    42 | loss: 9.2917747Losses:  8.71572494506836 4.20719575881958 0.22746089100837708
CurrentTrain: epoch  5, batch    43 | loss: 8.7157249Losses:  9.084393501281738 4.434506416320801 0.22989004850387573
CurrentTrain: epoch  5, batch    44 | loss: 9.0843935Losses:  7.244808673858643 2.7714593410491943 0.22969874739646912
CurrentTrain: epoch  5, batch    45 | loss: 7.2448087Losses:  7.142095565795898 2.672945022583008 0.2275020182132721
CurrentTrain: epoch  5, batch    46 | loss: 7.1420956Losses:  6.78842306137085 2.494927406311035 0.23891887068748474
CurrentTrain: epoch  5, batch    47 | loss: 6.7884231Losses:  6.979974269866943 2.197880268096924 0.2362174689769745
CurrentTrain: epoch  5, batch    48 | loss: 6.9799743Losses:  6.991400241851807 2.6213538646698 0.22236508131027222
CurrentTrain: epoch  5, batch    49 | loss: 6.9914002Losses:  9.704097747802734 5.248463153839111 0.26007145643234253
CurrentTrain: epoch  5, batch    50 | loss: 9.7040977Losses:  6.382155418395996 2.0659537315368652 0.2241886407136917
CurrentTrain: epoch  5, batch    51 | loss: 6.3821554Losses:  10.864013671875 6.44054651260376 0.2566586434841156
CurrentTrain: epoch  5, batch    52 | loss: 10.8640137Losses:  6.16306734085083 1.7442071437835693 0.21865686774253845
CurrentTrain: epoch  5, batch    53 | loss: 6.1630673Losses:  7.567477226257324 2.9622933864593506 0.24329471588134766
CurrentTrain: epoch  5, batch    54 | loss: 7.5674772Losses:  7.880955219268799 3.4369053840637207 0.22906294465065002
CurrentTrain: epoch  5, batch    55 | loss: 7.8809552Losses:  6.867999076843262 2.5387580394744873 0.22805190086364746
CurrentTrain: epoch  5, batch    56 | loss: 6.8679991Losses:  6.315886497497559 1.9171535968780518 0.22841285169124603
CurrentTrain: epoch  5, batch    57 | loss: 6.3158865Losses:  7.085069179534912 2.7564196586608887 0.22920134663581848
CurrentTrain: epoch  5, batch    58 | loss: 7.0850692Losses:  6.528429985046387 2.161104917526245 0.23202303051948547
CurrentTrain: epoch  5, batch    59 | loss: 6.5284300Losses:  9.0130615234375 4.639105796813965 0.25693464279174805
CurrentTrain: epoch  5, batch    60 | loss: 9.0130615Losses:  7.498395919799805 3.114236354827881 0.23176027834415436
CurrentTrain: epoch  5, batch    61 | loss: 7.4983959Losses:  5.201864242553711 0.7191851139068604 0.18065916001796722
CurrentTrain: epoch  5, batch    62 | loss: 5.2018642Losses:  10.317848205566406 5.923418998718262 0.2352408468723297
CurrentTrain: epoch  6, batch     0 | loss: 10.3178482Losses:  6.5534443855285645 2.2302398681640625 0.2378067672252655
CurrentTrain: epoch  6, batch     1 | loss: 6.5534444Losses:  6.814577579498291 2.291510581970215 0.23602375388145447
CurrentTrain: epoch  6, batch     2 | loss: 6.8145776Losses:  7.421264171600342 3.1106834411621094 0.2314087450504303
CurrentTrain: epoch  6, batch     3 | loss: 7.4212642Losses:  6.640875816345215 2.226119041442871 0.23576302826404572
CurrentTrain: epoch  6, batch     4 | loss: 6.6408758Losses:  7.17336893081665 2.781735420227051 0.2393007129430771
CurrentTrain: epoch  6, batch     5 | loss: 7.1733689Losses:  8.140578269958496 3.8104751110076904 0.24348528683185577
CurrentTrain: epoch  6, batch     6 | loss: 8.1405783Losses:  7.634971618652344 3.2056338787078857 0.24147725105285645
CurrentTrain: epoch  6, batch     7 | loss: 7.6349716Losses:  7.627092361450195 3.32098388671875 0.24723055958747864
CurrentTrain: epoch  6, batch     8 | loss: 7.6270924Losses:  6.682565212249756 2.2931389808654785 0.22363556921482086
CurrentTrain: epoch  6, batch     9 | loss: 6.6825652Losses:  8.311206817626953 3.0849828720092773 0.239275723695755
CurrentTrain: epoch  6, batch    10 | loss: 8.3112068Losses:  8.65622329711914 4.222721099853516 0.23043766617774963
CurrentTrain: epoch  6, batch    11 | loss: 8.6562233Losses:  6.176024436950684 1.873741865158081 0.22228331863880157
CurrentTrain: epoch  6, batch    12 | loss: 6.1760244Losses:  9.332289695739746 4.109238624572754 0.22483977675437927
CurrentTrain: epoch  6, batch    13 | loss: 9.3322897Losses:  7.809259414672852 3.3684144020080566 0.2423921674489975
CurrentTrain: epoch  6, batch    14 | loss: 7.8092594Losses:  7.9295172691345215 3.07761287689209 0.23023538291454315
CurrentTrain: epoch  6, batch    15 | loss: 7.9295173Losses:  6.621633052825928 2.163520097732544 0.22152739763259888
CurrentTrain: epoch  6, batch    16 | loss: 6.6216331Losses:  6.4186906814575195 2.011331558227539 0.2147637903690338
CurrentTrain: epoch  6, batch    17 | loss: 6.4186907Losses:  7.091952323913574 2.7400197982788086 0.24093171954154968
CurrentTrain: epoch  6, batch    18 | loss: 7.0919523Losses:  10.875378608703613 6.489993095397949 0.25137683749198914
CurrentTrain: epoch  6, batch    19 | loss: 10.8753786Losses:  7.00698184967041 2.3916051387786865 0.21645236015319824
CurrentTrain: epoch  6, batch    20 | loss: 7.0069818Losses:  8.055925369262695 3.362123966217041 0.25747665762901306
CurrentTrain: epoch  6, batch    21 | loss: 8.0559254Losses:  7.615385055541992 2.8923707008361816 0.2381439357995987
CurrentTrain: epoch  6, batch    22 | loss: 7.6153851Losses:  8.413689613342285 3.7566895484924316 0.24555131793022156
CurrentTrain: epoch  6, batch    23 | loss: 8.4136896Losses:  6.833512306213379 2.189128875732422 0.2220870852470398
CurrentTrain: epoch  6, batch    24 | loss: 6.8335123Losses:  7.751276969909668 3.202085494995117 0.23172996938228607
CurrentTrain: epoch  6, batch    25 | loss: 7.7512770Losses:  7.938673973083496 3.588210344314575 0.227145254611969
CurrentTrain: epoch  6, batch    26 | loss: 7.9386740Losses:  7.075528144836426 2.427809238433838 0.223170667886734
CurrentTrain: epoch  6, batch    27 | loss: 7.0755281Losses:  7.333558559417725 2.6602275371551514 0.23481324315071106
CurrentTrain: epoch  6, batch    28 | loss: 7.3335586Losses:  7.957825660705566 3.2729954719543457 0.234923854470253
CurrentTrain: epoch  6, batch    29 | loss: 7.9578257Losses:  7.479933738708496 3.0536136627197266 0.22779260575771332
CurrentTrain: epoch  6, batch    30 | loss: 7.4799337Losses:  6.669921875 2.265097141265869 0.22487598657608032
CurrentTrain: epoch  6, batch    31 | loss: 6.6699219Losses:  6.745349407196045 2.2564687728881836 0.22043287754058838
CurrentTrain: epoch  6, batch    32 | loss: 6.7453494Losses:  7.894957542419434 3.5090789794921875 0.2436513900756836
CurrentTrain: epoch  6, batch    33 | loss: 7.8949575Losses:  7.6857476234436035 3.1621923446655273 0.24116244912147522
CurrentTrain: epoch  6, batch    34 | loss: 7.6857476Losses:  6.5333638191223145 2.1878867149353027 0.23761005699634552
CurrentTrain: epoch  6, batch    35 | loss: 6.5333638Losses:  6.477046489715576 2.0721969604492188 0.22693687677383423
CurrentTrain: epoch  6, batch    36 | loss: 6.4770465Losses:  10.312617301940918 5.878476619720459 0.24440206587314606
CurrentTrain: epoch  6, batch    37 | loss: 10.3126173Losses:  8.950909614562988 4.375609397888184 0.2446943074464798
CurrentTrain: epoch  6, batch    38 | loss: 8.9509096Losses:  7.087732791900635 2.6079463958740234 0.24506989121437073
CurrentTrain: epoch  6, batch    39 | loss: 7.0877328Losses:  7.520906925201416 2.1216018199920654 0.21199855208396912
CurrentTrain: epoch  6, batch    40 | loss: 7.5209069Losses:  6.708720684051514 2.4211652278900146 0.23273319005966187
CurrentTrain: epoch  6, batch    41 | loss: 6.7087207Losses:  8.169609069824219 3.8856494426727295 0.2404724806547165
CurrentTrain: epoch  6, batch    42 | loss: 8.1696091Losses:  7.7246413230896 3.2430858612060547 0.24117699265480042
CurrentTrain: epoch  6, batch    43 | loss: 7.7246413Losses:  8.154786109924316 3.0597925186157227 0.22441630065441132
CurrentTrain: epoch  6, batch    44 | loss: 8.1547861Losses:  6.915133476257324 2.585688352584839 0.22055132687091827
CurrentTrain: epoch  6, batch    45 | loss: 6.9151335Losses:  9.56051254272461 4.62722110748291 0.25393185019493103
CurrentTrain: epoch  6, batch    46 | loss: 9.5605125Losses:  8.568920135498047 4.197939395904541 0.24706855416297913
CurrentTrain: epoch  6, batch    47 | loss: 8.5689201Losses:  8.183343887329102 3.2906675338745117 0.24006402492523193
CurrentTrain: epoch  6, batch    48 | loss: 8.1833439Losses:  8.285530090332031 3.211432456970215 0.23465599119663239
CurrentTrain: epoch  6, batch    49 | loss: 8.2855301Losses:  7.833285808563232 3.105288505554199 0.15078124403953552
CurrentTrain: epoch  6, batch    50 | loss: 7.8332858Losses:  7.3390889167785645 3.1522257328033447 0.22990216314792633
CurrentTrain: epoch  6, batch    51 | loss: 7.3390889Losses:  7.797369480133057 3.533740997314453 0.2568061947822571
CurrentTrain: epoch  6, batch    52 | loss: 7.7973695Losses:  6.952418327331543 2.3252947330474854 0.22872070968151093
CurrentTrain: epoch  6, batch    53 | loss: 6.9524183Losses:  8.229822158813477 3.0538692474365234 0.2208293378353119
CurrentTrain: epoch  6, batch    54 | loss: 8.2298222Losses:  8.77011489868164 4.456863880157471 0.24569721519947052
CurrentTrain: epoch  6, batch    55 | loss: 8.7701149Losses:  6.8597493171691895 2.4995791912078857 0.2360750138759613
CurrentTrain: epoch  6, batch    56 | loss: 6.8597493Losses:  7.686614990234375 2.9167542457580566 0.24511364102363586
CurrentTrain: epoch  6, batch    57 | loss: 7.6866150Losses:  6.529585838317871 2.14385724067688 0.21817393600940704
CurrentTrain: epoch  6, batch    58 | loss: 6.5295858Losses:  9.531671524047852 4.974748134613037 0.25709518790245056
CurrentTrain: epoch  6, batch    59 | loss: 9.5316715Losses:  6.379110813140869 1.898900032043457 0.2310892641544342
CurrentTrain: epoch  6, batch    60 | loss: 6.3791108Losses:  6.339139938354492 1.861431360244751 0.22901621460914612
CurrentTrain: epoch  6, batch    61 | loss: 6.3391399Losses:  4.7453293800354 0.21492820978164673 0.1563969999551773
CurrentTrain: epoch  6, batch    62 | loss: 4.7453294Losses:  8.969764709472656 4.519937515258789 0.2526986300945282
CurrentTrain: epoch  7, batch     0 | loss: 8.9697647Losses:  7.640215873718262 3.1343021392822266 0.22911150753498077
CurrentTrain: epoch  7, batch     1 | loss: 7.6402159Losses:  8.369251251220703 3.426344871520996 0.24435777962207794
CurrentTrain: epoch  7, batch     2 | loss: 8.3692513Losses:  10.8377046585083 6.607374668121338 0.17580276727676392
CurrentTrain: epoch  7, batch     3 | loss: 10.8377047Losses:  7.5903215408325195 3.2773046493530273 0.22465142607688904
CurrentTrain: epoch  7, batch     4 | loss: 7.5903215Losses:  6.130168914794922 1.8028745651245117 0.22059617936611176
CurrentTrain: epoch  7, batch     5 | loss: 6.1301689Losses:  7.573222637176514 2.80462646484375 0.23666732013225555
CurrentTrain: epoch  7, batch     6 | loss: 7.5732226Losses:  7.480035781860352 3.0117743015289307 0.22683313488960266
CurrentTrain: epoch  7, batch     7 | loss: 7.4800358Losses:  7.176672458648682 2.8609509468078613 0.23659417033195496
CurrentTrain: epoch  7, batch     8 | loss: 7.1766725Losses:  8.09133529663086 3.771780252456665 0.2540685832500458
CurrentTrain: epoch  7, batch     9 | loss: 8.0913353Losses:  7.514147758483887 3.051223039627075 0.2260606437921524
CurrentTrain: epoch  7, batch    10 | loss: 7.5141478Losses:  8.14525032043457 3.851806163787842 0.15575425326824188
CurrentTrain: epoch  7, batch    11 | loss: 8.1452503Losses:  7.210461139678955 2.8627634048461914 0.24137341976165771
CurrentTrain: epoch  7, batch    12 | loss: 7.2104611Losses:  6.217743873596191 1.8613173961639404 0.23122891783714294
CurrentTrain: epoch  7, batch    13 | loss: 6.2177439Losses:  7.853230953216553 3.5335159301757812 0.23318515717983246
CurrentTrain: epoch  7, batch    14 | loss: 7.8532310Losses:  8.637369155883789 4.399965763092041 0.23904046416282654
CurrentTrain: epoch  7, batch    15 | loss: 8.6373692Losses:  9.004039764404297 4.659409523010254 0.24608206748962402
CurrentTrain: epoch  7, batch    16 | loss: 9.0040398Losses:  6.479227066040039 2.1503140926361084 0.23031409084796906
CurrentTrain: epoch  7, batch    17 | loss: 6.4792271Losses:  9.343472480773926 3.7912213802337646 0.25050675868988037
CurrentTrain: epoch  7, batch    18 | loss: 9.3434725Losses:  6.985324382781982 2.6910126209259033 0.23715174198150635
CurrentTrain: epoch  7, batch    19 | loss: 6.9853244Losses:  6.571070671081543 2.1770615577697754 0.2312791347503662
CurrentTrain: epoch  7, batch    20 | loss: 6.5710707Losses:  8.623373985290527 4.440887451171875 0.2368820309638977
CurrentTrain: epoch  7, batch    21 | loss: 8.6233740Losses:  7.077444553375244 2.741337776184082 0.2273031622171402
CurrentTrain: epoch  7, batch    22 | loss: 7.0774446Losses:  7.3054118156433105 2.649055004119873 0.23140749335289001
CurrentTrain: epoch  7, batch    23 | loss: 7.3054118Losses:  8.154632568359375 3.891737937927246 0.24026323854923248
CurrentTrain: epoch  7, batch    24 | loss: 8.1546326Losses:  7.6819586753845215 3.1952242851257324 0.24596340954303741
CurrentTrain: epoch  7, batch    25 | loss: 7.6819587Losses:  7.008858680725098 2.5926437377929688 0.22980688512325287
CurrentTrain: epoch  7, batch    26 | loss: 7.0088587Losses:  8.111159324645996 3.7039642333984375 0.24111174046993256
CurrentTrain: epoch  7, batch    27 | loss: 8.1111593Losses:  6.590244770050049 2.1731674671173096 0.22134622931480408
CurrentTrain: epoch  7, batch    28 | loss: 6.5902448Losses:  7.115865230560303 2.788883686065674 0.2403489351272583
CurrentTrain: epoch  7, batch    29 | loss: 7.1158652Losses:  10.037951469421387 5.185604095458984 0.24018044769763947
CurrentTrain: epoch  7, batch    30 | loss: 10.0379515Losses:  7.798831939697266 3.481612205505371 0.23767313361167908
CurrentTrain: epoch  7, batch    31 | loss: 7.7988319Losses:  6.6996307373046875 2.4479994773864746 0.23215961456298828
CurrentTrain: epoch  7, batch    32 | loss: 6.6996307Losses:  6.835257530212402 2.508774518966675 0.23316025733947754
CurrentTrain: epoch  7, batch    33 | loss: 6.8352575Losses:  6.253541946411133 1.9425281286239624 0.21948370337486267
CurrentTrain: epoch  7, batch    34 | loss: 6.2535419Losses:  7.662563323974609 3.3354134559631348 0.2306222915649414
CurrentTrain: epoch  7, batch    35 | loss: 7.6625633Losses:  7.89333438873291 3.520036458969116 0.23603244125843048
CurrentTrain: epoch  7, batch    36 | loss: 7.8933344Losses:  7.516672611236572 3.242799758911133 0.23255056142807007
CurrentTrain: epoch  7, batch    37 | loss: 7.5166726Losses:  8.244890213012695 3.982346296310425 0.2331421673297882
CurrentTrain: epoch  7, batch    38 | loss: 8.2448902Losses:  6.311456203460693 1.9009760618209839 0.22255359590053558
CurrentTrain: epoch  7, batch    39 | loss: 6.3114562Losses:  6.488997936248779 2.225236415863037 0.22138965129852295
CurrentTrain: epoch  7, batch    40 | loss: 6.4889979Losses:  10.101253509521484 5.913345813751221 0.2581976056098938
CurrentTrain: epoch  7, batch    41 | loss: 10.1012535Losses:  7.0946807861328125 2.7301673889160156 0.2275402545928955
CurrentTrain: epoch  7, batch    42 | loss: 7.0946808Losses:  8.539743423461914 4.247817039489746 0.25093600153923035
CurrentTrain: epoch  7, batch    43 | loss: 8.5397434Losses:  7.404993057250977 2.086165428161621 0.21974892914295197
CurrentTrain: epoch  7, batch    44 | loss: 7.4049931Losses:  6.907042026519775 2.203800678253174 0.2146892249584198
CurrentTrain: epoch  7, batch    45 | loss: 6.9070420Losses:  7.029560089111328 2.6235594749450684 0.22111369669437408
CurrentTrain: epoch  7, batch    46 | loss: 7.0295601Losses:  6.504645347595215 2.133424997329712 0.22425483167171478
CurrentTrain: epoch  7, batch    47 | loss: 6.5046453Losses:  8.233531951904297 3.9924724102020264 0.24476414918899536
CurrentTrain: epoch  7, batch    48 | loss: 8.2335320Losses:  6.498680591583252 2.2262814044952393 0.22281961143016815
CurrentTrain: epoch  7, batch    49 | loss: 6.4986806Losses:  7.535122394561768 2.686906337738037 0.223796546459198
CurrentTrain: epoch  7, batch    50 | loss: 7.5351224Losses:  6.440307140350342 2.1264944076538086 0.22387826442718506
CurrentTrain: epoch  7, batch    51 | loss: 6.4403071Losses:  6.918022632598877 2.153799533843994 0.22372521460056305
CurrentTrain: epoch  7, batch    52 | loss: 6.9180226Losses:  9.409126281738281 5.137351036071777 0.23665273189544678
CurrentTrain: epoch  7, batch    53 | loss: 9.4091263Losses:  7.289872169494629 2.957662343978882 0.23506459593772888
CurrentTrain: epoch  7, batch    54 | loss: 7.2898722Losses:  6.640482425689697 1.986398696899414 0.2090713232755661
CurrentTrain: epoch  7, batch    55 | loss: 6.6404824Losses:  10.149894714355469 5.747623920440674 0.24086344242095947
CurrentTrain: epoch  7, batch    56 | loss: 10.1498947Losses:  6.587749004364014 2.170212745666504 0.23423893749713898
CurrentTrain: epoch  7, batch    57 | loss: 6.5877490Losses:  6.473793029785156 1.815106987953186 0.22659069299697876
CurrentTrain: epoch  7, batch    58 | loss: 6.4737930Losses:  6.4196062088012695 2.1846156120300293 0.21480710804462433
CurrentTrain: epoch  7, batch    59 | loss: 6.4196062Losses:  6.6963934898376465 2.3154165744781494 0.22547870874404907
CurrentTrain: epoch  7, batch    60 | loss: 6.6963935Losses:  7.056440353393555 2.7489843368530273 0.25764936208724976
CurrentTrain: epoch  7, batch    61 | loss: 7.0564404Losses:  4.784910202026367 0.4567083418369293 0.26592111587524414
CurrentTrain: epoch  7, batch    62 | loss: 4.7849102Losses:  6.112143516540527 1.8378820419311523 0.21777990460395813
CurrentTrain: epoch  8, batch     0 | loss: 6.1121435Losses:  6.6937360763549805 2.3602304458618164 0.23587697744369507
CurrentTrain: epoch  8, batch     1 | loss: 6.6937361Losses:  9.949990272521973 5.376081466674805 0.1525014340877533
CurrentTrain: epoch  8, batch     2 | loss: 9.9499903Losses:  8.867341995239258 4.657451629638672 0.23001950979232788
CurrentTrain: epoch  8, batch     3 | loss: 8.8673420Losses:  6.372020244598389 2.112224817276001 0.2244841456413269
CurrentTrain: epoch  8, batch     4 | loss: 6.3720202Losses:  9.404426574707031 5.059497833251953 0.14824479818344116
CurrentTrain: epoch  8, batch     5 | loss: 9.4044266Losses:  7.428099155426025 3.1691884994506836 0.23782502114772797
CurrentTrain: epoch  8, batch     6 | loss: 7.4280992Losses:  6.502353668212891 2.2817258834838867 0.22153477370738983
CurrentTrain: epoch  8, batch     7 | loss: 6.5023537Losses:  6.955373764038086 2.6118552684783936 0.23844599723815918
CurrentTrain: epoch  8, batch     8 | loss: 6.9553738Losses:  8.354342460632324 4.079105854034424 0.23443280160427094
CurrentTrain: epoch  8, batch     9 | loss: 8.3543425Losses:  7.135116100311279 2.902493476867676 0.2244504988193512
CurrentTrain: epoch  8, batch    10 | loss: 7.1351161Losses:  7.224618911743164 2.9252588748931885 0.2234725058078766
CurrentTrain: epoch  8, batch    11 | loss: 7.2246189Losses:  7.944150924682617 3.5071470737457275 0.24098604917526245
CurrentTrain: epoch  8, batch    12 | loss: 7.9441509Losses:  7.578554153442383 3.3081045150756836 0.23371915519237518
CurrentTrain: epoch  8, batch    13 | loss: 7.5785542Losses:  8.000082015991211 3.6856889724731445 0.25419044494628906
CurrentTrain: epoch  8, batch    14 | loss: 8.0000820Losses:  7.786285877227783 3.5179097652435303 0.24739620089530945
CurrentTrain: epoch  8, batch    15 | loss: 7.7862859Losses:  10.755819320678711 6.390596389770508 0.25856977701187134
CurrentTrain: epoch  8, batch    16 | loss: 10.7558193Losses:  6.481395244598389 2.1736631393432617 0.22865363955497742
CurrentTrain: epoch  8, batch    17 | loss: 6.4813952Losses:  8.005986213684082 3.6946704387664795 0.2258894145488739
CurrentTrain: epoch  8, batch    18 | loss: 8.0059862Losses:  7.357519149780273 3.157870054244995 0.22255448997020721
CurrentTrain: epoch  8, batch    19 | loss: 7.3575191Losses:  7.55540657043457 3.274526357650757 0.2430942952632904
CurrentTrain: epoch  8, batch    20 | loss: 7.5554066Losses:  6.529505252838135 2.2406609058380127 0.2211262583732605
CurrentTrain: epoch  8, batch    21 | loss: 6.5295053Losses:  9.799466133117676 5.620835781097412 0.2440091371536255
CurrentTrain: epoch  8, batch    22 | loss: 9.7994661Losses:  8.859089851379395 4.626428604125977 0.25179365277290344
CurrentTrain: epoch  8, batch    23 | loss: 8.8590899Losses:  7.2288312911987305 2.966028928756714 0.2279961109161377
CurrentTrain: epoch  8, batch    24 | loss: 7.2288313Losses:  6.56437349319458 2.308443069458008 0.22694620490074158
CurrentTrain: epoch  8, batch    25 | loss: 6.5643735Losses:  6.35737419128418 2.148655414581299 0.22462652623653412
CurrentTrain: epoch  8, batch    26 | loss: 6.3573742Losses:  8.200206756591797 3.821643829345703 0.22674430906772614
CurrentTrain: epoch  8, batch    27 | loss: 8.2002068Losses:  6.48071813583374 2.22414493560791 0.22758431732654572
CurrentTrain: epoch  8, batch    28 | loss: 6.4807181Losses:  8.37008285522461 4.068912029266357 0.23246583342552185
CurrentTrain: epoch  8, batch    29 | loss: 8.3700829Losses:  6.446876525878906 2.1771140098571777 0.22460997104644775
CurrentTrain: epoch  8, batch    30 | loss: 6.4468765Losses:  6.712543964385986 2.4276299476623535 0.23552125692367554
CurrentTrain: epoch  8, batch    31 | loss: 6.7125440Losses:  7.187525749206543 2.8998987674713135 0.22797396779060364
CurrentTrain: epoch  8, batch    32 | loss: 7.1875257Losses:  8.272411346435547 4.091980457305908 0.15691399574279785
CurrentTrain: epoch  8, batch    33 | loss: 8.2724113Losses:  9.070557594299316 4.815451622009277 0.22755950689315796
CurrentTrain: epoch  8, batch    34 | loss: 9.0705576Losses:  7.485128402709961 3.2212395668029785 0.23496118187904358
CurrentTrain: epoch  8, batch    35 | loss: 7.4851284Losses:  6.891798496246338 2.721280336380005 0.23000004887580872
CurrentTrain: epoch  8, batch    36 | loss: 6.8917985Losses:  9.543354988098145 5.428796768188477 0.14598338305950165
CurrentTrain: epoch  8, batch    37 | loss: 9.5433550Losses:  6.724600315093994 2.370887279510498 0.23122164607048035
CurrentTrain: epoch  8, batch    38 | loss: 6.7246003Losses:  6.209567070007324 1.8407728672027588 0.2273167073726654
CurrentTrain: epoch  8, batch    39 | loss: 6.2095671Losses:  6.816397190093994 2.6076064109802246 0.22094827890396118
CurrentTrain: epoch  8, batch    40 | loss: 6.8163972Losses:  6.971322536468506 2.7744736671447754 0.22817935049533844
CurrentTrain: epoch  8, batch    41 | loss: 6.9713225Losses:  7.128718376159668 2.8675436973571777 0.23091772198677063
CurrentTrain: epoch  8, batch    42 | loss: 7.1287184Losses:  7.697579383850098 3.3866167068481445 0.23603177070617676
CurrentTrain: epoch  8, batch    43 | loss: 7.6975794Losses:  6.962510585784912 2.740938663482666 0.23938268423080444
CurrentTrain: epoch  8, batch    44 | loss: 6.9625106Losses:  8.24401569366455 3.9967756271362305 0.23219715058803558
CurrentTrain: epoch  8, batch    45 | loss: 8.2440157Losses:  7.179084777832031 2.9376142024993896 0.2302408516407013
CurrentTrain: epoch  8, batch    46 | loss: 7.1790848Losses:  8.251110076904297 3.9750216007232666 0.23732037842273712
CurrentTrain: epoch  8, batch    47 | loss: 8.2511101Losses:  6.994518756866455 2.7593839168548584 0.22853785753250122
CurrentTrain: epoch  8, batch    48 | loss: 6.9945188Losses:  6.9911346435546875 2.717895030975342 0.22746455669403076
CurrentTrain: epoch  8, batch    49 | loss: 6.9911346Losses:  7.394732475280762 3.221280574798584 0.22885474562644958
CurrentTrain: epoch  8, batch    50 | loss: 7.3947325Losses:  6.30550479888916 2.084880828857422 0.22400802373886108
CurrentTrain: epoch  8, batch    51 | loss: 6.3055048Losses:  6.60701322555542 2.3938066959381104 0.23377159237861633
CurrentTrain: epoch  8, batch    52 | loss: 6.6070132Losses:  6.254728317260742 2.0121757984161377 0.22927848994731903
CurrentTrain: epoch  8, batch    53 | loss: 6.2547283Losses:  7.095806121826172 2.904362678527832 0.22787465155124664
CurrentTrain: epoch  8, batch    54 | loss: 7.0958061Losses:  9.714061737060547 5.510061264038086 0.21648432314395905
CurrentTrain: epoch  8, batch    55 | loss: 9.7140617Losses:  6.23898458480835 2.0091347694396973 0.22640497982501984
CurrentTrain: epoch  8, batch    56 | loss: 6.2389846Losses:  8.371292114257812 4.181089401245117 0.2352008819580078
CurrentTrain: epoch  8, batch    57 | loss: 8.3712921Losses:  6.932239055633545 2.7339932918548584 0.22483304142951965
CurrentTrain: epoch  8, batch    58 | loss: 6.9322391Losses:  6.816436767578125 2.5907673835754395 0.22339816391468048
CurrentTrain: epoch  8, batch    59 | loss: 6.8164368Losses:  6.850646018981934 2.665231227874756 0.22503304481506348
CurrentTrain: epoch  8, batch    60 | loss: 6.8506460Losses:  6.981234550476074 2.7703542709350586 0.22658729553222656
CurrentTrain: epoch  8, batch    61 | loss: 6.9812346Losses:  4.555062770843506 0.4651999771595001 0.16163502633571625
CurrentTrain: epoch  8, batch    62 | loss: 4.5550628Losses:  6.293641090393066 2.1237683296203613 0.2183074951171875
CurrentTrain: epoch  9, batch     0 | loss: 6.2936411Losses:  7.912055015563965 3.6844429969787598 0.237742617726326
CurrentTrain: epoch  9, batch     1 | loss: 7.9120550Losses:  5.402519226074219 1.2136316299438477 0.2053905874490738
CurrentTrain: epoch  9, batch     2 | loss: 5.4025192Losses:  8.092350006103516 3.872084617614746 0.22969681024551392
CurrentTrain: epoch  9, batch     3 | loss: 8.0923500Losses:  6.349758148193359 2.1754441261291504 0.2190535068511963
CurrentTrain: epoch  9, batch     4 | loss: 6.3497581Losses:  6.926171779632568 2.6964221000671387 0.22096294164657593
CurrentTrain: epoch  9, batch     5 | loss: 6.9261718Losses:  6.529402256011963 2.215461254119873 0.2141483724117279
CurrentTrain: epoch  9, batch     6 | loss: 6.5294023Losses:  7.374113082885742 3.1359076499938965 0.2260458618402481
CurrentTrain: epoch  9, batch     7 | loss: 7.3741131Losses:  6.554942607879639 2.3544044494628906 0.22384977340698242
CurrentTrain: epoch  9, batch     8 | loss: 6.5549426Losses:  9.128935813903809 5.007941246032715 0.1523871272802353
CurrentTrain: epoch  9, batch     9 | loss: 9.1289358Losses:  9.377703666687012 5.156439781188965 0.24596844613552094
CurrentTrain: epoch  9, batch    10 | loss: 9.3777037Losses:  10.322670936584473 5.586175918579102 0.15776985883712769
CurrentTrain: epoch  9, batch    11 | loss: 10.3226709Losses:  11.09379768371582 6.8201775550842285 0.2430671900510788
CurrentTrain: epoch  9, batch    12 | loss: 11.0937977Losses:  5.428888320922852 1.2387115955352783 0.20668776333332062
CurrentTrain: epoch  9, batch    13 | loss: 5.4288883Losses:  6.879091262817383 2.686196804046631 0.23047493398189545
CurrentTrain: epoch  9, batch    14 | loss: 6.8790913Losses:  6.738631248474121 2.545511245727539 0.2228165715932846
CurrentTrain: epoch  9, batch    15 | loss: 6.7386312Losses:  6.93596887588501 2.6895172595977783 0.22082394361495972
CurrentTrain: epoch  9, batch    16 | loss: 6.9359689Losses:  6.6428117752075195 2.45285701751709 0.2186654657125473
CurrentTrain: epoch  9, batch    17 | loss: 6.6428118Losses:  7.441413879394531 3.242003917694092 0.23229342699050903
CurrentTrain: epoch  9, batch    18 | loss: 7.4414139Losses:  6.983664512634277 2.705993175506592 0.23187677562236786
CurrentTrain: epoch  9, batch    19 | loss: 6.9836645Losses:  5.770456790924072 1.5958868265151978 0.20662647485733032
CurrentTrain: epoch  9, batch    20 | loss: 5.7704568Losses:  7.666608810424805 3.4795589447021484 0.2407372146844864
CurrentTrain: epoch  9, batch    21 | loss: 7.6666088Losses:  6.449217319488525 2.157435417175293 0.2213960886001587
CurrentTrain: epoch  9, batch    22 | loss: 6.4492173Losses:  8.148811340332031 3.891169786453247 0.23924997448921204
CurrentTrain: epoch  9, batch    23 | loss: 8.1488113Losses:  6.906111240386963 2.700350284576416 0.22222024202346802
CurrentTrain: epoch  9, batch    24 | loss: 6.9061112Losses:  5.999526023864746 1.8264451026916504 0.21858561038970947
CurrentTrain: epoch  9, batch    25 | loss: 5.9995260Losses:  8.347173690795898 4.161737442016602 0.24129286408424377
CurrentTrain: epoch  9, batch    26 | loss: 8.3471737Losses:  8.184966087341309 4.104313850402832 0.15036827325820923
CurrentTrain: epoch  9, batch    27 | loss: 8.1849661Losses:  8.160235404968262 3.924680471420288 0.2185412496328354
CurrentTrain: epoch  9, batch    28 | loss: 8.1602354Losses:  6.284850120544434 2.0856809616088867 0.22243690490722656
CurrentTrain: epoch  9, batch    29 | loss: 6.2848501Losses:  6.5142717361450195 2.345320463180542 0.2307855188846588
CurrentTrain: epoch  9, batch    30 | loss: 6.5142717Losses:  6.626004695892334 2.3632829189300537 0.23788690567016602
CurrentTrain: epoch  9, batch    31 | loss: 6.6260047Losses:  8.779624938964844 4.597135543823242 0.24143531918525696
CurrentTrain: epoch  9, batch    32 | loss: 8.7796249Losses:  7.340287208557129 2.8474366664886475 0.22629152238368988
CurrentTrain: epoch  9, batch    33 | loss: 7.3402872Losses:  7.184798717498779 3.0687198638916016 0.1356484293937683
CurrentTrain: epoch  9, batch    34 | loss: 7.1847987Losses:  7.408078193664551 2.906492233276367 0.24907098710536957
CurrentTrain: epoch  9, batch    35 | loss: 7.4080782Losses:  6.6530537605285645 2.41519832611084 0.23421260714530945
CurrentTrain: epoch  9, batch    36 | loss: 6.6530538Losses:  7.338851451873779 3.126030445098877 0.22607089579105377
CurrentTrain: epoch  9, batch    37 | loss: 7.3388515Losses:  6.966591835021973 2.713817834854126 0.2269585132598877
CurrentTrain: epoch  9, batch    38 | loss: 6.9665918Losses:  8.328710556030273 4.150257110595703 0.23393088579177856
CurrentTrain: epoch  9, batch    39 | loss: 8.3287106Losses:  7.357091426849365 3.146641731262207 0.24152792990207672
CurrentTrain: epoch  9, batch    40 | loss: 7.3570914Losses:  6.418788433074951 2.191333532333374 0.22382768988609314
CurrentTrain: epoch  9, batch    41 | loss: 6.4187884Losses:  6.532349586486816 2.338994264602661 0.21581366658210754
CurrentTrain: epoch  9, batch    42 | loss: 6.5323496Losses:  5.734019756317139 1.554261565208435 0.2045021951198578
CurrentTrain: epoch  9, batch    43 | loss: 5.7340198Losses:  7.622181415557861 3.4303956031799316 0.22238506376743317
CurrentTrain: epoch  9, batch    44 | loss: 7.6221814Losses:  7.114963054656982 2.9258556365966797 0.21788254380226135
CurrentTrain: epoch  9, batch    45 | loss: 7.1149631Losses:  7.952348232269287 2.6469714641571045 0.22795143723487854
CurrentTrain: epoch  9, batch    46 | loss: 7.9523482Losses:  8.645214080810547 4.044110298156738 0.22783605754375458
CurrentTrain: epoch  9, batch    47 | loss: 8.6452141Losses:  8.88561725616455 4.704054355621338 0.26091301441192627
CurrentTrain: epoch  9, batch    48 | loss: 8.8856173Losses:  7.010986804962158 2.7802112102508545 0.22266130149364471
CurrentTrain: epoch  9, batch    49 | loss: 7.0109868Losses:  7.475040912628174 3.0380544662475586 0.22900830209255219
CurrentTrain: epoch  9, batch    50 | loss: 7.4750409Losses:  6.414214134216309 2.1627278327941895 0.22819113731384277
CurrentTrain: epoch  9, batch    51 | loss: 6.4142141Losses:  6.146218299865723 1.8933184146881104 0.21147342026233673
CurrentTrain: epoch  9, batch    52 | loss: 6.1462183Losses:  6.907345294952393 2.5658485889434814 0.21391326189041138
CurrentTrain: epoch  9, batch    53 | loss: 6.9073453Losses:  7.052188873291016 2.8113842010498047 0.2322026491165161
CurrentTrain: epoch  9, batch    54 | loss: 7.0521889Losses:  6.875795364379883 2.6604318618774414 0.21599793434143066
CurrentTrain: epoch  9, batch    55 | loss: 6.8757954Losses:  7.616549968719482 3.0793261528015137 0.24451448023319244
CurrentTrain: epoch  9, batch    56 | loss: 7.6165500Losses:  8.69469928741455 4.411252021789551 0.24595610797405243
CurrentTrain: epoch  9, batch    57 | loss: 8.6946993Losses:  8.033819198608398 3.7628769874572754 0.25523024797439575
CurrentTrain: epoch  9, batch    58 | loss: 8.0338192Losses:  9.809407234191895 5.538270950317383 0.254861056804657
CurrentTrain: epoch  9, batch    59 | loss: 9.8094072Losses:  8.254518508911133 4.033064365386963 0.2354903221130371
CurrentTrain: epoch  9, batch    60 | loss: 8.2545185Losses:  6.712007522583008 2.5278916358947754 0.22104185819625854
CurrentTrain: epoch  9, batch    61 | loss: 6.7120075Losses:  7.249454021453857 2.891561269760132 0.27447840571403503
CurrentTrain: epoch  9, batch    62 | loss: 7.2494540
#### movement ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "movement" means literary, artistic, scientific or philosophical movement or scene associated with this person or work.
Here is an example:
Relation: movement
Context: the theme of the loss of identity in crowds in this painting can be seen as a precursor to expressionist painting , particularly the work of james ensor .
Head Entity: james ensor
Tail Entity: expressionist
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "movement":
Relation: movement  
Context: The surrealist movement, which sought to challenge the conventional perception of reality, was significantly influenced by the works of Salvador Dalí.  
Head Entity: Salvador Dalí  
Tail Entity: surrealist  

Relation: movement  
Context: The emergence of the romantic movement in literature was marked by the profound impact of poets like William Wordsworth and Samuel Taylor Coleridge.  
Head Entity: William Wordsworth  
Tail Entity: romantic  
#### located in the administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in the administrative territorial entity" means the item is located on the territory of the following administrative entity.
Here is an example:
Relation: located in the administrative territorial entity
Context: lake koshkonong is a census - designated place ( cdp ) in the town of sumner , jefferson county , wisconsin , united states .
Head Entity: sumner
Tail Entity: jefferson county
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in the administrative territorial entity":
Relation: located in the administrative territorial entity  
Context: the city of san francisco is located in san francisco county, california, united states.  
Head Entity: san francisco  
Tail Entity: san francisco county  

Relation: located in the administrative territorial entity  
Context: the university of toronto is situated in the city of toronto, ontario, canada.  
Head Entity: toronto  
Tail Entity: toronto city  
#### head of government ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "head of government" means head of the executive power of this town, city, municipality, state, country, or other governmental body.
Here is an example:
Relation: head of government
Context: on february 10 , 2010 , stefan mappus was elected minister - president of baden - württemberg , and asked that all ministers continue their roles .
Head Entity: baden - württemberg
Tail Entity: stefan mappus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "head of government":
Relation: head of government  
Context: in 2021, jacinda ardern was re-elected as the prime minister of new zealand, leading the labor party to a significant victory in the general elections.  
Head Entity: new zealand  
Tail Entity: jacinda ardern  

Relation: head of government  
Context: during the annual summit, justin trudeau addressed the nation as the prime minister of canada, outlining his government's plans for the upcoming year.  
Head Entity: canada  
Tail Entity: justin trudeau  
#### member of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of" means organization, club or musical group to which the subject belongs.
Here is an example:
Relation: member of
Context: rage against the machine bandmates , tom morello , brad wilk , and de la rocha , as well as audioslave bandmate , chris cornell , came to support commerford .
Head Entity: chris cornell
Tail Entity: audioslave
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of":
Relation: member of  
Context: The famous actor and musician, Jared Leto, is not only the lead vocalist of Thirty Seconds to Mars but also a member of the Academy Award-winning film community.  
Head Entity: Jared Leto  
Tail Entity: Thirty Seconds to Mars  

Relation: member of  
Context: During the annual conference, several prominent figures from the tech industry, including Sundar Pichai, who is a member of the Google team, shared their insights on innovation.  
Head Entity: Sundar Pichai  
Tail Entity: Google  
#### crosses ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "crosses" means obstacle which this bridge crosses over or this tunnel goes under.
Here is an example:
Relation: crosses
Context: on july 2 , 1935 , the rip van winkle bridge over the hudson river between catskill and greenport was opened to traffic .
Head Entity: rip van winkle bridge
Tail Entity: hudson river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "crosses":
Relation: crosses  
Context: The Golden Gate Bridge spans the Golden Gate Strait, connecting San Francisco to Marin County, and is a vital link for traffic crossing over the water.  
Head Entity: Golden Gate Bridge  
Tail Entity: Golden Gate Strait  

Relation: crosses  
Context: The Channel Tunnel, also known as the Chunnel, runs beneath the English Channel, providing a crucial transportation route between the UK and mainland Europe.  
Head Entity: Channel Tunnel  
Tail Entity: English Channel  
#### tributary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "tributary" means watercourse that flows into an other one.
Here is an example:
Relation: tributary
Context: it is on the panj river just downstream from the mouth of the bartang river at the point where the panj briefly turns west before resuming its northerly course .
Head Entity: panj river
Tail Entity: bartang river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "tributary":
Relation: tributary  
Context: The Missouri River, which is a major tributary of the Mississippi River, flows through several states before joining the larger watercourse.  
Head Entity: Missouri River  
Tail Entity: Mississippi River  

Relation: tributary  
Context: The Rio Grande serves as a natural border between the United States and Mexico, and it has several tributaries, including the Pecos River.  
Head Entity: Pecos River  
Tail Entity: Rio Grande  
#### position held ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position held" means subject currently or formerly holds the object position or public office.
Here is an example:
Relation: position held
Context: in september 2001 , mimica became minister of european integration under prime minister ivica račan , position he held until december 2003 .
Head Entity: ivica račan
Tail Entity: prime minister
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position held":
Relation: position held  
Context: in 2010, barack obama was elected as the 44th president of the united states, a position he held until 2017.  
Head Entity: barack obama  
Tail Entity: president of the united states  

Relation: position held  
Context: after serving as the governor of california, arnold schwarzenegger continued to hold the position until 2011.  
Head Entity: arnold schwarzenegger  
Tail Entity: governor of california  
#### developer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "developer" means organization or person that developed the item.
Here is an example:
Relation: developer
Context: it was developed by syn sophia , who also developed the other games in the " style savvy " series .
Head Entity: style savvy
Tail Entity: syn sophia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "developer":
Relation: developer  
Context: the popular game was created by the renowned studio, pixel perfect, which is known for its innovative gameplay mechanics.  
Head Entity: pixel perfect  
Tail Entity: the popular game  

Relation: developer  
Context: the groundbreaking software was launched by tech giants, future innovations, who have been at the forefront of technology for over a decade.  
Head Entity: future innovations  
Tail Entity: the groundbreaking software  
#### location of formation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location of formation" means location where a group or organization was formed.
Here is an example:
Relation: location of formation
Context: happy refugees are a british post - punk band from london , formed in 1981 .
Head Entity: happy refugees
Tail Entity: london
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location of formation":
Relation: location of formation  
Context: the tech startup was established in silicon valley in 2015, quickly gaining recognition for its innovative solutions.  
Head Entity: the tech startup  
Tail Entity: silicon valley  

Relation: location of formation  
Context: the renowned art collective originated in new york city during the late 1990s, bringing together diverse artists from various backgrounds.  
Head Entity: the renowned art collective  
Tail Entity: new york city  
#### country of origin ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of origin" means country of origin of this item (creative work, food, phrase, product, etc.).
Here is an example:
Relation: country of origin
Context: during the same month , the band supported kings of leon 's tour of the united states , and coldplay 's tour of the united kingdom .
Head Entity: kings of leon
Tail Entity: united states
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of origin":
Relation: country of origin  
Context: The famous Italian dish, pizza, has gained popularity worldwide, but its roots can be traced back to Naples, Italy.  
Head Entity: pizza  
Tail Entity: Italy  

Relation: country of origin  
Context: The iconic brand, Rolex, is renowned for its luxury watches, which are crafted in Switzerland, known for its precision and quality in watchmaking.  
Head Entity: Rolex  
Tail Entity: Switzerland  
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.89%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 95.11%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.28%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
[EVAL] batch:    0 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:    3 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 100.00%,  total acc: 93.12%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 94.12%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 94.10%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.41%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.69%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.94%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 94.89%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 95.11%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 95.05%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 94.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 94.71%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 94.68%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 94.64%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 94.61%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.79%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.96%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 95.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 95.27%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 95.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 95.36%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 95.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.61%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.56%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.67%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.78%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.88%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.98%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 96.08%   [EVAL] batch:   43 | acc: 100.00%,  total acc: 96.16%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 96.25%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 96.33%   [EVAL] batch:   46 | acc: 93.75%,  total acc: 96.28%   [EVAL] batch:   47 | acc: 93.75%,  total acc: 96.22%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 96.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 96.32%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 96.27%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 96.23%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 96.30%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 96.02%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 95.98%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 95.91%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 95.97%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 95.94%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 96.00%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 95.97%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 95.24%   
cur_acc:  ['0.9524']
his_acc:  ['0.9524']
Clustering into  9  clusters
Clusters:  [0 3 1 0 6 6 1 2 8 8 3 0 2 0 5 0 7 4 0 4]
Losses:  12.207189559936523 4.297410488128662 0.6287661790847778
CurrentTrain: epoch  0, batch     0 | loss: 12.2071896Losses:  12.193385124206543 4.5604472160339355 0.6625906229019165
CurrentTrain: epoch  0, batch     1 | loss: 12.1933851Losses:  12.067790985107422 3.3850698471069336 0.6805742979049683
CurrentTrain: epoch  0, batch     2 | loss: 12.0677910Losses:  7.636140823364258 -0.0 0.10519908368587494
CurrentTrain: epoch  0, batch     3 | loss: 7.6361408Losses:  10.226699829101562 2.9703540802001953 0.598117470741272
CurrentTrain: epoch  1, batch     0 | loss: 10.2266998Losses:  10.995440483093262 4.91019344329834 0.43555352091789246
CurrentTrain: epoch  1, batch     1 | loss: 10.9954405Losses:  12.586640357971191 4.228433609008789 0.685565173625946
CurrentTrain: epoch  1, batch     2 | loss: 12.5866404Losses:  7.361817359924316 -0.0 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 7.3618174Losses:  10.276875495910645 3.404794216156006 0.6296411752700806
CurrentTrain: epoch  2, batch     0 | loss: 10.2768755Losses:  9.63503646850586 3.534764289855957 0.508732795715332
CurrentTrain: epoch  2, batch     1 | loss: 9.6350365Losses:  8.621100425720215 2.4913816452026367 0.5910936594009399
CurrentTrain: epoch  2, batch     2 | loss: 8.6211004Losses:  4.939733028411865 -0.0 0.2335551679134369
CurrentTrain: epoch  2, batch     3 | loss: 4.9397330Losses:  8.515707969665527 2.257276773452759 0.5373982191085815
CurrentTrain: epoch  3, batch     0 | loss: 8.5157080Losses:  8.591330528259277 2.2474794387817383 0.5919905304908752
CurrentTrain: epoch  3, batch     1 | loss: 8.5913305Losses:  8.983155250549316 3.4356861114501953 0.5022842288017273
CurrentTrain: epoch  3, batch     2 | loss: 8.9831553Losses:  4.018045902252197 -0.0 0.11089657992124557
CurrentTrain: epoch  3, batch     3 | loss: 4.0180459Losses:  9.746638298034668 5.512506484985352 0.3736826181411743
CurrentTrain: epoch  4, batch     0 | loss: 9.7466383Losses:  11.701326370239258 5.3180131912231445 0.4579639434814453
CurrentTrain: epoch  4, batch     1 | loss: 11.7013264Losses:  8.64799690246582 2.789419651031494 0.5746140480041504
CurrentTrain: epoch  4, batch     2 | loss: 8.6479969Losses:  2.2071609497070312 -0.0 -0.0
CurrentTrain: epoch  4, batch     3 | loss: 2.2071609Losses:  7.881842613220215 2.70080304145813 0.5709733963012695
CurrentTrain: epoch  5, batch     0 | loss: 7.8818426Losses:  7.850856781005859 2.5228631496429443 0.5773646831512451
CurrentTrain: epoch  5, batch     1 | loss: 7.8508568Losses:  7.422084331512451 2.646461009979248 0.5013007521629333
CurrentTrain: epoch  5, batch     2 | loss: 7.4220843Losses:  8.053661346435547 -0.0 0.10585353523492813
CurrentTrain: epoch  5, batch     3 | loss: 8.0536613Losses:  9.66676139831543 4.371686935424805 0.41599684953689575
CurrentTrain: epoch  6, batch     0 | loss: 9.6667614Losses:  8.207005500793457 2.823364734649658 0.5486053824424744
CurrentTrain: epoch  6, batch     1 | loss: 8.2070055Losses:  6.156866073608398 1.9955520629882812 0.5274969339370728
CurrentTrain: epoch  6, batch     2 | loss: 6.1568661Losses:  2.5666661262512207 -0.0 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 2.5666661Losses:  9.433753967285156 3.5666966438293457 0.4830645024776459
CurrentTrain: epoch  7, batch     0 | loss: 9.4337540Losses:  7.3145551681518555 3.1339187622070312 0.5234649181365967
CurrentTrain: epoch  7, batch     1 | loss: 7.3145552Losses:  7.278646469116211 2.928769588470459 0.44419366121292114
CurrentTrain: epoch  7, batch     2 | loss: 7.2786465Losses:  5.657012462615967 -0.0 0.11692558228969574
CurrentTrain: epoch  7, batch     3 | loss: 5.6570125Losses:  8.631178855895996 3.51684308052063 0.5522444844245911
CurrentTrain: epoch  8, batch     0 | loss: 8.6311789Losses:  7.726253986358643 2.795197010040283 0.4322030246257782
CurrentTrain: epoch  8, batch     1 | loss: 7.7262540Losses:  6.155824184417725 1.986698865890503 0.49614936113357544
CurrentTrain: epoch  8, batch     2 | loss: 6.1558242Losses:  1.8363873958587646 -0.0 0.10741971433162689
CurrentTrain: epoch  8, batch     3 | loss: 1.8363874Losses:  7.8209028244018555 3.31036376953125 0.41266900300979614
CurrentTrain: epoch  9, batch     0 | loss: 7.8209028Losses:  9.0303373336792 4.782448768615723 0.5317103862762451
CurrentTrain: epoch  9, batch     1 | loss: 9.0303373Losses:  8.042899131774902 3.071040391921997 0.5017445683479309
CurrentTrain: epoch  9, batch     2 | loss: 8.0428991Losses:  3.728766679763794 -0.0 0.10838276147842407
CurrentTrain: epoch  9, batch     3 | loss: 3.7287667
#### applies to jurisdiction ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "applies to jurisdiction" means the item (institution, law, public office, public register...) or statement belongs to or has power over or applies to the value (a territorial jurisdiction: a country, state, municipality, ...).
Here is an example:
Relation: applies to jurisdiction
Context: in 2004 the catalan government gave him the george cross .
Head Entity: george cross
Tail Entity: catalan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "applies to jurisdiction":
Relation: applies to jurisdiction  
Context: The new environmental regulations introduced by the European Union are designed to protect biodiversity across member states.  
Head Entity: environmental regulations  
Tail Entity: European Union  

Relation: applies to jurisdiction  
Context: The Supreme Court's ruling clarified that federal laws take precedence over state laws in matters of immigration.  
Head Entity: federal laws  
Tail Entity: state laws  
#### performer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "performer" means actor, musician, band or other performer associated with this role or musical work.
Here is an example:
Relation: performer
Context: it is prominently featured on jean michel jarre 's albums " oxygène " ( 1977 ) and " équinoxe " ( 1978 ) .
Head Entity: équinoxe
Tail Entity: jean michel jarre
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "performer":
Relation: performer  
Context: The legendary band Queen is known for their iconic song "Bohemian Rhapsody," which showcases Freddie Mercury's extraordinary vocal talent.  
Head Entity: Bohemian Rhapsody  
Tail Entity: Freddie Mercury  

Relation: performer  
Context: Taylor Swift captivated audiences with her heartfelt performance of "Love Story" during the music festival last summer.  
Head Entity: Love Story  
Tail Entity: Taylor Swift  
#### manufacturer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "manufacturer" means manufacturer or producer of this product.
Here is an example:
Relation: manufacturer
Context: the glasflügel h-201 standard " libelle " ( german : " dragonfly " ) is an early composite standard class single - seat sailplane produced by glasflügel from 1967 .
Head Entity: dragonfly
Tail Entity: glasflügel
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "manufacturer":
Relation: manufacturer  
Context: The iPhone 13, a smartphone developed by Apple, was released in September 2021 and features advanced camera technology and improved battery life.  
Head Entity: iPhone 13  
Tail Entity: Apple  

Relation: manufacturer  
Context: The Tesla Model S is an all-electric luxury sedan that was first introduced by Tesla, Inc. in 2012, showcasing innovative technology and performance.  
Head Entity: Tesla Model S  
Tail Entity: Tesla, Inc.  
#### participant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant" means person, group of people or organization (object) that actively takes/took part in an event or process.
Here is an example:
Relation: participant
Context: in contrast to gwladys and roger 's allegiance to the house of lancaster , their three sons were staunch yorkists during the wars of the roses .
Head Entity: wars of the roses
Tail Entity: house of lancaster
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant":
Relation: participant  
Context: During the annual tech conference, several leading companies showcased their latest innovations, with Google and Microsoft being the most prominent participants.  
Head Entity: tech conference  
Tail Entity: Google  

Relation: participant  
Context: The local community organized a charity run, where various schools and organizations, including the Red Cross, participated to raise funds for the homeless.  
Head Entity: charity run  
Tail Entity: Red Cross  
#### operating system ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operating system" means operating system (OS) on which a software works or the OS installed on hardware.
Here is an example:
Relation: operating system
Context: the software is written in python and works on all the platforms supported by blender : windows , osx and linux .
Head Entity: blender
Tail Entity: windows
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operating system":
Relation: operating system  
Context: The latest version of the application is compatible with various operating systems, including macOS, Windows, and Ubuntu.  
Head Entity: application  
Tail Entity: macOS  

Relation: operating system  
Context: Many developers prefer to use Linux as their primary operating system due to its flexibility and open-source nature.  
Head Entity: developers  
Tail Entity: Linux  
#### competition class ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "competition class" means official classification by a regulating body under which the subject (events, teams, participants, or equipment) qualifies for inclusion.
Here is an example:
Relation: competition class
Context: in his next bout , he won the wbc light welterweight title by defeating roger mayweather for a second time .
Head Entity: roger mayweather
Tail Entity: light welterweight
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "competition class":
Relation: competition class  
Context: The team secured their place in the tournament by qualifying under the UEFA Champions League regulations.  
Head Entity: UEFA Champions League  
Tail Entity: tournament  

Relation: competition class  
Context: The athlete competed in the 100 meters event, which is classified under the Olympic Games standards.  
Head Entity: Olympic Games  
Tail Entity: 100 meters event  
#### successful candidate ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "successful candidate" means person elected after the election.
Here is an example:
Relation: successful candidate
Context: swartz served as swedish prime minister for only seven months before being forced from power after the swedish elections of 1917 and replaced by the liberal government of nils edén .
Head Entity: swedish elections of 1917
Tail Entity: nils edén
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "successful candidate":
Relation: successful candidate  
Context: in the recent municipal elections, the voters chose maria gonzalez as the new mayor of the city, marking a significant shift in local governance.  
Head Entity: municipal elections  
Tail Entity: maria gonzalez  

Relation: successful candidate  
Context: after a closely contested race, the citizens of springfield elected john doe as their new governor, succeeding the long-serving incumbent.  
Head Entity: springfield elections  
Tail Entity: john doe  
#### sibling ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sibling" means the subject and the object have the same parents.
Here is an example:
Relation: sibling
Context: although the latter 's eldest son , lǫgmaðr , appears to have succeeded to the kingship , he was soon forced to contend with factions supporting his younger brothers : haraldr , and óláfr .
Head Entity: lǫgmaðr
Tail Entity: óláfr
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sibling":
Relation: sibling  
Context: In the royal family of the ancient kingdom, the two brothers, Eirik and Sigurd, were known for their fierce loyalty to each other, having grown up under the same roof and shared the same parents.  
Head Entity: Eirik  
Tail Entity: Sigurd  

Relation: sibling  
Context: During the family reunion, it was evident that both Clara and her brother, James, inherited their parents' artistic talents, showcasing their skills in painting and music.  
Head Entity: Clara  
Tail Entity: James  
#### military branch ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military branch" means branch to which this military unit, award, office, or person belongs.
Here is an example:
Relation: military branch
Context: iain harrison is a competitive shooter and former british army captain .
Head Entity: iain harrison
Tail Entity: british army
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military branch":
Relation: military branch  
Context: sergeant johnson served in the united states marine corps during his military career.  
Head Entity: sergeant johnson  
Tail Entity: united states marine corps  

Relation: military branch  
Context: general smith was a prominent figure in the royal air force, leading several key missions.  
Head Entity: general smith  
Tail Entity: royal air force  
#### child ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "child" means subject has object as child.
Here is an example:
Relation: child
Context: ziad rahbani is the son of the lebanese famous composer assi rahbani and nouhad haddad , the famous lebanese female singer known as fairuz .
Head Entity: assi rahbani
Tail Entity: ziad rahbani
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "child":
Relation: child  
Context: in the animated series, the character of simon is the son of the adventurous couple, marie and tom.  
Head Entity: marie  
Tail Entity: simon  

Relation: child  
Context: the renowned physicist albert einstein had a daughter named lieserl, who was born in 1902.  
Head Entity: albert einstein  
Tail Entity: lieserl  
Losses:  7.1774516105651855 0.8539801836013794 0.7045866250991821
MemoryTrain:  epoch  0, batch     0 | loss: 7.1774516Losses:  6.260890483856201 0.4684326648712158 0.6281067132949829
MemoryTrain:  epoch  0, batch     1 | loss: 6.2608905Losses:  8.842774391174316 2.3362226486206055 0.6162876486778259
MemoryTrain:  epoch  0, batch     2 | loss: 8.8427744Losses:  6.331087589263916 0.7122683525085449 0.5810203552246094
MemoryTrain:  epoch  0, batch     3 | loss: 6.3310876Losses:  7.115927219390869 1.20772385597229 0.6828617453575134
MemoryTrain:  epoch  1, batch     0 | loss: 7.1159272Losses:  7.871178150177002 1.2459228038787842 0.7264151573181152
MemoryTrain:  epoch  1, batch     1 | loss: 7.8711782Losses:  6.815645694732666 1.4327328205108643 0.5456862449645996
MemoryTrain:  epoch  1, batch     2 | loss: 6.8156457Losses:  5.058473110198975 0.46581578254699707 0.664707601070404
MemoryTrain:  epoch  1, batch     3 | loss: 5.0584731Losses:  6.903169631958008 1.4614882469177246 0.6570248603820801
MemoryTrain:  epoch  2, batch     0 | loss: 6.9031696Losses:  6.5095391273498535 1.2766891717910767 0.7767897248268127
MemoryTrain:  epoch  2, batch     1 | loss: 6.5095391Losses:  4.622587203979492 0.9123352766036987 0.555404543876648
MemoryTrain:  epoch  2, batch     2 | loss: 4.6225872Losses:  5.174413204193115 1.3270562887191772 0.5161848664283752
MemoryTrain:  epoch  2, batch     3 | loss: 5.1744132Losses:  5.093018054962158 1.704403042793274 0.6719462275505066
MemoryTrain:  epoch  3, batch     0 | loss: 5.0930181Losses:  7.229151248931885 2.403743267059326 0.4778915047645569
MemoryTrain:  epoch  3, batch     1 | loss: 7.2291512Losses:  5.544851303100586 1.143191933631897 0.7887600660324097
MemoryTrain:  epoch  3, batch     2 | loss: 5.5448513Losses:  4.8186516761779785 0.8049119710922241 0.45956486463546753
MemoryTrain:  epoch  3, batch     3 | loss: 4.8186517Losses:  4.98814058303833 1.0250060558319092 0.6211351752281189
MemoryTrain:  epoch  4, batch     0 | loss: 4.9881406Losses:  4.12789249420166 0.2436915934085846 0.6554086208343506
MemoryTrain:  epoch  4, batch     1 | loss: 4.1278925Losses:  4.299485683441162 0.920366108417511 0.6412047147750854
MemoryTrain:  epoch  4, batch     2 | loss: 4.2994857Losses:  4.170131206512451 0.9092502593994141 0.5820273160934448
MemoryTrain:  epoch  4, batch     3 | loss: 4.1701312Losses:  3.8768649101257324 1.319214940071106 0.4662465453147888
MemoryTrain:  epoch  5, batch     0 | loss: 3.8768649Losses:  4.827720642089844 1.5319639444351196 0.6298573613166809
MemoryTrain:  epoch  5, batch     1 | loss: 4.8277206Losses:  3.9620800018310547 1.5265052318572998 0.7387188076972961
MemoryTrain:  epoch  5, batch     2 | loss: 3.9620800Losses:  5.548191070556641 1.2719444036483765 0.6533344388008118
MemoryTrain:  epoch  5, batch     3 | loss: 5.5481911Losses:  5.128314018249512 1.383691430091858 0.7061291933059692
MemoryTrain:  epoch  6, batch     0 | loss: 5.1283140Losses:  4.6727213859558105 2.2941508293151855 0.5946571230888367
MemoryTrain:  epoch  6, batch     1 | loss: 4.6727214Losses:  3.4734768867492676 0.47730278968811035 0.6852308511734009
MemoryTrain:  epoch  6, batch     2 | loss: 3.4734769Losses:  3.438690185546875 0.36220163106918335 0.5819584131240845
MemoryTrain:  epoch  6, batch     3 | loss: 3.4386902Losses:  4.0055365562438965 1.1527068614959717 0.7321695685386658
MemoryTrain:  epoch  7, batch     0 | loss: 4.0055366Losses:  3.4974751472473145 0.9308589696884155 0.5244525671005249
MemoryTrain:  epoch  7, batch     1 | loss: 3.4974751Losses:  3.1137688159942627 0.657874345779419 0.6547882556915283
MemoryTrain:  epoch  7, batch     2 | loss: 3.1137688Losses:  4.961568832397461 1.2698254585266113 0.4814330041408539
MemoryTrain:  epoch  7, batch     3 | loss: 4.9615688Losses:  3.6770522594451904 1.0259208679199219 0.6543571949005127
MemoryTrain:  epoch  8, batch     0 | loss: 3.6770523Losses:  3.933577060699463 1.3653979301452637 0.6174942255020142
MemoryTrain:  epoch  8, batch     1 | loss: 3.9335771Losses:  3.716120719909668 0.8320543766021729 0.5184346437454224
MemoryTrain:  epoch  8, batch     2 | loss: 3.7161207Losses:  3.2361507415771484 0.6451690196990967 0.637640118598938
MemoryTrain:  epoch  8, batch     3 | loss: 3.2361507Losses:  4.113888740539551 1.1983892917633057 0.7257435917854309
MemoryTrain:  epoch  9, batch     0 | loss: 4.1138887Losses:  3.0906338691711426 0.6759559512138367 0.6568397283554077
MemoryTrain:  epoch  9, batch     1 | loss: 3.0906339Losses:  4.681778907775879 2.095038414001465 0.4202868640422821
MemoryTrain:  epoch  9, batch     2 | loss: 4.6817789Losses:  2.9955496788024902 0.5518552660942078 0.6174730658531189
MemoryTrain:  epoch  9, batch     3 | loss: 2.9955497
[EVAL] batch:    0 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    1 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    2 | acc: 0.00%,  total acc: 0.00%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 3.12%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 2.50%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 2.08%   [EVAL] batch:    6 | acc: 68.75%,  total acc: 11.61%   [EVAL] batch:    7 | acc: 68.75%,  total acc: 18.75%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 26.39%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 32.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 36.93%   [EVAL] batch:   11 | acc: 81.25%,  total acc: 40.62%   [EVAL] batch:   12 | acc: 100.00%,  total acc: 45.19%   [EVAL] batch:   13 | acc: 87.50%,  total acc: 48.21%   [EVAL] batch:   14 | acc: 87.50%,  total acc: 50.83%   [EVAL] batch:   15 | acc: 87.50%,  total acc: 53.12%   [EVAL] batch:   16 | acc: 87.50%,  total acc: 55.15%   [EVAL] batch:   17 | acc: 100.00%,  total acc: 57.64%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 59.21%   [EVAL] batch:   19 | acc: 50.00%,  total acc: 58.75%   [EVAL] batch:   20 | acc: 68.75%,  total acc: 59.23%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 60.51%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 61.41%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 61.72%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 63.94%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 65.28%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 68.33%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 69.35%   [EVAL] batch:   31 | acc: 93.75%,  total acc: 70.12%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 71.02%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:   34 | acc: 100.00%,  total acc: 72.68%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 73.44%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 74.16%   [EVAL] batch:   37 | acc: 81.25%,  total acc: 74.34%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 74.04%   [EVAL] batch:   39 | acc: 56.25%,  total acc: 73.59%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 73.17%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 73.07%   [EVAL] batch:   42 | acc: 81.25%,  total acc: 73.26%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 73.15%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 73.06%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 72.83%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 72.61%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 72.79%   [EVAL] batch:   48 | acc: 68.75%,  total acc: 72.70%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 72.50%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 72.92%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 73.32%   [EVAL] batch:   52 | acc: 87.50%,  total acc: 73.58%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 73.73%   [EVAL] batch:   54 | acc: 93.75%,  total acc: 74.09%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 74.11%   [EVAL] batch:   56 | acc: 50.00%,  total acc: 73.68%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 73.17%   [EVAL] batch:   58 | acc: 31.25%,  total acc: 72.46%   [EVAL] batch:   59 | acc: 50.00%,  total acc: 72.08%   [EVAL] batch:   60 | acc: 18.75%,  total acc: 71.21%   [EVAL] batch:   61 | acc: 31.25%,  total acc: 70.56%   [EVAL] batch:   62 | acc: 18.75%,  total acc: 69.74%   
[EVAL] batch:    0 | acc: 87.50%,  total acc: 87.50%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 91.67%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 90.62%   [EVAL] batch:    4 | acc: 87.50%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 90.62%   [EVAL] batch:    6 | acc: 87.50%,  total acc: 90.18%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 91.41%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 92.36%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 92.50%   [EVAL] batch:   10 | acc: 93.75%,  total acc: 92.61%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 93.23%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 93.27%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 93.30%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 93.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 93.75%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 94.08%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 94.38%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 94.64%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 94.32%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 94.29%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 94.27%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 93.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 93.99%   [EVAL] batch:   26 | acc: 93.75%,  total acc: 93.98%   [EVAL] batch:   27 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 93.97%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 94.35%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 94.53%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 94.70%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 94.85%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 94.82%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 94.97%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 95.10%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 95.07%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 95.19%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 95.31%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 95.43%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 95.54%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 95.49%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 95.45%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 95.42%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 95.24%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 94.95%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 94.66%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 94.77%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 94.62%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 94.49%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 94.47%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 94.46%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 94.56%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 94.32%   [EVAL] batch:   55 | acc: 87.50%,  total acc: 94.20%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 94.19%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 94.07%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 94.17%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 94.17%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 94.26%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 94.25%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 93.55%   [EVAL] batch:   63 | acc: 0.00%,  total acc: 92.09%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 90.67%   [EVAL] batch:   65 | acc: 6.25%,  total acc: 89.39%   [EVAL] batch:   66 | acc: 6.25%,  total acc: 88.15%   [EVAL] batch:   67 | acc: 0.00%,  total acc: 86.86%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 86.05%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 85.89%   [EVAL] batch:   70 | acc: 81.25%,  total acc: 85.83%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 85.68%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 85.70%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 85.73%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 85.75%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 85.86%   [EVAL] batch:   76 | acc: 93.75%,  total acc: 85.96%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 85.90%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 85.92%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 86.02%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 86.19%   [EVAL] batch:   81 | acc: 50.00%,  total acc: 85.75%   [EVAL] batch:   82 | acc: 62.50%,  total acc: 85.47%   [EVAL] batch:   83 | acc: 93.75%,  total acc: 85.57%   [EVAL] batch:   84 | acc: 75.00%,  total acc: 85.44%   [EVAL] batch:   85 | acc: 75.00%,  total acc: 85.32%   [EVAL] batch:   86 | acc: 81.25%,  total acc: 85.27%   [EVAL] batch:   87 | acc: 87.50%,  total acc: 85.30%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 85.46%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 85.62%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 85.65%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 85.80%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 85.95%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 86.04%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 86.18%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 86.33%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 86.47%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 86.61%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 86.74%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 86.88%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 86.63%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 86.40%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 86.10%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 85.82%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 85.83%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 85.61%   [EVAL] batch:  106 | acc: 81.25%,  total acc: 85.57%   [EVAL] batch:  107 | acc: 56.25%,  total acc: 85.30%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 85.03%   [EVAL] batch:  109 | acc: 75.00%,  total acc: 84.94%   [EVAL] batch:  110 | acc: 81.25%,  total acc: 84.91%   [EVAL] batch:  111 | acc: 56.25%,  total acc: 84.65%   [EVAL] batch:  112 | acc: 87.50%,  total acc: 84.68%   [EVAL] batch:  113 | acc: 87.50%,  total acc: 84.70%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 84.84%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 84.81%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 84.83%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 84.85%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 84.77%   [EVAL] batch:  119 | acc: 31.25%,  total acc: 84.32%   [EVAL] batch:  120 | acc: 37.50%,  total acc: 83.94%   [EVAL] batch:  121 | acc: 37.50%,  total acc: 83.56%   [EVAL] batch:  122 | acc: 37.50%,  total acc: 83.18%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 82.61%   [EVAL] batch:  124 | acc: 43.75%,  total acc: 82.30%   
cur_acc:  ['0.9524', '0.6974']
his_acc:  ['0.9524', '0.8230']
Clustering into  14  clusters
Clusters:  [ 0  2  4  5  2  2  4  1 10 10  7  0  1  6  8  6 12 11  0 11  0  6  9 13
  5  2  3  0  1  0]
Losses:  12.074342727661133 4.509271144866943 0.6879026889801025
CurrentTrain: epoch  0, batch     0 | loss: 12.0743427Losses:  11.216690063476562 3.769230604171753 0.6832394599914551
CurrentTrain: epoch  0, batch     1 | loss: 11.2166901Losses:  10.83857536315918 2.920510768890381 0.6840914487838745
CurrentTrain: epoch  0, batch     2 | loss: 10.8385754Losses:  6.5882368087768555 -0.0 0.16591326892375946
CurrentTrain: epoch  0, batch     3 | loss: 6.5882368Losses:  9.371312141418457 2.8268582820892334 0.6802690029144287
CurrentTrain: epoch  1, batch     0 | loss: 9.3713121Losses:  11.928909301757812 4.7594428062438965 0.6481815576553345
CurrentTrain: epoch  1, batch     1 | loss: 11.9289093Losses:  12.333417892456055 5.039157390594482 0.7249231934547424
CurrentTrain: epoch  1, batch     2 | loss: 12.3334179Losses:  5.616758823394775 -0.0 0.25911960005760193
CurrentTrain: epoch  1, batch     3 | loss: 5.6167588Losses:  10.946077346801758 3.8507509231567383 0.6791536211967468
CurrentTrain: epoch  2, batch     0 | loss: 10.9460773Losses:  9.883955955505371 3.664175510406494 0.5707751512527466
CurrentTrain: epoch  2, batch     1 | loss: 9.8839560Losses:  9.316349029541016 2.899376153945923 0.6854507327079773
CurrentTrain: epoch  2, batch     2 | loss: 9.3163490Losses:  2.89737606048584 -0.0 0.1731606423854828
CurrentTrain: epoch  2, batch     3 | loss: 2.8973761Losses:  7.687398910522461 2.4393563270568848 0.7175605297088623
CurrentTrain: epoch  3, batch     0 | loss: 7.6873989Losses:  9.736942291259766 3.3735992908477783 0.7511734962463379
CurrentTrain: epoch  3, batch     1 | loss: 9.7369423Losses:  8.77729320526123 2.838479518890381 0.7254458069801331
CurrentTrain: epoch  3, batch     2 | loss: 8.7772932Losses:  3.8016300201416016 -0.0 -0.0
CurrentTrain: epoch  3, batch     3 | loss: 3.8016300Losses:  7.554444789886475 2.375241756439209 0.7384395003318787
CurrentTrain: epoch  4, batch     0 | loss: 7.5544448Losses:  8.26865291595459 2.9153971672058105 0.7150729298591614
CurrentTrain: epoch  4, batch     1 | loss: 8.2686529Losses:  10.174689292907715 3.8015198707580566 0.6889540553092957
CurrentTrain: epoch  4, batch     2 | loss: 10.1746893Losses:  2.741454839706421 -0.0 0.13560324907302856
CurrentTrain: epoch  4, batch     3 | loss: 2.7414548Losses:  8.466080665588379 3.7892062664031982 0.5346763730049133
CurrentTrain: epoch  5, batch     0 | loss: 8.4660807Losses:  9.911993026733398 4.589786529541016 0.6929775476455688
CurrentTrain: epoch  5, batch     1 | loss: 9.9119930Losses:  9.228006362915039 3.9435133934020996 0.5713539719581604
CurrentTrain: epoch  5, batch     2 | loss: 9.2280064Losses:  3.3385915756225586 -0.0 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 3.3385916Losses:  8.207267761230469 3.6483168601989746 0.6488811373710632
CurrentTrain: epoch  6, batch     0 | loss: 8.2072678Losses:  8.665680885314941 4.148982048034668 0.5846627950668335
CurrentTrain: epoch  6, batch     1 | loss: 8.6656809Losses:  8.639861106872559 3.508451223373413 0.6424875259399414
CurrentTrain: epoch  6, batch     2 | loss: 8.6398611Losses:  5.384613513946533 -0.0 0.10211174935102463
CurrentTrain: epoch  6, batch     3 | loss: 5.3846135Losses:  6.390195846557617 2.3038463592529297 0.6109785437583923
CurrentTrain: epoch  7, batch     0 | loss: 6.3901958Losses:  9.217738151550293 4.124868392944336 0.6258534789085388
CurrentTrain: epoch  7, batch     1 | loss: 9.2177382Losses:  7.284574031829834 2.7893428802490234 0.6397405862808228
CurrentTrain: epoch  7, batch     2 | loss: 7.2845740Losses:  1.989459753036499 -0.0 0.19528020918369293
CurrentTrain: epoch  7, batch     3 | loss: 1.9894598Losses:  8.694271087646484 3.6965203285217285 0.6112248301506042
CurrentTrain: epoch  8, batch     0 | loss: 8.6942711Losses:  6.369172096252441 2.2796130180358887 0.6973685026168823
CurrentTrain: epoch  8, batch     1 | loss: 6.3691721Losses:  7.530348777770996 3.5846641063690186 0.6177207827568054
CurrentTrain: epoch  8, batch     2 | loss: 7.5303488Losses:  1.991208791732788 -0.0 0.11562822014093399
CurrentTrain: epoch  8, batch     3 | loss: 1.9912088Losses:  7.9287109375 3.53576397895813 0.6209394335746765
CurrentTrain: epoch  9, batch     0 | loss: 7.9287109Losses:  6.257504940032959 2.141348361968994 0.5931180119514465
CurrentTrain: epoch  9, batch     1 | loss: 6.2575049Losses:  7.946773052215576 3.8385729789733887 0.680318295955658
CurrentTrain: epoch  9, batch     2 | loss: 7.9467731Losses:  1.8863518238067627 -0.0 0.11215239763259888
CurrentTrain: epoch  9, batch     3 | loss: 1.8863518
#### main subject ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "main subject" means primary topic of a work.
Here is an example:
Relation: main subject
Context: auerbach is prominently featured in the documentary film , " the first basket " , about jewish basketball history .
Head Entity: the first basket
Tail Entity: basketball
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "main subject":
Relation: main subject  
Context: the book "sapiens: a brief history of humankind" explores the evolution of human societies and cultures.  
Head Entity: sapiens: a brief history of humankind  
Tail Entity: human societies  

Relation: main subject  
Context: the documentary "our planet" showcases the beauty of nature and the impact of climate change on wildlife.  
Head Entity: our planet  
Tail Entity: nature  
#### participating team ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participating team" means teams that actively takes/took part in an event or process.
Here is an example:
Relation: participating team
Context: group b of uefa euro 2016 contained england , russia , wales and slovakia .
Head Entity: uefa euro 2016
Tail Entity: slovakia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participating team":
Relation: participating team  
Context: the 2020 summer olympics featured countries like the united states, china, and japan competing in various sports.  
Head Entity: 2020 summer olympics  
Tail Entity: japan  

Relation: participating team  
Context: the fifa world cup 2018 saw teams such as france, croatia, and belgium battling for the championship title.  
Head Entity: fifa world cup 2018  
Tail Entity: croatia  
#### has part ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "has part" means part of this subject.
Here is an example:
Relation: has part
Context: among the competing artists was previous eurovision song contest entrant robin juhkental who represented estonia in 2010 as part of the group malcolm lincoln .
Head Entity: malcolm lincoln
Tail Entity: robin juhkental
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "has part":
Relation: has part  
Context: The famous painting "The Last Supper" features several apostles, including Peter, who is depicted prominently on the left side of the artwork.  
Head Entity: The Last Supper  
Tail Entity: Peter  

Relation: has part  
Context: The smartphone model Galaxy S21 comes with a variety of features, including a high-resolution camera that is part of its advanced photography capabilities.  
Head Entity: Galaxy S21  
Tail Entity: high-resolution camera  
#### nominated for ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "nominated for" means award nomination received by a person, organisation or creative work.
Here is an example:
Relation: nominated for
Context: gyllenhaal was nominated for an academy award for best supporting actress for her performance .
Head Entity: gyllenhaal
Tail Entity: academy award for best supporting actress
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "nominated for":
Relation: nominated for  
Context: the film was nominated for several prestigious awards, including the golden globe for best drama.  
Head Entity: the film  
Tail Entity: golden globe for best drama  

Relation: nominated for  
Context: she was nominated for the emmy award for her outstanding performance in the television series.  
Head Entity: she  
Tail Entity: emmy award for her outstanding performance in the television series  
#### instance of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instance of" means that class of which this subject is a particular example and member.
Here is an example:
Relation: instance of
Context: on march 2015 , cube entertainment launched the multi - national girl group clc including original members seunghee , yujin , seungyeon , sorn and yeeun .
Head Entity: clc
Tail Entity: girl group
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instance of":
Relation: instance of  
Context: The smartphone brand Apple released its latest model, the iPhone 14, which features advanced technology and improved camera capabilities.  
Head Entity: iPhone 14  
Tail Entity: smartphone  

Relation: instance of  
Context: The famous painting "Starry Night" was created by the artist Vincent van Gogh and is considered one of the most recognized works in the history of Western art.  
Head Entity: Starry Night  
Tail Entity: painting  
#### mountain range ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mountain range" means range or subrange to which the geographical item belongs.
Here is an example:
Relation: mountain range
Context: brand peak is a sharp snow - covered peak located east - southeast of the eternity range and northwest of mount duemler , in palmer land .
Head Entity: mount duemler
Tail Entity: eternity range
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mountain range":
Relation: mountain range  
Context: the sierra nevada is a major mountain range in the western united states, stretching from the central valley of california to the great basin.  
Head Entity: sierra nevada  
Tail Entity: great basin  

Relation: mountain range  
Context: the appalachian mountains are a vast system of mountains in eastern north america, extending from canada down to alabama.  
Head Entity: appalachian mountains  
Tail Entity: alabama  
#### screenwriter ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "screenwriter" means person who wrote the script for subject item.
Here is an example:
Relation: screenwriter
Context: future films include " mcqueen " , ian bonhote ’s documentary about the fashion designer alexander mcqueen , brad anderson ’s thriller " beirut " , and mark pellington ’s drama " nostalgia " .
Head Entity: nostalgia
Tail Entity: mark pellington
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "screenwriter":
Relation: screenwriter  
Context: the upcoming movie "the great adventure" is penned by renowned screenwriter jessica taylor, who has previously won multiple awards for her work in the industry.  
Head Entity: the great adventure  
Tail Entity: jessica taylor  

Relation: screenwriter  
Context: in a recent interview, director samuel l. jackson revealed that the screenplay for "the last journey" was crafted by the talented writer robert m. clark, known for his gripping narratives.  
Head Entity: the last journey  
Tail Entity: robert m. clark  
#### language of work or name ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "language of work or name" means language associated with this creative work (such as books, shows, songs, broadcasts or websites) or a name.
Here is an example:
Relation: language of work or name
Context: the publisher found success in 1993 with tarō gomi 's " everyone poops " , a japanese language import that sold over one million copies and is now their best - known title .
Head Entity: everyone poops
Tail Entity: japanese language
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "language of work or name":
Relation: language of work or name  
Context: The animated series "Avatar: The Last Airbender" features a rich narrative that incorporates various cultural elements, primarily drawing from Chinese and Inuit languages.  
Head Entity: Avatar: The Last Airbender  
Tail Entity: Chinese and Inuit languages  

Relation: language of work or name  
Context: The novel "One Hundred Years of Solitude" by Gabriel García Márquez is celebrated for its magical realism and is originally written in Spanish, reflecting the author's cultural heritage.  
Head Entity: One Hundred Years of Solitude  
Tail Entity: Spanish
#### operator ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "operator" means person, profession, or organization that operates the equipment, facility, or service.
Here is an example:
Relation: operator
Context: it hosts both freight traffic and metra 's union pacific / northwest line commuter rail service .
Head Entity: union pacific / northwest line
Tail Entity: metra
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "operator":
Relation: operator  
Context: The city council has contracted with GreenTech Solutions to manage the waste disposal services in the area.  
Head Entity: GreenTech Solutions  
Tail Entity: city council  

Relation: operator  
Context: The airline has partnered with SkyHigh Services to provide ground handling for its international flights.  
Head Entity: SkyHigh Services  
Tail Entity: airline  
#### religion ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "religion" means religion of a person, organization or religious building, or associated with this subject.
Here is an example:
Relation: religion
Context: baianism is a term applied to the theology of catholic theologian michael baius ( 1513 - 1589 ) .
Head Entity: michael baius
Tail Entity: catholic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "religion":
Relation: religion  
Context: the church of england is a major christian denomination in england, known for its unique traditions and beliefs.  
Head Entity: church of england  
Tail Entity: christian  

Relation: religion  
Context: the baha'i faith emphasizes the spiritual unity of all humankind and was founded by baha'u'llah in the 19th century.  
Head Entity: baha'i faith  
Tail Entity: baha'u'llah  
Losses:  5.198183059692383 0.5660143494606018 0.9360554814338684
MemoryTrain:  epoch  0, batch     0 | loss: 5.1981831Losses:  5.867764949798584 0.44928309321403503 0.7925366163253784
MemoryTrain:  epoch  0, batch     1 | loss: 5.8677649Losses:  5.644007682800293 0.9301014542579651 0.7736694812774658
MemoryTrain:  epoch  0, batch     2 | loss: 5.6440077Losses:  6.973997116088867 0.6631511449813843 0.821997344493866
MemoryTrain:  epoch  0, batch     3 | loss: 6.9739971Losses:  6.01597261428833 0.8222327828407288 0.6998606324195862
MemoryTrain:  epoch  0, batch     4 | loss: 6.0159726Losses:  5.69832706451416 1.0528558492660522 0.45021766424179077
MemoryTrain:  epoch  0, batch     5 | loss: 5.6983271Losses:  6.250810146331787 1.2073817253112793 0.7492693066596985
MemoryTrain:  epoch  1, batch     0 | loss: 6.2508101Losses:  6.74620246887207 1.0474423170089722 0.7615538835525513
MemoryTrain:  epoch  1, batch     1 | loss: 6.7462025Losses:  4.591895580291748 0.2523396611213684 0.8079503178596497
MemoryTrain:  epoch  1, batch     2 | loss: 4.5918956Losses:  5.113023281097412 0.7263569235801697 0.7740562558174133
MemoryTrain:  epoch  1, batch     3 | loss: 5.1130233Losses:  4.0101213455200195 0.2187257707118988 0.6854283809661865
MemoryTrain:  epoch  1, batch     4 | loss: 4.0101213Losses:  4.637665748596191 -0.0 0.7675018310546875
MemoryTrain:  epoch  1, batch     5 | loss: 4.6376657Losses:  4.397689342498779 0.5061115026473999 0.8306223154067993
MemoryTrain:  epoch  2, batch     0 | loss: 4.3976893Losses:  4.862770080566406 0.3771214485168457 0.7437388896942139
MemoryTrain:  epoch  2, batch     1 | loss: 4.8627701Losses:  4.694160461425781 0.679121732711792 0.8645233511924744
MemoryTrain:  epoch  2, batch     2 | loss: 4.6941605Losses:  5.241135597229004 0.8157376050949097 0.670674204826355
MemoryTrain:  epoch  2, batch     3 | loss: 5.2411356Losses:  3.6841373443603516 0.2926263213157654 0.732046902179718
MemoryTrain:  epoch  2, batch     4 | loss: 3.6841373Losses:  4.459383010864258 0.7151488065719604 0.6259083151817322
MemoryTrain:  epoch  2, batch     5 | loss: 4.4593830Losses:  4.479979515075684 0.5868000388145447 0.8281974792480469
MemoryTrain:  epoch  3, batch     0 | loss: 4.4799795Losses:  3.9512104988098145 0.5752196311950684 0.6635140180587769
MemoryTrain:  epoch  3, batch     1 | loss: 3.9512105Losses:  4.374779224395752 0.7531161904335022 0.6896683573722839
MemoryTrain:  epoch  3, batch     2 | loss: 4.3747792Losses:  5.000361919403076 0.8466638326644897 0.7868799567222595
MemoryTrain:  epoch  3, batch     3 | loss: 5.0003619Losses:  4.506079196929932 1.3104381561279297 0.6939989328384399
MemoryTrain:  epoch  3, batch     4 | loss: 4.5060792Losses:  2.8656809329986572 -0.0 0.5938407778739929
MemoryTrain:  epoch  3, batch     5 | loss: 2.8656809Losses:  4.69173526763916 0.9635535478591919 0.8468595743179321
MemoryTrain:  epoch  4, batch     0 | loss: 4.6917353Losses:  3.2417638301849365 0.5048055052757263 0.5458028316497803
MemoryTrain:  epoch  4, batch     1 | loss: 3.2417638Losses:  3.6088967323303223 0.8575076460838318 0.7081800699234009
MemoryTrain:  epoch  4, batch     2 | loss: 3.6088967Losses:  5.352471351623535 1.0694196224212646 0.8837167620658875
MemoryTrain:  epoch  4, batch     3 | loss: 5.3524714Losses:  3.214360237121582 0.46471869945526123 0.7369459867477417
MemoryTrain:  epoch  4, batch     4 | loss: 3.2143602Losses:  3.2662603855133057 -0.0 0.4575119912624359
MemoryTrain:  epoch  4, batch     5 | loss: 3.2662604Losses:  4.177014350891113 0.8569296598434448 0.9138110876083374
MemoryTrain:  epoch  5, batch     0 | loss: 4.1770144Losses:  3.8152174949645996 0.8301989436149597 0.6751434803009033
MemoryTrain:  epoch  5, batch     1 | loss: 3.8152175Losses:  3.218830108642578 0.461744487285614 0.7988311052322388
MemoryTrain:  epoch  5, batch     2 | loss: 3.2188301Losses:  3.940230369567871 0.962778627872467 0.6570464372634888
MemoryTrain:  epoch  5, batch     3 | loss: 3.9402304Losses:  3.882978916168213 0.5726445913314819 0.8258438110351562
MemoryTrain:  epoch  5, batch     4 | loss: 3.8829789Losses:  3.7676095962524414 0.6721790432929993 0.546918511390686
MemoryTrain:  epoch  5, batch     5 | loss: 3.7676096Losses:  3.8427531719207764 0.815210223197937 0.6914488673210144
MemoryTrain:  epoch  6, batch     0 | loss: 3.8427532Losses:  3.031869649887085 -0.0 0.8146364092826843
MemoryTrain:  epoch  6, batch     1 | loss: 3.0318696Losses:  2.7985191345214844 0.25905439257621765 0.7964777946472168
MemoryTrain:  epoch  6, batch     2 | loss: 2.7985191Losses:  5.261227607727051 1.8275487422943115 0.827598512172699
MemoryTrain:  epoch  6, batch     3 | loss: 5.2612276Losses:  3.477602481842041 0.8389831185340881 0.7437559962272644
MemoryTrain:  epoch  6, batch     4 | loss: 3.4776025Losses:  3.28194522857666 0.3420792520046234 0.5551196336746216
MemoryTrain:  epoch  6, batch     5 | loss: 3.2819452Losses:  3.9586479663848877 0.844559907913208 0.7465710043907166
MemoryTrain:  epoch  7, batch     0 | loss: 3.9586480Losses:  3.2750344276428223 0.5381143093109131 0.7075850963592529
MemoryTrain:  epoch  7, batch     1 | loss: 3.2750344Losses:  4.131428241729736 1.2054693698883057 0.766555666923523
MemoryTrain:  epoch  7, batch     2 | loss: 4.1314282Losses:  4.111665725708008 1.0417555570602417 0.6491621732711792
MemoryTrain:  epoch  7, batch     3 | loss: 4.1116657Losses:  2.808056354522705 0.7501252889633179 0.7169986963272095
MemoryTrain:  epoch  7, batch     4 | loss: 2.8080564Losses:  3.2929799556732178 0.23825548589229584 0.5402578711509705
MemoryTrain:  epoch  7, batch     5 | loss: 3.2929800Losses:  3.4860899448394775 0.7742534875869751 0.8641408085823059
MemoryTrain:  epoch  8, batch     0 | loss: 3.4860899Losses:  3.394475221633911 0.5406460762023926 0.8168061375617981
MemoryTrain:  epoch  8, batch     1 | loss: 3.3944752Losses:  2.8909780979156494 0.9635531902313232 0.4088200032711029
MemoryTrain:  epoch  8, batch     2 | loss: 2.8909781Losses:  3.543513774871826 0.573292076587677 0.8691139221191406
MemoryTrain:  epoch  8, batch     3 | loss: 3.5435138Losses:  3.4816393852233887 1.0079532861709595 0.6530179381370544
MemoryTrain:  epoch  8, batch     4 | loss: 3.4816394Losses:  3.5440855026245117 0.5386276245117188 0.692196249961853
MemoryTrain:  epoch  8, batch     5 | loss: 3.5440855Losses:  3.0666563510894775 0.7375074625015259 0.7222580909729004
MemoryTrain:  epoch  9, batch     0 | loss: 3.0666564Losses:  2.598491668701172 0.29283708333969116 0.7646225094795227
MemoryTrain:  epoch  9, batch     1 | loss: 2.5984917Losses:  2.9197640419006348 0.7904722690582275 0.7141197919845581
MemoryTrain:  epoch  9, batch     2 | loss: 2.9197640Losses:  3.7451601028442383 0.572955846786499 0.8485199809074402
MemoryTrain:  epoch  9, batch     3 | loss: 3.7451601Losses:  3.4491076469421387 1.062692642211914 0.623993992805481
MemoryTrain:  epoch  9, batch     4 | loss: 3.4491076Losses:  3.2869536876678467 -0.0 0.7310857176780701
MemoryTrain:  epoch  9, batch     5 | loss: 3.2869537
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 70.83%   [EVAL] batch:    3 | acc: 68.75%,  total acc: 70.31%   [EVAL] batch:    4 | acc: 75.00%,  total acc: 71.25%   [EVAL] batch:    5 | acc: 75.00%,  total acc: 71.88%   [EVAL] batch:    6 | acc: 93.75%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 100.00%,  total acc: 78.12%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 79.17%   [EVAL] batch:    9 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 81.82%   [EVAL] batch:   11 | acc: 100.00%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 82.21%   [EVAL] batch:   13 | acc: 18.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 18.75%,  total acc: 73.75%   [EVAL] batch:   15 | acc: 12.50%,  total acc: 69.92%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 66.54%   [EVAL] batch:   17 | acc: 43.75%,  total acc: 65.28%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 63.49%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 65.31%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 66.96%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 68.18%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 69.57%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 70.83%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 71.75%   [EVAL] batch:   25 | acc: 18.75%,  total acc: 69.71%   [EVAL] batch:   26 | acc: 25.00%,  total acc: 68.06%   [EVAL] batch:   27 | acc: 25.00%,  total acc: 66.52%   [EVAL] batch:   28 | acc: 25.00%,  total acc: 65.09%   [EVAL] batch:   29 | acc: 18.75%,  total acc: 63.54%   [EVAL] batch:   30 | acc: 12.50%,  total acc: 61.90%   [EVAL] batch:   31 | acc: 81.25%,  total acc: 62.50%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 63.07%   [EVAL] batch:   33 | acc: 87.50%,  total acc: 63.79%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 64.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 65.62%   [EVAL] batch:   36 | acc: 93.75%,  total acc: 66.39%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 67.27%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 68.11%   [EVAL] batch:   39 | acc: 87.50%,  total acc: 68.59%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 69.21%   [EVAL] batch:   41 | acc: 93.75%,  total acc: 69.79%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 70.49%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 70.88%   [EVAL] batch:   44 | acc: 37.50%,  total acc: 70.14%   [EVAL] batch:   45 | acc: 62.50%,  total acc: 69.97%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 69.41%   [EVAL] batch:   47 | acc: 43.75%,  total acc: 68.88%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 68.49%   [EVAL] batch:   49 | acc: 43.75%,  total acc: 68.00%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 67.65%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 67.43%   [EVAL] batch:   52 | acc: 62.50%,  total acc: 67.33%   [EVAL] batch:   53 | acc: 62.50%,  total acc: 67.25%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 67.05%   [EVAL] batch:   55 | acc: 68.75%,  total acc: 67.08%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 67.43%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 67.78%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 68.22%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 68.44%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 68.75%   [EVAL] batch:   61 | acc: 75.00%,  total acc: 68.85%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 68.45%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 81.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 83.33%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 83.04%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 82.81%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 82.64%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 83.75%   [EVAL] batch:   10 | acc: 75.00%,  total acc: 82.95%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 83.33%   [EVAL] batch:   12 | acc: 81.25%,  total acc: 83.17%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 83.93%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 84.58%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 86.03%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 86.46%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 87.17%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 87.50%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 87.80%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 87.78%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 88.32%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 88.54%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 88.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 88.70%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 89.12%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 89.51%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 89.66%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 90.00%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 90.32%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 90.62%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 91.18%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 91.25%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 91.49%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 91.72%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 91.94%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 92.15%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 92.34%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 92.53%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 92.71%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 92.59%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 92.61%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 92.66%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 92.42%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 92.32%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 92.47%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 92.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 92.40%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 92.43%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 92.45%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 92.59%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 92.50%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 92.52%   [EVAL] batch:   56 | acc: 93.75%,  total acc: 92.54%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 92.46%   [EVAL] batch:   58 | acc: 100.00%,  total acc: 92.58%   [EVAL] batch:   59 | acc: 93.75%,  total acc: 92.60%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 92.73%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 92.84%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 92.16%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 90.82%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 89.42%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 88.07%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 86.75%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 85.57%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 84.69%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 84.73%   [EVAL] batch:   70 | acc: 75.00%,  total acc: 84.60%   [EVAL] batch:   71 | acc: 75.00%,  total acc: 84.46%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 84.50%   [EVAL] batch:   73 | acc: 87.50%,  total acc: 84.54%   [EVAL] batch:   74 | acc: 87.50%,  total acc: 84.58%   [EVAL] batch:   75 | acc: 100.00%,  total acc: 84.79%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 84.82%   [EVAL] batch:   77 | acc: 81.25%,  total acc: 84.78%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 84.81%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 84.84%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 85.03%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 84.30%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 83.43%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 82.59%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 81.84%   [EVAL] batch:   85 | acc: 25.00%,  total acc: 81.18%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 80.60%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 80.26%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 80.48%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 80.69%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 80.77%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 80.98%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 81.18%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 81.32%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 81.51%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 81.71%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 82.08%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 82.26%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 82.44%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 82.18%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 81.99%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 81.74%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 81.49%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 81.55%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 81.37%   [EVAL] batch:  106 | acc: 75.00%,  total acc: 81.31%   [EVAL] batch:  107 | acc: 37.50%,  total acc: 80.90%   [EVAL] batch:  108 | acc: 56.25%,  total acc: 80.68%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 80.45%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 80.29%   [EVAL] batch:  111 | acc: 50.00%,  total acc: 80.02%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 79.87%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 79.88%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 80.05%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 80.01%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 80.07%   [EVAL] batch:  117 | acc: 81.25%,  total acc: 80.08%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 80.04%   [EVAL] batch:  119 | acc: 43.75%,  total acc: 79.74%   [EVAL] batch:  120 | acc: 50.00%,  total acc: 79.49%   [EVAL] batch:  121 | acc: 43.75%,  total acc: 79.20%   [EVAL] batch:  122 | acc: 43.75%,  total acc: 78.91%   [EVAL] batch:  123 | acc: 37.50%,  total acc: 78.58%   [EVAL] batch:  124 | acc: 62.50%,  total acc: 78.45%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 78.37%   [EVAL] batch:  126 | acc: 81.25%,  total acc: 78.40%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 78.27%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 78.20%   [EVAL] batch:  129 | acc: 75.00%,  total acc: 78.17%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 78.15%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 78.27%   [EVAL] batch:  132 | acc: 100.00%,  total acc: 78.43%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 78.50%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 78.56%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 78.72%   [EVAL] batch:  136 | acc: 100.00%,  total acc: 78.88%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 78.80%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 78.37%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 77.95%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 77.48%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 77.02%   [EVAL] batch:  142 | acc: 43.75%,  total acc: 76.79%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 76.48%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 76.64%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 76.80%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 76.91%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 77.07%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 77.22%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 77.33%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 76.95%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 76.60%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 76.27%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 75.93%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 75.56%   [EVAL] batch:  155 | acc: 12.50%,  total acc: 75.16%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 75.20%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 75.24%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 75.31%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 75.43%   [EVAL] batch:  160 | acc: 100.00%,  total acc: 75.58%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 75.69%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 75.99%   [EVAL] batch:  164 | acc: 87.50%,  total acc: 76.06%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 76.17%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 76.27%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 76.41%   [EVAL] batch:  168 | acc: 87.50%,  total acc: 76.48%   [EVAL] batch:  169 | acc: 37.50%,  total acc: 76.25%   [EVAL] batch:  170 | acc: 62.50%,  total acc: 76.17%   [EVAL] batch:  171 | acc: 43.75%,  total acc: 75.98%   [EVAL] batch:  172 | acc: 43.75%,  total acc: 75.79%   [EVAL] batch:  173 | acc: 50.00%,  total acc: 75.65%   [EVAL] batch:  174 | acc: 43.75%,  total acc: 75.46%   [EVAL] batch:  175 | acc: 50.00%,  total acc: 75.32%   [EVAL] batch:  176 | acc: 56.25%,  total acc: 75.21%   [EVAL] batch:  177 | acc: 62.50%,  total acc: 75.14%   [EVAL] batch:  178 | acc: 62.50%,  total acc: 75.07%   [EVAL] batch:  179 | acc: 56.25%,  total acc: 74.97%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 74.93%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 75.00%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 75.07%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 75.17%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 75.20%   [EVAL] batch:  185 | acc: 87.50%,  total acc: 75.27%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 75.27%   [EVAL] batch:  187 | acc: 43.75%,  total acc: 75.10%   
cur_acc:  ['0.9524', '0.6974', '0.6845']
his_acc:  ['0.9524', '0.8230', '0.7510']
Clustering into  19  clusters
Clusters:  [ 8  4  1  2  4  4  1  6 14 14 10  8 17  0 11  0 15  7  3  7  3  0 13 16
  2  4 18  9  6  3 12  0  7  5  9  0  7  8 14  8]
Losses:  11.197484970092773 3.6685385704040527 0.5588645339012146
CurrentTrain: epoch  0, batch     0 | loss: 11.1974850Losses:  10.848410606384277 4.064213752746582 0.8197936415672302
CurrentTrain: epoch  0, batch     1 | loss: 10.8484106Losses:  12.054984092712402 4.411190032958984 0.7168018221855164
CurrentTrain: epoch  0, batch     2 | loss: 12.0549841Losses:  6.154847621917725 -0.0 0.14643022418022156
CurrentTrain: epoch  0, batch     3 | loss: 6.1548476Losses:  10.116682052612305 4.309293270111084 0.5949425101280212
CurrentTrain: epoch  1, batch     0 | loss: 10.1166821Losses:  10.749614715576172 3.9504733085632324 0.667270839214325
CurrentTrain: epoch  1, batch     1 | loss: 10.7496147Losses:  9.251348495483398 3.431304931640625 0.583419680595398
CurrentTrain: epoch  1, batch     2 | loss: 9.2513485Losses:  5.179866790771484 -0.0 -0.0
CurrentTrain: epoch  1, batch     3 | loss: 5.1798668Losses:  10.019230842590332 3.4482712745666504 0.6901522874832153
CurrentTrain: epoch  2, batch     0 | loss: 10.0192308Losses:  10.138967514038086 5.570554733276367 0.5656225681304932
CurrentTrain: epoch  2, batch     1 | loss: 10.1389675Losses:  9.07819652557373 4.394120216369629 0.5010389089584351
CurrentTrain: epoch  2, batch     2 | loss: 9.0781965Losses:  7.999286651611328 -0.0 0.14113754034042358
CurrentTrain: epoch  2, batch     3 | loss: 7.9992867Losses:  7.870000839233398 2.994419813156128 0.5685568451881409
CurrentTrain: epoch  3, batch     0 | loss: 7.8700008Losses:  8.3204345703125 3.394404172897339 0.4429503083229065
CurrentTrain: epoch  3, batch     1 | loss: 8.3204346Losses:  8.130770683288574 3.5906591415405273 0.6500489711761475
CurrentTrain: epoch  3, batch     2 | loss: 8.1307707Losses:  6.709183216094971 -0.0 0.17276421189308167
CurrentTrain: epoch  3, batch     3 | loss: 6.7091832Losses:  7.653133392333984 2.9786739349365234 0.5194729566574097
CurrentTrain: epoch  4, batch     0 | loss: 7.6531334Losses:  9.920023918151855 5.239487648010254 0.4428934156894684
CurrentTrain: epoch  4, batch     1 | loss: 9.9200239Losses:  6.73460054397583 2.3881494998931885 0.636063277721405
CurrentTrain: epoch  4, batch     2 | loss: 6.7346005Losses:  3.347687244415283 -0.0 0.21467021107673645
CurrentTrain: epoch  4, batch     3 | loss: 3.3476872Losses:  6.788326263427734 2.808053731918335 0.6583292484283447
CurrentTrain: epoch  5, batch     0 | loss: 6.7883263Losses:  7.52313232421875 3.9222660064697266 0.5378379821777344
CurrentTrain: epoch  5, batch     1 | loss: 7.5231323Losses:  7.601991653442383 3.422440528869629 0.6376426219940186
CurrentTrain: epoch  5, batch     2 | loss: 7.6019917Losses:  7.96043586730957 -0.0 -0.0
CurrentTrain: epoch  5, batch     3 | loss: 7.9604359Losses:  7.973045825958252 3.6897730827331543 0.6269887089729309
CurrentTrain: epoch  6, batch     0 | loss: 7.9730458Losses:  6.637783050537109 3.0469202995300293 0.5449745655059814
CurrentTrain: epoch  6, batch     1 | loss: 6.6377831Losses:  6.660160064697266 3.221696615219116 0.4478263854980469
CurrentTrain: epoch  6, batch     2 | loss: 6.6601601Losses:  4.762916088104248 -0.0 0.16510522365570068
CurrentTrain: epoch  6, batch     3 | loss: 4.7629161Losses:  6.417564868927002 2.4895741939544678 0.6216808557510376
CurrentTrain: epoch  7, batch     0 | loss: 6.4175649Losses:  6.551837921142578 3.691899538040161 0.5720913410186768
CurrentTrain: epoch  7, batch     1 | loss: 6.5518379Losses:  7.8913068771362305 3.690561532974243 0.6200407147407532
CurrentTrain: epoch  7, batch     2 | loss: 7.8913069Losses:  4.780716419219971 -0.0 0.10395608842372894
CurrentTrain: epoch  7, batch     3 | loss: 4.7807164Losses:  6.7356486320495605 3.4808034896850586 0.5522741079330444
CurrentTrain: epoch  8, batch     0 | loss: 6.7356486Losses:  6.156252384185791 2.7559733390808105 0.5193559527397156
CurrentTrain: epoch  8, batch     1 | loss: 6.1562524Losses:  5.343606948852539 1.84347665309906 0.5952061414718628
CurrentTrain: epoch  8, batch     2 | loss: 5.3436069Losses:  2.0524709224700928 -0.0 0.13015073537826538
CurrentTrain: epoch  8, batch     3 | loss: 2.0524709Losses:  6.639238357543945 3.586622953414917 0.43220099806785583
CurrentTrain: epoch  9, batch     0 | loss: 6.6392384Losses:  7.258212089538574 3.7543792724609375 0.6100103259086609
CurrentTrain: epoch  9, batch     1 | loss: 7.2582121Losses:  6.1513519287109375 3.3568170070648193 0.5203695297241211
CurrentTrain: epoch  9, batch     2 | loss: 6.1513519Losses:  2.6322927474975586 -0.0 0.1710096299648285
CurrentTrain: epoch  9, batch     3 | loss: 2.6322927
#### follows ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "follows" means immediately prior item in a series of which the subject is a part.
Here is an example:
Relation: follows
Context: ( 1988 ) , " night watch " ( 2004 ) , " day watch " ( 2006 ) and " legend № 17 " ( 2013 ) .
Head Entity: day watch
Tail Entity: night watch
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "follows":
Relation: follows  
Context: ( 1995 ) , " The Phantom Menace " ( 1999 ) , " Attack of the Clones " ( 2002 ) and " Revenge of the Sith " ( 2005 ) .  
Head Entity: Attack of the Clones  
Tail Entity: The Phantom Menace  

Relation: follows  
Context: ( 2010 ) , " The Hunger Games " ( 2012 ) , " Catching Fire " ( 2013 ) and " Mockingjay " ( 2014 ) .  
Head Entity: Catching Fire  
Tail Entity: The Hunger Games  
#### sport ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sport" means sport that the subject participates or participated in or is associated with.
Here is an example:
Relation: sport
Context: daniel fabrizi ( born february 22 , 1992 ) is a canadian soccer player who last played for toronto fc ii .
Head Entity: daniel fabrizi
Tail Entity: soccer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sport":
Relation: sport  
Context: serena williams, an american professional tennis player, has won 23 grand slam singles titles and is widely regarded as one of the greatest athletes in the sport.  
Head Entity: serena williams  
Tail Entity: tennis  

Relation: sport  
Context: lebron james, born on december 30, 1984, is an american basketball player who currently plays for the los angeles lakers in the national basketball association (nba).  
Head Entity: lebron james  
Tail Entity: basketball  
#### father ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "father" means male parent of the subject.
Here is an example:
Relation: father
Context: after the crowning of his nephew radoslav , the son of stefan , sava left the serbian maritime in 1229 for a trip to palestine .
Head Entity: radoslav
Tail Entity: stefan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "father":
Relation: father  
Context: in the historical records, it is noted that king henry was the father of prince edward, who later became a significant figure in the kingdom.  
Head Entity: prince edward  
Tail Entity: king henry  

Relation: father  
Context: during the family reunion, it was revealed that michael was the father of lucas, who had recently graduated from university.  
Head Entity: lucas  
Tail Entity: michael  
#### original network ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original network" means network or service that originally broadcast a radio or television program.
Here is an example:
Relation: original network
Context: on november 2 , 2016 msnbc 's " rachel maddow show " featured an expose on yahweh ben yahweh and his followers connection to the 2016 donald trump presidential campaign .
Head Entity: rachel maddow show
Tail Entity: msnbc
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original network":
Relation: original network  
Context: the popular series "breaking bad" premiered on january 20, 2008, on the amc network, quickly gaining a massive following and critical acclaim.  
Head Entity: breaking bad  
Tail Entity: amc  

Relation: original network  
Context: "the office," a beloved mockumentary-style sitcom, first aired on march 24, 2005, on nbc, becoming a cultural phenomenon over its nine-season run.  
Head Entity: the office  
Tail Entity: nbc  
#### original language of film or TV show ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "original language of film or TV show" means language in which a film or a performance work was originally created.
Here is an example:
Relation: original language of film or TV show
Context: the 1954 film , directed by kadri venkata reddy , " peddamanushulu " was honoured with the first president 's silver medal for best feature film in telugu .
Head Entity: peddamanushulu
Tail Entity: telugu
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "original language of film or TV show":
Relation: original language of film or TV show  
Context: The 1994 animated film "The Lion King," produced by Walt Disney Feature Animation, was originally created in English.  
Head Entity: The Lion King  
Tail Entity: English  

Relation: original language of film or TV show  
Context: The critically acclaimed 2001 film "Amélie," directed by Jean-Pierre Jeunet, was originally filmed in French.  
Head Entity: Amélie  
Tail Entity: French  
#### league ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "league" means league in which team or player plays or has played in.
Here is an example:
Relation: league
Context: in the summer of 2013 he returned to montenegro and joined with fk rudar pljevlja , playing with the club for two seasons in the montenegrin first league .
Head Entity: fk rudar pljevlja
Tail Entity: montenegrin first league
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "league":
Relation: league  
Context: After a successful stint in the English Premier League, the player signed with FC Barcelona, competing in La Liga for the next three seasons.  
Head Entity: FC Barcelona  
Tail Entity: La Liga  

Relation: league  
Context: The young athlete made headlines when he joined the New York Yankees, showcasing his talent in Major League Baseball.  
Head Entity: New York Yankees  
Tail Entity: Major League Baseball  
#### mother ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mother" means female parent of the subject.
Here is an example:
Relation: mother
Context: the first stewart king of scotland and son of marjorie bruce and walter stewart , robert ii , is believed to have been born in the abbey .
Head Entity: robert ii
Tail Entity: marjorie bruce
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mother":
Relation: mother  
Context: in the historical records, it is noted that queen elizabeth i was the daughter of anne boleyn, who served as her mother during her early years.  
Head Entity: queen elizabeth i  
Tail Entity: anne boleyn  

Relation: mother  
Context: the famous artist pablo picasso often credited his mother, maría ruiz, as a significant influence on his early artistic development.  
Head Entity: pablo picasso  
Tail Entity: maría ruiz  
#### instrument ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "instrument" means musical instrument that a person plays or teaches or used in a music occupation.
Here is an example:
Relation: instrument
Context: peter bastian ( 25 august 1943 in copenhagen – 27 march 2017 ) was a danish musician . peter bastian bio and cv he was educated in theoretical physics and classical bassoon .
Head Entity: peter bastian
Tail Entity: bassoon
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "instrument":
Relation: instrument  
Context: johann sebastian bach (31 march 1685 – 28 july 1750) was a german composer and musician of the baroque period. he was known for his exceptional skill in playing the organ and the harpsichord.  
Head Entity: johann sebastian bach  
Tail Entity: organ  

Relation: instrument  
Context: yo-yo ma, born on october 7, 1955, is a world-renowned cellist who has performed with numerous orchestras and in various genres. he is particularly celebrated for his interpretations of classical cello concertos.  
Head Entity: yo-yo ma  
Tail Entity: cello  
#### country ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country" means sovereign state of this item (not to be used for human beings).
Here is an example:
Relation: country
Context: the type locality given is " fan - si - pan mountains , tonkin , indo - china " ( mount fansipan , sa pa district , nghệ an province , northwestern vietnam ) .
Head Entity: sa pa district
Tail Entity: vietnam
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country":
Relation: country  
Context: the capital city of the country is located in the central region, known for its rich history and cultural heritage, particularly in the city of "hanoi" which serves as the political center of the nation.  
Head Entity: hanoi  
Tail Entity: vietnam  

Relation: country  
Context: the famous landmark "eiffel tower" is situated in the heart of the city, which is the capital of a country renowned for its art, fashion, and cuisine, specifically in "paris".  
Head Entity: paris  
Tail Entity: france  
#### characters ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "characters" means characters which appear in this item (like plays, operas, operettas, books, comics, films, TV series, video games).
Here is an example:
Relation: characters
Context: klaus kinski , who would play dracula himself nine years later in " nosferatu the vampyre " , is also featured in the film as renfield .
Head Entity: nosferatu the vampyre
Tail Entity: renfield
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "characters":
Relation: characters  
Context: in the animated series "avatar: the last airbender," zuko is a complex character who struggles with his identity and family legacy.  
Head Entity: avatar: the last airbender  
Tail Entity: zuko  

Relation: characters  
Context: the novel "pride and prejudice" features elizabeth bennet, a strong-willed protagonist who navigates societal expectations and romantic entanglements.  
Head Entity: pride and prejudice  
Tail Entity: elizabeth bennet  
Losses:  4.378107070922852 0.767604649066925 0.8955329656600952
MemoryTrain:  epoch  0, batch     0 | loss: 4.3781071Losses:  4.950927734375 1.7381209135055542 0.6619548797607422
MemoryTrain:  epoch  0, batch     1 | loss: 4.9509277Losses:  4.862232208251953 0.4940983057022095 0.9269400835037231
MemoryTrain:  epoch  0, batch     2 | loss: 4.8622322Losses:  6.011338233947754 1.6152465343475342 0.7763216495513916
MemoryTrain:  epoch  0, batch     3 | loss: 6.0113382Losses:  5.844969272613525 1.154371976852417 0.8932735323905945
MemoryTrain:  epoch  0, batch     4 | loss: 5.8449693Losses:  4.967286586761475 1.1922121047973633 0.6138615012168884
MemoryTrain:  epoch  0, batch     5 | loss: 4.9672866Losses:  5.023052215576172 0.9156968593597412 0.7976621389389038
MemoryTrain:  epoch  0, batch     6 | loss: 5.0230522Losses:  4.300765514373779 -0.0 0.601270854473114
MemoryTrain:  epoch  0, batch     7 | loss: 4.3007655Losses:  4.184756278991699 0.8004639148712158 0.6714209318161011
MemoryTrain:  epoch  1, batch     0 | loss: 4.1847563Losses:  5.48289680480957 0.3786311745643616 0.9812543392181396
MemoryTrain:  epoch  1, batch     1 | loss: 5.4828968Losses:  3.4243550300598145 0.7331379055976868 0.9392338991165161
MemoryTrain:  epoch  1, batch     2 | loss: 3.4243550Losses:  4.196481227874756 0.9392459392547607 0.8308383822441101
MemoryTrain:  epoch  1, batch     3 | loss: 4.1964812Losses:  4.543813228607178 0.5175671577453613 0.8977621793746948
MemoryTrain:  epoch  1, batch     4 | loss: 4.5438132Losses:  3.6919665336608887 0.7663586139678955 0.6709855794906616
MemoryTrain:  epoch  1, batch     5 | loss: 3.6919665Losses:  3.9915339946746826 0.5056635141372681 0.7020580768585205
MemoryTrain:  epoch  1, batch     6 | loss: 3.9915340Losses:  3.3292946815490723 0.3198089897632599 0.5405405759811401
MemoryTrain:  epoch  1, batch     7 | loss: 3.3292947Losses:  4.176514625549316 0.8507115244865417 0.8068764805793762
MemoryTrain:  epoch  2, batch     0 | loss: 4.1765146Losses:  4.100208282470703 1.1491203308105469 0.8394824862480164
MemoryTrain:  epoch  2, batch     1 | loss: 4.1002083Losses:  3.4217565059661865 0.4830748438835144 0.733154833316803
MemoryTrain:  epoch  2, batch     2 | loss: 3.4217565Losses:  4.077818393707275 0.8398386240005493 0.8797311782836914
MemoryTrain:  epoch  2, batch     3 | loss: 4.0778184Losses:  4.077744007110596 0.8598976731300354 0.7406076788902283
MemoryTrain:  epoch  2, batch     4 | loss: 4.0777440Losses:  3.6231117248535156 0.5765451192855835 0.7302637696266174
MemoryTrain:  epoch  2, batch     5 | loss: 3.6231117Losses:  3.6186938285827637 0.5983110666275024 0.8862247467041016
MemoryTrain:  epoch  2, batch     6 | loss: 3.6186938Losses:  4.084440231323242 0.3293229937553406 0.6541236042976379
MemoryTrain:  epoch  2, batch     7 | loss: 4.0844402Losses:  3.9121224880218506 0.3857019543647766 1.0190258026123047
MemoryTrain:  epoch  3, batch     0 | loss: 3.9121225Losses:  3.1891536712646484 0.2878585159778595 0.7994338870048523
MemoryTrain:  epoch  3, batch     1 | loss: 3.1891537Losses:  3.4035167694091797 1.0196120738983154 0.7938380241394043
MemoryTrain:  epoch  3, batch     2 | loss: 3.4035168Losses:  3.569289207458496 0.4656190574169159 0.8755439519882202
MemoryTrain:  epoch  3, batch     3 | loss: 3.5692892Losses:  2.9433250427246094 0.2116115242242813 0.8632521629333496
MemoryTrain:  epoch  3, batch     4 | loss: 2.9433250Losses:  3.353391408920288 0.32696816325187683 0.8552369475364685
MemoryTrain:  epoch  3, batch     5 | loss: 3.3533914Losses:  3.016448497772217 -0.0 1.0026527643203735
MemoryTrain:  epoch  3, batch     6 | loss: 3.0164485Losses:  2.3283398151397705 -0.0 0.5890635848045349
MemoryTrain:  epoch  3, batch     7 | loss: 2.3283398Losses:  3.590129852294922 0.8998530507087708 0.7536603212356567
MemoryTrain:  epoch  4, batch     0 | loss: 3.5901299Losses:  2.472113609313965 0.23085151612758636 0.6611374616622925
MemoryTrain:  epoch  4, batch     1 | loss: 2.4721136Losses:  2.8886754512786865 0.244599387049675 0.9410797953605652
MemoryTrain:  epoch  4, batch     2 | loss: 2.8886755Losses:  2.6989917755126953 -0.0 0.9805748462677002
MemoryTrain:  epoch  4, batch     3 | loss: 2.6989918Losses:  3.4071130752563477 0.47734811902046204 0.868270993232727
MemoryTrain:  epoch  4, batch     4 | loss: 3.4071131Losses:  3.46130108833313 0.5022018551826477 0.8066150546073914
MemoryTrain:  epoch  4, batch     5 | loss: 3.4613011Losses:  3.2076241970062256 0.279790997505188 0.9187003970146179
MemoryTrain:  epoch  4, batch     6 | loss: 3.2076242Losses:  2.130208969116211 -0.0 0.5492120981216431
MemoryTrain:  epoch  4, batch     7 | loss: 2.1302090Losses:  3.2979681491851807 0.25645267963409424 0.9071065783500671
MemoryTrain:  epoch  5, batch     0 | loss: 3.2979681Losses:  4.394079685211182 1.2095065116882324 0.9189010262489319
MemoryTrain:  epoch  5, batch     1 | loss: 4.3940797Losses:  2.378004550933838 -0.0 0.8476184606552124
MemoryTrain:  epoch  5, batch     2 | loss: 2.3780046Losses:  2.6823320388793945 -0.0 0.7602565884590149
MemoryTrain:  epoch  5, batch     3 | loss: 2.6823320Losses:  2.9131484031677246 0.44246894121170044 0.8395484089851379
MemoryTrain:  epoch  5, batch     4 | loss: 2.9131484Losses:  3.000013828277588 0.7241418361663818 0.8383978605270386
MemoryTrain:  epoch  5, batch     5 | loss: 3.0000138Losses:  2.9296977519989014 0.5468405485153198 0.7719051837921143
MemoryTrain:  epoch  5, batch     6 | loss: 2.9296978Losses:  3.254579544067383 0.710739016532898 0.5318727493286133
MemoryTrain:  epoch  5, batch     7 | loss: 3.2545795Losses:  3.4849495887756348 1.0815372467041016 0.7860793471336365
MemoryTrain:  epoch  6, batch     0 | loss: 3.4849496Losses:  3.0849084854125977 0.5454974174499512 0.8706780672073364
MemoryTrain:  epoch  6, batch     1 | loss: 3.0849085Losses:  3.360584259033203 1.017690658569336 0.7895006537437439
MemoryTrain:  epoch  6, batch     2 | loss: 3.3605843Losses:  3.3644890785217285 1.098667025566101 0.6283353567123413
MemoryTrain:  epoch  6, batch     3 | loss: 3.3644891Losses:  4.090023517608643 1.0025286674499512 0.992975652217865
MemoryTrain:  epoch  6, batch     4 | loss: 4.0900235Losses:  2.429157018661499 0.2525293529033661 0.7367232441902161
MemoryTrain:  epoch  6, batch     5 | loss: 2.4291570Losses:  2.978630781173706 0.6169575452804565 0.8663308620452881
MemoryTrain:  epoch  6, batch     6 | loss: 2.9786308Losses:  2.7657623291015625 -0.0 0.6242824792861938
MemoryTrain:  epoch  6, batch     7 | loss: 2.7657623Losses:  2.6702942848205566 0.4669591784477234 0.84077388048172
MemoryTrain:  epoch  7, batch     0 | loss: 2.6702943Losses:  2.721871852874756 0.2775834798812866 0.9835134148597717
MemoryTrain:  epoch  7, batch     1 | loss: 2.7218719Losses:  3.64182710647583 1.602386474609375 0.7001938223838806
MemoryTrain:  epoch  7, batch     2 | loss: 3.6418271Losses:  3.2274045944213867 0.8633295297622681 0.7338643074035645
MemoryTrain:  epoch  7, batch     3 | loss: 3.2274046Losses:  2.6823692321777344 0.2703917622566223 0.8319231867790222
MemoryTrain:  epoch  7, batch     4 | loss: 2.6823692Losses:  4.234997749328613 1.4816510677337646 0.7487833499908447
MemoryTrain:  epoch  7, batch     5 | loss: 4.2349977Losses:  2.966843605041504 0.49210506677627563 0.8761011958122253
MemoryTrain:  epoch  7, batch     6 | loss: 2.9668436Losses:  1.7757078409194946 -0.0 0.49494966864585876
MemoryTrain:  epoch  7, batch     7 | loss: 1.7757078Losses:  2.9552717208862305 0.5174316167831421 0.8446227312088013
MemoryTrain:  epoch  8, batch     0 | loss: 2.9552717Losses:  2.779773473739624 0.28304189443588257 0.8386633992195129
MemoryTrain:  epoch  8, batch     1 | loss: 2.7797735Losses:  2.4595022201538086 -0.0 0.906582236289978
MemoryTrain:  epoch  8, batch     2 | loss: 2.4595022Losses:  3.5164008140563965 0.6357281804084778 0.9869917631149292
MemoryTrain:  epoch  8, batch     3 | loss: 3.5164008Losses:  2.3368868827819824 -0.0 0.8572623133659363
MemoryTrain:  epoch  8, batch     4 | loss: 2.3368869Losses:  3.2139217853546143 1.1055597066879272 0.7989862561225891
MemoryTrain:  epoch  8, batch     5 | loss: 3.2139218Losses:  2.777374029159546 0.4988614618778229 0.8900327086448669
MemoryTrain:  epoch  8, batch     6 | loss: 2.7773740Losses:  2.007524251937866 -0.0 0.6562394499778748
MemoryTrain:  epoch  8, batch     7 | loss: 2.0075243Losses:  2.6321425437927246 0.26789379119873047 0.806463360786438
MemoryTrain:  epoch  9, batch     0 | loss: 2.6321425Losses:  2.851548671722412 0.22350192070007324 0.9932838082313538
MemoryTrain:  epoch  9, batch     1 | loss: 2.8515487Losses:  2.690363883972168 0.23644016683101654 0.8610209226608276
MemoryTrain:  epoch  9, batch     2 | loss: 2.6903639Losses:  2.6435294151306152 0.5396544933319092 0.7887539267539978
MemoryTrain:  epoch  9, batch     3 | loss: 2.6435294Losses:  3.4328606128692627 1.1141211986541748 0.8171402812004089
MemoryTrain:  epoch  9, batch     4 | loss: 3.4328606Losses:  2.7330853939056396 0.5759148597717285 0.8125030398368835
MemoryTrain:  epoch  9, batch     5 | loss: 2.7330854Losses:  2.3895421028137207 0.22935673594474792 0.8042951822280884
MemoryTrain:  epoch  9, batch     6 | loss: 2.3895421Losses:  2.1157703399658203 0.3354203701019287 0.5183405876159668
MemoryTrain:  epoch  9, batch     7 | loss: 2.1157703
[EVAL] batch:    0 | acc: 43.75%,  total acc: 43.75%   [EVAL] batch:    1 | acc: 56.25%,  total acc: 50.00%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 45.83%   [EVAL] batch:    3 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    4 | acc: 37.50%,  total acc: 47.50%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 44.79%   [EVAL] batch:    6 | acc: 37.50%,  total acc: 43.75%   [EVAL] batch:    7 | acc: 75.00%,  total acc: 47.66%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 50.00%   [EVAL] batch:    9 | acc: 81.25%,  total acc: 53.12%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 55.68%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 57.29%   [EVAL] batch:   12 | acc: 56.25%,  total acc: 57.21%   [EVAL] batch:   13 | acc: 37.50%,  total acc: 55.80%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 56.25%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 55.47%   [EVAL] batch:   16 | acc: 62.50%,  total acc: 55.88%   [EVAL] batch:   17 | acc: 50.00%,  total acc: 55.56%   [EVAL] batch:   18 | acc: 50.00%,  total acc: 55.26%   [EVAL] batch:   19 | acc: 100.00%,  total acc: 57.50%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 59.52%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 61.08%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.77%   [EVAL] batch:   23 | acc: 100.00%,  total acc: 64.32%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 65.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 70.04%   [EVAL] batch:   29 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 71.77%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 72.66%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 74.08%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 74.64%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 75.17%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 75.84%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 76.15%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 76.28%   [EVAL] batch:   39 | acc: 68.75%,  total acc: 76.09%   [EVAL] batch:   40 | acc: 75.00%,  total acc: 76.07%   [EVAL] batch:   41 | acc: 68.75%,  total acc: 75.89%   [EVAL] batch:   42 | acc: 62.50%,  total acc: 75.58%   [EVAL] batch:   43 | acc: 75.00%,  total acc: 75.57%   [EVAL] batch:   44 | acc: 93.75%,  total acc: 75.97%   [EVAL] batch:   45 | acc: 87.50%,  total acc: 76.22%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 76.73%   [EVAL] batch:   47 | acc: 81.25%,  total acc: 76.82%   [EVAL] batch:   48 | acc: 81.25%,  total acc: 76.91%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 77.25%   [EVAL] batch:   50 | acc: 50.00%,  total acc: 76.72%   [EVAL] batch:   51 | acc: 56.25%,  total acc: 76.32%   [EVAL] batch:   52 | acc: 43.75%,  total acc: 75.71%   [EVAL] batch:   53 | acc: 75.00%,  total acc: 75.69%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 75.68%   [EVAL] batch:   55 | acc: 37.50%,  total acc: 75.00%   [EVAL] batch:   56 | acc: 56.25%,  total acc: 74.67%   [EVAL] batch:   57 | acc: 81.25%,  total acc: 74.78%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 74.79%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 74.90%   [EVAL] batch:   60 | acc: 75.00%,  total acc: 74.90%   [EVAL] batch:   61 | acc: 93.75%,  total acc: 75.20%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 74.70%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 79.46%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 79.69%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 80.56%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 80.00%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 78.98%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 79.69%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 80.29%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 81.25%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 82.08%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 83.20%   [EVAL] batch:   16 | acc: 93.75%,  total acc: 83.82%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 84.38%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 85.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 85.71%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 85.80%   [EVAL] batch:   22 | acc: 87.50%,  total acc: 85.87%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 86.20%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 86.00%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 86.54%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 87.04%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.72%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 88.12%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 88.51%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.87%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 89.20%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 89.52%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.64%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.93%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 90.20%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 90.46%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.71%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.94%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 91.16%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 91.37%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 91.42%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 91.48%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 91.39%   [EVAL] batch:   45 | acc: 93.75%,  total acc: 91.44%   [EVAL] batch:   46 | acc: 81.25%,  total acc: 91.22%   [EVAL] batch:   47 | acc: 87.50%,  total acc: 91.15%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 91.33%   [EVAL] batch:   49 | acc: 93.75%,  total acc: 91.38%   [EVAL] batch:   50 | acc: 93.75%,  total acc: 91.42%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 91.47%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 91.51%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:   54 | acc: 87.50%,  total acc: 91.59%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 91.63%   [EVAL] batch:   56 | acc: 81.25%,  total acc: 91.45%   [EVAL] batch:   57 | acc: 62.50%,  total acc: 90.95%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 90.57%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 90.52%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 90.37%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 90.02%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 89.19%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 87.89%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 86.54%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 85.23%   [EVAL] batch:   66 | acc: 0.00%,  total acc: 83.96%   [EVAL] batch:   67 | acc: 6.25%,  total acc: 82.81%   [EVAL] batch:   68 | acc: 25.00%,  total acc: 81.97%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 82.05%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 81.87%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 81.60%   [EVAL] batch:   72 | acc: 87.50%,  total acc: 81.68%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 81.42%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 81.42%   [EVAL] batch:   75 | acc: 93.75%,  total acc: 81.58%   [EVAL] batch:   76 | acc: 87.50%,  total acc: 81.66%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 81.57%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 81.72%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 81.80%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 82.02%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 81.33%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 80.57%   [EVAL] batch:   83 | acc: 31.25%,  total acc: 79.99%   [EVAL] batch:   84 | acc: 18.75%,  total acc: 79.26%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 78.56%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 78.02%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 77.70%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 77.95%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 78.19%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 78.30%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 78.53%   [EVAL] batch:   92 | acc: 100.00%,  total acc: 78.76%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 78.92%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 79.14%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 79.36%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 79.57%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 79.78%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 79.99%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 80.19%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 79.95%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 79.78%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 79.55%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 79.33%   [EVAL] batch:  104 | acc: 81.25%,  total acc: 79.35%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 79.19%   [EVAL] batch:  106 | acc: 50.00%,  total acc: 78.91%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 78.47%   [EVAL] batch:  108 | acc: 31.25%,  total acc: 78.04%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 77.90%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 77.76%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 77.34%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 77.16%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 77.19%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 77.39%   [EVAL] batch:  115 | acc: 81.25%,  total acc: 77.42%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 77.51%   [EVAL] batch:  117 | acc: 93.75%,  total acc: 77.65%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 77.63%   [EVAL] batch:  119 | acc: 12.50%,  total acc: 77.08%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 76.55%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 75.97%   [EVAL] batch:  122 | acc: 18.75%,  total acc: 75.51%   [EVAL] batch:  123 | acc: 12.50%,  total acc: 75.00%   [EVAL] batch:  124 | acc: 18.75%,  total acc: 74.55%   [EVAL] batch:  125 | acc: 68.75%,  total acc: 74.50%   [EVAL] batch:  126 | acc: 56.25%,  total acc: 74.36%   [EVAL] batch:  127 | acc: 56.25%,  total acc: 74.22%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 74.22%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 74.18%   [EVAL] batch:  130 | acc: 75.00%,  total acc: 74.19%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 74.29%   [EVAL] batch:  132 | acc: 93.75%,  total acc: 74.44%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 74.49%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 74.58%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 74.77%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 74.91%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 74.82%   [EVAL] batch:  138 | acc: 18.75%,  total acc: 74.42%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 74.02%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 73.58%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 73.24%   [EVAL] batch:  142 | acc: 37.50%,  total acc: 72.99%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 72.70%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 72.84%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 73.03%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 73.17%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 73.35%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 73.53%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:  150 | acc: 31.25%,  total acc: 73.39%   [EVAL] batch:  151 | acc: 31.25%,  total acc: 73.11%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 72.88%   [EVAL] batch:  153 | acc: 25.00%,  total acc: 72.56%   [EVAL] batch:  154 | acc: 25.00%,  total acc: 72.26%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 71.92%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 71.97%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 72.03%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 72.17%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 72.30%   [EVAL] batch:  160 | acc: 93.75%,  total acc: 72.44%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 72.57%   [EVAL] batch:  162 | acc: 100.00%,  total acc: 72.74%   [EVAL] batch:  163 | acc: 100.00%,  total acc: 72.90%   [EVAL] batch:  164 | acc: 81.25%,  total acc: 72.95%   [EVAL] batch:  165 | acc: 93.75%,  total acc: 73.08%   [EVAL] batch:  166 | acc: 93.75%,  total acc: 73.20%   [EVAL] batch:  167 | acc: 100.00%,  total acc: 73.36%   [EVAL] batch:  168 | acc: 68.75%,  total acc: 73.34%   [EVAL] batch:  169 | acc: 6.25%,  total acc: 72.94%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 72.55%   [EVAL] batch:  171 | acc: 0.00%,  total acc: 72.13%   [EVAL] batch:  172 | acc: 6.25%,  total acc: 71.75%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 71.37%   [EVAL] batch:  174 | acc: 12.50%,  total acc: 71.04%   [EVAL] batch:  175 | acc: 43.75%,  total acc: 70.88%   [EVAL] batch:  176 | acc: 37.50%,  total acc: 70.69%   [EVAL] batch:  177 | acc: 56.25%,  total acc: 70.61%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 70.53%   [EVAL] batch:  179 | acc: 50.00%,  total acc: 70.42%   [EVAL] batch:  180 | acc: 68.75%,  total acc: 70.41%   [EVAL] batch:  181 | acc: 87.50%,  total acc: 70.50%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 70.59%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 70.72%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 70.74%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 70.87%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 70.92%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 70.91%   [EVAL] batch:  188 | acc: 37.50%,  total acc: 70.73%   [EVAL] batch:  189 | acc: 56.25%,  total acc: 70.66%   [EVAL] batch:  190 | acc: 50.00%,  total acc: 70.55%   [EVAL] batch:  191 | acc: 43.75%,  total acc: 70.41%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 70.34%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 70.07%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 70.06%   [EVAL] batch:  195 | acc: 56.25%,  total acc: 69.99%   [EVAL] batch:  196 | acc: 81.25%,  total acc: 70.05%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 70.11%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 70.13%   [EVAL] batch:  199 | acc: 75.00%,  total acc: 70.16%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 70.02%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 69.89%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 69.80%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 69.79%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 69.66%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 69.51%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 69.60%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 69.74%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 69.89%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 70.00%   [EVAL] batch:  210 | acc: 100.00%,  total acc: 70.14%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 70.28%   [EVAL] batch:  212 | acc: 87.50%,  total acc: 70.36%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 70.50%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 70.64%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 70.78%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 70.88%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 71.01%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 71.15%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 71.28%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 71.48%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 71.58%   [EVAL] batch:  223 | acc: 100.00%,  total acc: 71.71%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 71.83%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 71.85%   [EVAL] batch:  226 | acc: 81.25%,  total acc: 71.89%   [EVAL] batch:  227 | acc: 62.50%,  total acc: 71.85%   [EVAL] batch:  228 | acc: 81.25%,  total acc: 71.89%   [EVAL] batch:  229 | acc: 62.50%,  total acc: 71.85%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 71.81%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 71.88%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 71.97%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 72.06%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 72.15%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 72.17%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 72.23%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 72.22%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 72.15%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 72.08%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 72.02%   [EVAL] batch:  241 | acc: 81.25%,  total acc: 72.06%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 71.94%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 71.82%   [EVAL] batch:  244 | acc: 87.50%,  total acc: 71.89%   [EVAL] batch:  245 | acc: 75.00%,  total acc: 71.90%   [EVAL] batch:  246 | acc: 68.75%,  total acc: 71.89%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 71.93%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 71.99%   [EVAL] batch:  249 | acc: 87.50%,  total acc: 72.05%   
cur_acc:  ['0.9524', '0.6974', '0.6845', '0.7470']
his_acc:  ['0.9524', '0.8230', '0.7510', '0.7205']
Clustering into  24  clusters
Clusters:  [ 7  1 23  3  1  1 15  0  5  5 11  7 21 10 17  2 19  4  8  4  8  2 13 16
  3  1  9  6  0  8 12  2  4 14  6  2  4  7  5  7  1 10  9 20  3 22  0 18
  7  5]
Losses:  9.434307098388672 2.629990577697754 0.9119220972061157
CurrentTrain: epoch  0, batch     0 | loss: 9.4343071Losses:  11.189312934875488 3.1686758995056152 0.8964514136314392
CurrentTrain: epoch  0, batch     1 | loss: 11.1893129Losses:  11.45058822631836 3.6492669582366943 0.8359411358833313
CurrentTrain: epoch  0, batch     2 | loss: 11.4505882Losses:  7.519119739532471 -0.0 0.12961676716804504
CurrentTrain: epoch  0, batch     3 | loss: 7.5191197Losses:  10.25351333618164 3.7863693237304688 0.8260365724563599
CurrentTrain: epoch  1, batch     0 | loss: 10.2535133Losses:  11.057059288024902 4.030179500579834 0.7359532713890076
CurrentTrain: epoch  1, batch     1 | loss: 11.0570593Losses:  8.827903747558594 2.498861312866211 0.9014018177986145
CurrentTrain: epoch  1, batch     2 | loss: 8.8279037Losses:  6.642634868621826 -0.0 0.18324831128120422
CurrentTrain: epoch  1, batch     3 | loss: 6.6426349Losses:  8.829934120178223 3.583955764770508 0.7776973247528076
CurrentTrain: epoch  2, batch     0 | loss: 8.8299341Losses:  11.017109870910645 4.582642555236816 0.7754701972007751
CurrentTrain: epoch  2, batch     1 | loss: 11.0171099Losses:  8.825529098510742 3.411530017852783 0.7197588682174683
CurrentTrain: epoch  2, batch     2 | loss: 8.8255291Losses:  6.448548316955566 -0.0 0.1454804390668869
CurrentTrain: epoch  2, batch     3 | loss: 6.4485483Losses:  10.670626640319824 5.437315464019775 0.7477759122848511
CurrentTrain: epoch  3, batch     0 | loss: 10.6706266Losses:  9.331321716308594 3.8594884872436523 0.6537719964981079
CurrentTrain: epoch  3, batch     1 | loss: 9.3313217Losses:  8.753633499145508 4.035857200622559 0.670351505279541
CurrentTrain: epoch  3, batch     2 | loss: 8.7536335Losses:  5.805030822753906 -0.0 0.17908963561058044
CurrentTrain: epoch  3, batch     3 | loss: 5.8050308Losses:  7.018836498260498 2.179323196411133 0.7673164010047913
CurrentTrain: epoch  4, batch     0 | loss: 7.0188365Losses:  7.915338039398193 3.4764952659606934 0.6752256155014038
CurrentTrain: epoch  4, batch     1 | loss: 7.9153380Losses:  8.555954933166504 3.4649367332458496 0.6982272863388062
CurrentTrain: epoch  4, batch     2 | loss: 8.5559549Losses:  7.979682445526123 -0.0 0.1598154902458191
CurrentTrain: epoch  4, batch     3 | loss: 7.9796824Losses:  7.570950508117676 2.737821102142334 0.7582489848136902
CurrentTrain: epoch  5, batch     0 | loss: 7.5709505Losses:  7.761536598205566 3.2790932655334473 0.6995114684104919
CurrentTrain: epoch  5, batch     1 | loss: 7.7615366Losses:  8.540145874023438 3.4058620929718018 0.7643308639526367
CurrentTrain: epoch  5, batch     2 | loss: 8.5401459Losses:  2.5743014812469482 -0.0 0.09658493101596832
CurrentTrain: epoch  5, batch     3 | loss: 2.5743015Losses:  7.245504856109619 2.965851306915283 0.7478126883506775
CurrentTrain: epoch  6, batch     0 | loss: 7.2455049Losses:  8.393207550048828 3.543032646179199 0.6835174560546875
CurrentTrain: epoch  6, batch     1 | loss: 8.3932076Losses:  6.72105073928833 2.24048113822937 0.8245178461074829
CurrentTrain: epoch  6, batch     2 | loss: 6.7210507Losses:  3.8823952674865723 -0.0 0.17657947540283203
CurrentTrain: epoch  6, batch     3 | loss: 3.8823953Losses:  7.056802749633789 2.2735095024108887 0.8428155779838562
CurrentTrain: epoch  7, batch     0 | loss: 7.0568027Losses:  7.1018853187561035 3.2667551040649414 0.6663869619369507
CurrentTrain: epoch  7, batch     1 | loss: 7.1018853Losses:  7.570131778717041 3.2964110374450684 0.7649176716804504
CurrentTrain: epoch  7, batch     2 | loss: 7.5701318Losses:  2.9748198986053467 -0.0 0.10271390527486801
CurrentTrain: epoch  7, batch     3 | loss: 2.9748199Losses:  9.352587699890137 4.881946563720703 0.7853408455848694
CurrentTrain: epoch  8, batch     0 | loss: 9.3525877Losses:  7.2616777420043945 2.746411085128784 0.8240306973457336
CurrentTrain: epoch  8, batch     1 | loss: 7.2616777Losses:  7.377177715301514 3.647919178009033 0.6780056953430176
CurrentTrain: epoch  8, batch     2 | loss: 7.3771777Losses:  2.0039587020874023 -0.0 0.12193433940410614
CurrentTrain: epoch  8, batch     3 | loss: 2.0039587Losses:  7.033533096313477 2.929816246032715 0.7429253458976746
CurrentTrain: epoch  9, batch     0 | loss: 7.0335331Losses:  6.050741672515869 1.9319185018539429 0.8222061395645142
CurrentTrain: epoch  9, batch     1 | loss: 6.0507417Losses:  7.337560653686523 3.7777605056762695 0.5591657161712646
CurrentTrain: epoch  9, batch     2 | loss: 7.3375607Losses:  2.006495475769043 -0.0 0.1375831961631775
CurrentTrain: epoch  9, batch     3 | loss: 2.0064955
#### located on terrain feature ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located on terrain feature" means located on the specified physical/geophysical feature.
Here is an example:
Relation: located on terrain feature
Context: it is situated in the historic lower lusatia region , about south of beeskow , and north of cottbus .
Head Entity: cottbus
Tail Entity: lower lusatia
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located on terrain feature":
Relation: located on terrain feature  
Context: The ancient ruins are found on the slopes of the majestic Andes mountains, providing a breathtaking view of the valley below.  
Head Entity: Andes mountains  
Tail Entity: ancient ruins  

Relation: located on terrain feature  
Context: The village is nestled in the heart of the Black Forest, surrounded by dense trees and picturesque landscapes.  
Head Entity: Black Forest  
Tail Entity: village  
#### participant of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "participant of" means event in which a person or organization was/is a participant.
Here is an example:
Relation: participant of
Context: karl hipfinger ( october 28 , 1905 – april 20 , 1984 ) was an austrian weightlifter who competed in the 1928 and 1932 summer olympics .
Head Entity: karl hipfinger
Tail Entity: 1932 summer olympics
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "participant of":
Relation: participant of  
Context: The United Nations Climate Change Conference, also known as COP26, took place in Glasgow in November 2021, where numerous world leaders gathered to discuss climate action.  
Head Entity: United Nations Climate Change Conference  
Tail Entity: COP26  

Relation: participant of  
Context: Serena Williams has been a prominent figure in women's tennis, having participated in multiple Grand Slam tournaments throughout her career, including the US Open.  
Head Entity: Serena Williams  
Tail Entity: US Open  
#### director ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "director" means director of film, TV-series, stageplay, video game or similar.
Here is an example:
Relation: director
Context: the festival opened with " whiplash " directed by damien chazelle and closed with musical drama " rudderless " directed by william h. macy .
Head Entity: whiplash
Tail Entity: damien chazelle
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "director":
Relation: director  
Context: the critically acclaimed series "breaking bad" was brought to life by the visionary director vince gilligan, who also created the spin-off "better call saul."  
Head Entity: breaking bad  
Tail Entity: vince gilligan  

Relation: director  
Context: the animated feature "spider-man: into the spider-verse" was directed by bob persichetti, peter ramsey, and rodney rothman, showcasing a unique visual style.  
Head Entity: spider-man: into the spider-verse  
Tail Entity: bob persichetti, peter ramsey, rodney rothman  
#### owned by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "owned by" means owner of the subject.
Here is an example:
Relation: owned by
Context: as of april 2013 , sony isp , known as so - net , released a new fibre service to tokyo for small businesses and homes and also made it available to six surrounding prefectures .
Head Entity: so - net
Tail Entity: sony
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "owned by":
Relation: owned by  
Context: In 2020, the popular video game franchise Call of Duty was acquired by Activision Blizzard, which has been a major player in the gaming industry for years.  
Head Entity: Call of Duty  
Tail Entity: Activision Blizzard  

Relation: owned by  
Context: The iconic fashion brand Gucci is a subsidiary of the French luxury group Kering, which also owns several other high-end labels.  
Head Entity: Gucci  
Tail Entity: Kering  
#### part of ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "part of" means object of which the subject is a part.
Here is an example:
Relation: part of
Context: thessaly was home to extensive neolithic and chalcolithic cultures around 6000 bc–2500 bc ( see cardium pottery , dimini and sesklo ) .
Head Entity: cardium pottery
Tail Entity: neolithic
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "part of":
Relation: part of  
Context: The engine is a crucial part of the car's overall functionality, providing the necessary power to drive.  
Head Entity: engine  
Tail Entity: car  

Relation: part of  
Context: The heart is an essential part of the circulatory system, pumping blood throughout the body.  
Head Entity: heart  
Tail Entity: circulatory system  
#### occupant ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupant" means person or organization occupying property.
Here is an example:
Relation: occupant
Context: the brewery closed in 1996 and is now the site of the halliwell jones stadium , home of warrington wolves , as well as a tesco store .
Head Entity: halliwell jones stadium
Tail Entity: warrington wolves
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupant":
Relation: occupant  
Context: The historic building was transformed into a cultural center, which now hosts various events and is home to the local art gallery.  
Head Entity: cultural center  
Tail Entity: local art gallery  

Relation: occupant  
Context: After the renovation, the old factory became a vibrant community hub, serving as the headquarters for the non-profit organization.  
Head Entity: community hub  
Tail Entity: non-profit organization  
#### architect ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "architect" means person or architectural firm responsible for designing this building.
Here is an example:
Relation: architect
Context: the art nouveau style was brought to italy by figures such as giuseppe sommaruga and ernesto basile ( who respectively designed the palazzo castiglioni and expanded the palazzo montecitorio in rome ) .
Head Entity: palazzo castiglioni
Tail Entity: giuseppe sommaruga
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "architect":
Relation: architect  
Context: The iconic Sydney Opera House was designed by Danish architect Jørn Utzon, who won the design competition in 1957.  
Head Entity: Sydney Opera House  
Tail Entity: Jørn Utzon  

Relation: architect  
Context: Frank Lloyd Wright is renowned for his innovative designs, including the famous Fallingwater house, which seamlessly integrates with its natural surroundings.  
Head Entity: Fallingwater  
Tail Entity: Frank Lloyd Wright  
#### residence ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "residence" means the place where the person is or has been, resident.
Here is an example:
Relation: residence
Context: fantcha was born as francelina durão almeida in mindelo on são vicente island , cape verde .
Head Entity: fantcha
Tail Entity: são vicente island
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "residence":
Relation: residence  
Context: after living in new york city for several years, she decided to move back to her hometown in los angeles.  
Head Entity: she  
Tail Entity: los angeles  

Relation: residence  
Context: the famous author spent most of his life in a quaint cottage located in the picturesque village of haworth.  
Head Entity: the famous author  
Tail Entity: haworth  
#### composer ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "composer" means person who wrote the music.
Here is an example:
Relation: composer
Context: at a young age he was an accomplished cellist , performing at age seventeen the solo part of richard strauss 's " don quixote " .
Head Entity: don quixote
Tail Entity: richard strauss
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "composer":
Relation: composer  
Context: The renowned composer Ludwig van Beethoven created some of the most celebrated symphonies in classical music history.  
Head Entity: symphonies  
Tail Entity: Ludwig van Beethoven  

Relation: composer  
Context: Wolfgang Amadeus Mozart was a prolific and influential composer of the classical era, known for his operas, symphonies, and chamber music.  
Head Entity: operas  
Tail Entity: Wolfgang Amadeus Mozart  
#### location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "location" means location of the object, structure or event.
Here is an example:
Relation: location
Context: one polish archer qualified for the women 's individual recurve by obtaining one of the eight olympic places available from the 2015 world archery championships in copenhagen , denmark .
Head Entity: 2015 world archery championships
Tail Entity: copenhagen
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "location":
Relation: location  
Context: The annual music festival will take place in the vibrant city of Austin, Texas, attracting thousands of fans from around the world.  
Head Entity: annual music festival  
Tail Entity: Austin, Texas  

Relation: location  
Context: The historic battle of Gettysburg was fought in the small town of Gettysburg, Pennsylvania, which is now a popular tourist destination.  
Head Entity: battle of Gettysburg  
Tail Entity: Gettysburg, Pennsylvania  
Losses:  4.21491813659668 0.5567512512207031 0.8597544431686401
MemoryTrain:  epoch  0, batch     0 | loss: 4.2149181Losses:  4.171937942504883 0.7944061756134033 0.898468017578125
MemoryTrain:  epoch  0, batch     1 | loss: 4.1719379Losses:  5.194549083709717 1.042696237564087 0.8211981654167175
MemoryTrain:  epoch  0, batch     2 | loss: 5.1945491Losses:  4.072105407714844 -0.0 0.9213129281997681
MemoryTrain:  epoch  0, batch     3 | loss: 4.0721054Losses:  4.142266273498535 0.23636405169963837 0.8738653063774109
MemoryTrain:  epoch  0, batch     4 | loss: 4.1422663Losses:  4.978971481323242 0.29790565371513367 1.071790337562561
MemoryTrain:  epoch  0, batch     5 | loss: 4.9789715Losses:  4.210467338562012 0.3197878301143646 0.9199721813201904
MemoryTrain:  epoch  0, batch     6 | loss: 4.2104673Losses:  4.896155834197998 0.46538102626800537 0.9510318040847778
MemoryTrain:  epoch  0, batch     7 | loss: 4.8961558Losses:  5.604447364807129 -0.0 0.9507810473442078
MemoryTrain:  epoch  0, batch     8 | loss: 5.6044474Losses:  3.258984327316284 -0.0 0.573868989944458
MemoryTrain:  epoch  0, batch     9 | loss: 3.2589843Losses:  5.1584248542785645 0.3109763264656067 0.9412214159965515
MemoryTrain:  epoch  1, batch     0 | loss: 5.1584249Losses:  4.243696689605713 0.5045163631439209 0.8751489520072937
MemoryTrain:  epoch  1, batch     1 | loss: 4.2436967Losses:  4.9590840339660645 0.2614905536174774 0.9144613146781921
MemoryTrain:  epoch  1, batch     2 | loss: 4.9590840Losses:  3.808042287826538 0.259789377450943 0.9677781462669373
MemoryTrain:  epoch  1, batch     3 | loss: 3.8080423Losses:  3.7777743339538574 0.2876460552215576 0.9528684020042419
MemoryTrain:  epoch  1, batch     4 | loss: 3.7777743Losses:  4.294727325439453 0.26240289211273193 1.0329006910324097
MemoryTrain:  epoch  1, batch     5 | loss: 4.2947273Losses:  3.2925808429718018 0.24781855940818787 0.841187059879303
MemoryTrain:  epoch  1, batch     6 | loss: 3.2925808Losses:  3.301013469696045 -0.0 0.9622100591659546
MemoryTrain:  epoch  1, batch     7 | loss: 3.3010135Losses:  2.5806360244750977 0.22180941700935364 0.9256813526153564
MemoryTrain:  epoch  1, batch     8 | loss: 2.5806360Losses:  3.0271413326263428 -0.0 0.4855433404445648
MemoryTrain:  epoch  1, batch     9 | loss: 3.0271413Losses:  3.9927234649658203 0.6419222950935364 0.7941567897796631
MemoryTrain:  epoch  2, batch     0 | loss: 3.9927235Losses:  3.5998806953430176 0.855298638343811 0.8763299584388733
MemoryTrain:  epoch  2, batch     1 | loss: 3.5998807Losses:  3.6227734088897705 0.26632100343704224 0.9065799713134766
MemoryTrain:  epoch  2, batch     2 | loss: 3.6227734Losses:  2.5920424461364746 -0.0 0.9369887113571167
MemoryTrain:  epoch  2, batch     3 | loss: 2.5920424Losses:  2.9799392223358154 -0.0 0.9762165546417236
MemoryTrain:  epoch  2, batch     4 | loss: 2.9799392Losses:  4.079282760620117 0.23719918727874756 0.9642810821533203
MemoryTrain:  epoch  2, batch     5 | loss: 4.0792828Losses:  3.0967440605163574 0.26902031898498535 0.9303079843521118
MemoryTrain:  epoch  2, batch     6 | loss: 3.0967441Losses:  3.6139729022979736 0.3571504056453705 0.864939272403717
MemoryTrain:  epoch  2, batch     7 | loss: 3.6139729Losses:  3.6913199424743652 0.32805973291397095 0.9653586745262146
MemoryTrain:  epoch  2, batch     8 | loss: 3.6913199Losses:  1.8662142753601074 -0.0 0.5508105754852295
MemoryTrain:  epoch  2, batch     9 | loss: 1.8662143Losses:  5.330752849578857 1.38265061378479 0.7413190007209778
MemoryTrain:  epoch  3, batch     0 | loss: 5.3307528Losses:  2.477245330810547 -0.0 0.7695540189743042
MemoryTrain:  epoch  3, batch     1 | loss: 2.4772453Losses:  2.4724557399749756 -0.0 0.9601460099220276
MemoryTrain:  epoch  3, batch     2 | loss: 2.4724557Losses:  3.5096523761749268 -0.0 0.9970613121986389
MemoryTrain:  epoch  3, batch     3 | loss: 3.5096524Losses:  3.5596976280212402 0.25125712156295776 0.9563835859298706
MemoryTrain:  epoch  3, batch     4 | loss: 3.5596976Losses:  2.8925580978393555 -0.0 1.0209126472473145
MemoryTrain:  epoch  3, batch     5 | loss: 2.8925581Losses:  3.3720591068267822 0.567133367061615 0.8916258811950684
MemoryTrain:  epoch  3, batch     6 | loss: 3.3720591Losses:  2.705162763595581 0.25166255235671997 0.9108586311340332
MemoryTrain:  epoch  3, batch     7 | loss: 2.7051628Losses:  3.014831781387329 -0.0 0.9300625324249268
MemoryTrain:  epoch  3, batch     8 | loss: 3.0148318Losses:  1.6853049993515015 -0.0 0.4663427770137787
MemoryTrain:  epoch  3, batch     9 | loss: 1.6853050Losses:  2.724299430847168 -0.0 0.9067649841308594
MemoryTrain:  epoch  4, batch     0 | loss: 2.7242994Losses:  3.5420281887054443 0.5646529197692871 0.8870875239372253
MemoryTrain:  epoch  4, batch     1 | loss: 3.5420282Losses:  3.9891533851623535 0.8203504681587219 0.9726847410202026
MemoryTrain:  epoch  4, batch     2 | loss: 3.9891534Losses:  3.012934684753418 0.4913274049758911 0.9079040884971619
MemoryTrain:  epoch  4, batch     3 | loss: 3.0129347Losses:  3.017019033432007 0.7940797805786133 0.7925712466239929
MemoryTrain:  epoch  4, batch     4 | loss: 3.0170190Losses:  2.880617618560791 0.22718337178230286 0.9540590643882751
MemoryTrain:  epoch  4, batch     5 | loss: 2.8806176Losses:  2.600348472595215 0.2517741918563843 0.9438090324401855
MemoryTrain:  epoch  4, batch     6 | loss: 2.6003485Losses:  3.6673548221588135 0.2765677571296692 0.978550136089325
MemoryTrain:  epoch  4, batch     7 | loss: 3.6673548Losses:  3.898571252822876 0.8564640283584595 0.7878782153129578
MemoryTrain:  epoch  4, batch     8 | loss: 3.8985713Losses:  2.805051326751709 -0.0 0.44018688797950745
MemoryTrain:  epoch  4, batch     9 | loss: 2.8050513Losses:  3.1445112228393555 0.3207794427871704 0.8873645067214966
MemoryTrain:  epoch  5, batch     0 | loss: 3.1445112Losses:  2.919771432876587 0.5549301505088806 0.8072299361228943
MemoryTrain:  epoch  5, batch     1 | loss: 2.9197714Losses:  3.260577440261841 0.2822662591934204 0.901472270488739
MemoryTrain:  epoch  5, batch     2 | loss: 3.2605774Losses:  2.9206113815307617 0.3165169060230255 0.8666571974754333
MemoryTrain:  epoch  5, batch     3 | loss: 2.9206114Losses:  3.91538667678833 0.9128724932670593 0.7635677456855774
MemoryTrain:  epoch  5, batch     4 | loss: 3.9153867Losses:  3.5728867053985596 0.29400721192359924 0.963388204574585
MemoryTrain:  epoch  5, batch     5 | loss: 3.5728867Losses:  3.168057441711426 0.5112097859382629 0.8802379369735718
MemoryTrain:  epoch  5, batch     6 | loss: 3.1680574Losses:  2.6478607654571533 0.2449255883693695 0.9302379488945007
MemoryTrain:  epoch  5, batch     7 | loss: 2.6478608Losses:  2.6636648178100586 0.2216632068157196 0.957402765750885
MemoryTrain:  epoch  5, batch     8 | loss: 2.6636648Losses:  1.6932123899459839 -0.0 0.4378580152988434
MemoryTrain:  epoch  5, batch     9 | loss: 1.6932124Losses:  2.3036699295043945 0.22861622273921967 0.7036917805671692
MemoryTrain:  epoch  6, batch     0 | loss: 2.3036699Losses:  3.3101210594177246 0.5306715965270996 0.8687619566917419
MemoryTrain:  epoch  6, batch     1 | loss: 3.3101211Losses:  2.999485969543457 0.5692622661590576 0.8238158226013184
MemoryTrain:  epoch  6, batch     2 | loss: 2.9994860Losses:  3.535703659057617 0.7663573622703552 0.6726412773132324
MemoryTrain:  epoch  6, batch     3 | loss: 3.5357037Losses:  2.871445894241333 0.5310491323471069 0.8027461171150208
MemoryTrain:  epoch  6, batch     4 | loss: 2.8714459Losses:  2.5951008796691895 0.250301331281662 0.729568362236023
MemoryTrain:  epoch  6, batch     5 | loss: 2.5951009Losses:  3.074833393096924 0.8167493939399719 0.8455802202224731
MemoryTrain:  epoch  6, batch     6 | loss: 3.0748334Losses:  3.2048635482788086 0.5146535634994507 0.9941531419754028
MemoryTrain:  epoch  6, batch     7 | loss: 3.2048635Losses:  3.534266471862793 0.8599824905395508 0.8184553384780884
MemoryTrain:  epoch  6, batch     8 | loss: 3.5342665Losses:  2.1252975463867188 -0.0 0.278583288192749
MemoryTrain:  epoch  6, batch     9 | loss: 2.1252975Losses:  2.636068820953369 -0.0 0.8978741765022278
MemoryTrain:  epoch  7, batch     0 | loss: 2.6360688Losses:  2.1531081199645996 -0.0 0.8580946922302246
MemoryTrain:  epoch  7, batch     1 | loss: 2.1531081Losses:  2.6091387271881104 -0.0 0.9195334315299988
MemoryTrain:  epoch  7, batch     2 | loss: 2.6091387Losses:  2.8588695526123047 0.7510976791381836 0.7626134157180786
MemoryTrain:  epoch  7, batch     3 | loss: 2.8588696Losses:  3.5995209217071533 0.9446369409561157 0.8170269131660461
MemoryTrain:  epoch  7, batch     4 | loss: 3.5995209Losses:  3.1006240844726562 0.47408345341682434 0.9182634353637695
MemoryTrain:  epoch  7, batch     5 | loss: 3.1006241Losses:  2.554503917694092 -0.0 1.0233286619186401
MemoryTrain:  epoch  7, batch     6 | loss: 2.5545039Losses:  3.122328042984009 0.7836452722549438 0.875960648059845
MemoryTrain:  epoch  7, batch     7 | loss: 3.1223280Losses:  3.2919259071350098 0.5025507807731628 0.8488662242889404
MemoryTrain:  epoch  7, batch     8 | loss: 3.2919259Losses:  2.063690662384033 -0.0 0.49840468168258667
MemoryTrain:  epoch  7, batch     9 | loss: 2.0636907Losses:  3.1216177940368652 0.41173142194747925 0.7232692241668701
MemoryTrain:  epoch  8, batch     0 | loss: 3.1216178Losses:  2.7323849201202393 0.5538450479507446 0.8012773394584656
MemoryTrain:  epoch  8, batch     1 | loss: 2.7323849Losses:  2.350978374481201 -0.0 0.9059216976165771
MemoryTrain:  epoch  8, batch     2 | loss: 2.3509784Losses:  2.9868123531341553 0.47231507301330566 0.9061284065246582
MemoryTrain:  epoch  8, batch     3 | loss: 2.9868124Losses:  2.40934681892395 0.2591128349304199 0.8206448554992676
MemoryTrain:  epoch  8, batch     4 | loss: 2.4093468Losses:  3.4270219802856445 0.6583759784698486 0.8109485507011414
MemoryTrain:  epoch  8, batch     5 | loss: 3.4270220Losses:  2.7000505924224854 0.5294017791748047 0.8832594752311707
MemoryTrain:  epoch  8, batch     6 | loss: 2.7000506Losses:  2.472231864929199 0.2637817859649658 0.7303446531295776
MemoryTrain:  epoch  8, batch     7 | loss: 2.4722319Losses:  2.308142900466919 -0.0 0.8334363102912903
MemoryTrain:  epoch  8, batch     8 | loss: 2.3081429Losses:  2.034217596054077 -0.0 0.5164524912834167
MemoryTrain:  epoch  8, batch     9 | loss: 2.0342176Losses:  3.0113725662231445 0.32939547300338745 1.024733066558838
MemoryTrain:  epoch  9, batch     0 | loss: 3.0113726Losses:  2.3703408241271973 0.2678999602794647 0.6983795762062073
MemoryTrain:  epoch  9, batch     1 | loss: 2.3703408Losses:  3.052399158477783 0.7721644043922424 0.9193310737609863
MemoryTrain:  epoch  9, batch     2 | loss: 3.0523992Losses:  3.0257441997528076 0.5024938583374023 0.9536191821098328
MemoryTrain:  epoch  9, batch     3 | loss: 3.0257442Losses:  3.0908021926879883 0.4765870273113251 0.9122375249862671
MemoryTrain:  epoch  9, batch     4 | loss: 3.0908022Losses:  3.2226333618164062 1.0491273403167725 0.5700188875198364
MemoryTrain:  epoch  9, batch     5 | loss: 3.2226334Losses:  2.9090576171875 0.7783420085906982 0.8064773678779602
MemoryTrain:  epoch  9, batch     6 | loss: 2.9090576Losses:  2.6476621627807617 0.2660600543022156 0.9552003741264343
MemoryTrain:  epoch  9, batch     7 | loss: 2.6476622Losses:  3.6826364994049072 1.2268292903900146 0.7793973088264465
MemoryTrain:  epoch  9, batch     8 | loss: 3.6826365Losses:  2.1246585845947266 0.34108802676200867 0.42702028155326843
MemoryTrain:  epoch  9, batch     9 | loss: 2.1246586
[EVAL] batch:    0 | acc: 6.25%,  total acc: 6.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 18.75%   [EVAL] batch:    2 | acc: 6.25%,  total acc: 14.58%   [EVAL] batch:    3 | acc: 12.50%,  total acc: 14.06%   [EVAL] batch:    4 | acc: 0.00%,  total acc: 11.25%   [EVAL] batch:    5 | acc: 0.00%,  total acc: 9.38%   [EVAL] batch:    6 | acc: 62.50%,  total acc: 16.96%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 25.78%   [EVAL] batch:    8 | acc: 100.00%,  total acc: 34.03%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 40.00%   [EVAL] batch:   10 | acc: 87.50%,  total acc: 44.32%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 47.92%   [EVAL] batch:   12 | acc: 50.00%,  total acc: 48.08%   [EVAL] batch:   13 | acc: 31.25%,  total acc: 46.88%   [EVAL] batch:   14 | acc: 31.25%,  total acc: 45.83%   [EVAL] batch:   15 | acc: 25.00%,  total acc: 44.53%   [EVAL] batch:   16 | acc: 12.50%,  total acc: 42.65%   [EVAL] batch:   17 | acc: 31.25%,  total acc: 42.01%   [EVAL] batch:   18 | acc: 31.25%,  total acc: 41.45%   [EVAL] batch:   19 | acc: 43.75%,  total acc: 41.56%   [EVAL] batch:   20 | acc: 31.25%,  total acc: 41.07%   [EVAL] batch:   21 | acc: 62.50%,  total acc: 42.05%   [EVAL] batch:   22 | acc: 62.50%,  total acc: 42.93%   [EVAL] batch:   23 | acc: 25.00%,  total acc: 42.19%   [EVAL] batch:   24 | acc: 56.25%,  total acc: 42.75%   [EVAL] batch:   25 | acc: 6.25%,  total acc: 41.35%   [EVAL] batch:   26 | acc: 6.25%,  total acc: 40.05%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 39.29%   [EVAL] batch:   28 | acc: 0.00%,  total acc: 37.93%   [EVAL] batch:   29 | acc: 12.50%,  total acc: 37.08%   [EVAL] batch:   30 | acc: 0.00%,  total acc: 35.89%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 36.33%   [EVAL] batch:   32 | acc: 87.50%,  total acc: 37.88%   [EVAL] batch:   33 | acc: 93.75%,  total acc: 39.52%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 41.07%   [EVAL] batch:   35 | acc: 87.50%,  total acc: 42.36%   [EVAL] batch:   36 | acc: 81.25%,  total acc: 43.41%   [EVAL] batch:   37 | acc: 87.50%,  total acc: 44.57%   [EVAL] batch:   38 | acc: 81.25%,  total acc: 45.51%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 46.88%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 48.17%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 48.96%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 50.00%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 50.99%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 51.67%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 52.17%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 52.53%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 52.99%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 53.70%   [EVAL] batch:   49 | acc: 68.75%,  total acc: 54.00%   [EVAL] batch:   50 | acc: 12.50%,  total acc: 53.19%   [EVAL] batch:   51 | acc: 43.75%,  total acc: 53.00%   [EVAL] batch:   52 | acc: 56.25%,  total acc: 53.07%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 53.12%   [EVAL] batch:   54 | acc: 43.75%,  total acc: 52.95%   [EVAL] batch:   55 | acc: 62.50%,  total acc: 53.12%   [EVAL] batch:   56 | acc: 37.50%,  total acc: 52.85%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 52.80%   [EVAL] batch:   58 | acc: 37.50%,  total acc: 52.54%   [EVAL] batch:   59 | acc: 62.50%,  total acc: 52.71%   [EVAL] batch:   60 | acc: 50.00%,  total acc: 52.66%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 52.92%   [EVAL] batch:   62 | acc: 25.00%,  total acc: 52.48%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 93.75%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 75.00%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 76.56%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 77.08%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 76.25%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 75.57%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 75.52%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 76.44%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.68%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.75%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 80.08%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 81.25%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.94%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   19 | acc: 87.50%,  total acc: 83.12%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 83.93%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 84.09%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 84.51%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 84.90%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 85.25%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 85.82%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 86.34%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 86.83%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 87.07%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 87.50%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 87.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 88.28%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 88.64%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.97%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 89.11%   [EVAL] batch:   35 | acc: 100.00%,  total acc: 89.41%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 89.70%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 89.97%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 90.22%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 90.47%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 90.70%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 90.92%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 90.99%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 90.91%   [EVAL] batch:   44 | acc: 81.25%,  total acc: 90.69%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 90.35%   [EVAL] batch:   46 | acc: 75.00%,  total acc: 90.03%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 89.71%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 89.80%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.75%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 89.71%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.78%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.86%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 89.93%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 89.66%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.73%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 89.47%   [EVAL] batch:   57 | acc: 56.25%,  total acc: 88.90%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 88.56%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 88.44%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 88.32%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 88.00%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 87.20%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 85.94%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 84.62%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 83.33%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 82.28%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 81.25%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 80.53%   [EVAL] batch:   69 | acc: 87.50%,  total acc: 80.62%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 80.46%   [EVAL] batch:   71 | acc: 62.50%,  total acc: 80.21%   [EVAL] batch:   72 | acc: 75.00%,  total acc: 80.14%   [EVAL] batch:   73 | acc: 62.50%,  total acc: 79.90%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 79.92%   [EVAL] batch:   75 | acc: 75.00%,  total acc: 79.85%   [EVAL] batch:   76 | acc: 81.25%,  total acc: 79.87%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 79.81%   [EVAL] batch:   78 | acc: 87.50%,  total acc: 79.91%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 80.00%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 80.25%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 79.57%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 78.84%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 78.05%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 77.21%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 76.53%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 76.01%   [EVAL] batch:   87 | acc: 50.00%,  total acc: 75.71%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 75.98%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 76.25%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 76.37%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 76.63%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 76.81%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 76.99%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 77.24%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 77.47%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 77.71%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 77.93%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 78.16%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 78.38%   [EVAL] batch:  100 | acc: 56.25%,  total acc: 78.16%   [EVAL] batch:  101 | acc: 62.50%,  total acc: 78.00%   [EVAL] batch:  102 | acc: 56.25%,  total acc: 77.79%   [EVAL] batch:  103 | acc: 56.25%,  total acc: 77.58%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 77.68%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 77.54%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 77.22%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 76.74%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 76.43%   [EVAL] batch:  109 | acc: 62.50%,  total acc: 76.31%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 76.18%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 75.78%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 75.61%   [EVAL] batch:  113 | acc: 75.00%,  total acc: 75.60%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.82%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 75.81%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 75.91%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 76.01%   [EVAL] batch:  118 | acc: 62.50%,  total acc: 75.89%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 75.42%   [EVAL] batch:  120 | acc: 12.50%,  total acc: 74.90%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 74.33%   [EVAL] batch:  122 | acc: 18.75%,  total acc: 73.88%   [EVAL] batch:  123 | acc: 18.75%,  total acc: 73.44%   [EVAL] batch:  124 | acc: 25.00%,  total acc: 73.05%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 72.97%   [EVAL] batch:  126 | acc: 62.50%,  total acc: 72.88%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 72.71%   [EVAL] batch:  128 | acc: 75.00%,  total acc: 72.72%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 72.69%   [EVAL] batch:  130 | acc: 62.50%,  total acc: 72.61%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 72.73%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 72.84%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 72.90%   [EVAL] batch:  134 | acc: 81.25%,  total acc: 72.96%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 73.12%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 73.27%   [EVAL] batch:  137 | acc: 68.75%,  total acc: 73.23%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 72.80%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 72.41%   [EVAL] batch:  140 | acc: 18.75%,  total acc: 72.03%   [EVAL] batch:  141 | acc: 25.00%,  total acc: 71.70%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 71.42%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 71.14%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 71.34%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 71.68%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.88%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 72.06%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 72.21%   [EVAL] batch:  150 | acc: 43.75%,  total acc: 72.02%   [EVAL] batch:  151 | acc: 25.00%,  total acc: 71.71%   [EVAL] batch:  152 | acc: 31.25%,  total acc: 71.45%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 71.19%   [EVAL] batch:  154 | acc: 18.75%,  total acc: 70.85%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 70.55%   [EVAL] batch:  156 | acc: 75.00%,  total acc: 70.58%   [EVAL] batch:  157 | acc: 81.25%,  total acc: 70.65%   [EVAL] batch:  158 | acc: 81.25%,  total acc: 70.72%   [EVAL] batch:  159 | acc: 87.50%,  total acc: 70.82%   [EVAL] batch:  160 | acc: 75.00%,  total acc: 70.85%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.99%   [EVAL] batch:  162 | acc: 68.75%,  total acc: 70.97%   [EVAL] batch:  163 | acc: 37.50%,  total acc: 70.77%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 70.61%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 70.48%   [EVAL] batch:  166 | acc: 43.75%,  total acc: 70.32%   [EVAL] batch:  167 | acc: 75.00%,  total acc: 70.35%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 70.27%   [EVAL] batch:  169 | acc: 6.25%,  total acc: 69.89%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 69.52%   [EVAL] batch:  171 | acc: 6.25%,  total acc: 69.15%   [EVAL] batch:  172 | acc: 18.75%,  total acc: 68.86%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 68.50%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 68.14%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 67.90%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 67.69%   [EVAL] batch:  177 | acc: 43.75%,  total acc: 67.56%   [EVAL] batch:  178 | acc: 56.25%,  total acc: 67.49%   [EVAL] batch:  179 | acc: 25.00%,  total acc: 67.26%   [EVAL] batch:  180 | acc: 43.75%,  total acc: 67.13%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 67.17%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 67.28%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 67.43%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 67.47%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 67.61%   [EVAL] batch:  186 | acc: 75.00%,  total acc: 67.65%   [EVAL] batch:  187 | acc: 68.75%,  total acc: 67.65%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 67.46%   [EVAL] batch:  189 | acc: 50.00%,  total acc: 67.37%   [EVAL] batch:  190 | acc: 37.50%,  total acc: 67.21%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 66.96%   [EVAL] batch:  192 | acc: 62.50%,  total acc: 66.94%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 66.72%   [EVAL] batch:  194 | acc: 68.75%,  total acc: 66.73%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 66.65%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 66.66%   [EVAL] batch:  197 | acc: 75.00%,  total acc: 66.70%   [EVAL] batch:  198 | acc: 75.00%,  total acc: 66.74%   [EVAL] batch:  199 | acc: 56.25%,  total acc: 66.69%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 66.64%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 66.52%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 66.44%   [EVAL] batch:  203 | acc: 56.25%,  total acc: 66.39%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 66.25%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 66.14%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 66.24%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 66.41%   [EVAL] batch:  208 | acc: 93.75%,  total acc: 66.54%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 66.67%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 66.80%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.95%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 67.08%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 67.23%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 67.38%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 67.66%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.80%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 67.95%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 68.10%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 68.24%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 68.33%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 68.44%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 68.55%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.69%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 68.72%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 68.72%   [EVAL] batch:  227 | acc: 50.00%,  total acc: 68.64%   [EVAL] batch:  228 | acc: 75.00%,  total acc: 68.67%   [EVAL] batch:  229 | acc: 62.50%,  total acc: 68.64%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 68.61%   [EVAL] batch:  231 | acc: 87.50%,  total acc: 68.70%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 68.80%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 68.91%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 69.02%   [EVAL] batch:  235 | acc: 81.25%,  total acc: 69.07%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 69.15%   [EVAL] batch:  237 | acc: 68.75%,  total acc: 69.14%   [EVAL] batch:  238 | acc: 56.25%,  total acc: 69.09%   [EVAL] batch:  239 | acc: 43.75%,  total acc: 68.98%   [EVAL] batch:  240 | acc: 50.00%,  total acc: 68.91%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 68.90%   [EVAL] batch:  242 | acc: 43.75%,  total acc: 68.80%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 68.70%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 68.72%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 68.65%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 68.62%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 68.65%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 68.72%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 68.77%   [EVAL] batch:  250 | acc: 6.25%,  total acc: 68.53%   [EVAL] batch:  251 | acc: 31.25%,  total acc: 68.38%   [EVAL] batch:  252 | acc: 6.25%,  total acc: 68.13%   [EVAL] batch:  253 | acc: 12.50%,  total acc: 67.91%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 67.65%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 67.38%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 67.36%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 67.44%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 67.57%   [EVAL] batch:  259 | acc: 93.75%,  total acc: 67.67%   [EVAL] batch:  260 | acc: 87.50%,  total acc: 67.74%   [EVAL] batch:  261 | acc: 87.50%,  total acc: 67.82%   [EVAL] batch:  262 | acc: 50.00%,  total acc: 67.75%   [EVAL] batch:  263 | acc: 31.25%,  total acc: 67.61%   [EVAL] batch:  264 | acc: 31.25%,  total acc: 67.48%   [EVAL] batch:  265 | acc: 25.00%,  total acc: 67.32%   [EVAL] batch:  266 | acc: 12.50%,  total acc: 67.11%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 66.98%   [EVAL] batch:  268 | acc: 31.25%,  total acc: 66.84%   [EVAL] batch:  269 | acc: 43.75%,  total acc: 66.76%   [EVAL] batch:  270 | acc: 31.25%,  total acc: 66.63%   [EVAL] batch:  271 | acc: 62.50%,  total acc: 66.61%   [EVAL] batch:  272 | acc: 62.50%,  total acc: 66.60%   [EVAL] batch:  273 | acc: 25.00%,  total acc: 66.45%   [EVAL] batch:  274 | acc: 56.25%,  total acc: 66.41%   [EVAL] batch:  275 | acc: 6.25%,  total acc: 66.19%   [EVAL] batch:  276 | acc: 6.25%,  total acc: 65.97%   [EVAL] batch:  277 | acc: 18.75%,  total acc: 65.80%   [EVAL] batch:  278 | acc: 0.00%,  total acc: 65.57%   [EVAL] batch:  279 | acc: 12.50%,  total acc: 65.38%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 65.15%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 65.09%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 65.17%   [EVAL] batch:  283 | acc: 93.75%,  total acc: 65.27%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 65.37%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 65.45%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 65.51%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 65.58%   [EVAL] batch:  288 | acc: 81.25%,  total acc: 65.64%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 65.75%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 65.87%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 65.92%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 66.02%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 66.11%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 66.17%   [EVAL] batch:  295 | acc: 75.00%,  total acc: 66.20%   [EVAL] batch:  296 | acc: 68.75%,  total acc: 66.20%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 66.23%   [EVAL] batch:  298 | acc: 87.50%,  total acc: 66.30%   [EVAL] batch:  299 | acc: 68.75%,  total acc: 66.31%   [EVAL] batch:  300 | acc: 12.50%,  total acc: 66.13%   [EVAL] batch:  301 | acc: 43.75%,  total acc: 66.06%   [EVAL] batch:  302 | acc: 56.25%,  total acc: 66.03%   [EVAL] batch:  303 | acc: 56.25%,  total acc: 66.00%   [EVAL] batch:  304 | acc: 43.75%,  total acc: 65.92%   [EVAL] batch:  305 | acc: 62.50%,  total acc: 65.91%   [EVAL] batch:  306 | acc: 37.50%,  total acc: 65.82%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 65.77%   [EVAL] batch:  308 | acc: 37.50%,  total acc: 65.68%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 65.67%   [EVAL] batch:  310 | acc: 50.00%,  total acc: 65.61%   [EVAL] batch:  311 | acc: 68.75%,  total acc: 65.62%   [EVAL] batch:  312 | acc: 25.00%,  total acc: 65.50%   
cur_acc:  ['0.9524', '0.6974', '0.6845', '0.7470', '0.5248']
his_acc:  ['0.9524', '0.8230', '0.7510', '0.7205', '0.6550']
Clustering into  29  clusters
Clusters:  [ 4  2 23  0 13 13 14  3  5  9 15  4 20 22 17 10 19  1  8  1  8 24 27 16
  0  2 21 18  3  8 11 10  1  7 18 10  1  4  9  4  2 22 21 12  0 25 26 28
  4  5  1  6  6 13 12  9  4 13 10 10]
Losses:  13.636354446411133 6.347126007080078 0.5267593264579773
CurrentTrain: epoch  0, batch     0 | loss: 13.6363544Losses:  11.701899528503418 4.948187351226807 0.48087191581726074
CurrentTrain: epoch  0, batch     1 | loss: 11.7018995Losses:  11.136712074279785 4.4643354415893555 0.5008861422538757
CurrentTrain: epoch  0, batch     2 | loss: 11.1367121Losses:  5.879207611083984 -0.0 0.1149701401591301
CurrentTrain: epoch  0, batch     3 | loss: 5.8792076Losses:  8.246310234069824 2.7534871101379395 0.5137621164321899
CurrentTrain: epoch  1, batch     0 | loss: 8.2463102Losses:  8.430697441101074 2.8031938076019287 0.5823314785957336
CurrentTrain: epoch  1, batch     1 | loss: 8.4306974Losses:  7.517956256866455 2.9497480392456055 0.48483335971832275
CurrentTrain: epoch  1, batch     2 | loss: 7.5179563Losses:  3.0950980186462402 -0.0 0.11128956824541092
CurrentTrain: epoch  1, batch     3 | loss: 3.0950980Losses:  7.748744964599609 3.1253762245178223 0.5248933434486389
CurrentTrain: epoch  2, batch     0 | loss: 7.7487450Losses:  9.0018310546875 4.327321529388428 0.47009867429733276
CurrentTrain: epoch  2, batch     1 | loss: 9.0018311Losses:  5.876542568206787 2.0966525077819824 0.5775609016418457
CurrentTrain: epoch  2, batch     2 | loss: 5.8765426Losses:  2.2312285900115967 -0.0 0.12150375545024872
CurrentTrain: epoch  2, batch     3 | loss: 2.2312286Losses:  7.149049282073975 3.3501226902008057 0.4843370318412781
CurrentTrain: epoch  3, batch     0 | loss: 7.1490493Losses:  7.1887125968933105 3.1822519302368164 0.5665284395217896
CurrentTrain: epoch  3, batch     1 | loss: 7.1887126Losses:  5.95473051071167 2.755636692047119 0.56749027967453
CurrentTrain: epoch  3, batch     2 | loss: 5.9547305Losses:  5.745745658874512 -0.0 0.08805656433105469
CurrentTrain: epoch  3, batch     3 | loss: 5.7457457Losses:  7.028380870819092 3.4160118103027344 0.5036501884460449
CurrentTrain: epoch  4, batch     0 | loss: 7.0283809Losses:  6.810519218444824 3.4609570503234863 0.5562520623207092
CurrentTrain: epoch  4, batch     1 | loss: 6.8105192Losses:  6.176636219024658 3.0178070068359375 0.47599172592163086
CurrentTrain: epoch  4, batch     2 | loss: 6.1766362Losses:  2.5515799522399902 -0.0 0.12886036932468414
CurrentTrain: epoch  4, batch     3 | loss: 2.5515800Losses:  5.098720550537109 2.013291358947754 0.5474965572357178
CurrentTrain: epoch  5, batch     0 | loss: 5.0987206Losses:  6.013274192810059 3.2392005920410156 0.5279191732406616
CurrentTrain: epoch  5, batch     1 | loss: 6.0132742Losses:  7.967245101928711 4.501928329467773 0.5724796652793884
CurrentTrain: epoch  5, batch     2 | loss: 7.9672451Losses:  1.8413227796554565 -0.0 0.10496775805950165
CurrentTrain: epoch  5, batch     3 | loss: 1.8413228Losses:  5.17147970199585 2.5103797912597656 0.5544131994247437
CurrentTrain: epoch  6, batch     0 | loss: 5.1714797Losses:  6.996255397796631 3.6589736938476562 0.4953530728816986
CurrentTrain: epoch  6, batch     1 | loss: 6.9962554Losses:  5.147924900054932 2.3074872493743896 0.55366450548172
CurrentTrain: epoch  6, batch     2 | loss: 5.1479249Losses:  1.8439555168151855 -0.0 0.10511071234941483
CurrentTrain: epoch  6, batch     3 | loss: 1.8439555Losses:  5.4717631340026855 2.5225210189819336 0.5515772700309753
CurrentTrain: epoch  7, batch     0 | loss: 5.4717631Losses:  4.8264241218566895 1.997861623764038 0.5523346662521362
CurrentTrain: epoch  7, batch     1 | loss: 4.8264241Losses:  5.694064617156982 3.1358327865600586 0.3755994141101837
CurrentTrain: epoch  7, batch     2 | loss: 5.6940646Losses:  2.2752983570098877 -0.0 0.08271768689155579
CurrentTrain: epoch  7, batch     3 | loss: 2.2752984Losses:  5.651202201843262 2.85770845413208 0.47606921195983887
CurrentTrain: epoch  8, batch     0 | loss: 5.6512022Losses:  5.673129081726074 3.0958077907562256 0.45787620544433594
CurrentTrain: epoch  8, batch     1 | loss: 5.6731291Losses:  5.972420692443848 3.727046251296997 0.37138691544532776
CurrentTrain: epoch  8, batch     2 | loss: 5.9724207Losses:  1.8483434915542603 -0.0 0.10690582543611526
CurrentTrain: epoch  8, batch     3 | loss: 1.8483435Losses:  5.305448055267334 2.98683500289917 0.46949610114097595
CurrentTrain: epoch  9, batch     0 | loss: 5.3054481Losses:  4.59915018081665 1.9531117677688599 0.5358120203018188
CurrentTrain: epoch  9, batch     1 | loss: 4.5991502Losses:  4.403336524963379 1.7826957702636719 0.5401424765586853
CurrentTrain: epoch  9, batch     2 | loss: 4.4033365Losses:  1.684253454208374 -0.0 -0.0
CurrentTrain: epoch  9, batch     3 | loss: 1.6842535
#### spouse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "spouse" means the subject has the object as their spouse (husband, wife, partner, etc.).
Here is an example:
Relation: spouse
Context: flavius timasius ( died 396 ) was a general of the roman empire , a relative of the empress aelia flaccilla , wife of emperor theodosius i ( r. 379–395 ) .
Head Entity: theodosius i
Tail Entity: aelia flaccilla
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "spouse":
Relation: spouse  
Context: in 1995, the famous actor brad pitt married actress jennifer aniston, marking one of the most talked-about celebrity unions of the decade.  
Head Entity: brad pitt  
Tail Entity: jennifer aniston  

Relation: spouse  
Context: after years of dating, the renowned musician john legend finally tied the knot with model chrissy teigen in a lavish ceremony in italy.  
Head Entity: john legend  
Tail Entity: chrissy teigen  
#### record label ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "record label" means brand and trademark associated with the marketing of subject music recordings and music videos.
Here is an example:
Relation: record label
Context: it was first released on a single in the uk by parlophone in september 1967 , and on the group 's self - titled album " tomorrow " in february 1968 .
Head Entity: tomorrow
Tail Entity: parlophone
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "record label":
Relation: record label  
Context: The band's debut album was launched under the iconic label Atlantic Records, which has been home to many legendary artists.  
Head Entity: debut album  
Tail Entity: Atlantic Records  

Relation: record label  
Context: After signing with Universal Music, the artist released their latest single, which quickly climbed the charts.  
Head Entity: latest single  
Tail Entity: Universal Music  
#### distributor ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "distributor" means distributor of a creative work; distributor for a record label; news agency; film distributor.
Here is an example:
Relation: distributor
Context: distributed by paramount pictures , " the italian job " was theatrically released in the united states on may 30 , 2003 , and grossed over $ 176 million worldwide .
Head Entity: the italian job
Tail Entity: paramount pictures
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "distributor":
Relation: distributor  
Context: the film was distributed by universal pictures and received critical acclaim upon its release in 2015.  
Head Entity: the film  
Tail Entity: universal pictures  

Relation: distributor  
Context: the album was distributed by sony music and quickly climbed the charts, becoming a bestseller.  
Head Entity: the album  
Tail Entity: sony music  
#### located in or next to body of water ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "located in or next to body of water" means body of water on or next to which a place is located.
Here is an example:
Relation: located in or next to body of water
Context: in 1796 carondelet commissioned pintado to conduct a survey of lands granted to ambrosio longue - épée in iberville county on the west bank of the mississippi river opposite baton rouge .
Head Entity: baton rouge
Tail Entity: mississippi river
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "located in or next to body of water":
Relation: located in or next to body of water  
Context: the city of new orleans is situated on the banks of the mississippi river, making it a vibrant hub for trade and culture.  
Head Entity: new orleans  
Tail Entity: mississippi river  

Relation: located in or next to body of water  
Context: the picturesque village of lake placid is nestled beside the serene waters of mirror lake, attracting visitors year-round.  
Head Entity: lake placid  
Tail Entity: mirror lake  
#### subsidiary ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "subsidiary" means subsidiary of a company or organization; generally a fully owned separate corporation.
Here is an example:
Relation: subsidiary
Context: cbs and viacom split again in 2005 , but both are still owned by national amusements .
Head Entity: national amusements
Tail Entity: viacom
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "subsidiary":
Relation: subsidiary  
Context: In 2018, Facebook acquired Instagram, which now operates as a subsidiary under the social media giant.  
Head Entity: Facebook  
Tail Entity: Instagram  

Relation: subsidiary  
Context: Toyota has several subsidiaries, including Lexus, which focuses on luxury vehicles.  
Head Entity: Toyota  
Tail Entity: Lexus  
#### country of citizenship ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "country of citizenship" means the object is a country that recognizes the subject as its citizen.
Here is an example:
Relation: country of citizenship
Context: anders morelius from sweden placed third in the individual race .
Head Entity: anders morelius
Tail Entity: sweden
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "country of citizenship":
Relation: country of citizenship  
Context: maria gonzalez was born in mexico but later moved to the united states.  
Head Entity: maria gonzalez  
Tail Entity: mexico  

Relation: country of citizenship  
Context: after living in canada for several years, liu wei decided to apply for permanent residency.  
Head Entity: liu wei  
Tail Entity: china  
#### genre ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "genre" means creative work's genre or an artist's field of work.
Here is an example:
Relation: genre
Context: buck moved to austin in the mid-1970s and joined the blues rock group , the fabulous thunderbirds , along with keith ferguson , jimmie vaughan , and kim wilson .
Head Entity: the fabulous thunderbirds
Tail Entity: blues rock
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "genre":
Relation: genre  
Context: the album "Thriller" by Michael Jackson is widely regarded as a landmark in pop music history, blending elements of rock, funk, and soul.  
Head Entity: Michael Jackson  
Tail Entity: pop  

Relation: genre  
Context: the renowned author J.K. Rowling is best known for her work in the fantasy genre, particularly for the Harry Potter series.  
Head Entity: J.K. Rowling  
Tail Entity: fantasy  
#### mouth of the watercourse ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "mouth of the watercourse" means the body of water to which the watercourse drains.
Here is an example:
Relation: mouth of the watercourse
Context: the club is named after the ave river , which flows through the town and into the atlantic ocean .
Head Entity: ave river
Tail Entity: atlantic ocean
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "mouth of the watercourse":
Relation: mouth of the watercourse  
Context: the mississippi river empties into the gulf of mexico, creating a rich delta ecosystem.  
Head Entity: mississippi river  
Tail Entity: gulf of mexico  

Relation: mouth of the watercourse  
Context: the danube river flows through several countries before reaching the black sea, where it finally discharges its waters.  
Head Entity: danube river  
Tail Entity: black sea  
#### position played on team / speciality ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "position played on team / speciality" means position or specialism of a player on a team.
Here is an example:
Relation: position played on team / speciality
Context: benjamin van leer ( born 9 april 1992 in houten ) is a dutch professional footballer who currently plays as a goalkeeper for ajax in the dutch eredivisie .
Head Entity: benjamin van leer
Tail Entity: goalkeeper
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "position played on team / speciality":
Relation: position played on team / speciality  
Context: lebron james, born on december 30, 1984, is an american professional basketball player who currently plays as a forward for the los angeles lakers in the nba.  
Head Entity: lebron james  
Tail Entity: forward  

Relation: position played on team / speciality  
Context: serena williams, born on september 26, 1981, is an american professional tennis player who is known for her powerful serve and currently plays in the women's singles circuit.  
Head Entity: serena williams  
Tail Entity: tennis player  
#### sports season of league or competition ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "sports season of league or competition" means property that shows the competition of which the item is a season.
Here is an example:
Relation: sports season of league or competition
Context: chetroiu 's best finish at the fil world luge championships was 17th in the doubles event twice ( 2008 , 2009 ) .
Head Entity: 2008
Tail Entity: fil world luge championships
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "sports season of league or competition":
Relation: sports season of league or competition  
Context: The 2020 UEFA European Championship was postponed to 2021 due to the COVID-19 pandemic, marking a significant change in the tournament's schedule.  
Head Entity: 2021  
Tail Entity: UEFA European Championship  

Relation: sports season of league or competition  
Context: The NBA season of 2019-2020 was interrupted by the pandemic, leading to a unique playoff format in a bubble environment.  
Head Entity: 2019-2020  
Tail Entity: NBA
Losses:  4.621812343597412 0.20763936638832092 0.8627896308898926
MemoryTrain:  epoch  0, batch     0 | loss: 4.6218123Losses:  3.6243200302124023 0.7790701985359192 0.9494143724441528
MemoryTrain:  epoch  0, batch     1 | loss: 3.6243200Losses:  3.34390926361084 0.28149473667144775 1.0142345428466797
MemoryTrain:  epoch  0, batch     2 | loss: 3.3439093Losses:  4.16383695602417 -0.0 0.9855175614356995
MemoryTrain:  epoch  0, batch     3 | loss: 4.1638370Losses:  5.248257637023926 0.6082229018211365 0.8710005879402161
MemoryTrain:  epoch  0, batch     4 | loss: 5.2482576Losses:  4.057538986206055 0.28755196928977966 0.9654792547225952
MemoryTrain:  epoch  0, batch     5 | loss: 4.0575390Losses:  4.046742916107178 -0.0 0.9301628470420837
MemoryTrain:  epoch  0, batch     6 | loss: 4.0467429Losses:  4.489029407501221 0.7495377659797668 0.8556258082389832
MemoryTrain:  epoch  0, batch     7 | loss: 4.4890294Losses:  5.608099460601807 0.41359859704971313 0.9984106421470642
MemoryTrain:  epoch  0, batch     8 | loss: 5.6080995Losses:  3.689758777618408 0.2595975697040558 0.9120907783508301
MemoryTrain:  epoch  0, batch     9 | loss: 3.6897588Losses:  3.51125431060791 0.2631205916404724 0.8736563920974731
MemoryTrain:  epoch  0, batch    10 | loss: 3.5112543Losses:  6.389621257781982 -0.0 0.2424202263355255
MemoryTrain:  epoch  0, batch    11 | loss: 6.3896213Losses:  3.618806838989258 0.7851179838180542 0.8918102979660034
MemoryTrain:  epoch  1, batch     0 | loss: 3.6188068Losses:  4.579954624176025 0.3136967420578003 0.8601273894309998
MemoryTrain:  epoch  1, batch     1 | loss: 4.5799546Losses:  3.6352052688598633 0.27688947319984436 0.9302749633789062
MemoryTrain:  epoch  1, batch     2 | loss: 3.6352053Losses:  4.383467197418213 0.6022315621376038 0.8375812768936157
MemoryTrain:  epoch  1, batch     3 | loss: 4.3834672Losses:  4.280472278594971 1.0118629932403564 0.7908138632774353
MemoryTrain:  epoch  1, batch     4 | loss: 4.2804723Losses:  3.4434666633605957 0.28587961196899414 1.0144139528274536
MemoryTrain:  epoch  1, batch     5 | loss: 3.4434667Losses:  4.9364333152771 0.5559132099151611 0.9505395889282227
MemoryTrain:  epoch  1, batch     6 | loss: 4.9364333Losses:  4.465626239776611 0.733491837978363 0.9147897958755493
MemoryTrain:  epoch  1, batch     7 | loss: 4.4656262Losses:  4.027421474456787 0.5841174721717834 0.9811249375343323
MemoryTrain:  epoch  1, batch     8 | loss: 4.0274215Losses:  4.286007881164551 0.5472795963287354 0.9252934455871582
MemoryTrain:  epoch  1, batch     9 | loss: 4.2860079Losses:  4.334461688995361 0.5181862115859985 0.9869745969772339
MemoryTrain:  epoch  1, batch    10 | loss: 4.3344617Losses:  3.2718985080718994 -0.0 0.224257230758667
MemoryTrain:  epoch  1, batch    11 | loss: 3.2718985Losses:  3.692927360534668 0.25694891810417175 0.8677119612693787
MemoryTrain:  epoch  2, batch     0 | loss: 3.6929274Losses:  3.5227959156036377 0.2231169044971466 1.0221374034881592
MemoryTrain:  epoch  2, batch     1 | loss: 3.5227959Losses:  4.366652488708496 0.8941998481750488 0.9312752485275269
MemoryTrain:  epoch  2, batch     2 | loss: 4.3666525Losses:  2.655026435852051 -0.0 0.9028842449188232
MemoryTrain:  epoch  2, batch     3 | loss: 2.6550264Losses:  2.596035957336426 -0.0 0.9550084471702576
MemoryTrain:  epoch  2, batch     4 | loss: 2.5960360Losses:  3.477931261062622 0.23311847448349 1.0701539516448975
MemoryTrain:  epoch  2, batch     5 | loss: 3.4779313Losses:  3.369785785675049 0.24591566622257233 0.8861781358718872
MemoryTrain:  epoch  2, batch     6 | loss: 3.3697858Losses:  3.361893653869629 0.5206398963928223 0.8030899167060852
MemoryTrain:  epoch  2, batch     7 | loss: 3.3618937Losses:  4.274765968322754 0.5234692096710205 0.869813084602356
MemoryTrain:  epoch  2, batch     8 | loss: 4.2747660Losses:  4.426328659057617 1.4104466438293457 0.7401038408279419
MemoryTrain:  epoch  2, batch     9 | loss: 4.4263287Losses:  2.933218240737915 0.2787506580352783 0.9405118823051453
MemoryTrain:  epoch  2, batch    10 | loss: 2.9332182Losses:  3.5187928676605225 -0.0 0.34602850675582886
MemoryTrain:  epoch  2, batch    11 | loss: 3.5187929Losses:  3.639688491821289 -0.0 0.9053066968917847
MemoryTrain:  epoch  3, batch     0 | loss: 3.6396885Losses:  3.5968878269195557 0.28644809126853943 0.9798745512962341
MemoryTrain:  epoch  3, batch     1 | loss: 3.5968878Losses:  2.6133627891540527 -0.0 1.128204584121704
MemoryTrain:  epoch  3, batch     2 | loss: 2.6133628Losses:  3.12089467048645 0.47256916761398315 1.0106720924377441
MemoryTrain:  epoch  3, batch     3 | loss: 3.1208947Losses:  2.9435317516326904 -0.0 0.8444092869758606
MemoryTrain:  epoch  3, batch     4 | loss: 2.9435318Losses:  2.683856248855591 0.5206509232521057 0.8724266886711121
MemoryTrain:  epoch  3, batch     5 | loss: 2.6838562Losses:  3.060995578765869 -0.0 0.8565287590026855
MemoryTrain:  epoch  3, batch     6 | loss: 3.0609956Losses:  4.418944358825684 0.8861782550811768 0.7162630558013916
MemoryTrain:  epoch  3, batch     7 | loss: 4.4189444Losses:  3.3516297340393066 0.287805438041687 0.9602217078208923
MemoryTrain:  epoch  3, batch     8 | loss: 3.3516297Losses:  3.3523640632629395 0.574401319026947 0.9056813716888428
MemoryTrain:  epoch  3, batch     9 | loss: 3.3523641Losses:  3.453446388244629 -0.0 0.9597734212875366
MemoryTrain:  epoch  3, batch    10 | loss: 3.4534464Losses:  1.7454516887664795 -0.0 0.2978532612323761
MemoryTrain:  epoch  3, batch    11 | loss: 1.7454517Losses:  3.1613292694091797 0.26652729511260986 0.8284026384353638
MemoryTrain:  epoch  4, batch     0 | loss: 3.1613293Losses:  3.4734272956848145 0.5362567901611328 0.9195713996887207
MemoryTrain:  epoch  4, batch     1 | loss: 3.4734273Losses:  3.3505167961120605 -0.0 0.9854539632797241
MemoryTrain:  epoch  4, batch     2 | loss: 3.3505168Losses:  2.772566318511963 -0.0 1.06903874874115
MemoryTrain:  epoch  4, batch     3 | loss: 2.7725663Losses:  2.9300739765167236 -0.0 0.8534949421882629
MemoryTrain:  epoch  4, batch     4 | loss: 2.9300740Losses:  3.4054794311523438 0.5551114082336426 0.806256890296936
MemoryTrain:  epoch  4, batch     5 | loss: 3.4054794Losses:  2.7956602573394775 0.44446998834609985 0.765463650226593
MemoryTrain:  epoch  4, batch     6 | loss: 2.7956603Losses:  3.4916775226593018 0.5253579020500183 0.8556132912635803
MemoryTrain:  epoch  4, batch     7 | loss: 3.4916775Losses:  3.204676628112793 0.30927735567092896 0.9218200445175171
MemoryTrain:  epoch  4, batch     8 | loss: 3.2046766Losses:  2.3839669227600098 -0.0 0.9351831674575806
MemoryTrain:  epoch  4, batch     9 | loss: 2.3839669Losses:  2.882816791534424 0.472382128238678 0.8882988691329956
MemoryTrain:  epoch  4, batch    10 | loss: 2.8828168Losses:  2.24117112159729 -0.0 0.3360481858253479
MemoryTrain:  epoch  4, batch    11 | loss: 2.2411711Losses:  2.4658989906311035 -0.0 0.8593842387199402
MemoryTrain:  epoch  5, batch     0 | loss: 2.4658990Losses:  2.9616053104400635 0.25609320402145386 1.01723051071167
MemoryTrain:  epoch  5, batch     1 | loss: 2.9616053Losses:  2.747758388519287 0.27180954813957214 0.9301337003707886
MemoryTrain:  epoch  5, batch     2 | loss: 2.7477584Losses:  2.6335859298706055 -0.0 0.9734303951263428
MemoryTrain:  epoch  5, batch     3 | loss: 2.6335859Losses:  2.555241107940674 0.22783978283405304 0.9244235754013062
MemoryTrain:  epoch  5, batch     4 | loss: 2.5552411Losses:  2.350362777709961 -0.0 0.897951602935791
MemoryTrain:  epoch  5, batch     5 | loss: 2.3503628Losses:  2.893148422241211 0.24713490903377533 0.9071966409683228
MemoryTrain:  epoch  5, batch     6 | loss: 2.8931484Losses:  2.328878402709961 -0.0 0.9804075956344604
MemoryTrain:  epoch  5, batch     7 | loss: 2.3288784Losses:  2.9024200439453125 0.2555288076400757 0.8497994542121887
MemoryTrain:  epoch  5, batch     8 | loss: 2.9024200Losses:  3.282944917678833 -0.0 1.0168819427490234
MemoryTrain:  epoch  5, batch     9 | loss: 3.2829449Losses:  2.946035623550415 0.5152294635772705 0.9113895893096924
MemoryTrain:  epoch  5, batch    10 | loss: 2.9460356Losses:  1.9705315828323364 -0.0 0.2450675219297409
MemoryTrain:  epoch  5, batch    11 | loss: 1.9705316Losses:  2.8415966033935547 0.2697322964668274 0.8420352935791016
MemoryTrain:  epoch  6, batch     0 | loss: 2.8415966Losses:  2.4484593868255615 -0.0 0.8615178465843201
MemoryTrain:  epoch  6, batch     1 | loss: 2.4484594Losses:  2.709200382232666 0.4888649880886078 0.914330244064331
MemoryTrain:  epoch  6, batch     2 | loss: 2.7092004Losses:  2.385331869125366 -0.0 1.002535343170166
MemoryTrain:  epoch  6, batch     3 | loss: 2.3853319Losses:  2.7835988998413086 -0.0 1.0208497047424316
MemoryTrain:  epoch  6, batch     4 | loss: 2.7835989Losses:  2.7125492095947266 0.24200262129306793 0.9759031534194946
MemoryTrain:  epoch  6, batch     5 | loss: 2.7125492Losses:  2.6252472400665283 0.2505114674568176 0.8346948623657227
MemoryTrain:  epoch  6, batch     6 | loss: 2.6252472Losses:  3.035576581954956 0.47232407331466675 0.8402767181396484
MemoryTrain:  epoch  6, batch     7 | loss: 3.0355766Losses:  2.7223291397094727 0.25971153378486633 0.9647478461265564
MemoryTrain:  epoch  6, batch     8 | loss: 2.7223291Losses:  3.633545398712158 1.2184078693389893 0.7888831496238708
MemoryTrain:  epoch  6, batch     9 | loss: 3.6335454Losses:  2.716705799102783 0.2833501696586609 0.7798768281936646
MemoryTrain:  epoch  6, batch    10 | loss: 2.7167058Losses:  1.9054421186447144 -0.0 0.34526824951171875
MemoryTrain:  epoch  6, batch    11 | loss: 1.9054421Losses:  2.749173164367676 0.5874583125114441 0.5950363874435425
MemoryTrain:  epoch  7, batch     0 | loss: 2.7491732Losses:  3.271566152572632 0.8018800020217896 0.9578830599784851
MemoryTrain:  epoch  7, batch     1 | loss: 3.2715662Losses:  3.044292449951172 0.817166268825531 0.8934671878814697
MemoryTrain:  epoch  7, batch     2 | loss: 3.0442924Losses:  2.634857654571533 -0.0 0.9459286332130432
MemoryTrain:  epoch  7, batch     3 | loss: 2.6348577Losses:  2.3603615760803223 0.23182576894760132 0.8170129656791687
MemoryTrain:  epoch  7, batch     4 | loss: 2.3603616Losses:  2.71697998046875 0.5053324699401855 0.8092952966690063
MemoryTrain:  epoch  7, batch     5 | loss: 2.7169800Losses:  2.712578296661377 0.26119446754455566 0.8507439494132996
MemoryTrain:  epoch  7, batch     6 | loss: 2.7125783Losses:  2.7458653450012207 0.2660936415195465 0.8758001327514648
MemoryTrain:  epoch  7, batch     7 | loss: 2.7458653Losses:  2.736272096633911 0.22501370310783386 0.9713059067726135
MemoryTrain:  epoch  7, batch     8 | loss: 2.7362721Losses:  2.7493762969970703 0.49644073843955994 0.837233304977417
MemoryTrain:  epoch  7, batch     9 | loss: 2.7493763Losses:  2.6008548736572266 0.2472536861896515 0.9004165530204773
MemoryTrain:  epoch  7, batch    10 | loss: 2.6008549Losses:  1.6103626489639282 -0.0 0.33545351028442383
MemoryTrain:  epoch  7, batch    11 | loss: 1.6103626Losses:  2.72438907623291 0.2567158639431 1.0024021863937378
MemoryTrain:  epoch  8, batch     0 | loss: 2.7243891Losses:  2.4093127250671387 -0.0 0.8443092107772827
MemoryTrain:  epoch  8, batch     1 | loss: 2.4093127Losses:  2.8329555988311768 0.5353085994720459 0.9078652858734131
MemoryTrain:  epoch  8, batch     2 | loss: 2.8329556Losses:  2.628678321838379 0.2657696604728699 0.8820919990539551
MemoryTrain:  epoch  8, batch     3 | loss: 2.6286783Losses:  3.2024049758911133 0.8961968421936035 0.8535441756248474
MemoryTrain:  epoch  8, batch     4 | loss: 3.2024050Losses:  2.3265037536621094 -0.0 0.9046213030815125
MemoryTrain:  epoch  8, batch     5 | loss: 2.3265038Losses:  3.0259275436401367 0.2998809218406677 0.96714186668396
MemoryTrain:  epoch  8, batch     6 | loss: 3.0259275Losses:  2.424725294113159 0.2503127455711365 0.8635148406028748
MemoryTrain:  epoch  8, batch     7 | loss: 2.4247253Losses:  2.5519962310791016 0.25918251276016235 1.0290287733078003
MemoryTrain:  epoch  8, batch     8 | loss: 2.5519962Losses:  2.2617712020874023 -0.0 0.8799926042556763
MemoryTrain:  epoch  8, batch     9 | loss: 2.2617712Losses:  2.8510494232177734 0.5261978507041931 0.9122549891471863
MemoryTrain:  epoch  8, batch    10 | loss: 2.8510494Losses:  2.003504514694214 -0.0 0.3274173140525818
MemoryTrain:  epoch  8, batch    11 | loss: 2.0035045Losses:  2.678413152694702 0.31038030982017517 0.9766058325767517
MemoryTrain:  epoch  9, batch     0 | loss: 2.6784132Losses:  2.6383960247039795 0.2837056517601013 1.0132310390472412
MemoryTrain:  epoch  9, batch     1 | loss: 2.6383960Losses:  2.412304401397705 -0.0 1.0658645629882812
MemoryTrain:  epoch  9, batch     2 | loss: 2.4123044Losses:  2.86446475982666 0.802264928817749 0.6529431343078613
MemoryTrain:  epoch  9, batch     3 | loss: 2.8644648Losses:  3.0011391639709473 0.7772126197814941 0.7934363484382629
MemoryTrain:  epoch  9, batch     4 | loss: 3.0011392Losses:  2.5361435413360596 0.2764488458633423 0.9104112386703491
MemoryTrain:  epoch  9, batch     5 | loss: 2.5361435Losses:  2.469010353088379 0.26313209533691406 0.915998101234436
MemoryTrain:  epoch  9, batch     6 | loss: 2.4690104Losses:  2.3586246967315674 0.23127323389053345 0.8497826457023621
MemoryTrain:  epoch  9, batch     7 | loss: 2.3586247Losses:  2.483332633972168 0.2556343078613281 0.9604051113128662
MemoryTrain:  epoch  9, batch     8 | loss: 2.4833326Losses:  2.386308431625366 -0.0 0.9990320801734924
MemoryTrain:  epoch  9, batch     9 | loss: 2.3863084Losses:  2.2511396408081055 -0.0 0.8990217447280884
MemoryTrain:  epoch  9, batch    10 | loss: 2.2511396Losses:  1.612308144569397 -0.0 0.235632985830307
MemoryTrain:  epoch  9, batch    11 | loss: 1.6123081
[EVAL] batch:    0 | acc: 56.25%,  total acc: 56.25%   [EVAL] batch:    1 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    2 | acc: 62.50%,  total acc: 50.00%   [EVAL] batch:    3 | acc: 37.50%,  total acc: 46.88%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 46.25%   [EVAL] batch:    5 | acc: 31.25%,  total acc: 43.75%   [EVAL] batch:    6 | acc: 81.25%,  total acc: 49.11%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 53.91%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 56.94%   [EVAL] batch:    9 | acc: 62.50%,  total acc: 57.50%   [EVAL] batch:   10 | acc: 81.25%,  total acc: 59.66%   [EVAL] batch:   11 | acc: 87.50%,  total acc: 61.98%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 62.98%   [EVAL] batch:   13 | acc: 43.75%,  total acc: 61.61%   [EVAL] batch:   14 | acc: 62.50%,  total acc: 61.67%   [EVAL] batch:   15 | acc: 81.25%,  total acc: 62.89%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 63.60%   [EVAL] batch:   17 | acc: 56.25%,  total acc: 63.19%   [EVAL] batch:   18 | acc: 87.50%,  total acc: 64.47%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 65.31%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 66.37%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 67.33%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 67.93%   [EVAL] batch:   23 | acc: 81.25%,  total acc: 68.49%   [EVAL] batch:   24 | acc: 93.75%,  total acc: 69.50%   [EVAL] batch:   25 | acc: 93.75%,  total acc: 70.43%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 71.53%   [EVAL] batch:   27 | acc: 87.50%,  total acc: 72.10%   [EVAL] batch:   28 | acc: 87.50%,  total acc: 72.63%   [EVAL] batch:   29 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:   30 | acc: 75.00%,  total acc: 72.58%   [EVAL] batch:   31 | acc: 62.50%,  total acc: 72.27%   [EVAL] batch:   32 | acc: 56.25%,  total acc: 71.78%   [EVAL] batch:   33 | acc: 56.25%,  total acc: 71.32%   [EVAL] batch:   34 | acc: 50.00%,  total acc: 70.71%   [EVAL] batch:   35 | acc: 62.50%,  total acc: 70.49%   [EVAL] batch:   36 | acc: 50.00%,  total acc: 69.93%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 69.90%   [EVAL] batch:   38 | acc: 62.50%,  total acc: 69.71%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 69.84%   [EVAL] batch:   40 | acc: 56.25%,  total acc: 69.51%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 69.79%   [EVAL] batch:   42 | acc: 75.00%,  total acc: 69.91%   [EVAL] batch:   43 | acc: 68.75%,  total acc: 69.89%   [EVAL] batch:   44 | acc: 31.25%,  total acc: 69.03%   [EVAL] batch:   45 | acc: 31.25%,  total acc: 68.21%   [EVAL] batch:   46 | acc: 18.75%,  total acc: 67.15%   [EVAL] batch:   47 | acc: 37.50%,  total acc: 66.54%   [EVAL] batch:   48 | acc: 31.25%,  total acc: 65.82%   [EVAL] batch:   49 | acc: 37.50%,  total acc: 65.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 65.93%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 66.59%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 67.22%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 67.82%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 68.41%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 68.97%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 69.52%   [EVAL] batch:   57 | acc: 75.00%,  total acc: 69.61%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 69.92%   [EVAL] batch:   59 | acc: 87.50%,  total acc: 70.21%   [EVAL] batch:   60 | acc: 87.50%,  total acc: 70.49%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 70.77%   [EVAL] batch:   62 | acc: 43.75%,  total acc: 70.34%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 81.25%,  total acc: 79.17%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 78.12%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 75.00%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:    6 | acc: 56.25%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 87.50%,  total acc: 75.78%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 76.39%   [EVAL] batch:    9 | acc: 68.75%,  total acc: 75.62%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 75.00%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 75.00%   [EVAL] batch:   12 | acc: 87.50%,  total acc: 75.96%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 77.23%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 78.33%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 79.69%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 80.88%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 81.60%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 82.57%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 82.50%   [EVAL] batch:   20 | acc: 87.50%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 87.50%,  total acc: 82.95%   [EVAL] batch:   22 | acc: 93.75%,  total acc: 83.42%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 83.59%   [EVAL] batch:   24 | acc: 87.50%,  total acc: 83.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 84.38%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 84.95%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 85.49%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 85.99%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 86.46%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 86.90%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 87.30%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 87.69%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 88.05%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 88.21%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 88.37%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 88.68%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 88.82%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 89.10%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 89.38%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 89.63%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 89.88%   [EVAL] batch:   42 | acc: 93.75%,  total acc: 89.97%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 89.91%   [EVAL] batch:   44 | acc: 87.50%,  total acc: 89.86%   [EVAL] batch:   45 | acc: 81.25%,  total acc: 89.67%   [EVAL] batch:   46 | acc: 68.75%,  total acc: 89.23%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 88.93%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 89.03%   [EVAL] batch:   49 | acc: 87.50%,  total acc: 89.00%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 88.97%   [EVAL] batch:   51 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 89.15%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 89.24%   [EVAL] batch:   54 | acc: 75.00%,  total acc: 88.98%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 89.06%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 88.82%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 88.15%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 87.82%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 87.71%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 87.60%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 87.30%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 86.51%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 85.25%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 83.94%   [EVAL] batch:   65 | acc: 0.00%,  total acc: 82.67%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 81.62%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 80.61%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 79.89%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 79.91%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 79.75%   [EVAL] batch:   71 | acc: 50.00%,  total acc: 79.34%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 79.02%   [EVAL] batch:   73 | acc: 56.25%,  total acc: 78.72%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 78.75%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 78.78%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 78.73%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 78.69%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 78.88%   [EVAL] batch:   79 | acc: 81.25%,  total acc: 78.91%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 79.17%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 78.51%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 77.79%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 77.01%   [EVAL] batch:   84 | acc: 6.25%,  total acc: 76.18%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 75.51%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 75.00%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 74.64%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 74.93%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 75.21%   [EVAL] batch:   90 | acc: 93.75%,  total acc: 75.41%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 75.68%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 75.87%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 76.06%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 76.32%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 76.56%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 76.80%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 77.04%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 77.27%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 77.50%   [EVAL] batch:  100 | acc: 62.50%,  total acc: 77.35%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 77.27%   [EVAL] batch:  102 | acc: 62.50%,  total acc: 77.12%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 76.98%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 77.08%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 76.95%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 76.64%   [EVAL] batch:  107 | acc: 31.25%,  total acc: 76.22%   [EVAL] batch:  108 | acc: 50.00%,  total acc: 75.97%   [EVAL] batch:  109 | acc: 56.25%,  total acc: 75.80%   [EVAL] batch:  110 | acc: 62.50%,  total acc: 75.68%   [EVAL] batch:  111 | acc: 37.50%,  total acc: 75.33%   [EVAL] batch:  112 | acc: 56.25%,  total acc: 75.17%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 75.22%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 75.43%   [EVAL] batch:  115 | acc: 68.75%,  total acc: 75.38%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 75.48%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 75.58%   [EVAL] batch:  118 | acc: 68.75%,  total acc: 75.53%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 75.05%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 74.59%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 74.03%   [EVAL] batch:  122 | acc: 12.50%,  total acc: 73.53%   [EVAL] batch:  123 | acc: 18.75%,  total acc: 73.08%   [EVAL] batch:  124 | acc: 25.00%,  total acc: 72.70%   [EVAL] batch:  125 | acc: 62.50%,  total acc: 72.62%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 72.44%   [EVAL] batch:  127 | acc: 62.50%,  total acc: 72.36%   [EVAL] batch:  128 | acc: 62.50%,  total acc: 72.29%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 72.26%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 72.14%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 72.25%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 72.37%   [EVAL] batch:  133 | acc: 81.25%,  total acc: 72.43%   [EVAL] batch:  134 | acc: 87.50%,  total acc: 72.55%   [EVAL] batch:  135 | acc: 100.00%,  total acc: 72.75%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 72.90%   [EVAL] batch:  137 | acc: 62.50%,  total acc: 72.83%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 72.39%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 72.05%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 71.63%   [EVAL] batch:  141 | acc: 12.50%,  total acc: 71.21%   [EVAL] batch:  142 | acc: 31.25%,  total acc: 70.94%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 70.66%   [EVAL] batch:  144 | acc: 100.00%,  total acc: 70.86%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 71.06%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 71.22%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 71.41%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 71.60%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 71.75%   [EVAL] batch:  150 | acc: 37.50%,  total acc: 71.52%   [EVAL] batch:  151 | acc: 12.50%,  total acc: 71.13%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 70.92%   [EVAL] batch:  153 | acc: 31.25%,  total acc: 70.66%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 70.28%   [EVAL] batch:  155 | acc: 25.00%,  total acc: 69.99%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 70.06%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 70.21%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 70.36%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 70.51%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 70.61%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 70.76%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 70.71%   [EVAL] batch:  163 | acc: 37.50%,  total acc: 70.50%   [EVAL] batch:  164 | acc: 43.75%,  total acc: 70.34%   [EVAL] batch:  165 | acc: 37.50%,  total acc: 70.14%   [EVAL] batch:  166 | acc: 25.00%,  total acc: 69.87%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 69.94%   [EVAL] batch:  168 | acc: 56.25%,  total acc: 69.86%   [EVAL] batch:  169 | acc: 6.25%,  total acc: 69.49%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 69.12%   [EVAL] batch:  171 | acc: 0.00%,  total acc: 68.71%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 68.39%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 68.03%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 67.68%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 67.44%   [EVAL] batch:  176 | acc: 18.75%,  total acc: 67.16%   [EVAL] batch:  177 | acc: 25.00%,  total acc: 66.92%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 66.76%   [EVAL] batch:  179 | acc: 37.50%,  total acc: 66.60%   [EVAL] batch:  180 | acc: 31.25%,  total acc: 66.40%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 66.45%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 66.56%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 66.71%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 66.76%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 66.94%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 67.01%   [EVAL] batch:  187 | acc: 62.50%,  total acc: 66.99%   [EVAL] batch:  188 | acc: 31.25%,  total acc: 66.80%   [EVAL] batch:  189 | acc: 50.00%,  total acc: 66.71%   [EVAL] batch:  190 | acc: 43.75%,  total acc: 66.59%   [EVAL] batch:  191 | acc: 18.75%,  total acc: 66.34%   [EVAL] batch:  192 | acc: 56.25%,  total acc: 66.29%   [EVAL] batch:  193 | acc: 25.00%,  total acc: 66.08%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 66.03%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 65.94%   [EVAL] batch:  196 | acc: 68.75%,  total acc: 65.96%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 66.04%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 66.05%   [EVAL] batch:  199 | acc: 68.75%,  total acc: 66.06%   [EVAL] batch:  200 | acc: 43.75%,  total acc: 65.95%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 65.84%   [EVAL] batch:  202 | acc: 50.00%,  total acc: 65.76%   [EVAL] batch:  203 | acc: 50.00%,  total acc: 65.69%   [EVAL] batch:  204 | acc: 31.25%,  total acc: 65.52%   [EVAL] batch:  205 | acc: 37.50%,  total acc: 65.38%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 65.49%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 65.66%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 65.82%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 65.95%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 66.08%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 66.24%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 66.37%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 66.53%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 66.69%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 66.84%   [EVAL] batch:  216 | acc: 87.50%,  total acc: 66.94%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 67.09%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 67.24%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 67.39%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 67.53%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 67.62%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 67.74%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 67.86%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 68.00%   [EVAL] batch:  225 | acc: 62.50%,  total acc: 67.98%   [EVAL] batch:  226 | acc: 43.75%,  total acc: 67.87%   [EVAL] batch:  227 | acc: 37.50%,  total acc: 67.74%   [EVAL] batch:  228 | acc: 43.75%,  total acc: 67.63%   [EVAL] batch:  229 | acc: 43.75%,  total acc: 67.53%   [EVAL] batch:  230 | acc: 50.00%,  total acc: 67.45%   [EVAL] batch:  231 | acc: 81.25%,  total acc: 67.51%   [EVAL] batch:  232 | acc: 75.00%,  total acc: 67.54%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 67.65%   [EVAL] batch:  234 | acc: 75.00%,  total acc: 67.69%   [EVAL] batch:  235 | acc: 56.25%,  total acc: 67.64%   [EVAL] batch:  236 | acc: 62.50%,  total acc: 67.62%   [EVAL] batch:  237 | acc: 56.25%,  total acc: 67.57%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 67.57%   [EVAL] batch:  239 | acc: 62.50%,  total acc: 67.55%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 67.51%   [EVAL] batch:  241 | acc: 75.00%,  total acc: 67.54%   [EVAL] batch:  242 | acc: 56.25%,  total acc: 67.49%   [EVAL] batch:  243 | acc: 43.75%,  total acc: 67.39%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 67.42%   [EVAL] batch:  245 | acc: 50.00%,  total acc: 67.35%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 67.33%   [EVAL] batch:  247 | acc: 81.25%,  total acc: 67.39%   [EVAL] batch:  248 | acc: 81.25%,  total acc: 67.44%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 67.50%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 67.23%   [EVAL] batch:  251 | acc: 18.75%,  total acc: 67.04%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 66.77%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 66.54%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 66.27%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 66.02%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 66.00%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 66.06%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 66.19%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 66.25%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 66.31%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 66.36%   [EVAL] batch:  262 | acc: 50.00%,  total acc: 66.30%   [EVAL] batch:  263 | acc: 31.25%,  total acc: 66.17%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 66.11%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 65.98%   [EVAL] batch:  266 | acc: 6.25%,  total acc: 65.75%   [EVAL] batch:  267 | acc: 37.50%,  total acc: 65.65%   [EVAL] batch:  268 | acc: 43.75%,  total acc: 65.57%   [EVAL] batch:  269 | acc: 6.25%,  total acc: 65.35%   [EVAL] batch:  270 | acc: 6.25%,  total acc: 65.13%   [EVAL] batch:  271 | acc: 12.50%,  total acc: 64.94%   [EVAL] batch:  272 | acc: 18.75%,  total acc: 64.77%   [EVAL] batch:  273 | acc: 6.25%,  total acc: 64.55%   [EVAL] batch:  274 | acc: 12.50%,  total acc: 64.36%   [EVAL] batch:  275 | acc: 0.00%,  total acc: 64.13%   [EVAL] batch:  276 | acc: 6.25%,  total acc: 63.92%   [EVAL] batch:  277 | acc: 12.50%,  total acc: 63.74%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 63.53%   [EVAL] batch:  279 | acc: 12.50%,  total acc: 63.35%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 63.12%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 63.08%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 63.16%   [EVAL] batch:  283 | acc: 93.75%,  total acc: 63.27%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 63.38%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 63.46%   [EVAL] batch:  286 | acc: 81.25%,  total acc: 63.52%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 63.63%   [EVAL] batch:  288 | acc: 87.50%,  total acc: 63.71%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 63.84%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 63.96%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 64.02%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 64.12%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 64.22%   [EVAL] batch:  294 | acc: 81.25%,  total acc: 64.28%   [EVAL] batch:  295 | acc: 87.50%,  total acc: 64.36%   [EVAL] batch:  296 | acc: 62.50%,  total acc: 64.35%   [EVAL] batch:  297 | acc: 75.00%,  total acc: 64.39%   [EVAL] batch:  298 | acc: 93.75%,  total acc: 64.49%   [EVAL] batch:  299 | acc: 75.00%,  total acc: 64.52%   [EVAL] batch:  300 | acc: 12.50%,  total acc: 64.35%   [EVAL] batch:  301 | acc: 50.00%,  total acc: 64.30%   [EVAL] batch:  302 | acc: 62.50%,  total acc: 64.29%   [EVAL] batch:  303 | acc: 56.25%,  total acc: 64.27%   [EVAL] batch:  304 | acc: 50.00%,  total acc: 64.22%   [EVAL] batch:  305 | acc: 68.75%,  total acc: 64.24%   [EVAL] batch:  306 | acc: 37.50%,  total acc: 64.15%   [EVAL] batch:  307 | acc: 43.75%,  total acc: 64.08%   [EVAL] batch:  308 | acc: 37.50%,  total acc: 64.00%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 63.99%   [EVAL] batch:  310 | acc: 56.25%,  total acc: 63.97%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 64.02%   [EVAL] batch:  312 | acc: 56.25%,  total acc: 64.00%   [EVAL] batch:  313 | acc: 37.50%,  total acc: 63.91%   [EVAL] batch:  314 | acc: 43.75%,  total acc: 63.85%   [EVAL] batch:  315 | acc: 62.50%,  total acc: 63.84%   [EVAL] batch:  316 | acc: 37.50%,  total acc: 63.76%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 63.68%   [EVAL] batch:  318 | acc: 50.00%,  total acc: 63.64%   [EVAL] batch:  319 | acc: 93.75%,  total acc: 63.73%   [EVAL] batch:  320 | acc: 87.50%,  total acc: 63.80%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 63.84%   [EVAL] batch:  322 | acc: 75.00%,  total acc: 63.87%   [EVAL] batch:  323 | acc: 75.00%,  total acc: 63.91%   [EVAL] batch:  324 | acc: 75.00%,  total acc: 63.94%   [EVAL] batch:  325 | acc: 62.50%,  total acc: 63.94%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 63.91%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 63.95%   [EVAL] batch:  328 | acc: 68.75%,  total acc: 63.96%   [EVAL] batch:  329 | acc: 75.00%,  total acc: 64.00%   [EVAL] batch:  330 | acc: 62.50%,  total acc: 63.99%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 64.08%   [EVAL] batch:  332 | acc: 75.00%,  total acc: 64.11%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 64.20%   [EVAL] batch:  334 | acc: 81.25%,  total acc: 64.25%   [EVAL] batch:  335 | acc: 87.50%,  total acc: 64.32%   [EVAL] batch:  336 | acc: 81.25%,  total acc: 64.37%   [EVAL] batch:  337 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:  338 | acc: 93.75%,  total acc: 64.56%   [EVAL] batch:  339 | acc: 87.50%,  total acc: 64.63%   [EVAL] batch:  340 | acc: 87.50%,  total acc: 64.70%   [EVAL] batch:  341 | acc: 75.00%,  total acc: 64.73%   [EVAL] batch:  342 | acc: 87.50%,  total acc: 64.80%   [EVAL] batch:  343 | acc: 50.00%,  total acc: 64.75%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 64.76%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 64.70%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 64.70%   [EVAL] batch:  347 | acc: 56.25%,  total acc: 64.67%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 64.68%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 64.64%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 64.64%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 64.65%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 64.64%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 64.69%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 64.72%   [EVAL] batch:  355 | acc: 75.00%,  total acc: 64.75%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 64.71%   [EVAL] batch:  357 | acc: 25.00%,  total acc: 64.59%   [EVAL] batch:  358 | acc: 25.00%,  total acc: 64.48%   [EVAL] batch:  359 | acc: 37.50%,  total acc: 64.41%   [EVAL] batch:  360 | acc: 25.00%,  total acc: 64.30%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 64.26%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 64.24%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 64.34%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 64.43%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 64.53%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 64.63%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 64.72%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 64.82%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 64.88%   [EVAL] batch:  370 | acc: 81.25%,  total acc: 64.93%   [EVAL] batch:  371 | acc: 87.50%,  total acc: 64.99%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 65.06%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 65.11%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 65.17%   
cur_acc:  ['0.9524', '0.6974', '0.6845', '0.7470', '0.5248', '0.7034']
his_acc:  ['0.9524', '0.8230', '0.7510', '0.7205', '0.6550', '0.6517']
Clustering into  34  clusters
Clusters:  [ 5  1 26  4 30 30 20  0 29  6  2  7 25 27 23 14 21 13  3 16  3 24 12 18
  4  1  9  5  0  3 31 14 13 32 17 14 16  7  6  5  1 27  9 10  4 33 22  8
  7 29 13  0  0 30 10  6  5 30 14 14  2 11 18  3  0 19  8 15 28  7]
Losses:  8.7452392578125 2.3629932403564453 0.8023048043251038
CurrentTrain: epoch  0, batch     0 | loss: 8.7452393Losses:  8.941673278808594 3.115917921066284 0.7926717400550842
CurrentTrain: epoch  0, batch     1 | loss: 8.9416733Losses:  9.752099990844727 3.3139240741729736 0.7050287127494812
CurrentTrain: epoch  0, batch     2 | loss: 9.7521000Losses:  5.16909646987915 -0.0 0.15399497747421265
CurrentTrain: epoch  0, batch     3 | loss: 5.1690965Losses:  8.724864959716797 3.01357364654541 0.8126455545425415
CurrentTrain: epoch  1, batch     0 | loss: 8.7248650Losses:  8.57182502746582 2.7047362327575684 0.7304152846336365
CurrentTrain: epoch  1, batch     1 | loss: 8.5718250Losses:  8.37994384765625 3.285978317260742 0.6875770092010498
CurrentTrain: epoch  1, batch     2 | loss: 8.3799438Losses:  2.484699249267578 -0.0 0.14042559266090393
CurrentTrain: epoch  1, batch     3 | loss: 2.4846992Losses:  9.488080978393555 4.182819366455078 0.6739003658294678
CurrentTrain: epoch  2, batch     0 | loss: 9.4880810Losses:  7.478837490081787 3.1698243618011475 0.6554245352745056
CurrentTrain: epoch  2, batch     1 | loss: 7.4788375Losses:  9.03880500793457 4.1390862464904785 0.7154880166053772
CurrentTrain: epoch  2, batch     2 | loss: 9.0388050Losses:  2.642723560333252 -0.0 0.13528358936309814
CurrentTrain: epoch  2, batch     3 | loss: 2.6427236Losses:  7.1192097663879395 3.294065237045288 0.7527357339859009
CurrentTrain: epoch  3, batch     0 | loss: 7.1192098Losses:  7.019067287445068 2.6633801460266113 0.7299273610115051
CurrentTrain: epoch  3, batch     1 | loss: 7.0190673Losses:  7.673365116119385 2.629899263381958 0.8050865530967712
CurrentTrain: epoch  3, batch     2 | loss: 7.6733651Losses:  2.488473415374756 -0.0 0.08858504146337509
CurrentTrain: epoch  3, batch     3 | loss: 2.4884734Losses:  7.166170597076416 2.6894311904907227 0.7328980565071106
CurrentTrain: epoch  4, batch     0 | loss: 7.1661706Losses:  9.24325942993164 4.816471576690674 0.6474595069885254
CurrentTrain: epoch  4, batch     1 | loss: 9.2432594Losses:  6.46326208114624 3.2592506408691406 0.6594732403755188
CurrentTrain: epoch  4, batch     2 | loss: 6.4632621Losses:  2.6114132404327393 -0.0 0.14160427451133728
CurrentTrain: epoch  4, batch     3 | loss: 2.6114132Losses:  7.9123148918151855 3.733135223388672 0.641414999961853
CurrentTrain: epoch  5, batch     0 | loss: 7.9123149Losses:  6.880819797515869 3.5096144676208496 0.7334574460983276
CurrentTrain: epoch  5, batch     1 | loss: 6.8808198Losses:  6.337639331817627 2.426403522491455 0.7127479314804077
CurrentTrain: epoch  5, batch     2 | loss: 6.3376393Losses:  2.783668279647827 -0.0 0.0907270610332489
CurrentTrain: epoch  5, batch     3 | loss: 2.7836683Losses:  8.281097412109375 3.9462709426879883 0.6057736873626709
CurrentTrain: epoch  6, batch     0 | loss: 8.2810974Losses:  6.496100425720215 3.423184633255005 0.6490070819854736
CurrentTrain: epoch  6, batch     1 | loss: 6.4961004Losses:  6.749165058135986 3.3601479530334473 0.6684756875038147
CurrentTrain: epoch  6, batch     2 | loss: 6.7491651Losses:  2.4806923866271973 -0.0 0.11938001215457916
CurrentTrain: epoch  6, batch     3 | loss: 2.4806924Losses:  5.424751281738281 2.2923145294189453 0.7305147051811218
CurrentTrain: epoch  7, batch     0 | loss: 5.4247513Losses:  6.82465124130249 3.4557993412017822 0.6661105155944824
CurrentTrain: epoch  7, batch     1 | loss: 6.8246512Losses:  6.011307716369629 2.6777708530426025 0.704944372177124
CurrentTrain: epoch  7, batch     2 | loss: 6.0113077Losses:  1.9305903911590576 -0.0 0.09412694722414017
CurrentTrain: epoch  7, batch     3 | loss: 1.9305904Losses:  6.73738431930542 3.2843456268310547 0.7112739682197571
CurrentTrain: epoch  8, batch     0 | loss: 6.7373843Losses:  4.996342658996582 1.8403700590133667 0.7744323015213013
CurrentTrain: epoch  8, batch     1 | loss: 4.9963427Losses:  5.524942874908447 2.9782071113586426 0.6376957893371582
CurrentTrain: epoch  8, batch     2 | loss: 5.5249429Losses:  4.559426307678223 -0.0 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 4.5594263Losses:  6.225854396820068 3.169811487197876 0.6257863640785217
CurrentTrain: epoch  9, batch     0 | loss: 6.2258544Losses:  5.504234313964844 2.5502753257751465 0.6900194883346558
CurrentTrain: epoch  9, batch     1 | loss: 5.5042343Losses:  6.977653503417969 3.9033312797546387 0.6611301898956299
CurrentTrain: epoch  9, batch     2 | loss: 6.9776535Losses:  1.9587461948394775 -0.0 0.1581134796142578
CurrentTrain: epoch  9, batch     3 | loss: 1.9587462
#### contains administrative territorial entity ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "contains administrative territorial entity" means direct subdivisions of an administrative territorial entity.
Here is an example:
Relation: contains administrative territorial entity
Context: the sutagao are the chibcha - speaking indios sutagaos indigenous people from the region of fusagasugá , bogotá savanna , cundinamarca , colombia .
Head Entity: colombia
Tail Entity: cundinamarca
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "contains administrative territorial entity":
Relation: contains administrative territorial entity  
Context: The state of California is known for its diverse geography and is divided into several counties, including Los Angeles County, which is the most populous county in the United States.  
Head Entity: California  
Tail Entity: Los Angeles County  

Relation: contains administrative territorial entity  
Context: The country of Japan is made up of several prefectures, with Tokyo being the most famous and densely populated among them.  
Head Entity: Japan  
Tail Entity: Tokyo  
#### said to be the same as ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "said to be the same as" means this item is said to be the same as that item, but it's uncertain or disputed.
Here is an example:
Relation: said to be the same as
Context: this contrasts with the common usage of harem as an english loan - word , which implies a female - only enclave or seraglio .
Head Entity: harem
Tail Entity: seraglio
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "said to be the same as":
Relation: said to be the same as  
Context: Some researchers argue that the term "artificial intelligence" is often used interchangeably with "machine learning," although they are not exactly the same.  
Head Entity: artificial intelligence  
Tail Entity: machine learning  

Relation: said to be the same as  
Context: The two species, Canis lupus and Canis familiaris, are frequently said to be the same as they share a common ancestor, yet their classifications differ in the scientific community.  
Head Entity: Canis lupus  
Tail Entity: Canis familiaris  
#### winner ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "winner" means winner of a competition or similar event.
Here is an example:
Relation: winner
Context: the filming was primarily held at pollachi . it fetched pawan kalyan the filmfare award for best actor – telugu award at 59th filmfare awards south .
Head Entity: filmfare award for best actor – telugu
Tail Entity: pawan kalyan
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "winner":
Relation: winner  
Context: in the thrilling finale of the championship, the underdog team triumphed, securing the title and making history as they celebrated their victory at the national sports arena. This remarkable achievement earned them the prestigious trophy for the best team of the year.  
Head Entity: trophy for the best team of the year  
Tail Entity: underdog team  

Relation: winner  
Context: during the annual music awards, the talented singer captivated the audience with her powerful performance, ultimately leading her to win the coveted title of best new artist. The event was a celebration of emerging talent in the music industry.  
Head Entity: title of best new artist  
Tail Entity: talented singer  
#### military rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "military rank" means military rank achieved by a person, or military rank associated with a position.
Here is an example:
Relation: military rank
Context: the railroad car they were deported in was attached to the end of the last train out of drancy which also carried drancy commandant ss hauptsturmführer alois brunner and other german military personnel .
Head Entity: alois brunner
Tail Entity: hauptsturmführer
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "military rank":
Relation: military rank  
Context: General John Smith was awarded the title of Major General for his exceptional leadership during the conflict, which significantly contributed to the victory of his battalion.  
Head Entity: John Smith  
Tail Entity: Major General  

Relation: military rank  
Context: During the ceremony, Colonel Jane Doe was recognized for her outstanding service and was promoted to the rank of Brigadier General, marking a significant milestone in her military career.  
Head Entity: Jane Doe  
Tail Entity: Brigadier General  
#### publisher ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "publisher" means organization or person responsible for publishing books, periodicals, printed music, podcasts, games or software.
Here is an example:
Relation: publisher
Context: tokyopop also released a collection of karin short stories under the title : " chibi vampire : airmail " on august 31 , 2010 .
Head Entity: karin
Tail Entity: tokyopop
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "publisher":
Relation: publisher  
Context: Penguin Random House announced the release of a new novel by the acclaimed author John Grisham, set to hit the shelves on June 15, 2021.  
Head Entity: John Grisham  
Tail Entity: Penguin Random House  

Relation: publisher  
Context: The popular online platform Wattpad has been instrumental in launching the careers of many writers, including Anna Todd, whose series "After" gained immense popularity.  
Head Entity: Anna Todd  
Tail Entity: Wattpad  
#### followed by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "followed by" means immediately following item in a series of which the subject is a part.
Here is an example:
Relation: followed by
Context: " muzai moratorium " and " shōso strip " were released on vinyl in 2008 , as a part of her 10-year anniversary celebrations .
Head Entity: muzai moratorium
Tail Entity: shōso strip
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "followed by":
Relation: followed by  
Context: The first chapter of the book introduces the main character, while the second chapter follows with a detailed backstory.  
Head Entity: first chapter  
Tail Entity: second chapter  

Relation: followed by  
Context: In the concert lineup, the opening act performed a lively set, followed by the headliner who captivated the audience with their hits.  
Head Entity: opening act  
Tail Entity: headliner  
#### work location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "work location" means location where persons or organisations were actively participating in employment, business or other work.
Here is an example:
Relation: work location
Context: since then they had been under the supervision of valerand poullain , formerly john calvin 's successor as minister of the french congregation in strasbourg .
Head Entity: john calvin
Tail Entity: strasbourg
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "work location":
Relation: work location  
Context: The headquarters of the company is situated in the bustling city of San Francisco, where many tech startups thrive.  
Head Entity: tech startups  
Tail Entity: San Francisco  

Relation: work location  
Context: After years of research, the team relocated to a new facility in Austin, Texas, to better collaborate with local universities.  
Head Entity: research team  
Tail Entity: Austin, Texas  
#### occupation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "occupation" means occupation of a person.
Here is an example:
Relation: occupation
Context: andrei droznin ( russian : андрей дрознин ) is a russian theatre director and movement coach .
Head Entity: andrei droznin
Tail Entity: theatre director
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "occupation":
Relation: occupation  
Context: dr. emily thompson is a renowned biochemist known for her groundbreaking research in enzyme catalysis.  
Head Entity: dr. emily thompson  
Tail Entity: biochemist  

Relation: occupation  
Context: john smith, a talented software engineer, has been working on innovative artificial intelligence projects for over a decade.  
Head Entity: john smith  
Tail Entity: software engineer  
#### heritage designation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "heritage designation" means heritage designation of a cultural or natural site.
Here is an example:
Relation: heritage designation
Context: altamaha town , bluffton historic district , church of the cross , and rose hill plantation house are listed on the national register of historic places .
Head Entity: altamaha town
Tail Entity: national register of historic places
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "heritage designation":
Relation: heritage designation  
Context: The Great Wall of China is recognized as a UNESCO World Heritage Site due to its historical significance and architectural grandeur.  
Head Entity: Great Wall of China  
Tail Entity: UNESCO World Heritage Site  

Relation: heritage designation  
Context: The ancient city of Petra in Jordan is designated as a World Heritage Site, attracting tourists from around the globe.  
Head Entity: ancient city of Petra  
Tail Entity: World Heritage Site  
#### voice type ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "voice type" means person's voice type.
Here is an example:
Relation: voice type
Context: beverly bower ( september 30 , 1925 – march 24 , 2002 ) was an american operatic soprano who had an active international opera career from the mid-1950s through the early 1970s .
Head Entity: beverly bower
Tail Entity: soprano
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "voice type":
Relation: voice type  
Context: luciano pavarotti ( october 12 , 1935 – september 6 , 2007 ) was an italian tenor who gained worldwide fame for his televised concerts and media appearances.  
Head Entity: luciano pavarotti  
Tail Entity: tenor  

Relation: voice type  
Context: mariah carey ( born march 27 , 1969 ) is an american singer, songwriter, and actress known for her five-octave vocal range and her use of the whistle register.  
Head Entity: mariah carey  
Tail Entity: singer  
Losses:  3.621431827545166 0.2641032338142395 0.9421709775924683
MemoryTrain:  epoch  0, batch     0 | loss: 3.6214318Losses:  4.193922996520996 -0.0 1.0373343229293823
MemoryTrain:  epoch  0, batch     1 | loss: 4.1939230Losses:  3.3404715061187744 -0.0 1.0335681438446045
MemoryTrain:  epoch  0, batch     2 | loss: 3.3404715Losses:  3.063323736190796 -0.0 0.9395530223846436
MemoryTrain:  epoch  0, batch     3 | loss: 3.0633237Losses:  3.565098285675049 0.7284590005874634 0.9239233732223511
MemoryTrain:  epoch  0, batch     4 | loss: 3.5650983Losses:  4.937577724456787 0.3089646100997925 1.0075459480285645
MemoryTrain:  epoch  0, batch     5 | loss: 4.9375777Losses:  3.5523669719696045 0.2408542037010193 0.9310576319694519
MemoryTrain:  epoch  0, batch     6 | loss: 3.5523670Losses:  3.4860761165618896 0.24173137545585632 0.9529291987419128
MemoryTrain:  epoch  0, batch     7 | loss: 3.4860761Losses:  3.8532497882843018 0.25658297538757324 0.9629220366477966
MemoryTrain:  epoch  0, batch     8 | loss: 3.8532498Losses:  4.2513580322265625 0.549125075340271 0.7951105833053589
MemoryTrain:  epoch  0, batch     9 | loss: 4.2513580Losses:  5.572778224945068 0.9944308400154114 0.8630808591842651
MemoryTrain:  epoch  0, batch    10 | loss: 5.5727782Losses:  3.954451560974121 0.23604466021060944 0.9586080312728882
MemoryTrain:  epoch  0, batch    11 | loss: 3.9544516Losses:  4.4381608963012695 0.27015161514282227 1.0212042331695557
MemoryTrain:  epoch  0, batch    12 | loss: 4.4381609Losses:  1.797772765159607 -0.0 0.1254023015499115
MemoryTrain:  epoch  0, batch    13 | loss: 1.7977728Losses:  3.448667526245117 0.2485182285308838 0.8543984293937683
MemoryTrain:  epoch  1, batch     0 | loss: 3.4486675Losses:  3.7804198265075684 0.7788819074630737 0.92291259765625
MemoryTrain:  epoch  1, batch     1 | loss: 3.7804198Losses:  2.8059165477752686 0.2533950209617615 0.9504322409629822
MemoryTrain:  epoch  1, batch     2 | loss: 2.8059165Losses:  4.174371719360352 -0.0 0.9648714661598206
MemoryTrain:  epoch  1, batch     3 | loss: 4.1743717Losses:  3.3229076862335205 -0.0 0.8403729796409607
MemoryTrain:  epoch  1, batch     4 | loss: 3.3229077Losses:  3.3572731018066406 0.26576340198516846 0.7324443459510803
MemoryTrain:  epoch  1, batch     5 | loss: 3.3572731Losses:  3.9979395866394043 0.46639642119407654 0.9738930463790894
MemoryTrain:  epoch  1, batch     6 | loss: 3.9979396Losses:  3.5123205184936523 0.7785526514053345 0.9800142049789429
MemoryTrain:  epoch  1, batch     7 | loss: 3.5123205Losses:  4.7127366065979 -0.0 0.9297748804092407
MemoryTrain:  epoch  1, batch     8 | loss: 4.7127366Losses:  3.226884126663208 0.49942922592163086 0.8725064396858215
MemoryTrain:  epoch  1, batch     9 | loss: 3.2268841Losses:  3.4286341667175293 0.5001065135002136 0.9125697016716003
MemoryTrain:  epoch  1, batch    10 | loss: 3.4286342Losses:  3.4354848861694336 0.22635632753372192 0.8615087270736694
MemoryTrain:  epoch  1, batch    11 | loss: 3.4354849Losses:  3.354016065597534 0.2623833417892456 0.9350359439849854
MemoryTrain:  epoch  1, batch    12 | loss: 3.3540161Losses:  1.960314154624939 -0.0 0.16723191738128662
MemoryTrain:  epoch  1, batch    13 | loss: 1.9603142Losses:  3.2487826347351074 0.26548635959625244 0.9620308876037598
MemoryTrain:  epoch  2, batch     0 | loss: 3.2487826Losses:  2.9707858562469482 0.5387357473373413 0.8457005620002747
MemoryTrain:  epoch  2, batch     1 | loss: 2.9707859Losses:  3.7180233001708984 0.23455023765563965 0.9557884931564331
MemoryTrain:  epoch  2, batch     2 | loss: 3.7180233Losses:  3.727850914001465 0.7960792779922485 0.8596922159194946
MemoryTrain:  epoch  2, batch     3 | loss: 3.7278509Losses:  3.2122623920440674 -0.0 1.061826467514038
MemoryTrain:  epoch  2, batch     4 | loss: 3.2122624Losses:  3.992393970489502 0.7854756116867065 0.929527759552002
MemoryTrain:  epoch  2, batch     5 | loss: 3.9923940Losses:  3.687586784362793 0.24461567401885986 0.855342447757721
MemoryTrain:  epoch  2, batch     6 | loss: 3.6875868Losses:  2.728253126144409 0.2895737886428833 0.9085806608200073
MemoryTrain:  epoch  2, batch     7 | loss: 2.7282531Losses:  3.2664849758148193 0.8097715377807617 0.8697118163108826
MemoryTrain:  epoch  2, batch     8 | loss: 3.2664850Losses:  2.891925573348999 0.5470355749130249 0.894870936870575
MemoryTrain:  epoch  2, batch     9 | loss: 2.8919256Losses:  2.9005753993988037 -0.0 1.0148706436157227
MemoryTrain:  epoch  2, batch    10 | loss: 2.9005754Losses:  3.0335588455200195 0.5048912763595581 0.8736855387687683
MemoryTrain:  epoch  2, batch    11 | loss: 3.0335588Losses:  3.2003276348114014 0.2511979937553406 1.008803367614746
MemoryTrain:  epoch  2, batch    12 | loss: 3.2003276Losses:  1.7867927551269531 -0.0 0.12991967797279358
MemoryTrain:  epoch  2, batch    13 | loss: 1.7867928Losses:  3.046658992767334 0.23746557533740997 0.9429069757461548
MemoryTrain:  epoch  3, batch     0 | loss: 3.0466590Losses:  2.3539280891418457 -0.0 0.9445587396621704
MemoryTrain:  epoch  3, batch     1 | loss: 2.3539281Losses:  2.9424333572387695 0.5348495841026306 0.9005779027938843
MemoryTrain:  epoch  3, batch     2 | loss: 2.9424334Losses:  2.817809581756592 0.24044394493103027 1.0195919275283813
MemoryTrain:  epoch  3, batch     3 | loss: 2.8178096Losses:  2.76265811920166 -0.0 0.8477261662483215
MemoryTrain:  epoch  3, batch     4 | loss: 2.7626581Losses:  3.2838852405548096 0.5802682638168335 0.9025980830192566
MemoryTrain:  epoch  3, batch     5 | loss: 3.2838852Losses:  2.7793216705322266 -0.0 1.0771766901016235
MemoryTrain:  epoch  3, batch     6 | loss: 2.7793217Losses:  2.9858145713806152 0.24630841612815857 1.0204521417617798
MemoryTrain:  epoch  3, batch     7 | loss: 2.9858146Losses:  2.81347393989563 0.30400270223617554 0.9307305812835693
MemoryTrain:  epoch  3, batch     8 | loss: 2.8134739Losses:  2.554126739501953 0.23170214891433716 0.9096161127090454
MemoryTrain:  epoch  3, batch     9 | loss: 2.5541267Losses:  3.1096370220184326 0.2872318625450134 0.9856922030448914
MemoryTrain:  epoch  3, batch    10 | loss: 3.1096370Losses:  3.1729843616485596 0.5231977105140686 0.7875856757164001
MemoryTrain:  epoch  3, batch    11 | loss: 3.1729844Losses:  2.9805068969726562 0.4930570423603058 0.8554101586341858
MemoryTrain:  epoch  3, batch    12 | loss: 2.9805069Losses:  1.897979736328125 -0.0 -0.0
MemoryTrain:  epoch  3, batch    13 | loss: 1.8979797Losses:  2.5319623947143555 -0.0 1.0190263986587524
MemoryTrain:  epoch  4, batch     0 | loss: 2.5319624Losses:  2.6005465984344482 0.2713102698326111 0.9709506630897522
MemoryTrain:  epoch  4, batch     1 | loss: 2.6005466Losses:  2.856232166290283 0.5196964740753174 0.8996518850326538
MemoryTrain:  epoch  4, batch     2 | loss: 2.8562322Losses:  2.4523673057556152 -0.0 1.1213572025299072
MemoryTrain:  epoch  4, batch     3 | loss: 2.4523673Losses:  2.3751392364501953 -0.0 1.0388532876968384
MemoryTrain:  epoch  4, batch     4 | loss: 2.3751392Losses:  2.6629157066345215 0.2503677010536194 0.8660283088684082
MemoryTrain:  epoch  4, batch     5 | loss: 2.6629157Losses:  2.9465584754943848 0.2475702166557312 0.9143757820129395
MemoryTrain:  epoch  4, batch     6 | loss: 2.9465585Losses:  3.2358200550079346 0.7542369961738586 0.8487878441810608
MemoryTrain:  epoch  4, batch     7 | loss: 3.2358201Losses:  2.379335403442383 -0.0 1.000588059425354
MemoryTrain:  epoch  4, batch     8 | loss: 2.3793354Losses:  2.7453317642211914 -0.0 0.9865745306015015
MemoryTrain:  epoch  4, batch     9 | loss: 2.7453318Losses:  2.8835296630859375 0.2737245261669159 0.9538183212280273
MemoryTrain:  epoch  4, batch    10 | loss: 2.8835297Losses:  2.664280652999878 -0.0 0.9690680503845215
MemoryTrain:  epoch  4, batch    11 | loss: 2.6642807Losses:  2.8249573707580566 -0.0 0.9941394329071045
MemoryTrain:  epoch  4, batch    12 | loss: 2.8249574Losses:  3.351228952407837 -0.0 0.14648228883743286
MemoryTrain:  epoch  4, batch    13 | loss: 3.3512290Losses:  3.105907917022705 0.7517777681350708 0.9835983514785767
MemoryTrain:  epoch  5, batch     0 | loss: 3.1059079Losses:  2.584717273712158 0.25773581862449646 0.9959331750869751
MemoryTrain:  epoch  5, batch     1 | loss: 2.5847173Losses:  2.421802520751953 -0.0 0.9228195548057556
MemoryTrain:  epoch  5, batch     2 | loss: 2.4218025Losses:  2.981823444366455 0.4792589545249939 0.9215506315231323
MemoryTrain:  epoch  5, batch     3 | loss: 2.9818234Losses:  2.6911633014678955 0.4487948417663574 0.9168025851249695
MemoryTrain:  epoch  5, batch     4 | loss: 2.6911633Losses:  2.8154444694519043 0.2600680887699127 1.0241607427597046
MemoryTrain:  epoch  5, batch     5 | loss: 2.8154445Losses:  2.995253086090088 0.5508449077606201 0.8658770322799683
MemoryTrain:  epoch  5, batch     6 | loss: 2.9952531Losses:  2.4978878498077393 -0.0 0.9742102026939392
MemoryTrain:  epoch  5, batch     7 | loss: 2.4978878Losses:  2.55328369140625 0.22372332215309143 1.0132193565368652
MemoryTrain:  epoch  5, batch     8 | loss: 2.5532837Losses:  2.5338826179504395 -0.0 0.8544310927391052
MemoryTrain:  epoch  5, batch     9 | loss: 2.5338826Losses:  2.5157313346862793 0.25643253326416016 0.8772929906845093
MemoryTrain:  epoch  5, batch    10 | loss: 2.5157313Losses:  2.726377010345459 0.2883386015892029 0.9677034616470337
MemoryTrain:  epoch  5, batch    11 | loss: 2.7263770Losses:  3.0845820903778076 0.24936269223690033 0.9919098019599915
MemoryTrain:  epoch  5, batch    12 | loss: 3.0845821Losses:  2.0397262573242188 -0.0 0.13636720180511475
MemoryTrain:  epoch  5, batch    13 | loss: 2.0397263Losses:  2.686885118484497 0.2515684962272644 0.9806523323059082
MemoryTrain:  epoch  6, batch     0 | loss: 2.6868851Losses:  2.8769471645355225 0.5452224612236023 0.9537622332572937
MemoryTrain:  epoch  6, batch     1 | loss: 2.8769472Losses:  2.641667127609253 0.46112942695617676 0.9031233191490173
MemoryTrain:  epoch  6, batch     2 | loss: 2.6416671Losses:  3.4317691326141357 0.8232139348983765 0.8615757822990417
MemoryTrain:  epoch  6, batch     3 | loss: 3.4317691Losses:  3.1983466148376465 0.818343460559845 0.8379441499710083
MemoryTrain:  epoch  6, batch     4 | loss: 3.1983466Losses:  2.7506613731384277 0.2806064188480377 0.9426924586296082
MemoryTrain:  epoch  6, batch     5 | loss: 2.7506614Losses:  2.5661914348602295 -0.0 0.9890642762184143
MemoryTrain:  epoch  6, batch     6 | loss: 2.5661914Losses:  2.394623279571533 -0.0 1.015005111694336
MemoryTrain:  epoch  6, batch     7 | loss: 2.3946233Losses:  2.5038652420043945 0.24146519601345062 0.7874512076377869
MemoryTrain:  epoch  6, batch     8 | loss: 2.5038652Losses:  2.3406805992126465 -0.0 1.000344157218933
MemoryTrain:  epoch  6, batch     9 | loss: 2.3406806Losses:  2.3789920806884766 -0.0 1.0513153076171875
MemoryTrain:  epoch  6, batch    10 | loss: 2.3789921Losses:  2.2694029808044434 -0.0 0.9701839685440063
MemoryTrain:  epoch  6, batch    11 | loss: 2.2694030Losses:  2.491584062576294 0.26245996356010437 0.9632834792137146
MemoryTrain:  epoch  6, batch    12 | loss: 2.4915841Losses:  2.129544496536255 -0.0 0.11109127849340439
MemoryTrain:  epoch  6, batch    13 | loss: 2.1295445Losses:  2.4972381591796875 0.2567855715751648 0.9167290925979614
MemoryTrain:  epoch  7, batch     0 | loss: 2.4972382Losses:  2.5785880088806152 0.3023858964443207 0.9080086946487427
MemoryTrain:  epoch  7, batch     1 | loss: 2.5785880Losses:  2.703902006149292 0.5044005513191223 0.9707849025726318
MemoryTrain:  epoch  7, batch     2 | loss: 2.7039020Losses:  2.599851608276367 0.26983922719955444 0.9300395250320435
MemoryTrain:  epoch  7, batch     3 | loss: 2.5998516Losses:  2.7977142333984375 0.24100345373153687 0.9120323061943054
MemoryTrain:  epoch  7, batch     4 | loss: 2.7977142Losses:  3.104206085205078 0.5574297904968262 1.0485295057296753
MemoryTrain:  epoch  7, batch     5 | loss: 3.1042061Losses:  2.701350212097168 0.27018052339553833 0.9901655912399292
MemoryTrain:  epoch  7, batch     6 | loss: 2.7013502Losses:  2.59844708442688 0.23544126749038696 0.9593570232391357
MemoryTrain:  epoch  7, batch     7 | loss: 2.5984471Losses:  2.5840044021606445 0.24862787127494812 1.0554804801940918
MemoryTrain:  epoch  7, batch     8 | loss: 2.5840044Losses:  2.5630273818969727 0.26883256435394287 0.9646250009536743
MemoryTrain:  epoch  7, batch     9 | loss: 2.5630274Losses:  2.424288034439087 0.2569049298763275 0.8445729613304138
MemoryTrain:  epoch  7, batch    10 | loss: 2.4242880Losses:  2.591801643371582 0.2626515030860901 0.8623005747795105
MemoryTrain:  epoch  7, batch    11 | loss: 2.5918016Losses:  2.270482063293457 -0.0 0.9713879823684692
MemoryTrain:  epoch  7, batch    12 | loss: 2.2704821Losses:  1.6347626447677612 -0.0 0.13912084698677063
MemoryTrain:  epoch  7, batch    13 | loss: 1.6347626Losses:  3.3199009895324707 0.5918803215026855 1.0343059301376343
MemoryTrain:  epoch  8, batch     0 | loss: 3.3199010Losses:  2.4306390285491943 0.24475648999214172 0.9676751494407654
MemoryTrain:  epoch  8, batch     1 | loss: 2.4306390Losses:  3.069772243499756 0.8840194940567017 0.883686900138855
MemoryTrain:  epoch  8, batch     2 | loss: 3.0697722Losses:  2.5867648124694824 0.48281329870224 0.8488032817840576
MemoryTrain:  epoch  8, batch     3 | loss: 2.5867648Losses:  2.6588447093963623 0.2374023050069809 1.022209644317627
MemoryTrain:  epoch  8, batch     4 | loss: 2.6588447Losses:  3.192931890487671 0.8655549883842468 0.880873441696167
MemoryTrain:  epoch  8, batch     5 | loss: 3.1929319Losses:  2.318516731262207 -0.0 1.0213823318481445
MemoryTrain:  epoch  8, batch     6 | loss: 2.3185167Losses:  2.7158045768737793 0.5253844261169434 0.858185350894928
MemoryTrain:  epoch  8, batch     7 | loss: 2.7158046Losses:  2.6342225074768066 0.24774163961410522 0.8880184888839722
MemoryTrain:  epoch  8, batch     8 | loss: 2.6342225Losses:  2.624662399291992 0.2515794038772583 1.0650142431259155
MemoryTrain:  epoch  8, batch     9 | loss: 2.6246624Losses:  2.5958385467529297 0.2563713788986206 1.0481988191604614
MemoryTrain:  epoch  8, batch    10 | loss: 2.5958385Losses:  2.954719066619873 0.7909508943557739 0.7937474250793457
MemoryTrain:  epoch  8, batch    11 | loss: 2.9547191Losses:  2.2406258583068848 -0.0 0.9295053482055664
MemoryTrain:  epoch  8, batch    12 | loss: 2.2406259Losses:  1.4797128438949585 -0.0 0.10139855742454529
MemoryTrain:  epoch  8, batch    13 | loss: 1.4797128Losses:  2.2953011989593506 -0.0 0.9626815915107727
MemoryTrain:  epoch  9, batch     0 | loss: 2.2953012Losses:  2.5488879680633545 0.2842104434967041 0.967312753200531
MemoryTrain:  epoch  9, batch     1 | loss: 2.5488880Losses:  2.799846887588501 0.5482873320579529 0.9628761410713196
MemoryTrain:  epoch  9, batch     2 | loss: 2.7998469Losses:  2.9753875732421875 0.9961222410202026 0.729282021522522
MemoryTrain:  epoch  9, batch     3 | loss: 2.9753876Losses:  2.471776008605957 -0.0 1.1117422580718994
MemoryTrain:  epoch  9, batch     4 | loss: 2.4717760Losses:  2.671532154083252 0.25356829166412354 1.0266516208648682
MemoryTrain:  epoch  9, batch     5 | loss: 2.6715322Losses:  2.245056390762329 -0.0 0.9767836928367615
MemoryTrain:  epoch  9, batch     6 | loss: 2.2450564Losses:  2.3349552154541016 -0.0 0.9604133367538452
MemoryTrain:  epoch  9, batch     7 | loss: 2.3349552Losses:  2.888741970062256 0.7728793621063232 0.9097046852111816
MemoryTrain:  epoch  9, batch     8 | loss: 2.8887420Losses:  2.8368442058563232 0.7902578115463257 0.7288525104522705
MemoryTrain:  epoch  9, batch     9 | loss: 2.8368442Losses:  2.5746867656707764 0.48702549934387207 0.8358538746833801
MemoryTrain:  epoch  9, batch    10 | loss: 2.5746868Losses:  2.3378818035125732 -0.0 0.96685391664505
MemoryTrain:  epoch  9, batch    11 | loss: 2.3378818Losses:  2.7880587577819824 0.7310540676116943 0.8434785008430481
MemoryTrain:  epoch  9, batch    12 | loss: 2.7880588Losses:  1.2682209014892578 -0.0 0.12536601722240448
MemoryTrain:  epoch  9, batch    13 | loss: 1.2682209
[EVAL] batch:    0 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    1 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    2 | acc: 37.50%,  total acc: 37.50%   [EVAL] batch:    3 | acc: 43.75%,  total acc: 39.06%   [EVAL] batch:    4 | acc: 43.75%,  total acc: 40.00%   [EVAL] batch:    5 | acc: 50.00%,  total acc: 41.67%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 42.86%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 49.22%   [EVAL] batch:    8 | acc: 68.75%,  total acc: 51.39%   [EVAL] batch:    9 | acc: 75.00%,  total acc: 53.75%   [EVAL] batch:   10 | acc: 43.75%,  total acc: 52.84%   [EVAL] batch:   11 | acc: 68.75%,  total acc: 54.17%   [EVAL] batch:   12 | acc: 62.50%,  total acc: 54.81%   [EVAL] batch:   13 | acc: 50.00%,  total acc: 54.46%   [EVAL] batch:   14 | acc: 43.75%,  total acc: 53.75%   [EVAL] batch:   15 | acc: 43.75%,  total acc: 53.12%   [EVAL] batch:   16 | acc: 50.00%,  total acc: 52.94%   [EVAL] batch:   17 | acc: 62.50%,  total acc: 53.47%   [EVAL] batch:   18 | acc: 81.25%,  total acc: 54.93%   [EVAL] batch:   19 | acc: 93.75%,  total acc: 56.88%   [EVAL] batch:   20 | acc: 100.00%,  total acc: 58.93%   [EVAL] batch:   21 | acc: 100.00%,  total acc: 60.80%   [EVAL] batch:   22 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   23 | acc: 93.75%,  total acc: 63.80%   [EVAL] batch:   24 | acc: 100.00%,  total acc: 65.25%   [EVAL] batch:   25 | acc: 50.00%,  total acc: 64.66%   [EVAL] batch:   26 | acc: 18.75%,  total acc: 62.96%   [EVAL] batch:   27 | acc: 18.75%,  total acc: 61.38%   [EVAL] batch:   28 | acc: 31.25%,  total acc: 60.34%   [EVAL] batch:   29 | acc: 25.00%,  total acc: 59.17%   [EVAL] batch:   30 | acc: 50.00%,  total acc: 58.87%   [EVAL] batch:   31 | acc: 50.00%,  total acc: 58.59%   [EVAL] batch:   32 | acc: 50.00%,  total acc: 58.33%   [EVAL] batch:   33 | acc: 25.00%,  total acc: 57.35%   [EVAL] batch:   34 | acc: 43.75%,  total acc: 56.96%   [EVAL] batch:   35 | acc: 18.75%,  total acc: 55.90%   [EVAL] batch:   36 | acc: 18.75%,  total acc: 54.90%   [EVAL] batch:   37 | acc: 62.50%,  total acc: 55.10%   [EVAL] batch:   38 | acc: 50.00%,  total acc: 54.97%   [EVAL] batch:   39 | acc: 75.00%,  total acc: 55.47%   [EVAL] batch:   40 | acc: 68.75%,  total acc: 55.79%   [EVAL] batch:   41 | acc: 81.25%,  total acc: 56.40%   [EVAL] batch:   42 | acc: 68.75%,  total acc: 56.69%   [EVAL] batch:   43 | acc: 56.25%,  total acc: 56.68%   [EVAL] batch:   44 | acc: 62.50%,  total acc: 56.81%   [EVAL] batch:   45 | acc: 56.25%,  total acc: 56.79%   [EVAL] batch:   46 | acc: 56.25%,  total acc: 56.78%   [EVAL] batch:   47 | acc: 62.50%,  total acc: 56.90%   [EVAL] batch:   48 | acc: 50.00%,  total acc: 56.76%   [EVAL] batch:   49 | acc: 81.25%,  total acc: 57.25%   [EVAL] batch:   50 | acc: 100.00%,  total acc: 58.09%   [EVAL] batch:   51 | acc: 100.00%,  total acc: 58.89%   [EVAL] batch:   52 | acc: 100.00%,  total acc: 59.67%   [EVAL] batch:   53 | acc: 100.00%,  total acc: 60.42%   [EVAL] batch:   54 | acc: 100.00%,  total acc: 61.14%   [EVAL] batch:   55 | acc: 100.00%,  total acc: 61.83%   [EVAL] batch:   56 | acc: 100.00%,  total acc: 62.50%   [EVAL] batch:   57 | acc: 87.50%,  total acc: 62.93%   [EVAL] batch:   58 | acc: 93.75%,  total acc: 63.45%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 64.06%   [EVAL] batch:   60 | acc: 93.75%,  total acc: 64.55%   [EVAL] batch:   61 | acc: 100.00%,  total acc: 65.12%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 64.88%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 87.50%,  total acc: 81.25%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 79.69%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 76.25%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    6 | acc: 50.00%,  total acc: 74.11%   [EVAL] batch:    7 | acc: 81.25%,  total acc: 75.00%   [EVAL] batch:    8 | acc: 81.25%,  total acc: 75.69%   [EVAL] batch:    9 | acc: 56.25%,  total acc: 73.75%   [EVAL] batch:   10 | acc: 68.75%,  total acc: 73.30%   [EVAL] batch:   11 | acc: 62.50%,  total acc: 72.40%   [EVAL] batch:   12 | acc: 75.00%,  total acc: 72.60%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 74.11%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 75.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 76.95%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 78.31%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 79.17%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 80.26%   [EVAL] batch:   19 | acc: 81.25%,  total acc: 80.31%   [EVAL] batch:   20 | acc: 93.75%,  total acc: 80.95%   [EVAL] batch:   21 | acc: 93.75%,  total acc: 81.53%   [EVAL] batch:   22 | acc: 81.25%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 81.77%   [EVAL] batch:   24 | acc: 81.25%,  total acc: 81.75%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 82.45%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 83.10%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 83.71%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 84.05%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 84.58%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 85.08%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 85.55%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 85.98%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 86.40%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 86.61%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 86.81%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 87.16%   [EVAL] batch:   37 | acc: 93.75%,  total acc: 87.34%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 87.66%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 87.97%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 88.11%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 88.10%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 88.08%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 88.07%   [EVAL] batch:   44 | acc: 75.00%,  total acc: 87.78%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   46 | acc: 62.50%,  total acc: 86.97%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 86.72%   [EVAL] batch:   48 | acc: 93.75%,  total acc: 86.86%   [EVAL] batch:   49 | acc: 62.50%,  total acc: 86.38%   [EVAL] batch:   50 | acc: 81.25%,  total acc: 86.27%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 86.06%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 86.20%   [EVAL] batch:   53 | acc: 81.25%,  total acc: 86.11%   [EVAL] batch:   54 | acc: 56.25%,  total acc: 85.57%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 85.38%   [EVAL] batch:   56 | acc: 68.75%,  total acc: 85.09%   [EVAL] batch:   57 | acc: 43.75%,  total acc: 84.38%   [EVAL] batch:   58 | acc: 68.75%,  total acc: 84.11%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 84.06%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 84.02%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 83.77%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 83.04%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 81.84%   [EVAL] batch:   64 | acc: 0.00%,  total acc: 80.58%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 79.55%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 78.54%   [EVAL] batch:   67 | acc: 12.50%,  total acc: 77.57%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 76.90%   [EVAL] batch:   69 | acc: 81.25%,  total acc: 76.96%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 76.85%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 76.56%   [EVAL] batch:   72 | acc: 56.25%,  total acc: 76.28%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 75.84%   [EVAL] batch:   74 | acc: 75.00%,  total acc: 75.83%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 75.90%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 75.89%   [EVAL] batch:   77 | acc: 68.75%,  total acc: 75.80%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 76.03%   [EVAL] batch:   79 | acc: 93.75%,  total acc: 76.25%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 76.54%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 75.91%   [EVAL] batch:   82 | acc: 18.75%,  total acc: 75.23%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 74.48%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 73.60%   [EVAL] batch:   85 | acc: 18.75%,  total acc: 72.97%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 72.49%   [EVAL] batch:   87 | acc: 43.75%,  total acc: 72.16%   [EVAL] batch:   88 | acc: 100.00%,  total acc: 72.47%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 72.78%   [EVAL] batch:   90 | acc: 87.50%,  total acc: 72.94%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 73.23%   [EVAL] batch:   92 | acc: 93.75%,  total acc: 73.45%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 73.67%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 73.95%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 74.22%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 74.48%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 74.74%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 75.00%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 75.25%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 75.19%   [EVAL] batch:  101 | acc: 75.00%,  total acc: 75.18%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 75.18%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 75.06%   [EVAL] batch:  104 | acc: 93.75%,  total acc: 75.24%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 75.12%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 74.82%   [EVAL] batch:  107 | acc: 25.00%,  total acc: 74.36%   [EVAL] batch:  108 | acc: 43.75%,  total acc: 74.08%   [EVAL] batch:  109 | acc: 50.00%,  total acc: 73.86%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 73.65%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 73.27%   [EVAL] batch:  112 | acc: 62.50%,  total acc: 73.17%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 73.25%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 73.48%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 73.49%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 73.61%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 73.73%   [EVAL] batch:  118 | acc: 75.00%,  total acc: 73.74%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 73.28%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 72.83%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 72.28%   [EVAL] batch:  122 | acc: 25.00%,  total acc: 71.90%   [EVAL] batch:  123 | acc: 18.75%,  total acc: 71.47%   [EVAL] batch:  124 | acc: 25.00%,  total acc: 71.10%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 70.98%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 70.82%   [EVAL] batch:  127 | acc: 50.00%,  total acc: 70.65%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 70.64%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 70.62%   [EVAL] batch:  130 | acc: 56.25%,  total acc: 70.52%   [EVAL] batch:  131 | acc: 87.50%,  total acc: 70.64%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 70.77%   [EVAL] batch:  133 | acc: 75.00%,  total acc: 70.80%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 70.83%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 71.00%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 71.17%   [EVAL] batch:  137 | acc: 43.75%,  total acc: 70.97%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 70.55%   [EVAL] batch:  139 | acc: 18.75%,  total acc: 70.18%   [EVAL] batch:  140 | acc: 6.25%,  total acc: 69.73%   [EVAL] batch:  141 | acc: 6.25%,  total acc: 69.28%   [EVAL] batch:  142 | acc: 12.50%,  total acc: 68.88%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 68.62%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 68.79%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 69.01%   [EVAL] batch:  146 | acc: 93.75%,  total acc: 69.18%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 69.38%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 69.59%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 69.75%   [EVAL] batch:  150 | acc: 18.75%,  total acc: 69.41%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 69.00%   [EVAL] batch:  152 | acc: 37.50%,  total acc: 68.79%   [EVAL] batch:  153 | acc: 6.25%,  total acc: 68.38%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 68.02%   [EVAL] batch:  155 | acc: 6.25%,  total acc: 67.63%   [EVAL] batch:  156 | acc: 68.75%,  total acc: 67.64%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 67.80%   [EVAL] batch:  158 | acc: 87.50%,  total acc: 67.92%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 68.09%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 68.21%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 68.36%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 68.33%   [EVAL] batch:  163 | acc: 37.50%,  total acc: 68.14%   [EVAL] batch:  164 | acc: 37.50%,  total acc: 67.95%   [EVAL] batch:  165 | acc: 50.00%,  total acc: 67.85%   [EVAL] batch:  166 | acc: 37.50%,  total acc: 67.66%   [EVAL] batch:  167 | acc: 81.25%,  total acc: 67.75%   [EVAL] batch:  168 | acc: 62.50%,  total acc: 67.71%   [EVAL] batch:  169 | acc: 6.25%,  total acc: 67.35%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 67.00%   [EVAL] batch:  171 | acc: 0.00%,  total acc: 66.61%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 66.29%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 65.95%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 65.61%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 65.38%   [EVAL] batch:  176 | acc: 31.25%,  total acc: 65.18%   [EVAL] batch:  177 | acc: 31.25%,  total acc: 64.99%   [EVAL] batch:  178 | acc: 50.00%,  total acc: 64.91%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 64.79%   [EVAL] batch:  180 | acc: 37.50%,  total acc: 64.64%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 64.70%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 64.82%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 64.98%   [EVAL] batch:  184 | acc: 75.00%,  total acc: 65.03%   [EVAL] batch:  185 | acc: 100.00%,  total acc: 65.22%   [EVAL] batch:  186 | acc: 81.25%,  total acc: 65.31%   [EVAL] batch:  187 | acc: 50.00%,  total acc: 65.23%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 64.91%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 64.64%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 64.30%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 64.00%   [EVAL] batch:  192 | acc: 6.25%,  total acc: 63.70%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 63.47%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 63.43%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 63.36%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 63.42%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 63.51%   [EVAL] batch:  198 | acc: 62.50%,  total acc: 63.51%   [EVAL] batch:  199 | acc: 62.50%,  total acc: 63.50%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 63.46%   [EVAL] batch:  201 | acc: 43.75%,  total acc: 63.37%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 63.27%   [EVAL] batch:  203 | acc: 62.50%,  total acc: 63.27%   [EVAL] batch:  204 | acc: 37.50%,  total acc: 63.14%   [EVAL] batch:  205 | acc: 43.75%,  total acc: 63.05%   [EVAL] batch:  206 | acc: 87.50%,  total acc: 63.16%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 63.34%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 63.52%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 63.66%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 63.80%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 64.11%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 64.28%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 64.45%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 64.61%   [EVAL] batch:  216 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 64.91%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 65.07%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 65.23%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 65.38%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 65.48%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 65.61%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 65.74%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 65.89%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 65.93%   [EVAL] batch:  226 | acc: 50.00%,  total acc: 65.86%   [EVAL] batch:  227 | acc: 37.50%,  total acc: 65.73%   [EVAL] batch:  228 | acc: 62.50%,  total acc: 65.72%   [EVAL] batch:  229 | acc: 50.00%,  total acc: 65.65%   [EVAL] batch:  230 | acc: 56.25%,  total acc: 65.61%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 65.73%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 65.85%   [EVAL] batch:  233 | acc: 87.50%,  total acc: 65.95%   [EVAL] batch:  234 | acc: 93.75%,  total acc: 66.06%   [EVAL] batch:  235 | acc: 75.00%,  total acc: 66.10%   [EVAL] batch:  236 | acc: 87.50%,  total acc: 66.19%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 66.18%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 66.19%   [EVAL] batch:  239 | acc: 68.75%,  total acc: 66.20%   [EVAL] batch:  240 | acc: 56.25%,  total acc: 66.16%   [EVAL] batch:  241 | acc: 68.75%,  total acc: 66.17%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 66.10%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 65.98%   [EVAL] batch:  244 | acc: 87.50%,  total acc: 66.07%   [EVAL] batch:  245 | acc: 62.50%,  total acc: 66.06%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 66.04%   [EVAL] batch:  247 | acc: 75.00%,  total acc: 66.08%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 66.16%   [EVAL] batch:  249 | acc: 81.25%,  total acc: 66.22%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 65.96%   [EVAL] batch:  251 | acc: 12.50%,  total acc: 65.75%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 65.49%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 65.26%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 65.00%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 64.75%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 64.74%   [EVAL] batch:  257 | acc: 87.50%,  total acc: 64.83%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 64.96%   [EVAL] batch:  259 | acc: 87.50%,  total acc: 65.05%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 65.11%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 65.17%   [EVAL] batch:  262 | acc: 50.00%,  total acc: 65.11%   [EVAL] batch:  263 | acc: 25.00%,  total acc: 64.96%   [EVAL] batch:  264 | acc: 50.00%,  total acc: 64.91%   [EVAL] batch:  265 | acc: 31.25%,  total acc: 64.78%   [EVAL] batch:  266 | acc: 12.50%,  total acc: 64.58%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 64.46%   [EVAL] batch:  268 | acc: 25.00%,  total acc: 64.31%   [EVAL] batch:  269 | acc: 12.50%,  total acc: 64.12%   [EVAL] batch:  270 | acc: 25.00%,  total acc: 63.98%   [EVAL] batch:  271 | acc: 18.75%,  total acc: 63.81%   [EVAL] batch:  272 | acc: 37.50%,  total acc: 63.71%   [EVAL] batch:  273 | acc: 12.50%,  total acc: 63.53%   [EVAL] batch:  274 | acc: 31.25%,  total acc: 63.41%   [EVAL] batch:  275 | acc: 0.00%,  total acc: 63.18%   [EVAL] batch:  276 | acc: 6.25%,  total acc: 62.97%   [EVAL] batch:  277 | acc: 12.50%,  total acc: 62.79%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 62.59%   [EVAL] batch:  279 | acc: 18.75%,  total acc: 62.43%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 62.21%   [EVAL] batch:  281 | acc: 50.00%,  total acc: 62.17%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 62.26%   [EVAL] batch:  283 | acc: 93.75%,  total acc: 62.37%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 62.48%   [EVAL] batch:  285 | acc: 87.50%,  total acc: 62.57%   [EVAL] batch:  286 | acc: 75.00%,  total acc: 62.61%   [EVAL] batch:  287 | acc: 93.75%,  total acc: 62.72%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 62.82%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 62.95%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 63.08%   [EVAL] batch:  291 | acc: 75.00%,  total acc: 63.12%   [EVAL] batch:  292 | acc: 93.75%,  total acc: 63.23%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 63.33%   [EVAL] batch:  294 | acc: 31.25%,  total acc: 63.22%   [EVAL] batch:  295 | acc: 25.00%,  total acc: 63.09%   [EVAL] batch:  296 | acc: 12.50%,  total acc: 62.92%   [EVAL] batch:  297 | acc: 18.75%,  total acc: 62.77%   [EVAL] batch:  298 | acc: 62.50%,  total acc: 62.77%   [EVAL] batch:  299 | acc: 25.00%,  total acc: 62.65%   [EVAL] batch:  300 | acc: 12.50%,  total acc: 62.48%   [EVAL] batch:  301 | acc: 50.00%,  total acc: 62.44%   [EVAL] batch:  302 | acc: 62.50%,  total acc: 62.44%   [EVAL] batch:  303 | acc: 56.25%,  total acc: 62.42%   [EVAL] batch:  304 | acc: 50.00%,  total acc: 62.38%   [EVAL] batch:  305 | acc: 68.75%,  total acc: 62.40%   [EVAL] batch:  306 | acc: 37.50%,  total acc: 62.32%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 62.28%   [EVAL] batch:  308 | acc: 31.25%,  total acc: 62.18%   [EVAL] batch:  309 | acc: 62.50%,  total acc: 62.18%   [EVAL] batch:  310 | acc: 43.75%,  total acc: 62.12%   [EVAL] batch:  311 | acc: 75.00%,  total acc: 62.16%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 62.10%   [EVAL] batch:  313 | acc: 31.25%,  total acc: 62.00%   [EVAL] batch:  314 | acc: 37.50%,  total acc: 61.92%   [EVAL] batch:  315 | acc: 43.75%,  total acc: 61.87%   [EVAL] batch:  316 | acc: 25.00%,  total acc: 61.75%   [EVAL] batch:  317 | acc: 37.50%,  total acc: 61.67%   [EVAL] batch:  318 | acc: 43.75%,  total acc: 61.62%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 61.74%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 61.86%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 61.90%   [EVAL] batch:  322 | acc: 81.25%,  total acc: 61.96%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 62.04%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 62.10%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 62.12%   [EVAL] batch:  326 | acc: 56.25%,  total acc: 62.10%   [EVAL] batch:  327 | acc: 75.00%,  total acc: 62.14%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 62.18%   [EVAL] batch:  329 | acc: 87.50%,  total acc: 62.25%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 62.29%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 62.39%   [EVAL] batch:  332 | acc: 68.75%,  total acc: 62.41%   [EVAL] batch:  333 | acc: 93.75%,  total acc: 62.50%   [EVAL] batch:  334 | acc: 75.00%,  total acc: 62.54%   [EVAL] batch:  335 | acc: 87.50%,  total acc: 62.61%   [EVAL] batch:  336 | acc: 75.00%,  total acc: 62.65%   [EVAL] batch:  337 | acc: 93.75%,  total acc: 62.74%   [EVAL] batch:  338 | acc: 93.75%,  total acc: 62.83%   [EVAL] batch:  339 | acc: 81.25%,  total acc: 62.89%   [EVAL] batch:  340 | acc: 81.25%,  total acc: 62.94%   [EVAL] batch:  341 | acc: 62.50%,  total acc: 62.94%   [EVAL] batch:  342 | acc: 81.25%,  total acc: 62.99%   [EVAL] batch:  343 | acc: 62.50%,  total acc: 62.99%   [EVAL] batch:  344 | acc: 75.00%,  total acc: 63.03%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 62.97%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 62.97%   [EVAL] batch:  347 | acc: 56.25%,  total acc: 62.95%   [EVAL] batch:  348 | acc: 68.75%,  total acc: 62.97%   [EVAL] batch:  349 | acc: 43.75%,  total acc: 62.91%   [EVAL] batch:  350 | acc: 56.25%,  total acc: 62.89%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 62.91%   [EVAL] batch:  352 | acc: 62.50%,  total acc: 62.91%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 62.96%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 62.99%   [EVAL] batch:  355 | acc: 68.75%,  total acc: 63.01%   [EVAL] batch:  356 | acc: 43.75%,  total acc: 62.96%   [EVAL] batch:  357 | acc: 18.75%,  total acc: 62.83%   [EVAL] batch:  358 | acc: 18.75%,  total acc: 62.71%   [EVAL] batch:  359 | acc: 37.50%,  total acc: 62.64%   [EVAL] batch:  360 | acc: 18.75%,  total acc: 62.52%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 62.48%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 62.47%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 62.57%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 62.67%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 62.77%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 62.87%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 62.98%   [EVAL] batch:  368 | acc: 100.00%,  total acc: 63.08%   [EVAL] batch:  369 | acc: 93.75%,  total acc: 63.16%   [EVAL] batch:  370 | acc: 87.50%,  total acc: 63.22%   [EVAL] batch:  371 | acc: 93.75%,  total acc: 63.31%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 63.44%   [EVAL] batch:  374 | acc: 87.50%,  total acc: 63.50%   [EVAL] batch:  375 | acc: 37.50%,  total acc: 63.43%   [EVAL] batch:  376 | acc: 37.50%,  total acc: 63.36%   [EVAL] batch:  377 | acc: 37.50%,  total acc: 63.29%   [EVAL] batch:  378 | acc: 43.75%,  total acc: 63.24%   [EVAL] batch:  379 | acc: 43.75%,  total acc: 63.19%   [EVAL] batch:  380 | acc: 50.00%,  total acc: 63.16%   [EVAL] batch:  381 | acc: 50.00%,  total acc: 63.12%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 63.20%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 63.22%   [EVAL] batch:  384 | acc: 75.00%,  total acc: 63.25%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 63.20%   [EVAL] batch:  386 | acc: 68.75%,  total acc: 63.21%   [EVAL] batch:  387 | acc: 62.50%,  total acc: 63.21%   [EVAL] batch:  388 | acc: 50.00%,  total acc: 63.17%   [EVAL] batch:  389 | acc: 43.75%,  total acc: 63.12%   [EVAL] batch:  390 | acc: 43.75%,  total acc: 63.08%   [EVAL] batch:  391 | acc: 50.00%,  total acc: 63.04%   [EVAL] batch:  392 | acc: 62.50%,  total acc: 63.04%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 63.09%   [EVAL] batch:  394 | acc: 93.75%,  total acc: 63.16%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 63.26%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 63.35%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 63.44%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 63.52%   [EVAL] batch:  399 | acc: 100.00%,  total acc: 63.61%   [EVAL] batch:  400 | acc: 50.00%,  total acc: 63.58%   [EVAL] batch:  401 | acc: 18.75%,  total acc: 63.46%   [EVAL] batch:  402 | acc: 18.75%,  total acc: 63.35%   [EVAL] batch:  403 | acc: 31.25%,  total acc: 63.27%   [EVAL] batch:  404 | acc: 25.00%,  total acc: 63.18%   [EVAL] batch:  405 | acc: 50.00%,  total acc: 63.15%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 63.11%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 63.08%   [EVAL] batch:  408 | acc: 25.00%,  total acc: 62.99%   [EVAL] batch:  409 | acc: 43.75%,  total acc: 62.94%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 62.83%   [EVAL] batch:  411 | acc: 18.75%,  total acc: 62.73%   [EVAL] batch:  412 | acc: 62.50%,  total acc: 62.73%   [EVAL] batch:  413 | acc: 50.00%,  total acc: 62.70%   [EVAL] batch:  414 | acc: 75.00%,  total acc: 62.73%   [EVAL] batch:  415 | acc: 68.75%,  total acc: 62.74%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 62.78%   [EVAL] batch:  417 | acc: 68.75%,  total acc: 62.80%   [EVAL] batch:  418 | acc: 56.25%,  total acc: 62.78%   [EVAL] batch:  419 | acc: 62.50%,  total acc: 62.78%   [EVAL] batch:  420 | acc: 56.25%,  total acc: 62.77%   [EVAL] batch:  421 | acc: 56.25%,  total acc: 62.75%   [EVAL] batch:  422 | acc: 62.50%,  total acc: 62.75%   [EVAL] batch:  423 | acc: 50.00%,  total acc: 62.72%   [EVAL] batch:  424 | acc: 81.25%,  total acc: 62.76%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 62.85%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 62.94%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 63.03%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 63.11%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 63.20%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 63.28%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 63.37%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 63.42%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 63.49%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 63.58%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 63.65%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 63.73%   [EVAL] batch:  437 | acc: 50.00%,  total acc: 63.70%   
cur_acc:  ['0.9524', '0.6974', '0.6845', '0.7470', '0.5248', '0.7034', '0.6488']
his_acc:  ['0.9524', '0.8230', '0.7510', '0.7205', '0.6550', '0.6517', '0.6370']
Clustering into  39  clusters
Clusters:  [ 5  0 26 16  1  1 37  6  0 11  2  8 29 14 23 19 22 24  4 17  4 25 30  7
  3  0 13  5 32  4 31 19 24 34 27 19 17  8 11  5  1 14 13 36  3 21 28  9
  8  0 24  6  6  1 36 11  5  1 19 19  2 35  7  4  6 20  9 10 38  8 16  5
  5  0 12 10 15 33 18  0]
Losses:  7.703237056732178 2.3165342807769775 0.7046486139297485
CurrentTrain: epoch  0, batch     0 | loss: 7.7032371Losses:  9.583235740661621 3.601184368133545 0.5406316518783569
CurrentTrain: epoch  0, batch     1 | loss: 9.5832357Losses:  8.626029968261719 3.226349353790283 0.5281636118888855
CurrentTrain: epoch  0, batch     2 | loss: 8.6260300Losses:  3.7662994861602783 -0.0 0.10381946712732315
CurrentTrain: epoch  0, batch     3 | loss: 3.7662995Losses:  8.451547622680664 3.706510066986084 0.5763847231864929
CurrentTrain: epoch  1, batch     0 | loss: 8.4515476Losses:  7.699841022491455 3.563260793685913 0.6156964302062988
CurrentTrain: epoch  1, batch     1 | loss: 7.6998410Losses:  5.954009056091309 2.486268997192383 0.6723579168319702
CurrentTrain: epoch  1, batch     2 | loss: 5.9540091Losses:  5.20680046081543 -0.0 0.1647615283727646
CurrentTrain: epoch  1, batch     3 | loss: 5.2068005Losses:  7.1798295974731445 3.1536507606506348 0.6603901386260986
CurrentTrain: epoch  2, batch     0 | loss: 7.1798296Losses:  5.607668876647949 2.05667781829834 0.6625673770904541
CurrentTrain: epoch  2, batch     1 | loss: 5.6076689Losses:  5.732826232910156 2.6633968353271484 0.5682717561721802
CurrentTrain: epoch  2, batch     2 | loss: 5.7328262Losses:  2.142258405685425 -0.0 0.13838857412338257
CurrentTrain: epoch  2, batch     3 | loss: 2.1422584Losses:  4.733880043029785 1.6735966205596924 0.6560120582580566
CurrentTrain: epoch  3, batch     0 | loss: 4.7338800Losses:  6.7080841064453125 3.94873046875 0.49719080328941345
CurrentTrain: epoch  3, batch     1 | loss: 6.7080841Losses:  7.839980125427246 4.166448593139648 0.6090954542160034
CurrentTrain: epoch  3, batch     2 | loss: 7.8399801Losses:  1.8766350746154785 -0.0 0.11680223047733307
CurrentTrain: epoch  3, batch     3 | loss: 1.8766351Losses:  5.964127063751221 3.2193894386291504 0.6884385943412781
CurrentTrain: epoch  4, batch     0 | loss: 5.9641271Losses:  5.4878153800964355 2.493704319000244 0.6544429063796997
CurrentTrain: epoch  4, batch     1 | loss: 5.4878154Losses:  6.143256187438965 3.1131420135498047 0.650259256362915
CurrentTrain: epoch  4, batch     2 | loss: 6.1432562Losses:  4.850634574890137 -0.0 0.11166742444038391
CurrentTrain: epoch  4, batch     3 | loss: 4.8506346Losses:  5.5712971687316895 2.3961141109466553 0.6612077951431274
CurrentTrain: epoch  5, batch     0 | loss: 5.5712972Losses:  6.4171624183654785 3.540431261062622 0.5732110142707825
CurrentTrain: epoch  5, batch     1 | loss: 6.4171624Losses:  5.321077346801758 2.558016538619995 0.5543029308319092
CurrentTrain: epoch  5, batch     2 | loss: 5.3210773Losses:  2.1716220378875732 -0.0 0.11425766348838806
CurrentTrain: epoch  5, batch     3 | loss: 2.1716220Losses:  5.642487525939941 2.7686498165130615 0.5763721466064453
CurrentTrain: epoch  6, batch     0 | loss: 5.6424875Losses:  6.55821418762207 3.9186275005340576 0.5828191637992859
CurrentTrain: epoch  6, batch     1 | loss: 6.5582142Losses:  6.437221050262451 3.794811964035034 0.4986911416053772
CurrentTrain: epoch  6, batch     2 | loss: 6.4372211Losses:  1.9003643989562988 -0.0 -0.0
CurrentTrain: epoch  6, batch     3 | loss: 1.9003644Losses:  5.375909328460693 2.9199790954589844 0.5671890377998352
CurrentTrain: epoch  7, batch     0 | loss: 5.3759093Losses:  5.469947338104248 2.8869190216064453 0.6261221766471863
CurrentTrain: epoch  7, batch     1 | loss: 5.4699473Losses:  6.310513496398926 3.865389585494995 0.5666124224662781
CurrentTrain: epoch  7, batch     2 | loss: 6.3105135Losses:  2.497896194458008 -0.0 0.1545485556125641
CurrentTrain: epoch  7, batch     3 | loss: 2.4978962Losses:  5.793644905090332 3.4382762908935547 0.5606456995010376
CurrentTrain: epoch  8, batch     0 | loss: 5.7936449Losses:  4.309952735900879 1.7930046319961548 0.5560314655303955
CurrentTrain: epoch  8, batch     1 | loss: 4.3099527Losses:  4.580613613128662 2.1451656818389893 0.5579974055290222
CurrentTrain: epoch  8, batch     2 | loss: 4.5806136Losses:  1.6963920593261719 -0.0 -0.0
CurrentTrain: epoch  8, batch     3 | loss: 1.6963921Losses:  4.257803440093994 1.7980961799621582 0.6182326078414917
CurrentTrain: epoch  9, batch     0 | loss: 4.2578034Losses:  4.210819244384766 1.824023723602295 0.6165697574615479
CurrentTrain: epoch  9, batch     1 | loss: 4.2108192Losses:  4.395875930786133 1.9470628499984741 0.6370276212692261
CurrentTrain: epoch  9, batch     2 | loss: 4.3958759Losses:  1.8320605754852295 -0.0 0.10437506437301636
CurrentTrain: epoch  9, batch     3 | loss: 1.8320606
#### member of political party ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "member of political party" means the political party of which a person is or has been a member or otherwise affiliated.
Here is an example:
Relation: member of political party
Context: karl johan aarønes ( 8 may 1900 – 12 august 1969 ) was a norwegian politician for the labour party .
Head Entity: karl johan aarønes
Tail Entity: labour party
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "member of political party":
Relation: member of political party  
Context: barack obama served as a member of the democratic party during his presidency from 2009 to 2017.  
Head Entity: barack obama  
Tail Entity: democratic party  

Relation: member of political party  
Context: angela merkel was a prominent member of the christian democratic union throughout her political career in germany.  
Head Entity: angela merkel  
Tail Entity: christian democratic union  
#### after a work by ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "after a work by" means artist whose work strongly inspired/ was copied in this item.
Here is an example:
Relation: after a work by
Context: it is a partly fictional biography , adapted by shane connaughton and jim sheridan from the book " my left foot " by christy brown .
Head Entity: my left foot
Tail Entity: christy brown
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "after a work by":
Relation: after a work by  
Context: the film is based on the novel "the great gatsby" by f. scott fitzgerald, capturing the essence of the roaring twenties.  
Head Entity: the great gatsby  
Tail Entity: f. scott fitzgerald  

Relation: after a work by  
Context: this musical is inspired by the play "hamlet" written by william shakespeare, reimagining the classic tragedy in a contemporary setting.  
Head Entity: hamlet  
Tail Entity: william shakespeare  
#### notable work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "notable work" means notable scientific, artistic or literary work, or other work of significance among subject's works.
Here is an example:
Relation: notable work
Context: another was " the death of nelson " by daniel maclise , a large wall painting in the royal gallery of the palace of westminster .
Head Entity: daniel maclise
Tail Entity: the death of nelson
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "notable work":
Relation: notable work  
Context: one of his most famous pieces is "the persistence of memory," a surreal painting that features melting clocks and explores the concept of time.  
Head Entity: salvador dalí  
Tail Entity: the persistence of memory  

Relation: notable work  
Context: her acclaimed novel "to kill a mockingbird" addresses serious issues of race and injustice in the American South through the eyes of a young girl.  
Head Entity: harper lee  
Tail Entity: to kill a mockingbird  
#### headquarters location ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "headquarters location" means city, where an organization's headquarters is or has been situated.
Here is an example:
Relation: headquarters location
Context: at the request of the stockholm institute for bible translation , mirza khazar translated the bible – the new testament and old testament – into the azerbaijani language .
Head Entity: institute for bible translation
Tail Entity: stockholm
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "headquarters location":
Relation: headquarters location  
Context: the global technology company, headquartered in san francisco, has been a leader in innovation for over two decades.  
Head Entity: global technology company  
Tail Entity: san francisco  

Relation: headquarters location  
Context: after years of expansion, the non-profit organization moved its headquarters to new york city to better serve its growing community.  
Head Entity: non-profit organization  
Tail Entity: new york city  
#### taxon rank ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "taxon rank" means level in a taxonomic hierarchy.
Here is an example:
Relation: taxon rank
Context: a number of species that were placed in the family phyllophoridae by thander in 1989 and 1990 are now included in cucumariidae .
Head Entity: phyllophoridae
Tail Entity: family
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "taxon rank":
Relation: taxon rank  
Context: The genus Canis includes several species, with Canis lupus being the most recognized as the wolf, which belongs to the family Canidae.  
Head Entity: Canis  
Tail Entity: genus  

Relation: taxon rank  
Context: In the classification of plants, the family Rosaceae encompasses a variety of species, including roses and strawberries, which are categorized under the order Rosales.  
Head Entity: Rosaceae  
Tail Entity: family  
#### field of work ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "field of work" means specialization of a person or organization.
Here is an example:
Relation: field of work
Context: he later collaborated with vito volterra and explored laplace transforms and other parts of functional analysis .
Head Entity: vito volterra
Tail Entity: functional analysis
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "field of work":
Relation: field of work  
Context: Marie Curie conducted pioneering research on radioactivity, which laid the groundwork for advancements in nuclear physics.  
Head Entity: Marie Curie  
Tail Entity: nuclear physics  

Relation: field of work  
Context: The company specializes in artificial intelligence and machine learning, providing innovative solutions for various industries.  
Head Entity: The company  
Tail Entity: artificial intelligence  
#### licensed to broadcast to ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "licensed to broadcast to" means place that a radio/TV station is licensed/required to broadcast to.
Here is an example:
Relation: licensed to broadcast to
Context: since march 2006 , wkxp 's programming has been simulcasted on 97.3 wzad wurtsboro , new york .
Head Entity: wzad
Tail Entity: wurtsboro , new york
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "licensed to broadcast to":
Relation: licensed to broadcast to  
Context: in 2010, the station wxyz began broadcasting to the community of springfield, providing local news and entertainment.  
Head Entity: wxyz  
Tail Entity: springfield  

Relation: licensed to broadcast to  
Context: the radio station kqrs has been granted a license to broadcast to the greater minneapolis area since 1985.  
Head Entity: kqrs  
Tail Entity: greater minneapolis area  
#### constellation ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "constellation" means the area of the celestial sphere of which the subject is a part.
Here is an example:
Relation: constellation
Context: ngc 192 is a type sba spiral galaxy located in the constellation cetus .
Head Entity: ngc 192
Tail Entity: cetus
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "constellation":
Relation: constellation  
Context: the orion nebula is located in the constellation orion and is one of the brightest nebulae visible to the naked eye.  
Head Entity: orion nebula  
Tail Entity: orion  

Relation: constellation  
Context: the star sirius is part of the constellation canis major, which is known for its prominence in the night sky.  
Head Entity: sirius  
Tail Entity: canis major  
#### platform ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "platform" means platform for which a work was developed or released, or the specific platform version of a software product.
Here is an example:
Relation: platform
Context: " toy soldiers : war chest " was released on august 11 , 2015 for windows , playstation 4 and xbox one .
Head Entity: toy soldiers : war chest
Tail Entity: xbox one
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "platform":
Relation: platform  
Context: "The Legend of Zelda: Breath of the Wild" was launched on March 3, 2017 for the Nintendo Switch and Wii U.  
Head Entity: The Legend of Zelda: Breath of the Wild  
Tail Entity: Nintendo Switch  

Relation: platform  
Context: "Cyberpunk 2077" was made available on December 10, 2020 for PC, PlayStation 4, and Xbox One.  
Head Entity: Cyberpunk 2077  
Tail Entity: PlayStation 4  
#### place served by transport hub ####
You are a data scientist working on a relation extraction task. Please do the following task and do not give output in the markdown format.One sample in relation extraction datasets consists of a relation, a context, a pair of head and tail entities in the context.The head entity has the relation with the tail entity.
Relation "place served by transport hub" means territorial entity or entities served by this transport hub (airport, train station, etc.).
Here is an example:
Relation: place served by transport hub
Context: the dallas - fort worth metroplex 's major international airport , dallas - fort worth international airport , is located a short distance to the northwest .
Head Entity: dallas - fort worth international airport
Tail Entity: dallas - fort worth metroplex
Please generate 2 diversity samples (must have full : Relation , Context , Head Entity , Tail Entity) like the above example for the relation "place served by transport hub":
Relation: place served by transport hub  
Context: the bustling city of los angeles is served by several major airports, including los angeles international airport, which connects travelers to destinations worldwide.  
Head Entity: los angeles international airport  
Tail Entity: los angeles  

Relation: place served by transport hub  
Context: the central train station in berlin, berlin hauptbahnhof, serves as a key transport hub for the city, linking various regional and international train services.  
Head Entity: berlin hauptbahnhof  
Tail Entity: berlin  
Losses:  2.6595003604888916 -0.0 0.9676273465156555
MemoryTrain:  epoch  0, batch     0 | loss: 2.6595004Losses:  2.476795196533203 0.23234161734580994 0.8087630271911621
MemoryTrain:  epoch  0, batch     1 | loss: 2.4767952Losses:  2.885446071624756 -0.0 0.9155791997909546
MemoryTrain:  epoch  0, batch     2 | loss: 2.8854461Losses:  3.6700663566589355 0.8874801397323608 0.9776808023452759
MemoryTrain:  epoch  0, batch     3 | loss: 3.6700664Losses:  3.257120132446289 -0.0 0.947585940361023
MemoryTrain:  epoch  0, batch     4 | loss: 3.2571201Losses:  3.82692551612854 -0.0 1.0142760276794434
MemoryTrain:  epoch  0, batch     5 | loss: 3.8269255Losses:  3.210296630859375 -0.0 0.9697762727737427
MemoryTrain:  epoch  0, batch     6 | loss: 3.2102966Losses:  4.472955226898193 0.5422192215919495 0.9934406876564026
MemoryTrain:  epoch  0, batch     7 | loss: 4.4729552Losses:  3.289926528930664 -0.0 0.9934772253036499
MemoryTrain:  epoch  0, batch     8 | loss: 3.2899265Losses:  3.1744778156280518 0.297043114900589 0.9282898902893066
MemoryTrain:  epoch  0, batch     9 | loss: 3.1744778Losses:  2.8684892654418945 -0.0 1.0035417079925537
MemoryTrain:  epoch  0, batch    10 | loss: 2.8684893Losses:  4.184844970703125 0.7425872087478638 0.8460168838500977
MemoryTrain:  epoch  0, batch    11 | loss: 4.1848450Losses:  3.0138659477233887 -0.0 0.961117148399353
MemoryTrain:  epoch  0, batch    12 | loss: 3.0138659Losses:  3.217388868331909 -0.0 1.0452709197998047
MemoryTrain:  epoch  0, batch    13 | loss: 3.2173889Losses:  4.177214622497559 0.2187204658985138 1.0337684154510498
MemoryTrain:  epoch  0, batch    14 | loss: 4.1772146Losses:  2.9329562187194824 -0.0 1.0114527940750122
MemoryTrain:  epoch  1, batch     0 | loss: 2.9329562Losses:  2.900665760040283 0.5145601630210876 0.9027519226074219
MemoryTrain:  epoch  1, batch     1 | loss: 2.9006658Losses:  5.069300651550293 1.1251637935638428 0.803433358669281
MemoryTrain:  epoch  1, batch     2 | loss: 5.0693007Losses:  2.9775798320770264 0.48034054040908813 0.9878931641578674
MemoryTrain:  epoch  1, batch     3 | loss: 2.9775798Losses:  3.0014560222625732 0.23678749799728394 0.8422313332557678
MemoryTrain:  epoch  1, batch     4 | loss: 3.0014560Losses:  2.6667981147766113 0.22154943645000458 0.8513698577880859
MemoryTrain:  epoch  1, batch     5 | loss: 2.6667981Losses:  2.5277774333953857 -0.0 1.0689995288848877
MemoryTrain:  epoch  1, batch     6 | loss: 2.5277774Losses:  3.1366159915924072 0.5003147721290588 0.8714950084686279
MemoryTrain:  epoch  1, batch     7 | loss: 3.1366160Losses:  3.0317811965942383 -0.0 1.0148158073425293
MemoryTrain:  epoch  1, batch     8 | loss: 3.0317812Losses:  3.6020352840423584 1.1852426528930664 0.9481555819511414
MemoryTrain:  epoch  1, batch     9 | loss: 3.6020353Losses:  3.2643094062805176 0.5165777206420898 0.9324254989624023
MemoryTrain:  epoch  1, batch    10 | loss: 3.2643094Losses:  2.5256271362304688 -0.0 0.9651671648025513
MemoryTrain:  epoch  1, batch    11 | loss: 2.5256271Losses:  2.8019042015075684 0.2595709562301636 1.0379822254180908
MemoryTrain:  epoch  1, batch    12 | loss: 2.8019042Losses:  3.0533080101013184 0.5612170100212097 0.798393964767456
MemoryTrain:  epoch  1, batch    13 | loss: 3.0533080Losses:  3.233947277069092 0.5559384226799011 0.9838327169418335
MemoryTrain:  epoch  1, batch    14 | loss: 3.2339473Losses:  2.4999728202819824 0.25402241945266724 0.9517649412155151
MemoryTrain:  epoch  2, batch     0 | loss: 2.4999728Losses:  3.1922996044158936 0.48644834756851196 0.9155041575431824
MemoryTrain:  epoch  2, batch     1 | loss: 3.1922996Losses:  2.829855442047119 0.5169196724891663 0.8580927848815918
MemoryTrain:  epoch  2, batch     2 | loss: 2.8298554Losses:  2.9824676513671875 0.49031490087509155 0.8421584367752075
MemoryTrain:  epoch  2, batch     3 | loss: 2.9824677Losses:  2.4373021125793457 -0.0 1.0599533319473267
MemoryTrain:  epoch  2, batch     4 | loss: 2.4373021Losses:  3.2543482780456543 -0.0 0.9875617027282715
MemoryTrain:  epoch  2, batch     5 | loss: 3.2543483Losses:  3.205493211746216 0.8859730362892151 0.9318885803222656
MemoryTrain:  epoch  2, batch     6 | loss: 3.2054932Losses:  2.689039707183838 0.24622592329978943 0.9181243181228638
MemoryTrain:  epoch  2, batch     7 | loss: 2.6890397Losses:  3.041092872619629 0.48490995168685913 0.9522435665130615
MemoryTrain:  epoch  2, batch     8 | loss: 3.0410929Losses:  2.6482303142547607 0.27327942848205566 0.7950208783149719
MemoryTrain:  epoch  2, batch     9 | loss: 2.6482303Losses:  2.6922736167907715 -0.0 1.0731993913650513
MemoryTrain:  epoch  2, batch    10 | loss: 2.6922736Losses:  2.8153576850891113 0.23374027013778687 0.9538272619247437
MemoryTrain:  epoch  2, batch    11 | loss: 2.8153577Losses:  2.6360301971435547 0.281554639339447 0.9727987051010132
MemoryTrain:  epoch  2, batch    12 | loss: 2.6360302Losses:  2.3057656288146973 -0.0 0.911483645439148
MemoryTrain:  epoch  2, batch    13 | loss: 2.3057656Losses:  2.382450580596924 -0.0 1.0670793056488037
MemoryTrain:  epoch  2, batch    14 | loss: 2.3824506Losses:  2.7647745609283447 0.4727334976196289 0.9518419504165649
MemoryTrain:  epoch  3, batch     0 | loss: 2.7647746Losses:  2.1669259071350098 -0.0 0.9099223613739014
MemoryTrain:  epoch  3, batch     1 | loss: 2.1669259Losses:  2.6808369159698486 0.2877856492996216 0.9522324204444885
MemoryTrain:  epoch  3, batch     2 | loss: 2.6808369Losses:  2.333991765975952 -0.0 0.9717350602149963
MemoryTrain:  epoch  3, batch     3 | loss: 2.3339918Losses:  2.6595749855041504 0.23791834712028503 1.0550645589828491
MemoryTrain:  epoch  3, batch     4 | loss: 2.6595750Losses:  2.276122808456421 -0.0 1.050480604171753
MemoryTrain:  epoch  3, batch     5 | loss: 2.2761228Losses:  2.4324417114257812 0.24437592923641205 0.9225344657897949
MemoryTrain:  epoch  3, batch     6 | loss: 2.4324417Losses:  2.4258148670196533 -0.0 1.0843074321746826
MemoryTrain:  epoch  3, batch     7 | loss: 2.4258149Losses:  2.7033495903015137 -0.0 0.922240674495697
MemoryTrain:  epoch  3, batch     8 | loss: 2.7033496Losses:  2.779689311981201 0.49646294116973877 0.9761974811553955
MemoryTrain:  epoch  3, batch     9 | loss: 2.7796893Losses:  2.6567065715789795 0.23960861563682556 0.9967136979103088
MemoryTrain:  epoch  3, batch    10 | loss: 2.6567066Losses:  2.8232738971710205 0.2561495900154114 1.0063440799713135
MemoryTrain:  epoch  3, batch    11 | loss: 2.8232739Losses:  2.775292158126831 0.2714167535305023 1.006788969039917
MemoryTrain:  epoch  3, batch    12 | loss: 2.7752922Losses:  2.9747135639190674 0.240130215883255 0.9645066857337952
MemoryTrain:  epoch  3, batch    13 | loss: 2.9747136Losses:  2.8046631813049316 0.5597652196884155 0.7322917580604553
MemoryTrain:  epoch  3, batch    14 | loss: 2.8046632Losses:  2.3612027168273926 -0.0 1.010542631149292
MemoryTrain:  epoch  4, batch     0 | loss: 2.3612027Losses:  2.3250441551208496 -0.0 1.0560617446899414
MemoryTrain:  epoch  4, batch     1 | loss: 2.3250442Losses:  2.5349206924438477 0.22705340385437012 0.9784156680107117
MemoryTrain:  epoch  4, batch     2 | loss: 2.5349207Losses:  2.524944305419922 0.27433478832244873 0.9273622035980225
MemoryTrain:  epoch  4, batch     3 | loss: 2.5249443Losses:  2.6378254890441895 0.2307857871055603 0.9984261989593506
MemoryTrain:  epoch  4, batch     4 | loss: 2.6378255Losses:  2.8324692249298096 0.543529748916626 0.8525357842445374
MemoryTrain:  epoch  4, batch     5 | loss: 2.8324692Losses:  2.3967695236206055 -0.0 0.9508156180381775
MemoryTrain:  epoch  4, batch     6 | loss: 2.3967695Losses:  2.5523180961608887 0.26262807846069336 0.9768693447113037
MemoryTrain:  epoch  4, batch     7 | loss: 2.5523181Losses:  2.9773755073547363 0.5289543867111206 0.9127764701843262
MemoryTrain:  epoch  4, batch     8 | loss: 2.9773755Losses:  2.745161533355713 0.23723016679286957 1.010877013206482
MemoryTrain:  epoch  4, batch     9 | loss: 2.7451615Losses:  2.633049964904785 0.2665490508079529 1.000745177268982
MemoryTrain:  epoch  4, batch    10 | loss: 2.6330500Losses:  2.4959678649902344 0.2321884036064148 0.7938002347946167
MemoryTrain:  epoch  4, batch    11 | loss: 2.4959679Losses:  2.5095930099487305 0.49305224418640137 0.7985350489616394
MemoryTrain:  epoch  4, batch    12 | loss: 2.5095930Losses:  2.6173481941223145 0.5010478496551514 0.8528186082839966
MemoryTrain:  epoch  4, batch    13 | loss: 2.6173482Losses:  2.691931962966919 0.5037440061569214 0.9266548156738281
MemoryTrain:  epoch  4, batch    14 | loss: 2.6919320Losses:  2.6282172203063965 0.27641457319259644 1.066977858543396
MemoryTrain:  epoch  5, batch     0 | loss: 2.6282172Losses:  2.714512586593628 0.5137108564376831 0.8841284513473511
MemoryTrain:  epoch  5, batch     1 | loss: 2.7145126Losses:  2.7324023246765137 0.5079922080039978 0.8611793518066406
MemoryTrain:  epoch  5, batch     2 | loss: 2.7324023Losses:  2.2971363067626953 -0.0 1.0127363204956055
MemoryTrain:  epoch  5, batch     3 | loss: 2.2971363Losses:  2.7345523834228516 -0.0 0.9045299291610718
MemoryTrain:  epoch  5, batch     4 | loss: 2.7345524Losses:  3.1507787704467773 0.5067698955535889 0.8549290895462036
MemoryTrain:  epoch  5, batch     5 | loss: 3.1507788Losses:  2.4274017810821533 0.2725438177585602 0.904953122138977
MemoryTrain:  epoch  5, batch     6 | loss: 2.4274018Losses:  2.538055658340454 0.24967290461063385 0.9917795062065125
MemoryTrain:  epoch  5, batch     7 | loss: 2.5380557Losses:  2.648956775665283 0.48910999298095703 0.8350327610969543
MemoryTrain:  epoch  5, batch     8 | loss: 2.6489568Losses:  2.8238368034362793 0.5077883005142212 0.9515165090560913
MemoryTrain:  epoch  5, batch     9 | loss: 2.8238368Losses:  2.783543825149536 0.5305059552192688 0.9243054389953613
MemoryTrain:  epoch  5, batch    10 | loss: 2.7835438Losses:  2.682844400405884 0.2553519606590271 0.8568921685218811
MemoryTrain:  epoch  5, batch    11 | loss: 2.6828444Losses:  2.26483154296875 -0.0 1.0010758638381958
MemoryTrain:  epoch  5, batch    12 | loss: 2.2648315Losses:  2.490962266921997 0.24804463982582092 0.9873562455177307
MemoryTrain:  epoch  5, batch    13 | loss: 2.4909623Losses:  2.82765531539917 0.25552427768707275 0.9790136814117432
MemoryTrain:  epoch  5, batch    14 | loss: 2.8276553Losses:  2.423433303833008 -0.0 1.1064363718032837
MemoryTrain:  epoch  6, batch     0 | loss: 2.4234333Losses:  2.538524627685547 0.2930777072906494 0.9490556120872498
MemoryTrain:  epoch  6, batch     1 | loss: 2.5385246Losses:  2.6828763484954834 0.5080243945121765 0.914886474609375
MemoryTrain:  epoch  6, batch     2 | loss: 2.6828763Losses:  2.53297758102417 0.24791869521141052 0.8888575434684753
MemoryTrain:  epoch  6, batch     3 | loss: 2.5329776Losses:  2.5393052101135254 0.26242706179618835 0.8645116090774536
MemoryTrain:  epoch  6, batch     4 | loss: 2.5393052Losses:  2.5739758014678955 0.47728171944618225 0.8351973295211792
MemoryTrain:  epoch  6, batch     5 | loss: 2.5739758Losses:  2.2037060260772705 -0.0 0.9097375869750977
MemoryTrain:  epoch  6, batch     6 | loss: 2.2037060Losses:  2.6963322162628174 0.24839454889297485 0.9046444296836853
MemoryTrain:  epoch  6, batch     7 | loss: 2.6963322Losses:  2.865806818008423 0.7357648611068726 0.9068242907524109
MemoryTrain:  epoch  6, batch     8 | loss: 2.8658068Losses:  2.565229892730713 0.514681339263916 0.787859320640564
MemoryTrain:  epoch  6, batch     9 | loss: 2.5652299Losses:  2.5852952003479004 -0.0 1.021413803100586
MemoryTrain:  epoch  6, batch    10 | loss: 2.5852952Losses:  2.175589084625244 -0.0 0.9595648050308228
MemoryTrain:  epoch  6, batch    11 | loss: 2.1755891Losses:  2.427363872528076 0.25425010919570923 0.9081635475158691
MemoryTrain:  epoch  6, batch    12 | loss: 2.4273639Losses:  2.155863046646118 -0.0 0.8995199203491211
MemoryTrain:  epoch  6, batch    13 | loss: 2.1558630Losses:  2.6530778408050537 0.2749943733215332 0.9722530245780945
MemoryTrain:  epoch  6, batch    14 | loss: 2.6530778Losses:  2.8408782482147217 0.769432008266449 0.8513383865356445
MemoryTrain:  epoch  7, batch     0 | loss: 2.8408782Losses:  2.0367274284362793 -0.0 0.8512471318244934
MemoryTrain:  epoch  7, batch     1 | loss: 2.0367274Losses:  2.255859375 -0.0 1.0103189945220947
MemoryTrain:  epoch  7, batch     2 | loss: 2.2558594Losses:  2.33162260055542 -0.0 0.963880181312561
MemoryTrain:  epoch  7, batch     3 | loss: 2.3316226Losses:  2.464982748031616 0.24192163348197937 0.9879844784736633
MemoryTrain:  epoch  7, batch     4 | loss: 2.4649827Losses:  2.458756685256958 0.2514217793941498 0.9264981150627136
MemoryTrain:  epoch  7, batch     5 | loss: 2.4587567Losses:  2.377448081970215 0.2414640486240387 0.9074351191520691
MemoryTrain:  epoch  7, batch     6 | loss: 2.3774481Losses:  2.3963499069213867 -0.0 1.0184487104415894
MemoryTrain:  epoch  7, batch     7 | loss: 2.3963499Losses:  2.341616630554199 -0.0 1.1050150394439697
MemoryTrain:  epoch  7, batch     8 | loss: 2.3416166Losses:  2.702589511871338 0.5272371768951416 0.8534849882125854
MemoryTrain:  epoch  7, batch     9 | loss: 2.7025895Losses:  2.296424388885498 0.24416109919548035 0.8262648582458496
MemoryTrain:  epoch  7, batch    10 | loss: 2.2964244Losses:  2.7935280799865723 0.494922935962677 0.9538640379905701
MemoryTrain:  epoch  7, batch    11 | loss: 2.7935281Losses:  2.603957176208496 0.48573678731918335 0.8937019109725952
MemoryTrain:  epoch  7, batch    12 | loss: 2.6039572Losses:  2.369931697845459 0.23904797434806824 0.9051820635795593
MemoryTrain:  epoch  7, batch    13 | loss: 2.3699317Losses:  2.191028356552124 -0.0 0.9621202349662781
MemoryTrain:  epoch  7, batch    14 | loss: 2.1910284Losses:  2.478123664855957 0.23377420008182526 1.0138088464736938
MemoryTrain:  epoch  8, batch     0 | loss: 2.4781237Losses:  2.9413223266601562 0.7372666597366333 0.9619090557098389
MemoryTrain:  epoch  8, batch     1 | loss: 2.9413223Losses:  2.4084556102752686 0.26179957389831543 0.9271647334098816
MemoryTrain:  epoch  8, batch     2 | loss: 2.4084556Losses:  2.4146974086761475 0.22900667786598206 0.8803662061691284
MemoryTrain:  epoch  8, batch     3 | loss: 2.4146974Losses:  2.6315064430236816 0.5205391049385071 0.8578536510467529
MemoryTrain:  epoch  8, batch     4 | loss: 2.6315064Losses:  2.48710036277771 0.4913244843482971 0.7823421955108643
MemoryTrain:  epoch  8, batch     5 | loss: 2.4871004Losses:  2.5769705772399902 0.4837794899940491 0.8459112048149109
MemoryTrain:  epoch  8, batch     6 | loss: 2.5769706Losses:  2.087463855743408 -0.0 0.8651400804519653
MemoryTrain:  epoch  8, batch     7 | loss: 2.0874639Losses:  2.419217586517334 -0.0 1.1109620332717896
MemoryTrain:  epoch  8, batch     8 | loss: 2.4192176Losses:  2.741128921508789 0.7354688048362732 0.7886320352554321
MemoryTrain:  epoch  8, batch     9 | loss: 2.7411289Losses:  2.5903210639953613 0.2782551646232605 0.958828330039978
MemoryTrain:  epoch  8, batch    10 | loss: 2.5903211Losses:  2.283102035522461 -0.0 1.0545778274536133
MemoryTrain:  epoch  8, batch    11 | loss: 2.2831020Losses:  2.320204734802246 -0.0 1.0654985904693604
MemoryTrain:  epoch  8, batch    12 | loss: 2.3202047Losses:  2.264012098312378 -0.0 1.0742294788360596
MemoryTrain:  epoch  8, batch    13 | loss: 2.2640121Losses:  2.457315683364868 0.2582278847694397 0.9049012064933777
MemoryTrain:  epoch  8, batch    14 | loss: 2.4573157Losses:  2.3048551082611084 -0.0 1.0008814334869385
MemoryTrain:  epoch  9, batch     0 | loss: 2.3048551Losses:  2.2042956352233887 -0.0 0.9531440138816833
MemoryTrain:  epoch  9, batch     1 | loss: 2.2042956Losses:  2.5935287475585938 0.24612799286842346 0.9717648029327393
MemoryTrain:  epoch  9, batch     2 | loss: 2.5935287Losses:  2.715430974960327 0.5093737244606018 0.9639198184013367
MemoryTrain:  epoch  9, batch     3 | loss: 2.7154310Losses:  2.4653990268707275 0.2512449026107788 0.9728261828422546
MemoryTrain:  epoch  9, batch     4 | loss: 2.4653990Losses:  2.8767971992492676 0.7527458667755127 0.9017571210861206
MemoryTrain:  epoch  9, batch     5 | loss: 2.8767972Losses:  2.563749074935913 0.24761319160461426 1.0201194286346436
MemoryTrain:  epoch  9, batch     6 | loss: 2.5637491Losses:  2.1918797492980957 -0.0 0.9480414986610413
MemoryTrain:  epoch  9, batch     7 | loss: 2.1918797Losses:  2.1591100692749023 -0.0 0.9028816223144531
MemoryTrain:  epoch  9, batch     8 | loss: 2.1591101Losses:  2.3802433013916016 0.2452736794948578 0.9533801674842834
MemoryTrain:  epoch  9, batch     9 | loss: 2.3802433Losses:  2.339813470840454 0.24076709151268005 0.9074578285217285
MemoryTrain:  epoch  9, batch    10 | loss: 2.3398135Losses:  2.135185956954956 -0.0 0.9032294154167175
MemoryTrain:  epoch  9, batch    11 | loss: 2.1351860Losses:  2.228214979171753 0.23039129376411438 0.7957659959793091
MemoryTrain:  epoch  9, batch    12 | loss: 2.2282150Losses:  2.515310764312744 0.2661028504371643 0.9837498664855957
MemoryTrain:  epoch  9, batch    13 | loss: 2.5153108Losses:  2.1793570518493652 -0.0 0.9173893332481384
MemoryTrain:  epoch  9, batch    14 | loss: 2.1793571
[EVAL] batch:    0 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    1 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    2 | acc: 93.75%,  total acc: 93.75%   [EVAL] batch:    3 | acc: 87.50%,  total acc: 92.19%   [EVAL] batch:    4 | acc: 81.25%,  total acc: 90.00%   [EVAL] batch:    5 | acc: 100.00%,  total acc: 91.67%   [EVAL] batch:    6 | acc: 75.00%,  total acc: 89.29%   [EVAL] batch:    7 | acc: 93.75%,  total acc: 89.84%   [EVAL] batch:    8 | acc: 87.50%,  total acc: 89.58%   [EVAL] batch:    9 | acc: 93.75%,  total acc: 90.00%   [EVAL] batch:   10 | acc: 100.00%,  total acc: 90.91%   [EVAL] batch:   11 | acc: 75.00%,  total acc: 89.58%   [EVAL] batch:   12 | acc: 93.75%,  total acc: 89.90%   [EVAL] batch:   13 | acc: 81.25%,  total acc: 89.29%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 89.58%   [EVAL] batch:   15 | acc: 68.75%,  total acc: 88.28%   [EVAL] batch:   16 | acc: 75.00%,  total acc: 87.50%   [EVAL] batch:   17 | acc: 68.75%,  total acc: 86.46%   [EVAL] batch:   18 | acc: 75.00%,  total acc: 85.86%   [EVAL] batch:   19 | acc: 56.25%,  total acc: 84.38%   [EVAL] batch:   20 | acc: 50.00%,  total acc: 82.74%   [EVAL] batch:   21 | acc: 68.75%,  total acc: 82.10%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 81.52%   [EVAL] batch:   23 | acc: 68.75%,  total acc: 80.99%   [EVAL] batch:   24 | acc: 43.75%,  total acc: 79.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 80.29%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 81.02%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 81.70%   [EVAL] batch:   28 | acc: 100.00%,  total acc: 82.33%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 82.92%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 83.47%   [EVAL] batch:   31 | acc: 75.00%,  total acc: 83.20%   [EVAL] batch:   32 | acc: 81.25%,  total acc: 83.14%   [EVAL] batch:   33 | acc: 62.50%,  total acc: 82.54%   [EVAL] batch:   34 | acc: 75.00%,  total acc: 82.32%   [EVAL] batch:   35 | acc: 68.75%,  total acc: 81.94%   [EVAL] batch:   36 | acc: 62.50%,  total acc: 81.42%   [EVAL] batch:   37 | acc: 68.75%,  total acc: 81.09%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.57%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.03%   [EVAL] batch:   40 | acc: 100.00%,  total acc: 82.47%   [EVAL] batch:   41 | acc: 100.00%,  total acc: 82.89%   [EVAL] batch:   42 | acc: 100.00%,  total acc: 83.28%   [EVAL] batch:   43 | acc: 93.75%,  total acc: 83.52%   [EVAL] batch:   44 | acc: 100.00%,  total acc: 83.89%   [EVAL] batch:   45 | acc: 100.00%,  total acc: 84.24%   [EVAL] batch:   46 | acc: 100.00%,  total acc: 84.57%   [EVAL] batch:   47 | acc: 100.00%,  total acc: 84.90%   [EVAL] batch:   48 | acc: 100.00%,  total acc: 85.20%   [EVAL] batch:   49 | acc: 100.00%,  total acc: 85.50%   [EVAL] batch:   50 | acc: 87.50%,  total acc: 85.54%   [EVAL] batch:   51 | acc: 87.50%,  total acc: 85.58%   [EVAL] batch:   52 | acc: 93.75%,  total acc: 85.73%   [EVAL] batch:   53 | acc: 93.75%,  total acc: 85.88%   [EVAL] batch:   54 | acc: 81.25%,  total acc: 85.80%   [EVAL] batch:   55 | acc: 93.75%,  total acc: 85.94%   [EVAL] batch:   56 | acc: 87.50%,  total acc: 85.96%   [EVAL] batch:   57 | acc: 93.75%,  total acc: 86.10%   [EVAL] batch:   58 | acc: 87.50%,  total acc: 86.12%   [EVAL] batch:   59 | acc: 100.00%,  total acc: 86.35%   [EVAL] batch:   60 | acc: 100.00%,  total acc: 86.58%   [EVAL] batch:   61 | acc: 87.50%,  total acc: 86.59%   [EVAL] batch:   62 | acc: 50.00%,  total acc: 86.01%   
[EVAL] batch:    0 | acc: 68.75%,  total acc: 68.75%   [EVAL] batch:    1 | acc: 87.50%,  total acc: 78.12%   [EVAL] batch:    2 | acc: 75.00%,  total acc: 77.08%   [EVAL] batch:    3 | acc: 75.00%,  total acc: 76.56%   [EVAL] batch:    4 | acc: 62.50%,  total acc: 73.75%   [EVAL] batch:    5 | acc: 87.50%,  total acc: 76.04%   [EVAL] batch:    6 | acc: 31.25%,  total acc: 69.64%   [EVAL] batch:    7 | acc: 43.75%,  total acc: 66.41%   [EVAL] batch:    8 | acc: 62.50%,  total acc: 65.97%   [EVAL] batch:    9 | acc: 50.00%,  total acc: 64.38%   [EVAL] batch:   10 | acc: 37.50%,  total acc: 61.93%   [EVAL] batch:   11 | acc: 43.75%,  total acc: 60.42%   [EVAL] batch:   12 | acc: 68.75%,  total acc: 61.06%   [EVAL] batch:   13 | acc: 93.75%,  total acc: 63.39%   [EVAL] batch:   14 | acc: 93.75%,  total acc: 65.42%   [EVAL] batch:   15 | acc: 100.00%,  total acc: 67.58%   [EVAL] batch:   16 | acc: 100.00%,  total acc: 69.49%   [EVAL] batch:   17 | acc: 93.75%,  total acc: 70.83%   [EVAL] batch:   18 | acc: 100.00%,  total acc: 72.37%   [EVAL] batch:   19 | acc: 75.00%,  total acc: 72.50%   [EVAL] batch:   20 | acc: 62.50%,  total acc: 72.02%   [EVAL] batch:   21 | acc: 75.00%,  total acc: 72.16%   [EVAL] batch:   22 | acc: 68.75%,  total acc: 72.01%   [EVAL] batch:   23 | acc: 87.50%,  total acc: 72.66%   [EVAL] batch:   24 | acc: 68.75%,  total acc: 72.50%   [EVAL] batch:   25 | acc: 100.00%,  total acc: 73.56%   [EVAL] batch:   26 | acc: 100.00%,  total acc: 74.54%   [EVAL] batch:   27 | acc: 100.00%,  total acc: 75.45%   [EVAL] batch:   28 | acc: 93.75%,  total acc: 76.08%   [EVAL] batch:   29 | acc: 100.00%,  total acc: 76.88%   [EVAL] batch:   30 | acc: 100.00%,  total acc: 77.62%   [EVAL] batch:   31 | acc: 100.00%,  total acc: 78.32%   [EVAL] batch:   32 | acc: 100.00%,  total acc: 78.98%   [EVAL] batch:   33 | acc: 100.00%,  total acc: 79.60%   [EVAL] batch:   34 | acc: 93.75%,  total acc: 80.00%   [EVAL] batch:   35 | acc: 93.75%,  total acc: 80.38%   [EVAL] batch:   36 | acc: 100.00%,  total acc: 80.91%   [EVAL] batch:   37 | acc: 100.00%,  total acc: 81.41%   [EVAL] batch:   38 | acc: 100.00%,  total acc: 81.89%   [EVAL] batch:   39 | acc: 100.00%,  total acc: 82.34%   [EVAL] batch:   40 | acc: 93.75%,  total acc: 82.62%   [EVAL] batch:   41 | acc: 87.50%,  total acc: 82.74%   [EVAL] batch:   42 | acc: 87.50%,  total acc: 82.85%   [EVAL] batch:   43 | acc: 87.50%,  total acc: 82.95%   [EVAL] batch:   44 | acc: 68.75%,  total acc: 82.64%   [EVAL] batch:   45 | acc: 75.00%,  total acc: 82.47%   [EVAL] batch:   46 | acc: 43.75%,  total acc: 81.65%   [EVAL] batch:   47 | acc: 75.00%,  total acc: 81.51%   [EVAL] batch:   48 | acc: 87.50%,  total acc: 81.63%   [EVAL] batch:   49 | acc: 75.00%,  total acc: 81.50%   [EVAL] batch:   50 | acc: 75.00%,  total acc: 81.37%   [EVAL] batch:   51 | acc: 75.00%,  total acc: 81.25%   [EVAL] batch:   52 | acc: 75.00%,  total acc: 81.13%   [EVAL] batch:   53 | acc: 56.25%,  total acc: 80.67%   [EVAL] batch:   54 | acc: 50.00%,  total acc: 80.11%   [EVAL] batch:   55 | acc: 75.00%,  total acc: 80.02%   [EVAL] batch:   56 | acc: 75.00%,  total acc: 79.93%   [EVAL] batch:   57 | acc: 50.00%,  total acc: 79.42%   [EVAL] batch:   58 | acc: 75.00%,  total acc: 79.34%   [EVAL] batch:   59 | acc: 81.25%,  total acc: 79.38%   [EVAL] batch:   60 | acc: 81.25%,  total acc: 79.41%   [EVAL] batch:   61 | acc: 68.75%,  total acc: 79.23%   [EVAL] batch:   62 | acc: 37.50%,  total acc: 78.57%   [EVAL] batch:   63 | acc: 6.25%,  total acc: 77.44%   [EVAL] batch:   64 | acc: 6.25%,  total acc: 76.35%   [EVAL] batch:   65 | acc: 12.50%,  total acc: 75.38%   [EVAL] batch:   66 | acc: 12.50%,  total acc: 74.44%   [EVAL] batch:   67 | acc: 18.75%,  total acc: 73.62%   [EVAL] batch:   68 | acc: 31.25%,  total acc: 73.01%   [EVAL] batch:   69 | acc: 75.00%,  total acc: 73.04%   [EVAL] batch:   70 | acc: 68.75%,  total acc: 72.98%   [EVAL] batch:   71 | acc: 56.25%,  total acc: 72.74%   [EVAL] batch:   72 | acc: 62.50%,  total acc: 72.60%   [EVAL] batch:   73 | acc: 43.75%,  total acc: 72.21%   [EVAL] batch:   74 | acc: 81.25%,  total acc: 72.33%   [EVAL] batch:   75 | acc: 81.25%,  total acc: 72.45%   [EVAL] batch:   76 | acc: 75.00%,  total acc: 72.48%   [EVAL] batch:   77 | acc: 75.00%,  total acc: 72.52%   [EVAL] batch:   78 | acc: 93.75%,  total acc: 72.78%   [EVAL] batch:   79 | acc: 87.50%,  total acc: 72.97%   [EVAL] batch:   80 | acc: 100.00%,  total acc: 73.30%   [EVAL] batch:   81 | acc: 25.00%,  total acc: 72.71%   [EVAL] batch:   82 | acc: 12.50%,  total acc: 71.99%   [EVAL] batch:   83 | acc: 12.50%,  total acc: 71.28%   [EVAL] batch:   84 | acc: 0.00%,  total acc: 70.44%   [EVAL] batch:   85 | acc: 6.25%,  total acc: 69.69%   [EVAL] batch:   86 | acc: 31.25%,  total acc: 69.25%   [EVAL] batch:   87 | acc: 37.50%,  total acc: 68.89%   [EVAL] batch:   88 | acc: 93.75%,  total acc: 69.17%   [EVAL] batch:   89 | acc: 100.00%,  total acc: 69.51%   [EVAL] batch:   90 | acc: 81.25%,  total acc: 69.64%   [EVAL] batch:   91 | acc: 100.00%,  total acc: 69.97%   [EVAL] batch:   92 | acc: 87.50%,  total acc: 70.16%   [EVAL] batch:   93 | acc: 93.75%,  total acc: 70.41%   [EVAL] batch:   94 | acc: 100.00%,  total acc: 70.72%   [EVAL] batch:   95 | acc: 100.00%,  total acc: 71.03%   [EVAL] batch:   96 | acc: 100.00%,  total acc: 71.33%   [EVAL] batch:   97 | acc: 100.00%,  total acc: 71.62%   [EVAL] batch:   98 | acc: 100.00%,  total acc: 71.91%   [EVAL] batch:   99 | acc: 100.00%,  total acc: 72.19%   [EVAL] batch:  100 | acc: 68.75%,  total acc: 72.15%   [EVAL] batch:  101 | acc: 68.75%,  total acc: 72.12%   [EVAL] batch:  102 | acc: 75.00%,  total acc: 72.15%   [EVAL] batch:  103 | acc: 62.50%,  total acc: 72.06%   [EVAL] batch:  104 | acc: 87.50%,  total acc: 72.20%   [EVAL] batch:  105 | acc: 62.50%,  total acc: 72.11%   [EVAL] batch:  106 | acc: 43.75%,  total acc: 71.85%   [EVAL] batch:  107 | acc: 18.75%,  total acc: 71.35%   [EVAL] batch:  108 | acc: 37.50%,  total acc: 71.04%   [EVAL] batch:  109 | acc: 43.75%,  total acc: 70.80%   [EVAL] batch:  110 | acc: 50.00%,  total acc: 70.61%   [EVAL] batch:  111 | acc: 31.25%,  total acc: 70.26%   [EVAL] batch:  112 | acc: 68.75%,  total acc: 70.24%   [EVAL] batch:  113 | acc: 81.25%,  total acc: 70.34%   [EVAL] batch:  114 | acc: 100.00%,  total acc: 70.60%   [EVAL] batch:  115 | acc: 75.00%,  total acc: 70.64%   [EVAL] batch:  116 | acc: 87.50%,  total acc: 70.78%   [EVAL] batch:  117 | acc: 87.50%,  total acc: 70.92%   [EVAL] batch:  118 | acc: 81.25%,  total acc: 71.01%   [EVAL] batch:  119 | acc: 18.75%,  total acc: 70.57%   [EVAL] batch:  120 | acc: 18.75%,  total acc: 70.14%   [EVAL] batch:  121 | acc: 6.25%,  total acc: 69.62%   [EVAL] batch:  122 | acc: 31.25%,  total acc: 69.31%   [EVAL] batch:  123 | acc: 18.75%,  total acc: 68.90%   [EVAL] batch:  124 | acc: 25.00%,  total acc: 68.55%   [EVAL] batch:  125 | acc: 56.25%,  total acc: 68.45%   [EVAL] batch:  126 | acc: 50.00%,  total acc: 68.31%   [EVAL] batch:  127 | acc: 37.50%,  total acc: 68.07%   [EVAL] batch:  128 | acc: 68.75%,  total acc: 68.07%   [EVAL] batch:  129 | acc: 68.75%,  total acc: 68.08%   [EVAL] batch:  130 | acc: 50.00%,  total acc: 67.94%   [EVAL] batch:  131 | acc: 93.75%,  total acc: 68.13%   [EVAL] batch:  132 | acc: 87.50%,  total acc: 68.28%   [EVAL] batch:  133 | acc: 87.50%,  total acc: 68.42%   [EVAL] batch:  134 | acc: 75.00%,  total acc: 68.47%   [EVAL] batch:  135 | acc: 93.75%,  total acc: 68.66%   [EVAL] batch:  136 | acc: 93.75%,  total acc: 68.84%   [EVAL] batch:  137 | acc: 50.00%,  total acc: 68.70%   [EVAL] batch:  138 | acc: 12.50%,  total acc: 68.30%   [EVAL] batch:  139 | acc: 25.00%,  total acc: 67.99%   [EVAL] batch:  140 | acc: 12.50%,  total acc: 67.60%   [EVAL] batch:  141 | acc: 18.75%,  total acc: 67.25%   [EVAL] batch:  142 | acc: 12.50%,  total acc: 66.87%   [EVAL] batch:  143 | acc: 31.25%,  total acc: 66.62%   [EVAL] batch:  144 | acc: 93.75%,  total acc: 66.81%   [EVAL] batch:  145 | acc: 100.00%,  total acc: 67.04%   [EVAL] batch:  146 | acc: 100.00%,  total acc: 67.26%   [EVAL] batch:  147 | acc: 100.00%,  total acc: 67.48%   [EVAL] batch:  148 | acc: 100.00%,  total acc: 67.70%   [EVAL] batch:  149 | acc: 93.75%,  total acc: 67.88%   [EVAL] batch:  150 | acc: 12.50%,  total acc: 67.51%   [EVAL] batch:  151 | acc: 6.25%,  total acc: 67.11%   [EVAL] batch:  152 | acc: 25.00%,  total acc: 66.83%   [EVAL] batch:  153 | acc: 12.50%,  total acc: 66.48%   [EVAL] batch:  154 | acc: 12.50%,  total acc: 66.13%   [EVAL] batch:  155 | acc: 18.75%,  total acc: 65.83%   [EVAL] batch:  156 | acc: 81.25%,  total acc: 65.92%   [EVAL] batch:  157 | acc: 93.75%,  total acc: 66.10%   [EVAL] batch:  158 | acc: 93.75%,  total acc: 66.27%   [EVAL] batch:  159 | acc: 93.75%,  total acc: 66.45%   [EVAL] batch:  160 | acc: 87.50%,  total acc: 66.58%   [EVAL] batch:  161 | acc: 93.75%,  total acc: 66.74%   [EVAL] batch:  162 | acc: 62.50%,  total acc: 66.72%   [EVAL] batch:  163 | acc: 25.00%,  total acc: 66.46%   [EVAL] batch:  164 | acc: 31.25%,  total acc: 66.25%   [EVAL] batch:  165 | acc: 31.25%,  total acc: 66.04%   [EVAL] batch:  166 | acc: 25.00%,  total acc: 65.79%   [EVAL] batch:  167 | acc: 43.75%,  total acc: 65.66%   [EVAL] batch:  168 | acc: 31.25%,  total acc: 65.46%   [EVAL] batch:  169 | acc: 6.25%,  total acc: 65.11%   [EVAL] batch:  170 | acc: 6.25%,  total acc: 64.77%   [EVAL] batch:  171 | acc: 0.00%,  total acc: 64.39%   [EVAL] batch:  172 | acc: 12.50%,  total acc: 64.09%   [EVAL] batch:  173 | acc: 6.25%,  total acc: 63.76%   [EVAL] batch:  174 | acc: 6.25%,  total acc: 63.43%   [EVAL] batch:  175 | acc: 25.00%,  total acc: 63.21%   [EVAL] batch:  176 | acc: 25.00%,  total acc: 62.99%   [EVAL] batch:  177 | acc: 25.00%,  total acc: 62.78%   [EVAL] batch:  178 | acc: 37.50%,  total acc: 62.64%   [EVAL] batch:  179 | acc: 43.75%,  total acc: 62.53%   [EVAL] batch:  180 | acc: 37.50%,  total acc: 62.40%   [EVAL] batch:  181 | acc: 75.00%,  total acc: 62.47%   [EVAL] batch:  182 | acc: 87.50%,  total acc: 62.60%   [EVAL] batch:  183 | acc: 93.75%,  total acc: 62.77%   [EVAL] batch:  184 | acc: 81.25%,  total acc: 62.87%   [EVAL] batch:  185 | acc: 93.75%,  total acc: 63.04%   [EVAL] batch:  186 | acc: 68.75%,  total acc: 63.07%   [EVAL] batch:  187 | acc: 37.50%,  total acc: 62.93%   [EVAL] batch:  188 | acc: 6.25%,  total acc: 62.63%   [EVAL] batch:  189 | acc: 12.50%,  total acc: 62.37%   [EVAL] batch:  190 | acc: 0.00%,  total acc: 62.04%   [EVAL] batch:  191 | acc: 6.25%,  total acc: 61.75%   [EVAL] batch:  192 | acc: 6.25%,  total acc: 61.46%   [EVAL] batch:  193 | acc: 18.75%,  total acc: 61.24%   [EVAL] batch:  194 | acc: 56.25%,  total acc: 61.22%   [EVAL] batch:  195 | acc: 50.00%,  total acc: 61.16%   [EVAL] batch:  196 | acc: 75.00%,  total acc: 61.23%   [EVAL] batch:  197 | acc: 81.25%,  total acc: 61.33%   [EVAL] batch:  198 | acc: 68.75%,  total acc: 61.37%   [EVAL] batch:  199 | acc: 62.50%,  total acc: 61.38%   [EVAL] batch:  200 | acc: 56.25%,  total acc: 61.35%   [EVAL] batch:  201 | acc: 37.50%,  total acc: 61.23%   [EVAL] batch:  202 | acc: 43.75%,  total acc: 61.15%   [EVAL] batch:  203 | acc: 68.75%,  total acc: 61.18%   [EVAL] batch:  204 | acc: 43.75%,  total acc: 61.10%   [EVAL] batch:  205 | acc: 50.00%,  total acc: 61.04%   [EVAL] batch:  206 | acc: 93.75%,  total acc: 61.20%   [EVAL] batch:  207 | acc: 100.00%,  total acc: 61.39%   [EVAL] batch:  208 | acc: 100.00%,  total acc: 61.57%   [EVAL] batch:  209 | acc: 93.75%,  total acc: 61.73%   [EVAL] batch:  210 | acc: 93.75%,  total acc: 61.88%   [EVAL] batch:  211 | acc: 100.00%,  total acc: 62.06%   [EVAL] batch:  212 | acc: 93.75%,  total acc: 62.21%   [EVAL] batch:  213 | acc: 100.00%,  total acc: 62.38%   [EVAL] batch:  214 | acc: 100.00%,  total acc: 62.56%   [EVAL] batch:  215 | acc: 100.00%,  total acc: 62.73%   [EVAL] batch:  216 | acc: 100.00%,  total acc: 62.90%   [EVAL] batch:  217 | acc: 100.00%,  total acc: 63.07%   [EVAL] batch:  218 | acc: 100.00%,  total acc: 63.24%   [EVAL] batch:  219 | acc: 100.00%,  total acc: 63.41%   [EVAL] batch:  220 | acc: 100.00%,  total acc: 63.57%   [EVAL] batch:  221 | acc: 87.50%,  total acc: 63.68%   [EVAL] batch:  222 | acc: 93.75%,  total acc: 63.82%   [EVAL] batch:  223 | acc: 93.75%,  total acc: 63.95%   [EVAL] batch:  224 | acc: 100.00%,  total acc: 64.11%   [EVAL] batch:  225 | acc: 75.00%,  total acc: 64.16%   [EVAL] batch:  226 | acc: 68.75%,  total acc: 64.18%   [EVAL] batch:  227 | acc: 37.50%,  total acc: 64.06%   [EVAL] batch:  228 | acc: 68.75%,  total acc: 64.08%   [EVAL] batch:  229 | acc: 43.75%,  total acc: 63.99%   [EVAL] batch:  230 | acc: 62.50%,  total acc: 63.99%   [EVAL] batch:  231 | acc: 93.75%,  total acc: 64.12%   [EVAL] batch:  232 | acc: 93.75%,  total acc: 64.24%   [EVAL] batch:  233 | acc: 93.75%,  total acc: 64.37%   [EVAL] batch:  234 | acc: 87.50%,  total acc: 64.47%   [EVAL] batch:  235 | acc: 68.75%,  total acc: 64.49%   [EVAL] batch:  236 | acc: 93.75%,  total acc: 64.61%   [EVAL] batch:  237 | acc: 62.50%,  total acc: 64.60%   [EVAL] batch:  238 | acc: 68.75%,  total acc: 64.62%   [EVAL] batch:  239 | acc: 56.25%,  total acc: 64.58%   [EVAL] batch:  240 | acc: 62.50%,  total acc: 64.57%   [EVAL] batch:  241 | acc: 56.25%,  total acc: 64.54%   [EVAL] batch:  242 | acc: 50.00%,  total acc: 64.48%   [EVAL] batch:  243 | acc: 37.50%,  total acc: 64.37%   [EVAL] batch:  244 | acc: 75.00%,  total acc: 64.41%   [EVAL] batch:  245 | acc: 56.25%,  total acc: 64.38%   [EVAL] batch:  246 | acc: 62.50%,  total acc: 64.37%   [EVAL] batch:  247 | acc: 87.50%,  total acc: 64.47%   [EVAL] batch:  248 | acc: 87.50%,  total acc: 64.56%   [EVAL] batch:  249 | acc: 56.25%,  total acc: 64.53%   [EVAL] batch:  250 | acc: 0.00%,  total acc: 64.27%   [EVAL] batch:  251 | acc: 12.50%,  total acc: 64.06%   [EVAL] batch:  252 | acc: 0.00%,  total acc: 63.81%   [EVAL] batch:  253 | acc: 6.25%,  total acc: 63.58%   [EVAL] batch:  254 | acc: 0.00%,  total acc: 63.33%   [EVAL] batch:  255 | acc: 0.00%,  total acc: 63.09%   [EVAL] batch:  256 | acc: 62.50%,  total acc: 63.08%   [EVAL] batch:  257 | acc: 81.25%,  total acc: 63.15%   [EVAL] batch:  258 | acc: 100.00%,  total acc: 63.30%   [EVAL] batch:  259 | acc: 81.25%,  total acc: 63.37%   [EVAL] batch:  260 | acc: 81.25%,  total acc: 63.43%   [EVAL] batch:  261 | acc: 81.25%,  total acc: 63.50%   [EVAL] batch:  262 | acc: 50.00%,  total acc: 63.45%   [EVAL] batch:  263 | acc: 18.75%,  total acc: 63.28%   [EVAL] batch:  264 | acc: 37.50%,  total acc: 63.18%   [EVAL] batch:  265 | acc: 43.75%,  total acc: 63.11%   [EVAL] batch:  266 | acc: 12.50%,  total acc: 62.92%   [EVAL] batch:  267 | acc: 31.25%,  total acc: 62.80%   [EVAL] batch:  268 | acc: 6.25%,  total acc: 62.59%   [EVAL] batch:  269 | acc: 31.25%,  total acc: 62.48%   [EVAL] batch:  270 | acc: 18.75%,  total acc: 62.32%   [EVAL] batch:  271 | acc: 43.75%,  total acc: 62.25%   [EVAL] batch:  272 | acc: 37.50%,  total acc: 62.16%   [EVAL] batch:  273 | acc: 18.75%,  total acc: 62.00%   [EVAL] batch:  274 | acc: 43.75%,  total acc: 61.93%   [EVAL] batch:  275 | acc: 0.00%,  total acc: 61.71%   [EVAL] batch:  276 | acc: 6.25%,  total acc: 61.51%   [EVAL] batch:  277 | acc: 12.50%,  total acc: 61.33%   [EVAL] batch:  278 | acc: 6.25%,  total acc: 61.13%   [EVAL] batch:  279 | acc: 18.75%,  total acc: 60.98%   [EVAL] batch:  280 | acc: 0.00%,  total acc: 60.77%   [EVAL] batch:  281 | acc: 56.25%,  total acc: 60.75%   [EVAL] batch:  282 | acc: 87.50%,  total acc: 60.84%   [EVAL] batch:  283 | acc: 87.50%,  total acc: 60.94%   [EVAL] batch:  284 | acc: 93.75%,  total acc: 61.05%   [EVAL] batch:  285 | acc: 93.75%,  total acc: 61.17%   [EVAL] batch:  286 | acc: 68.75%,  total acc: 61.19%   [EVAL] batch:  287 | acc: 87.50%,  total acc: 61.28%   [EVAL] batch:  288 | acc: 93.75%,  total acc: 61.40%   [EVAL] batch:  289 | acc: 100.00%,  total acc: 61.53%   [EVAL] batch:  290 | acc: 100.00%,  total acc: 61.66%   [EVAL] batch:  291 | acc: 81.25%,  total acc: 61.73%   [EVAL] batch:  292 | acc: 100.00%,  total acc: 61.86%   [EVAL] batch:  293 | acc: 93.75%,  total acc: 61.97%   [EVAL] batch:  294 | acc: 37.50%,  total acc: 61.89%   [EVAL] batch:  295 | acc: 25.00%,  total acc: 61.76%   [EVAL] batch:  296 | acc: 12.50%,  total acc: 61.60%   [EVAL] batch:  297 | acc: 18.75%,  total acc: 61.45%   [EVAL] batch:  298 | acc: 56.25%,  total acc: 61.43%   [EVAL] batch:  299 | acc: 25.00%,  total acc: 61.31%   [EVAL] batch:  300 | acc: 12.50%,  total acc: 61.15%   [EVAL] batch:  301 | acc: 37.50%,  total acc: 61.07%   [EVAL] batch:  302 | acc: 37.50%,  total acc: 60.99%   [EVAL] batch:  303 | acc: 50.00%,  total acc: 60.96%   [EVAL] batch:  304 | acc: 43.75%,  total acc: 60.90%   [EVAL] batch:  305 | acc: 56.25%,  total acc: 60.89%   [EVAL] batch:  306 | acc: 37.50%,  total acc: 60.81%   [EVAL] batch:  307 | acc: 50.00%,  total acc: 60.78%   [EVAL] batch:  308 | acc: 37.50%,  total acc: 60.70%   [EVAL] batch:  309 | acc: 68.75%,  total acc: 60.73%   [EVAL] batch:  310 | acc: 43.75%,  total acc: 60.67%   [EVAL] batch:  311 | acc: 81.25%,  total acc: 60.74%   [EVAL] batch:  312 | acc: 43.75%,  total acc: 60.68%   [EVAL] batch:  313 | acc: 12.50%,  total acc: 60.53%   [EVAL] batch:  314 | acc: 31.25%,  total acc: 60.44%   [EVAL] batch:  315 | acc: 25.00%,  total acc: 60.32%   [EVAL] batch:  316 | acc: 31.25%,  total acc: 60.23%   [EVAL] batch:  317 | acc: 25.00%,  total acc: 60.12%   [EVAL] batch:  318 | acc: 37.50%,  total acc: 60.05%   [EVAL] batch:  319 | acc: 100.00%,  total acc: 60.18%   [EVAL] batch:  320 | acc: 100.00%,  total acc: 60.30%   [EVAL] batch:  321 | acc: 75.00%,  total acc: 60.35%   [EVAL] batch:  322 | acc: 87.50%,  total acc: 60.43%   [EVAL] batch:  323 | acc: 87.50%,  total acc: 60.51%   [EVAL] batch:  324 | acc: 81.25%,  total acc: 60.58%   [EVAL] batch:  325 | acc: 68.75%,  total acc: 60.60%   [EVAL] batch:  326 | acc: 62.50%,  total acc: 60.61%   [EVAL] batch:  327 | acc: 81.25%,  total acc: 60.67%   [EVAL] batch:  328 | acc: 75.00%,  total acc: 60.71%   [EVAL] batch:  329 | acc: 87.50%,  total acc: 60.80%   [EVAL] batch:  330 | acc: 75.00%,  total acc: 60.84%   [EVAL] batch:  331 | acc: 93.75%,  total acc: 60.94%   [EVAL] batch:  332 | acc: 68.75%,  total acc: 60.96%   [EVAL] batch:  333 | acc: 87.50%,  total acc: 61.04%   [EVAL] batch:  334 | acc: 75.00%,  total acc: 61.08%   [EVAL] batch:  335 | acc: 87.50%,  total acc: 61.16%   [EVAL] batch:  336 | acc: 62.50%,  total acc: 61.16%   [EVAL] batch:  337 | acc: 87.50%,  total acc: 61.24%   [EVAL] batch:  338 | acc: 75.00%,  total acc: 61.28%   [EVAL] batch:  339 | acc: 62.50%,  total acc: 61.29%   [EVAL] batch:  340 | acc: 87.50%,  total acc: 61.36%   [EVAL] batch:  341 | acc: 68.75%,  total acc: 61.39%   [EVAL] batch:  342 | acc: 68.75%,  total acc: 61.41%   [EVAL] batch:  343 | acc: 56.25%,  total acc: 61.39%   [EVAL] batch:  344 | acc: 68.75%,  total acc: 61.41%   [EVAL] batch:  345 | acc: 43.75%,  total acc: 61.36%   [EVAL] batch:  346 | acc: 62.50%,  total acc: 61.37%   [EVAL] batch:  347 | acc: 56.25%,  total acc: 61.35%   [EVAL] batch:  348 | acc: 62.50%,  total acc: 61.35%   [EVAL] batch:  349 | acc: 50.00%,  total acc: 61.32%   [EVAL] batch:  350 | acc: 62.50%,  total acc: 61.32%   [EVAL] batch:  351 | acc: 68.75%,  total acc: 61.35%   [EVAL] batch:  352 | acc: 68.75%,  total acc: 61.37%   [EVAL] batch:  353 | acc: 81.25%,  total acc: 61.42%   [EVAL] batch:  354 | acc: 75.00%,  total acc: 61.46%   [EVAL] batch:  355 | acc: 62.50%,  total acc: 61.46%   [EVAL] batch:  356 | acc: 50.00%,  total acc: 61.43%   [EVAL] batch:  357 | acc: 18.75%,  total acc: 61.31%   [EVAL] batch:  358 | acc: 18.75%,  total acc: 61.19%   [EVAL] batch:  359 | acc: 43.75%,  total acc: 61.15%   [EVAL] batch:  360 | acc: 18.75%,  total acc: 61.03%   [EVAL] batch:  361 | acc: 50.00%,  total acc: 61.00%   [EVAL] batch:  362 | acc: 56.25%,  total acc: 60.98%   [EVAL] batch:  363 | acc: 100.00%,  total acc: 61.09%   [EVAL] batch:  364 | acc: 100.00%,  total acc: 61.20%   [EVAL] batch:  365 | acc: 100.00%,  total acc: 61.30%   [EVAL] batch:  366 | acc: 100.00%,  total acc: 61.41%   [EVAL] batch:  367 | acc: 100.00%,  total acc: 61.51%   [EVAL] batch:  368 | acc: 93.75%,  total acc: 61.60%   [EVAL] batch:  369 | acc: 87.50%,  total acc: 61.67%   [EVAL] batch:  370 | acc: 75.00%,  total acc: 61.71%   [EVAL] batch:  371 | acc: 81.25%,  total acc: 61.76%   [EVAL] batch:  372 | acc: 93.75%,  total acc: 61.85%   [EVAL] batch:  373 | acc: 81.25%,  total acc: 61.90%   [EVAL] batch:  374 | acc: 81.25%,  total acc: 61.95%   [EVAL] batch:  375 | acc: 31.25%,  total acc: 61.87%   [EVAL] batch:  376 | acc: 43.75%,  total acc: 61.82%   [EVAL] batch:  377 | acc: 43.75%,  total acc: 61.77%   [EVAL] batch:  378 | acc: 50.00%,  total acc: 61.74%   [EVAL] batch:  379 | acc: 43.75%,  total acc: 61.69%   [EVAL] batch:  380 | acc: 50.00%,  total acc: 61.66%   [EVAL] batch:  381 | acc: 56.25%,  total acc: 61.65%   [EVAL] batch:  382 | acc: 93.75%,  total acc: 61.73%   [EVAL] batch:  383 | acc: 68.75%,  total acc: 61.75%   [EVAL] batch:  384 | acc: 68.75%,  total acc: 61.77%   [EVAL] batch:  385 | acc: 43.75%,  total acc: 61.72%   [EVAL] batch:  386 | acc: 62.50%,  total acc: 61.72%   [EVAL] batch:  387 | acc: 56.25%,  total acc: 61.71%   [EVAL] batch:  388 | acc: 43.75%,  total acc: 61.66%   [EVAL] batch:  389 | acc: 43.75%,  total acc: 61.62%   [EVAL] batch:  390 | acc: 31.25%,  total acc: 61.54%   [EVAL] batch:  391 | acc: 50.00%,  total acc: 61.51%   [EVAL] batch:  392 | acc: 50.00%,  total acc: 61.48%   [EVAL] batch:  393 | acc: 81.25%,  total acc: 61.53%   [EVAL] batch:  394 | acc: 87.50%,  total acc: 61.60%   [EVAL] batch:  395 | acc: 100.00%,  total acc: 61.70%   [EVAL] batch:  396 | acc: 100.00%,  total acc: 61.79%   [EVAL] batch:  397 | acc: 100.00%,  total acc: 61.89%   [EVAL] batch:  398 | acc: 93.75%,  total acc: 61.97%   [EVAL] batch:  399 | acc: 93.75%,  total acc: 62.05%   [EVAL] batch:  400 | acc: 43.75%,  total acc: 62.00%   [EVAL] batch:  401 | acc: 25.00%,  total acc: 61.91%   [EVAL] batch:  402 | acc: 31.25%,  total acc: 61.83%   [EVAL] batch:  403 | acc: 37.50%,  total acc: 61.77%   [EVAL] batch:  404 | acc: 25.00%,  total acc: 61.68%   [EVAL] batch:  405 | acc: 50.00%,  total acc: 61.65%   [EVAL] batch:  406 | acc: 50.00%,  total acc: 61.62%   [EVAL] batch:  407 | acc: 50.00%,  total acc: 61.60%   [EVAL] batch:  408 | acc: 31.25%,  total acc: 61.52%   [EVAL] batch:  409 | acc: 43.75%,  total acc: 61.48%   [EVAL] batch:  410 | acc: 18.75%,  total acc: 61.37%   [EVAL] batch:  411 | acc: 18.75%,  total acc: 61.27%   [EVAL] batch:  412 | acc: 50.00%,  total acc: 61.24%   [EVAL] batch:  413 | acc: 37.50%,  total acc: 61.19%   [EVAL] batch:  414 | acc: 81.25%,  total acc: 61.23%   [EVAL] batch:  415 | acc: 68.75%,  total acc: 61.25%   [EVAL] batch:  416 | acc: 81.25%,  total acc: 61.30%   [EVAL] batch:  417 | acc: 75.00%,  total acc: 61.33%   [EVAL] batch:  418 | acc: 56.25%,  total acc: 61.32%   [EVAL] batch:  419 | acc: 62.50%,  total acc: 61.32%   [EVAL] batch:  420 | acc: 62.50%,  total acc: 61.33%   [EVAL] batch:  421 | acc: 56.25%,  total acc: 61.32%   [EVAL] batch:  422 | acc: 56.25%,  total acc: 61.30%   [EVAL] batch:  423 | acc: 50.00%,  total acc: 61.28%   [EVAL] batch:  424 | acc: 75.00%,  total acc: 61.31%   [EVAL] batch:  425 | acc: 100.00%,  total acc: 61.40%   [EVAL] batch:  426 | acc: 100.00%,  total acc: 61.49%   [EVAL] batch:  427 | acc: 100.00%,  total acc: 61.58%   [EVAL] batch:  428 | acc: 100.00%,  total acc: 61.67%   [EVAL] batch:  429 | acc: 100.00%,  total acc: 61.76%   [EVAL] batch:  430 | acc: 100.00%,  total acc: 61.85%   [EVAL] batch:  431 | acc: 100.00%,  total acc: 61.94%   [EVAL] batch:  432 | acc: 87.50%,  total acc: 61.99%   [EVAL] batch:  433 | acc: 93.75%,  total acc: 62.07%   [EVAL] batch:  434 | acc: 100.00%,  total acc: 62.16%   [EVAL] batch:  435 | acc: 93.75%,  total acc: 62.23%   [EVAL] batch:  436 | acc: 100.00%,  total acc: 62.31%   [EVAL] batch:  437 | acc: 93.75%,  total acc: 62.39%   [EVAL] batch:  438 | acc: 93.75%,  total acc: 62.46%   [EVAL] batch:  439 | acc: 93.75%,  total acc: 62.53%   [EVAL] batch:  440 | acc: 100.00%,  total acc: 62.61%   [EVAL] batch:  441 | acc: 75.00%,  total acc: 62.64%   [EVAL] batch:  442 | acc: 93.75%,  total acc: 62.71%   [EVAL] batch:  443 | acc: 93.75%,  total acc: 62.78%   [EVAL] batch:  444 | acc: 75.00%,  total acc: 62.81%   [EVAL] batch:  445 | acc: 93.75%,  total acc: 62.88%   [EVAL] batch:  446 | acc: 87.50%,  total acc: 62.93%   [EVAL] batch:  447 | acc: 100.00%,  total acc: 63.02%   [EVAL] batch:  448 | acc: 87.50%,  total acc: 63.07%   [EVAL] batch:  449 | acc: 87.50%,  total acc: 63.12%   [EVAL] batch:  450 | acc: 81.25%,  total acc: 63.17%   [EVAL] batch:  451 | acc: 87.50%,  total acc: 63.22%   [EVAL] batch:  452 | acc: 93.75%,  total acc: 63.29%   [EVAL] batch:  453 | acc: 56.25%,  total acc: 63.27%   [EVAL] batch:  454 | acc: 75.00%,  total acc: 63.30%   [EVAL] batch:  455 | acc: 68.75%,  total acc: 63.31%   [EVAL] batch:  456 | acc: 81.25%,  total acc: 63.35%   [EVAL] batch:  457 | acc: 50.00%,  total acc: 63.32%   [EVAL] batch:  458 | acc: 50.00%,  total acc: 63.29%   [EVAL] batch:  459 | acc: 75.00%,  total acc: 63.32%   [EVAL] batch:  460 | acc: 68.75%,  total acc: 63.33%   [EVAL] batch:  461 | acc: 43.75%,  total acc: 63.28%   [EVAL] batch:  462 | acc: 81.25%,  total acc: 63.32%   [EVAL] batch:  463 | acc: 100.00%,  total acc: 63.40%   [EVAL] batch:  464 | acc: 100.00%,  total acc: 63.48%   [EVAL] batch:  465 | acc: 100.00%,  total acc: 63.56%   [EVAL] batch:  466 | acc: 100.00%,  total acc: 63.64%   [EVAL] batch:  467 | acc: 100.00%,  total acc: 63.72%   [EVAL] batch:  468 | acc: 93.75%,  total acc: 63.78%   [EVAL] batch:  469 | acc: 68.75%,  total acc: 63.79%   [EVAL] batch:  470 | acc: 81.25%,  total acc: 63.83%   [EVAL] batch:  471 | acc: 62.50%,  total acc: 63.82%   [EVAL] batch:  472 | acc: 68.75%,  total acc: 63.83%   [EVAL] batch:  473 | acc: 75.00%,  total acc: 63.86%   [EVAL] batch:  474 | acc: 43.75%,  total acc: 63.82%   [EVAL] batch:  475 | acc: 100.00%,  total acc: 63.89%   [EVAL] batch:  476 | acc: 100.00%,  total acc: 63.97%   [EVAL] batch:  477 | acc: 100.00%,  total acc: 64.04%   [EVAL] batch:  478 | acc: 100.00%,  total acc: 64.12%   [EVAL] batch:  479 | acc: 100.00%,  total acc: 64.19%   [EVAL] batch:  480 | acc: 93.75%,  total acc: 64.25%   [EVAL] batch:  481 | acc: 100.00%,  total acc: 64.33%   [EVAL] batch:  482 | acc: 100.00%,  total acc: 64.40%   [EVAL] batch:  483 | acc: 100.00%,  total acc: 64.48%   [EVAL] batch:  484 | acc: 100.00%,  total acc: 64.55%   [EVAL] batch:  485 | acc: 100.00%,  total acc: 64.62%   [EVAL] batch:  486 | acc: 100.00%,  total acc: 64.69%   [EVAL] batch:  487 | acc: 93.75%,  total acc: 64.75%   [EVAL] batch:  488 | acc: 81.25%,  total acc: 64.79%   [EVAL] batch:  489 | acc: 93.75%,  total acc: 64.85%   [EVAL] batch:  490 | acc: 100.00%,  total acc: 64.92%   [EVAL] batch:  491 | acc: 81.25%,  total acc: 64.95%   [EVAL] batch:  492 | acc: 93.75%,  total acc: 65.01%   [EVAL] batch:  493 | acc: 81.25%,  total acc: 65.04%   [EVAL] batch:  494 | acc: 100.00%,  total acc: 65.11%   [EVAL] batch:  495 | acc: 87.50%,  total acc: 65.16%   [EVAL] batch:  496 | acc: 93.75%,  total acc: 65.22%   [EVAL] batch:  497 | acc: 100.00%,  total acc: 65.29%   [EVAL] batch:  498 | acc: 93.75%,  total acc: 65.34%   [EVAL] batch:  499 | acc: 93.75%,  total acc: 65.40%   
cur_acc:  ['0.9524', '0.6974', '0.6845', '0.7470', '0.5248', '0.7034', '0.6488', '0.8601']
his_acc:  ['0.9524', '0.8230', '0.7510', '0.7205', '0.6550', '0.6517', '0.6370', '0.6540']
----------END
his_acc mean:  [0.9463 0.8347 0.779  0.7395 0.7156 0.692  0.6734 0.649 ]
